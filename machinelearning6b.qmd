## Replicate Results {.unnumbered}

The tutorial aims to guide the users through fitting machine learning techniques with health survey data. We will replicate some of the results of this article by [Falasinnu et al. (2023)](https://doi.org/10.1016/j.jpain.2023.03.008).

::: column-margin
Falasinnu T, Hossain MB, Weber II KA, Helmick CG, Karim ME, Mackey S. The Problem of Pain in the United States: A Population-Based Characterization of Biopsychosocial Correlates of High Impact Chronic Pain Using the National Health Interview Survey. The Journal of Pain. 2023;24(6):1094-103. DOI: [10.1016/j.jpain.2023.03.008](https://doi.org/10.1016/j.jpain.2023.03.008)
:::

The authors used the [National Health Interview Survey (NHIS) 2016](https://www.cdc.gov/nchs/nhis/nhis_2016_data_release.htm) dataset to develop prediction models for predicting high impact chronic pain (HICP). They also evaluated the predictive performances of the models within sociodemographic subgroups, such as sex (male, female), age ($<65$, $\ge 65$), and race/ethnicity (White, Black, Hispanic). They used LASSO and random forest models with 5-fold cross-validation as an internal validation. To obtain population-level predictions, they account for survey weights in both models.

::: column-margin
For those interested in the National Health Interview Survey (NHIS) dataset, can review the [earlier tutorial](accessing5.html) about the dataset.
:::

::: callout-note
To handle [missing data](missingdata.html) in the predictors, they used multiple imputation technique. However, for simplicity, this tutorial focuses on a complete case dataset. We will also only focus on predicting HICP for people aged 65 years or older (a dataset of \~8,800 participants compared to the dataset of 33,000 participants aged 18 years or older).
:::

### Load packages

We load several R packages required for fitting LASSO and random forest models.

```{r setup, warning=FALSE, message=FALSE, cache=TRUE}
# Load required packages
library(tableone)
library(gtsummary)
library(glmnet)
library(WeightedROC)
library(ranger)
library(scoring)
library(DescTools)
library(ggplot2)
library(mlr3misc)
```

### Analytic dataset

#### Load

We load the dataset into the R environment and lists all available variables and objects.

```{r load, warning=FALSE, message=FALSE, cache = TRUE}
load("Data/machinelearning/Falasinnu2023.RData")
ls()
```

```{r dim, warning=FALSE, message=FALSE, cache = TRUE}
dim(dat)
```

The dataset contains 8,881 participants aged 65 years or older with 49 variables:

-   `studyid`: Unique identifier
-   `psu`: Pseudo-PSU
-   `strata`: Pseudo-stratum
-   `weight`: Sampling weight
-   `HICP`: HICP (binary outcome variable)
-   `age`: Age
-   `sex`: Sex
-   `hhsize`: Number of people in household
-   `born`: Citizenship
-   `marital`: Marital status
-   `region`: Region
-   `race`: Race/ethnicity
-   `education`: Education
-   `employment.status`: Employment status
-   `poverty.status`: Poverty status
-   `veteran`: Veteran
-   `insurance`: Health insurance coverage
-   `sex.orientation`: Sexual orientation
-   `worried.money`: Worried about money
-   `good.neighborhood`: Good neighborhood
-   `psy.symptom`: Psychological symptoms
-   `visit.ED`: Number of times in ER/ED
-   `surgery`: Number of surgeries in past 12 months
-   `dr.visit`: Time since doctor visits
-   `cancer`: Cancer
-   `asthma`: Asthma
-   `htn`: Hypertension
-   `liver.disease`: Liver disease
-   `diabetes`: Diabetes
-   `ulcer`: Ulcer
-   `stroke`: Stroke
-   `emphysema`: Emphysema
-   `copd`: COPD
-   `high.cholesterol`: High cholesterol
-   `coronary.heart.disease`: Coronary heart disease
-   `angina`: Angina pectoris
-   `heart.attack`: Heart attack
-   `heart.disease`: Heart condition/disease
-   `arthritis`: Arthritis and rheumatism
-   `crohns.disease`: Crohn's disease
-   `place.routine.care`: Usual place for routine care
-   `trouble.asleep`: Trouble falling asleep
-   `obese`: Obesity
-   `current.smoker`: Current smoker
-   `heavy.drinker`: Heavy drinker
-   `hospitalization`: Hospital stay days
-   `better.health.status`: Better health status
-   `physical.activity`: Physical activity

See the NHIS 2016 dataset and the article for better understanding of the variables.

#### Complete case data

```{r desc, warning=FALSE, message=FALSE, cache = TRUE}
# Age
table(dat$age, useNA = "always")
```

Let us consider a complete case dataset

```{r completecase, warning=FALSE, message=FALSE, cache = TRUE}
dat.complete <- na.omit(dat)
dim(dat.complete)
```

As we can see, there are 7,280 participants with complete case information. Let's see the descriptive statistics of the predictors stratified by HICP.

#### Descriptive statistics

```{r tab1, warning=FALSE, message=FALSE, cache = TRUE}
# Predictors
predictors <- c("sex", "hhsize", "born", "marital", 
                "region", "race", "education", 
                "employment.status", "poverty.status",
                "veteran", "insurance", 
                "sex.orientation", "worried.money", 
                "good.neighborhood", 
                "psy.symptom", "visit.ED", "surgery", 
                "dr.visit", "cancer", 
                "asthma", "htn", "liver.disease", 
                "diabetes", "ulcer", "stroke",
                "emphysema", "copd", "high.cholesterol",
                "coronary.heart.disease", 
                "angina", "heart.attack", 
                "heart.disease", "arthritis", 
                "crohns.disease", "place.routine.care", 
                "trouble.asleep", "obese", 
                "current.smoker", "heavy.drinker",
                "hospitalization", 
                "better.health.status", 
                "physical.activity")

# Table 1 - Unweighted 
tbl_summary(data = dat.complete, 
            include = predictors, 
            by = HICP, missing = "no") %>% 
  modify_spanning_header(c("stat_1", 
                           "stat_2") ~ "**HICP**")
```

### LASSO for surveys

Now, we will fit the LASSO model for predicting binary HICP with the listed predictors. Similar to the [previous chapter](machinelearning6a.html), we will normalize the weight.

#### Weight normalization

```{r weight, warning=FALSE, message=FALSE, cache = TRUE}
# Normalize weight
dat.complete$wgt <- dat.complete$weight * 
  nrow(dat.complete)/sum(dat.complete$weight)

# Weight summary
summary(dat.complete$weight)
summary(dat.complete$wgt)

# The weighted and unweighted n are equal
nrow(dat.complete)
sum(dat.complete$wgt)
```

#### Folds

Let's create five random folds and specify the regression formula.

```{r folds, warning=FALSE, message=FALSE, cache = TRUE}
k <- 5
set.seed(604)
nfolds <- sample(1:k, 
                 size = nrow(dat.complete), 
                 replace = T)
table(nfolds)
```

#### Formula

```{r formula, warning=FALSE, message=FALSE, cache = TRUE}
Formula <- formula(paste("HICP ~ ", paste(predictors, 
                                          collapse=" + ")))
Formula
```

#### 5-fold CV LASSO

Now, we will fit the LASSO model with 5-fold cross-validation (CV). Here are the steps:

-   For fold 1, folds 2-5 is the training set and fold 1 is the test set
-   Fit 5-fold cross-validation on the training set to find the value of lambda that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.
-   Fit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey design.
-   Calculate predictive performance (e.g., AUC) on the test set
-   Repeat the analysis for all folds.

```{r lassofit, warning=FALSE, message=FALSE, cache = TRUE}
fit.lasso <- list(NULL)
auc.lasso <- NULL
cal.slope.lasso <- NULL
brier.lasso <- NULL
for (fold in 1:k) {
  # Training data
  dat.train <- dat.complete[nfolds != fold, ]
  X.train <- model.matrix(Formula, dat.train)[,-1]
  y.train <- as.matrix(dat.train$HICP)
  
  # Test data
  dat.test <- dat.complete[nfolds == fold, ]
  X.test <- model.matrix(Formula, dat.test)[,-1]
  y.test <- as.matrix(dat.test$HICP)
  
  # Find the optimum lambda using 5-fold CV
  fit.cv.lasso <- cv.glmnet(x = X.train, 
                            y = y.train, 
                            nfolds = 5, 
                            alpha = 1, 
                            family = "binomial", 
                            weights = dat.train$wgt)
  
  # Fit the model on the training set with optimum lambda
  fit.lasso[[fold]] <- glmnet(
    x = X.train, 
    y = y.train, 
    alpha = 1, 
    family = "binomial",
    lambda = fit.cv.lasso$lambda.min,
    weights = dat.train$wgt)
  
  # Prediction on the test set
  dat.test$pred.lasso <- predict(fit.lasso[[fold]], 
                                 newx = X.test, 
                                 type = "response")
  
  # AUC on the test set with sampling weights
  auc.lasso[fold] <- WeightedAUC(
    WeightedROC(dat.test$pred.lasso,
                dat.test$HICP, 
                weight = dat.test$wgt))
  
  # Weighted calibration slope
  mod.cal <- glm(
    HICP ~ Logit(dat.test$pred.lasso), 
    data = dat.test, 
    family = binomial, 
    weights = wgt)
  cal.slope.lasso[fold] <- summary(mod.cal)$coef[2,1]
  
  # Weighted Brier Score
  brier.lasso[fold] <- mean(
    brierscore(HICP ~ dat.test$pred.lasso,
               data = dat.test, 
               wt = dat.test$wgt))
}
```

#### Model performance

Let's check how prediction worked.

```{r lassofit2, warning=FALSE, message=FALSE, cache = TRUE}
# Fitted LASSO models
fit.lasso[[1]]
fit.lasso[[2]]
fit.lasso[[3]]
fit.lasso[[4]]
fit.lasso[[5]]
```

```{r lassofit3, warning=FALSE, message=FALSE, cache = TRUE}
# Intercept from the LASSO models in different folds
fit.lasso[[1]]$a0
fit.lasso[[2]]$a0
fit.lasso[[3]]$a0
fit.lasso[[4]]$a0
fit.lasso[[5]]$a0
```

```{r lassofit4, warning=FALSE, message=FALSE, cache = TRUE}
# Beta coefficients from the LASSO models in different folds
fit.lasso[[1]]$beta
fit.lasso[[2]]$beta
fit.lasso[[3]]$beta
fit.lasso[[4]]$beta
fit.lasso[[5]]$beta
```

```{r lassoauc, warning=FALSE, message=FALSE, cache = TRUE}
# AUCs from different folds
auc.lasso

# Calibration slope from different folds
cal.slope.lasso

# Brier score from different folds
brier.lasso
```

Now we will average out the model performance measures:

```{r lassoaucmean, warning=FALSE, message=FALSE, cache = TRUE}
# Average AUC
mean(auc.lasso)

# Average calibration slope
mean(cal.slope.lasso)

# Average Brier score
mean(brier.lasso)
```

Although the authors used multiple imputation, our AUC from the LASSO model with complete case data analysis is not that different. Note: the authors reported the AUC values in Table 2.

### Random forest for surveys

Now, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model with 5-fold CV:

-   For fold 1, folds 2-5 is the training set and fold 1 is the test set
-   Fit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.
-   Grid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.
-   Fit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey design.
-   Calculate predictive performance (e.g., AUC) on the test set
-   Repeat the analysis for all folds.

#### Folds

```{r foldsrf, warning=FALSE, message=FALSE, cache = TRUE}
k <- 5
table(nfolds)
```

#### Formula

```{r formularf, warning=FALSE, message=FALSE, cache = TRUE}
Formula
```

#### 5-fold CV random forest

```{r rffit, warning=FALSE, message=FALSE, cache = TRUE}
fit.rf <- list(NULL)
auc.rf <- NULL
cal.slope.rf <- brier.rf <- NULL
for (fold in 1:k) {
  # Training data
  dat.train <- dat.complete[nfolds != fold, ]
  
  # Test data
  dat.test <- dat.complete[nfolds == fold, ]
  
  # Tuning the hyperparameters 
  ## Grid with 1000 models - huge time consuming
  #grid.search <- expand.grid(mtry = 1:10, node.size = 1:10, 
  #                          num.trees = seq(50,500,50), 
  #                           OOB_RMSE = 0)
  
  ## Grid with 36 models as an exercise
  grid.search <- expand.grid(
    mtry = 5:7, 
    node.size = 1:3, 
    num.trees = seq(200,500,100),
    OOB_RMSE = 0) 
  
  ## Model with grids 
  for(ii in 1:nrow(grid.search)) {
    # Model on training set with grid
    fit.rf.tune <- ranger(
      formula = Formula,
      data = dat.train, 
      num.trees = grid.search$num.trees[ii],
      mtry = grid.search$mtry[ii], 
      min.node.size = grid.search$node.size[ii],
      importance = 'impurity', 
      case.weights = dat.train$wgt)
    
    # Add Out-of-bag (OOB) error to grid
    grid.search$OOB_RMSE[ii] <- 
      sqrt(fit.rf.tune$prediction.error)
  }
  # Position of the tuned hyperparameters
  position <- which.min(grid.search$OOB_RMSE)
  
  # Fit the model on the training set with tuned hyperparameters
  fit.rf[[fold]] <- ranger(
    formula = Formula,
    data = dat.train, 
    case.weights = dat.train$wgt, 
    probability = T,
    num.trees = grid.search$num.trees[position],
    min.node.size = grid.search$node.size[position], 
    mtry = grid.search$mtry[position], 
    importance = 'impurity')
  
  # Prediction on the test set
  dat.test$pred.rf <- predict(
    fit.rf[[fold]], 
    data = dat.test)$predictions[,2]
  
  # AUC on the test set with sampling weights
  auc.rf[fold] <- WeightedAUC(
    WeightedROC(dat.test$pred.rf, 
                dat.test$HICP, 
                weight = dat.test$wgt))
  
  # Weighted calibration slope
  dat.test$pred.rf[dat.test$pred.rf == 0] <- 0.00001
  mod.cal <- glm(HICP ~ Logit(dat.test$pred.rf), 
                 data = dat.test, 
                 family = binomial, 
                 weights = wgt)
  cal.slope.rf[fold] <- summary(mod.cal)$coef[2,1]
  
  # Weighted Brier Score
  brier.rf[fold] <- mean(brierscore(
    HICP ~ dat.test$pred.rf, 
    data = dat.test,
    wt = dat.test$wgt))
}
```

#### Model performance

Let's check how prediction worked.

```{r rffit2, warning=FALSE, message=FALSE, cache = TRUE}
# Fitted random forest models
fit.rf[[1]]
fit.rf[[2]]
fit.rf[[3]]
fit.rf[[4]]
fit.rf[[5]]
```

```{r rfauc, warning=FALSE, message=FALSE, cache = TRUE}
# AUCs from different folds
auc.rf

# Calibration slope from different folds
cal.slope.rf

# Brier score from different folds
brier.rf
```

Now we will average out the model performance measures:

```{r rfaucmean, warning=FALSE, message=FALSE, cache = TRUE}
# Average AUC
mean(auc.rf)

# Average calibration slope
mean(cal.slope.rf)

# Average Brier score
mean(brier.rf)
```

This AUC from random forest is approximately the same as obtained from the LASSO model.

#### Variable importance

One nice feature of random forest is that we can rank the variables and generate a variable importance plot.

```{r vim, warning=FALSE, message=FALSE, cache = TRUE}
# Fold 1
ggplot(
  enframe(fit.rf[[1]]$variable.importance, 
          name = "variable", 
          value = "importance"),
  aes(x = reorder(variable, importance), 
      y = importance, fill = importance)) +
  geom_bar(stat = "identity", 
           position = "dodge") +
  coord_flip() +
  ylab("Variable Importance") +
  xlab("") + 
  ggtitle("") +
  guides(fill = "none") +
  scale_fill_gradient(low = "grey", 
                      high = "grey10") + 
  theme_bw()

# Fold 5
ggplot(
  enframe(fit.rf[[5]]$variable.importance,
          name = "variable", 
          value = "importance"),
  aes(x = reorder(variable, importance), 
      y = importance, fill = importance)) +
  geom_bar(stat = "identity", 
           position = "dodge") +
  coord_flip() +
  ylab("Variable Importance") +
  xlab("") + 
  ggtitle("") +
  guides(fill = "none") +
  scale_fill_gradient(low = "grey", 
                      high = "grey10") + 
  theme_bw()
```

### References
