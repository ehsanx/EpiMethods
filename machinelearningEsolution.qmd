## Exercise 1 Solution (L) {.unnumbered}

```{r chunkSetup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tableone)
library(survey)
library(Publish)
library(DescTools)
library(glmnet)
```

We will revisit the article by [Flegal et al. (2016)](https://jamanetwork.com/journals/jama/article-abstract/2526639). We will use the same dataset as in the previous lab exercise on [survey data analysis](https://ehsanx.github.io/EpiMethods/surveydataE.html), with some additional predictors in predicting obesity.

Our primary aim is to predict **grade 3 obesity** with the following predictors:

-   Age: Age in years at screening
-   Gender
-   Race: Race/ethnicity
-   Education: Education level
-   Smoking: Smoking status
-   Physical activity: Level of vigorous work activity
-   Sleep: Hours of sleep
-   High blood pressure: Ever doctor told a high blood pressure
-   General health: General health condition

## Question 1: Creating data

### 1(a) Downloading the datasets

You can see how datasets are downloaded and merged:

```{r, eval=FALSE}
library(nhanesA)
# library(SASxport)
library(plyr)

# Demographic data
demo <- nhanes('DEMO_H') # Both males and females: 0 - 150 YEARS
demo1 <- demo[c("SEQN", # Respondent sequence number
                "RIDAGEYR", # Age in years at screening
                "RIAGENDR", # gender
                "DMDEDUC2", # Education level - Adults 20+
                "RIDRETH3", # Race/Hispanic origin w/ NH Asian
                "RIDEXPRG", # Pregnancy status at exam
                "WTINT2YR", #  Full sample 2 year weights
                "SDMVPSU", # Masked variance pseudo-PSU
                "SDMVSTRA")] # Masked variance pseudo-stratum
demo_vars <- names(demo1)
demo2 <- nhanesTranslate('DEMO_H', demo_vars, data = demo1)

# BMI
bmx <- nhanes('BMX_H')
bmx1 <- bmx[c("SEQN", # Respondent sequence number
              "BMXBMI")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS
bmx_vars <- names(bmx1)
bmx2 <- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)

# Smoking
smq <- nhanes('SMQ_H')
smq1 <- smq[c("SEQN", # Respondent sequence number
              "SMQ020", # Smoked at least 100 cigarettes in life
              "SMQ040")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS
smq_vars <- names(smq1)
smq2 <- nhanesTranslate('SMQ_H', smq_vars, data = smq1)

# Physical activity
paq <- nhanes('PAQ_H')
paq1 <- paq[c("SEQN", # Respondent sequence number
              "PAQ605")] # Vigorous work activity
paq_vars <- names(paq1)
paq2 <- nhanesTranslate('PAQ_H', paq_vars, data = paq1)

# Sleep
slq <- nhanes('SLQ_H')
slq1 <- slq[c("SEQN", # Respondent sequence number
              "SLD010H")] # Hours of sleep
slq_vars <- names(slq1)
slq2 <- nhanesTranslate('SLQ_H', slq_vars, data = slq1)

# High blood pressure
bpq <- nhanes('BPQ_H')
bpq1 <- bpq[c("SEQN", # Respondent sequence number
              "BPQ020")] # Ever told you had high blood pressure
bpq_vars <- names(bpq1)
bpq2 <- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)

# General health condition
huq <- nhanes('HUQ_H')
huq1 <- huq[c("SEQN", # Respondent sequence number
              "HUQ010")] # General health condition
huq_vars <- names(huq1)
huq2 <- nhanesTranslate('HUQ_H', huq_vars, data = huq1)

# Combined data
dat.full <- join_all(list(demo2, bmx2, smq2, paq2, slq2, bpq2, huq2), by = "SEQN",
                     type='full') 
dim(dat.full) # N = 10,175
```

### 1(b) Recoding

Let us recode the outcome and predictors to make them suitable for analysis:

```{r, eval=FALSE}
# Survey design
dat.full$survey.weight <- dat.full$WTINT2YR
dat.full$psu <- dat.full$SDMVPSU
dat.full$strata <- dat.full$SDMVSTRA

# Class 3 obesity - BMI >= 40 kg/m^2
summary(dat.full$BMXBMI)
dat.full$obesity <- ifelse(dat.full$BMXBMI >= 40, 1, 0)
table(dat.full$obesity, useNA = "always")

# Age
dat.full$age.cat <- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)
table(dat.full$age.cat, useNA = "always")

# Gender
dat.full$gender <- dat.full$RIAGENDR
table(dat.full$gender, useNA = "always")

# Race/Hispanic origin group
dat.full$race <- dat.full$RIDRETH3
table(dat.full$age.cat, dat.full$race, useNA = "always")
dat.full$race <- car::recode(dat.full$race, 
                             " 'Non-Hispanic White'='White'; 
                             'Non-Hispanic Black' = 'Black'; 
                             c('Mexican American','Other Hispanic')='Hispanic'; 
                             c('Non-Hispanic Asian', 
                             'Other Race - Including Multi-Rac')= 'Other';
                             else=NA", 
                             levels = c("White", "Black", "Hispanic", "Other"))
table(dat.full$race, useNA = "always")

# Education
dat.full$education <- dat.full$DMDEDUC2
dat.full$education <- car::recode(dat.full$education, 
                                  " c('Some college or AA degree', 
                                  'College graduate or above') = '>High school'; 
                                  'High school graduate/GED or equi' = 'High school';
                                  c('Less than 9th grade',
                                  '9-11th grade (Includes 12th grad') = 
                                  '<High school'; 
                                  else = NA", 
                                  levels = c("<High school", "High school", 
                                                    ">High school"))
table(dat.full$education, useNA = "always")

# Smoking status
dat.full$smoking <- dat.full$SMQ020
table(dat.full$smoking, useNA = "always")
dat.full$smoking <- car::recode(dat.full$smoking, " 'Yes'='Current smoker'; 
                                'No'='Never smoker'; else=NA  ",
                                levels = c("Never smoker", "Former smoker", 
                                           "Current smoker"))
dat.full$smoking[dat.full$SMQ040 == "Not at all"] <- "Former smoker"
table(dat.full$smoking, useNA = "always")

# Physical activity
dat.full$physical.activity <- dat.full$PAQ605
table(dat.full$physical.activity, useNA = "always")
dat.full$physical.activity <- car::recode(dat.full$physical.activity, 
                                          " 'Yes'='Yes'; 'No'='No'; else=NA ", 
                                          levels = c("No", "Yes"))
table(dat.full$physical.activity, useNA = "always")

# Sleep
dat.full$sleep <- dat.full$SLD010H
dat.full$sleep <- car::recode(dat.full$sleep, " 1:6 = 'Less than 7'; 7:9 = '7-9'; 
                              10:24 = 'More than 9';  else=NA ",
                              levels = c("Less than 7", "7-9", "More than 9"))
table(dat.full$sleep, useNA = "always")

# High blood pressure
dat.full$high.blood.pressure <- dat.full$BPQ020
table(dat.full$high.blood.pressure, useNA = "always")
dat.full$high.blood.pressure <- car::recode(dat.full$high.blood.pressure, 
                                            " 'Yes'='Yes'; 'No'='No'; else=NA ",
                                            levels = c("No", "Yes"))
table(dat.full$high.blood.pressure, useNA = "always")

# General health condition
dat.full$general.health <- dat.full$HUQ010
table(dat.full$general.health, useNA = "always")
dat.full$general.health <- car::recode(dat.full$general.health, 
                                       "c('Excellent,', 'Very good,')=
                                       'Very good or Excellent'; 
                                       'Good,'='Good';
                                       c('Fair, or', 'Poor?') ='Poor or Fair'; 
                                       else=NA  ",
                                       levels = c("Poor or Fair", "Good", 
                                                  "Very good or Excellent"))
table(dat.full$general.health, useNA = "always")
```

### 1(c) Keep relevant variables

Let's keep only the relevant variables for this exercise:

```{r, eval=FALSE}
# Keep relevant variables
vars <- c(
  # Unique identifier
  "SEQN", 
  
  # Survey features
  "survey.weight", "psu", "strata", 
  
  # Eligibility
  "RIDAGEYR", "BMXBMI", "RIDEXPRG",
  
  # Outcome
  "obesity", 
  
  # Predictors
  "age.cat", "gender", "race", "education", "smoking", "physical.activity", 
  "sleep", "high.blood.pressure", "general.health")

dat.full2 <- dat.full[,vars]
```

### 1(d) Weight normalization

Large weights can cause issues when evaluating model performance. Let's normalize the survey weights to address this problem:

```{r, eval=FALSE}
dat.full2$wgt <- dat.full2$survey.weight * nrow(dat.full2)/sum(dat.full2$survey.weight)
summary(dat.full2$wgt)
```

### 1(e) Analytic dataset

The authors restricted their study to - adults aged 20 years and more, - non-missing body mass index, and - non-pregnant

```{r, eval=FALSE}
# Aged 20 years or more
dat.analytic <- subset(dat.full2, RIDAGEYR>=20) # N = 5,769

# Non-missing outcome
dat.analytic <- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520

# Non-pregnant
table(dat.analytic$RIDEXPRG)
dat.analytic <- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != 
                         "Yes, positive lab pregnancy test") # N = 5,455
nrow(dat.analytic)

# Drop irrelevant variables
dat.analytic$RIDAGEYR <- dat.analytic$BMXBMI <- dat.analytic$RIDEXPRG <- NULL
```

### 1(f) Complete case data

Below is the code for creating the complete case dataset (no missing for the outcome or predictors):

```{r, eval=FALSE}
# Drop missing values
dat.complete <- dat.analytic[complete.cases(dat.analytic),] # N = 5,433
```

### 1(g) Save daatsets

```{r, eval=FALSE}
save(dat.full, dat.full2, dat.analytic, dat.complete, file = "Data/machinelearning/Flegal2016_v2.RData")
```

## Question 2: Importing data and creating Table 1

### 2(a) Importing dataset

Let's load the dataset:

```{r}
load("Data/machinelearning/Flegal2016_v2.RData")
ls()
```

Here,

-   dat.full: the full dataset with all variables
-   dat.full2: the full dataset with only relevant variables for this exercise
-   dat.analytic: the analytic dataset with only adults aged 20 years and more, non-missing BMI, and non-pregnant
-   dat.complete: the complete case dataset without missing values in the outcome and predictors

```{r}
names(dat.full2)
```

### 2(b) Creating Table 1

Let's create Table 1 for the complete case dataset with unweighted frequencies:

```{r}
library(tableone)
predictors <- c("age.cat", "gender", "race", "education", "smoking", 
                "physical.activity", "sleep", "high.blood.pressure", 
                "general.health")
tab1 <- CreateTableOne(vars = predictors, strata = "obesity", 
                       data = dat.complete, test = F, addOverall = T)
print(tab1, format="f") # Showing only frequencies 
```

## Question 3: Prediction using split sample approach

In this exercise, we will use the split-sample approach to predict obesity. We will create our training and test data using a 60-40 split for the training and test data. We will use the following two methods to predict obesity:

-   Design-adjusted logistic with all survey features (psu, strata, and survey weights)
-   LASSO with survey weights

### 3(a) Split the data into training and test

Let us create our training and test data using the split-sample approach:

```{r}
set.seed(900)
dat.complete$datasplit <- rbinom(nrow(dat.complete), size = 1, prob = 0.6) 
table(dat.complete$datasplit)

# Training data
dat.train <- dat.complete[dat.complete$datasplit == 1,]
dim(dat.train)

# Test data
dat.test <- dat.complete[dat.complete$datasplit == 0,]
dim(dat.test)
```

### 3(b) Prediction with design-adjusted logistic

We will use the design-adjusted logistic regression to predict obesity with the following predictors:

-   age.cat, gender, race, education, smoking, physical.activity, sleep, high.blood.pressure, general.health

Instructions:

-   1: Create the survey design on the full data and subset the design for those individuals in the training data.
-   2: Use the **training data design** created in step 1 to fit the model
-   3: Use the test data to predict the probability of obesity.
-   4: Calculate AUC on the test data.
-   5: Calculate calibration slope with 95% confidence interval on the test data.

Hints:

-   `WeightedAUC` and `WeightedROC` are helpful functions in calculating AUC.
-   The `Logit` function from the `DescTools` package is helpful in calculating the logit of predicted probabilities for calculating calibration slope.
-   Use the **normalized weight** variable to calculate the AUC and calibration slope.

#### Model

```{r, warning=FALSE, message=FALSE}
library(survey)
library(DescTools)
library(WeightedROC)
library(Publish)

# Design
dat.full2$miss <- 1
dat.full2$miss[dat.full2$SEQN %in% dat.train$SEQN] <- 0
svy.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,
                         data = dat.full2, nest = TRUE)
svy.design <- subset(svy.design0, miss == 0)

# Formula
Formula <- formula(paste("obesity ~ ", paste(predictors, collapse=" + ")))

# Model
fit.glm <- svyglm(Formula, design = svy.design, family = binomial)

# Prediction on the test set
dat.test$pred.glm <- predict(fit.glm, newdata = dat.test, type = "response")

# AUC on the test set with sampling weights
auc.glm <- WeightedAUC(WeightedROC(dat.test$pred.glm, dat.test$obesity, 
                                   weight = dat.test$wgt))
auc.glm

# Weighted calibration slope
dat.test$pred.glm.logit <- DescTools::Logit(dat.test$pred.glm)
slope.glm <- glm(obesity ~ pred.glm.logit, data = dat.test, family = quasibinomial,
                 weights = wgt)
publish(slope.glm)
```

#### Interpretation \[optional\]

Interpret the AUC and calibration slope based on the following criteria:

| AUC       | Interpretation                     |
|-----------|------------------------------------|
| 0.50      | No better than a random chance     |
| 0.51-0.70 | Poor discrimination ability        |
| 0.71-0.80 | Acceptable discrimination ability  |
| 0.81-0.90 | Excellent discrimination ability   |
| 0.90-1.00 | Outstanding discrimination ability |

| Calibration slope            | Interpretation   |
|------------------------------|------------------|
| 1 and 95% CI includes 1      | Well-calibration |
| Significantly less than 1    | Overfitting      |
| Significantly greater than 1 | Underfitting     |

### 3(c) Prediction with LASSO

Now we will use the LASSO method to predict obesity. We will incorporate sampling weights in the model to account for survey data (no psu or strata). Note that we are not interested in the statistical significance of the beta coefficients. Hence, not utilizing psu and strata should not be an issue in this prediction problem.

Instructions:

-   1: Use the training data with normalized weight to fit the model.
-   2: Find the optimum lambda using 5-fold cross-validation. Consider the lambda value that gives the minimum prediction error.
-   3: Predict the probability of obesity on the test set
-   3: Calculate AUC on the test data.
-   4: Calculate calibration slope with 95% confidence interval on the test data.

#### Model

```{r, warning=FALSE, message=FALSE}
library(glmnet)
library(DescTools)
library(WeightedROC)

# Training data - X: predictor, y: outcome
X.train <- model.matrix(Formula, dat.train)[,-1] 
y.train <- as.matrix(dat.train$obesity) 

# Test data - X: predictor, y: outcome
X.test <- model.matrix(Formula, dat.test)[,-1] 
y.test <- as.matrix(dat.test$obesity)

# Find the best lambda using 5-fold CV
fit.cv.lasso <- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, 
                          family = "binomial", weights = dat.train$wgt)

# Prediction on the test set
dat.test$pred.lasso <- predict(fit.cv.lasso, newx = X.test, type = "response", 
                               s = fit.cv.lasso$lambda.min)

# AUC on the test set with sampling weights
auc.lasso <- WeightedAUC(WeightedROC(dat.test$pred.lasso, dat.test$obesity, 
                                     weight = dat.test$wgt))
auc.lasso

# Weighted calibration slope
dat.test$pred.lasso.logit <- DescTools::Logit(dat.test$pred.lasso)
slope.lasso <- glm(obesity ~ pred.lasso.logit, data = dat.test, 
                   family = quasibinomial, weights = wgt)
publish(slope.lasso)
```

#### Interpretation \[optional\]

Interpret the AUC and calibration slope based on the following criteria:

| AUC       | Interpretation                     |
|-----------|------------------------------------|
| 0.50      | No better than a random chance     |
| 0.51-0.70 | Poor discrimination ability        |
| 0.71-0.80 | Acceptable discrimination ability  |
| 0.81-0.90 | Excellent discrimination ability   |
| 0.90-1.00 | Outstanding discrimination ability |

| Calibration slope            | Interpretation   |
|------------------------------|------------------|
| 1 and 95% CI includes 1      | Well-calibration |
| Significantly less than 1    | Overfitting      |
| Significantly greater than 1 | Underfitting     |

## Question 4: Prediction using croos-validation approach \[optional\]

Use LASSO with 5-fold cross-validation to predict obesity with the same set of predictors used in Question 2. Report the average AUC and average calibration slope with 95% confidence interval over 5 folds.

```{r, eval=T, warning=FALSE, message=FALSE}
library(glmnet)
library(DescTools)
library(WeightedROC)

k <- 5
set.seed(604)
nfolds <- sample(1:k, size = nrow(dat.complete), replace = T)
table(nfolds)

auc.lasso <- cal.slope.lasso <- cal.slope.se.lasso <- NULL
for (fold in 1:k) {
  # Training data
  dat.train <- dat.complete[nfolds != fold, ]
  X.train <- model.matrix(Formula, dat.train)[,-1]
  y.train <- as.matrix(dat.train$obesity)
  
  # Test data
  dat.test <- dat.complete[nfolds == fold, ]
  X.test <- model.matrix(Formula, dat.test)[,-1]
  y.test <- as.matrix(dat.test$obesity)
  
  # Find the optimum lambda using 5-fold CV
  fit.cv.lasso <- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, 
                            family = "binomial", weights = dat.train$wgt)

  # Prediction on the test set
  dat.test$pred.lasso <- predict(fit.cv.lasso, newx = X.test, type = "response", 
                                 s = fit.cv.lasso$lambda.min)
  
  # AUC on the test set with sampling weights
  auc.lasso[fold] <- WeightedAUC(WeightedROC(dat.test$pred.lasso,dat.test$obesity, 
                                             weight = dat.test$wgt))
  
  # Weighted calibration slope
  dat.test$pred.lasso.logit <- DescTools::Logit(dat.test$pred.lasso)
  mod.cal <- glm(obesity ~ pred.lasso.logit, data = dat.test, family = binomial, 
                 weights = wgt)
  cal.slope.lasso[fold] <- summary(mod.cal)$coef[2,1]
  cal.slope.se.lasso[fold] <- summary(mod.cal)$coef[2,2]
}

# Average AUC
mean(auc.lasso)

# Average calibration slope
mean(cal.slope.lasso)

# 95% CI for calibration slope
cbind(mean(cal.slope.lasso) - 1.96 * mean(cal.slope.se.lasso), 
      mean(cal.slope.lasso) + 1.96 * mean(cal.slope.se.lasso))
```
