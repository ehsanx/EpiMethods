## Collinear predictors {.unnumbered}

In this tutorial, we'll continue with the data analysis. We'll focus on an analysis of an NHANES data. This data contains over 1200 observations and 33 variables. These variables come in various types: numeric, categorical, and binary. Our primary goal is to fit a linear regression model to predict cholesterol levels.


```{r setup, warning=FALSE, cache=TRUE, message=FALSE}
# Load required packages
library(rms)
library(Hmisc)
```

### Load data

Let us load the dataset and see structure of the variables:

```{r load, warning=FALSE, cache=TRUE}
load(file = "Data/predictivefactors/cholesterolNHANES15.RData")
#head(analytic)
str(analytic)
```

### Describe the data

```{r data, warning=FALSE, cache=TRUE}
require(rms)
describe(analytic) 
```

### Collinearity

Avoiding collinear variables can result in a more interpretable, stable, and efficient predictive model. Collinearity refers to a situation in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with substantial accuracy. Collinearity poses several issues for predictive analysis:

**Reduced Interpretability**:
When predictor variables are highly correlated, it becomes challenging to isolate the impact of individual predictors on the response variable. In other words, it is difficult to determine which predictor is genuinely influential in explaining variance in the response variable. This reduces the interpretability of the model.

**Unstable Coefficients**:
Collinearity can lead to inflated standard errors of the regression coefficients. This means that the coefficients can be very sensitive to small changes in the data, making the model unstable and less generalizable to new, unseen data.

**Overfitting**:
When predictors are collinear, the model is more likely to fit to the noise in the data rather than the actual signal. This is a manifestation of overfitting, where the model becomes too complex and captures random noise. Overfitted models will perform poorly on new, unseen data.

**Inefficiency**:
Including redundant variables (collinear variables) does not add additional information to the model. This could be inefficient, especially when dealing with large datasets, as it increases computational costs without improving model performance.

#### Multicollinearity Diagnostics
Several techniques are available for diagnosing multicollinearity, including:

- [Variance Inflation Factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor): The VIF is a measure of multicollinearity, which measures how much the variance of a regression coefficient is increased because of multicollinearity. A general rule of thumb is that if VIF exceeds 4 or 5, multicollinearity is present. 

- [Eigenvalues and Eigenvectors of the correlation/covariance matrix](https://sam-black.medium.com/analyzing-the-eigenvalues-of-a-covariance-matrix-to-identify-multicollinearity-de488c719e1b): The eigenvalues of the correlation matrix can also be used to measure the presence of multicollinearity, with one or more eigenvalues close to zero indicating multicollinearity.

- Pairwise correlation matrices: Large correlation coefficients in the correlation matrix of predictors indicate the possibility of multicollinearity. 

#### Remedies

Some common ways to handle collinearity include:

- Removing one of the correlated predictors
- Combining correlated predictors into a single composite predictor
- Using regularization techniques like [Ridge Regression](https://en.wikipedia.org/wiki/Ridge_regression), which can help handle collinearity by adding a penalty term. Ridge regression adds a small amount of bias to the regression estimates, which in turn reduces the standard error, addresses the collinearity issue to some extent, and makes the results more reliable.

### Identify collinear predictors

::: callout-tip
We can also use `hclust` and `varclus` or variable clustering, i.e., to identify collinear predictors
:::

::: column-margin
`hclust` is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.
:::

```{r collinear, warning=FALSE, cache=TRUE}
require(Hmisc)
sel.names <- c("gender", "age", "born", "race", "education", 
               "married", "income", "diastolicBP", "systolicBP", 
               "bodyweight", "bodyheight", "bmi", "waist", "smoke",
               "alcohol",  "cholesterol", "triglycerides", 
               "uric.acid", "protein", "bilirubin", "phosphorus",
               "sodium", "potassium", "globulin", "calcium", 
               "physical.work", "physical.recreational", 
               "diabetes")
var.cluster <- varclus(~., data = analytic[sel.names])
# var.cluster
plot(var.cluster)
```

In this plot, [Spearman correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) or Spearman's $\rho$(rho) assesses the monotonic but nonlinear relationships predictors. We can also specify linear relationships such as Pearson correlation. A high correlation coefficient indicates predictors are highly correlated. For example, Spearman's $\rho$ is more than 0.8 for bodyweight, BMI, and waist, indicating high collinearity between these predictors.

### Video content (optional)

::: callout-tip
For those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.
:::

::: {style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"}
<iframe src="https://www.youtube.com/embed/5nNZpsuQ_Fg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen>

</iframe>
:::
