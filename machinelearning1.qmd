## Continuous outcome {.unnumbered}

::: callout-important
This tutorial is very similar to one of the [previous tutorials](predictivefactors2.html), but uses a different data (we used [RHC data](researchquestion1.html) here). We are revisiting concepts related to [prediction](predictivefactors.html) before introducing ideas related to machine learning.
:::

In this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction model.

```{r setup02, include=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
require(tableone)
require(Publish)
require(MatchIt)
require(cobalt)
require(ggplot2)
require(caret)
```

### Load dataset

```{r data, cache=TRUE}
ObsData <- readRDS(file = "Data/machinelearning/rhcAnalytic.RDS")
head(ObsData)
```

### Prediction for length of stay

Now, we show the regression fitting when outcome is continuous (length of stay).

#### Variables

```{r vars2, cache=TRUE, echo = TRUE}
baselinevars <- names(dplyr::select(ObsData, 
                         !c(Length.of.Stay,Death)))
baselinevars
```

#### Model

```{r reg2, cache=TRUE, echo = TRUE, results='hide'}
# adjust covariates
out.formula1 <- as.formula(paste("Length.of.Stay~ ", 
                               paste(baselinevars, 
                                     collapse = "+")))
saveRDS(out.formula1, file = "Data/machinelearning/form1.RDS")
fit1 <- lm(out.formula1, data = ObsData)
require(Publish)
adj.fit1 <- publish(fit1, digits=1)$regressionTable
```

```{r reg2a, cache=TRUE, echo = TRUE}
out.formula1
adj.fit1
```

#### Design Matrix

-   Notations
    -   n is number of observations
    -   p is number of covariates

Expands factors to a set of dummy variables.

```{r mat0, cache=TRUE}
dim(ObsData)
length(attr(terms(out.formula1), "term.labels"))
```

```{r mat1, cache=TRUE}
head(model.matrix(fit1))
dim(model.matrix(fit1))
p <- dim(model.matrix(fit1))[2] # intercept + slopes
p
```

#### Obtain prediction

```{r pred, cache=TRUE}
obs.y <- ObsData$Length.of.Stay
summary(obs.y)
# Predict the above fit on ObsData data
pred.y1 <- predict(fit1, ObsData)
summary(pred.y1)
n <- length(pred.y1)
n
plot(obs.y,pred.y1)
lines(lowess(obs.y,pred.y1), col = "red")
```

### Measuring prediction error

**Prediction error** measures how well the model can predict the outcome for new data that were **not** used in developing the prediction model.

-   Bias reduced for models with more variables
-   Unimportant variables lead to noise / variability
-   Bias variance trade-off / need penalization

#### R2


The provided information describes a statistical context involving a dataset of `n` values, $y_1, ..., y_n$ (referred to as $y_i$ or as a vector $y = [y_1,...,y_n]^T$), each paired with a fitted value $f_1,...,f_n$ (denoted as $f_i$ or sometimes $\hat{y_i}$, and as a vector $f$). The residuals, represented as $e_i$, are defined as the differences between the observed and the fitted values: 
$ e_i = y_i − f_i$ 

The mean of the observed data is denoted by 
$$ \bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i $$

The variability of the dataset can be quantified using two sums of squares formulas:
1. **Residual Sum of Squares (SSres)** or `SSE`: It quantifies the variance remaining in the data after fitting a model, calculated as:
$$ SS_{res} = \sum_{i}(y_i - f_i)^2 = \sum_{i}e_i^2 $$
2. **Total Sum of Squares (SStot)** or `SST`: It represents the total variance in the observed data, calculated as:
$$ SS_{tot} = \sum_{i}(y_i - \bar{y})^2 $$

The **Coefficient of Determination (R²)** or `R.2`, which provides a measure of how well the model's predictions match the observed data, is defined as:
$$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$

In the ideal scenario where the model fits the data perfectly, we have $SS_{res} = 0$ and thus $R^2 = 1$. Conversely, a baseline model, which always predicts the mean $\bar{y}$ of the observed data, would yield $R^2 = 0$. Models performing worse than this baseline model would result in a negative R² value. This metric is widely utilized in regression analysis to evaluate model performance, where a higher R² indicates a better fit of the model to the data.


```{r r2, cache=TRUE}
# Find SSE
SSE <- sum( (obs.y - pred.y1)^2 )
SSE
# Find SST
mean.obs.y <- mean(obs.y)
SST <- sum( (obs.y - mean.obs.y)^2 )
SST
# Find R2
R.2 <- 1- SSE/SST
R.2
require(caret)
caret::R2(pred.y1, obs.y)
```

[ref](https://en.wikipedia.org/wiki/Coefficient_of_determination)


#### RMSE

```{r rmse, cache=TRUE}
# Find RMSE
Rmse <- sqrt(SSE/(n-p)) 
Rmse
caret::RMSE(pred.y1, obs.y)
```

See [@anova]


#### Adj R2

The **Adjusted R²** statistic modifies the $R^2$ value to counteract the automatic increase of $R^2$ when extra explanatory variables are added to a model, even if they do not improve the model fit. This adjustment is crucial for ensuring that the metric offers a reliable indication of the explanatory power of the model, especially in multiple regression where several predictors are involved.

The commonly used formula  is defined as:

$$
\bar{R}^{2} = 1 - \frac{SS_{\text{res}} / df_{\text{res}}}{SS_{\text{tot}} / df_{\text{tot}}}
$$

Where:

- $SS_{\text{res}}$ and $SS_{\text{tot}}$ represent the residual and total sums of squares respectively.
- $df_{\text{res}}$ and $df_{\text{tot}}$ refer to the degrees of freedom of the residual and total sums of squares. Usually, $df_{\text{res}} = n - p$ and $df_{\text{tot}} = n - 1$, where:
  - $n$ signifies the sample size.
  - $p$ denotes the number of variables in the model.

This metric plays a vital role in model selection and safeguards against overfitting by penalizing the inclusion of non-informative variables

The alternate formula is:

$$
\bar{R}^2 = 1 - (1 - R^2) \frac{n-1}{n-p-1}
$$

This formula modifies the $R^2$ value, accounting for the number of predictors and offering a more parsimonious model fit measure. 

```{r adjr2, cache=TRUE}
# Find adj R2
adjR2 <- 1-(1-R.2)*((n-1)/(n-p-1))
adjR2
```

See [@coefd]

### Overfitting and Optimism

-   Model usually performs very well in the empirical data where the model was fitted in the same data (optimistic)
-   Model performs poorly in the new data (generalization is not as good)

#### Causes

-   Model determined by data at hand without expert opinion
-   Too many model parameters ($age$, $age^2$, $age^3$) / predictors
-   Too small dataset (training) / data too noisy

#### Consequences

-   Overestimation of effects of predictors
-   Reduction in model performance in new observations

#### Proposed solutions

We generally use procedures such as

-   Internal validation
    -   sample splitting
    -   cross-validation
    -   bootstrap
-   External validation
    -   Temporal
    -   Geographical
    -   Different data source to calculate same variable
    -   Different disease

### Video content (optional)

::: callout-tip
For those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.
:::

::: {style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"}
<iframe src="https://www.youtube.com/embed/TmrG2ZbeJoY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen>

</iframe>
:::

### References
