## Data spliting {.unnumbered}

This section of the code is focused on a crucial aspect of model building: splitting your data into training and test sets to avoid overfitting. Overfitting occurs when your model learns the noise in the data, rather than the underlying trend. As a result, the model performs well on the training data but poorly on new, unseen data. To mitigate this, you often split your data.

### Load data anf files

Initially, several libraries are loaded to facilitate data manipulation and analysis. 

```{r setup, warning=FALSE, message=FALSE,cache=TRUE}
# Load required packages
library(caret)
library(knitr)
library(Publish)
library(car)
library(DescTools)
```

Then, previously saved data related to cholesterol and other factors is loaded for further use.

```{r load, warning=FALSE, cache=TRUE}
load(file="Data/predictivefactors/cholesterolNHANES15part2.RData")
```

### Data spliting to avoid model overfitting

You start by setting a random seed to ensure that the random splitting of data is reproducible. A specified function is then used to partition the data, taking as arguments the outcome variable (cholesterol level in this case) and the percentage of data that you want to allocate to the training set (70% in this example).

::: column-margin
@datasplit
:::

::: column-margin
@datasplit2
:::

::: callout-tip
We can use the `createDataPartition` function to split a dataset into training and testing datasets. The function will return the row indices that should go into the training set. These indices are stored in a variable, and its dimensions are displayed to provide an understanding of the size of the training set that will be created. Additionally, you can calculate what 70% of your entire dataset would look like to verify the approximation of the training data size, as well as what the remaining 30% (for the test set) would look like.
:::

```{r split0, warning=FALSE, cache=TRUE}
# Using a seed to randomize in a reproducible way 
set.seed(123)
split <- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)
str(split)
dim(split)

# Approximate train data
dim(analytic3)*.7 

# Approximate test data
dim(analytic3)*(1-.7) 
```

#### Split the data

After determining how to partition the data, the next step is to actually create the training and test datasets. The indices are used to subset the original dataset into these two new datasets. The dimensions of each dataset are displayed to confirm their sizes.

```{r split1, warning=FALSE, cache=TRUE}
# Create train data
train.data <- analytic3[split,]
dim(train.data)

# Create test data
test.data <- analytic3[-split,]
dim(test.data)
```

Our next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.

#### Train the model

Once the training dataset is created, you can proceed to train the model using the training data. A previously defined formula containing the predictor variables is used in a linear regression model. After fitting the model, a summary is generated to display key statistics that help in evaluating the model's performance.

```{r split2, warning=FALSE, cache=TRUE}
formula4
fit4.train1 <- lm(formula4, data = train.data)
summary(fit4.train1)
```

#### Extract performance measures

You can use a saved function to measure the performance of the trained model. The function will return performance metrics like R-squared, RMSE, etc. This function is applied not just on the training data but also on the test data, the full dataset, and a separate, fictitious dataset.

::: callout-tip
Below we use the `perform` function that we saved to evaluate the model performances
:::

```{r perf, warning=FALSE, cache=TRUE}
perform(new.data = train.data,y.name = "cholesterol", model.fit = fit4.train1)
perform(new.data = test.data,y.name = "cholesterol", model.fit = fit4.train1)
perform(new.data = analytic3,y.name = "cholesterol", model.fit = fit4.train1)
perform(new.data = fictitious.data,y.name = "cholesterol", model.fit = fit4.train1)
```

Evaluating the model's performance on the test data provides insights into how well the model will generalize to new, unseen data. Comparing the performance metrics across different datasets can give you a robust view of your model's predictive power and reliability.

::: column-margin
For more on model training and tuning, see @tuning
:::

### References
