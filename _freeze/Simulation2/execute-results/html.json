{
  "hash": "5b52f1166e639c9603f062c6da4040c9",
  "result": {
    "markdown": "## Performance Measures {.unnumbered}\n\nNow that we understand Monte Carlo simulation, how can we evaluate how close our Monte Carlo estimates are to the true parameter value? To do this, we can use numerical techniques called **Performance Measures**.\n\nPerformance measures are calculations used to assess how well our simulation method estimates the parameter of interest [@Sim_Morris]\n\nThere are various ways to evaluate the accuracy of a Monte Carlo simulation. However, the choice of method depends on the aim of the simulation and the parameter being estimated.\n\n\n::: {.cell hash='Simulation2_cache/html/unnamed-chunk-1_2ec7853e07b4f96d81121e19905bfda2'}\n\n```{.r .cell-code}\n# Load Required Packages\nrequire(ggplot2)\nrequire(rsimsum)\n```\n:::\n\n\n### Convergence\n\nOne measure that we have already encountered, without explicitly mentioning it, is convergence! Convergence tells us how quickly our estimate stabilizes at a value close to or equivalent to the true parameter.\n\nThis can be assessed using a trace plot. If the estimate stabilizes after a relatively small number of iterations, this suggests the method/estimator is producing stable estimates with fewer iterations.\n\nLet's take a look again at the trace plot from our coin flip example:\n\n\n::: {.cell hash='Simulation2_cache/html/unnamed-chunk-2_fc1d49a6aad3146637129bc91e628d31'}\n\n```{.r .cell-code}\n# Simulate flipping a fair coin 5 times and count how often the sum of heads equals 3\nset.seed(123)  # Set seed for reproducibility\n\n# Establish variables prior to simulation\ncount <- 0    # Initialize count variable      \niter <- 100000  # Number of iterations for the simulation    \nsave.sum <- numeric(iter) # Numeric vector to store flip results\nresults <- numeric(iter)  # Numeric vector to store cumulative parameter estimate\n\n# Loop through a specified number of iterations\nfor(n in 1:iter) {  \n  # Generate a sample of 5 values (either 0 or 1), then sum them\n  save.sum[n] <- sum(sample(c(0,1), 5, replace = TRUE))\n  \n  # Check if the sum of the sampled values equals 3\n  if(save.sum[n] == 3){  \n    count = count + 1  # Increment the count if condition is met\n  }\n  \n  # Compute the cumulative proportion of times \n  # the sum was 3 up to the current iteration\n  results[n] <- count / n  \n}\n\n# Convert results into a data frame for plotting\ntrace_data <- data.frame(\n  Iteration = 1:iter,\n  ProbabilityEstimate = results \n)\n\n# Create a line plot using ggplot2\nggplot(trace_data, aes(x = Iteration, y = ProbabilityEstimate)) +  \n  # Add a blue line to represent probability estimates over iterations\n  geom_line(color = \"blue\") + \n  # Add a horizontal dashed red line at y = 0.3125, \n  # the true probability of filling 3 heads in 5 flips\n  geom_hline(yintercept = 0.3125, linetype = \"dashed\", color = \"red\") + \n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",  # Plot Title\n    x = \"Iteration Number\",  # x-axis label for the x-axis\n    y = \"Estimated Probability\"  # y-axis label\n  ) +\n  theme_minimal()  #ggplot2 minimal theme for clean appearance\n```\n\n::: {.cell-output-display}\n![](Simulation2_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nFrom the plot, we can see that our estimate appears to stabilize around 10,000 iterations, suggesting that the method produces stable estimates of the true probability of flipping exactly 3 heads in 5 coin flips.\n\n------------------------------------------------------------------------\n\nIn addition to observing the rate at which an estimate converges to its true value, mathematical measures can also be calculated to assess how well our simulation is performing. The following performance measures and syntax are referenced from the `rsimsum` package, which we will use to verify our simulations [@Gasparini2018].\n\nFor the explanations of the different performance measures, the following notation will be used:\n\n-   $\\theta$ : the true population parameter, what we are trying to estimate.\n-   $\\hat{\\theta_i}$ : the estimated value of $\\theta$, for the $ith$ iteration of the simulation\n-   $n_{sim}$ : the number of simulations\n-   $i = 1, . . . , n_{sim}$ : the index of the current iteration\n\n------------------------------------------------------------------------\n\n### Bias\n\nBias measures the difference between the Monte Carlo Estimatea and the true parameter. It is calculated as the average difference between each estimate and the true parameter value (i.e., summing all differences and dividing by the number of iterations).\n\n$$\n\\text{Bias} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} ( \\hat{\\theta_i} - \\theta)\n$$ [@Gasparini2024]\n\nWe want the bias to be zero or close to zero, as that indicates that our estimate is accurately capturing the true parameter. Bias can be interpreted as follows:\n\n$$\n\\begin{aligned}\n\\text{Bias} & \\approx 0 \\to \\text{unbiased} \\\\\n\\text{Bias} & > 0 \\to \\text{Biased, Overestimated} \\\\\n\\text{Bias} & < 0 \\to \\text{Biased, Underestimated}\n\\end{aligned}\n$$\n\n[@Biostats]\n\n### Relative Bias\n\nRelative bias measures how much the Monte Carlo estimate differs relative to the true parameter value. While bias provides the absolute difference between the estimate and the true parameter, relative bias expresses this difference as a proportion of the true parameter value.\n\n$$\n\\text{Relative Bias} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} \\bigg(\\frac{\\hat{\\theta_i} - \\theta}{\\theta} \\bigg)\n$$\n\n[@Gasparini2024]\n\n### Empirical Standard Error (Empirical SE)\n\nThe empirical standard error is the empirical standard deviation of the estimates across all simulation iterations. This tells us, on average, how much the Monte Carlo estimates differ from their mean (by summing all the Monte Carlo estimates and dividing by the number of iterations).\n\n$$\n\\text{Empirical Standard Error} = \\sqrt{\\frac{1}{n_{sim} - 1} \\sum^{n_{sim}}_{i=1} (\\hat{\\theta_i} - \\bar{\\theta})^2 }\n$$\n\n[@Gasparini2024]\n\nThe term empirical standard deviation generally refers to the standard deviation computed from observed or sampled data, estimating the population standard deviation. The simulation is drawing a finite number of parameter estimates from a theoretical infinite population of estimates. Since we have a sample, we calculate the empirical standard deviation of the estimates, which is used to approximate the variability of an estimator across repeated samples.\n\n### Model-Based Standard Error (Model SE)\n\nThe Model-Based Standard Error (Model SE) represents the expected variability of an estimator, computed as the average of the estimated standard errors across multiple simulation iterations. In each iteration of a Monte Carlo simulation, we obtain an estimate of the parameter of interest along with an estimated standard error, often derived from a statistical model (e.g., based on likelihood methods or asymptotic approximations). The Model SE is then calculated as the square root of the average of these estimated variances. It essentially reflects how much uncertainty the model predicts for each estimate within a single simulation run.\n\n$$\n\\text{Model SE} = \\sqrt{\\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} \\widehat{\\text{Var}}(\\hat\\theta_i)} \n$$\n\n[@Gasparini2024]\n\nWhere $\\widehat{\\text{Var}}(\\hat\\theta_i)$ is the estimated variance of the estimator $\\theta$ for the $ith$ iteration of the simulation.\n\nIn contrast, the Empirical Standard Error (Empirical SE) measures the actual observed variability of the estimates across all Monte Carlo iterations. It is computed as the standard deviation of the simulated estimates. While Model SE is derived from model-based assumptions at the iteration level, Empirical SE reflects the true variability of the estimator across repeated simulations.\n\nIdeally we would like both the Empirical and Model-based SEs to be low, as this indicates that either the estimates from each simulation replication have small variation to the overall mean across simulation repetitions, or the individual standard errors from each simulation iteration are small.\n\n### Relative % error in model SE\n\nThe Relative % Error in Model SE quantifies how much the model-based estimate of standard error deviates from the actual observed variation in estimates. If the relative error is small, it suggests that the modelâ€™s standard error assumptions align well with reality. However, a large discrepancy (positive or negative) may indicate issues with model misspecification, biased estimators, or incorrect variance assumptions in the statistical model used for inference.\n\n$$\n\\text{Relative % error in Model SE} = 100 \\bigg(\\frac{\\text{Model SE}}{\\text{Empirical SE}} - 1\\bigg)\n$$\n\n[@Gasparini2024]\n\n### Mean Squared Error (MSE)\n\nThe Mean Squared Error (MSE) is the average squared differences between the Monte Carlo estimate and the true parameter value [@Biostats].\n\n$$\n\\text{MSE} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} (\\hat{\\theta_i} - \\theta)^2\n$$\n\n[@Gasparini2024]\n\nThe MSE combines both the Bias and the variance of the estimator [@Biostats].\n\n$$\n\\text{MSE}(\\hat\\theta) = \\text{Bias}(\\hat\\theta, \\theta)^2 + \\text{Var}(\\hat\\theta)\n$$ In the event that our estimator is unbiased (i.e., bias = 0), the MSE would be equal to the variance.\n\n### Coverage\n\nCoverage is the probability that a confidence interval will contain the true parameter, $\\theta$ [@Sim_Morris; @Gasparini2024]. In a 95% confidence interval for $\\hat\\theta$, we expect that 95% of all confidence intervals constructed from our estimates will contain the true parameter, $\\theta$.\n\nA confidence interval provides a range within which the true parameter is expected to fall in with a given probability. Thus, coverage gives the probability that a confidence interval from each simulation iteration contains the true parameter $\\theta$.\n\n$$\n\\text{Coverage} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1}I(\\hat\\theta_{i,low} \\le \\theta \\le \\hat\\theta_{i,upp})\n$$ [@Gasparini2024]\n\nA confidence interval is calculated for each iteration of the simulation, where $\\hat\\theta_{i,low}$, and $\\hat\\theta_{i,upp}$, represent the lower and upper bounds of the $ith$ confidence interval.\n\n$I(\\cdot)$ is an indicator function that returns 1 if the condition inside it is true (i.e., if $\\theta$ is within $\\hat\\theta_{i,low}$, and $\\hat\\theta_{i,upp}$), and returns 0 if the condition is false (i.e., $\\theta$ falls outside the interval)\n\nThus, coverage represents the probability that $\\theta$ is captured within a confidence interval across simulation iterations.\n\n### Bias-eliminated coverage\n\nThe bias-eliminated coverage calculates how often the confidence intervals contain the average estimate $\\bar{\\theta}$ instead of the true parameter $\\theta$. If our estimate is considered bias, using $\\bar{\\theta}$ rather than $\\theta$ eliminates its impact on the coverage calculation [@Sim_Morris; @Gasparini2024].\n\n$$\n\\text{Bias-eliminated coverage} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1}I(\\hat\\theta_{i,low} \\le \\bar\\theta \\le \\hat\\theta_{i,upp})\n$$\n\n[@Gasparini2024]\n\n### Power\n\nPower is a key performance measure in Monte Carlo simulations, ensuring that our tests detect true effects with high probability. It measures the probability of correctly rejecting the null hypothesis (H0) when the alternative hypothesis (Ha) is true. In other words, power quantifies how effective a statistical test is at detecting a real effect when one exists. If power is too low, we may need to increase the sample size or improve the efficiency of our estimator to avoid missing important findings. Conversely, very high power may indicate that the test is too sensitive, potentially detecting trivial effects.\n\n$$\n\\text{Power} = \\frac{1}{n_{sim}} \\sum^{n_{sim}}_{i=1} I\\bigg[| \\hat\\theta_i | \\ge z_{\\alpha/2} \\times \\sqrt{\\widehat{Var}(\\hat\\theta_i)} \\bigg]\n$$\n\n[@Gasparini2024]\n\n### Example\n\nLet's revisit our first example, where we were interested in flipping exactly 3 heads in 5 tosses of a fair coin. We previously established that the theoretical probability of this event is 0.3125. Let's explore the performance measures of this simulation using the `rsimsum` package [@Gasparini2018].\n\nIn this package we will be using the `simsum()` function. This function computed the performance measures.\n\nThe `simsum()` function computes the performance measures. It requires a data.frame in tidy format (`data`), the variable name containing the estimates (`estvarname`), and the true parameter value, which is used to calculate bias, relative bias, coverage, and mean squared error [@Sim_Morris; @Gasparini2018]. Additionally, this function can take on an `se` parameter, which is the variable name containing the standard errors of the estimates for each simulation iteration.\n\n\n::: {.cell hash='Simulation2_cache/html/unnamed-chunk-3_3ca309d20a0ecbc49986c83f71e94122'}\n\n```{.r .cell-code}\n# simsum(\n#   data,\n#   estvarname,\n#   se = NULL,\n#   true = NULL,\n#   methodvar = NULL,\n#   ref = NULL,\n#   by = NULL,\n#   ci.limits = NULL,\n#   df = NULL,\n#   dropbig = FALSE,\n#   x = FALSE,\n#   control = list()\n# )\n\n```\n:::\n\n\nIn this simulation, instead of saving the number of heads each iteration, we save a binary response indicating whether 3 heads have been flipped or not (1 for exactly 3 heads, 0 otherwise). This converts each iteration from a binomial experiment to a Bernoulli trial, where success (1) represents getting exactly 3 heads in 5 flips, and failure (0) represents any other outcome. The probability of success for this Bernoulli trial is 0.3125.\n\n\n::: {.cell hash='Simulation2_cache/html/unnamed-chunk-4_d109a8ae6ac36348c8b5bd5d41f4327a'}\n\n```{.r .cell-code}\nset.seed(123)  # Set seed for reproducibility\n\n# Simulation\niter <- 100000  # Number of iterations for the simulation\nsave.sum <- numeric(iter)  # Numeric vector to store results\n\n# Loop through each iteration\nfor(n in 1:iter) {  \n  # Generate a random sample of 5 values (either 0 or 1) with replacement\n  # Count how many 1s were drawn and check if the sum equals 3\n  # This returns TRUE (1) if exactly 3 ones are drawn, otherwise FALSE (0)\n  save.sum[n] <- sum(sample(c(0,1), 5, replace = TRUE)) == 3  \n}\n\n# Create data frame for rsimsum package analysis\nsim_data <- data.frame(\n  iteration = 1:iter,  # Iteration index\n  prob = save.sum  # Store whether each iteration resulted in a sum of exactly 3\n)\n\n# Analyze the simulated results using the rsimsum package\nsim_analysis <- simsum(\n  data = sim_data,  # Our simulated dataset\n  estvarname = \"prob\",  # Variable name containing event results (TRUE/FALSE)\n  true = 0.3125  # The theoretical probability of getting exactly 3 ones in 5 trials\n)\n\n# Output summary of the analysis\nsim_analysis$summ\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"stat\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"est\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mcse\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"nsim\",\"2\":\"1.000000e+05\",\"3\":\"NA\"},{\"1\":\"thetamean\",\"2\":\"3.103400e-01\",\"3\":\"NA\"},{\"1\":\"thetamedian\",\"2\":\"0.000000e+00\",\"3\":\"NA\"},{\"1\":\"bias\",\"2\":\"-2.160000e-03\",\"3\":\"0.0014629806\"},{\"1\":\"rbias\",\"2\":\"-6.912000e-03\",\"3\":\"0.0046815379\"},{\"1\":\"empse\",\"2\":\"4.626351e-01\",\"3\":\"0.0010344887\"},{\"1\":\"mse\",\"2\":\"2.140338e-01\",\"3\":\"0.0005486177\"},{\"1\":\"relprec\",\"2\":\"NA\",\"3\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe `simsum()` output shows that the mean of all estimates is 0.3103, which is close to our theoretical probability of 0.3125. The bias is -0.00216, indicating that our estimated mean slightly underestimates the true probability, though this bias is very small. The relative bias of -0.691% further confirms this minimal bias.\n\nThe empirical SE is 0.4626. Since we converted our experiment to a Bernoulli trial, we can compare this to the theoretical standard deviation of a Bernoulli random variable with probability 0.3125:\n\n$$\n\\text{Standard Deviation} = \\sqrt{(p\\times(1-p))} = \\sqrt{0.3125*(1-0.3125)} = 0.4635\n$$\n\nThe close match between our empirical SE (0.4626) and the theoretical standard deviation (0.4635) validates that our simulation is accurately capturing the variability of this Bernoulli process. Note that in larger simulations, the empirical SE will converge to the theoretical SE.\n\nThe MSE is 0.214, which should approximately equal the variance since MSE = biasÂ² + variance and our bias is very small. We can verify this:\n\n$$\n\\text{Variance} = \\text{Standard Deviation}^2 = 0.4635^2 =  0.2148\n$$\n\nThe closeness between our MSE (0.214) and the theoretical variance (0.2148) further confirms that our simulation is accurately estimating the properties of this probability experiment.\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}