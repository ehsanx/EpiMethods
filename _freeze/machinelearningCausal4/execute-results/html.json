{
  "hash": "1c977e69e7f15c288017ff833730f5e0",
  "result": {
    "markdown": "## Continuous Outcomes  {.unnumbered}\n\n\n::: {.cell}\n\n:::\n\n\nWe will now go through an example of using TMLE for a continuous outcome. The setup for SuperLearner in this case is similar to that for binary outcomes, so rather than going through the SuperLearner steps again, we will instead focus on the additional steps that are necessary for running the `tmle` method on continuous outcomes.\n\n::: column-margin\n@frank2023implementing extensively discussed the implementation of TMLE for continuous outcomes, providing a detailed step-by-step guide using the openly accessible RHC dataset. In this tutorial, we will revisit the same example with additional explanations.\n:::\n\n::: callout-note\nOnly outcome variable (Length of stay); slightly different than Table 2 in @connors1996effectiveness (means were 20.5 vs. 25.7; and medians were 16 vs. 17).\n:::\n\n\n::: {.cell hash='machinelearningCausal4_cache/html/tab1_c782f3af2ed6362963feec2005303050'}\n\n```{.r .cell-code}\ntab1 <- CreateTableOne(vars = c(\"Length.of.Stay\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#>                             Stratified by RHC.use\n#>                              0             1            \n#>   n                           3551          2184        \n#>   Length.of.Stay (mean (SD)) 19.53 (23.59) 24.86 (28.90)\n```\n:::\n\n::: {.cell hash='machinelearningCausal4_cache/html/tab1x_7c224f8b4c8895f0621a4c1f96227b60'}\n\n```{.r .cell-code}\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==0])\n#> [1] 12\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==1])\n#> [1] 16\n```\n:::\n\n\n### Constructing SuperLearner {-}\n\nJust as we did for a binary outcome, we will need to specify two SuperLearners, one for the exposure and one for the outcome model.\n\nThe effective sample size for a continuous outcome is just $n_{eff}=n=5735$. We calculated the effective sample size for the exposure model earlier, which also turned out to be $n_{eff}=n=5735$. So once again we will use *10 folds* because $500 \\leq n_{eff} \\leq 5000$ [@phillips2023ConsiderationsSL].\n\nSimilarly to our example with the binary outcome, the key considerations for the library of learners are:\n\n-   We have some continuous covariates, and should therefore include learners that allow non-linear/monotonic relationships.\n\n-   We have a large $n$, so should include as many learners as is computationally feasible.\n\n-   We have 49 covariates and 5735 observations, so we do not have high-dimensional data and including screeners is optional.\n\nAgain the requirements for the exposure and outcome models are the same and we can use the same library for both models. Note that even though one model will have a binary dependent variable, and one will have a continuous dependent variable, most of the available learners automatically adapt to binary and continuous dependent variables.\n\nFor this example, we will use the same SuperLearner library as for the binary outcome example.\n\n\n::: {.cell hash='machinelearningCausal4_cache/html/unnamed-chunk-2_46f7ccaab2a0bd69cb3c7a3ab96accad'}\n\n```{.r .cell-code}\n# Construct the SuperLearner library\nSL.library <- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n```\n:::\n\n\n### Dealing with continuous outcomes {-}\n\nFor this example, we will be examining the length of stay in hospital outcome.\n\nThe key difference between running TMLE on a continuous outcome in comparison to running it with a binary outcome, is that we must **transform** the outcome to fall within the range of 0 to 1, so that the modeled outcomes fall within the range of the outcome's true distribution [@gruber2010targeted].\n\nTo transform the outcome, we can use min-max normalization:\n\n$$\nY_{transformed} = \\frac{Y-Y_{min}}{Y_{max}-Y_{min}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y <- min(ObsData$Length.of.Stay)\nmax.Y <- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf <- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n```\n:::\n\n\nOnce we have transformed the outcome to fall within the range of 0 to 1, we can run TMLE as before, using the `tmle` method in the `tmle` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create data frame containing only covariates\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# run tmle\ntmle.fit.cont <- tmle::tmle(Y = ObsData$Length.of.Stay_transf, \n                       A = ObsData$RHC.use, \n                       W = ObsData.noYA, \n                       family = \"gaussian\", \n                       V.Q = 10,\n                       V.g = 10,\n                       Q.SL.library = SL.library,\n                       g.SL.library = SL.library)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nOnce the `tmle` method has run, we still have one step to complete to get our final estimate. At this point, we must transform the average treatment effect generated by the `tmle` method ($\\widehat{ATE}$) back to the outcome's original scale:\n\n$$\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n$$\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# transform back the ATE estimate\ntmle.est.cont <- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$psi\ntmle.est.cont\n#> [1] 5.336911\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nWe also have to transform the confidence interval back to the original scale:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmle.ci.cont <- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$CI\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nATE for continuous outcome: 5.3369112, and 95 % CI is 3.9001351, 6.7797942.\n\nThe results indicate that if all participants had received RHC, the average length of stay in hospital would be 2.95 (1.99, 3.91) days longer than if no participants had received RHC.\n\n### Understanding defaults {-}\n\nTransform outcome:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y <- min(ObsData$Length.of.Stay)\nmax.Y <- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf <- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n```\n:::\n\n\nRun TMLE, using the `tmle` package's default SuperLearner library:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create data frame containing only covariates\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# run tmle\ntmle.fit.cont.def <- tmle::tmle(\n  Y = ObsData$Length.of.Stay_transf, \n  A = ObsData$RHC.use, \n  W = ObsData.noYA,\n  family = \"gaussian\",\n  V.Q = 10,\n  V.g = 10)\n# Q.SL.library = SL.library.test,  \n## removed this line\n# g.SL.library = SL.library.test)  \n## removed this line\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nTransform the average treatment effect generated by the `tmle` method ($\\widehat{ATE}$) back to the outcome's original scale:\n\n$$\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n$$\n\n\n::: {.cell hash='machinelearningCausal4_cache/html/unnamed-chunk-4_7d48db00031b85b21cd39877b7191dff'}\n\n:::\n\n::: {.cell hash='machinelearningCausal4_cache/html/unnamed-chunk-5_9f17e199c256c902da5af798d0e538da'}\n\n```{.r .cell-code}\n# transform back the ATE estimate\ntmle.est.cont.def <- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$psi\ntmle.est.cont.def\n#> [1] 3.352891\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nTransform the confidence interval back to the original scale:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmle.ci.cont.def <- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$CI\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nATE for continuous outcome using default library: 3.3528911, and 95% CI 1.4320906, 4.856064.\n\nThe estimate using the default SuperLearner library (2.18) is similar to the estimate we got when using our user-specified SuperLearner library (2.95). However, the confidence interval using the default SuperLearner library (1.25, 4.37) was much wider than that using our user-specified SuperLearner library (1.99, 3.91).\n\n### Comparison of results {-}\n\nAdjusted regression:\n\n\n::: {.cell hash='machinelearningCausal4_cache/html/reg1cont_cda424bf1b24303ccab3c806da4fd51d'}\n\n:::\n\n::: {.cell hash='machinelearningCausal4_cache/html/reg2cont_834377b700fecd55c3cc927dd6f41265'}\n\n```{.r .cell-code}\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.LoS <- c(baselinevars, \"Death\")\nout.formula.cont <- as.formula(\n  paste(\"Length.of.Stay~ RHC.use +\", \n        paste(baselineVars.LoS,\n              collapse = \"+\")))\nfit1.cont <- lm(out.formula.cont, data = ObsData)\npublish(fit1.cont, digits=1)$regressionTable[2,]\n```\n:::\n\n::: {.cell hash='machinelearningCausal4_cache/html/unnamed-chunk-8_e70a21b67e533a6aa0b21e3ce3ba7777'}\n\n:::\n\n\n@connors1996effectiveness conducted a propensity score matching analysis. Table 5 showed that, after propensity score pair (1-to-1) matching, means of length of stay ($Y$), when stratified by RHC ($A$) were not significantly different ($p = 0.14$).\n\n\n::: {.cell hash='machinelearningCausal4_cache/html/summarytable0cont_1cdd7281e04123624cb07cc0a9e00a8a'}\n\n:::\n\n::: {.cell hash='machinelearningCausal4_cache/html/summarytablecont_240cabcad01418211dd5f32964e52833'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> method.list </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> 2.5 % </th>\n   <th style=\"text-align:right;\"> 97.5 % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Adjusted Regression </td>\n   <td style=\"text-align:right;\"> 3.04 </td>\n   <td style=\"text-align:right;\"> 1.51 </td>\n   <td style=\"text-align:right;\"> 4.58 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TMLE (user-specified SL library) </td>\n   <td style=\"text-align:right;\"> 5.34 </td>\n   <td style=\"text-align:right;\"> 3.90 </td>\n   <td style=\"text-align:right;\"> 6.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TMLE (default SL library) </td>\n   <td style=\"text-align:right;\"> 3.35 </td>\n   <td style=\"text-align:right;\"> 1.43 </td>\n   <td style=\"text-align:right;\"> 4.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Keele and Small (2021) paper </td>\n   <td style=\"text-align:right;\"> 2.01 </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 3.41 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### References {-}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}