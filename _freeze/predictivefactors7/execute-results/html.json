{
  "hash": "efa43d28ab01cbc74ba0c2d76d02b54f",
  "result": {
    "engine": "knitr",
    "markdown": "## Bootstrap {.unnumbered}\n\nThe tutorial is on bootstrapping methods, mainly using R. Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling, with replacement (after a data point is chosen randomly from the original dataset and included in the sample, it is \"replaced\" back into the original dataset, making it possible for that same data point to be picked again in the same sampling process), from the observed data points. It is a way to quantify the uncertainty associated with a given estimator or statistical measure, such as the mean, median, variance, or correlation coefficient, among others. Bootstrapping is widely applicable and very straightforward to implement, which has made it a popular choice for statistical inference when analytical solutions are not available or are difficult to derive.\n\n::: callout-important\nBootstrapping is a powerful statistical tool for making inferences by empirically estimating the sampling distribution of a statistic. It is especially useful when the underlying distribution is unknown or when an analytical solution is difficult to obtain.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n```\n:::\n\n\n### Load data\n\nLoad the data saved at the end of previous part of the lab.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n```\n:::\n\n\n### Resampling a vector\n\nHere, the document introduces basic resampling of a simple vector. The code creates a new sample using the `sample` function with replacement. It also discusses \"out-of-bag\" samples which are the samples not chosen during the resampling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampled.fake.data <- sample(fake.data, size = length(fake.data), \n                              replace = TRUE)\nresampled.fake.data\n#> [1] 2 1 1 3 3\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 2 1 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 4 5\n```\n:::\n\n\nThe samples not selected are known as the out-of-bag samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), \n                            replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 4 3 2 3 3\n#> [1] 1 2 2 1 3\n#> [1] 1 4 4 1 5\n#> [1] 2 2 4 3 5\n#> [1] 4 3 3 5 5\n#> [1] 3 5 1 4 1\n#> [1] 4 2 2 2 5\n#> [1] 5 4 5 4 3\n#> [1] 4 1 3 4 4\n#> [1] 3 2 4 4 2\n```\n:::\n\n\n### Calculating SD of a statistics\n\nWe introduce the concept of calculating confidence intervals (CIs) using bootstrapping when the distribution of data is not known. It uses resampling to create multiple bootstrap samples, then calculates means and standard deviations (SD) for those samples.\n\nIdea:\n\n-   Not sure about what distribution is appropriate to make inference?\n-   If that is the case, calculating CI is hard.\n-   resample and get a new bootstrap sample\n-   calculate a statistic (say, mean) from that sample\n-   find SD of those statistic (say, means)\n-   Use those SD to calculate CI\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 4 5 5 5 1\n#>  $ : int [1:5] 5 2 5 1 3\n#>  $ : int [1:5] 3 2 4 1 4\n#>  $ : int [1:5] 4 3 5 5 5\n#>  $ : int [1:5] 5 2 1 4 5\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 4.0 3.2 2.8 4.4 3.4\nmean(B.means)\n#> [1] 3.56\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6387488\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 3.053\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6155367\nmean(B.medians)\n#> [1] 3.075\nhist(B.means)\n```\n\n::: {.cell-output-display}\n![](predictivefactors7_files/figure-html/sd2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 1.012175\nhist(B.medians)\n```\n\n::: {.cell-output-display}\n![](predictivefactors7_files/figure-html/sd2-2.png){width=672}\n:::\n:::\n\n\n### Resampling a data or matrix\n\nWe show how to resample a data frame or a matrix, and how to identify which rows have been selected and which haven't, introducing the concept of \"out-of-bag samples\" for matrices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n```\n\n::: {.cell-output-display}\n\n\n|   |    ID|gender | age|\n|:--|-----:|:------|---:|\n|1  | 83732|Male   |  62|\n|2  | 83733|Male   |  53|\n|10 | 83741|Male   |  22|\n|16 | 83747|Male   |  46|\n|19 | 83750|Male   |  45|\n|21 | 83752|Female |  30|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n```\n\n::: {.cell-output-display}\n\n\n|     |    ID|gender | age|\n|:----|-----:|:------|---:|\n|21   | 83752|Female |  30|\n|2    | 83733|Male   |  53|\n|21.1 | 83752|Female |  30|\n|21.2 | 83752|Female |  30|\n|21.3 | 83752|Female |  30|\n|2.1  | 83733|Male   |  53|\n\n\n:::\n\n```{.r .cell-code}\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83752 83733\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732 83741 83747 83750\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n```\n\n::: {.cell-output-display}\n\n\n|     |    ID|gender | age|\n|:----|-----:|:------|---:|\n|2    | 83733|Male   |  53|\n|1    | 83732|Male   |  62|\n|16   | 83747|Male   |  46|\n|1.1  | 83732|Male   |  62|\n|16.1 | 83747|Male   |  46|\n|16.2 | 83747|Male   |  46|\n\n\n:::\n\n```{.r .cell-code}\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83732 83747\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83741 83750 83752\n```\n:::\n\n\n### The caret package / boot\n\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time). We introduce the `trainControl` and `train` functions from the caret package. It sets up a linear model and demonstrates how bootstrapping can be done to estimate the variability in R-squared, a measure of goodness-of-fit for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"RMSE\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rsquared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MAE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Resample\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"35.94822\",\"2\":\"0.2065802\",\"3\":\"27.99081\",\"4\":\"Resample01\",\"_rn_\":\"1\"},{\"1\":\"34.63989\",\"2\":\"0.2176692\",\"3\":\"26.94006\",\"4\":\"Resample02\",\"_rn_\":\"2\"},{\"1\":\"36.09819\",\"2\":\"0.2067883\",\"3\":\"27.92833\",\"4\":\"Resample03\",\"_rn_\":\"3\"},{\"1\":\"35.50718\",\"2\":\"0.2260809\",\"3\":\"27.05782\",\"4\":\"Resample04\",\"_rn_\":\"4\"},{\"1\":\"35.07730\",\"2\":\"0.2161031\",\"3\":\"27.21376\",\"4\":\"Resample05\",\"_rn_\":\"5\"},{\"1\":\"36.87788\",\"2\":\"0.2504643\",\"3\":\"28.70333\",\"4\":\"Resample06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nmean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n```\n:::\n\n\n### Method boot632\n\nA specific bootstrapping method called \"boot632\", which aims to reduce bias but can provide unstable results if the sample size is small. Compared to the original bootstrap method, boot632 addresses the bias that is due to this the sampling with replacement.\n\n::: column-margin\nSee @boot632\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"RMSE\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rsquared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MAE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Resample\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"35.71916\",\"2\":\"0.2091238\",\"3\":\"27.79221\",\"4\":\"Resample01\",\"_rn_\":\"1\"},{\"1\":\"36.03436\",\"2\":\"0.2088865\",\"3\":\"27.87216\",\"4\":\"Resample02\",\"_rn_\":\"2\"},{\"1\":\"35.54895\",\"2\":\"0.2196406\",\"3\":\"27.77477\",\"4\":\"Resample03\",\"_rn_\":\"3\"},{\"1\":\"34.96061\",\"2\":\"0.2374635\",\"3\":\"27.27394\",\"4\":\"Resample04\",\"_rn_\":\"4\"},{\"1\":\"35.90109\",\"2\":\"0.2154780\",\"3\":\"28.11286\",\"4\":\"Resample05\",\"_rn_\":\"5\"},{\"1\":\"35.84707\",\"2\":\"0.2395014\",\"3\":\"27.66055\",\"4\":\"Resample06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n```\n:::\n\n\n### Method boot632 for stepwise\n\nWe discuss the use of stepwise regression models in conjunction with the \"boot632\" method. It highlights the trade-offs and explains that models could be unstable depending on the data.\n\n#### A stable model\n\n::: column-margin\nSee @models\n:::\n\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"RMSE\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rsquared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MAE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Resample\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"35.42297\",\"2\":\"0.2571716\",\"3\":\"27.94171\",\"4\":\"Resample01\",\"_rn_\":\"1\"},{\"1\":\"35.84409\",\"2\":\"0.2347763\",\"3\":\"28.07948\",\"4\":\"Resample02\",\"_rn_\":\"2\"},{\"1\":\"35.90088\",\"2\":\"0.2271463\",\"3\":\"27.84382\",\"4\":\"Resample03\",\"_rn_\":\"3\"},{\"1\":\"34.83679\",\"2\":\"0.2264647\",\"3\":\"27.53455\",\"4\":\"Resample04\",\"_rn_\":\"4\"},{\"1\":\"35.17698\",\"2\":\"0.2093156\",\"3\":\"27.45790\",\"4\":\"Resample05\",\"_rn_\":\"5\"},{\"1\":\"35.08153\",\"2\":\"0.1811553\",\"3\":\"27.56418\",\"4\":\"Resample06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n```\n:::\n\n\n#### An unstable model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"RMSE\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rsquared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MAE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Resample\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"36.11835\",\"2\":\"0.2015042\",\"3\":\"27.87232\",\"4\":\"Resample01\",\"_rn_\":\"1\"},{\"1\":\"36.05382\",\"2\":\"0.2255470\",\"3\":\"28.11558\",\"4\":\"Resample02\",\"_rn_\":\"2\"},{\"1\":\"36.28309\",\"2\":\"0.2064684\",\"3\":\"28.44588\",\"4\":\"Resample03\",\"_rn_\":\"3\"},{\"1\":\"35.64308\",\"2\":\"0.2076057\",\"3\":\"27.79977\",\"4\":\"Resample04\",\"_rn_\":\"4\"},{\"1\":\"37.05636\",\"2\":\"0.2151877\",\"3\":\"28.77376\",\"4\":\"Resample05\",\"_rn_\":\"5\"},{\"1\":\"34.96517\",\"2\":\"0.2257338\",\"3\":\"27.32477\",\"4\":\"Resample06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n```\n:::\n\n\nNote that SD should be higher for larger B.\n\n### Optimism corrected bootstrap\n\nWe discuss a specific type of bootstrap called the \"Optimism corrected bootstrap\". It's a way to adjust performance metrics for the optimism that is often present when a model is tested on the data used to create it.\n\n::: column-margin\nSee @Bootstrap\n:::\n\nSteps:\n\n-   Fit a model M to entire data D and estimate predictive ability R2.\n-   Iterate from b=1 to B:\n    -   Take a resample from the original data, and name it D.star\n    -   Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    -   Use the bootstrap model M.star to get predictive ability on D, R2.fullData\n-   Optimism Opt is calculated as mean(R2.boot - R2.fullData)\n-   Calculate optimism corrected performance as R2-Opt.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n```\n:::\n\n\n### Binary outcome\n\nHere, bootstrapping and cross-validation are used for a logistic regression model. It calculates the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), a measure for the performance of classification models.\n\n-   AUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\n\n-   AUC = 1 (perfect classification)\n\n-   AUC = 0.5 (random classification such as a coin toss)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"ROC\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sens\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Spec\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Resample\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0.6956364\",\"2\":\"0.4593301\",\"3\":\"0.7823344\",\"4\":\"Fold1\"},{\"1\":\"0.7217183\",\"2\":\"0.4114833\",\"3\":\"0.8422713\",\"4\":\"Fold2\"},{\"1\":\"0.7214511\",\"2\":\"0.4809524\",\"3\":\"0.8044164\",\"4\":\"Fold3\"},{\"1\":\"0.7383768\",\"2\":\"0.4354067\",\"3\":\"0.8427673\",\"4\":\"Fold4\"},{\"1\":\"0.7686143\",\"2\":\"0.4688995\",\"3\":\"0.8548896\",\"4\":\"Fold5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nmean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n```\n:::\n\n\n[Brier Score](https://en.wikipedia.org/wiki/Brier_score) is another metric for evaluating the performance of binary classification models. Brier Score is equivalent to the mean squared error, which we calculate for a continuous outcome. A Brier score of 0 indicates perfect accuracy and a score of 1 indicates perfect inaccuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n```\n:::\n\n\n### Video content (optional)\n\n::: callout-tip\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/uqCHghT1oIo\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}