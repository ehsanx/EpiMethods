{
  "hash": "60fda2f93bf9068b99313e824b53b3b1",
  "result": {
    "markdown": "## Bootstrap {.unnumbered}\n\nThe tutorial is on bootstrapping methods, mainly using R. Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling, with replacement, from the observed data points. It is a way to quantify the uncertainty associated with a given estimator or statistical measure, such as the mean, median, variance, or correlation coefficient, among others. Bootstrapping is widely applicable and very straightforward to implement, which has made it a popular choice for statistical inference when analytical solutions are not available or are difficult to derive.\n\n::: callout-important\nBootstrapping is a powerful statistical tool for making inferences by empirically estimating the sampling distribution of a statistic. It is especially useful when the underlying distribution is unknown or when an analytical solution is difficult to obtain.\n:::\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/setup_3cbbb1b29c37be45afab426096a0ee26'}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n```\n:::\n\n\n\n### Load data\n\nLoad the data saved at the end of previous part of the lab.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/load_14db34d5a5cfd04f6d0bf60a22588338'}\n\n```{.r .cell-code}\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n```\n:::\n\n\n\n### Resampling a vector\n\nHere, the document introduces basic resampling of a simple vector. The code creates a new sample using the `sample` function with replacement. It also discusses \"out-of-bag\" samples which are the samples not chosen during the resampling.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/vector_369f7b79d362f91dc745aee7f42f3ec4'}\n\n```{.r .cell-code}\nfake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n```\n:::\n\n::: {.cell hash='predictivefactors7_cache/pdf/sample_dbfc39a40c8a0ad69b62961d7b89e541'}\n\n```{.r .cell-code}\nresampled.fake.data <- sample(fake.data, size = length(fake.data), \n                              replace = TRUE)\nresampled.fake.data\n#> [1] 4 3 2 5 3\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 4 3 2 5\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 1\n```\n:::\n\n\n\nThe samples not selected are known as the out-of-bag samples\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/oob_6c75d5d4fa9ba1b9a8106ce6e7e82165'}\n\n```{.r .cell-code}\nB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), \n                            replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 1 2 4 2 1\n#> [1] 4 4 2 2 1\n#> [1] 1 1 4 3 2\n#> [1] 4 5 3 5 3\n#> [1] 2 2 1 3 4\n#> [1] 1 1 4 2 4\n#> [1] 2 3 4 2 1\n#> [1] 4 2 1 3 2\n#> [1] 1 4 3 1 4\n#> [1] 1 5 5 3 4\n```\n:::\n\n\n\n### Calculating SD of a statistics\n\nWe introduce the concept of calculating confidence intervals (CIs) using bootstrapping when the distribution of data is not known. It uses resampling to create multiple bootstrap samples, then calculates means and standard deviations (SD) for those samples.\n\nIdea:\n\n-   Not sure about what distribution is appropriate to make inference?\n-   If that is the case, calculating CI is hard.\n-   resample and get a new bootstrap sample\n-   calculate a statistic (say, mean) from that sample\n-   find SD of those statistic (say, means)\n-   Use those SD to calculate CI\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/sd_786dd8cc76cdf7994a83f17d7a0cbe28'}\n\n```{.r .cell-code}\nmean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 1 2 4 3 1\n#>  $ : int [1:5] 4 1 5 2 2\n#>  $ : int [1:5] 2 4 3 2 2\n#>  $ : int [1:5] 4 4 4 4 2\n#>  $ : int [1:5] 3 2 5 1 4\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 2.2 2.8 2.6 3.6 3.0\nmean(B.means)\n#> [1] 2.84\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.5176872\n```\n:::\n\n::: {.cell hash='predictivefactors7_cache/pdf/sd2_3594cfac99be2af3747316cd27455a6f'}\n\n```{.r .cell-code}\nmean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 3.034\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6202966\nmean(B.medians)\n#> [1] 3.115\nhist(B.means)\n```\n\n::: {.cell-output-display}\n![](predictivefactors7_files/figure-pdf/sd2-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 0.9780377\nhist(B.medians)\n```\n\n::: {.cell-output-display}\n![](predictivefactors7_files/figure-pdf/sd2-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Resampling a data or matrix\n\nWe show how to resample a data frame or a matrix, and how to identify which rows have been selected and which haven't, introducing the concept of \"out-of-bag samples\" for matrices.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/mat_4cb18e29d56ca646158e90a03a70d326'}\n\n```{.r .cell-code}\nanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n```\n\n::: {.cell-output-display}\n|   |    ID|gender | age|\n|:--|-----:|:------|---:|\n|1  | 83732|Male   |  62|\n|2  | 83733|Male   |  53|\n|10 | 83741|Male   |  22|\n|16 | 83747|Male   |  46|\n|19 | 83750|Male   |  45|\n|21 | 83752|Female |  30|\n:::\n:::\n\n::: {.cell hash='predictivefactors7_cache/pdf/mat2_4397c053708d4ecc5166d5a3abb5a10e'}\n\n```{.r .cell-code}\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n```\n\n::: {.cell-output-display}\n|     |    ID|gender | age|\n|:----|-----:|:------|---:|\n|2    | 83733|Male   |  53|\n|19   | 83750|Male   |  45|\n|19.1 | 83750|Male   |  45|\n|19.2 | 83750|Male   |  45|\n|10   | 83741|Male   |  22|\n|10.1 | 83741|Male   |  22|\n:::\n\n```{.r .cell-code}\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83750 83741\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732 83747 83752\n```\n:::\n\n::: {.cell hash='predictivefactors7_cache/pdf/mat3_72ffb00f5a03aee6ca8254059edb5a3a'}\n\n```{.r .cell-code}\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n```\n\n::: {.cell-output-display}\n|     |    ID|gender | age|\n|:----|-----:|:------|---:|\n|21   | 83752|Female |  30|\n|10   | 83741|Male   |  22|\n|19   | 83750|Male   |  45|\n|16   | 83747|Male   |  46|\n|2    | 83733|Male   |  53|\n|21.1 | 83752|Female |  30|\n:::\n\n```{.r .cell-code}\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83752 83741 83750 83747 83733\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732\n```\n:::\n\n\n\n### The caret package / boot\n\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time). We introduce the `trainControl` and `train` functions from the caret package. It sets up a linear model and demonstrates how bootstrapping can be done to estimate the variability in R-squared, a measure of goodness-of-fit for the model.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/b206_397968b08bd7c122eda04fb247a8bc21'}\n\n```{.r .cell-code}\nset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n#>       RMSE  Rsquared      MAE   Resample\n#> 1 35.94822 0.2065802 27.99081 Resample01\n#> 2 34.63989 0.2176692 26.94006 Resample02\n#> 3 36.09819 0.2067883 27.92833 Resample03\n#> 4 35.50718 0.2260809 27.05782 Resample04\n#> 5 35.07730 0.2161031 27.21376 Resample05\n#> 6 36.87788 0.2504643 28.70333 Resample06\nmean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n```\n:::\n\n\n\n### Method boot632\n\nA specific bootstrapping method called \"boot632\", which aims to reduce bias but can provide unstable results if the sample size is small. Compared to the original bootstrap method, boot632 addressed the bias that is due to this the sampling with replacement. \n\n::: column-margin\nSee @boot632\n:::\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/b2_a7f7928a06018dd71ae02697e3fe3bbc'}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n#>       RMSE  Rsquared      MAE   Resample\n#> 1 35.71916 0.2091238 27.79221 Resample01\n#> 2 36.03436 0.2088865 27.87216 Resample02\n#> 3 35.54895 0.2196406 27.77477 Resample03\n#> 4 34.96061 0.2374635 27.27394 Resample04\n#> 5 35.90109 0.2154780 28.11286 Resample05\n#> 6 35.84707 0.2395014 27.66055 Resample06\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n```\n:::\n\n\n\n### Method boot632 for stepwise\n\nWe discuss the use of stepwise regression models in conjunction with the \"boot632\" method. It highlights the trade-offs and explains that models could be unstable depending on the data.\n\n#### A stable model\n\n::: column-margin\nSee @models\n:::\n\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/b3_044868609eb0d2d1f0b4e82e7ee1f0ef'}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n#>       RMSE  Rsquared      MAE   Resample\n#> 1 35.42297 0.2571716 27.94171 Resample01\n#> 2 35.84409 0.2347763 28.07948 Resample02\n#> 3 35.90088 0.2271463 27.84382 Resample03\n#> 4 34.83679 0.2264647 27.53455 Resample04\n#> 5 35.17698 0.2093156 27.45790 Resample05\n#> 6 35.08153 0.1811553 27.56418 Resample06\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n```\n:::\n\n\n\n#### An unstable model\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/b4_aef4fc2a3b9f504196adc79320d26645'}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n#>       RMSE  Rsquared      MAE   Resample\n#> 1 36.11835 0.2015042 27.87232 Resample01\n#> 2 36.05382 0.2255470 28.11558 Resample02\n#> 3 36.28309 0.2064684 28.44588 Resample03\n#> 4 35.64308 0.2076057 27.79977 Resample04\n#> 5 37.05636 0.2151877 28.77376 Resample05\n#> 6 34.96517 0.2257338 27.32477 Resample06\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n```\n:::\n\n\n\nNote that SD should be higher for larger B.\n\n### Optimism corrected bootstrap\n\nWe discuss a specific type of bootstrap called the \"Optimism corrected bootstrap\". It's a way to adjust performance metrics for the optimism that is often present when a model is tested on the data used to create it.\n\n::: column-margin\nSee @Bootstrap\n:::\n\nSteps:\n\n-   Fit a model M to entire data D and estimate predictive ability R2.\n-   Iterate from b=1 to B:\n    -   Take a resample from the original data, and name it D.star\n    -   Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    -   Use the bootstrap model M.star to get predictive ability on D, R2.fullData\n-   Optimism Opt is calculated as mean(R2.boot - R2.fullData)\n-   Calculate optimism corrected performance as R2-Opt.\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/bo_30e6c1038989273861780da4684acb90'}\n\n```{.r .cell-code}\nR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n```\n:::\n\n\n\n### Binary outcome\n\nHere, bootstrapping and cross-validation are used for a logistic regression model. It calculates the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), a measure for the performance of classification models.\n\n-   AUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\n\n-   AUC = 1 (perfect classification) \n\n-   AUC = 0.5 (random classification such as a coin toss)\n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/bin_1cc74a829987e0afed0c8629e55c6226'}\n\n```{.r .cell-code}\nset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n#>         ROC      Sens      Spec Resample\n#> 1 0.6956364 0.4593301 0.7823344    Fold1\n#> 2 0.7217183 0.4114833 0.8422713    Fold2\n#> 3 0.7214511 0.4809524 0.8044164    Fold3\n#> 4 0.7383768 0.4354067 0.8427673    Fold4\n#> 5 0.7686143 0.4688995 0.8548896    Fold5\nmean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n```\n:::\n\n\n\n[Brier Score](https://en.wikipedia.org/wiki/Brier_score) is another metric for evaluating the performance of binary classification models. Brier Score is equivalent to the mean squared error, which we calculate for a continuous outcome. A Brier score of 0 indicates perfect accuracy and a score of 1 indicates perfect inaccuracy. \n\n\n\n::: {.cell hash='predictivefactors7_cache/pdf/brier_9b0da6f9b56952e728c627ec210acbc7'}\n\n```{.r .cell-code}\nrequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n```\n:::\n\n\n\n### Video content (optional)\n\n::: callout-tip\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/uqCHghT1oIo\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}