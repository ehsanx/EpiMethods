{
  "hash": "bc4292c9823ff3dae01a55a57a1a75f0",
  "result": {
    "markdown": "## Supervised learning {.unnumbered}\n\nIn this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods.\n\nIn the first code chunk, we load necessary R libraries that will be utilized throughout the chapter for various machine learning methods and data visualization.\n\n\n\n\n\n\n\n### Read previously saved data\n\nThe second chunk is dedicated to reading previously saved data and formulas from specified file paths, ensuring that the dataset and predefined formulas are available for subsequent analyses.\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/data_f1b29690ceda607992ddf25ea1c41a49'}\n\n```{.r .cell-code}\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nlevels(ObsData$Death)=c(\"No\",\"Yes\")\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula2 <- readRDS(file = \"Data/machinelearning/form2.RDS\")\n```\n:::\n\n\n\n### Continuous outcome\n\n#### Cross-validation LASSO\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Images/machinelearning/enet.png){width=5.46in}\n:::\n:::\n\n\n\nIn this code chunk, we implement a machine learning model training process with a focus on utilizing cross-validation and tuning parameters to optimize the model. Cross-validation is a technique used to assess how well the model will generalize to an independent dataset by partitioning the original dataset into a training set to train the model, and a test set to evaluate it. Here, we specify that we are using a particular type of cross-validation, denoted as \"cv\", and that we will be creating 5 folds (or partitions) of the data, as indicated by `number = 5`.\n\nThe model being trained is specified to use a method known as \"glmnet\", which is capable of performing lasso, ridge, and elastic net regularization regressions. Tuning parameters are crucial in controlling the behavior of our learning algorithm. In this instance, we specify `lambda` and `alpha` as our tuning parameters, which control the amount of regularization applied to the model and the mixing percentage between lasso and ridge regression, respectively. The `tuneGrid` argument is used to specify the exact values of `alpha` and `lambda` that the model should consider during training. The `verbose = FALSE` argument ensures that additional model training details are not printed during the training process. Finally, the trained model is stored in an object for further examination and use.\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinlasso22_e3b906e520453d9ac9559a627c3d49f1'}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <- train(out.formula1, \n                    trControl = ctrl,\n                    data = ObsData, method = \"glmnet\",\n                    lambda= 0,\n                    tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                    verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.10032  0.05691432  15.18983\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n```\n:::\n\n\n\n#### Cross-validation Ridge\n\nSubsequent code chunks explore Ridge regression and Elastic Net, employing similar methodologies but adjusting tuning parameters accordingly.\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinridge22_42a13a847420f5e0ad201df5733f5962'}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <-train(out.formula1, \n                   trControl = ctrl,\n                   data = ObsData, method = \"glmnet\",\n                   lambda= 0,\n                   tuneGrid = expand.grid(alpha = 0, lambda = 0),\n                   verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4587, 4589, 4588, 4589 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.08404  0.06102837  15.16965\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n```\n:::\n\n\n\n### Binary outcome\n\n#### Cross-validation LASSO\n\nWe then shift to binary outcomes, exploring LASSO and Ridge regression with similar implementations but adjusting for the binary nature of the outcome variable.\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinlasso_91caf4b052101b4feba74374fa449488'}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, \n                  trControl = ctrl,\n                  data = ObsData, \n                  method = \"glmnet\",\n                  lambda= 0,\n                  tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                  verbose = FALSE,\n                  metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4588, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens      Spec     \n#>   0.7556458  0.461499  0.8589496\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n```\n:::\n\n\n\n-   Not okay to select variables from a shrinkage model, and then use them in a regular regression\n\n#### Cross-validation Ridge\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinridge_a797cb87691b20668044608b71cb4e6c'}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               lambda= 0,\n               tuneGrid = expand.grid(alpha = 0,  \n                                      lambda = 0),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4589, 4589, 4587, 4587 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7563217  0.4659531  0.8538417\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n```\n:::\n\n\n\n#### Cross-validation Elastic net\n\n-   Alpha = mixing parameter\n-   Lambda = regularization or tuning parameter\n-   We can use `expand.grid` for model tuning\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinenet_0d874e41e9bac32c9c778e6eff3d59eb'}\n\n```{.r .cell-code}\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  \n                                      lambda = seq(0.05,0.3,by = 0.05)),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4588, 4588, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   alpha  lambda  ROC        Sens          Spec     \n#>   0.10   0.05    0.7512540  0.3641507105  0.8962892\n#>   0.10   0.10    0.7472327  0.2667777737  0.9387429\n#>   0.10   0.15    0.7423392  0.1748700665  0.9658811\n#>   0.10   0.20    0.7367765  0.0919040036  0.9862990\n#>   0.10   0.25    0.7306710  0.0248398207  0.9962384\n#>   0.10   0.30    0.7240627  0.0009950249  0.9997312\n#>   0.15   0.05    0.7501873  0.3442773724  0.9056939\n#>   0.15   0.10    0.7429037  0.2180894535  0.9521765\n#>   0.15   0.15    0.7338555  0.1043232967  0.9830757\n#>   0.15   0.20    0.7234855  0.0188783131  0.9962384\n#>   0.15   0.25    0.7162060  0.0000000000  0.9997312\n#>   0.15   0.30    0.7110072  0.0000000000  1.0000000\n#>   0.20   0.05    0.7483007  0.3268841895  0.9150981\n#>   0.20   0.10    0.7373304  0.1689073244  0.9669557\n#>   0.20   0.15    0.7231992  0.0397454415  0.9924782\n#>   0.20   0.20    0.7139636  0.0004975124  0.9997312\n#>   0.20   0.25    0.7072906  0.0000000000  1.0000000\n#>   0.20   0.30    0.6985762  0.0000000000  1.0000000\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were alpha = 0.1 and lambda = 0.05.\nplot(fit.cv.bin)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/cvbinenet-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n#### Decision tree\n\nDecision trees are then introduced and implemented, with visualizations and evaluation metrics provided to assess their performance.\n\n-   Decision tree\n    -   Referred to as Classification and regression trees or CART\n    -   Covers\n        -   Classification (categorical outcome)\n        -   Regression (continuous outcome)\n    -   Flexible to incorporate non-linear effects automatically\n        -   No need to specify higher order terms / interactions\n    -   Unstable, prone to overfitting, suffers from high variance\n\n##### Simple CART\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cart_f4a05d7aaf4dfbe36a9dd26f171e12e1'}\n\n```{.r .cell-code}\nrequire(rpart)\nsummary(ObsData$DASIndex) # Duke Activity Status Index\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   11.00   16.06   19.75   20.50   23.43   33.00\ncart.fit <- rpart(Death~DASIndex, data = ObsData)\npar(mfrow = c(1,1), xpd = NA)\nplot(cart.fit)\ntext(cart.fit, use.n = TRUE)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/cart-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nprint(cart.fit)\n#> n= 5735 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 5735 2013 Yes (0.3510026 0.6489974)  \n#>   2) DASIndex>=24.92383 1143  514 No (0.5503062 0.4496938)  \n#>     4) DASIndex>=29.14648 561  199 No (0.6452763 0.3547237) *\n#>     5) DASIndex< 29.14648 582  267 Yes (0.4587629 0.5412371) *\n#>   3) DASIndex< 24.92383 4592 1384 Yes (0.3013937 0.6986063) *\nrequire(rattle)\nrequire(rpart.plot)\nrequire(RColorBrewer)\nfancyRpartPlot(cart.fit, caption = NULL)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/cart-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n###### AUC\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/auc1_2e1810fb1fe82667b6f6f0e4c9815117'}\n\n```{.r .cell-code}\nrequire(pROC)\n#> Loading required package: pROC\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5912\nplot(rocobj)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/auc1-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nauc(rocobj)\n#> Area under the curve: 0.5912\n```\n:::\n\n\n\n##### Complex CART\n\nMore variables\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cart2_969366ce780760a6e4522a7bf5543a4a'}\n\n```{.r .cell-code}\nout.formula2\n#> Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#>     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#>     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#>     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#>     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#>     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#>     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nrequire(rpart)\ncart.fit <- rpart(out.formula2, data = ObsData)\n```\n:::\n\n\n\n##### CART Variable importance\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cart3_781b7b78d3070ca3bdb14c4079c1eca1'}\n\n```{.r .cell-code}\ncart.fit$variable.importance\n#>            DASIndex              Cancer               Tumor                 age \n#>         123.2102455          33.4559400          32.5418433          24.0804860 \n#>   Medical.insurance                 WBC                 edu Cardiovascular.Diag \n#>          14.5199953           5.6673997           3.7441554           3.6449371 \n#>          Heart.rate      Cardiovascular         Trauma.Diag               PaCo2 \n#>           3.4059248           3.1669125           0.5953098           0.2420672 \n#>           Potassium              Sodium             Albumin \n#>           0.2420672           0.2420672           0.1984366\n```\n:::\n\n\n\n###### AUC\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/auc2_091a605f3e841e18bd59868e91edc301'}\n\n```{.r .cell-code}\nrequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5981\nplot(rocobj)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/auc2-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nauc(rocobj)\n#> Area under the curve: 0.5981\n```\n:::\n\n\n\n##### Cross-validation CART\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbincart_d812929adec13bec7886acef7b1feb37'}\n\n```{.r .cell-code}\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"rpart\",\n              metric=\"ROC\")\nfit.cv.bin\n#> CART \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   cp           ROC        Sens       Spec     \n#>   0.007203179  0.6304911  0.2816488  0.9086574\n#>   0.039741679  0.5725283  0.2488649  0.8981807\n#>   0.057128664  0.5380544  0.1287804  0.9473284\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final value used for the model was cp = 0.007203179.\n# extract results from each test data \nsummary.res <- fit.cv.bin$resample\nsummary.res\n#>         ROC      Sens      Spec Resample\n#> 1 0.6847220 0.3746898 0.8590604    Fold1\n#> 2 0.6729625 0.2985075 0.8924731    Fold2\n#> 3 0.6076153 0.2754342 0.9287634    Fold5\n#> 4 0.5873154 0.2238806 0.9274194    Fold4\n#> 5 0.5998401 0.2357320 0.9355705    Fold3\n```\n:::\n\n\n\n### Ensemble methods (Type I)\n\nWe explore ensemble methods, specifically bagging and boosting, through implementation and evaluation in the context of binary outcomes.\n\nTraining same model to different samples (of the same data)\n\n#### Cross-validation bagging\n\n-   Bagging or bootstrap aggregation\n    -   independent bootstrap samples (sampling with replacement, B times),\n    -   applies CART on each i (no prunning)\n    -   Average the resulting predictions\n    -   Reduces variance as a result of using bootstrap\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinbag_6541b30c0b560dbc524362b2ceb21f77'}\n\n```{.r .cell-code}\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"bag\",\n               bagControl = bagControl(fit = ldaBag$fit, \n                                       predict = ldaBag$pred, \n                                       aggregate = ldaBag$aggregate),\n               metric=\"ROC\")\n#> Warning: executing %dopar% sequentially: no parallel backend registered\nfit.cv.bin\n#> Bagged Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7506666  0.4485809  0.8602811\n#> \n#> Tuning parameter 'vars' was held constant at a value of 63\n```\n:::\n\n\n\n-   Bagging improves prediction accuracy\n    -   over prediction using a single tree\n-   Looses interpretability\n    -   as this is an average of many diagrams now\n-   But we can get a summary of the importance of each variable\n\n##### Bagging Variable importance\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/baggvar_2b06b2c7ae71a74f114db2a716e8e3a8'}\n\n```{.r .cell-code}\ncaret::varImp(fit.cv.bin, scale = FALSE)\n#> ROC curve variable importance\n#> \n#>   only 20 most important variables shown (out of 50)\n#> \n#>                    Importance\n#> age                    0.6159\n#> APACHE.score           0.6140\n#> DASIndex               0.5962\n#> Cancer                 0.5878\n#> Creatinine             0.5835\n#> Tumor                  0.5807\n#> blood.pressure         0.5697\n#> Glasgow.Coma.Score     0.5656\n#> Disease.category       0.5641\n#> Temperature            0.5584\n#> DNR.status             0.5572\n#> Hematocrit             0.5525\n#> Weight                 0.5424\n#> Bilirubin              0.5397\n#> income                 0.5319\n#> Immunosupperssion      0.5278\n#> RHC.use                0.5263\n#> Dementia               0.5252\n#> Congestive.HF          0.5250\n#> Hematologic.Diag       0.5250\n```\n:::\n\n\n\n#### Cross-validation boosting\n\n-   Boosting\n    -   sequentially updated/weighted bootstrap based on previous learning\n\n\n\n::: {.cell hash='machinelearning5_cache/pdf/cvbinboost_f207dcf3be3affb3890045fcdb29cb1a'}\n\n```{.r .cell-code}\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"gbm\",\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> Stochastic Gradient Boosting \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  ROC        Sens       Spec     \n#>   1                   50      0.7218938  0.2145970  0.9505647\n#>   1                  100      0.7410292  0.2980581  0.9234228\n#>   1                  150      0.7483014  0.3487142  0.9030028\n#>   2                   50      0.7414513  0.2960631  0.9263816\n#>   2                  100      0.7534264  0.3869684  0.8917212\n#>   2                  150      0.7575826  0.4187512  0.8777477\n#>   3                   50      0.7496078  0.3626125  0.9070358\n#>   3                  100      0.7579645  0.4078244  0.8764076\n#>   3                  150      0.7637074  0.4445909  0.8702298\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were n.trees = 150, interaction.depth =\n#>  3, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n:::\n\n::: {.cell hash='machinelearning5_cache/pdf/plotcv_c3b65a4713b51fda069fdf9431d4675f'}\n\n```{.r .cell-code}\nplot(fit.cv.bin)\n```\n\n::: {.cell-output-display}\n![](machinelearning5_files/figure-pdf/plotcv-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Ensemble methods (Type II)\n\nWe introduce the concept of Super Learner, providing external resources for further exploration.\n\nTraining different models on the same data\n\n#### Super Learner\n\n-   Large number of candidate learners (CL) with different strengths\n    -   Parametric (logistic)\n    -   Non-parametric (CART)\n-   Cross-validation: CL applied on training data, prediction made on test data\n-   Final prediction uses a weighted version of all predictions\n    -   Weights = coef of Observed outcome \\~ prediction from each CL\n\n#### Steps\n\nRefer to [this tutorial](https://ehsanx.github.io/TMLEworkshop/g-computation-using-ml.html#g-comp-using-superlearner) for steps and examples!\n\n### Video content (optional)\n\n::: callout-tip\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/lzr8GOq_Ph0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/Q59yffGr8qI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}