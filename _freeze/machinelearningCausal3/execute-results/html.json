{
  "hash": "0a38cde9be16746640de683aeb35619a",
  "result": {
    "markdown": "## Binary Outcomes  {.unnumbered}\n\n\n\n::: {.cell}\n\n:::\n\n\nFor this example we will be looking at the binary outcome variable `death`.\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/tab1bin_ea81a55c2f92be4aa2162b55400aa1cb'}\n\n```{.r .cell-code}\n# Data\nload(file = \"Data/machinelearningCausal/cl2.RData\")\n\n# Table 1\ntab1 <- CreateTableOne(vars = c(\"Death\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#>                    Stratified by RHC.use\n#>                     0           1          \n#>   n                 3551        2184       \n#>   Death (mean (SD)) 0.63 (0.48) 0.68 (0.47)\n```\n:::\n\n\n### TMLE {-}\n\nTMLE works by first constructing an initial outcome and extracting a crude estimate of the treatment effect. Then, TMLE aims to refine the initial estimate in the direction of the true value of the parameter of interest through use of the exposure model.\n\n::: column-margin\n@luque2018targeted discussed the implementation of TMLE, and providing a detailed step-by-step guide, primarily focusing on a binary outcome.\n:::\n\nThe basic steps are:\n\n1.  Construct initial outcome model & get crude estimate\n2.  Construct exposure model and use propensity scores to update the initial outcome model through a targeted adjustment\n3.  Extract treatment effect estimate\n4.  Estimate confidence interval based on a closed-form formula\n\nThe tmle package implements TMLE for both binary and continuous outcomes, and uses the SuperLearner to construct the exposure and outcome models.\n\nThe *tmle* method takes a number of parameters, including:\n\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;font-weight: bold;color: white !important;background-color: #D3D3D3 !important;\"> Term </th>\n   <th style=\"text-align:left;font-weight: bold;color: white !important;background-color: #D3D3D3 !important;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Y </td>\n   <td style=\"text-align:left;\"> Outcome vector </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> Exposure vector </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> W </td>\n   <td style=\"text-align:left;\"> Matrix that includes vectors of all covariates </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> family </td>\n   <td style=\"text-align:left;\"> Distribution </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V </td>\n   <td style=\"text-align:left;\"> Cross-validation folds for exposure and outcome modeling </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q.SL.library </td>\n   <td style=\"text-align:left;\"> Set of machine learning methods to use for SuperLearner for outcome modeling </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> g.SL.library </td>\n   <td style=\"text-align:left;\"> Set of machine learning methods to use for SuperLearner for exposure modeling </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\n### Constructing SuperLearner  {-}\n\nWe will need to specify two SuperLearners, one for the exposure and one for the outcome model. We will need to consider the characteristics of our sample in order to decide the number of cross-validation folds and construct a diverse and computationally feasible library of algorithms.\n\n#### Number of folds  {-}\n\nFirst, we need to define the number of cross-validation folds to use for each model. This depends on our effective sample size [@phillips2023ConsiderationsSL].\n\nOur effective sample size for the outcome model is:\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-3_395a735e7bfe8156581a440a27eb1250'}\n\n```{.r .cell-code}\nn <- nrow(ObsData) \np <- nrow(ObsData[ObsData$Death == 1,])/n \nn_eff <- min(n, 5*(n*min(p, 1-p))) \nn_eff\n#> [1] 5735\n```\n:::\n\n\nOur effective sample size for the exposure model is:\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-4_5278f6d7f3ae20bbdae8cd06076a0baf'}\n\n```{.r .cell-code}\np_exp <- nrow(ObsData[ObsData$RHC.use == 1,])/n \nn_eff_exp <- min(n, 5*(n*min(p, 1-p))) \nn_eff_exp\n#> [1] 5735\n```\n:::\n\n\nFor both models, the effective sample size is the same as our sample size, $n = 5735$.\n\nSince $500 \\leq n_{eff} \\leq 5000$, we should use 10 or more cross-validation folds according to @phillips2023ConsiderationsSL. For the sake of computational feasibility, we will use **10 folds** in this example.\n\n#### Candidate learners  {-}\n\nThe second step is to define the library of learners we will feed in to SuperLearner as potential options for each model (exposure and outcome). In this example, some of our covariates are continuous variables, such as temperature and blood pressure, so we need to include learners that allow non-linear/monotonic relationships.\n\nSince $n$ is large ($>500$), we should include as many learners as is computationally feasible in our libraries.\n\nFurthermore, we have 50 covariates:\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-5_761ba531574b010a0930994338c54836'}\n\n```{.r .cell-code}\nlength(c(baselinevars, \"Length.of.Stay\"))\n#> [1] 50\n```\n:::\n\n\n$5735/20 = 286.75$, and $50<286.75$, so we do not have high-dimensional data and including screeners is optional [@phillips2023ConsiderationsSL].\n\nSince the requirements for the exposure and outcome models are the same in this example, we will use the same SuperLearner library for both. Overall for this example we need to make sure to include:\n\n-   Parametric learners\n\n-   Highly data-adaptive learners\n\n-   Multiple variants of the same learner with different parameter specifications\n\n-   Learners that allow non-linear/monotonic relationships\n\nFor this example, we will include the following learners:\n\n-   Parametric\n\n    -   `SL.mean`: only mean\n\n    -   `SL.glm`: generalized linear model\n\n-   Highly data-adaptive\n\n    -   `SL.glmnet`: penalized regression such as lasso\n\n    -   `SL.xgboost`: extreme gradient boosting\n\n-   Allowing non-linear/monotonic relationships\n\n    -   `SL.randomForest`: random forest\n\n    -   `tmle.SL.dbarts2`: bayesian additive regression tree\n\n    -   `SL.svm`: support vector machine\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-6_213727d0c7fb854e528080a71adcfc6a'}\n\n```{.r .cell-code}\n# Construct the SuperLearner library\nSL.library <- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n```\n:::\n\n\n### TMLE with SuperLearner   {-}\n\nTo run TMLE, we need to install the tmle package and load it on the R environment. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(c('tmle', 'xgboost'))\nrequire(tmle)\nrequire(xgboost)\n```\n:::\n\n\nWe also need to create a data frame containing only the covariates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Death, RHC.use))\nObsData$Death <- as.numeric(ObsData$Death)\n```\n:::\n\n\nThen we can run TMLE using the `tmle` method from the `tmle` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmle.fit <- tmle::tmle(Y = ObsData$Death, \n                   A = ObsData$RHC.use, \n                   W = ObsData.noYA, \n                   family = \"binomial\", \n                   V.Q = 10,\n                   V.g = 10,\n                   Q.SL.library = SL.library, \n                   g.SL.library = SL.library)\n\ntmle.est.bin <- tmle.fit$estimates$OR$psi\ntmle.ci.bin <- tmle.fit$estimates$OR$CI\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-11_ee7cda428bc57d1c924d384f68306aad'}\n\n:::\n\n\nATE for binary outcome using user-specified library: 1.28 and 95% CI is 1.1992981, 1.3761869\n\nThese results show those who received RHC had odds of death that were 1.28 times as high as the odds of death in those who did not receive RHC.\n\n### Understanding defaults  {-}\n\nWe can compare the results using our specified SuperLearner library to the results we would get when using the `tmle` package's default SuperLearner libraries. To do this we simply do not specify libraries for the `Q.SL.library` and `g.SL.library` arguments.\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-12_588b5471761774cb5f863b2931bc525a'}\n\n```{.r .cell-code}\n# small test library \n# with only glm just \n# for sake of making this work\nSL.library.test <- c(\"SL.glm\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmle.fit.def <- tmle::tmle(Y = ObsData$Death, \n                           A = ObsData$RHC.use, \n                           W = ObsData.noYA, \n                           family = \"binomial\", \n                           V.Q = 10,\n                           V.g = 10)\n# Q.SL.library = SL.library.test,  ## removed this line\n# g.SL.library = SL.library.test)  ## removed this line\n\ntmle.est.bin.def <- tmle.fit.def$estimates$OR$psi\ntmle.ci.bin.def <- tmle.fit.def$estimates$OR$CI\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-15_22efa904edf6c40c0bf7d4bbd3133dbc'}\n\n:::\n\n\nATE for binary outcome using default library: 1.32 with 95% CI 1.1466162, 1.5219877.\n\nThese ATE when using the default SuperLearner library (1.31) is very close to the ATE when using our user-specified SuperLearner library (1.29). However, the confidence interval from TMLE using the default SuperLearner library (1.17, 1.46) is slightly wider than the confidence interval from TMLE using our user-specified SuperLearner library (1.20, 1.39).\n\n### Comparison of results  {-}\n\nWe can also compare these results to those from a basic regression and from the literature.\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/reg1bin_8f78b3f1ece4358328c5f1b87901d6da'}\n\n:::\n\n::: {.cell hash='machinelearningCausal3_cache/html/reg2bin_70846281d0c66e5ae664153dbd6ab6de'}\n\n```{.r .cell-code}\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.Death <- c(baselinevars, \"Length.of.Stay\")\nout.formula.bin <- as.formula(\n  paste(\"Death~ RHC.use +\",\n        paste(baselineVars.Death, \n              collapse = \"+\")))\nfit1.bin <- lm(out.formula.bin, data = ObsData)\n```\n:::\n\n::: {.cell hash='machinelearningCausal3_cache/html/unnamed-chunk-16_87f65c7506699fa9131838bc8b5dcb8a'}\n\n:::\n\n\n@connors1996effectiveness conducted a propensity score matching analysis. Table 4 showed that, after propensity score pair (1-to-1) matching, the odds of in-hospital mortality were 39% higher in those who received RHC (OR: 1.39 (1.15, 1.67)).\n\n\n::: {.cell hash='machinelearningCausal3_cache/html/summarytable0bin_911e5e9134ce979894dbcbb4643ab0ae'}\n\n:::\n\n::: {.cell hash='machinelearningCausal3_cache/html/summarytablebin_7ebdac933ca5a680738bae7e8c1dfc98'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> method.list </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> 2.5 % </th>\n   <th style=\"text-align:right;\"> 97.5 % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Adjusted Regression </td>\n   <td style=\"text-align:right;\"> 0.07 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TMLE (user-specified SL library) </td>\n   <td style=\"text-align:right;\"> 1.28 </td>\n   <td style=\"text-align:right;\"> 1.20 </td>\n   <td style=\"text-align:right;\"> 1.38 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TMLE (default SL library) </td>\n   <td style=\"text-align:right;\"> 1.32 </td>\n   <td style=\"text-align:right;\"> 1.15 </td>\n   <td style=\"text-align:right;\"> 1.52 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Connors et al. (1996) paper </td>\n   <td style=\"text-align:right;\"> 1.39 </td>\n   <td style=\"text-align:right;\"> 1.15 </td>\n   <td style=\"text-align:right;\"> 1.67 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### References {-}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}