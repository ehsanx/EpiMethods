{
  "hash": "2b647a0946b2b592598c3a1bfc29032c",
  "result": {
    "markdown": "## Confounding {.unnumbered}\n\nThis tutorial aims to delve into the role of confounding variables in data analysis, especially in the context of big data. We will examine each of these using simulations built on Directed Acyclic Graphs (DAGs). The objective is to understand whether a simple regression adjusting for the confounder variable can correctly estimate treatment effects in such a large sample.\n\n\n\n::: {.cell hash='confounding1_cache/pdf/setup_01e4910048d8cd123b0287d97c3de9ac'}\n\n```{.r .cell-code}\n# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n```\n:::\n\n\n\nBig data: **What if we had 1,000,000 (one million) observations?** Would that give us true result? Let's try to answer that using DAGs.\n\nLet us consider\n\n-   L is continuous variable\n-   A is binary treatment\n-   Y is continuous outcome\n\n### Non-null effect\n\n-   True treatment effect = 1.3\n\n#### Data generating process\n\nTo perform the lab, we'll need the `simcausal` R package. This package may not be available on CRAN but can be installed from the author's GitHub page.\n\n\n\n::: {.cell hash='confounding1_cache/pdf/dgm1_90dd4031efad67f45f3e738ffb4852f3'}\n\n```{.r .cell-code}\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n```\n:::\n\n\n\n#### Generate DAG\n\n\n\n::: {.cell hash='confounding1_cache/pdf/dag1_6adab3e964c01359b2a933a2145035c1'}\n\n```{.r .cell-code}\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n```\n\n::: {.cell-output-display}\n![](confounding1_files/figure-pdf/dag1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n#### Generate Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n#>   ID         L A        Y\n#> 1  1  9.439524 1 6.165097\n#> 2  2  9.769823 1 6.040117\n#> 3  3 11.558708 1 7.121178\n#> 4  4 10.070508 0 4.943313\n#> 5  5 10.129288 1 6.438245\n#> 6  6 11.715065 1 7.073990\n```\n:::\n\n\n\n#### Estimate effect\n\n\n\n::: {.cell hash='confounding1_cache/pdf/est_caa217ce3bc70c3ef9648b449f510af5'}\n\n```{.r .cell-code}\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        1.75\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         0.5\n```\n:::\n\n\n\n::: callout-important\nIn this case, our true treatment effect is 1.3. When we estimate the relationship between A and Y without adjusting for L, we obtain an estimated effect of 1.75. However, this is not the true effect. The true treatment effect of 1.3 is recovered when we adjust for L.\n::: \n\n### Null effect\n\n-   True treatment effect = 0\n\n#### Data generating process\n\n\n\n::: {.cell hash='confounding1_cache/pdf/dgm2_caf3ab350af784e51d9c1c5d818a9d81'}\n\n```{.r .cell-code}\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L, sd = .1)\nDset <- set.DAG(D)\n```\n:::\n\n\n\n#### Generate DAG\n\n\n\n::: {.cell hash='confounding1_cache/pdf/dag2_8690b4c400876e2f2654ec58545038da'}\n\n```{.r .cell-code}\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n```\n\n::: {.cell-output-display}\n![](confounding1_files/figure-pdf/dag2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n#### Generate Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n#>   ID         L A        Y\n#> 1  1  9.439524 1 4.865097\n#> 2  2  9.769823 1 4.740117\n#> 3  3 11.558708 1 5.821178\n#> 4  4 10.070508 0 4.943313\n#> 5  5 10.129288 1 5.138245\n#> 6  6 11.715065 1 5.773990\n```\n:::\n\n\n\n#### Estimate effect\n\n\n\n::: {.cell hash='confounding1_cache/pdf/est2_78036db61bfc8d271afcf4c24145ac17'}\n\n```{.r .cell-code}\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        0.45\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         0.0         0.5\n```\n:::\n\n\n\n\n::: callout-important\nIn this second scenario, the true treatment effect is zero. There is no arrow from A to Y in the DAG, but L remains a common cause for both. Upon analyzing the data without adjusting for L, we observe an induced correlation between A and Y. This correlation disappears, confirming the true null effect, when we adjust for L.\n::: \n\n### Video content (optional)\n\n::: callout-tip\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/EcX7ILtp9aQ\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}