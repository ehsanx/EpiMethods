{
  "hash": "e230770e659f8c3d81dce6a2f71c5e30",
  "result": {
    "markdown": "## SuperLearner {.unnumbered}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### Choosing Learners  {-}\n\nSuperLearner is a *type 2* ensemble method, meaning it combines many methods of different types into one predictive model. SuperLearner uses cross-validation to find the best weighted combination of algorithms based on the predictive performance measure specified (default in the `SuperLearner` package is non-negative least squares based on the Lawson-Hanson algorithm [@nnlsPkgDocs], but measures such as AUC can also be used). To run SuperLearner, the user needs to specify a *library* consisting of all the different methods SuperLearner should incorporate in the final model, as well as the number of cross-validation folds.\n\n::: column-margin\nSee [previous chapter](https://ehsanx.github.io/EpiMethods/machinelearning5.html#ensemble-methods-type-ii) for other types of ensemble learning methods.\n:::\n\nSuperLearner will perform as well as possible *given the library of algorithms considered*. A very recent paper by @phillips2023ConsiderationsSL provides some concrete guidelines for the determination of the number of cross-validation folds necessary and the selection of algorithms to include. Overall, we want to make sure the set of algorithms provided is:\n\n-   **Diverse**: Having a rich library of algorithms allows the SuperLearner to adapt to a range of underlying data structures. Diverse libraries include:\n\n    -   Parametric learners such as generalized linear models (GLMs)\n    -   Highly data-adaptive learners\n    -   Multiple variants of the same learner with different parameter specifications\n\n-   **Computationally feasible**: Lots of machine learning algorithms take a long time to run. Having multiple computationally intensive algorithms in your library will cause the SuperLearner as a whole to take much too long to run.\n\n::: callout-note\nSome of the more specific guidelines depend on our effective sample size. For binary outcomes, this can be calculated as:\n\n$$ n_{eff}=min(n, 5*(n*min(\\bar{p},1-\\bar{p})))   $$\n\nwhere $\\bar{p}$: prevalence of the outcome.\n\nFor continuous outcomes, the effective sample size is the same as the sample size ($n_{eff} = n$).\n:::\n\nWe also want to consider the characteristics of our particular sample.\n\n-   **If there are continuous covariates**: We should include learners that do not force relationships to be linear/monotonic. For example, we could include regression splines, support vector machines, and tree-based methods like regression trees.\n\n-   **If we have high-dimensional data (a large number of covariates** **e.g. more than** $n_{eff}/20$ **)**: We should include some learners that fall under the class of *screeners*. These are learners which incorporate dimension reduction such as LASSO and random forests.\n\n-   **If the sample size is very large** (i.e. $n_{eff}>500$ ): We should include as many learners as is computationally feasible.\n\n-   **If the sample size is small** (i.e. $n_{eff} \\leq 500$ ): We should include fewer learners (e.g. up to $n_{eff}/5$ ), and include less flexible learners.\n\nSome examples of learners that could be included are given in the table below [@SuperLearnerPkgDocs]:\n\n+---------------------------------------------+--------------------------------------------------------------------------------------------+\n| Type of learner                             | Examples                                                                                   |\n+=============================================+============================================================================================+\n| Parametric                                  | -   `SL.mean`: simple mean                                                                 |\n|                                             |                                                                                            |\n|                                             | -   `SL.glm`: generalized linear models                                                    |\n|                                             |                                                                                            |\n|                                             | -   `SL.lm`: ordinary least squares                                                        |\n|                                             |                                                                                            |\n|                                             | -   `SL.speedglm`: fast version of glm                                                     |\n|                                             |                                                                                            |\n|                                             | -   `SL.speedlm`: fast version of lm                                                       |\n|                                             |                                                                                            |\n|                                             | -   `SL.gam`: generalized additive methods                                                 |\n|                                             |                                                                                            |\n|                                             | -   `SL.step`: choose model based on AIC (backwards or forwards or both)                   |\n+---------------------------------------------+--------------------------------------------------------------------------------------------+\n| Highly data-adaptive                        | -   `SL.glmnet`: penalized regression using elastic net (ridge regression and Lasso)       |\n|                                             |                                                                                            |\n|                                             | -   Kernel-based methods                                                                   |\n|                                             |                                                                                            |\n|                                             |     -   `SL.kernelKnn`: k-nearest neighbours                                               |\n|                                             |                                                                                            |\n|                                             |     -   `SL.ksvm`: kernel-based support vector machine                                     |\n|                                             |                                                                                            |\n|                                             | -   `SL.xgboost`: extreme gradient boosting                                                |\n|                                             |                                                                                            |\n|                                             | -   `SL.gbm`: gradient-boosted machines                                                    |\n|                                             |                                                                                            |\n|                                             | -   `SL.nnet`: neural networks                                                             |\n+---------------------------------------------+--------------------------------------------------------------------------------------------+\n| Allowing non-linear/monotonic relationships | -   `SL.earth`: multivariate adaptive regression splines                                   |\n|                                             |                                                                                            |\n|                                             | -   Tree-based methods                                                                     |\n|                                             |                                                                                            |\n|                                             |     -   `SL.randomForest`: random forests                                                  |\n|                                             |                                                                                            |\n|                                             |     -   `tmle.SL.dbarts2`: bayesian additive regression trees                              |\n|                                             |                                                                                            |\n|                                             |     -   `SL.cforest`: random forests using conditional inference trees                     |\n|                                             |                                                                                            |\n|                                             |     -   `SL.ranger`: fast implementation of random forest suited for high dimensional data |\n|                                             |                                                                                            |\n|                                             | -   `SL.svm`: support vector machines                                                      |\n+---------------------------------------------+--------------------------------------------------------------------------------------------+\n| Screeners                                   | -   `screen.corP`: retain covariates with correlation with outcome p-value \\<0.1           |\n|                                             |                                                                                            |\n|                                             | -   `screen.corRank`: retain top *j* covariates with highest correlation with outcome      |\n|                                             |                                                                                            |\n|                                             | -   `screen.glmnet`: Lasso                                                                 |\n|                                             |                                                                                            |\n|                                             | -   `screen.randomForest`: random forests                                                  |\n|                                             |                                                                                            |\n|                                             | -   `screen.SIS`: retain covariates based on distance correlation                          |\n+---------------------------------------------+--------------------------------------------------------------------------------------------+\n\nThere is also a useful tool implemented in the `SuperLearner` library which allows us to easily see a list of all available learners.\n\n\n\n::: {.cell hash='machinelearningCausal2_cache/pdf/unnamed-chunk-2_76bc0bb5f505822b542d2006a5cbb266'}\n\n```{.r .cell-code}\nSuperLearner::listWrappers()\n#> All prediction algorithm wrappers in SuperLearner:\n#>  [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n#>  [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n#>  [7] \"SL.earth\"            \"SL.extraTrees\"       \"SL.gam\"             \n#> [10] \"SL.gbm\"              \"SL.glm\"              \"SL.glm.interaction\" \n#> [13] \"SL.glmnet\"           \"SL.ipredbagg\"        \"SL.kernelKnn\"       \n#> [16] \"SL.knn\"              \"SL.ksvm\"             \"SL.lda\"             \n#> [19] \"SL.leekasso\"         \"SL.lm\"               \"SL.loess\"           \n#> [22] \"SL.logreg\"           \"SL.mean\"             \"SL.nnet\"            \n#> [25] \"SL.nnls\"             \"SL.polymars\"         \"SL.qda\"             \n#> [28] \"SL.randomForest\"     \"SL.ranger\"           \"SL.ridge\"           \n#> [31] \"SL.rpart\"            \"SL.rpartPrune\"       \"SL.speedglm\"        \n#> [34] \"SL.speedlm\"          \"SL.step\"             \"SL.step.forward\"    \n#> [37] \"SL.step.interaction\" \"SL.stepAIC\"          \"SL.svm\"             \n#> [40] \"SL.template\"         \"SL.xgboost\"\n#> \n#> All screening algorithm wrappers in SuperLearner:\n#> [1] \"All\"\n#> [1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n#> [4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n#> [7] \"screen.ttest\"          \"write.screen.template\"\n```\n:::\n\n\n\n### SuperLearner in TMLE  {-}\n\n-   The default SuperLearner library for estimating the outcome includes [@tmlePkgDocs]\n\n    -   `SL.glm`: generalized linear models (GLMs)\n    -   `SL.glmnet`: least absolute shrinkage and selection operator (LASSO)\n    -   `tmle.SL.dbarts2`: modeling and prediction using Bayesian Additive Regression Trees (BART)\n\n-   The default library for estimating the propensity scores includes\n\n    -   `SL.glm`: generalized linear models (GLMs)\n    -   `tmle.SL.dbarts.k.5`: SL wrappers for modeling and prediction using BART\n    -   `SL.gam`: generalized additive models (GAMs)\n\n-   It is certainly possible to use different set of learners\n\n    -   More methods can be added by\n        -   specifying lists of models in the *Q.SL.library* (for the outcome model) and *g.SL.library* (for the propensity score model)\n\n\n\n::: {.cell hash='machinelearningCausal2_cache/pdf/unnamed-chunk-3_c776bbafb0f7900610eb7e2ed1b28f3a'}\n\n:::\n\n\n\n### References  {-}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}