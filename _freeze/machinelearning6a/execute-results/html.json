{
  "hash": "340898d2651c7f3d682d23b142717bae",
  "result": {
    "engine": "knitr",
    "markdown": "## NHIS Example {.unnumbered}\n\nThe tutorial aims to guide the users through fitting machine learning (ML) techniques with health survey data. We will use the [National Health Interview Survey (NHIS) 2016](https://www.cdc.gov/nchs/nhis/nhis_2016_data_release.htm) dataset to develop prediction models for predicting high impact chronic pain (HICP) among adults aged 65 years or older. We will use LASSO and random forest models with sampling weights to obtain population-level predictions. In this tutorial, the split-sample approach as an internal validation technique will be used. You can review the [earlier tutorial](predictivefactors5.html) on data splitting technique. Note that this split-sample approach is flagged as a problematic approach in the literature [@steyerberg2001internal]. The better approach could be cross-validation and bootstrapping [@steyerberg2001internal,@steyerberg2019overfitting]. In the [next tutorial](machinelearning6b.html), we will apply the ML techniques for survey data with cross-validation.\n\n::: column-margin\nSteyerberg EW, Harrell Jr FE, Borsboom GJ, Eijkemans MJ, Vergouwe Y, Habbema JD. Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of Clinical Epidemiology. 2001; 54(8):774-81. DOI: [10.1016/S0895-4356(01)00341-9](https://doi.org/10.1016/S0895-4356(01)00341-9)\n\nSteyerberg EW, Steyerberg EW. Overfitting and optimism in prediction models. Clinical prediction models: A practical approach to development, validation, and updating. 2019:95-112. DOI: [10.1007/978-3-030-16399-0_5](https://link.springer.com/chapter/10.1007/978-3-030-16399-0_5)\n:::\n\n::: callout-note\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the [earlier tutorial](accessing5.html) about the dataset.\n:::\n\n### Load packages\n\nWe load several R packages required for fitting LASSO and random forest models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n```\n:::\n\n\n### Analytic dataset\n\n#### Load\n\nWe load the dataset into the R environment and lists all available variables and objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"Data/machinelearning/nhis2016.RData\")\nls()\n#> [1] \"dat.analytic\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(dat.analytic)\n#> [1] 7828   14\n\nhead(dat.analytic)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"studyid\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"psu\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"strata\"],\"name\":[3],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"weight\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"HICP\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sex\"],\"name\":[6],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"marital\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"race\"],\"name\":[8],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"poverty.status\"],\"name\":[9],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"diabetes\"],\"name\":[10],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"high.cholesterol\"],\"name\":[11],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"stroke\"],\"name\":[12],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"arthritis\"],\"name\":[13],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"current.smoker\"],\"name\":[14],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"20160000020101\",\"2\":\"2\",\"3\":\"149\",\"4\":\"1717\",\"5\":\"0\",\"6\":\"Male\",\"7\":\"Divorced/separated\",\"8\":\"White\",\"9\":\"200-400% FPL\",\"10\":\"No\",\"11\":\"Yes\",\"12\":\"No\",\"13\":\"No\",\"14\":\"No\",\"_rn_\":\"3\"},{\"1\":\"20160000250101\",\"2\":\"2\",\"3\":\"117\",\"4\":\"4088\",\"5\":\"0\",\"6\":\"Male\",\"7\":\"Married/with partner\",\"8\":\"Others\",\"9\":\"400%+ FPL\",\"10\":\"No\",\"11\":\"Yes\",\"12\":\"No\",\"13\":\"No\",\"14\":\"No\",\"_rn_\":\"11\"},{\"1\":\"20160000260102\",\"2\":\"5\",\"3\":\"149\",\"4\":\"2566\",\"5\":\"0\",\"6\":\"Male\",\"7\":\"Married/with partner\",\"8\":\"White\",\"9\":\"200-400% FPL\",\"10\":\"No\",\"11\":\"No\",\"12\":\"No\",\"13\":\"Yes\",\"14\":\"No\",\"_rn_\":\"12\"},{\"1\":\"20160000290101\",\"2\":\"57\",\"3\":\"100\",\"4\":\"1311\",\"5\":\"1\",\"6\":\"Male\",\"7\":\"Divorced/separated\",\"8\":\"White\",\"9\":\"400%+ FPL\",\"10\":\"No\",\"11\":\"No\",\"12\":\"No\",\"13\":\"Yes\",\"14\":\"No\",\"_rn_\":\"13\"},{\"1\":\"20160000350101\",\"2\":\"26\",\"3\":\"136\",\"4\":\"2903\",\"5\":\"0\",\"6\":\"Female\",\"7\":\"Divorced/separated\",\"8\":\"Black\",\"9\":\"200-400% FPL\",\"10\":\"No\",\"11\":\"Yes\",\"12\":\"No\",\"13\":\"Yes\",\"14\":\"No\",\"_rn_\":\"16\"},{\"1\":\"20160000740101\",\"2\":\"36\",\"3\":\"103\",\"4\":\"1278\",\"5\":\"0\",\"6\":\"Female\",\"7\":\"Widowed\",\"8\":\"White\",\"9\":\"200-400% FPL\",\"10\":\"No\",\"11\":\"No\",\"12\":\"No\",\"13\":\"Yes\",\"14\":\"No\",\"_rn_\":\"28\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe dataset contains 7,828 complete case participants (i.e., no missing) with 14 variables:\n\n-   `studyid`: Unique identifier\n-   `psu`: Pseudo-PSU\n-   `strata`: Pseudo-stratum\n-   `weight`: Sampling weight\n-   `HICP`: HICP (high impact chronic pain, the binary outcome variable)\n-   `sex`: Sex\n-   `marital`: Marital status\n-   `race`: Race/ethnicity\n-   `poverty.status`: Poverty status\n-   `diabetes`: Diabetes\n-   `high.cholesterol`: High cholesterol\n-   `stroke`: Stroke\n-   `arthritis`: Arthritis and rheumatism\n-   `current.smoker`: Current smoker\n\nLet's see the descriptive statistics of the predictors stratified by the outcome variable (HICP).\n\n#### Descriptive statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictors\npredictors <- c(\"sex\", \"marital\", \"race\", \"poverty.status\", \n                \"diabetes\", \"high.cholesterol\", \"stroke\",\n                \"arthritis\", \"current.smoker\")\n\n# Table 1 - Unweighted \n#tab1 <- CreateTableOne(vars = predictors, strata = \"HICP\", \n#                       data = dat.analytic, test = F)\n#print(tab1, showAllLevels = T)\n\ntbl_summary(data = dat.analytic, include = predictors, \n            by = HICP, missing = \"no\") %>% \n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**HICP**\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"zoxmuwhdff\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#zoxmuwhdff table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#zoxmuwhdff thead, #zoxmuwhdff tbody, #zoxmuwhdff tfoot, #zoxmuwhdff tr, #zoxmuwhdff td, #zoxmuwhdff th {\n  border-style: none;\n}\n\n#zoxmuwhdff p {\n  margin: 0;\n  padding: 0;\n}\n\n#zoxmuwhdff .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#zoxmuwhdff .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#zoxmuwhdff .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#zoxmuwhdff .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#zoxmuwhdff .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#zoxmuwhdff .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#zoxmuwhdff .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#zoxmuwhdff .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#zoxmuwhdff .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#zoxmuwhdff .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#zoxmuwhdff .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#zoxmuwhdff .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#zoxmuwhdff .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#zoxmuwhdff .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#zoxmuwhdff .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#zoxmuwhdff .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#zoxmuwhdff .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#zoxmuwhdff .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#zoxmuwhdff .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#zoxmuwhdff .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#zoxmuwhdff .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#zoxmuwhdff .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#zoxmuwhdff .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#zoxmuwhdff .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#zoxmuwhdff .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#zoxmuwhdff .gt_left {\n  text-align: left;\n}\n\n#zoxmuwhdff .gt_center {\n  text-align: center;\n}\n\n#zoxmuwhdff .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#zoxmuwhdff .gt_font_normal {\n  font-weight: normal;\n}\n\n#zoxmuwhdff .gt_font_bold {\n  font-weight: bold;\n}\n\n#zoxmuwhdff .gt_font_italic {\n  font-style: italic;\n}\n\n#zoxmuwhdff .gt_super {\n  font-size: 65%;\n}\n\n#zoxmuwhdff .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#zoxmuwhdff .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#zoxmuwhdff .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#zoxmuwhdff .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#zoxmuwhdff .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#zoxmuwhdff .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#zoxmuwhdff .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#zoxmuwhdff .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#zoxmuwhdff div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"label\"><span data-qmd-base64=\"KipDaGFyYWN0ZXJpc3RpYyoq\"><span class='gt_from_md'><strong>Characteristic</strong></span></span></th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"level 1; stat_1\">\n        <div class=\"gt_column_spanner\"><span data-qmd-base64=\"KipISUNQKio=\"><span class='gt_from_md'><strong>HICP</strong></span></span></div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_1\"><span data-qmd-base64=\"KiowKiogIApOID0gNiw4NDc=\"><span class='gt_from_md'><strong>0</strong><br />\nN = 6,847</span></span><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_2\"><span data-qmd-base64=\"KioxKiogIApOID0gOTgx\"><span class='gt_from_md'><strong>1</strong><br />\nN = 981</span></span><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span></th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">sex</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Female</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">3,841 (56%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">623 (64%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Male</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">3,006 (44%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">358 (36%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">marital</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Never married</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">444 (6.5%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">60 (6.1%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Married/with partner</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">3,183 (46%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">379 (39%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Divorced/separated</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,252 (18%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">198 (20%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Widowed</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,968 (29%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">344 (35%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">race</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    White</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">5,455 (80%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">756 (77%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Black</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">606 (8.9%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">99 (10%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Hispanic</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">431 (6.3%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">79 (8.1%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Others</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">355 (5.2%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">47 (4.8%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">poverty.status</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    &lt;100% FPL</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">543 (7.9%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">176 (18%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    100-200% FPL</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,520 (22%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">307 (31%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    200-400% FPL</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">2,309 (34%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">309 (31%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    400%+ FPL</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">2,475 (36%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">189 (19%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">diabetes</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,275 (19%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">315 (32%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">high.cholesterol</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">3,602 (53%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">633 (65%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">stroke</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">527 (7.7%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">146 (15%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">arthritis</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">3,226 (47%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">769 (78%)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">current.smoker</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">619 (9.0%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">123 (13%)</td></tr>\n  </tbody>\n  <tfoot>\n    <tr class=\"gt_footnotes\">\n      <td class=\"gt_footnote\" colspan=\"3\"><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span> <span data-qmd-base64=\"biAoJSk=\"><span class='gt_from_md'>n (%)</span></span></td>\n    </tr>\n  </tfoot>\n</table>\n</div>\n```\n\n:::\n:::\n\n\n### Weight normalization\n\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Note that we are not interested in the statistical significance of the $\\beta$ coefficients. Hence, not utilizing PSU and strata should not be an issue in this prediction problem. However, we still need to use sampling weights to get population-level predictions. Large weights are usually problematic, particularly with model evaluation. One way to solve the problem is weight normalization [@advancedsurvey].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normalize weight\ndat.analytic$wgt <- dat.analytic$weight * \n  nrow(dat.analytic)/sum(dat.analytic$weight)\n\n# Weight summary\nsummary(dat.analytic$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     243    1521    2604    2914    3791   14662\nsummary(dat.analytic$wgt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.08339 0.52198 0.89365 1.00000 1.30109 5.03175\n\n# The weighted and unweighted n are equal\nnrow(dat.analytic)\n#> [1] 7828\nsum(dat.analytic$wgt)\n#> [1] 7828\n```\n:::\n\n\n### Split-sample\n\nLet us create our training and test data using the split-sample approach. We created 70% training and 30% test data for our example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(604001)\ndat.analytic$datasplit <- rbinom(nrow(dat.analytic), \n                                 size = 1, prob = 0.7) \ntable(dat.analytic$datasplit)\n#> \n#>    0    1 \n#> 2343 5485\n\n# Training data\ndat.train <- dat.analytic[dat.analytic$datasplit == 1,]\ndim(dat.train)\n#> [1] 5485   16\n\n# Test data\ndat.test <- dat.analytic[dat.analytic$datasplit == 0,]\ndim(dat.test)\n#> [1] 2343   16\n```\n:::\n\n\n### Regression formula\n\nLet's us define the regression formula:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFormula <- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n```\n:::\n\n\n### LASSO for Surveys\n\nNow, we will fit the LASSO model for our survey data. Here are the steps:\n\n-   We will fit 5-fold cross-validation on the training data to find the value of lambda that gives minimum prediction error. We will incorporate sampling weights in the model to account for survey data.\n-   Fit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey data.\n-   Calculate predictive performance (e.g., AUC) on the test data.\n\n#### Data in matrix\n\nTo perform LASSO with the `glmnet` package, we need to set the predictors in the data.matrix format and outcome variable as a vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training data - X: predictor, y: outcome\nX.train <- model.matrix(Formula, dat.train)[,-1] \ny.train <- as.matrix(dat.train$HICP) \n\n# Test data - X: predictor, y: outcome\nX.test <- model.matrix(Formula, dat.test)[,-1] \ny.test <- as.matrix(dat.test$HICP) \n```\n:::\n\n\nLet us see the few rows of the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(X.train)\n#>    sexMale maritalMarried/with partner maritalDivorced/separated maritalWidowed\n#> 12       1                           1                         0              0\n#> 13       1                           0                         1              0\n#> 16       0                           0                         1              0\n#> 42       0                           0                         0              1\n#> 63       0                           0                         0              1\n#> 65       0                           0                         0              1\n#>    raceBlack raceHispanic raceOthers poverty.status100-200% FPL\n#> 12         0            0          0                          0\n#> 13         0            0          0                          0\n#> 16         1            0          0                          0\n#> 42         0            0          0                          0\n#> 63         0            0          0                          0\n#> 65         0            1          0                          1\n#>    poverty.status200-400% FPL poverty.status400%+ FPL diabetesYes\n#> 12                          1                       0           0\n#> 13                          0                       1           0\n#> 16                          1                       0           0\n#> 42                          0                       1           0\n#> 63                          1                       0           0\n#> 65                          0                       0           1\n#>    high.cholesterolYes strokeYes arthritisYes current.smokerYes\n#> 12                   0         0            1                 0\n#> 13                   0         0            1                 0\n#> 16                   1         0            1                 0\n#> 42                   1         0            0                 0\n#> 63                   0         0            0                 0\n#> 65                   1         0            1                 0\n```\n:::\n\n\nAs we can see, factor predictors are coded into dummy variables. It is important to note that the continuous predictors should be standardized. glmnet does this by default. Next, we will use the `glmnet` function to fit the LASSO model.\n\n::: callout-note\nIn glmnet function, `alpha = 1` for the LASSO, `alpha = 0` for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n:::\n\n#### Find best lambda\n\nNow, we will use k-fold cross-validation with the `cv.glmnet` function to find the best lambda value. In this example, we choose k = 5. Note that we must incorporate sampling weight to account for survey data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the best lambda using 5-fold CV\nfit.cv.lasso <- cv.glmnet(x = X.train, y = y.train, \n                          nfolds = 5, alpha = 1, \n                          family = \"binomial\", \n                          weights = dat.train$wgt)\nfit.cv.lasso\n#> \n#> Call:  cv.glmnet(x = X.train, y = y.train, weights = dat.train$wgt,      nfolds = 5, alpha = 1, family = \"binomial\") \n#> \n#> Measure: Binomial Deviance \n#> \n#>       Lambda Index Measure      SE Nonzero\n#> min 0.001355    43  0.6856 0.01905      12\n#> 1se 0.020128    14  0.7036 0.01446       5\n```\n:::\n\n\nWe can also plot all the lambda values against the deviance (i.e., prediction error).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit.cv.lasso)\n```\n\n::: {.cell-output-display}\n![](machinelearning6a_files/figure-html/lambdaplot-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Best lambda\nfit.cv.lasso$lambda.min\n#> [1] 0.001355482\n```\n:::\n\n\nThe lambda value that has the lowest deviance is 0.0013555. Our next step is to fit the LASSO model with the best lambda. Again, we must incorporate sampling weight to account for survey data.\n\n#### LASSO with best lambda\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model on the training set with optimum lambda\nfit.lasso <- glmnet(x = X.train, y = y.train, \n                    alpha = 1, family = \"binomial\",\n                    lambda = fit.cv.lasso$lambda.min, \n                    weights = dat.train$wgt)\nfit.lasso\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df %Dev   Lambda\n#> 1 12 9.68 0.001355\n```\n:::\n\n\nLet's check the coefficients from the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intercept \nfit.lasso$a0\n#>        s0 \n#> -2.491484\n\n# Beta coefficients\nfit.lasso$beta\n#> 15 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                       s0\n#> sexMale                     -0.009832818\n#> maritalMarried/with partner  .          \n#> maritalDivorced/separated    .          \n#> maritalWidowed               0.041111308\n#> raceBlack                   -0.064853067\n#> raceHispanic                -0.037985160\n#> raceOthers                   .          \n#> poverty.status100-200% FPL  -0.268494032\n#> poverty.status200-400% FPL  -0.602097419\n#> poverty.status400%+ FPL     -1.052635980\n#> diabetesYes                  0.369667918\n#> high.cholesterolYes          0.299475863\n#> strokeYes                    0.449050679\n#> arthritisYes                 1.241288036\n#> current.smokerYes            0.253439388\n```\n:::\n\n\nAs we can see, the coefficient is not shown for some predictors. This is because the LASSO model shrunk the coefficient to zero. In other words, these predictors were dropped entirely from the model because they were not contributing enough to predict the outcome. next, we will use the final model to make predictions on new observations or our test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.lasso <- predict(fit.lasso, \n                               newx = X.test, \n                               type = \"response\")\nhead(dat.test$pred.lasso)\n#>            s0\n#> 3  0.05711170\n#> 11 0.03716633\n#> 28 0.14049527\n#> 30 0.05764351\n#> 31 0.11408034\n#> 59 0.32540267\n```\n:::\n\n\n#### Model performance\n\nNow, we will calculate the model performance measures such as AUC, calibration slope, and Brier score [@steyerberg2010assessing,@steyerberg2014towards,@christodoulou2019systematic]. We will incorporate sampling weights to get population-level estimates.\n\n::: column-margin\nSteyerberg EW, Vickers AJ, Cook NR, Gerds T, Gonen M, Obuchowski N, Pencina MJ, Kattan MW. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.). 2010;21(1):128. DOI: [10.1097/EDE.0b013e3181c30fb2](https://doi.org/10.1097%2FEDE.0b013e3181c30fb2)\n\nSteyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European heart journal. 2014;35(29):1925-31. DOI: [10.1093/eurheartj/ehu207](https://doi.org/10.1093/eurheartj/ehu207)\n\nChristodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology. 2019;110:12-22. DOI: [10.1016/j.jclinepi.2019.02.004](https://doi.org/10.1016/j.jclinepi.2019.02.004)\n:::\n\n::: callout-note\n-   Area under the curve (AUC) is a measure of discrimination or accuracy of a model. A higher AUC is better. An AUC value of 1 is considered a perfect prediction, while an AUC value of 0.50 is no better than a coin toss. In practice, AUC values of 0.70 to 0.80 are considered good, and those $>0.80$ are considered very good.\n\n-   Calibration is defined as the agreement between observed and predicted probability of the outcome. In this exercise, we will estimate the calibration slope as a measure of calibration. A calibration slope of 1 reflects a well-calibrated model, a calibration slope less than 1 indicates overfitting and greater than 1 indicates underfitting of the model.\n\n-   The Brier score is a measure of overall performance. The Brier score can range from 0 to 1 and is similar to the mean squared error. A lower Brier score value (closer to 0) indicates a better model.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AUC on the test set with sampling weights\nauc.lasso <- WeightedAUC(WeightedROC(dat.test$pred.lasso, \n                                     dat.test$HICP, \n                                     weight = dat.test$wgt))\nauc.lasso\n#> [1] 0.7662941\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logit of the predicted probability\ndat.test$pred.lasso.logit <- Logit(dat.test$pred.lasso)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.lasso.logit, data = dat.test, \n               family = binomial, weights = wgt)\ncal.slope.lasso <- summary(mod.cal)$coef[2,1]\ncal.slope.lasso\n#> [1] 1.244645\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Weighted Brier Score\nbrier.lasso <- mean(brierscore(HICP ~ dat.test$pred.lasso, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier.lasso\n#> [1] 0.09978551\n```\n:::\n\n\n### Random Forest for Surveys\n\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model:\n\n-   Fit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey data.\n-   Grid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\n-   Fit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey data.\n-   Calculate predictive performance (e.g., AUC) on the test data.\n\n#### Formula\n\nWe will use the same formula defined above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n```\n:::\n\n\n#### Hyperparameter tuning\n\nFor tuning the hyperparameters, let's use the grid search approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Grid with 1000 models - huge time consuming\n#grid.search <- expand.grid(mtry = 1:10, node.size = 1:10, \n#                           num.trees = seq(50,500,50), \n#                           oob_error = 0)\n  \n# Grid with 36 models as an exercise\ngrid.search <- expand.grid(mtry = 5:7, node.size = 1:3, \n                           num.trees = seq(200,500,100), \n                           oob_error = 0)\nhead(grid.search)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"node.size\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"num.trees\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"oob_error\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"1\"},{\"1\":\"6\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"2\"},{\"1\":\"7\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"4\"},{\"1\":\"6\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"5\"},{\"1\":\"7\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNow, we will fit the random forest model with the selected grids. We will incorporate sampling weight as the case weight in the `ranger` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate prediction error for each grid \nfor(ii in 1:nrow(grid.search)) {\n  # Model on training set with grid\n  fit.rf.tune <- ranger(formula = Formula, \n                        data = dat.train, \n                        num.trees = grid.search$num.trees[ii],\n                        mtry = grid.search$mtry[ii], \n                        min.node.size = grid.search$node.size[ii],\n                        importance = 'impurity', \n                        case.weights = dat.train$wgt)\n  \n  # Add Out-of-bag (OOB) error to each grid\n  grid.search$oob_error[ii] <- sqrt(fit.rf.tune$prediction.error)\n}\nhead(grid.search)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"node.size\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"num.trees\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"oob_error\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0.3343642\",\"_rn_\":\"1\"},{\"1\":\"6\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0.3377220\",\"_rn_\":\"2\"},{\"1\":\"7\",\"2\":\"1\",\"3\":\"200\",\"4\":\"0.3404995\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0.3342145\",\"_rn_\":\"4\"},{\"1\":\"6\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0.3375921\",\"_rn_\":\"5\"},{\"1\":\"7\",\"2\":\"2\",\"3\":\"200\",\"4\":\"0.3410897\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nLet's check which combination of hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) gives minimum prediction error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposition <- which.min(grid.search$oob_error)\ngrid.search[position,]\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"node.size\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"num.trees\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"oob_error\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"3\",\"3\":\"300\",\"4\":\"0.3327845\",\"_rn_\":\"16\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Model after tuning\n\nNow, we will fit the random forest model with the tuned hyperparameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model on the training set \nfit.rf <- ranger(formula = Formula, \n                 data = dat.train, \n                 case.weights = dat.train$wgt, \n                 probability = T,\n                 num.trees = grid.search$num.trees[position], \n                 min.node.size = grid.search$node.size[position], \n                 mtry = grid.search$mtry[position], \n                 importance = 'impurity')\n\n# Fitted random forest model\nfit.rf\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  300 \n#> Sample size:                      5485 \n#> Number of independent variables:  9 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1110941\n```\n:::\n\n\nNow, we can use the model to make predictions on our test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.rf <- predict(fit.rf, \n                            data = dat.test)$predictions[,2]\nhead(dat.test$pred.rf)\n#> [1] 0.031093101 0.003712312 0.129992168 0.044802288 0.025764960 0.307125058\n```\n:::\n\n\n#### Model performance\n\nThe same as the LASSO model, we can calculate the AUC, calibration slope, and Brier score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AUC on the test set with sampling weights\nauc.rf <- WeightedAUC(WeightedROC(dat.test$pred.rf, \n                                  dat.test$HICP, \n                                  weight = dat.test$wgt))\nauc.rf\n#> [1] 0.6941022\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logit of the predicted probability\ndat.test$pred.rf[dat.test$pred.rf == 0] <- 0.00001\ndat.test$pred.rf.logit <- Logit(dat.test$pred.rf)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.rf.logit, \n               data = dat.test, \n               family = binomial, \n               weights = wgt)\ncal.slope.rf <- summary(mod.cal)$coef[2,1]\ncal.slope.rf\n#> [1] 0.4977901\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Weighted Brier Score\nbrier.rf <- mean(brierscore(HICP ~ dat.test$pred.rf, \n                            data = dat.test,\n                            wt = dat.test$wgt))\nbrier.rf\n#> [1] 0.1095384\n```\n:::\n\n\n#### Variable importance\n\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  enframe(fit.rf$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") + \n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](machinelearning6a_files/figure-html/vim-1.png){width=672}\n:::\n:::\n\n\nAs per the figure, marital status, poverty status, sex, and arthritis are the most influential predictors in predicting HICP, while stroke is the least important predictor.\n\n### Performance comparison\n\n| Model         | AUC           | Calibration slope   | Brier score     |\n|---------------|---------------|---------------------|-----------------|\n| LASSO         | 0.7662941 | 1.2446448 | 0.0997855 |\n| Random forest | 0.6941022    | 0.4977901    | 0.1095384    |\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}