{
  "hash": "340898d2651c7f3d682d23b142717bae",
  "result": {
    "markdown": "## NHIS Example {.unnumbered}\n\nThe tutorial aims to guide the users through fitting machine learning (ML) techniques with health survey data. We will use the [National Health Interview Survey (NHIS) 2016](https://www.cdc.gov/nchs/nhis/nhis_2016_data_release.htm) dataset to develop prediction models for predicting high impact chronic pain (HICP) among adults aged 65 years or older. We will use LASSO and random forest models with sampling weights to obtain population-level predictions. In this tutorial, the split-sample approach as an internal validation technique will be used. You can review the [earlier tutorial](predictivefactors5.html) on data splitting technique. Note that this split-sample approach is flagged as a problematic approach in the literature [@steyerberg2001internal]. The better approach could be cross-validation and bootstrapping [@steyerberg2001internal,@steyerberg2019overfitting]. In the [next tutorial](machinelearning6b.html), we will apply the ML techniques for survey data with cross-validation.\n\n::: column-margin\nSteyerberg EW, Harrell Jr FE, Borsboom GJ, Eijkemans MJ, Vergouwe Y, Habbema JD. Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of Clinical Epidemiology. 2001; 54(8):774-81. DOI: [10.1016/S0895-4356(01)00341-9](https://doi.org/10.1016/S0895-4356(01)00341-9)\n\nSteyerberg EW, Steyerberg EW. Overfitting and optimism in prediction models. Clinical prediction models: A practical approach to development, validation, and updating. 2019:95-112. DOI: [10.1007/978-3-030-16399-0_5](https://link.springer.com/chapter/10.1007/978-3-030-16399-0_5)\n:::\n\n::: callout-note\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the [earlier tutorial](accessing5.html) about the dataset.\n:::\n\n### Load packages\n\nWe load several R packages required for fitting LASSO and random forest models.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/setup_867c720158c6ef7c3fea148ca3751a29'}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n```\n:::\n\n\n\n### Analytic dataset\n\n#### Load\n\nWe load the dataset into the R environment and lists all available variables and objects.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/load_af3b8d973edb68110533dcf9faebb9d0'}\n\n```{.r .cell-code}\nload(\"Data/machinelearning/nhis2016.RData\")\nls()\n#> [1] \"dat.analytic\"    \"has_annotations\"\n```\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/dim_ca783938ef4f75e85ee9585050683ef8'}\n\n```{.r .cell-code}\ndim(dat.analytic)\n#> [1] 7828   14\n\nhead(dat.analytic)\n#>           studyid psu strata weight HICP    sex              marital   race\n#> 3  20160000020101   2    149   1717    0   Male   Divorced/separated  White\n#> 11 20160000250101   2    117   4088    0   Male Married/with partner Others\n#> 12 20160000260102   5    149   2566    0   Male Married/with partner  White\n#> 13 20160000290101  57    100   1311    1   Male   Divorced/separated  White\n#> 16 20160000350101  26    136   2903    0 Female   Divorced/separated  Black\n#> 28 20160000740101  36    103   1278    0 Female              Widowed  White\n#>    poverty.status diabetes high.cholesterol stroke arthritis current.smoker\n#> 3    200-400% FPL       No              Yes     No        No             No\n#> 11      400%+ FPL       No              Yes     No        No             No\n#> 12   200-400% FPL       No               No     No       Yes             No\n#> 13      400%+ FPL       No               No     No       Yes             No\n#> 16   200-400% FPL       No              Yes     No       Yes             No\n#> 28   200-400% FPL       No               No     No       Yes             No\n```\n:::\n\n\n\nThe dataset contains 7,828 complete case participants (i.e., no missing) with 14 variables:\n\n-   `studyid`: Unique identifier\n-   `psu`: Pseudo-PSU\n-   `strata`: Pseudo-stratum\n-   `weight`: Sampling weight\n-   `HICP`: HICP (high impact chronic pain, the binary outcome variable)\n-   `sex`: Sex\n-   `marital`: Marital status\n-   `race`: Race/ethnicity\n-   `poverty.status`: Poverty status\n-   `diabetes`: Diabetes\n-   `high.cholesterol`: High cholesterol\n-   `stroke`: Stroke\n-   `arthritis`: Arthritis and rheumatism\n-   `current.smoker`: Current smoker\n\nLet's see the descriptive statistics of the predictors stratified by the outcome variable (HICP).\n\n#### Descriptive statistics\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/tab1_71c6ceb1156b509f3d310a1e10575816'}\n\n```{.r .cell-code}\n# Predictors\npredictors <- c(\"sex\", \"marital\", \"race\", \"poverty.status\", \n                \"diabetes\", \"high.cholesterol\", \"stroke\",\n                \"arthritis\", \"current.smoker\")\n\n# Table 1 - Unweighted \n#tab1 <- CreateTableOne(vars = predictors, strata = \"HICP\", \n#                       data = dat.analytic, test = F)\n#print(tab1, showAllLevels = T)\n\ntbl_summary(data = dat.analytic, include = predictors, \n            by = HICP, missing = \"no\") %>% \n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**HICP**\")\n```\n\n::: {.cell-output-display}\n|**Characteristic**   | **0**, N = 6,847 | **1**, N = 981 |\n|:--------------------|:----------------:|:--------------:|\n|sex                  |                  |                |\n|Female               |   3,841 (56%)    |   623 (64%)    |\n|Male                 |   3,006 (44%)    |   358 (36%)    |\n|marital              |                  |                |\n|Never married        |    444 (6.5%)    |   60 (6.1%)    |\n|Married/with partner |   3,183 (46%)    |   379 (39%)    |\n|Divorced/separated   |   1,252 (18%)    |   198 (20%)    |\n|Widowed              |   1,968 (29%)    |   344 (35%)    |\n|race                 |                  |                |\n|White                |   5,455 (80%)    |   756 (77%)    |\n|Black                |    606 (8.9%)    |    99 (10%)    |\n|Hispanic             |    431 (6.3%)    |   79 (8.1%)    |\n|Others               |    355 (5.2%)    |   47 (4.8%)    |\n|poverty.status       |                  |                |\n|<100% FPL            |    543 (7.9%)    |   176 (18%)    |\n|100-200% FPL         |   1,520 (22%)    |   307 (31%)    |\n|200-400% FPL         |   2,309 (34%)    |   309 (31%)    |\n|400%+ FPL            |   2,475 (36%)    |   189 (19%)    |\n|diabetes             |   1,275 (19%)    |   315 (32%)    |\n|high.cholesterol     |   3,602 (53%)    |   633 (65%)    |\n|stroke               |    527 (7.7%)    |   146 (15%)    |\n|arthritis            |   3,226 (47%)    |   769 (78%)    |\n|current.smoker       |    619 (9.0%)    |   123 (13%)    |\n:::\n:::\n\n\n\n### Weight normalization\n\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Note that we are not interested in the statistical significance of the $\\beta$ coefficients. Hence, not utilizing PSU and strata should not be an issue in this prediction problem. However, we still need to use sampling weights to get population-level predictions. Large weights are usually problematic, particularly with model evaluation. One way to solve the problem is weight normalization [@advancedsurvey].\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/weight_fad071884ffdff8aca70f43028b13c17'}\n\n```{.r .cell-code}\n# Normalize weight\ndat.analytic$wgt <- dat.analytic$weight * \n  nrow(dat.analytic)/sum(dat.analytic$weight)\n\n# Weight summary\nsummary(dat.analytic$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     243    1521    2604    2914    3791   14662\nsummary(dat.analytic$wgt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.08339 0.52198 0.89365 1.00000 1.30109 5.03175\n\n# The weighted and unweighted n are equal\nnrow(dat.analytic)\n#> [1] 7828\nsum(dat.analytic$wgt)\n#> [1] 7828\n```\n:::\n\n\n\n### Split-sample\n\nLet us create our training and test data using the split-sample approach. We created 70% training and 30% test data for our example.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/splitsample_022905ef234a7a94cbb05703d0a0a776'}\n\n```{.r .cell-code}\nset.seed(604001)\ndat.analytic$datasplit <- rbinom(nrow(dat.analytic), \n                                 size = 1, prob = 0.7) \ntable(dat.analytic$datasplit)\n#> \n#>    0    1 \n#> 2343 5485\n\n# Training data\ndat.train <- dat.analytic[dat.analytic$datasplit == 1,]\ndim(dat.train)\n#> [1] 5485   16\n\n# Test data\ndat.test <- dat.analytic[dat.analytic$datasplit == 0,]\ndim(dat.test)\n#> [1] 2343   16\n```\n:::\n\n\n\n### Regression formula\n\nLet's us define the regression formula:\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/formula_4dc23e430932c08f15432096fa0074cc'}\n\n```{.r .cell-code}\nFormula <- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n```\n:::\n\n\n\n### LASSO for Surveys\n\nNow, we will fit the LASSO model for our survey data. Here are the steps:\n\n-   We will fit 5-fold cross-validation on the training data to find the value of lambda that gives minimum prediction error. We will incorporate sampling weights in the model to account for survey data.\n-   Fit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey data.\n-   Calculate predictive performance (e.g., AUC) on the test data.\n\n#### Data in matrix\n\nTo perform LASSO with the `glmnet` package, we need to set the predictors in the data.matrix format and outcome variable as a vector.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassodata_6cee3dc1a9666f6c21d88764a329b47c'}\n\n```{.r .cell-code}\n# Training data - X: predictor, y: outcome\nX.train <- model.matrix(Formula, dat.train)[,-1] \ny.train <- as.matrix(dat.train$HICP) \n\n# Test data - X: predictor, y: outcome\nX.test <- model.matrix(Formula, dat.test)[,-1] \ny.test <- as.matrix(dat.test$HICP) \n```\n:::\n\n\n\nLet us see the few rows of the data:\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassodata2_030f8f3ffe5c7e12b61bc209e79d637c'}\n\n```{.r .cell-code}\nhead(X.train)\n#>    sexMale maritalMarried/with partner maritalDivorced/separated maritalWidowed\n#> 12       1                           1                         0              0\n#> 13       1                           0                         1              0\n#> 16       0                           0                         1              0\n#> 42       0                           0                         0              1\n#> 63       0                           0                         0              1\n#> 65       0                           0                         0              1\n#>    raceBlack raceHispanic raceOthers poverty.status100-200% FPL\n#> 12         0            0          0                          0\n#> 13         0            0          0                          0\n#> 16         1            0          0                          0\n#> 42         0            0          0                          0\n#> 63         0            0          0                          0\n#> 65         0            1          0                          1\n#>    poverty.status200-400% FPL poverty.status400%+ FPL diabetesYes\n#> 12                          1                       0           0\n#> 13                          0                       1           0\n#> 16                          1                       0           0\n#> 42                          0                       1           0\n#> 63                          1                       0           0\n#> 65                          0                       0           1\n#>    high.cholesterolYes strokeYes arthritisYes current.smokerYes\n#> 12                   0         0            1                 0\n#> 13                   0         0            1                 0\n#> 16                   1         0            1                 0\n#> 42                   1         0            0                 0\n#> 63                   0         0            0                 0\n#> 65                   1         0            1                 0\n```\n:::\n\n\n\nAs we can see, factor predictors are coded into dummy variables. It is important to note that the continuous predictors should be standardized. glmnet does this by default. Next, we will use the `glmnet` function to fit the LASSO model.\n\n::: callout-note\nIn glmnet function, `alpha = 1` for the LASSO, `alpha = 0` for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n:::\n\n#### Find best lambda\n\nNow, we will use k-fold cross-validation with the `cv.glmnet` function to find the best lambda value. In this example, we choose k = 5. Note that we must incorporate sampling weight to account for survey data.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lambda_8b2ff1a2d05857af56f8d6722c80da17'}\n\n```{.r .cell-code}\n# Find the best lambda using 5-fold CV\nfit.cv.lasso <- cv.glmnet(x = X.train, y = y.train, \n                          nfolds = 5, alpha = 1, \n                          family = \"binomial\", \n                          weights = dat.train$wgt)\nfit.cv.lasso\n#> \n#> Call:  cv.glmnet(x = X.train, y = y.train, weights = dat.train$wgt,      nfolds = 5, alpha = 1, family = \"binomial\") \n#> \n#> Measure: Binomial Deviance \n#> \n#>       Lambda Index Measure      SE Nonzero\n#> min 0.001355    43  0.6856 0.01905      12\n#> 1se 0.020128    14  0.7036 0.01446       5\n```\n:::\n\n\n\nWe can also plot all the lambda values against the deviance (i.e., prediction error).\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lambdaplot_75924c89d6ded9fa71d93ec157ca02b5'}\n\n```{.r .cell-code}\nplot(fit.cv.lasso)\n```\n\n::: {.cell-output-display}\n![](machinelearning6a_files/figure-pdf/lambdaplot-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/bestlambda_68c7bc1c363c37730b8997edcf54de0d'}\n\n```{.r .cell-code}\n# Best lambda\nfit.cv.lasso$lambda.min\n#> [1] 0.001355482\n```\n:::\n\n\n\nThe lambda value that has the lowest deviance is 0.0013555. Our next step is to fit the LASSO model with the best lambda. Again, we must incorporate sampling weight to account for survey data.\n\n#### LASSO with best lambda\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassofit_b802437b928595ca05218fc0288c0bae'}\n\n```{.r .cell-code}\n# Fit the model on the training set with optimum lambda\nfit.lasso <- glmnet(x = X.train, y = y.train, \n                    alpha = 1, family = \"binomial\",\n                    lambda = fit.cv.lasso$lambda.min, \n                    weights = dat.train$wgt)\nfit.lasso\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df %Dev   Lambda\n#> 1 12 9.68 0.001355\n```\n:::\n\n\n\nLet's check the coefficients from the model:\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassofit2_51c7bd2dbdc33e202efd9048f7097511'}\n\n```{.r .cell-code}\n# Intercept \nfit.lasso$a0\n#>        s0 \n#> -2.491484\n\n# Beta coefficients\nfit.lasso$beta\n#> 15 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                       s0\n#> sexMale                     -0.009832818\n#> maritalMarried/with partner  .          \n#> maritalDivorced/separated    .          \n#> maritalWidowed               0.041111308\n#> raceBlack                   -0.064853067\n#> raceHispanic                -0.037985160\n#> raceOthers                   .          \n#> poverty.status100-200% FPL  -0.268494032\n#> poverty.status200-400% FPL  -0.602097419\n#> poverty.status400%+ FPL     -1.052635980\n#> diabetesYes                  0.369667918\n#> high.cholesterolYes          0.299475863\n#> strokeYes                    0.449050679\n#> arthritisYes                 1.241288036\n#> current.smokerYes            0.253439388\n```\n:::\n\n\n\nAs we can see, the coefficient is not shown for some predictors. This is because the LASSO model shrunk the coefficient to zero. In other words, these predictors were dropped entirely from the model because they were not contributing enough to predict the outcome. next, we will use the final model to make predictions on new observations or our test data.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassopred_09a5bee140f1f6ae133c8a8e8814f7b5'}\n\n```{.r .cell-code}\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.lasso <- predict(fit.lasso, \n                               newx = X.test, \n                               type = \"response\")\nhead(dat.test$pred.lasso)\n#>            s0\n#> 3  0.05711170\n#> 11 0.03716633\n#> 28 0.14049527\n#> 30 0.05764351\n#> 31 0.11408034\n#> 59 0.32540267\n```\n:::\n\n\n\n#### Model performance\n\nNow, we will calculate the model performance measures such as AUC, calibration slope, and Brier score [@steyerberg2010assessing,@steyerberg2014towards,@christodoulou2019systematic]. We will incorporate sampling weights to get population-level estimates.\n\n::: column-margin\nSteyerberg EW, Vickers AJ, Cook NR, Gerds T, Gonen M, Obuchowski N, Pencina MJ, Kattan MW. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.). 2010;21(1):128. DOI: [10.1097/EDE.0b013e3181c30fb2](https://doi.org/10.1097%2FEDE.0b013e3181c30fb2)\n\nSteyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European heart journal. 2014;35(29):1925-31. DOI: [10.1093/eurheartj/ehu207](https://doi.org/10.1093/eurheartj/ehu207)\n\nChristodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology. 2019;110:12-22. DOI: [10.1016/j.jclinepi.2019.02.004](https://doi.org/10.1016/j.jclinepi.2019.02.004)\n:::\n\n::: callout-note\n-   Area under the curve (AUC) is a measure of discrimination or accuracy of a model. A higher AUC is better. An AUC value of 1 is considered a perfect prediction, while an AUC value of 0.50 is no better than a coin toss. In practice, AUC values of 0.70 to 0.80 are considered good, and those $>0.80$ are considered very good.\n\n-   Calibration is defined as the agreement between observed and predicted probability of the outcome. In this exercise, we will estimate the calibration slope as a measure of calibration. A calibration slope of 1 reflects a well-calibrated model, a calibration slope less than 1 indicates overfitting and greater than 1 indicates underfitting of the model.\n\n-   The Brier score is a measure of overall performance. The Brier score can range from 0 to 1 and is similar to the mean squared error. A lower Brier score value (closer to 0) indicates a better model.\n:::\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassoauc_c5e2bc1b8613292ed5262aa3198c45aa'}\n\n```{.r .cell-code}\n# AUC on the test set with sampling weights\nauc.lasso <- WeightedAUC(WeightedROC(dat.test$pred.lasso, \n                                     dat.test$HICP, \n                                     weight = dat.test$wgt))\nauc.lasso\n#> [1] 0.7662941\n```\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassoslope_87306e93ef5b800a72f36e9477d510c9'}\n\n```{.r .cell-code}\n# Logit of the predicted probability\ndat.test$pred.lasso.logit <- Logit(dat.test$pred.lasso)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.lasso.logit, data = dat.test, \n               family = binomial, weights = wgt)\ncal.slope.lasso <- summary(mod.cal)$coef[2,1]\ncal.slope.lasso\n#> [1] 1.244645\n```\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/lassobrier_4a691c86c235c9fc288ba20677881749'}\n\n```{.r .cell-code}\n# Weighted Brier Score\nbrier.lasso <- mean(brierscore(HICP ~ dat.test$pred.lasso, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier.lasso\n#> [1] 0.09978551\n```\n:::\n\n\n\n### Random Forest for Surveys\n\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model:\n\n-   Fit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey data.\n-   Grid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\n-   Fit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey data.\n-   Calculate predictive performance (e.g., AUC) on the test data.\n\n#### Formula\n\nWe will use the same formula defined above.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/formularf_e4b632e246779db67110217e3f5e7067'}\n\n```{.r .cell-code}\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n```\n:::\n\n\n\n#### Hyperparameter tuning\n\nFor tuning the hyperparameters, let's use the grid search approach.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rftuning_f91d478aa62c977905ac2b599f2e375e'}\n\n```{.r .cell-code}\n# Grid with 1000 models - huge time consuming\n#grid.search <- expand.grid(mtry = 1:10, node.size = 1:10, \n#                           num.trees = seq(50,500,50), \n#                           oob_error = 0)\n  \n# Grid with 36 models as an exercise\ngrid.search <- expand.grid(mtry = 5:7, node.size = 1:3, \n                           num.trees = seq(200,500,100), \n                           oob_error = 0)\nhead(grid.search)\n#>   mtry node.size num.trees oob_error\n#> 1    5         1       200         0\n#> 2    6         1       200         0\n#> 3    7         1       200         0\n#> 4    5         2       200         0\n#> 5    6         2       200         0\n#> 6    7         2       200         0\n```\n:::\n\n\n\nNow, we will fit the random forest model with the selected grids. We will incorporate sampling weight as the case weight in the `ranger` function.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rftuning2_37495d66daeb592dff534980c8291828'}\n\n```{.r .cell-code}\n## Calculate prediction error for each grid \nfor(ii in 1:nrow(grid.search)) {\n  # Model on training set with grid\n  fit.rf.tune <- ranger(formula = Formula, \n                        data = dat.train, \n                        num.trees = grid.search$num.trees[ii],\n                        mtry = grid.search$mtry[ii], \n                        min.node.size = grid.search$node.size[ii],\n                        importance = 'impurity', \n                        case.weights = dat.train$wgt)\n  \n  # Add Out-of-bag (OOB) error to each grid\n  grid.search$oob_error[ii] <- sqrt(fit.rf.tune$prediction.error)\n}\nhead(grid.search)\n#>   mtry node.size num.trees oob_error\n#> 1    5         1       200 0.3343642\n#> 2    6         1       200 0.3377220\n#> 3    7         1       200 0.3404995\n#> 4    5         2       200 0.3342145\n#> 5    6         2       200 0.3375921\n#> 6    7         2       200 0.3410897\n```\n:::\n\n\n\nLet's check which combination of hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) gives minimum prediction error.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rftuningposition_d3206485eae9cf8e0e9a7ec17bb6e2a6'}\n\n```{.r .cell-code}\nposition <- which.min(grid.search$oob_error)\ngrid.search[position,]\n#>    mtry node.size num.trees oob_error\n#> 16    5         3       300 0.3327845\n```\n:::\n\n\n\n#### Model after tuning\n\nNow, we will fit the random forest model with the tuned hyperparameters.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rffit_c7ad9b1db97c72005d70028601699afd'}\n\n```{.r .cell-code}\n# Fit the model on the training set \nfit.rf <- ranger(formula = Formula, \n                 data = dat.train, \n                 case.weights = dat.train$wgt, \n                 probability = T,\n                 num.trees = grid.search$num.trees[position], \n                 min.node.size = grid.search$node.size[position], \n                 mtry = grid.search$mtry[position], \n                 importance = 'impurity')\n\n# Fitted random forest model\nfit.rf\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  300 \n#> Sample size:                      5485 \n#> Number of independent variables:  9 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1110941\n```\n:::\n\n\n\nNow, we can use the model to make predictions on our test data.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rfprediction_e68fa787b0a2317bfca0dbbb62fab4b9'}\n\n```{.r .cell-code}\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.rf <- predict(fit.rf, \n                            data = dat.test)$predictions[,2]\nhead(dat.test$pred.rf)\n#> [1] 0.031093101 0.003712312 0.129992168 0.044802288 0.025764960 0.307125058\n```\n:::\n\n\n\n#### Model performance\n\nThe same as the LASSO model, we can calculate the AUC, calibration slope, and Brier score.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/rfauc_4ae582e96af2961b5a6d4fc4055e914a'}\n\n```{.r .cell-code}\n# AUC on the test set with sampling weights\nauc.rf <- WeightedAUC(WeightedROC(dat.test$pred.rf, \n                                  dat.test$HICP, \n                                  weight = dat.test$wgt))\nauc.rf\n#> [1] 0.6941022\n```\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/rfslope_1ffa181257952ba914a49948cc54a72a'}\n\n```{.r .cell-code}\n# Logit of the predicted probability\ndat.test$pred.rf[dat.test$pred.rf == 0] <- 0.00001\ndat.test$pred.rf.logit <- Logit(dat.test$pred.rf)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.rf.logit, \n               data = dat.test, \n               family = binomial, \n               weights = wgt)\ncal.slope.rf <- summary(mod.cal)$coef[2,1]\ncal.slope.rf\n#> [1] 0.4977901\n```\n:::\n\n::: {.cell hash='machinelearning6a_cache/pdf/rfbrier_532e45420a8a7e6b73c0dc16846fce0a'}\n\n```{.r .cell-code}\n# Weighted Brier Score\nbrier.rf <- mean(brierscore(HICP ~ dat.test$pred.rf, \n                            data = dat.test,\n                            wt = dat.test$wgt))\nbrier.rf\n#> [1] 0.1095384\n```\n:::\n\n\n\n#### Variable importance\n\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\n\n\n::: {.cell hash='machinelearning6a_cache/pdf/vim_c62d3052a9b91c5605eb6087a5131caa'}\n\n```{.r .cell-code}\nggplot(\n  enframe(fit.rf$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") + \n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](machinelearning6a_files/figure-pdf/vim-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAs per the figure, marital status, poverty status, sex, and arthritis are the most influential predictors in predicting HICP, while stroke is the least important predictor.\n\n### Performance comparison\n\n| Model         | AUC           | Calibration slope   | Brier score     |\n|---------------|---------------|---------------------|-----------------|\n| LASSO         | 0.7662941 | 1.2446448 | 0.0997855 |\n| Random forest | 0.6941022    | 0.4977901    | 0.1095384    |\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}