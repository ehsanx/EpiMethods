{
  "hash": "f0e1d9a36c15c4875f6aa1c909af020a",
  "result": {
    "engine": "knitr",
    "markdown": "## Concepts (M) {.unnumbered}\n\n## Missing Data Analysis\n\n\n::: {.cell}\n\n:::\n\n\n\nThis section is about understanding, categorizing, and addressing missing data in clinical and epidemiological research. It highlights the prevalence of missing data in these fields, the common use of complete case analysis without considering the implications, and the types of missingness: Missing Completely at Random (MCAR), Missing at Random (MAR), and Not Missing at Random (NMAR), each requiring different approaches and considerations. The consequences of not properly addressing missing data are detailed as bias, incorrect standard errors/precision, and a substantial loss of power.\n\nThis section also delves into strategies for addressing missing data, focusing on ad-hoc approaches and imputation methods. Ad-hoc approaches, such as ignoring missing data or using a missing category indicator, are generally dismissed as statistically invalid. In contrast, imputation, particularly MI, is presented as a more robust and statistically sound method. Multiple imputation involves creating multiple complete datasets by predicting missing values and pooling the results to address the uncertainty associated with missing data. The section further discusses the types of imputation, the necessity of including a sufficient number of predictive variables, and the use of subject-area knowledge in building imputation models, providing a nuanced understanding of the challenges and solutions associated with missing data in research.\n\nReporting Guideline section delves into the complexities of handling missing data in statistical analysis, primarily through MI methods, especially Multiple Imputation by Chained Equations (MICE). It lays out the assumptions necessary for these methods (MCAR, MAR, MNAR). The guide also details how MICE works, using sequential regression imputation to create multiple imputed datasets, thereby allowing for more accurate and robust statistical inferences. Additionally, it provides comprehensive instructions on reporting MICE analysis, including detailing the missingness rates, the reasons for missing data, the assumptions made, and the specifics of the imputation and pooling methods used, ensuring transparency and reproducibility in research.\n\n## Reading list\n\nKey reference: [@sterne2009multiple]\n\nOptional reading: [@vanbuuren2018flexible]\n\nFurther optional readings: [@lumley2011complex; @granger2019avoiding; @hughes2019accounting]\n\n## Video Lessons\n\n::: callout-tip\n## Missing Data Analysis\n\n## The Unseen Threat: Why Missing Data Matters in Research\n\nMissing data is an inevitable feature of nearly all clinical and epidemiological research. From survey non-response to equipment malfunction or participant dropout, gaps in a dataset are the rule, not the exception. For many years, the profound impact of these gaps on the validity of research findings was often overlooked, partly because the statistical methods to properly address them were not readily accessible to most researchers. However, the landscape has changed. Powerful, principled methods for handling missing data, such as multiple imputation, are now available in standard statistical software, raising the standard of evidence and the expectation of rigor for all quantitative research. Understanding and correctly applying these methods is no longer a niche specialty but a core competency for any modern researcher.\n\n### The Critical Consequences of Inaction\n\nIgnoring missing data or handling it improperly is not a neutral act; it actively degrades the quality of scientific inquiry. The consequences are severe and can undermine the very conclusions of a study. There are three primary ways in which missing data can corrupt research findings:\n\n1.  **Bias**: When the missing values are systematically different from the observed ones, any analysis that ignores this fact will produce biased results. The estimates, such as regression coefficients or odds ratios, will be consistently wrong, misrepresenting the true relationships that exist in the population.\n2.  **Loss of Power**: The most common (and often default) method for handling missing data is to simply discard any observation that has a missing value. This approach, known as complete case analysis, reduces the sample size. A smaller sample size diminishes the statistical power of a study, meaning it reduces the ability to detect real effects or relationships, even when they truly exist.\n3.  **Incorrect Precision**: Improperly handling missing data can lead to incorrect standard errors. For instance, some naive methods make the data appear more perfect and less variable than it truly is. This results in standard errors that are too small and confidence intervals that are too narrow, giving a false sense of certainty in the findings.\n\n### Critique of Flawed \"Ad-Hoc\" Approaches\n\nGiven the challenges, researchers have often resorted to simple, \"ad-hoc\" solutions. While appealing in their simplicity, these methods are statistically invalid under most realistic conditions and should be avoided.\n\n-   **Complete Case Analysis (Listwise Deletion)**: This method, the default in many software packages, involves analyzing only the subset of observations with no missing data on any variable. While simple, it is only statistically valid under a very strict and rare assumption about the missing data mechanism Its widespread use without proper justification is one of the most common and serious errors in the literature. As a general rule of thumb, some methodologists suggest that complete case analysis could be considered for the primary analysis if the percentage of missing observations across all variables combined is below approximately 5%, but this requires a very strong justification and should not be based solely on a statistical test. Furthermore, if only the outcome variable has missing values, complete case analysis can be more statistically efficient than multiple imputation.\n-   **Single Imputation (e.g., Mean/Median)**: This approach involves \"filling in\" each missing value with a single number, such as the mean or median of the observed values for that variable. While this creates a complete dataset, it artificially reduces the natural variability of the data. All the imputed values are identical, which shrinks the standard deviation and leads to underestimated standard errors and overly optimistic (i.e., too small) p-values.\n-   **Indicator Method**: Another flawed technique is to create a new \"missing\" category for a variable and include this indicator in a regression model [@greenland1995critical; @vach1991biased]. This is not a valid statistical approach and can introduce significant bias into the model's estimates. This method treats the lack of information as if it were a meaningful, substantive category. For example, if income data is missing for lower-income individuals, creating a \"Missing\" category can mask the true relationship between income and health, potentially causing the model to underestimate the effect of income. The bias can be especially noticeable if the variable with the missing category is an important confounder.\n\nThe persistence of these suboptimal methods points to a critical issue beyond mere statistical technique. The failure to explicitly report the extent of missing data or to justify the method used to handle it is a matter of scientific integrity. Principled missing data analysis is not just about getting a more accurate p-value; it is about a commitment to transparency and producing the most robust and honest results possible from the available evidence.\n\n------------------------------------------------------------------------\n\n## The \"Why\" Behind the \"What\": Assumptions of Missingness\n\nBefore any action can be taken to address missing data, the researcher must make a reasoned judgment about the *mechanism* that caused the data to be missing. This is not a statistical procedure but a theoretical assessment based on subject-matter knowledge. The choice of assumption is the single most important step, as it dictates the entire analytical strategy that follows. There are three core mechanisms of missingness.\n\n### A Detailed Breakdown of the Three Core Mechanisms\n\n-   **Missing Completely at Random (MCAR)**: This is the simplest but most restrictive assumption. Data are said to be MCAR if the probability of a value being missing is completely unrelated to any other variable in the dataset, whether observed or unobserved. The missingness is a pure, random process. A classic, albeit rare, example would be a researcher accidentally dropping a test tube, causing a random data point to be lost. Under MCAR, the complete cases are a random subsample of the original target sample.\n-   **Missing at Random (MAR)**: This is a more relaxed and often more plausible assumption. Data are MAR if the probability of a value being missing *can be fully explained by other observed variables* in the dataset. The \"at random\" part of the name can be misleading; it does not mean the missingness is truly random. It means that *conditional on the data we have observed*, the missingness is random. For example, in a health survey, men might be less likely than women to answer questions about depression. Here, the probability of missingness on the depression variable depends on the 'gender' variable, which is observed. As long as we account for gender in our analysis, we can correct for the potential bias.\n-   **Missing Not at Random (NMAR)**: This is the most challenging scenario. Data are NMAR if the probability of a value being missing is related to the value of that variable *itself*, even after accounting for all other observed variables. In this case, the reason for the missingness is the unobserved value. For example, individuals with very high incomes may be less likely to report their income, or patients who are feeling very ill may be more likely to miss a follow-up appointment. Under NMAR, the missingness is non-ignorable, and standard methods are generally biased.\n\n### Table: Summary of Missing Data Mechanisms\n\nA clear way to distinguish these abstract concepts is with a summary table. It serves as a quick reference and reinforces the key distinctions that guide the choice of analytical method.\n\n| Mechanism | Definition | Implication for Analysis | Plausibility in Practice |\n|:-----------------|:-----------------|:-----------------|:-----------------|\n| **MCAR** | Missingness is a purely random process, unrelated to any data. | Complete Case Analysis is unbiased but may be inefficient (loss of power). | Rare. Often an unrealistic assumption. |\n| **MAR** | Missingness is explainable by other *observed* variables. | Complete Case Analysis is biased. Principled methods like Multiple Imputation are required and valid. | Often considered a plausible working assumption, especially with a rich dataset containing many predictors of missingness. |\n| **NMAR** | Missingness depends on the *unobserved* missing value itself. | Both Complete Case Analysis and standard Multiple Imputation are biased. Requires specialized sensitivity analyses. | Plausible in many scenarios, especially those involving social stigma, extreme values, or health outcomes. |\n\nThe crucial takeaway is that the most powerful and widely used methods for handling missing data, such as Multiple Imputation, operate under the MAR assumption. This leads to a fundamental challenge for the researcher. It is not possible to distinguish between MAR and MNAR using observed data. This creates an apparent paradox: to proceed with the best available methods, one must make an assumption that cannot be statistically proven or disproven with the data at hand.\n\nThe resolution to this paradox lies in shifting the burden of proof from a statistical test to a well-reasoned, subject-matter argument. A researcher cannot simply run a test to \"choose\" MAR. Instead, they must build a compelling case for why MAR is a *plausible* assumption in their specific research context. This involves a deep understanding of the data collection process and the substantive area of study. The strength of the final analysis rests not on a p-value from a test, but on the plausibility of this foundational, untestable assumption.\n\n------------------------------------------------------------------------\n\n## Can We Test the Assumptions? The Role and Limits of MCAR Tests\n\nGiven the importance of the missingness assumption, it is natural to ask if there are formal statistical tests to guide the decision. While there is no test to distinguish between MAR and NMAR, there are tests for the strictest assumption, MCAR. These tests, however, should be seen as limited diagnostic tools, not as definitive oracles.\n\n### Conceptual Goal of MCAR Tests\n\nTests for MCAR, such as **Little's Chi-Squared Test**, are designed to evaluate the null hypothesis that the data are, in fact, Missing Completely at Random. Conceptually, they work by partitioning the data based on the pattern of missingness (e.g., one group missing variable X, another group missing variable Y, a third group with complete data). The test then compares the characteristics of the observed data—typically the means of the variables—across these different groups. If the data are truly MCAR, one would expect the variable means to be similar across all patterns of missingness. A statistically significant test result suggests that the means differ, which provides evidence *against* the MCAR assumption.\n\n### The Critical Limitations\n\nWhile useful, it is essential to understand the significant limitations of MCAR tests to avoid misinterpreting their results.\n\n-   **A One-Way Street**: Hypothesis tests are designed to reject, not accept, a null hypothesis. Therefore, a significant p-value from an MCAR test provides evidence to *reject* the MCAR assumption. However, a non-significant result does *not* prove that the data are MCAR. It simply means there was insufficient evidence in the data to reject the null hypothesis. This could be due to the data truly being MCAR, or it could be due to low statistical power.\n-   **The MAR/NMAR Blind Spot**: This is the most critical limitation. An MCAR test provides no information whatsoever to help distinguish between MAR and NMAR. If the test rejects MCAR, the researcher knows the data are either MAR or NMAR, but the test offers no guidance on which is more likely. This is often the more crucial decision for the subsequent analysis.\n-   **Power Issues**: MCAR tests can have low statistical power, especially in smaller datasets or when the departure from MCAR is subtle. This means the test might fail to detect a true deviation from MCAR, leading to a non-significant result even when the data are actually MAR or NMAR.\n\nThese tests should be viewed as one piece of exploratory evidence in a broader investigation of the missing data mechanism, not as a standalone decision-making algorithm. Their primary utility is to serve as a statistical \"red flag.\" If an MCAR test is significant, it provides strong evidence that a naive approach like complete case analysis is inappropriate and will likely lead to biased results. If the test is not significant, the researcher is not absolved of responsibility. They must still rely on their subject-matter expertise and knowledge of the data collection process to make a reasoned judgment about the plausibility of MAR versus NMAR before proceeding with more advanced methods.\n\n------------------------------------------------------------------------\n\n## The Solution: A Journey Through Imputation Methods\n\nOnce the missing data problem has been diagnosed and an assumption about its mechanism has been made, the next step is to implement a solution. The most principled solutions involve **imputation**, the process of filling in missing data with substituted values to create a complete dataset. This section explores the evolution of imputation techniques, from flawed single imputation methods to the more robust multiple imputation framework.\n\n### Single Imputation: A First Step\n\nSingle imputation methods replace each missing value with one plausible value. While this produces a conveniently complete dataset, it is a fundamentally flawed approach because it fails to account for the uncertainty inherent in the imputation process.\n\n-   **Mean Imputation**: The simplest method, where each missing value is replaced by the mean of the observed values for that variable. This artificially reduces the variance of the variable and distorts its relationships with other variables.\n-   **Regression Imputation**: An improvement that uses the relationships between variables. A regression model is built using the complete cases to predict the missing variable from other variables. The missing values are then filled in with their predicted values. However, this method is still flawed because all the imputed values fall perfectly on the regression line, understating the true variability of the data.\n-   **Stochastic Regression Imputation**: This method addresses the flaw of regression imputation by adding a random error term to each predicted value. This restores the natural variance but can sometimes produce implausible values (e.g., negative height) if the error term is large.\n-   **Hot-Deck Imputation**: In this method, a missing value is filled with an observed response from a \"donor\" individual who is similar on key matching variables. The donor is picked at random from a pool of similar individuals, ensuring the imputed value is a realistic, observed value from the dataset.\n-   **Predictive Mean Matching (PMM)**: A sophisticated and generally well-regarded single imputation method. Like regression imputation, it starts by generating a predicted value for each missing entry. However, instead of using this prediction directly, it identifies a small set of \"donor\" observations from the complete cases whose predicted values are closest to the prediction for the missing entry. It then randomly selects one of these donors and uses their *actual, observed* value as the imputed value. This ensures that all imputed values are plausible and realistic, as they are drawn from the set of observed data.\n\n### When Single Imputation May Be Considered\n\nWhile generally discouraged for final inferential analysis, there are specific scenarios where single imputation may be considered a pragmatic choice : \n\n* **Clinical Trials**: It is often preferred for imputing missing baseline covariates in randomized clinical trials. \n* **Missing Outcome with Auxiliary Variables**: If only the outcome variable is missing and strong auxiliary variables (proxies for the outcome) are available, single imputation may be more effective than complete case analysis. \n* **Prediction Problems**: In machine learning contexts focused on prediction, single imputation methods can be used, though pooling results from multiple imputations is not straightforward [@hossain2025lasso].\n\n### The Unifying Flaw of Single Imputation\n\nDespite their increasing sophistication, all single imputation methods share a critical, unifying flaw: the subsequent statistical analysis treats the imputed values as if they were real, observed data. This failure to acknowledge the *uncertainty* of the imputation process—the fact that we do not know the true missing value and have only made an educated guess—leads to standard errors that are too small, confidence intervals that are too narrow, and p-values that are artificially significant. The analysis becomes overly precise and overly optimistic.\n\n### Multiple Imputation (MI): The Gold Standard\n\n**MI** was developed specifically to solve this uncertainty problem. Instead of creating one \"complete\" dataset, MI creates *multiple* (e.g., $m=20$ or $m=40$) complete datasets. Each dataset is generated using a similar process to stochastic imputation, but because of the random component, the imputed values are slightly different in each of the *m* datasets. This collection of datasets explicitly represents our uncertainty about what the true missing values might have been.\n\nBy analyzing all *m* datasets and then formally combining the results, MI provides a single final estimate that correctly incorporates both the normal sampling uncertainty (from having a finite sample) and the additional uncertainty that arises from the missing data. This makes it the gold standard approach for handling missing data under the MAR assumption.\n\n------------------------------------------------------------------------\n\n## The Multiple Imputation Workflow in Detail\n\nThe MI process can be demystified by breaking it down into three conceptual steps: Impute, Analyze, and Pool. This workflow provides a flexible and powerful framework for obtaining valid statistical inferences in the presence of missing data.\n\n### Step 1: The Imputation Phase - Creating Plausible Realities\n\nThe goal of this phase is to generate *m* complete datasets where the imputed values are plausible draws from their predicted distribution, conditional on all the observed data.\n\n-   **Method (MICE)**: The most common and flexible algorithm for this phase is **Multiple Imputation by Chained Equations (MICE)**, also known as Fully Conditional Specification (FCS). MICE is an iterative process that handles missing data on multiple variables at once. It tackles the problem one variable at a time, cycling through the variables with missing data. For each variable, it fits a regression model to predict it from all other variables in the dataset and then imputes the missing values based on that model's predictions, including a random component. This cycle is repeated several times until the process converges, resulting in one complete dataset. The entire process is then repeated *m* times to generate the *m* imputed datasets.\n-   **Building the Imputation Model**: The success of MI hinges on the quality of the imputation model. This model should be *inclusive* and, in general, more complex than the final scientific model. The goal of the imputation model is not to test a hypothesis but to accurately preserve the complex web of relationships (correlations, means, variances) among all variables in the dataset. A good imputation model should contain:\n    -   All variables from the final analysis model, including the **outcome variable**.\n    -   **Auxiliary variables**: These are variables that are correlated with the variables that have missingness, or are correlated with the missingness itself, even if they are not of scientific interest in the final analysis. Including them helps make the MAR assumption more plausible and can improve the precision of the final estimates.\n    -   Higher-order terms (e.g., squared terms) or interactions if they are thought to be important for capturing the relationships in the data.\n-   **Practical Considerations for the Imputation Model**:\n    -   **Number of Imputations (m)**: A common rule of thumb suggests that the number of imputations, *m*, should be at least as large as the percentage of subjects with any missing data [@austin2021missing]. Modern recommendations often suggest between 20 and 100 imputations.\n    -   **Number of Iterations**: MICE is an iterative algorithm. In each cycle, it updates the imputed values based on the progressively improved predictions from the other variables. The algorithm is run for a set number of iterations to allow the imputed values to stabilize, a state known as convergence.\n    -   **Handling Non-Normal Data**: For continuous variables that are not normally distributed (e.g., skewed), one approach is to transform the variable before imputation and transform it back afterward. However, this can distort relationships and complicate interpretation. A more robust and often preferred strategy within MICE is to use PMM, which is well-suited for non-normal data because it imputes values directly from the observed data, thereby preserving the original distribution.\n\nA common point of confusion is why the outcome variable should be included as a predictor in the imputation model [@white2011multiple]. This seems circular or like \"cheating.\" However, this stems from a misunderstanding of the imputation model's goal. The goal is not merely to predict a missing covariate $X$, but to impute $X$ in a way that *preserves its true relationship with the outcome Y*. The outcome $Y$ is often the single best predictor of $X$. Excluding it from the imputation model would cause the imputed values of $X$ to have a weaker relationship with $Y$ than the observed values of $X$ do, biasing any estimated association between $X$ and $Y$ towards zero. The imputation model's purpose is structural preservation, which enables the subsequent analysis model to accurately test a specific hypothesis.\n\n### Step 2: The Analysis Phase - Analyzing Each Reality\n\nOnce the *m* complete datasets have been generated, the researcher performs their primary scientific analysis independently on *each* of the datasets. For example, if the research question involves fitting a logistic regression model, that exact same model is fitted to dataset 1, dataset 2, and so on, up to dataset *m*. This step is straightforward and results in *m* different sets of parameter estimates (e.g., *m* regression coefficients) and *m* different standard errors.\n\n### Step 3: The Pooling Phase - Synthesizing the Results with Rubin's Rules\n\nThis is the final and crucial step where the results from the *m* separate analyses are combined into a single, valid inference using a set of formulas known as **Rubin's Rules**.\n\n-   **The Pooled Estimate**: The final point estimate for any parameter (e.g., a regression coefficient) is simply the average of the *m* estimates obtained in the analysis phase.\n-   **The Pooled Variance**: This is the key to MI's success. The total variance of the pooled estimate correctly accounts for all sources of uncertainty and is composed of two parts:\n    1.  **Within-Imputation Variance (**$\\bar{U}$): This is the average of the variances from each of the *m* analyses. It represents the normal sampling uncertainty we would have if our data had been complete from the start.\n    2.  **Between-Imputation Variance (**$B$): This is the variance of the parameter estimates *across* the *m* datasets. It directly captures the extra uncertainty that is due to the missing data. If the missing data were not very influential, the estimates from all *m* datasets would be very similar, and $B$ would be small. If the missing data were very influential, the estimates would vary more, and $B$ would be large.\n\nThe formula for the total variance ($T$) is $T = \\bar{U} + B(1 + 1/m)$. This elegant formula shows how MI correctly inflates the standard error to account for the uncertainty from missing data ($B$), solving the primary problem of single imputation and yielding valid confidence intervals and p-values. The \"fraction of missing information\" (FMI) is a useful metric derived from this process, which quantifies the proportion of the total variance that is attributable to the missing data.\n\n### Step 4: Convergence and Diagnostics\n\nAfter running the imputation, it is essential to perform diagnostic checks. A key diagnostic is the **convergence plot**, which traces the mean and standard deviation of the imputed values for each variable across the iterations for each imputed dataset. For healthy convergence, these trace lines should appear as stationary, horizontal bands of random noise, without any clear upward or downward trends. This indicates that the algorithm has stabilized and the imputed values are reliable.\n\n------------------------------------------------------------------------\n\n## Handling Missing Outcomes with MID\n\nA common point of hesitation for researchers new to imputation is what to do when the dependent variable (outcome) itself is missing. There is often a fear that imputing the outcome might artificially create the very results the study aims to find. While this concern is understandable, simply deleting subjects with missing outcomes (complete case analysis) is often biased under the MAR assumption. A strategy known as **'Multiple Imputation, then Deletion' (MID)** offers a principled solution.\n\n### The Dilemma of Imputing the Outcome\n\nIf a predictor variable $X$ is missing for a subject, the value of their outcome $Y$ can be very informative for imputing $X$. Ignoring subjects with a missing outcome during the imputation phase means throwing away valuable information that could have improved the imputation of other variables. However, some argue that using the imputed outcomes in the final analysis model may add unnecessary noise, especially if the imputation model for the outcome is not perfectly specified.\n\n### The 'Multiple Imputation, then Deletion' (MID) Strategy\n\nThe MID approach cleverly navigates this dilemma with a three-step conceptual process. It is particularly popular when there is a high percentage of missing values in the outcome (e.g., 20%-50%).\n\n-   **Step A (Impute)**: Perform a standard multiple imputation on the entire dataset. Crucially, the outcome variable ($Y$) is included in the imputation model and is itself imputed. This ensures that all available information, including from subjects with missing outcomes, is used to create the best possible imputations for the predictor variables ($X$s).\n-   **Step B (Delete)**: After the imputation phase is complete and the *m* datasets have been generated, *delete the observations for which the outcome variable was originally missing*. This means the imputed values of $Y$ are discarded and will not be used in the final analysis model.\n-   **Step C (Analyze & Pool)**: Proceed with the standard analysis and pooling steps using only the observations that had an observed outcome from the beginning. The analysis is performed on the *m* datasets, each of which now contains only subjects with observed outcomes but has fully imputed predictors.\n\n### Rationale, Extensions, and Sensitivity Analysis\n\nThe core idea behind MID is to separate the task of imputing predictors from the task of estimating the relationship of interest. It uses the information from the full dataset (including subjects with missing $Y$) to get the best possible imputations for the *predictors*, and then uses only the reliable, observed data to get the best possible estimate of the *relationship* between those predictors and the outcome. This strategy operates under the assumption that the imputed outcomes themselves do not add useful information to the regression analysis of interest and may only add statistical noise. The same MID logic can be applied if a key exposure variable is missing. When in doubt, MID can also be used as a sensitivity analysis: a researcher can compare the results from a full MI analysis with the results from an MID analysis to gauge the impact of the imputed outcomes on the final conclusions.\n\n------------------------------------------------------------------------\n\n## Effect Modification Analysis with MI\n\nThe flexible `Impute -> Analyze -> Pool` framework of MI is not limited to simple main effects models. It can be readily extended to investigate more complex scientific questions.\n\nEffect modification occurs when the effect of an exposure on an outcome differs across levels of a third variable, the effect modifier. For example, a new drug's effect on blood pressure might be stronger in women than in men. Here, gender is an effect modifier. Statistically, this is often tested by including an interaction term in a regression model (e.g., $Y \\sim \\text{Drug} + \\text{Gender} + \\text{Drug} \\times \\text{Gender}$).\n\nTo test for effect modification in the presence of missing data, the MI workflow is adapted as follows:\n\n-   **Step 1 (Impute)**: Perform multiple imputation as usual. It is critical that the exposure, the outcome, and the potential effect modifier are all included in the imputation model. To best preserve the potential interaction, it is also highly recommended to include the interaction term itself in the imputation model.\n-   **Step 2 (Analyze)**: In the analysis phase, fit the regression model that includes the interaction term (e.g., $Y \\sim X + Z + X \\times Z$) to *each* of the *m* imputed datasets.\n-   **Step 3 (Pool)**: Pool the results from the *m* models using Rubin's Rules. This will yield a single pooled estimate, standard error, and p-value for the main effects of $X$ and $Z$, and, most importantly, for the interaction term $X \\times Z$. A statistically significant pooled interaction term provides evidence for effect modification.\n\nWhile pooling the interaction term is statistically valid, interpreting the coefficient for an interaction term can be non-intuitive. A more practical and often more interpretable approach involves performing a stratified analysis in Step 2. Instead of fitting one interaction model, one can fit separate, simpler models for each level of the effect modifier. This process yields stratum-specific effect estimates (e.g., the final pooled Odds Ratio for treatment in males and the final pooled Odds Ratio for treatment in females). These can then be directly compared to assess effect modification in a way that is often easier to communicate and understand than an interaction coefficient.\n\n## Variable Selection with MI\n\nA common challenge is how to perform variable selection (e.g., stepwise regression) when using MI. Because the analysis is run on *m* different datasets, the variable selection procedure might choose a different set of \"best\" predictors for each one, making it difficult to pool the results into a single final model. Several strategies have been proposed to handle this :\n\n-   **Majority Rule**: Perform variable selection on each of the *m* imputed datasets. The final model includes only those variables that are selected in a majority (more than half) of the analyses.\n-   **Stacked Regression**: Stack all *m* imputed datasets into one large dataset. Then, perform a single variable selection procedure on this large, stacked dataset.\n-   **Wald Test Approach**: This method involves fitting nested models and using a pooled Wald test (or a similar test statistic) to compare them. This is generally considered a highly principled approach for variable selection with multiply imputed data.\n\n------------------------------------------------------------------------\n\n## The Challenge of NMAR and Sensitivity Analysis\n\nThe most difficult missing data mechanism to handle is Missing Not at Random (NMAR), where the probability of missingness depends on the unobserved value itself.\n\n### Why NMAR Produces Bias\n\nStandard methods like complete case analysis and MAR-based multiple imputation assume that the missingness can be explained by observed data. This assumption is violated under NMAR. For example, if patients who are sicker are more likely to drop out of a study, their missing health data is directly related to their unobserved, worsening health status. Because the reason for missingness cannot be directly observed or modeled from the available data, standard methods will produce biased estimates.\n\n### Sensitivity Analysis for NMAR\n\nSince the NMAR assumption cannot be formally tested against MAR, the recommended approach is to conduct a **sensitivity analysis**. This involves intentionally imputing the missing values under different plausible NMAR scenarios to see how sensitive the study's conclusions are to these changes. For example, one might impute missing health data under a \"best-case\" scenario (assuming dropouts were healthier than observed) and a \"worst-case\" scenario (assuming they were sicker). If the study's main conclusions remain unchanged across these different scenarios, it provides greater confidence in the robustness of the findings. One common technique for this is **delta-adjustment**, where the imputed values are systematically shifted to reflect a hypothesized difference between the missing and observed groups.\n\n------------------------------------------------------------------------\n:::\n\n### Imputation vs. Pooling: How Non-Normality Affects Inference\n\nWhen you use MI to handle missing data, you combine the results from several imputed datasets using a set of formulas called Rubin's rules [@rubin1988overview]. A key assumption behind these rules is that the estimates you're combining, e.g., ORs or HRs, follow a roughly bell-shaped (normal) distribution. If this assumption is violated, the validity of your final confidence intervals and p-values can be compromised. The problem is worse in small samples or with high rates of missingness because the Central Limit Theorem is less effective.\n\nThis issue, known as, non-normality, can arise from two distinct sources: problems with the (1) imputation process itself, or the (2) inherent nature of the statistic (HR, OR) you are estimating.\n\n**(1) Imputation-Induced Skewness: Garbage In, Garbage Out**: This happens when your imputation model is a poor fit for the variable with missing data. The model then generates imputed values that are implausible, which distorts the variable's distribution and, in turn, skews the results of your analysis model.\n\nFor example, consider the scenario when you are missing data on patient income, a variable that is almost always right-skewed. If you use a standard imputation method that assumes normality (e.g., simple linear regression), it might generate negative or unrealistically high income values. When you then run a regression model using these flawed imputed datasets, the resulting odds ratios can become skewed or spread out unnaturally. This increases the between-imputation variance (the measure of uncertainty from the missing data), often leading to overly wide confidence intervals and a loss of statistical power. The fix here is to improve the imputation model. Use methods that respect the natural distribution of your data, such as PMM (`pmm`), or apply a transformation (e.g., a log transform) to the variable before imputing it.\n\n**(2) Parameter-Inherent Skewness: It's Not the Imputation, It's the Ratio**: Sometimes, the imputation model is perfect, but the statistic you are trying to estimate is naturally skewed. Ratios, such as HRs or ORs, and bounded measures such as correlation coefficients, are prime examples of parameters with inherently non-normal sampling distributions. An HR, for instance, cannot be less than zero but can be very large, leading to a right-skewed distribution.\n\nWhen you directly pool HRs or ORs from your imputed datasets, their inherent skew can make the standard error calculation from Rubin's rules less accurate. The t-distribution used to construct the confidence interval doesn't quite fit, which can lead to confidence intervals that are too narrow (inflating your Type I error rate) or too wide.\n\nThe standard and recommended approach is to transform the estimate before pooling [@van2018flexible]:\n\n1. For each imputed dataset, calculate the log(HR) or log(OR) and its standard error.  \n2. Use Rubin's rules to pool the log-transformed estimates and their standard errors.  \n3. Back-transform the final pooled estimate and its confidence interval limits (by exponentiating them) to get your result back on the original HR or OR scale.\n\n**Practical Steps and Diagnostics**:\n\nA simple diagnostic is to plot a histogram or density plot of your estimates (e.g., the 20+ odds ratios from your imputed datasets).\n\n- If the plot shows a few extreme outliers and is widely spread, it suggests an imputation problem. You should revisit your imputation model.  \n- If the plot is consistently skewed in one direction across all imputations, it likely points to parameter-inherent skewness. You should use the transform–pool–backtransform approach.\n\nBy correctly diagnosing the source of non-normality, you can ensure your final results are both valid and reliable, preserving the statistical power that multiple imputation is designed to provide. Increasing the number of imputations (`m`) does not solve either of these fundamental non-normality problems. While increasing `m` does not fix non-normality, it can stabilize the Monte Carlo error of pooled estimates: so increasing `m` helps precision but not distributional validity. Better methods, not more imputations, are the key.\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/Wnw5sssW8LI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n::: callout-tip\n## Reporting guidelines when missing data is present\n\n### Best Practices for Transparent Reporting\n\nThis guide has journeyed from the fundamental problems caused by missing data—bias, power loss, and incorrect precision—to the principled, modern solution of Multiple Imputation. The core takeaways are that handling missing data requires careful thought, that the choice of an underlying assumption like MAR is a reasoned argument based on subject-matter knowledge rather than a statistical fact, and that the ultimate goal of imputation is not just to fill in blanks, but to do so in a way that preserves the original data structure and correctly represents our uncertainty about the missing values.\n\nTo ensure that research is both reproducible and credible, transparent reporting is paramount. Based on common problems identified in the scientific literature, any analysis using MI should be accompanied by a clear and detailed description of the process.\n\n### A Blueprint for Reporting Multiple Imputation\n\nA robust report or publication should include the following key elements:\n\n-   **Extent of Missing Data**: Report the percentage of missing observations for each variable included in the analysis.\n-   **Assumed Missing Data Mechanism**: Explicitly state the assumed mechanism (e.g., MAR) and provide a brief, clear justification for why this assumption is plausible in the context of the study's design and data collection procedures.\n-   **Imputation Software**: State the specific software package and version used to perform the multiple imputation (e.g., `mice` package in R, version 3.13.0).\n-   **Imputation Model Specification**: Describe the imputation model in detail. This includes listing all variables used as predictors in the imputation model, specifying any auxiliary variables that were included to improve the imputation, and noting the type of model used for each variable being imputed (e.g., predictive mean matching, logistic regression).\n-   **Number of Imputations**: Report the number of imputed datasets (*m*) that were created.\n-   **Pooling Method**: State that the results were combined across the *m* datasets using Rubin's Rules.\n-   **Diagnostics**: Briefly mention any diagnostic checks that were performed to assess the convergence of the imputation algorithm and the plausibility of the imputed values.\n\nBy following this blueprint, researchers can provide the necessary information for readers and reviewers to critically evaluate the analysis, thereby strengthening the credibility of the findings and contributing to a more rigorous and transparent scientific culture.\n:::\n\n### Checklist: RAISE-MI (Reproducible Analysis and Imputation Standards for Epidemiology – Multiple Imputation)\n\nRAISE-MI is a structured reporting framework developed to promote transparency, reproducibility, and methodological rigor in epidemiologic analyses that use MI: particularly when working with complex survey data such as NHANES. The guideline organizes reporting expectations across five domains, guiding researchers from conceptual justification to interpretation. It is designed for both manuscript preparation and instructional use, ensuring that analytic workflows are well-documented, assumptions are explicitly justified, and reproducibility is verifiable through software, code, and diagnostic reporting.\n\n**Explanation of the Five Sections**\n\n1. **Justify Mechanism**: The first domain establishes the foundation by requiring authors to state and defend the assumed missing-data mechanism (e.g., MCAR, MAR, MNAR) and explain its plausibility within the study context and survey design.\n2. **Characterize Missingness** (2a–2c): This group focuses on describing the scope and structure of missingness in the dataset. It includes quantifying missing data across variables (2a), documenting any excluded records and their impact (2b), and performing basic diagnostic tests for randomness of missingness, such as Little’s MCAR test (2c).\n3. **Imputation Model Specification** (3a–3c): These items require full transparency about the imputation model and computational setup. This includes listing variables and predictors used (3a), describing imputation parameters such as the number of imputations and method (3b), and reporting software versions and random seeds to ensure reproducibility (3c).\n4. **Analysis and Evaluation** (4a–4b): This section addresses how imputed data are analyzed and validated. Authors should explain how survey design features (weights, strata, clusters) are incorporated and how results are pooled using Rubin’s Rules (4a), and provide evidence of imputation quality through diagnostic checks like trace or density plots (4b).\n5. **Robustness and Interpretation** (5a–5b): The final domain emphasizes critical reflection. It calls for conducting and reporting sensitivity analyses comparing MI to complete-case or alternative models (5a), and discussing limitations, including residual bias and the plausibility of assumptions like MAR (5b).\n\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"font-size: 13px; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">RAISE-MI Checklist</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Item </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 1. Justify Mechanism </td>\n   <td style=\"text-align:left;width: 70em; \"> State the assumed missing-data mechanism (typically Missing at Random, MAR). Provide a subject-matter rationale for why MAR is plausible in the NHANES context (e.g., missingness related to health status, age, or survey design features). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 2a. Report Missingness Extent </td>\n   <td style=\"text-align:left;width: 70em; \"> For each variable, report the number and percentage of missing values. Highlight variables with high missingness (e.g., &gt;20–30%) and discuss whether they were imputed, handled separately, or excluded. Include an overall summary (e.g., proportion of participants with ≥1 missing variable). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 2b. Describe Data Exclusions </td>\n   <td style=\"text-align:left;width: 70em; \"> Document any records excluded from the analytic dataset (e.g., missing weights or design variables). Report the resulting sample size and, if appropriate, use a flow diagram to illustrate inclusion/exclusion and missingness. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 2c. Conduct MCAR Diagnostics </td>\n   <td style=\"text-align:left;width: 70em; \"> Report results from an exploratory test for Missing Completely at Random (e.g., Little’s MCAR test), acknowledging the test’s limitations (low power and restrictive assumptions). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 3a. Detail Imputation Model </td>\n   <td style=\"text-align:left;width: 70em; \"> List all variables included in the imputation model—outcome(s), exposures, confounders, auxiliary variables, and survey design variables (weights, strata, clusters). Describe handling of structural or skip-pattern missingness. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 3b. Specify Imputation Parameters </td>\n   <td style=\"text-align:left;width: 70em; \"> Report the number of imputations (m) and justify it (e.g., one per percent of incomplete cases). Indicate the number of iterations used for convergence and the imputation method for each variable type (e.g., predictive mean matching for continuous, logistic for binary). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 3c. Report Software &amp; Seed </td>\n   <td style=\"text-align:left;width: 70em; \"> State the software, version, and key packages used (e.g., R 4.4.0, mice 3.16.0). Document the random seed(s) or initialization method to ensure reproducibility. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 4a. Describe Analysis &amp; Pooling </td>\n   <td style=\"text-align:left;width: 70em; \"> Explain how survey design features (weights, strata, clusters) were incorporated into the analysis. Describe how estimates were pooled across imputations (e.g., Rubin’s Rules) and report the fraction of missing information (λ) for key parameters when available. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 4b. Provide Diagnostics </td>\n   <td style=\"text-align:left;width: 70em; \"> Present convergence diagnostics (e.g., trace plots) and distributional checks (e.g., density plots, mean comparisons) to demonstrate imputation stability and plausibility of imputed values. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 5a. Include Sensitivity Analyses </td>\n   <td style=\"text-align:left;width: 70em; \"> Summarize results from sensitivity analyses assessing robustness to different assumptions (e.g., compare MI with complete-case results, re-run with new seeds, or apply delta-adjustments for MNAR scenarios). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; font-weight: bold;\"> 5b. Discuss Limitations </td>\n   <td style=\"text-align:left;width: 70em; \"> In the discussion, reflect on the potential impact of missing data on study conclusions. Revisit the plausibility of MAR, acknowledge unmeasured factors, and discuss possible bias if data are MNAR. </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n\n\n### Assessment of Missing Data Analysis Reporting though RAISE-MI Checklist\n\nWe take an example article to assess adherence to the above checklist [@hossain2022association]. In this article, however, complete-case analysis was treated as the primary analysis (implicitly assuming MCAR). MI was performed as a sensitivity analysis under a MAR assumption to verify whether conclusions were robust to missing data treatment. The article is accompanied by additional information through an [Appendix](https://www.researchgate.net/publication/362125046_Supplemental_Material_The_association_between_rheumatoid_arthritis_and_cardiovascular_disease_among_adults_in_the_United_States_during_1999-2018_and_age-related_effect_modification_in_relative_and_abs).\n\n\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"font-size: 13px; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">RAISE-MI Checklist assessing Hossain et al. (2022) (with Appendix)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Item </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:left;\"> Manuscript Compliance: Met vs. Not Met </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 1. Justify Mechanism </td>\n   <td style=\"text-align:left;width: 25em; \"> State the assumed missing-data mechanism (typically Missing at Random, MAR). Provide a subject-matter rationale for why MAR is plausible in the NHANES context. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Met.</b> The appendix explicitly states the missing data assumption used for the sensitivity analysis.<br><br><i>What they wrote:</i> “Briefly, we imputed five datasets with five iterations under the missing at random (MAR) assumption.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 2a. Report Missingness Extent </td>\n   <td style=\"text-align:left;width: 25em; \"> For each variable, report the number and percentage of missing values. Highlight variables with high missingness and discuss whether they were imputed, handled separately, or excluded. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Partially Met.</b> The appendix provides percentages for key covariates but not a full list.<br><br><i>What they wrote:</i> “In our primary analysis using complete cases, we excluded 8,122 respondents due to missing data in covariates, particularly for family income (7.4%), hypertension (4.5%), and BMI (6.5%).”<br><br><i>What they should have written:</i> A supplementary table listing missingness for each covariate. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 2b. Describe Data Exclusions </td>\n   <td style=\"text-align:left;width: 25em; \"> Document any records excluded from the analytic dataset (e.g., missing weights or design variables). Report the resulting sample size and, if appropriate, use a flow diagram. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Met.</b> The main manuscript uses a flow diagram to document exclusions.<br><br><i>What they wrote:</i> “We excluded participants with other arthritis types (e.g., osteoarthritis, psoriatic arthritis)...” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 2c. Conduct MCAR Diagnostics </td>\n   <td style=\"text-align:left;width: 25em; \"> Report results from an exploratory test for Missing Completely at Random (e.g., Little’s test), acknowledging the test’s limitations. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Not Met.</b> No formal MCAR diagnostic (e.g., Little’s test) reported.<br><br><i>What they should have written:</i> “We conducted Little’s MCAR test (p &lt; .001), suggesting MCAR was violated and justifying MI under MAR.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 3a. Detail Imputation Model </td>\n   <td style=\"text-align:left;width: 25em; \"> List all variables included in the imputation model—outcome(s), exposures, confounders, auxiliary variables, and survey design variables (weights, strata, clusters). </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Partially Met.</b> Appendix lists predictor types but omits survey weights and clusters.<br><br><i>What they wrote:</i> “Predictors used included all confounders and risk factors, as well as geographical strata.”<br><br><i>What they should have written:</i> “Survey design variables (SDMVPSU, SDMVSTRA, WTMEC2YR) were also included.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 3b. Specify Imputation Parameters </td>\n   <td style=\"text-align:left;width: 25em; \"> Report the number of imputations (m) and justify it. Indicate the number of iterations used for convergence and the imputation method for each variable type. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Partially Met.</b> Appendix states 5 imputations and 5 iterations, but not the method for variable types.<br><br><i>What they wrote:</i> “We imputed five datasets with five iterations under MAR.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 3c. Report Software &amp; Seed </td>\n   <td style=\"text-align:left;width: 25em; \"> State the software, version, and key packages used. Document the random seed(s) or initialization method to ensure reproducibility. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Partially Met.</b> Software and packages listed but versions and random seed omitted.<br><br><i>What they wrote:</i> “We used R 4.0.5... and the mice package.”<br><br><i>What they should have written:</i> “mice (v3.13.0); random seed = 123.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 4a. Describe Analysis &amp; Pooling </td>\n   <td style=\"text-align:left;width: 25em; \"> Explain how survey design features were incorporated into the analysis. Describe how estimates were pooled across imputations (e.g., Rubin’s Rules). </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Met.</b> The manuscript and appendix describe survey-aware models and pooling via Rubin’s Rules.<br><br><i>What they wrote:</i> “We implemented design-based models in all imputed datasets and pooled estimates using Rubin’s rule.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 4b. Provide Diagnostics </td>\n   <td style=\"text-align:left;width: 25em; \"> Present convergence diagnostics (e.g., trace plots) and distributional checks to demonstrate imputation stability and plausibility of imputed values. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Not Met.</b> No diagnostics reported for imputation procedure.<br><br><i>What they should have written:</i> “Convergence assessed via trace plots; imputed vs. observed distributions compared via density plots.” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 5a. Include Sensitivity Analyses </td>\n   <td style=\"text-align:left;width: 25em; \"> Summarize results from sensitivity analyses assessing robustness to different assumptions (e.g., compare MI with complete-case results). </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Met.</b> MI framed and reported as sensitivity analysis.<br><br><i>What they wrote:</i> “The RA–CVD association did not change substantially in sensitivity analyses using multiple imputations (eTable 4).” </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 12em; font-weight: bold;\"> 5b. Discuss Limitations </td>\n   <td style=\"text-align:left;width: 25em; \"> In the discussion, reflect on the potential impact of missing data on study conclusions. Revisit the plausibility of MAR and discuss possible bias if data are MNAR. </td>\n   <td style=\"text-align:left;width: 45em; \"> <b>Met.</b> Limitations section discusses robustness to missing data assumptions.<br><br><i>What they wrote:</i> “Sensitivity analyses using multiple imputations yielded similar estimates, suggesting robustness to MCAR and MAR assumptions.” </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/Fv4FjZzNI_A\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n## Video Lesson Slides\n\nMissing data\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRQfavQM7rta4wE-5eBOg_1Z2z-67uLmOqFM7_I2k7g6YwlwhNcNNAxve_XPrddbQSb421RbwFM7y-w/embed?start=false&amp;loop=false&amp;delayms=3000\" frameborder=\"0\" width=\"672\" height=\"398\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n\n</iframe>\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQsPyxXYcVV9aDxKiQCOkkwCGq0ChizfR0Z2e-xPyAfFK2MuGeOr_NH6r4P-iviXDvRTBD5PcYf8FeZ/embed?start=false&amp;loop=false&amp;delayms=3000\" frameborder=\"0\" width=\"672\" height=\"398\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n\n</iframe>\n\nReporting guideline\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSi4nsgMV3iCEHEVDGiwnMxvumKE_muP6xyh8GYSx7m2g60ma5Pc_bUBC1rRD6MApwI189vn_RcPL4Q/embed?start=false&amp;loop=false&amp;delayms=3000\" frameborder=\"0\" width=\"672\" height=\"398\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n\n</iframe>\n\n## Links\n\nVideo Lessons\n\n-   [Google Slides](https://docs.google.com/presentation/d/1-H4_AorB9mJfJZMnpz4r2bo5yXFldCDFpQQKdtUSHD0/edit?usp=sharing)\n-   [PDF Slides](slides/Missing.pdf)\n-   [PDF Slides for FAQ](slides/missingFAQ.pdf)\n\nReporting guideline\n\n-   [Google Slides](https://docs.google.com/presentation/d/1apk6C-kW-YOBZcBR5D2Hijm_s8BNa0T7eLnT2LHkPbc/edit?usp=sharing)\n-   [PDF Slides](slides/MissingReport.pdf)\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}