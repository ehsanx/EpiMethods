{
  "hash": "6bd8a6f5f0522bdb1fd78ccf8c92fe94",
  "result": {
    "markdown": "## Unsupervised learning {.unnumbered}\n\nIn this chapter, we will talk about unsupervised learning.\n\nIn the initial code chunk, we load a specific library that will be utilized for publishing-related functionality throughout the chapter.\n\n\n\n\n\n\n\n\n\n### Clustering\n\nClustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity.\n\nGroup characteristics include (to the extent that is possible)\n\n-   low inter-class similarity: observation from different clusters would be dissimilar\n-   high intra-class similarity: observation from the same cluster would be similar\n\nWithin-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances [@Cross-validation]\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Images/machinelearning/wcss.png){width=90%}\n:::\n:::\n\n\n\n\n### K-means\n\nK-means is a very popular clustering algorithm, that partitions the data into $k$ groups.\n\nAlgorithm:\n\n-   Determine a number $k$ (e.g., could be 3)\n-   randomly select $k$ subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster.\n-   By Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong.\n-   compute new mean value for each cluster.\n-   based on this new mean, try to determine again in which cluster the data points belong.\n-   process continues until the data points do not change cluster membership.\n\n### Read previously saved data\n\nWe read a previously saved dataset from a specified file path.\n\n\n\n\n::: {.cell hash='machinelearning6_cache/pdf/data_aa0c81e18e7b4bb0df2abdfea8a1709f'}\n\n```{.r .cell-code}\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n```\n:::\n\n\n\n\nIn the next few code chunks, we implement k-means clustering on various subsets of the data, visualizing the results and displaying the cluster centers. The first example uses two variables, the second example uses three, and in the third example, a larger subset of variables is selected but not immediately utilized in the clustering. In the subsequent code chunk, we apply k-means clustering to the larger subset of variables, displaying various results and aggregating data by cluster to display mean and standard deviation values for each variable within each cluster.\n\n#### Example 1\n\n\n\n\n::: {.cell hash='machinelearning6_cache/pdf/ex1_a6b8e01dfbc5eb7e7681f44eb69e62fd'}\n\n```{.r .cell-code}\ndatax0 <- ObsData[c(\"Heart.rate\", \"edu\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   Heart.rate      edu\n#> 1   54.55138 11.44494\n#> 2  134.96277 11.75466\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n```\n\n::: {.cell-output-display}\n![](machinelearning6_files/figure-pdf/ex1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n#### Example 2\n\n\n\n\n::: {.cell hash='machinelearning6_cache/pdf/ex2_80c2dd7931c6080e5257f36152de5c45'}\n\n```{.r .cell-code}\ndatax0 <- ObsData[c(\"blood.pressure\", \"Heart.rate\", \"Respiratory.rate\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   blood.pressure Heart.rate Respiratory.rate\n#> 1       73.71684   54.95789         22.76723\n#> 2       80.10812  135.08956         29.85267\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n```\n\n::: {.cell-output-display}\n![](machinelearning6_files/figure-pdf/ex2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n#### Example with many variables\n\n\n\n\n::: {.cell hash='machinelearning6_cache/pdf/exmany_9838444f5588bfe47ea6ac456592ad66'}\n\n```{.r .cell-code}\ndatax <- ObsData[c(\"edu\", \"blood.pressure\", \"Heart.rate\", \n                   \"Respiratory.rate\" , \"Temperature\",\n                   \"PH\", \"Weight\", \"Length.of.Stay\")]\n```\n:::\n\n::: {.cell hash='machinelearning6_cache/pdf/kmeans_30d3af43af1550bf02878dcb447ad995'}\n\n```{.r .cell-code}\nkres <- kmeans(datax, centers = 3)\n#kres\nhead(kres$cluster)\n#> [1] 1 1 1 3 1 2\nkres$size\n#> [1] 2793 1688 1254\nkres$centers\n#>        edu blood.pressure Heart.rate Respiratory.rate Temperature       PH\n#> 1 11.85833       54.26924  136.37451         29.76119    37.85078 7.385249\n#> 2 11.54214      128.33886  126.12026         29.36611    37.68129 7.401027\n#> 3 11.46134       65.47249   53.24242         22.65973    37.01597 7.378482\n#>     Weight Length.of.Stay\n#> 1 68.63384       23.42356\n#> 2 66.68351       20.68128\n#> 3 67.57291       18.58931\naggregate(datax, by = list(cluster = kres$cluster), mean)\n#>   cluster      edu blood.pressure Heart.rate Respiratory.rate Temperature\n#> 1       1 11.85833       54.26924  136.37451         29.76119    37.85078\n#> 2       2 11.54214      128.33886  126.12026         29.36611    37.68129\n#> 3       3 11.46134       65.47249   53.24242         22.65973    37.01597\n#>         PH   Weight Length.of.Stay\n#> 1 7.385249 68.63384       23.42356\n#> 2 7.401027 66.68351       20.68128\n#> 3 7.378482 67.57291       18.58931\naggregate(datax, by = list(cluster = kres$cluster), sd)\n#>   cluster      edu blood.pressure Heart.rate Respiratory.rate Temperature\n#> 1       1 3.162485       11.93763   23.13140         13.67791    1.781692\n#> 2       2 3.091605       18.58070   27.68369         14.08169    1.610746\n#> 3       3 3.160538       31.89150   23.63993         13.60831    1.832389\n#>          PH   Weight Length.of.Stay\n#> 1 0.1082140 27.99506       29.01143\n#> 2 0.1009567 32.15078       23.37223\n#> 3 0.1226041 26.87075       20.82024\n```\n:::\n\n\n\n\n### Optimal number of clusters\n\nNext, we explore determining the optimal number of clusters, visualizing the total within-cluster sum of squares for different values of k and indicating a chosen value of k with a vertical line on the plot.\n\n\n\n\n::: {.cell hash='machinelearning6_cache/pdf/optk_893b89d67af580218702b1bf5af6239b'}\n\n```{.r .cell-code}\nrequire(factoextra)\n#> Loading required package: factoextra\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_nbclust(datax, kmeans, method = \"wss\")+\n  geom_vline(xintercept=3,linetype=3)\n```\n\n::: {.cell-output-display}\n![](machinelearning6_files/figure-pdf/optk-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nHere the vertical line is chosen based on elbow method [@clustering].\n\n### Discussion\n\n-   We need to supply a number, $k$: but we can test different $k$s to identify optimal value\n-   Clustering can be influenced by outliners, so median based clustering is possible\n-   mere ordering can influence clustering, hence we should choose different initial means (e.g., `nstart` should be greater than 1).\n\n### Video content (optional)\n\n::: callout-tip\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n:::\n\n::: {style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"}\n<iframe src=\"https://www.youtube.com/embed/xhjcfvWDZVk\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen>\n\n</iframe>\n:::\n\n### References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}