## Cross-validation {.unnumbered}

Cross-validation is another important technique used to assess the performance of machine learning models and mitigate the risk of overfitting. This tutorial focuses on k-fold cross-validation as a strategy to obtain a more generalized and robust assessment of the model's performance. It shows both manual calculations for individual folds and an automated approach using the caret package. This ensures that you aren't simply fitting your model well to a specific subset of your data but are achieving good performance in a general sense.

```{r setup, warning=FALSE, message=FALSE,cache=TRUE}
# Load required packages
library(caret)
library(knitr)
library(Publish)
library(car)
library(DescTools)
```

### Load data

Load the data saved at the end of previous part of the lab.

```{r load, warning=FALSE, cache=TRUE}
load(file="Data/predictivefactors/cholesterolNHANES15part2.RData")
```

### k-fold cross-vaildation

::: column-margin
See @Cross-validation
:::

We can set the number of folds to 5 (k = 5). A random seed is used for reproducibility. We use the function `createFolds` to create the folds. The data is divided based on the cholesterol levels, with each fold having approximately equal numbers of data points. The resulting structure contains training indices for each fold.

We can also examine the approximate size of training and test sets for each fold. The dimensions are displayed to understand the partitioning, and you can examine the length of indices in each fold to confirm the size of the training sets.

```{r kcv, warning=FALSE, cache=TRUE}
k = 5
dim(analytic3)
set.seed(567)

# Create folds (based on the outcome)
folds <- createFolds(analytic3$cholesterol, k = k, list = TRUE, 
                     returnTrain = TRUE)
mode(folds)

# Approximate training data size
dim(analytic3)*4/5

# Approximate test data size
dim(analytic3)/5  

length(folds[[1]])
length(folds[[2]])
length(folds[[3]])
length(folds[[4]])
length(folds[[5]])

str(folds[[1]])
str(folds[[2]])
str(folds[[3]])
str(folds[[4]])
str(folds[[5]])
```

#### Calculation for Fold 1

The first fold is used as an example. The indices for the training data in the first fold are extracted and used to subset the main data set into training and test sets for that fold. Then a linear regression model is fitted using the training data, and predictions are made on the test set. The model's performance is evaluated using the same performance function as before.

```{r fold1, warning=FALSE, cache=TRUE}
fold.index <- 1
fold1.train.ids <- folds[[fold.index]]
head(fold1.train.ids)

fold1.train <- analytic3[fold1.train.ids,]
fold1.test <- analytic3[-fold1.train.ids,]
formula4

model.fit <- lm(formula4, data = fold1.train)
predictions <- predict(model.fit, newdata = fold1.test)

perform(new.data=fold1.test, y.name = "cholesterol", 
        model.fit = model.fit)
```

#### Calculation for Fold 2

The same process is repeated for the second fold. This way, you can manually evaluate how the model performs on different subsets of the data, making the performance assessment more robust.

```{r fold2, warning=FALSE, cache=TRUE}
fold.index <- 2
fold1.train.ids <- folds[[fold.index]]
head(fold1.train.ids)

fold1.train <- analytic3[fold1.train.ids,]
fold1.test <- analytic3[-fold1.train.ids,]

model.fit <- lm(formula4, data = fold1.train)

predictions <- predict(model.fit, newdata = fold1.test)
perform(new.data=fold1.test, y.name = "cholesterol", 
        model.fit = model.fit)
```

#### Using caret package to automate

::: column-margin
See @tuning
:::

Instead of manually running the process for each fold, the caret package can be used to automate k-fold cross-validation. A control object is set up specifying that 5-fold cross-validation should be used. Then, the train function from the caret package can be used to fit the linear regression model on each fold.

After fitting, you can access summary results for each fold in the resampling results. This summary provides performance metrics such as R-squared for each fold. You can calculate the mean and standard deviation of these metrics to get an overall sense of the model's performance.

Additionally, an adjusted R-squared can be calculated to consider the number of predictors in the model, giving a more accurate sense of the model's explanatory power when you have multiple predictors.

```{r caret, cache= TRUE}
# Using Caret package
set.seed(567)

# make a 5-fold CV
ctrl<-trainControl(method = "cv",number = 5)

# fit the model with formula = formula4
# use training method lm
fit4.cv<-train(formula4, trControl = ctrl,
               data = analytic3, method = "lm")
fit4.cv

# extract results from each test data 
summary.res <- fit4.cv$resample
summary.res
mean(fit4.cv$resample$Rsquared)
sd(fit4.cv$resample$Rsquared)

# # extract adj R2
# k <- 5
# p <- 2
# n <- round(nrow(analytic3)/k)
# summary.res$adjR2 <- 1-(1-fit4.cv$resample$Rsquared)*
#  ((n-1)/(n-p))
# summary.res
```

### References
