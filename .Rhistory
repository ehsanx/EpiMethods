# analytic.data2 <- merge(analytic.data1, smq1, by = c("SEQN"), all=TRUE)
# analytic.data3 <- merge(analytic.data2, alq1, by = c("SEQN"), all=TRUE)
# analytic.data4 <- merge(analytic.data3, chl1, by = c("SEQN"), all=TRUE)
# analytic.data5 <- merge(analytic.data4, tri1, by = c("SEQN"), all=TRUE)
# analytic.data6 <- merge(analytic.data5, paq1, by = c("SEQN"), all=TRUE)
# analytic.data7 <- merge(analytic.data6, diq1, by = c("SEQN"), all=TRUE)
# dim(analytic.data7)
str(analytic.data7)
head(analytic.data7)
summary(analytic.data7$RIDAGEYR)
dim(analytic.data7)
analytic.data8 <- analytic.data7
analytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR < 20] <- NA
#analytic.data8 <- subset(analytic.data7, RIDAGEYR >= 20)
dim(analytic.data8)
analytic.data8$DMDEDUC3 <- NULL # not relevant for adults
#analytic.data8$BMDBMIC <- NULL # not relevant for adults
factor.names <- c("RIAGENDR","DMDBORN4","RIDRETH3",
"DMDEDUC2","DMDMARTL","INDHHIN2",
"SMQ040", "PAQ605", "PAQ650", "DIQ010")
numeric.names <- c("SEQN","RIDAGEYR","WTMEC2YR",
"SDMVPSU", "SDMVSTRA",
"BPXDI1", "BPXSY1", "BMXWT", "BMXHT",
"BMXBMI", "BMXWAIST",
"ALQ130", "LBXTC", "LBDTCSI",
"LBXSTR", "LBXSUA", "LBXSTP", "LBXSTB",
"LBXSPH", "LBXSNASI", "LBXSKSI",
"LBXSGB","LBXSCA")
analytic.data8[factor.names] <- apply(X = analytic.data8[factor.names],
MARGIN = 2, FUN = as.factor)
# analytic.data8[numeric.names] <- apply(X = analytic.data8[numeric.names],
#                                        MARGIN = 2, FUN =
#                                          function (x) as.numeric(as.character(x)))
analytic.data9 <- analytic.data8
analytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == "Don't Know"] <- NA
#analytic.data9 <- subset(analytic.data8, DMDBORN4 != "Don't Know")
dim(analytic.data9)
analytic.data10 <- analytic.data9
analytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == "Don't Know"] <- NA
#analytic.data10 <- subset(analytic.data9, DMDEDUC2 != "Don't Know")
dim(analytic.data10)
analytic.data11 <- analytic.data10
analytic.data11$DMDMARTL[analytic.data11$DMDMARTL == "Don't Know"] <- NA
analytic.data11$DMDMARTL[analytic.data11$DMDMARTL == "Refused"] <- NA
# analytic.data11 <- subset(analytic.data10, DMDMARTL != "Don't Know" & DMDMARTL != "Refused")
dim(analytic.data11)
analytic.data12 <- analytic.data11
analytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == "Don't Know"] <- NA
analytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == "Refused"] <- NA
analytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == "Under $20,000"] <- NA
analytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == "$20,000 and Over"] <- NA
# analytic.data12 <- subset(analytic.data11, INDHHIN2 != "Don't know" & INDHHIN2 !=  "Refused" & INDHHIN2 != "Under $20,000" & INDHHIN2 != "$20,000 and Over" )
dim(analytic.data12)
#analytic.data11 <- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )
#dim(analytic.data11) # this are listed as NA anyway
analytic.data13 <- analytic.data12
analytic.data13$PAQ605[analytic.data13$PAQ605 == "Don't know"] <- NA
analytic.data13$PAQ605[analytic.data13$PAQ605 == "Refused"] <- NA
# analytic.data13 <- subset(analytic.data12, PAQ605 != "Don't know" & PAQ605 != "Refused")
dim(analytic.data13)
analytic.data14 <- analytic.data13
analytic.data14$PAQ650[analytic.data14$PAQ650 == "Don't know"] <- NA
analytic.data14$PAQ650[analytic.data14$PAQ650 == "Refused"] <- NA
# analytic.data14 <- subset(analytic.data13, PAQ650 != "Don't Know" & PAQ650 != "Refused")
dim(analytic.data14)
analytic.data15 <- analytic.data14
analytic.data15$DIQ010[analytic.data15$DIQ010 == "Don't know"] <- NA
analytic.data15$DIQ010[analytic.data15$DIQ010 == "Refused"] <- NA
# analytic.data15 <- subset(analytic.data14, DIQ010 != "Don't Know" & DIQ010 != "Refused")
dim(analytic.data15)
# analytic.data15$ALQ130[analytic.data15$ALQ130 > 100] <- NA
# summary(analytic.data15$ALQ130)
table(analytic.data15$SMQ040,useNA = "always")
table(analytic.data15$PAQ605,useNA = "always")
table(analytic.data15$PAQ650,useNA = "always")
table(analytic.data15$PAQ650,useNA = "always")
require(car)
analytic.data15$RIDRETH3 <- recode(analytic.data15$RIDRETH3,
"c('Mexican American','Other Hispanic')='Hispanic';
'Non-Hispanic White'='White';
'Non-Hispanic Black'='Black';
c('Non-Hispanic Asian',
'Other Race - Including Multi-Rac')='Other';
else=NA")
analytic.data15$DMDEDUC2 <- recode(analytic.data15$DMDEDUC2,
"c('Some college or AA degree',
'College graduate or above')='College';
c('9-11th grade (Includes 12th grad',
'High school graduate/GED or equi')
='High.School';
'Less than 9th grade'='School';
else=NA")
analytic.data15$DMDMARTL <- recode(analytic.data15$DMDMARTL,
"c('Divorced','Separated','Widowed')
='Previously.married';
c('Living with partner', 'Married')
='Married';
'Never married'='Never.married';
else=NA")
analytic.data15$INDHHIN2 <- recode(analytic.data15$INDHHIN2,
"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',
'$10,000 to $14,999', '$15,000 to $19,999',
'$20,000 to $24,999')='<25k';
c('$25,000 to $34,999', '$35,000 to $44,999',
'$45,000 to $54,999') = 'Between.25kto54k';
c('$55,000 to $64,999', '$65,000 to $74,999',
'$75,000 to $99,999')='Between.55kto99k';
'$100,000 and Over'= 'Over100k';
else=NA")
analytic.data15$SMQ040 <- recode(analytic.data15$SMQ040,
"'Every day'='Every.day';
'Not at all'='Not.at.all';
'Some days'='Some.days';
else=NA")
analytic.data15$DIQ010 <- recode(analytic.data15$DIQ010,
"'No'='No';
c('Yes', 'Borderline')='Yes';
else=NA")
require(DataExplorer)
plot_missing(analytic.data15)
?str
?hclust
?varclus
?glm
# Load required packages
library(rms)
library(Hmisc)
library(Publish)
library(car)
library(pROC)
#### AUC
pred.y <- predict(fit5, type = "response")
# Load required packages
library(rms)
library(Hmisc)
library(Publish)
library(car)
library(pROC)
load(file = "Data/predictivefactors/cholesterolNHANES15part1.RData")
# Binary variable
analytic3$cholesterol.bin <- ifelse(analytic3$cholesterol < 200, "healthy", "unhealthy")
table(analytic3$cholesterol.bin)
# Changing the reference category
analytic3$cholesterol.bin <- as.factor(analytic3$cholesterol.bin)
analytic3$cholesterol.bin <- relevel(analytic3$cholesterol.bin, ref = "unhealthy")
table(analytic3$cholesterol.bin)
# Regression model
formula5x <- as.formula("cholesterol.bin~gender + age + born +
race + education + married +
income + diastolicBP + systolicBP +
bmi + bodyweight + bodyheight + waist +
triglycerides + uric.acid +
protein + bilirubin + phosphorus + sodium + potassium +
globulin + calcium + physical.work + physical.recreational +
diabetes")
# Summary
fit5x <- glm(formula5x, family = binomial(), data = analytic3)
publish(fit5x)
# VIF
car::vif(fit5x)
require(pROC)
pred.y <- predict(fit5x, type = "response")
rocobj <- roc(analytic3$cholesterol.bin, pred.y)
rocobj
auc(rocobj)
formula5 <- as.formula("cholesterol.bin~gender + age + born +
race + education + married +
income + diastolicBP + systolicBP +
bmi +
triglycerides + uric.acid +
protein + bilirubin + phosphorus + sodium + potassium +
globulin + calcium + physical.work + physical.recreational +
diabetes")
fit5 <- glm(formula5, family = binomial(), data = analytic3)
publish(fit5)
# VIF
car::vif(fit5)
#### AUC
pred.y <- predict(fit5, type = "response")
rocobj <- roc(analytic3$cholesterol.bin, pred.y)
rocobj
auc(rocobj)
install.packages("SASxport")
devtools::install_github("warnes/SASxport")
install.packages("C:/Users/Ehsan/Desktop/SASxport_1.7.0.tar.gz", repos = NULL, type = "source")
install.packages("nhanesA")
install.packages("MatchIt")
install.packages("WeightIt")
install.packages("skimr")
install.packages("jtools")
install.packages("xml2", dependencies = TRUE)
install.packages("table1")
install.packages("downlit")
install.packages("huxtable")
install.packages("olsrr")
install.packages("psych")
install.packages("DescTools")
install.packages("MissMech")
install_github("cran/MissMech")
library(devtools)
install.packages("MissMech")
install_github("cran/MissMech")
install.packages("mitml")
# Load required packages
library(mice)
library(DataExplorer)
library(VIM)
library(mitools)
require(VIM)
load("Data/missingdata/NHANES17.RData")
NHANES17s <- analytic.with.miss[1:30,c("age", "bmi", "cholesterol","diastolicBP")]
NHANES17s
NHANES17s[complete.cases(NHANES17s),]
md.pattern(NHANES17s)
# Inspect the missing data pattern (each row = pattern)
# possible missingness (0,1) pattern and counts
# last col = missing counts for each variables
# last row = how many variable values missing in the row
# First col: Frequency of the pattern
# e,g, 2 cases missing for bmi
require(DataExplorer)
plot_missing(NHANES17s)
# check the missingness
require(VIM)
marginplot(NHANES17s[, c("diastolicBP", "bmi")])
marginplot(NHANES17s[, c("diastolicBP", "cholesterol")])
marginplot(NHANES17s[, c("cholesterol", "bmi")])
# distribution of observed data given the other variable is observed
# for MCAR, blue and red box plots should be similar
# Replace missing values by mean
imputation1 <- mice(NHANES17s,
method = "mean", # Replace by mean of the other values
m = 1, # Number of multiple imputations.
maxit = 1) # Number of iteration; mostly useful for convergence
imputation1$imp
complete(imputation1, action = 1) # this is a function from mice
# there is another function in tidyr with the same name!
# use mice::complete() to avoid conflict
## the imputed dataset
imputation2 <- mice(NHANES17s,
method = "norm.predict", # regression imputation
seed = 1,
m = 1,
print = FALSE)
# look at all imputed values
imputation2$imp
# examine the correlation between age and bmi before and after imputation
fit1 <- lm(age ~ bmi, NHANES17s)
summary(fit1) ## original data
sqrt(summary(fit1)$r.squared)
fit2 <- lm(age ~ bmi, mice::complete(imputation2))
summary(fit2) ## imputed complete data
sqrt(summary(fit2)$r.squared)
## Relationship become stronger before imputation.
# with(data=NHANES17s, cor(age, bmi, use = "complete.obs"))
with(data=NHANES17s, cor(age, bmi, use = "pairwise.complete.obs"))
with(data = mice::complete(imputation2), cor(age, bmi))
imputation3 <- mice(NHANES17s, method = "norm.nob", # stochastic regression imputation
m = 1, maxit = 1, seed = 504, print = FALSE)
# look at all imputed values
imputation3$imp
#mice::complete(imputation3)
# examine the correlation between age and bmi before and after imputation
fit1 <- lm(age ~ bmi, NHANES17s)
summary(fit1)
fit3 <- lm(age ~ bmi, mice::complete(imputation3))
summary(fit3)
## Fitted coefficients of bmi are much closer before and after imputation
# with(data=NHANES17s, cor(age, bmi, use = "complete.obs"))
with(data=NHANES17s, cor(age, bmi, use = "pairwise.complete.obs"))
with(data = mice::complete(imputation3), cor(age, bmi))
# see the direction change?
imputation3b <- mice(NHANES17s, method = "pmm",
m = 1, maxit = 1,
seed = 504, print = FALSE)
with(data=NHANES17s, cor(age, bmi, use = "pairwise.complete.obs"))
with(data = mice::complete(imputation3b), cor(age, bmi))
ini <- mice(data = NHANES17s, maxit = 0, print = FALSE)
pred <- ini$pred
pred
# A value of 1 indicates that column variables (say, bmi, cholesterol, diastolicBP)
# are used as a predictor to impute the a row variable (say, age).
pred[,"diastolicBP"] <- 0
# if you believe 'diastolicBP' should not be a predictor in any imputation model
pred
# for cholesterol: bmi and age used to predict cholesterol (diastolicBP is not a predictor)
# for diastolicBP: bmi, age and cholesterol used to predict diastolicBP
# (diastolicBP itself is not a predictor)
meth <- ini$meth
meth
# pmm is generally a good method,
# but let's see how to work with other methods
# just as an example.
# Specifying imputation method:
meth["bmi"] <- "mean"
# for BMI: no predictor used in mean method
# (only average of observed bmi)
meth["cholesterol"] <- "norm.predict"
meth["diastolicBP"] <- "norm.nob"
meth
predictor.selection <- quickpred(NHANES17s,
mincor=0.1, # absolute correlation
minpuc=0.1) # proportion of usable cases
predictor.selection
# Step 1 Impute the incomplete data m=10 times
imputation4 <- mice(data=NHANES17s,
seed=504,
method = meth,
predictorMatrix = predictor.selection,
m=10, # imputation will be done 10 times (i.e., 10 imputed datasets)
maxit=3)
imputation4$pred
## look at the variables used for imputation
mice::complete(imputation4, action = 1) # 1 imputed data
all <- mice::complete(imputation4, action="long") # combine all 5 imputed datasets
dim(all)
head(all)
## you can change the way of displaying the data
data_hori <- mice::complete(imputation4, action="broad") # display five imputations horizontally
dim(data_hori)
head(data_hori)
## Compare means of each imputed dataset
colMeans(data_hori)
imputation4
imputation4[[1]]
mice::complete(imputation4, action = 1)
mice::complete(imputation4, action = 10)
# Step 2 Analyze the imputed data
fit4 <- with(data = imputation4, exp = lm(cholesterol ~ age + bmi + diastolicBP))
## fit model with each of 10 datasets separately
fit4
# Step 3 pool the analysis results
est1 <- mice::pool(fit4)
## pool all estimated together using Rubin's rule
est1
require(dplyr)
res10 <- summary(fit4) %>% as_tibble %>% print(n=40)
m10 <- res10[res10$term == "age",]
m10
m.number <- 10
# estimate = pooled estimate
# = sum of (m “beta-hat” estimates) / m (mean of m estimated statistics)
estimate <- mean(m10$estimate)
estimate
# ubar = sum of (m variance[beta] estimates) / m
# = within-imputation variance (mean of estimated variances)
ubar.var <- mean(m10$std.error^2)
ubar.var
# b =  variance of (m “beta-hat” estimates)
# = between-imputation variance
# (degree to which estimated statistic /
# “beta-hat” varies across m imputed datasets).
# This b is not available for single imputation when m = 1.
b.var <- var(m10$estimate)
b.var
# t = ubar + b + b/m = total variance according to Rubin’s rules
# (within-imputation & between imputation variation)
t.var <- ubar.var + b.var + b.var/m.number
t.var
# riv = relative increase in variance
riv = (b.var + b.var/m.number)/ubar.var
riv
# lambda = proportion of variance to due nonresponse
lambda = (b.var + b.var/m.number)/t.var
lambda
# df (approximate for large sample without correction)
df.large.sample <- (m.number - 1)/lambda^2
df.large.sample
# df (hypothetical complete data)
dfcom <- m10$nobs[1] - 4 # n = 30, # parameters = 4
dfcom
# df (Barnard-Rubin correction)
df.obs <- (dfcom + 1)/(dfcom + 3) * dfcom * (1 - lambda)
df.c <- df.large.sample * df.obs/(df.large.sample + df.obs)
df.c
# fmi = fraction of missing information per parameter
fmi = (riv + 2/(df.large.sample +3)) / (1 + riv)
fmi # based on large sample approximation
fmi = (riv + 2/(df.c +3)) / (1 + riv)
fmi # Barnard-Rubin correction
est1
data <- NHANES17s
imp <- mice(data, seed = 504, m = 100, print = FALSE)
## Multiple imputation with 100 imputations, resulting in 100 imputed datasets
scope0 <- list(upper = ~ age + bmi + cholesterol, lower = ~1)
expr <- expression(f1 <- lm(diastolicBP ~ age),
f2 <- step(f1, scope = scope0, trace = FALSE))
fit5 <- with(imp, expr)
## apply stepwise on each of the imputed dataset separately
formulas <- lapply(fit5$analyses, formula)
## fit5$analyses returns the selection result for each imputed dataset
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
## look at the terms on each models
table(votes)
## Set up the stepwise variable selection, from null model to full model
scope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)
## Set up the stepwise variable selection, from important only model to full model
expr <- expression(f1 <- lm(diastolicBP ~ age),
f2 <- step(f1, scope = scope, trace = FALSE))
fit5 <- with(imp, expr)
## apply stepwise on each of the imputed dataset separately
formulas <- lapply(fit5$analyses, formula)
## fit5$analyses returns the selection result for each imputed dataset
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
## look at the terms on each models
table(votes)
Stack.data <- mice::complete(imp, action="long")
head(Stack.data)
tail(Stack.data)
fitx <- lm(diastolicBP ~ age + bmi + cholesterol, data = Stack.data)
fity <- step(fitx, scope = scope0, trace = FALSE)
require(Publish)
publish(fity)
# m = 100
fit7 <- with(data=imp, expr=lm(diastolicBP ~ 1))
names(fit7)
fit7$analyses[[1]]
fit7$analyses[[100]]
fit8 <- with(data=imp, expr=lm(diastolicBP ~ bmi))
fit8$analyses[[45]]
fit8$analyses[[99]]
# The D1-statistics is the multivariate Wald test.
stat <- D1(fit8, fit7)
## use Wald test to see if we should add bmi into the model
stat
# which indicates that adding bmi into our model might not be useful
fit9 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi))
stat <- D1(fit9, fit8)
## use Wald test to see if we should add age into the model
stat
# which indicates that adding age into our model might not be useful
fit10 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi + cholesterol))
stat <- D1(fit10, fit9)
## use Wald test to see if we should add cholesterol into the model
stat
# which indicates that adding cholesterol into our model might not be useful
stat <- pool.compare(fit10, fit7, method = "likelihood", data=imp)
## test to see if we should add all 3 variables into the model
stat$pvalue
# which indicates that adding none of the variables into our model might be useful
impx <- mice(NHANES17s, seed = 504, m=1, print=FALSE)
completedata <- mice::complete(impx)
set.seed(504)
votes <-c()
formula0 <- as.formula("diastolicBP ~ age + bmi + cholesterol")
scope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)
for (i in 1:200){
ind <- sample(1:nrow(completedata),replace = TRUE)
newdata <- completedata[ind,]
full.model <- glm(formula0, data = newdata)
f2 <- MASS::stepAIC(full.model,
scope = scope, trace = FALSE)
formulas <- as.formula(f2)
temp <- unlist(labels(terms(formulas)))
votes <- c(votes,temp)
}
table(votes)
## among 200 bootstrap samples how many times that each
## variable appears in the final model. Models have different
## variables are attributed to sampling variation
## Recall the imputation we have done before
imputation5 <- mice(NHANES17s, seed = 504,
m=10,
maxit = 5,
print=FALSE)
plot(imputation5)
## We hope to see no pattern in the trace lines
## Sometimes to comfirm this we may want to run with more iterations
imputation5_2 <- mice(NHANES17s, seed = 504,
m=10,
maxit = 50,
print=FALSE)
plot(imputation5_2)
## We could compare the imputed and observed data using Density plots
densityplot(imputation5, layout = c(2, 2))
imputation5_3 <- mice(NHANES17s, seed = 504,
m=50,
maxit = 50,
print=FALSE)
densityplot(imputation5_3)
## a subjective judgment on whether you think if there is significant discrepancy
bwplot(imputation5, age + bmi + cholesterol +diastolicBP ~ .imp, layout = c(2, 2))
bwplot(imputation5_3)
## Plot a box plot to compare the imputed and observed values
est1
fit4
mice::pool(fit4)
install.packages("mice")
install.packages("mice")
require("mice")
fit4
summary(fit4)
mice::pool(fit4)
est1
# Load required packages
library(mice)
library(DataExplorer)
library(VIM)
library(mitools)
# Step 3 pool the analysis results
est1 <- mice::pool(fit4)
## pool all estimated together using Rubin's rule
est1
fit4
mice::pool(fit4)
est1
edit(est1)
require(xtable)
xtable(est1)
est1
