dfcom <- m10$nobs[1] - 4 # n = 30, # parameters = 4
dfcom
# df (Barnard-Rubin correction)
df.obs <- (dfcom + 1)/(dfcom + 3) * dfcom * (1 - lambda)
df.c <- df.large.sample * df.obs/(df.large.sample + df.obs)
df.c
# fmi = fraction of missing information per parameter
fmi = (riv + 2/(df.large.sample +3)) / (1 + riv)
fmi # based on large sample approximation
fmi = (riv + 2/(df.c +3)) / (1 + riv)
fmi # Barnard-Rubin correction
est1
data <- NHANES17s
imp <- mice(data, seed = 504, m = 100, print = FALSE)
## Multiple imputation with 100 imputations, resulting in 100 imputed datasets
scope0 <- list(upper = ~ age + bmi + cholesterol, lower = ~1)
expr <- expression(f1 <- lm(diastolicBP ~ age),
f2 <- step(f1, scope = scope0, trace = FALSE))
fit5 <- with(imp, expr)
## apply stepwise on each of the imputed dataset separately
formulas <- lapply(fit5$analyses, formula)
## fit5$analyses returns the selection result for each imputed dataset
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
## look at the terms on each models
table(votes)
## Set up the stepwise variable selection, from null model to full model
scope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)
## Set up the stepwise variable selection, from important only model to full model
expr <- expression(f1 <- lm(diastolicBP ~ age),
f2 <- step(f1, scope = scope, trace = FALSE))
fit5 <- with(imp, expr)
## apply stepwise on each of the imputed dataset separately
formulas <- lapply(fit5$analyses, formula)
## fit5$analyses returns the selection result for each imputed dataset
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
## look at the terms on each models
table(votes)
Stack.data <- mice::complete(imp, action="long")
head(Stack.data)
tail(Stack.data)
fitx <- lm(diastolicBP ~ age + bmi + cholesterol, data = Stack.data)
fity <- step(fitx, scope = scope0, trace = FALSE)
require(Publish)
publish(fity)
# m = 100
fit7 <- with(data=imp, expr=lm(diastolicBP ~ 1))
names(fit7)
fit7$analyses[[1]]
fit7$analyses[[100]]
fit8 <- with(data=imp, expr=lm(diastolicBP ~ bmi))
fit8$analyses[[45]]
fit8$analyses[[99]]
# The D1-statistics is the multivariate Wald test.
stat <- D1(fit8, fit7)
## use Wald test to see if we should add bmi into the model
stat
# which indicates that adding bmi into our model might not be useful
fit9 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi))
stat <- D1(fit9, fit8)
## use Wald test to see if we should add age into the model
stat
# which indicates that adding age into our model might not be useful
fit10 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi + cholesterol))
stat <- D1(fit10, fit9)
## use Wald test to see if we should add cholesterol into the model
stat
# which indicates that adding cholesterol into our model might not be useful
stat <- pool.compare(fit10, fit7, method = "likelihood", data=imp)
## test to see if we should add all 3 variables into the model
stat$pvalue
# which indicates that adding none of the variables into our model might be useful
impx <- mice(NHANES17s, seed = 504, m=1, print=FALSE)
completedata <- mice::complete(impx)
set.seed(504)
votes <-c()
formula0 <- as.formula("diastolicBP ~ age + bmi + cholesterol")
scope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)
for (i in 1:200){
ind <- sample(1:nrow(completedata),replace = TRUE)
newdata <- completedata[ind,]
full.model <- glm(formula0, data = newdata)
f2 <- MASS::stepAIC(full.model,
scope = scope, trace = FALSE)
formulas <- as.formula(f2)
temp <- unlist(labels(terms(formulas)))
votes <- c(votes,temp)
}
table(votes)
## among 200 bootstrap samples how many times that each
## variable appears in the final model. Models have different
## variables are attributed to sampling variation
## Recall the imputation we have done before
imputation5 <- mice(NHANES17s, seed = 504,
m=10,
maxit = 5,
print=FALSE)
plot(imputation5)
## We hope to see no pattern in the trace lines
## Sometimes to comfirm this we may want to run with more iterations
imputation5_2 <- mice(NHANES17s, seed = 504,
m=10,
maxit = 50,
print=FALSE)
plot(imputation5_2)
## We could compare the imputed and observed data using Density plots
densityplot(imputation5, layout = c(2, 2))
imputation5_3 <- mice(NHANES17s, seed = 504,
m=50,
maxit = 50,
print=FALSE)
densityplot(imputation5_3)
## a subjective judgment on whether you think if there is significant discrepancy
bwplot(imputation5, age + bmi + cholesterol +diastolicBP ~ .imp, layout = c(2, 2))
bwplot(imputation5_3)
## Plot a box plot to compare the imputed and observed values
est1
fit4
mice::pool(fit4)
install.packages("mice")
install.packages("mice")
require("mice")
fit4
summary(fit4)
mice::pool(fit4)
est1
# Load required packages
library(mice)
library(DataExplorer)
library(VIM)
library(mitools)
# Step 3 pool the analysis results
est1 <- mice::pool(fit4)
## pool all estimated together using Rubin's rule
est1
fit4
mice::pool(fit4)
est1
edit(est1)
require(xtable)
xtable(est1)
est1
install.packages("factoextra")
install.packages("pak")
pak::pak("r-lib/vctrs")
remotes::install_github("tidyverse/purrr")
remotes::install_github("tidyverse/purrr")
install.packages("rticles")
rticles::journals()
install.packages("svyVGAM")
install.packages("survminer")
install.packages("dplyr")
citation("dplyr")
install.packages("gtsummary")
install.packages("xfun")
ObsData <- readRDS(file =
"Data/machinelearningCausal/rhcAnalyticTest.RDS")
baselinevars <- names(dplyr::select(ObsData,
!c(RHC.use,Length.of.Stay,Death)))
head(ObsData)
tab0 <- CreateTableOne(vars = c("age", "sex", "race",
"Disease.category", "Cancer"),
data = ObsData,
strata = "RHC.use",
test = FALSE)
require(tableone)
require(Publish)
require(randomForest)
tab0 <- CreateTableOne(vars = c("age", "sex", "race",
"Disease.category", "Cancer"),
data = ObsData,
strata = "RHC.use",
test = FALSE)
print(tab0, showAllLevels = FALSE)
form1 <- readRDS("E:/GitHub/EpiMethods/Data/form1.RDS")
form1
require(tableone)
require(Publish)
require(randomForest)
load(file = "Data/machinelearningCausal/cl2.RData")
require(tableone)
require(Publish)
require(randomForest)
load(file = "Data/machinelearningCausal/cl2.RData")
tab1 <- CreateTableOne(vars = c("Death"),
data = ObsData,
strata = "RHC.use",
test = FALSE)
print(tab1, showAllLevels = FALSE, )
n <- nrow(ObsData)
p <- nrow(ObsData[ObsData$Death == 1,])/n
n_eff <- min(n, 5*(n*min(p, 1-p)))
n_eff
p_exp <- nrow(ObsData[ObsData$RHC.use == 1,])/n
n_eff_exp <- min(n, 5*(n*min(p, 1-p)))
n_eff_exp
length(c(baselinevars, "Length.of.Stay"))
# Construct the SuperLearner library
SL.library <- c("SL.mean",
"SL.glm",
"SL.glmnet",
"SL.xgboost",
"SL.randomForest",
"tmle.SL.dbarts2",
"SL.svm")
# install.packages(c('tmle', 'xgboost'))
require(tmle)
require(xgboost)
ObsData.noYA <- dplyr::select(ObsData,
!c(Death, RHC.use))
ObsData$Death <- as.numeric(ObsData$Death)
tmle.fit <- tmle::tmle(Y = ObsData$Death,
A = ObsData$RHC.use,
W = ObsData.noYA,
family = "binomial",
V = 10,
Q.SL.library = SL.library,
g.SL.library = SL.library)
tmle.est.bin <- readRDS("Data/machinelearningCausal/tmlebin.RDS")
tmle.ci.bin <- readRDS("Data/machinelearningCausal/tmlebinci.RDS")
cat("ATE for binary outcome using user-specified library: ", round(tmle.est.bin, 2), tmle.ci.bin, sep = "   ")
tmle.est.bin <- readRDS("Data/machinelearningCausal/tmlebin.RDS")
tmle.ci.bin <- readRDS("Data/machinelearningCausal/tmlebinci.RDS")
cat("ATE for binary outcome using user-specified library: ", round(tmle.est.bin, 2), tmle.ci.bin, sep = "   ")
?SuperLearner
?tmle.SL.dbarts2
data <- data.frame(
"Function name" = c("CreateTableOne", "tmle", "publish", "median", "listWrappers", "SL.mean", "SL.glm", "SL.glmnet", "SL.xgboost", "SL.randomForest", "tmle.SL.dbarts2", "SL.svm"),
"Package name" = c("tableone", "tmle", "Publish", "stats", "SuperLearner", "SuperLearner", "SuperLearner", "SuperLearner", "SuperLearner", "SuperLearner", "SuperLearner", "SuperLearner"),
"Use" = c("To create a summary table for a dataset: CreateTableOne(vars, data, strata, test)",
"To run Targeted Maximum Likelihood Estimation (TMLE) for causal inference: tmle(Y, A, W, family, V, Q.SL.library, g.SL.library)",
"To publish regression models: publish(fit, digits)",
"To calculate the median of a numeric vector: median(x)",
"To see the list of wrapper functions, i.e., list of learners, in SuperLearner",
"SuperLearner wrapper for the mean learner",
"SuperLearner wrapper for the generalized linear model learner",
"SuperLearner wrapper for the generalized linear model with elastic net penalty learner",
"SuperLearner wrapper for the extreme gradient boosting learner",
"SuperLearner wrapper for the random forest learner",
"SuperLearner wrapper for the Bayesian Additive Regression Trees (BART) learner",
"SuperLearner wrapper for the support vector machine (SVM) learner")
)
data <- data[order(data$`Package name`),]
data <- data[order(data$"Package name"),]
data <- data[order(data[["Package name"]]),]
data <- data[order(data[[2]]),]
data
install.packages("webshot")
install.packages("webshot2")
require(mice)
require(remotes)
require(simcausal)
require(naniar)
require(misty)
install.packages("mice")
install.packages("remotes")
install.packages("simcausal")
install.packages("naniar")
install.packages("misty")
install.packages("mice")
# Load required packages
require(mice)
require(mitools)
require(survey)
require(remotes)
require(simcausal)
require(simcausal)
D <- DAG.empty()
D <- D +
node("B", distr = "rnorm", mean = 0, sd = 1) +
node("P", distr = "rnorm", mean = 0, sd = .7) +
node("L", distr = "rnorm", mean = 2 + 2 * P + 3 * B, sd = 3) +
node("A", distr = "rnorm", mean = 0.5 + L + 2 * P, sd = 1) +
node("Y", distr = "rnorm", mean = 1.1 * L + 1.3 * A + B + 2 * P, sd = .5)
Dset <- set.DAG(D)
plotDAG(Dset, xjitter = 0.1, yjitter = .9,
edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),
vertex_attrs = list(size = 12, label.cex = 0.8))
Obs.Data <- sim(DAG = Dset, n = 10000, rndseed = 123)
head(Obs.Data)
Obs.Data.original <- Obs.Data
set.seed(123)
Obs.Data$L[sample(1:length(Obs.Data$L), size = 1000)] <- NA
summary(Obs.Data$L)
require(VIM)
res <- aggr(Obs.Data, plot = FALSE)
plot(res, numbers = TRUE, prop = FALSE)
marginplot(Obs.Data[,c("L", "P")])
marginplot(Obs.Data[,c("L", "B")])
marginplot(Obs.Data[,c("L", "A")])
marginplot(Obs.Data[,c("L", "Y")])
#library(devtools)
#install_github("cran/MissMech")
library(MissMech)
test.result <- TestMCARNormality(data = Obs.Data)
test.result
summary(test.result)
boxplot(test.result)
boxplot(test.result)
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png")
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png")
boxplot(test.result)
dev.off()
?ggsave
ggplot2::ggsave("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png")
boxplot(test.result)
dev.off()
boxplot(test.result)
ggplot2::ggsave("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png")
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png")
boxplot(test.result)
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png", width = 800, height = 600)
boxplot(test.result)
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot.png", width = 600, height = 600)
boxplot(test.result)
dev.off()
marginplot(Obs.Data[,c("L", "P")])
png("E:/GitHub/EpiMethods/Images/missingdata/lp.png", width = 600, height = 600)
marginplot(Obs.Data[,c("L", "P")])
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/lb.png", width = 600, height = 600)
marginplot(Obs.Data[,c("L", "B")])
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/la.png", width = 600, height = 600)
marginplot(Obs.Data[,c("L", "A")])
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/ly.png", width = 600, height = 600)
marginplot(Obs.Data[,c("L", "Y")])
dev.off()
png("E:/GitHub/EpiMethods/Images/missingdata/boxplot1.png", width = 600, height = 600)
boxplot(test.result)
dev.off()
install.packages("Matching")
install.packages("svyVGAM")
install.packages("survminer")
install.packages("sjstats")
install.packages("HSAUR2")
install.packages("gee")
install.packages("MuMIn")
install.packages("geepack")
install.packages("afex")
install.packages("jtools")
sessionInfo()
require(jtools)
install.packages("cli")
require(jtools)
install.packages("fansi")
require(jtools)
# Load required packages
knitr::opts_chunk$set(echo = TRUE)
require(kableExtra)
require(Matrix)
require(jtools)
library(HSAUR2)
data("BtheB")
kable(head(BtheB))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
## Box-plot of responses at different time points in treatment and control groups
data("BtheB", package = "HSAUR2")
layout(matrix(1:2, nrow = 1))
ylim <- range(BtheB[,grep("bdi", names(BtheB))],na.rm = TRUE)
tau <- subset(BtheB, treatment == "TAU")[, grep("bdi", names(BtheB))]
boxplot(tau, main = "Treated as Usual", ylab = "BDI",xlab = "Time (in months)",
names = c(0, 2, 3, 5, 8),ylim = ylim)
btheb <- subset(BtheB, treatment == "BtheB")[, grep("bdi", names(BtheB))]
boxplot(btheb, main = "Beat the Blues", ylab = "BDI",xlab = "Time (in months)",
names = c(0, 2, 3, 5, 8),ylim = ylim)
## To analyze the data, we first need to convert the dataset to a analysis-ready form
BtheB$subject <- factor(rownames(BtheB))
nobs <- nrow(BtheB)
BtheB_long <- reshape(BtheB, idvar = "subject", varying = c("bdi.2m", "bdi.3m", "bdi.5m", "bdi.8m"),
direction = "long")
BtheB_long$time <- rep(c(2, 3, 5, 8), rep(nobs, 4))
kable(head(BtheB_long))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
unique(BtheB_long$subject)
kable(subset(BtheB_long,  subject == 2))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
kable(subset(BtheB_long,  subject == 99))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
lmfit <- lm(bdi ~ bdi.pre + time + treatment + drug +length, data = BtheB_long,
na.action = na.omit)
require(jtools)
summ(lmfit)
## Fit a random intercept model with lme4 package
library("lme4")
BtheB_lmer1 <- lmer(bdi ~ bdi.pre + time + treatment + drug +length
+ (1 | subject), data = BtheB_long, REML = FALSE,
na.action = na.omit)
install.packages("Matrix", dependencies = TRUE)
install.packages("Matrix", dependencies = TRUE)
# Load required packages
knitr::opts_chunk$set(echo = TRUE)
require(kableExtra)
require(Matrix)
require(jtools)
library(HSAUR2)
data("BtheB")
kable(head(BtheB))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
## Box-plot of responses at different time points in treatment and control groups
data("BtheB", package = "HSAUR2")
layout(matrix(1:2, nrow = 1))
ylim <- range(BtheB[,grep("bdi", names(BtheB))],na.rm = TRUE)
tau <- subset(BtheB, treatment == "TAU")[, grep("bdi", names(BtheB))]
boxplot(tau, main = "Treated as Usual", ylab = "BDI",xlab = "Time (in months)",
names = c(0, 2, 3, 5, 8),ylim = ylim)
btheb <- subset(BtheB, treatment == "BtheB")[, grep("bdi", names(BtheB))]
boxplot(btheb, main = "Beat the Blues", ylab = "BDI",xlab = "Time (in months)",
names = c(0, 2, 3, 5, 8),ylim = ylim)
## To analyze the data, we first need to convert the dataset to a analysis-ready form
BtheB$subject <- factor(rownames(BtheB))
nobs <- nrow(BtheB)
BtheB_long <- reshape(BtheB, idvar = "subject", varying = c("bdi.2m", "bdi.3m", "bdi.5m", "bdi.8m"),
direction = "long")
BtheB_long$time <- rep(c(2, 3, 5, 8), rep(nobs, 4))
kable(head(BtheB_long))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
unique(BtheB_long$subject)
kable(subset(BtheB_long,  subject == 2))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
kable(subset(BtheB_long,  subject == 99))%>%
kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
lmfit <- lm(bdi ~ bdi.pre + time + treatment + drug +length, data = BtheB_long,
na.action = na.omit)
require(jtools)
summ(lmfit)
## Fit a random intercept model with lme4 package
library("lme4")
BtheB_lmer1 <- lmer(bdi ~ bdi.pre + time + treatment + drug +length
+ (1 | subject), data = BtheB_long, REML = FALSE,
na.action = na.omit)
install.packages("lme4", type = "source")
install.packages("lme4", type = "source")
install.packages("htmlwidgets")
install.packages("cowplot")
install.packages("sjstats")
install.packages("emmeans")
# Load required packages
require(Publish)
require(survey)
require(svyVGAM)
require(car)
library(knitr)
# Function to exclude invalid responses
invalid.exclude <- function(Data, Var){
subset.data <- subset(Data, eval(parse(text = Var)) != "DON'T KNOW" &
eval(parse(text = Var)) != "REFUSAL" &
eval(parse(text = Var)) != "NOT STATED")
x1 <- dim(Data)[1]
x2 <- dim(subset.data)[1]
cat( format(x1-x2, big.mark = ","),
"subjects deleted, and current N =" , format(x2, big.mark = ",") , "\n")
return(subset.data)
}
# Load required packages
require(knitr)
require(Publish)
require(survey)
library(broom)
library(tidyverse)
require(survminer)
library(survival)
head(lung)
kable(head(lung))
# kable(lung)
require(car)
require(survey)
analytic <- readRDS("Data/nonbinary/cmh.Rds")
str(analytic)
install.packages("kableExtra")
install.packages("DT")
# Load required packages
library(learnr)
library(xfun)
install.packages("shiny")
library(shiny); runApp('ShinyApp/wrangling1.R')
runApp('ShinyApp/wrangling')
# Define the paths
app_path <- paste0(getwd(), "/ShinyApp/wrangling/")
output_path <- paste0(getwd(), "/ShinyApp/wrangling/out")
# Use the paths in the export function
shinylive::export(app_path, output_path)
#httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::httd("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::daemon_stop(1)
#httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::httd("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
#httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::httd("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
getwd()
# Define the paths
app_path <- paste0(getwd(), "/ShinyApp/wrangling/")
output_path <- paste0(getwd(), "/ShinyApp/wrangling/out")
# Use the paths in the export function
shinylive::export(app_path, output_path)
#httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::httd("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
pak::pak("rstudio/httpuv")
httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
#httpuv::runStaticServer("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
servr::httd("E:/GitHub/EpiMethods/ShinyApp/wrangling/out")
getwd()
pak::pak("posit-dev/r-shinylive")
install.packages("janitor")
require(skimr)
require(corrplot)
require(visdat)
require(naniar)
