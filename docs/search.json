[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Epidemiological Methods",
    "section": "",
    "text": "The Project\nWelcome to a website designed to bridge a unique gap in the health research world. This platform specifically targets the complex intersection of health research and advanced statistics; a niche often perceived as challenging by many newcomers. Whether you’re taking your first steps into health research or you’re grappling with the intricacies of advanced statistical methods, you are in the right place. Here, we offer:\nThis hub is a part of an open educational initiative, meaning it is available to everyone. We hope to raise the standard of health research methodology through this endeavor."
  },
  {
    "objectID": "index.html#what-we-aim-to-achieve",
    "href": "index.html#what-we-aim-to-achieve",
    "title": "Advanced Epidemiological Methods",
    "section": "What We Aim to Achieve",
    "text": "What We Aim to Achieve\nWe are on a mission to:\n\nEquip public health learners with hands-on experience.\nTeach the nuances of applying advanced epidemiological methods using real data.\nOffer a unique open textbook that’s enriched with interactive tools and quizzes for a self-paced learning experience."
  },
  {
    "objectID": "index.html#dive-into-our-modules",
    "href": "index.html#dive-into-our-modules",
    "title": "Advanced Epidemiological Methods",
    "section": "Dive into Our Modules",
    "text": "Dive into Our Modules\nEmbark on a journey through\n\n1 introductory module about R (indicator W),\n9 core learning modules (letters in the parentheses: A, Q, R, P, D, M, S, L, C are the chapter indicators), and\n4 bonus modules, with N, T, I, G being bonus chapter indicators.\n\nIndicators are listed along with quizzes, R functions, and exercises associated with the corresponding chapters. Only key chapters have exercises (W, A, D, M, S).\n\n\n\n\n\n Module \n    Topics.Indicators \n    Descriptions \n  \n\n\n 1 \n    R for Data Wrangling (W) \n    Get to know R. \n  \n\n 2 \n    Accessing (A) Survey Data Resources \n    Understand and source reliable national survey data. \n  \n\n 3 \n    Crafting Analytic Data for Research Questions (Q) \n    Customize data to your research query. \n  \n\n 4 \n    Causal Roles (R) \n    Delve into the concept of confounding and its implications. \n  \n\n 5 \n    Predictive (P) modeling \n    Introduction to key concepts of prediction modelling. \n  \n\n 6 \n    Complex Survey Data (D) Analysis \n    Handle data sets obtained from complex survey designs. \n  \n\n 7 \n    Missing (M) Data Analysis \n    Understand and tackle missingness in your data. \n  \n\n 8 \n    Propensity Score (S) Analysis \n    Dive deeper into advanced observational data analyses. \n  \n\n 9 \n    Machine Learning (L) \n    Introduction to machine learning algorithms, and applications. \n  \n\n 10 \n    Intergrating Machine Learners in Causal (C) Inference \n    Discusses the potential pitfalls and challenges in merging machine learning with causal inference, and a way forward. \n  \n\n 11 \n    Non-binary Outcomes (N) \n    Statistical techniques to deal with complex or non-binary outcomes \n  \n\n 12 \n    Longitudinal Analysis (T) \n    Longitudinal data analysis techniques \n  \n\n 13 \n    Mediation Analysis (I) \n    Mediation: decomposing the total effect \n  \n\n 14 \n    Scientific Writing Tools (G) \n    Tools and guides for scientific writing and collaboration. \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tutorial is designed with a consistent structure across all chapters to provide a cohesive and thorough learning experience. Here is what you can expect in each chapter:\n\nOverview: The first page of each chapter offers a concise summary that outlines the key learning objectives, topics covered, and what you can expect to gain from the chapter. The overview page will also feature links to the data sources used in the tutorials as well as a form where you can report any bugs or issues you encounter. This helps you quickly grasp the chapter’s essence and set learning expectations.\nConcepts: Selected core chapters will include a concept page, where materials (e.g., slides, video lessons, additional FAQs where available) will be included. All of the videos linked here (for lessons or labs) are hosted in YouTube, where users can automatically generate subtitles and captions.\nTutorial topics: Immediately following the overview/concepts, you will find in-depth tutorials that cover each topic in detail. These are designed to provide comprehensive insights and are spread across multiple pages for easier navigation and understanding.\nSummary of R functions: Each chapter includes a succinct summary of the R functions used in the tutorials. This serves as a quick reference guide for learners to understand the tools they will be applying.\nChapter-specific quiz: For those interested in self-assessment, each chapter concludes with an optional quiz. This is a self-paced learning tool to help reinforce the chapter’s key concepts.\nWeb-App: A few chapters include shiny apps. Users can work with these apps directly from this website, or will have the option to download and run the app locally.\nPractice exercises: Finally, practice exercises are available for selected chapters to help you apply what you have learned in a hands-on manner. These exercises are designed to reinforce your understanding and give you practical experience with the chapter’s topics. Some of these practice exercises may be used in future versions of the course, so you may see references to submitting assignments or the points value of a question in an assignment."
  },
  {
    "objectID": "index.html#how-our-content-is-presented",
    "href": "index.html#how-our-content-is-presented",
    "title": "Advanced Epidemiological Methods",
    "section": "How Our Content is Presented",
    "text": "How Our Content is Presented\nAll our resources are hosted on an easy-to-access GitHub page. The format? Engaging text, reproducible software codes, clear analysis outputs, and crisp videos that distill complex topics. And do not miss our quiz section at the end of each module for a quick self-check on what you have learned. This document is created using quarto and R.\nThe content was primarily designed for a website in HTML format; however, a PDF version based on the website has also been created. Although the formatting is not perfect for this converted PDF, this PDF can be downloaded from here and used for offline reading."
  },
  {
    "objectID": "index.html#open-copyright-license",
    "href": "index.html#open-copyright-license",
    "title": "Advanced Epidemiological Methods",
    "section": "Open Copyright License",
    "text": "Open Copyright License\nCC-BY 4.0"
  },
  {
    "objectID": "index.html#grant-applicants",
    "href": "index.html#grant-applicants",
    "title": "Advanced Epidemiological Methods",
    "section": "Grant Applicants",
    "text": "Grant Applicants\nDive into this captivating content, brought to life with the generous support of the UBC OER Fund Implementation Grant and further supported by UBC SPPH. The foundation of this content traces back to the PI’s work over five years while instructing SPPH 604 (2018-2022). That knowledge has now been transformed into an open educational resource, thanks to this grant. Meet the innovative minds behind the grant proposal below.\n\n\n\n\n\n Role \n    Team_Member \n    Affiliation \n  \n\n\n Principal Applicant (PI) \n    Dr. M Ehsan Karim \n    UBC School of Population and Public Health \n  \n\n Co-applicant (Co-I) \n    Dr. Suborna Ahmed \n    UBC Department of Forest Resources Management \n  \n\n Trainee co-applicants \n    Md Belal Hossain \n    UBC School of Population and Public Health \n  \n\n  \n    Fardowsa Yusuf \n    UBC School of Population and Public Health \n  \n\n  \n    Hanna Frank \n    UBC School of Population and Public Health \n  \n\n  \n    Dr. Michael Asamoah-Boaheng \n    UBC Department of Emergency Medicine \n  \n\n  \n    Chuyi (Astra) Zheng \n    UBC Faculty of Arts \n  \n\n\n\n\nA presentation about the output of this grant in the OER Project Virtual Showcase and Poster Session (March 7, 2024):\n\nToggle Show/Hide"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced Epidemiological Methods",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe also want to acknowledge earlier contributors to the course material development, who were not part of this current OER grant, include Derek Ouyang, Kate McLeod (both from UBC School of Population and Public Health), and Mohammad Atiquzzaman (UBC Pharmaceutical Sciences). Numerous pieces of student feedback were also incorporated in order to update the content."
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Advanced Epidemiological Methods",
    "section": "How to Cite",
    "text": "How to Cite\n\n\n\n\n\n\n\n Style \n    Citation \n  \n\n\n APA \n    Karim M. E., Epi-OER team (  2024 ).  Advanced Epidemiological Methods . Retrieved from  https://ehsanx.github.io/EpiMethods/  on  April 20, 2024 . \n  \n\n MLA \n    Karim M. E., Epi-OER team . \" Advanced Epidemiological Methods .\" Web.  April 20, 2024  < https://ehsanx.github.io/EpiMethods/ >. \n  \n\n Chicago \n    Karim M. E., Epi-OER team .  \" Advanced Epidemiological Methods .\"   2024 . Web.  April 20, 2024  < https://ehsanx.github.io/EpiMethods/ >. \n  \n\n Harvard \n    Karim M. E., Epi-OER team  (  2024 ) ' Advanced Epidemiological Methods '. Available at:  https://ehsanx.github.io/EpiMethods/  (Accessed:  April 20, 2024 ). \n  \n\n Vancouver \n    Karim M. E., Epi-OER team .  Advanced Epidemiological Methods .   2024 . [Online]. Available at:  https://ehsanx.github.io/EpiMethods/  (Accessed  April 20, 2024 ). \n  \n\n IEEE \n    Karim M. E., Epi-OER team , \" Advanced Epidemiological Methods ,\"   2024 , [Online]. Available:  https://ehsanx.github.io/EpiMethods/ . Accessed on:  April 20, 2024 . \n  \n\n AMA \n    Karim M. E., Epi-OER team Advanced Epidemiological Methods .   2024 . [Online]. Available at:  https://ehsanx.github.io/EpiMethods/  (Accessed  April 20, 2024 ). \n  \n\n\n\n\n\nEpi-OER team: Hossain MB, Frank HA, Yusuf FL, Ahmed SS, Asamoah-Boaheng M, Zheng C (team is listed in order of contribution to the creation of this book)\n\nThe BibTex format can be downloaded from here."
  },
  {
    "objectID": "wrangling.html#background",
    "href": "wrangling.html#background",
    "title": "Data wrangling",
    "section": "Background",
    "text": "Background\nThe realm of data science is vast, and one of its foundational pillars is data wrangling. Data wrangling, often known as data munging, is the process of transforming raw data into a more digestible and usable format for analysis. In the context of R, a powerful statistical programming language, data wrangling becomes an essential skill for any data enthusiast. This chapter is dedicated to imparting practical knowledge on various data manipulation, import, and summarization techniques in R. Through a series of meticulously crafted tutorials, you will be equipped with the tools and techniques to handle, transform, and visualize data efficiently.\n\n\nIntroducing R Basics at the outset of an epidemiological methods tutorial book is akin to laying the foundation before building a house. It ensures that all readers, regardless of their prior experience, start on the same page, understanding the fundamental tools and language of R. This foundational knowledge not only smoothens the learning curve but also boosts confidence, allowing learners to focus on complex epidemiological techniques without being bogged down by the intricacies of the R language. In essence, mastering the basics first ensures a more cohesive and effective learning experience as the material advances.\nIn this chapter, we embark on a structured journey through the intricate world of data wrangling in R. We begin by laying a solid foundation with R Basics, ensuring you grasp the essential elements of R programming. Once grounded in the basics, we progress to understanding the core R Data Types, diving deep into matrices, lists, and data frames. With a firm grasp of these structures, we introduce Automating Tasks to empower you with techniques that streamline the handling of vast datasets. Following this, we delve into the practical aspects of Importing Datasets, showcasing various methods to bring data from different formats into R. Building on this, Data Manipulation comes next, where we explore the myriad ways to modify and reshape your datasets to suit analytical needs. We then turn our attention to Importing External Data, offering a hands-on demonstration of how to integrate specific external datasets into your R environment. As we approach the chapter’s culmination, we emphasize the importance of Summary Tables in medical research, teaching you the art and science of data summarization. Finally, we wrap up with R Markdown, providing a comprehensive guide on how to seamlessly document your R code and analytical findings, ensuring your work is both reproducible and presentable.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "wrangling.html#overview-of-tutorials",
    "href": "wrangling.html#overview-of-tutorials",
    "title": "Data wrangling",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nR Basics\nThis tutorial introduces the basics of R programming. It covers topics such as setting up R and RStudio, using R as a calculator, creating variables, working with vectors, plotting data, and accessing help resources.\n\n\nR Data Types\nThis tutorial covers three primary data structures in R: matrices, lists, and data frames. Matrices are two-dimensional arrays with elements of the same type, and their manipulation includes reshaping and combining. Lists in R are versatile collections that can store various R objects, including matrices. Data frames, on the other hand, are akin to matrices but permit columns of diverse data types. The tutorial offers guidance on creating, modifying, and merging data frames and checking their dimensions.\n\n\nAutomating Tasks\nMedical data analysis often grapples with vast and intricate data sets. Manual handling isn’t just tedious; it’s error-prone, especially given the critical decisions hinging on the results. This tutorial introduces automation techniques in R, a leading language for statistical analysis. By learning to use loops and functions, you can automate repetitive tasks, minimize errors, and conduct analyses more efficiently. Dive in to enhance your data handling skills.\n\n\nImporting Dataset from the Local Computer\nThis tutorial focuses on importing data into R. It demonstrates how to import data from CSV and SAS formats using functions like read.csv and sasxport.get. It also includes examples of loading specific variables, dropping variables, subsetting observations based on certain criteria, and handling missing values.\n\n\nData Manipulation\nThis tutorial explores various data manipulation techniques in R. It covers topics such as dropping variables from a dataset, keeping specific variables, subsetting observations based on specific criteria, converting variable types (e.g., factors, strings), and handling missing values.\n\n\nImport External Data Over the Internet\nThis tutorial provides examples of importing external data into R. It includes specific examples of importing a CSV file (Employee Salaries - 2017 data) and a SAS file (NHANES 2015-2016 data). It also demonstrates how to save a working dataset in different formats, such as CSV and RData.\n\n\nSummary Tables\nThis tutorial emphasizes the importance of data summarization in medical research and epidemiology, specifically how to summarize medical data using R. It demonstrates creating “Table 1”, a typical descriptive statistics table in research papers, with examples that use the built-in R functions and specialized packages to efficiently summarize and stratify data.\n\n\nR Markdown\nThis beginner-friendly tutorial guides you through working with R Markdown (RMD) files in RStudio, a popular IDE for R. The tutorial covers installing prerequisites, creating a new RMD file, and the basics of “knitting” to compile the document into various formats like HTML, PDF, or Word. It delves into embedding R code chunks and plain text within the RMD file, using the knitr package for document rendering. Tips for troubleshooting common issues and additional resources for further learning are also provided.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "wrangling1a.html",
    "href": "wrangling1a.html",
    "title": "R basics",
    "section": "",
    "text": "Start using R\nTo get started with R, follow these steps:\n\nDownload and Install R: Grab the newest version from the official R website. > Tip: Download from a Comprehensive R Archive Network (CRAN) server near your geographic location.\nDownload and Install RStudio: You can get it from this link. > Note: RStudio serves as an Integrated Development Environment (IDE) offering a user-friendly interface. It facilitates operations such as executing R commands, preserving scripts, inspecting results, managing data, and more.\nBegin with RStudio: Once you open RStudio, delve into using R. For starters, employ the R syntax for script preservation, allowing future code adjustments and additions.\nBasic syntax\n\n\n\n\n\n\nTip\n\n\n\nR, a versatile programming language for statistics and data analysis, can execute numerous tasks. Let’s break down some of the fundamental aspects of R’s syntax.\n\n\n\nUsing R as a Calculator\n\nSimilar to how you’d use a traditional calculator for basic arithmetic operations, R can perform these functions with ease. For instance:\n\n# Simple arithmetic\n1 + 1\n#> [1] 2\n\nThis is a basic addition, resulting in 2.\nA more intricate calculation:\n\n# Complex calculation involving \n# multiplication, subtraction, division, powers, and square root\n20 * 5 - 10 * (3/4) * (2^3) + sqrt(25)\n#> [1] 45\n\nThis demonstrates R’s capability to handle complex arithmetic operations.\n\nVariable Assignment in R\n\nR allows you to store values in variables, acting like labeled containers that can be recalled and manipulated later. For example,\n\n# Assigning a value of 2 to variable x1\nx1 <- 2\nprint(x1)\n#> [1] 2\n\nSimilarly:\n\nx2 <- 9\nx2\n#> [1] 9\n\n\nCreating New Variables Using Existing Ones\n\nYou can combine and manipulate previously assigned variables to create new ones.\n\n# Using variable x1 \n# to compute its square and assign to y1\ny1 <- x1^2\ny1\n#> [1] 4\n\nYou can also use multiple variables in a single expression:\n\ny2 <- 310 - x1 + 2*x2 - 5*y1^3\ny2\n#> [1] 6\n\n\nCreating Functions\n\nFunctions act as reusable blocks of code. Once defined, they can be called multiple times with different arguments. Here’s how to define a function that squares a number:\n\nz <- function(x) {x^2}\n\nCall the function\n\nz(x = 0.5)\n#> [1] 0.25\nz(x = 2)\n#> [1] 4\n\nR also comes with a plethora of built-in functions. Examples include exp (exponential function) and rnorm (random number generation from a normal distribution).\n\nUtilizing Built-In Functions\n\nFor instance, using the exponential function:\n\n# Calling functions\nexp(x1)\n#> [1] 7.389056\nlog(exp(x1))\n#> [1] 2\n\nThe rnorm function can generate random samples from a normal distribution: below we are generating 10 random sampling from the normal distribution with mean 0 and standard deviation 1:\n\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1]  0.4099246 -1.2598025 -0.8645756 -0.6440206 -1.6115908 -1.7678979\n#>  [7] -0.1551297  0.1208372  0.4685043  1.0896852\n\nAs random number generation relies on algorithms, results will differ with each execution.\n\n# Random sampling (again)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -1.5526619 -0.3321571 -0.6226656  0.6621347 -0.2998815 -0.3847619\n#>  [7]  1.9435499  0.2719384 -0.2506405 -0.6404346\n\nHowever, by setting a seed, we can reproduce identical random results:\n\n# Random sampling (again, but with a seed)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\n# random sampling (reproducing the same numbers)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\nAs we can see, when we set the same seed, we get exactly the same random number. This is very important for reproducing the same results. There are many other pre-existing functions in R.\n\nSeeking Help in R\n\n\n\n\n\n\n\nTip\n\n\n\nR’s help function, invoked with ?function_name, provides detailed documentation on functions, assisting users with unclear or forgotten arguments:\n\n\n\n# Searching for help if you know \n# the exact name of the function with a question mark\n?curve\n\nBelow is an example of using the pre-exiting function for plotting a curve ranging from -10 to 10.\n\n# Plotting a function\ncurve(z, from = -10, to = 10, xlab = \"x\", ylab = \"Squared x\")\n\n\n\n\nIf some of the arguments are difficult to remember or what else could be done with that function, we could use the help function. For example, we can simply type help(curve) or ?curve to get help on the curve function:\n\n\n\n\n\n\nTip\n\n\n\nIf you’re uncertain about a function’s precise name, two question marks can assist in the search:\n\n\n\n# Searching for help if don't know \n# the exact name of the function\n??boxplot\n\n\nCreating Vectors\n\nVectors are sequences of data elements of the same basic type. Here are some methods to create them:\n\n# Creating vectors in different ways\nx3 <- c(1, 2, 3, 4, 5)\nprint(x3)\n#> [1] 1 2 3 4 5\n\nx4 <- 1:7\nprint(x4)\n#> [1] 1 2 3 4 5 6 7\n\nx5 <- seq(from = 0, to = 100, by = 10)\nprint(x5)\n#>  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nx6 <- seq(10, 30, length = 7)\nx6\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n\n\nPlotting in R\n\nR provides numerous plotting capabilities. For instance, the plot function can create scatter plots and line graphs:\n\n# Scatter plot\nplot(x5, type = \"p\", main = \"Scatter plot\")\n\n\n\n\n\n# Line graph\nplot(x = x6, y = x6^2, type = \"l\", main = \"Line graph\")\n\n\n\n\n\nCharacter Vectors Apart from numeric values, R also allows for character vectors. For example, we can create a sex variable coded as females, males and other.\n\n\n# Character vector\nsex <- c(\"females\", \"males\", \"other\")\nsex\n#> [1] \"females\" \"males\"   \"other\"\n\nTo determine a variable’s type, use the mode function:\n\n# Check data type\nmode(sex)\n#> [1] \"character\"\n\nPackage Management\nPackages in R are collections of functions and datasets developed by the community. They enhance the capability of R by adding new functions for data analysis, visualization, data import, and more. Understanding how to install and load packages is essential for effective R programming.\n\nInstalling Packages from CRAN\n\nThe CRAN is a major source of R packages. You can install them directly from within R using the install.packages() function.\n\n# Installing the 'ggplot2' package\ninstall.packages(\"ggplot2\")\n\n\nLoading a Package\n\nAfter a package is installed, it must be loaded to use its functions. This is done with the library() function.\n\n# Loading the 'ggplot2' package\nlibrary(ggplot2)\n\nYou only need to install a package once, but you’ll need to load it every time you start a new R session and want to use its functions.\n\nUpdating Packages\n\nR packages are frequently updated. To ensure you have the latest version of a package, use the update.packages() function.\n\n# Updating all installed packages\n# could be time consuming!\nupdate.packages(ask = FALSE)  \n# 'ask = FALSE' updates all without asking for confirmation\n\n\nListing Installed Packages\n\nYou can view all the installed packages on your R setup using the installed.packages() function.\n\n# Listing installed packages\ninstalled.packages()[, \"Package\"]\n\n\nRemoving a Package\n\nIf you no longer need a package, it can be removed using the remove.packages() function.\n\n# Removing the 'ggplot2' package\nremove.packages(\"ggplot2\")\n\n\nInstalling Packages from Other Sources\n\nWhile CRAN is the primary source, sometimes you might need to install packages from GitHub or other repositories. The devtools package provides a function for this.\n\n# Installing devtools first\ninstall.packages(\"devtools\")\n# Loading devtools\nlibrary(devtools)\n# Install a package from GitHub\n# https://github.com/ehsanx/simMSM\ninstall_github(\"ehsanx/simMSM\")\n\nWhen you are working on a project, it’s a good practice to list and install required packages at the beginning of your R script.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling1b.html",
    "href": "wrangling1b.html",
    "title": "Data types",
    "section": "",
    "text": "Matrix\n\n\n\n\n\n\nTip\n\n\n\nIn R, matrices are two-dimensional rectangular data sets, which can be created using the matrix() function. It’s essential to remember that all the elements of a matrix must be of the same type, such as all numeric or all character.\n\n\nTo construct a matrix, we often start with a vector and specify how we want to reshape it. For instance:\n\n# Matrix 1\nx <- 1:10\nmatrix1 <- matrix(x, nrow = 5, ncol = 2, byrow = TRUE)\nmatrix1\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\nHere, the vector x contains numbers from 1 to 10. We reshape it into a matrix with 5 rows and 2 columns. The byrow = TRUE argument means the matrix will be filled row-wise, with numbers from the vector.\nConversely, if you want the matrix to be filled column-wise, you’d set byrow = FALSE:\n\n# matrix 2\nmatrix2 <- matrix(x, nrow = 5, ncol = 2, byrow = FALSE)\nmatrix2\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\nYou can also combine or concatenate matrices. cbind() joins matrices by columns while rbind() joins them by rows.\n\n# Merging 2 matrices\ncbind(matrix1, matrix2)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    1    6\n#> [2,]    3    4    2    7\n#> [3,]    5    6    3    8\n#> [4,]    7    8    4    9\n#> [5,]    9   10    5   10\n\n\n# Appending 2 matrices\nrbind(matrix1, matrix2)\n#>       [,1] [,2]\n#>  [1,]    1    2\n#>  [2,]    3    4\n#>  [3,]    5    6\n#>  [4,]    7    8\n#>  [5,]    9   10\n#>  [6,]    1    6\n#>  [7,]    2    7\n#>  [8,]    3    8\n#>  [9,]    4    9\n#> [10,]    5   10\n\nCreating an empty matrix is also possible:\n\nmatrix(nrow=5, ncol=5)\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]   NA   NA   NA   NA   NA\n#> [2,]   NA   NA   NA   NA   NA\n#> [3,]   NA   NA   NA   NA   NA\n#> [4,]   NA   NA   NA   NA   NA\n#> [5,]   NA   NA   NA   NA   NA\n\nList\n\n\n\n\n\n\nTip\n\n\n\nIn R, lists can be seen as a collection where you can store a variety of different objects under a single name. This includes vectors, matrices, or even other lists. It’s very versatile because its components can be of any type of R object, such as vector, matrix, array, dataframe, table, list, and so on.\n\n\nFor instance:\n\n# List of 2 matrices\nlist1 <- list(matrix1, matrix2)\nlist1\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\nLists can also be expanded to include multiple items:\n\nx6 <- seq(10, 30, length = 7)\nsex <- c(\"females\", \"males\", \"other\")\n# Expanding list to include more items\nlist2 <- list(list1, x6, sex, matrix1)\nlist2 \n#> [[1]]\n#> [[1]][[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[1]][[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n#> \n#> \n#> [[2]]\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n#> \n#> [[3]]\n#> [1] \"females\" \"males\"   \"other\"  \n#> \n#> [[4]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\nCombining different types of data into a single matrix converts everything to a character type:\n\n# A matrix with numeric and character variables\nid <- c(1, 2)\nscore <- c(85, 85)\nsex <- c(\"M\", \"F\")\nnew.matrix <- cbind(id, score, sex)\nnew.matrix\n#>      id  score sex\n#> [1,] \"1\" \"85\"  \"M\"\n#> [2,] \"2\" \"85\"  \"F\"\n\nTo check the type of data in your matrix:\n\nmode(new.matrix)\n#> [1] \"character\"\n\nData frame\n\n\n\n\n\n\nTip\n\n\n\nAs we can see combining both numeric and character variables into a matrix ended up with a matrix of character values. To keep the numeric variables as numeric and character variables as character, we can use the data.frame function.\n\n\n\nCreating a data frame\n\n\n\nA data frame is similar to a matrix but allows for columns of different types (numeric, character, factor, etc.). It’s a standard format for storing data sets in R.\n\ndf <- data.frame(id, score, sex)\ndf\n\n\n\n  \n\n\n\nTo check the mode or type of your data frame:\n\nmode(df)\n#> [1] \"list\"\n\n\nExtract elements\n\nData frames allow easy extraction and modification of specific elements. For example, we can extract the values on the first row and first column as follow:\n\ndf[1,1]\n#> [1] 1\n\nSimilarly, the first column can be extracted as follows:\n\ndf[,1]\n#> [1] 1 2\n\nThe first row can be extracted as follows:\n\ndf[1,]\n\n\n\n  \n\n\n\nColumns can also be accessed using $. Below we are calling column id:\n\ndf$id\n#> [1] 1 2\n\n\nModifying values\n\nWe can edit the values in the data frame as well. For example, we can change the score from 85 to 90 for the id 1:\n\ndf$score[df$id == 1] <- 90\ndf\n\n\n\n  \n\n\n\nWe can also change the name of the variables/columns:\n\ncolnames(df) <- c(\"Studyid\", \"Grade\", \"Sex\")\ndf\n\n\n\n  \n\n\n\n\nCombining data frames\n\nWe can also merge another data frame with the same variables using the rbind function:\n\n# Create a new dataset\ndf2 <- data.frame(Studyid = c(10, 15, 50), Grade = c(75, 90, 65), Sex = c(\"F\", \"M\", \"M\"))\n\n# Combining two data frames\ndf.new <- rbind(df, df2)\n\n# Print the first 6 rows\nhead(df.new)\n\n\n\n  \n\n\n\n\nChecking the dimensions\n\nTo see the dimension of the data frame (i.e., number of rows and columns), we can use the dim function:\n\ndim(df.new)\n#> [1] 5 3\n\nAs we can see, we have 5 rows and 3 columns. We can use the nrow and ncol functions respectively for the same output:\n\nnrow(df.new)\n#> [1] 5\nncol(df.new)\n#> [1] 3"
  },
  {
    "objectID": "wrangling1c.html",
    "href": "wrangling1c.html",
    "title": "Automating tasks",
    "section": "",
    "text": "Repeating a task\n\n\n\n\n\n\nTip\n\n\n\nThe for loop is a control flow statement in R that lets you repeat a particular task multiple times. This repetition is based on a sequence of numbers or values in a vector.\n\n\nConsider a simple real-life analogy: Imagine you are filling water in 10 bottles, one by one. Instead of doing it manually 10 times, you can set a machine to do it in a loop until all 10 bottles are filled.\n\nExample 1\n\nLet’s initiate a counter k at 0 and add 5 to k with each iteration of the loop (i.e., every time it “runs”). After 10 cycles, the loop will stop, but not before printing k in each cycle.\n\n# Looping and adding\nk <- 0\nfor (i in 1:10){\n  k <- k + 5\n  print(k)\n}\n#> [1] 5\n#> [1] 10\n#> [1] 15\n#> [1] 20\n#> [1] 25\n#> [1] 30\n#> [1] 35\n#> [1] 40\n#> [1] 45\n#> [1] 50\n\n\nExample 2\n\nWe create a variable x5 containing the values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. Let us print the first 5 values using the for loop function:\n\nx5 <- seq(from = 0, to = 100, by = 10)\n# Looping through a vector\nk <- 1:5\nfor (ii in k){\n  print(x5[ii])\n}\n#> [1] 0\n#> [1] 10\n#> [1] 20\n#> [1] 30\n#> [1] 40\n\nThis loop cycles through the first five values of a previously created variable x5 and prints them. Each value printed corresponds to the positions 1 to 5 in x5.\n\nExample 3\n\nLet us use the for loop in a more complicated scenario. First, we create a vector of numeric values and square it:\n\n# Create a vector\nk <- c(1, 3, 6, 2, 0)\nk^2\n#> [1]  1  9 36  4  0\n\nThis is just squaring each value in the vector k.\n\nExample 4\n\nUsing the for loop function, we can create the same vector of square values as in Example 3. To do so, (i) we create a null object, (ii) use the loop for each of the elements in the vector (k), (iii) square each of the elements, and (iv) store each of the elements of the new vector. In the example below, the length of k is 5, and the loop will run from the first to the fifth element of k. Also, k.sq[1] is the first stored value for squared-k, and k.sq[2] is the second stored value for squared-k, and so on.\n\n# Looping through a vector with function\nk.sq <- NULL\nfor (i in 1:length(k)){\n  k.sq[i] <- k[i]^2\n}\n\n# Print the values\nk.sq\n#> [1]  1  9 36  4  0\n\nHere, we achieve the same result as the third example but use a for loop. We prepare an empty object k.sq and then use the loop to square each value in k, storing the result in k.sq.\n\nExample 5\n\n\ndf.new <- data.frame(\n  Studyid = c(1, 2, 10, 15, 50),\n  Grade = c(90, 85, 75, 90, 65),\n  Sex = c('M', 'F', 'F', 'M', 'M')\n)\n# Looping through a data frame\nfor (i in 1:nrow(df.new)){\n  print(df.new[i,\"Sex\"])\n}\n#> [1] \"M\"\n#> [1] \"F\"\n#> [1] \"F\"\n#> [1] \"M\"\n#> [1] \"M\"\n\nThis loop prints the “Sex” column value for each row in the df.new data frame.\nFunctions\n\n\n\n\n\n\nTip\n\n\n\nA function in R is a piece of code that can take inputs, process them, and return an output. There are functions built into R, like mean(), which calculates the average of a set of numbers.\n\n\n\nBuilt-in function\n\n\n# Calculating a mean from a vector\nVector <- 1:100\nmean(Vector)\n#> [1] 50.5\n\nHere, we’re using the built-in mean() function to find the average of numbers from 1 to 100.\n\nCustom-made function\n\nTo understand how functions work, sometimes it’s helpful to build our own. Now we will create our own function to calculate the mean, where we will use the following equation to calculate it:\n\\(\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n},\\)\nwhere \\(x_1\\), \\(x_2\\),…, \\(x_n\\) are the values in the vector and \\(n\\) is the sample size. Let us create the function for calculation the mean:\nThis function, mean.own, calculates the average. We add up all the numbers in a vector (Sum <- sum(x)) and divide by the number of items in that vector (n <- length(x)). The result is then returned.\n\nmean.own <- function(x){\n  Sum <- sum(x)\n  n <- length(x)\n  return(Sum/n)\n}\n\nBy using our custom-made function, we calculate the mean of numbers from 1 to 100, getting the same result as the built-in mean() function.\n\nmean.own(Vector)\n#> [1] 50.5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling2.html",
    "href": "wrangling2.html",
    "title": "Importing dataset",
    "section": "",
    "text": "Introduction to Data Importing\nBefore analyzing data in R, one of the first steps you’ll typically undertake is importing your dataset. R provides numerous methods to do this, depending on the format of your dataset.\nDatasets come in a variety of file formats, with .csv (Comma-Separated Values) and .txt (Text file) being among the most common. While R’s interface offers manual ways to load these datasets, knowing how to code this step ensures better reproducibility and automation.\nImporting .txt files\nA .txt data file can be imported using the read.table function. As an example, consider you have a dataset named grade in the specified path.\nLet’s briefly glance at the file without concerning ourselves with its formatting.\n\n# Read and print the content of the TXT file\ncontent <- readLines(\"Data/wrangling/grade.txt\")\ncat(content, sep = \"\\n\")\n#> Studyid Grade Sex\n#> 1    90   M\n#> 2    85   F\n#> 10    75   F\n#> 15    90   M\n#> 50    65   M\n\nUsing the read.table function, you can load this dataset in R properly. It’s important to specify header = TRUE if the first row of your dataset contains variable names.\n\nTip: Always ensure the header argument matches the structure of your dataset. If your dataset contains variable names, set header = TRUE.\n\n\n## Read a text dataset\ngrade <- read.table(\"Data/wrangling/grade.txt\", header = TRUE)\n\n# Display the first few rows of the dataset\nhead(grade)\n\n\n\n  \n\n\n\nImporting .csv files\nSimilarly, .csv files can be loaded using the read.csv function. Here’s how you can load a .csv dataset named mpg:\n\n## Read a csv dataset\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n# Display the first few rows of the dataset\nhead(mpg)\n\n\n\n  \n\n\n\nWhile we’ve discussed two popular data formats, R can handle a plethora of other formats. For further details, refer to Quick-R (2023). Notably, some datasets come built-in with R packages, like the mpg dataset in the ggplot2 package. To load such a dataset:\n\ndata(mpg, package = \"ggplot2\")\nhead(mpg)\n\n\n\n  \n\n\n\nTo understand more about the variables and the dataset’s structure, you can consult the documentation:\n\n?mpg\n\nData Screening and Understanding Your Dataset\ndim(), nrow(), ncol(), and str() are incredibly handy functions when initially exploring your dataset.\nOnce your data is in R, the next logical step is to get familiar with it. Knowing the dimensions of your dataset, types of variables, and the first few entries can give you a quick sense of what you’re dealing with.\nFor instance, str (short for structure) is a concise way to display information about your data. It reveals the type of each variable, the first few entries, and the total number of observations:\n\nstr(mpg)\n#> tibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n#>  $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n#>  $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n#>  $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n#>  $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n#>  $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n#>  $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n#>  $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n#>  $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n#>  $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n#>  $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n#>  $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nIn summary, becoming proficient in data importing and initial screening is a fundamental step in any data analysis process in R. It ensures that subsequent stages of data manipulation and analysis are based on a clear understanding of the dataset at hand.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nQuick-R. 2023. “Importing Data.” https://www.statmethods.net/input/importingdata.html."
  },
  {
    "objectID": "wrangling3.html",
    "href": "wrangling3.html",
    "title": "Data manipulation",
    "section": "",
    "text": "Data manipulation is a foundational skill for data analysis. This guide introduces common methods for subsetting datasets, handling variable types, creating summary tables, and dealing with missing values using R.\nLoad dataset\nUnderstanding the dataset’s structure is the first step in data manipulation. Here, we’re using the mpg dataset, which provides information on various car models:\n\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\nSubset\nOften, you’ll need to subset your data for analysis. Here, we’ll explore different methods to both drop unwanted variables and keep desired observations.\nDrop variables\nSometimes, only part of the variables will be used in your analysis. Therefore, you may want to drop the variables you do not need. There are multiple ways to drop variables from a dataset. Below are two examples without using any package and using the dplyr package.\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[, c(columns_names_you_want_to_KEEP)]\n\n\nSay, we want to keep only three variables in the mpg dataset: manufacturer, model and cyl. For Option 1 (without package), we can use the following R codes to keep these three variables:\n\nmpg1 <- mpg[, c(\"manufacturer\", \"model\", \"cyl\")]\nhead(mpg1)\n\n\n\n  \n\n\n\nHere mpg1 is a new dataset containing only three variables (manufacturer, model and cyl).\n\n\n\n\n\n\nTip\n\n\n\nOption 2: use select in dplyr\nselect(dataset.name, c(columns_names_you_want_to_KEEP))\n\n\nFor Option 2, the dplyr package offers the select function, which provides a more intuitive way to subset data.\n\nmpg2 <- select(mpg, c(\"manufacturer\", \"model\", \"cyl\"))\nhead(mpg2)\n\n\n\n  \n\n\n\nWe can also exclude any variables from the dataset by using the minus (-) sign with the select function. For example, we we want to drop trans, drv, and cty from the mpg dataset, we can use the following codes:\n\nmpg3 <- select(mpg, -c(\"trans\", \"drv\", \"cty\"))\nhead(mpg3)\n\n\n\n  \n\n\n\nThis mpg3 is a new dataset from mpg after dropping three variables (trans, drv, and cty).\nKeep observations\nIt often happens that we only want to investigate a subset of a population which only requires a subset of our dataset. In this case, we need to subset the dataset to meet certain requirements. Again, there are multiple ways to do this task. Below is an example without a package and with the dplyr package:\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[rows_you_want_to_KEEP, ]\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 2: No package needed\nsubset(dataset.name, rows_you_want_to_KEEP)\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 3: use filter in dplyr\nfilter(dataset.name, rows_you_want_to_KEEP)\n\n\nWe can use the logical tests for the rows you want to keep or drop.\n\n\n\n\n\n\nTip\n\n\n\nCommon logical tests are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n X <(=) Y \n    Smaller (equal) than \n  \n\n X >(=) Y \n    Larger (equal) than \n  \n\n X == Y \n    Equal to \n  \n\n X != Y \n    Not equal to \n  \n\n is.na(X) \n    is NA/missing? \n  \n\n\n\n\n\n\nSay, we want to keep the observations for which cars are manufactured in 2008. We can use the following R codes to do it:\n\n# Option 1\nmpg4 <- mpg[mpg$year == \"2008\",]\nhead(mpg4)\n\n\n\n  \n\n\n\nThe following codes with the subset and filter function will do the same:\n\n# Option 2\nmpg5 <- subset(mpg, year == \"2008\")\nhead(mpg5)\n\n\n\n  \n\n\n\n\n# Option 3\nmpg6 <- filter(mpg, year == \"2008\") \nhead(mpg6)\n\n\n\n  \n\n\n\nThe filter function can also work when you have multiple criteria (i.e., multiple logical tests) to satisfy. Here, we need Boolean operators to connect different logical tests.\n\n\n\n\n\n\nTip\n\n\n\nCommon boolean operators are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n & \n    and \n  \n\n | \n    or \n  \n\n ! \n    not \n  \n\n == \n    equals to \n  \n\n != \n    not equal to \n  \n\n > \n    greater than \n  \n\n < \n    less than \n  \n\n >= \n    greater than or equal to \n  \n\n <= \n    less than or equal to \n  \n\n\n\n\n\n\nSay, we want to keep the observations for 6 and 8 cylinders (cyl) and engine displacement (displ) greater than or equal to 4 litres. We can use the following codes to do the task:\n\nmpg7 <- filter(mpg, cyl %in% c(\"6\",\"8\") & displ >= 4)\nhead(mpg7)\n\n\n\n  \n\n\n\n\n\nThe %in% operator is used to determine whether the values of the first argument are present in the second argument.\nHandling Variable Types\n\n\n\n\n\n\nTip\n\n\n\nMost common types of variable in R are\n\nnumbers,\nfactors and\nstrings(or character).\n\nUnderstanding and manipulating these types are crucial for data analysis.\n\n\n\nIdentifying Variable Type\n\nWhen we analyze the data, we usually just deal with numbers and factors. If there are variables are strings, we could convert them to factors using as.factors(variable.name)\n\nmode(mpg$trans)\n#> [1] \"character\"\n\n\nstr(mpg$trans)\n#>  chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" \"auto(l5)\" ...\n\n\nConverting Characters to Factors\n\nSometimes, it’s necessary to treat text data as categorical by converting them into factors. as.numeric() converts other types of variables to numbers. For a factor variable, we usually we want to access the categories (or levels) it has. We can use a build-in function to explore: levels(variable.name)\n\n# no levels for character\nlevels(mpg$trans)\n#> NULL\n\n\n## Ex check how many different trans the dataset has\nmpg$trans <- as.factor(mpg$trans)\nlevels(mpg$trans)\n#>  [1] \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"   \"auto(l6)\"  \n#>  [6] \"auto(s4)\"   \"auto(s5)\"   \"auto(s6)\"   \"manual(m5)\" \"manual(m6)\"\n\nThe levels usually will be ordered alphabetically. The first level is called “baseline”. However, the users may/may not want to keep this baseline and want to relevel/change the reference group. We can do it using the relevel function:\nrelevel(variable.name, ref=)\n\nmpg$trans <- relevel(mpg$trans, ref = \"auto(s6)\")\nlevels(mpg$trans)\n#>  [1] \"auto(s6)\"   \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"  \n#>  [6] \"auto(l6)\"   \"auto(s4)\"   \"auto(s5)\"   \"manual(m5)\" \"manual(m6)\"\nnlevels(mpg$trans)\n#> [1] 10\n\nThe factor function can also be used to combine multiple factors into one factor.\n\n## EX re-group trans to \"auto\" and \"manual\"\nlevels(mpg$trans) <- list(auto = c(\"auto(av)\", \"auto(l3)\", \"auto(l4)\", \"auto(l5)\", \"auto(l6)\", \n                                   \"auto(s4)\", \"auto(s5)\", \"auto(s6)\"), \n                          manual = c(\"manual(m5)\", \"manual(m6)\"))\nlevels(mpg$trans)\n#> [1] \"auto\"   \"manual\"\n\nYou can also change the order of all factors using the following code: factor(variable.name, levels = c(“new order”))\n\n## EX. Change the order of trans to manual\nmpg$trans <- factor(mpg$trans, levels = c(\"manual\", \"auto\"))\nlevels(mpg$trans)\n#> [1] \"manual\" \"auto\"\n\n\n\nIn R, the use of factors with multiple levels is primarily a memory optimization strategy. While users may not directly see this, R assigns internal numerical identifiers to each level, which is a more memory-efficient way of handling such data. Unlike some other software packages that generate multiple dummy variables to represent a single variable, R’s approach is generally more resource-efficient.\n\nConverting back from Factors Characters\n\nYou can also convert factor back to character using the as.character function.\n\n# Convert factor back to character\nmpg$trans <- as.character(mpg$trans)\nlevels(mpg$trans) # no levels for character\n#> NULL\n\nConvert continuous variables to categorical variables\n\n\n\n\n\n\nTip\n\n\n\nifelse, cut, recode all are helpful functions to convert numerical variables to categorical variables.\n\n\nLet’s see the summary of the cty variable first.\n\nsummary(mpg$cty)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    9.00   14.00   17.00   16.86   19.00   35.00\n\nsay, we may want to change continuous ‘cty’ into groups 0-14, 15-18, and 18-40. Below is an example with the cut function.\n\n## EX. change the cty into two categories (0,14], (14,18] and (18,40]\nmpg$cty.num <- cut(mpg$cty, c(0, 14, 18, 40), right = TRUE)\ntable(mpg$cty.num)\n#> \n#>  (0,14] (14,18] (18,40] \n#>      73      85      76\n\n\n## Try this: do you see a difference?: [0,14), [14,18) and [18,40)\nmpg$cty.num2 <- cut(mpg$cty, c(0, 14, 18, 40), right = FALSE)\ntable(mpg$cty.num2)\n#> \n#>  [0,14) [14,18) [18,40) \n#>      54      78     102\n\n\n\n] stands for closed interval, i.e., right = TRUE. On the other hand, ) means open interval. Hence, there will be a huge difference when setting right = TRUE vs. right = FALSE\nMissing value\n\n\n\n\n\n\nTip\n\n\n\nIncomplete datasets can distort analysis. Identifying and managing these missing values is thus crucial.\n\n\nWe can check how many missing values we have by: table(is.na(variable.name))\nLet’s us check whether the cty variable contains any missing values:\n\ntable(is.na(mpg$cty))\n#> \n#> FALSE \n#>   234\n\nIf you want to return all non-missing values, i.e., complete case values: na.omit(variable.name). For more extensive methods on handling missing values, see subsequent tutorials."
  },
  {
    "objectID": "wrangling4.html",
    "href": "wrangling4.html",
    "title": "Import external data",
    "section": "",
    "text": "# Load required packages\nlibrary(dplyr)\nrequire(Hmisc)\n\nWhen dealing with data analysis in R, it’s common to need to import external data. This tutorial will walk you through importing data in different formats.\nCSV format data\nCSV stands for “Comma-Separated Values” and it’s a widely used format for data. We’ll be looking at the “Employee Salaries - 2017” dataset, which contains salary information for permanent employees of Montgomery County in 2017.\n\n\nEmployee Salaries - 2017 data\n\n\n\n\n\n\nTip\n\n\n\nWe’ll be loading the Employee_Salaries_-_2017.csv dataset into R from its saved location at Data/wrangling/. Do note, the directory path might vary for you based on where you’ve stored the downloaded data.\n\n\n\ndata.download <- read.csv(\"Data/wrangling/Employee_Salaries_-_2017.csv\")\n\nHere, the read.csv function reads the data from the CSV file and stores it in a variable called data.download.\nTo understand the structure of our dataset, We can see the number of rows and columns and the names of the columns/variables as follows:\n\ndim(data.download) # check dimension / row / column numbers\n#> [1] 9398   12\nnrow(data.download) # check row numbers\n#> [1] 9398\nnames(data.download) # check column names\n#>  [1] \"Full.Name\"                \"Gender\"                  \n#>  [3] \"Current.Annual.Salary\"    \"X2017.Gross.Pay.Received\"\n#>  [5] \"X2017.Overtime.Pay\"       \"Department\"              \n#>  [7] \"Department.Name\"          \"Division\"                \n#>  [9] \"Assignment.Category\"      \"Employee.Position.Title\" \n#> [11] \"Position.Under.Filled\"    \"Date.First.Hired\"\n\n\n\n\n\n\n\nTip\n\n\n\nhead shows the first 6 elements of an object, giving you a sneak peek into the data you’re dealing with, while tail shows the last 6 elements.\n\n\nWe can see the first see six rows of the dataset as follows:\n\nhead(data.download)\n\n\n\n  \n\n\n\nNext, for learning purposes, let’s artificially assign all male genders in our dataset as missing:\n\n# Assigning male gender as missing\ndata.download$Gender[data.download$Gender == \"M\"] <- NA\nhead(data.download)\n\n\n\n  \n\n\n\nThis chunk sets the Gender column’s value to NA (missing) wherever the gender is “M”. This is a form of data manipulation, sometimes used to handle missing or incorrect data. If you want to work with datasets that exclude any missing values:\n\n\n\n\n\n\nTip\n\n\n\nna.omit and complete.cases are useful functions to to create datasets with non-NA values\n\n\n\n# deleting/dropping missing components\ndata.download2 <- na.omit(data.download)\nhead(data.download2)\n\n\n\n  \n\n\ndim(data.download2)\n#> [1] 3806   12\n\nHere, na.omit is used to remove rows with any missing values. This can be essential when preparing data for certain analyses.\nAlternatively, we could have selected only females to drop all males:\n\ndata.download3 <- filter(data.download, Gender != \"M\")\nhead(data.download3)\n\n\n\n  \n\n\n\nAnd to check the size of this new dataset:\n\n# new dimension / row / column numbers\ndim(data.download3)\n#> [1] 3806   12\n\nSAS format data\n\n\n\n\n\n\nTip\n\n\n\nSAS is another data format, commonly used in professional statistics and analytics.\n\n\nLet’s explore importing a SAS dataset. We download a SAS formatted dataset from the CDC website.\n\n# Link\nx <- \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\"\n\n# Data\nNHANES1516data <- sasxport.get(x)\n#> Processing SAS dataset DEMO_I     ..\n\n# Check dimension / row / column numbers\ndim(NHANES1516data) \n#> [1] 9971   47\n\n# Check row numbers\nnrow(NHANES1516data) \n#> [1] 9971\n\n# Check first 10 names\nnames(NHANES1516data)[1:10] \n#>  [1] \"seqn\"     \"sddsrvyr\" \"ridstatr\" \"riagendr\" \"ridageyr\" \"ridagemn\"\n#>  [7] \"ridreth1\" \"ridreth3\" \"ridexmon\" \"ridexagm\"\n\nThe sasxport.get function retrieves the SAS dataset. The following lines, just like before, help understand its structure.\nTo analyze some of the data:\n\ntable(NHANES1516data$riagendr) # tabulating gender variable\n#> \n#>    1    2 \n#> 4892 5079\n\n\n\nVerify these numbers from CDC website\nThis code creates a frequency table of the riagendr variable, which represents gender."
  },
  {
    "objectID": "wrangling5.html",
    "href": "wrangling5.html",
    "title": "Summary tables",
    "section": "",
    "text": "Medical research and epidemiology often involve large, complex datasets. Data summarization is a vital step that transforms these vast datasets into concise, understandable insights. In medical contexts, these summaries can highlight patterns, indicate data inconsistencies, and guide further research. This tutorial will teach you how to use R to efficiently summarize medical data.\nIn epidemiology and medical research, “Table 1” typically refers to the first table in a research paper or report that provides descriptive statistics of the study population. It offers a snapshot of the baseline characteristics of the study groups, whether in a cohort study, clinical trial, or any other study design.\n\n# Data\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\n# Frequency table for drv\ntable(mpg$drv)\n#> \n#>   4   f   r \n#> 103 106  25\n\n# Frequency table for manufacturer\ntable(mpg$manufacturer)\n#> \n#>       audi  chevrolet      dodge       ford      honda    hyundai       jeep \n#>         18         19         37         25          9         14          8 \n#> land rover    lincoln    mercury     nissan    pontiac     subaru     toyota \n#>          4          3          4         13          5         14         34 \n#> volkswagen \n#>         27\n\n## Ex create a summary table between manufacturer and drv\ntable(mpg$drv, mpg$manufacturer)\n#>    \n#>     audi chevrolet dodge ford honda hyundai jeep land rover lincoln mercury\n#>   4   11         4    26   13     0       0    8          4       0       4\n#>   f    7         5    11    0     9      14    0          0       0       0\n#>   r    0        10     0   12     0       0    0          0       3       0\n#>    \n#>     nissan pontiac subaru toyota volkswagen\n#>   4      4       0     14     15          0\n#>   f      9       5      0     19         27\n#>   r      0       0      0      0          0\n\nThe first line reads a CSV file. It uses the table() function to generate a contingency table (cross-tabulation) between two categorical variables: drv (drive) and manufacturer. It essentially counts how many times each combination of drv and manufacturer appears in the dataset.\n\n## Get the percentage summary using prop.table\nprop.table(table(mpg$drv, mpg$manufacturer), margin = 2)\n#>    \n#>          audi chevrolet     dodge      ford     honda   hyundai      jeep\n#>   4 0.6111111 0.2105263 0.7027027 0.5200000 0.0000000 0.0000000 1.0000000\n#>   f 0.3888889 0.2631579 0.2972973 0.0000000 1.0000000 1.0000000 0.0000000\n#>   r 0.0000000 0.5263158 0.0000000 0.4800000 0.0000000 0.0000000 0.0000000\n#>    \n#>     land rover   lincoln   mercury    nissan   pontiac    subaru    toyota\n#>   4  1.0000000 0.0000000 1.0000000 0.3076923 0.0000000 1.0000000 0.4411765\n#>   f  0.0000000 0.0000000 0.0000000 0.6923077 1.0000000 0.0000000 0.5588235\n#>   r  0.0000000 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>    \n#>     volkswagen\n#>   4  0.0000000\n#>   f  1.0000000\n#>   r  0.0000000\n## margin = 1 sum across row, 2 across col\n\nThis code calculates the column-wise proportion (as percentages) for each combination of drv and manufacturer. The prop.table() function is used to compute the proportions. The margin = 2 argument indicates that the proportions are to be computed across columns (margin = 1 would compute them across rows).\ntableone package\n\n\n\n\n\n\nTip\n\n\n\nCreateTableOne function from tableone package could be a very useful function to see the summary table. Type ?tableone::CreateTableOne to see for more details.\n\n\nThis section introduces the tableone package, which offers the CreateTableOne function. This function helps in creating “Table 1” type summary tables, commonly used in epidemiological studies.\n\nrequire(tableone)\nCreateTableOne(vars = c(\"cyl\", \"drv\", \"hwy\", \"cty\"), data = mpg, \n               strata = \"trans\", includeNA = TRUE, test = FALSE)\n#>                  Stratified by trans\n#>                   auto(av)       auto(l3)       auto(l4)      auto(l5)     \n#>   n                   5              2             83            39        \n#>   cyl (mean (SD))  5.20 (1.10)    4.00 (0.00)    6.14 (1.62)   6.56 (1.45) \n#>   drv (%)                                                                  \n#>      4                0 (  0.0)      0 (  0.0)     34 (41.0)     29 (74.4) \n#>      f                5 (100.0)      2 (100.0)     37 (44.6)      8 (20.5) \n#>      r                0 (  0.0)      0 (  0.0)     12 (14.5)      2 ( 5.1) \n#>   hwy (mean (SD)) 27.80 (2.59)   27.00 (4.24)   21.96 (5.64)  20.72 (6.04) \n#>   cty (mean (SD)) 20.00 (2.00)   21.00 (4.24)   15.94 (3.98)  14.72 (3.49) \n#>                  Stratified by trans\n#>                   auto(l6)      auto(s4)      auto(s5)      auto(s6)     \n#>   n                   6             3             3            16        \n#>   cyl (mean (SD))  7.33 (1.03)   5.33 (2.31)   6.00 (2.00)   6.00 (1.59) \n#>   drv (%)                                                                \n#>      4                2 (33.3)      2 (66.7)      1 (33.3)      7 (43.8) \n#>      f                2 (33.3)      1 (33.3)      2 (66.7)      8 (50.0) \n#>      r                2 (33.3)      0 ( 0.0)      0 ( 0.0)      1 ( 6.2) \n#>   hwy (mean (SD)) 20.00 (2.37)  25.67 (1.15)  25.33 (6.66)  25.19 (3.99) \n#>   cty (mean (SD)) 13.67 (1.86)  18.67 (2.31)  17.33 (5.03)  17.38 (3.22) \n#>                  Stratified by trans\n#>                   manual(m5)    manual(m6)   \n#>   n                  58            19        \n#>   cyl (mean (SD))  5.00 (1.30)   6.00 (1.76) \n#>   drv (%)                                    \n#>      4               21 (36.2)      7 (36.8) \n#>      f               33 (56.9)      8 (42.1) \n#>      r                4 ( 6.9)      4 (21.1) \n#>   hwy (mean (SD)) 26.29 (5.99)  24.21 (5.75) \n#>   cty (mean (SD)) 19.26 (4.56)  16.89 (3.83)\n\nThe CreateTableOne function is used to create a summary table for the variables cyl, drv, hwy, and cty from the mpg dataset. The strata = trans argument means that the summary is stratified by the trans variable. The includeNA = TRUE argument means that missing values (NAs) are included in the summary. The test = FALSE argument indicates that no statistical tests should be applied to the data (often tests are used to compare groups in the table).\ntable1 package\nThis section introduces another package, table1, which can also be used to create “Table 1” type summary tables.\n\nrequire(table1)\ntable1(~ cyl + drv + hwy + cty | trans, data=mpg)\n\n\n\n\n\nauto(av)(N=5)\nauto(l3)(N=2)\nauto(l4)(N=83)\nauto(l5)(N=39)\nauto(l6)(N=6)\nauto(s4)(N=3)\nauto(s5)(N=3)\nauto(s6)(N=16)\nmanual(m5)(N=58)\nmanual(m6)(N=19)\nOverall(N=234)\n\n\n\ncyl\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n5.20 (1.10)\n4.00 (0)\n6.14 (1.62)\n6.56 (1.45)\n7.33 (1.03)\n5.33 (2.31)\n6.00 (2.00)\n6.00 (1.59)\n5.00 (1.30)\n6.00 (1.76)\n5.89 (1.61)\n\n\nMedian [Min, Max]\n6.00 [4.00, 6.00]\n4.00 [4.00, 4.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n8.00 [6.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n\n\ndrv\n\n\n\n\n\n\n\n\n\n\n\n\n\nf\n5 (100%)\n2 (100%)\n37 (44.6%)\n8 (20.5%)\n2 (33.3%)\n1 (33.3%)\n2 (66.7%)\n8 (50.0%)\n33 (56.9%)\n8 (42.1%)\n106 (45.3%)\n\n\n4\n0 (0%)\n0 (0%)\n34 (41.0%)\n29 (74.4%)\n2 (33.3%)\n2 (66.7%)\n1 (33.3%)\n7 (43.8%)\n21 (36.2%)\n7 (36.8%)\n103 (44.0%)\n\n\nr\n0 (0%)\n0 (0%)\n12 (14.5%)\n2 (5.1%)\n2 (33.3%)\n0 (0%)\n0 (0%)\n1 (6.3%)\n4 (6.9%)\n4 (21.1%)\n25 (10.7%)\n\n\nhwy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n27.8 (2.59)\n27.0 (4.24)\n22.0 (5.64)\n20.7 (6.04)\n20.0 (2.37)\n25.7 (1.15)\n25.3 (6.66)\n25.2 (3.99)\n26.3 (5.99)\n24.2 (5.75)\n23.4 (5.95)\n\n\nMedian [Min, Max]\n27.0 [25.0, 31.0]\n27.0 [24.0, 30.0]\n22.0 [14.0, 41.0]\n19.0 [12.0, 36.0]\n19.0 [18.0, 23.0]\n25.0 [25.0, 27.0]\n27.0 [18.0, 31.0]\n26.0 [18.0, 29.0]\n26.0 [16.0, 44.0]\n26.0 [12.0, 32.0]\n24.0 [12.0, 44.0]\n\n\ncty\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n20.0 (2.00)\n21.0 (4.24)\n15.9 (3.98)\n14.7 (3.49)\n13.7 (1.86)\n18.7 (2.31)\n17.3 (5.03)\n17.4 (3.22)\n19.3 (4.56)\n16.9 (3.83)\n16.9 (4.26)\n\n\nMedian [Min, Max]\n19.0 [18.0, 23.0]\n21.0 [18.0, 24.0]\n16.0 [11.0, 29.0]\n14.0 [9.00, 25.0]\n13.0 [12.0, 16.0]\n20.0 [16.0, 20.0]\n18.0 [12.0, 22.0]\n17.0 [12.0, 22.0]\n19.0 [11.0, 35.0]\n16.0 [9.00, 23.0]\n17.0 [9.00, 35.0]\n\n\n\n\n\nThe table1() function is used to generate a summary table for the specified variables. The formula-like syntax (~ cyl + drv + hwy + cty | trans) indicates that the summary should be stratified by the trans variable."
  },
  {
    "objectID": "wrangling6.html#introduction",
    "href": "wrangling6.html#introduction",
    "title": "R Markdown",
    "section": "Introduction",
    "text": "Introduction\nWelcome to this tutorial on working with RMD files in RStudio! RStudio is an IDE that makes R programming easier and more efficient, while R Markdown (RMD) is a file format that enables you to create dynamic reports with R code and narrative text. Using R Markdown within RStudio allows you to compile your analyses and reports into a single, easily shareable document in multiple formats like HTML, PDF, or Word.\n\n\nRStudio is an Integrated Development Environment (IDE), which is a software application that provides comprehensive facilities for software development. An IDE typically includes a text editor, tools for building and running code, and debugging utilities."
  },
  {
    "objectID": "wrangling6.html#prerequisites",
    "href": "wrangling6.html#prerequisites",
    "title": "R Markdown",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this tutorial, make sure you have both R and RStudio installed on your computer, as explaied in an earlier tutorial."
  },
  {
    "objectID": "wrangling6.html#knitting-rmd",
    "href": "wrangling6.html#knitting-rmd",
    "title": "R Markdown",
    "section": "Knitting RMD",
    "text": "Knitting RMD\nThis document shows how to work with an RMD file. We can create dynamic documents, e.g., a document with simple plain text combined with R code and its outputs. Note that RMD files are designed to be used with the R package rmarkdown. In RStudio IDE, the rmarkdown package could be already installed.\n\n# install.packages(\"rmarkdown\")\n\nOpen on RStudio\nLet us open an RMD file in RStudio. From the file menu (on the side), we can create a new RMD document. First, we need to click on the + symbol to create a new file, as follows:\n\n\n\n\n\n\n\n\nSecond, we need to click R markdown... to create a new RMD document as follows:\n\n\n\n\n\n\n\n\nWe will see a pop-up window as follows:\n\n\n\n\n\n\n\n\nWe can select whether we want to convert our RMD file to an HTML, PDF, or a Word document. These options can also be selected later. Let us select the default option (HTML) and press OK. We will see the markdown file as shown in the picture below:\n\n\n\n\n\n\n\n\nKnit\nWe use the knit option to create a document (e.g., making a PDF, HTML, or Word document) from the RMD file. Before knitting, we need to save the file. Let us save the file as working.RMD. After saving the file, we can knit it by clicking on the knit option, as shown below:\n\n\nThe term “knit” may sound a bit strange in the context of programming. However, it aptly describes the process of combining your R code and narrative text to produce a cohesive, final document. Think of it as “weaving” your code and text together into various output formats like HTML, PDF, or Word.\n\n\n\n\n\n\n\n\nOnce we knit the file, it will produce an HTML output, since our default option was HTML.\n\n\n\n\n\n\n\n\nFor formats other than HTML (e.g., PDF or Word), we can click on the dropdown menu:\n\n\n\n\n\n\n\n\nLet us select Knit to Word and knit it. Once the file is rendered, RStudio will show us a preview of the output in a word file and save the file in our working directory. We can also see that Word is added as another output:\n\n\nWhen you’re working in RStudio, all your files and outputs will be saved in a ‘working directory.’ This is simply the folder on your computer where RStudio will look for files and save outputs. To find out what your current working directory is, you can run the command getwd() in the R console.\n\n\n\n\n\n\n\n\nIn the R terminal, we can see that a Word document is created, which is stored in our working directory:\n\n\n\n\n\n\n\n\nSimilarly, we can create a pdf by clicking Knit to PDF option from the Knit menu. However, we could see an error message as follows:\n\n\n\n\n\n\n\n\nIt is important to note that RStudio does not build PDF documents from scratch. If we want to create PDF documents using RMD, we must have a LaTeX distribution installed on our computer. There are several options for LaTeX distributions, including MiKTeX, MacTeX, TeX Live, and so on. However, the recommended option for R Markdown users is TinyTeX. We can install TinyTeX using the R package tinytex. To install the package, run the following command: install.packages(\"tinytex\").\n\n\nLaTeX is a typesetting system commonly used for technical and scientific documentation. It is required for converting R Markdown documents to PDF format because it provides the text formatting commands that the rmarkdown package uses behind the scenes.\n\n\n\n\n\n\n\n\nOnce the tinytex package installation is complete, we can type tinytex::install_tinytex() to install the LaTeX distribution on our computer.\n\n\n\n\n\n\n\n\nTinyTeX is a large package (~123 MB). The installation time will vary depending on your machine. Once the installation is complete, we can click Knit to PDF. Similar to the Word file, RStudio will display a preview of the PDF output and save the PDF in our working directory. We will also see that a PDF file has been created:\n\n\n\n\n\n\n\n\nWorking with RMD\nNow we are ready to start writing plain text intermixed with embedded R code. For plain text, we can use the whitespace:\n\n\n\n\n\n\n\n\nOn the other hand, to embed a chunk of R code into our report, we use R code chunks. An R chunk surrounds the code with two lines that each contain three backticks. After the first set of backticks, we include {r}, which alerts knitr that we are going to include a chunk of R code:\n\n\nCode chunks are segments of code that are contained within an R Markdown document. They allow you to run R code within the document itself, making your report dynamic and reproducible.\n\n\n\n\n\n\n\n\nBelow are some codes:\n\n\n\n\n\n\n\n\nWe can knit the file to see the document, which will include plain text, R code, and outputs from the R code. We can also see the output from a code chunk without knitting the entire file. For example, we can click the arrow on the right-hand side to execute the current code chunk:\n\n\n\n\n\n\n\n\nNow we can see the following outputs:\n\n\n\n\n\n\n\n\nPlease also explore the drop down menu under Run to see the further options, including run the current code chunk, run all code chunk above, etc.\n\n\n\n\n\n\n\n\nTo omit the code from the final report while still including the results, add the argument echo = FALSE. This will place a copy of the results into your report.\n\n\nBesides echo = FALSE, there are several other options you can include in your code chunks to control their behavior, like eval = FALSE if you don’t want to evaluate the code, or message = FALSE to hide messages. Take a look at the author’s page of comprehensive list of chunk options.\nIn the final report (e.g., Word or PDF), we often want to omit the code and only show the outputs. To do this, we added the argument echo = FALSE in the R code chunk:\n\n\n\n\n\n\n\n\nThe resulting output will look as follows:"
  },
  {
    "objectID": "wrangling6.html#tips-and-troubleshooting",
    "href": "wrangling6.html#tips-and-troubleshooting",
    "title": "R Markdown",
    "section": "Tips and Troubleshooting",
    "text": "Tips and Troubleshooting\n\nIf the knit button is grayed out, make sure you have saved your RMD file first.\nEncountering LaTeX errors? Make sure you’ve installed a LaTeX distribution like TinyTeX.\n\n\n\n\n\n\n\nTip\n\n\n\nThe following links could also be useful if you want to learn more:\n\nR Markdown Cheat Sheet\nIntroduction to R Markdown\nR Markdown: The Definitive Guide\nReports with R Markdown\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wranglingF.html",
    "href": "wranglingF.html",
    "title": "R Functions (W)",
    "section": "",
    "text": "Note\n\n\n\nThis review/summary page provides an extensive list of R functions tailored for data wrangling tasks that we have used in this chapter. Each function is systematically described, highlighting its primary package source and its specific utility.\n\n\nTo learn more about these functions, readers can:\n\nUse R’s Built-in Help System: For each function, access its documentation by prefixing the function name with a question mark in the R console, e.g., ?as.factor. This displays the function’s manual page with descriptions, usage, and examples.\nSearch Websites: Simply Google, or visit the CRAN website to search for specific function documentation. Websites like Stack Overflow and RStudio Community often have discussions related to R functions.\nTutorials and Online Courses: Platforms like DataCamp, Coursera, and edX offer R courses that cover many functions in depth. Also there are examples of dedicated R tutorial websites that you might find useful. One example is “Introduction to R for health data analysis” by Ehsan Karim, An Hoang and Qu.\nBooks: There are numerous R programming books, such as “R for Data Science” by Hadley Wickham and “The Art of R Programming” by Norman Matloff.\nWorkshops and Webinars: Institutions and organizations occasionally offer R programming workshops or webinars.\n\nWhenever in doubt, exploring existing resources can be highly beneficial.\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.factor \n    base \n    Converts a variable to factors. `as.factor` is a wrapper for the `factor` function. \n  \n\n cbind \n    base \n    Merges matrices. \n  \n\n CreateTableOne \n    tableone \n    Creates a frequency table. \n  \n\n data.frame \n    base \n    Creates a dataset with both numeric and character variables. Requires unique column names and equal length for all variables. \n  \n\n dim \n    base \n    Returns the dimensions of a data frame (rows x columns). \n  \n\n filter \n    dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n function \n    base \n    Used to define custom functions, e.g., for calculating standard deviation. \n  \n\n head \n    base \n    Displays the first six elements of an object (e.g., a dataset). `tail` displays the last six. \n  \n\n is.na \n    base \n    Checks for missing values in a variable. \n  \n\n levels \n    base \n    Displays the levels of a factor variable. \n  \n\n list \n    base \n    Stores vectors, matrices, or lists of differing types. \n  \n\n mode \n    base \n    Determines the type of a variable. \n  \n\n na.omit \n    base/stats \n    Removes all rows with missing values from a dataset. \n  \n\n names \n    base \n    Displays names of objects, e.g., variable names of a data frame. \n  \n\n nlevels \n    base \n    Shows the number of levels in a factor variable. \n  \n\n nrow \n    base \n    Returns the dimensions of a data frame. `nrow` gives row count and `ncol` gives column count. \n  \n\n plot \n    base/graphics \n    Draws scatter plots or line graphs. \n  \n\n print \n    base \n    Prints the output to console. \n  \n\n prop.table \n    base \n    Displays percentage summary for a table. \n  \n\n rbind \n    base \n    Appends matrices row-wise. \n  \n\n read.csv \n    base/utils \n    Reads data from a CSV file. \n  \n\n relevel \n    base/stats \n    Changes the reference group of a factor variable. \n  \n\n sasxport.get \n    Hmisc \n    Loads data in the SAS format. \n  \n\n save \n    base \n    Saves R objects, such as datasets. \n  \n\n select \n    dplyr \n    Selects specified variables from a dataset. \n  \n\n set.seed \n    base \n    Sets a seed for random number generation ensuring reproducibility. \n  \n\n str \n    base/utils \n    Displays the structure of a dataset, including data type of variables. \n  \n\n subset \n    base, dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n summary \n    base \n    Provides a summary of an object, like variable statistics. \n  \n\n table \n    base \n    Displays frequency counts for a variable. \n  \n\n write.csv \n    base/utils \n    Saves a data frame to a CSV file in a specified directory."
  },
  {
    "objectID": "wranglingQ.html#live-quiz",
    "href": "wranglingQ.html#live-quiz",
    "title": "Quiz (W)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "wranglingQ.html#download-quiz",
    "href": "wranglingQ.html#download-quiz",
    "title": "Quiz (W)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here. If not downloading immediately, right-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "wranglingS.html",
    "href": "wranglingS.html",
    "title": "App (W)",
    "section": "",
    "text": "Below is an example of an app that utilizes the mpg dataset from the ggplot2 package following the tutorial materials. Users can subset the data and generate a stratified Table 1 from it.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveW\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, tableone and ggplot2 packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "wranglingE.html#problem-statement",
    "href": "wranglingE.html#problem-statement",
    "title": "Exercise (W)",
    "section": "Problem Statement",
    "text": "Problem Statement\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nYou can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(tableone)\n\n\n# Data import: name it rhc\n#rhc <- ...(\"Data/wrangling/rhc.csv\", ...)"
  },
  {
    "objectID": "wranglingE.html#problem-1-basic-manipulation-60",
    "href": "wranglingE.html#problem-1-basic-manipulation-60",
    "title": "Exercise (W)",
    "section": "Problem 1: Basic Manipulation [60%]",
    "text": "Problem 1: Basic Manipulation [60%]\n\nContinuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\n\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\n\n\n\nSet reference: Change the reference category for gender to Male\n\n\n\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\n\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\n\n\n\ncomorbidities:\n\n\nCreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nReport maximum and minimum values of numcom:\n\n\n# See head of comorbidities\n# head(rhc[,c(\"cardiohx\", \"chfhx\", \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n#             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \"transhx\", \"amihx\")])\n\n# your codes here\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\nage, sex, race, cat1, ca, dnr1, aps1, surv2md1, numcom, adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21, ph1, crea1, alb1, scoma1, swang1\nname the dataset as rhc2"
  },
  {
    "objectID": "wranglingE.html#problem-2-table-1-10",
    "href": "wranglingE.html#problem-2-table-1-10",
    "title": "Exercise (W)",
    "section": "Problem 2: Table 1 [10%]",
    "text": "Problem 2: Table 1 [10%]\nRe-produce the sample table 1 from the rhc2 data (see the Table below). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\nNo RHC\nRHC\n\n\n\nn\n3551\n2184\n\n\nage (%)\n\n\n\n\n   [-Inf,50)\n   884 (24.9)\n   540 (24.7)\n\n\n   [50,60)\n   546 (15.4)\n   371 (17.0)\n\n\n   [60,70)\n   812 (22.9)\n   577 (26.4)\n\n\n   [70,80)\n   809 (22.8)\n   529 (24.2)\n\n\n   [80, Inf)\n   500 (14.1)\n   167 ( 7.6)\n\n\nsex = Female (%)\n  1637 (46.1)\n   906 (41.5)\n\n\nrace (%)\n\n\n\n\n   white\n  2753 (77.5)\n  1707 (78.2)\n\n\n   black\n   585 (16.5)\n   335 (15.3)\n\n\n   other\n   213 ( 6.0)\n   142 ( 6.5)\n\n\ncat1 (%)\n\n\n\n\n   ARF\n  1581 (44.5)\n   909 (41.6)\n\n\n   CHF\n   247 ( 7.0)\n   209 ( 9.6)\n\n\n   Other\n   955 (26.9)\n   208 ( 9.5)\n\n\n   MOSF\n   768 (21.6)\n   858 (39.3)\n\n\nca (%)\n\n\n\n\n   None\n  2652 (74.7)\n  1727 (79.1)\n\n\n   Localized (Yes)\n   638 (18.0)\n   334 (15.3)\n\n\n   Metastatic\n   261 ( 7.4)\n   123 ( 5.6)\n\n\ndnr1 = Yes (%)\n   499 (14.1)\n   155 ( 7.1)\n\n\naps1 (mean (SD))\n 50.93 (18.81)\n 60.74 (20.27)\n\n\nsurv2md1 (mean (SD))\n  0.61 (0.19)\n  0.57 (0.20)\n\n\nnumcom (mean (SD))\n  1.52 (1.17)\n  1.48 (1.13)\n\n\nadld3p (mean (SD))\n  1.24 (1.86)\n  1.02 (1.69)\n\n\ndas2d3pc (mean (SD))\n 20.37 (5.48)\n 20.70 (5.03)\n\n\ntemp1 (mean (SD))\n 37.63 (1.74)\n 37.59 (1.83)\n\n\nhrt1 (mean (SD))\n112.87 (40.94)\n118.93 (41.47)\n\n\nmeanbp1 (mean (SD))\n 84.87 (38.87)\n 68.20 (34.24)\n\n\nresp1 (mean (SD))\n 28.98 (13.95)\n 26.65 (14.17)\n\n\nwblc1 (mean (SD))\n 15.26 (11.41)\n 16.27 (12.55)\n\n\npafi1 (mean (SD))\n240.63 (116.66)\n192.43 (105.54)\n\n\npaco21 (mean (SD))\n 39.95 (14.24)\n 36.79 (10.97)\n\n\nph1 (mean (SD))\n  7.39 (0.11)\n  7.38 (0.11)\n\n\ncrea1 (mean (SD))\n  1.92 (2.03)\n  2.47 (2.05)\n\n\nalb1 (mean (SD))\n  3.16 (0.67)\n  2.98 (0.93)\n\n\nscoma1 (mean (SD))\n 22.25 (31.37)\n 18.97 (28.26)"
  },
  {
    "objectID": "wranglingE.html#problem-3-table-1-for-subset-10",
    "href": "wranglingE.html#problem-3-table-1-for-subset-10",
    "title": "Exercise (W)",
    "section": "Problem 3: Table 1 for subset [10%]",
    "text": "Problem 3: Table 1 for subset [10%]\nProduce a similar table as Problem 2 but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]"
  },
  {
    "objectID": "wranglingE.html#problem-4-considering-eligibility-criteria-20",
    "href": "wranglingE.html#problem-4-considering-eligibility-criteria-20",
    "title": "Exercise (W)",
    "section": "Problem 4: Considering eligibility criteria [20%]",
    "text": "Problem 4: Considering eligibility criteria [20%]\nProduce a similar table as Problem 2 but only for the subjects who meet all of the following eligibility criteria: (i) age is equal to or above 50, (ii) age is below 80 (iii) Glasgow Coma Score is below 61 and (iv) Primary disease categories are either ARF or MOSF. [Hint: droplevels.data.frame can be a useful function]"
  },
  {
    "objectID": "wranglingE.html#optional-0",
    "href": "wranglingE.html#optional-0",
    "title": "Exercise (W)",
    "section": "Optional [0%]",
    "text": "Optional [0%]\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\n\n\n\nCount how many NAs does that variable have?\n\n\n\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\n\n\n\n\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nVector <- 1:100\n\n#variance.est <- function(?){?}\n\n#variance.est(Vector)\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nvar(Vector)\n#> [1] 841.6667"
  },
  {
    "objectID": "accessing.html#background",
    "href": "accessing.html#background",
    "title": "Accessing data",
    "section": "Background",
    "text": "Background\nSurveys serve as a pivotal tool for collecting and evaluating health-related information on a national scale. More often than not, it’s the governmental bodies that take the lead in gathering this data. Recognizing the value of this information, many governments not only compile and analyze these datasets but also ensure they are accessible to the public, especially for research purposes. In this guide, we will delve into an array of survey methodologies, provide illustrative examples, and guide you through the process of downloading pertinent data from both Canadian and American repositories. To make this more tangible, we will conclude with a hands-on example, showcasing how to replicate findings from an academic paper that leveraged one of these publicly available datasets.\n\n\nNow that we are familiar with R basics and data wrangling, we are now dedicating a chapter to accessing and downloading nationally representative survey datasets is pivotal for a hands-on epidemiological tutorial. These datasets, often rich in information and reflective of diverse populations, serve as the backbone for real-world analysis. By guiding learners on how to obtain these datasets, the book ensures that they not only grasp theoretical concepts but also gain practical experience working with authentic, large-scale data. This approach bridges the gap between theory and practice, allowing readers to apply learned techniques on datasets that mirror real-world complexities, thereby enhancing the relevance and applicability of their analytical skills.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "accessing.html#overview-of-tutorials",
    "href": "accessing.html#overview-of-tutorials",
    "title": "Accessing data",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nSurvey data sources\nThe tutorial lists primary complex survey data sources, including the Canadian Community Health Survey and National Health and Nutrition Examination Survey, with several offering dedicated R packages for data access.\n\n\nDescriptions of data sources\nThis tutorial provides comprehensive instructions on how to import and process health survey datasets, specifically focusing on the Canadian Community Health Survey (CCHS), National Health and Nutrition Examination Survey (NHANES), and National Health Interview Survey (NHIS).\n\n\nImporting CCHS to R\nThe section provides detailed steps for importing the Canadian Community Health Survey dataset from the UBC library into RStudio, with processing options using SAS, the free software PSPP, and directly in R.\n\n\nImporting NHANES to R\nThe tutorial guides users on how to access and import the NHANES dataset from the CDC website into RStudio, detailing the dataset’s structure and providing methods both manually and using an R package.\n\n\nReproducing results\nThe tutorial guides users through accessing, processing, and analyzing NHANES data to reproduce the results from a referenced article using R code.\n\n\nImporting NHIS to R\nThis chapter serves as a tutorial on accessing and importing the National Health Interview Survey (NHIS) dataset from the US Centers for Disease Control and Prevention (CDC) website into RStudio. The NHIS is an annual cross-sectional survey managed by the CDC, offering insights into population disease prevalence, disability extent, and utilization of health care services. The data files are available in various formats, including ASCII, CSV, and SAS. Users can combine datasets from different years; however, tracing the same individual across cycles is not feasible. The chapter provides step-by-step guidance on downloading the NHIS dataset, particularly the ‘Adult’ data from 2021, verifying the imported data, and merging datasets within the same survey cycle using the unique household identifier.\n\n\nLinking mortality data\nThis tutorial provides instructions on how to link public-use US mortality data with NHANES dataset, with the option to do the same with NHIS. The process involves downloading the mortality data in dat format from the CDC website, loading it into the R environment, and then merging it with the NHANES data using a unique identifier. Various variables related to mortality status, underlying causes of death, and other health-related factors are explained, and Table 1 is created with unweighted and weighted statistics.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThe subsequent chapter on Research Questions serves as a valuable guide for constructing an analytics-driven data set tailored to your specific research queries. It will cover crucial aspects such as the types of variables to collect and how to set eligibility criteria, followed by approaches to data analysis based on your research questions. It’s important to note that research questions can fall into two main categories: predictive or causal. For a deeper understanding of variable selection and analytical tools suited to these types of questions, the chapters on the Roles of Variables and Predictive Models offer insightful guidance.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "accessing0.html#model-based-approach",
    "href": "accessing0.html#model-based-approach",
    "title": "Concepts (A)",
    "section": "Model-based approach",
    "text": "Model-based approach\nThe model-based approach to statistical analysis is heavily reliant on the specification of a probability model for data generation, typically assuming that data come from an infinite population that follows a specific distribution, such as the Normal distribution. Inferences about the population, including point estimates and hypothesis testing, are made based on how well the sample data fit these model assumptions."
  },
  {
    "objectID": "accessing0.html#design-based-approach",
    "href": "accessing0.html#design-based-approach",
    "title": "Concepts (A)",
    "section": "Design-based approach",
    "text": "Design-based approach\nThe design-based approach emphasizes the use of sampling methods and the design of the study itself to make inferences about a real/finite population. The design-based approach takes into account the actual structure of the data collection process to make inferences, ensuring that each unit in the population has a known and often non-zero chance of being included in the sample, thus addressing the potential biases and variance issues arising from the sampling design. This approach is critical in understanding and analyzing data from surveys with complex designs, including those with stratification, clustering, and weighting."
  },
  {
    "objectID": "accessing0.html#reading-list",
    "href": "accessing0.html#reading-list",
    "title": "Concepts (A)",
    "section": "Reading list",
    "text": "Reading list\nKey reference:\n\n(Heeringa, West, and Berglund 2017) (chapter 2)\n\nOptional reading:\n\n(Lumley 2011) (chapter 1)\n(Vittinghoff et al. 2011) (chapter 12)\n(Bilder and Loughin 2014) (section 6.3)"
  },
  {
    "objectID": "accessing0.html#video-lessons",
    "href": "accessing0.html#video-lessons",
    "title": "Concepts (A)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nModel-based approach\n\n\n\nReview materials from pre-requisite statistics courses (optional)\n\n\n\n\n\n\n\n\n\n\n\n\nDesign-based approach\n\n\n\nWhat is included in this Video Lesson:\n\nModel-based approach review: 0:00\nDesign-based approach: 1:15\nTypes of sampling techniques 6:46\nStatistical inference 8:25\nNHANES 12:02\nSurvey weight 20:40\nCCHS download 23:45\nNHANES download 24:50\nNHANES sampling design 27:24\nHow to find NHANES data from CDC website 27:42\n\nThe timestamps are also included in the YouTube video description."
  },
  {
    "objectID": "accessing0.html#video-lesson-slides",
    "href": "accessing0.html#video-lesson-slides",
    "title": "Concepts (A)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "accessing0.html#links",
    "href": "accessing0.html#links",
    "title": "Concepts (A)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides\nModel-based approach (Review/optional content)\nDesign-based approach"
  },
  {
    "objectID": "accessing0.html#references",
    "href": "accessing0.html#references",
    "title": "Concepts (A)",
    "section": "References",
    "text": "References\n\n\n\n\nBilder, Christopher R, and Thomas M Loughin. 2014. Analysis of Categorical Data with r. CRC Press.\n\n\nHeeringa, Steven G, Brady T West, and Patricia A Berglund. 2017. Applied Survey Data Analysis. Chapman; Hall/CRC.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using r. Vol. 565. John Wiley & Sons.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2011. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Springer Science & Business Media."
  },
  {
    "objectID": "accessing1.html",
    "href": "accessing1.html",
    "title": "Survey data sources",
    "section": "",
    "text": "The tutorial discusses complex survey data and highlights potential data sources. Key datasets with survey features include the Canadian Community Health Survey (CCHS), the National Health and Nutrition Examination Survey (NHANES), and the European Social Survey (ESS), among others. Many of these sources, like NHANES and ESS, have specific R packages for data retrieval. In addition, there are other data sources such as the Vanderbilt Biostatistics Datasets and the World Bank Open Data, with the latter also offering dedicated R packages for data access.\n\nSurvey features\nGenerally survey features include - Strata - Cluster/primary sampling unit (PSU) - Weight, e.g., interview weight, survey weight\n\n\nDataset with survey features\n\nCanadian Community Health Survey - Annual Component CCHS\n\nDownload link UBC library\n\nNational Health and Nutrition Examination Survey NHANES\n\nR packages to download data: nhanesA, RNHANES\n\nNational Longitudinal Study of Adolescent to Adult Health [Add Health], 1994-2008 ICPSR 21600\nEuropean Social Survey ESS\n\nR package to download data: essurvey\n\nBehavioral Risk Factor Surveillance System BRFSS\nBureau of Economic Analysis BEA\nUS National Vital Statistics System NVSS\nDemographic and Health Surveys DHS\n\n\n\nOthers\n\nVanderbilt Biostatistics Datasets link\nWorld Bank Open Data WBOD\n\nR packages to download data: wbstats, WDI"
  },
  {
    "objectID": "accessing1i.html",
    "href": "accessing1i.html",
    "title": "Descriptions",
    "section": "",
    "text": "This tutorial introduces CCHS as a cross-sectional survey that collects health-related data and discusses its objectives and data usage. Additionally, it highlights the survey’s evolution and redesigns. For NHANES, the tutorial covers the importance of the dataset, its sampling procedures, history, data files, and documents. It also discusses how to combine data from different cycles, handle missing data, and deal with outliers. NHIS, another CDC-supported survey, is briefly introduced as a source of annual health-related data. There are some changes in the dataset due to the COVID-19 pandemic, e.g., fewer variables compared to pre-pandemic datasets. Otherwise, the main purpose of these datasets remained the same across survey cycles.\nCCHS\nOverview\nCCHS is a cross-sectional survey that collects vital health-related data, including health status, healthcare utilization, and health determinants, from the Canadian population. Available in both official languages, this survey relies on a substantial sample size to provide reliable estimates at various geographical levels every two years.\nObjectives of the CCHS\nThe CCHS has four primary objectives: supporting health surveillance programs at national, provincial, and intra-provincial levels; offering a single data source for health research on small populations and rare characteristics; providing timely and easily accessible information to a diverse user community; and maintaining flexibility to address emerging health issues within the population.\nData Products and Usage\nThe CCHS generates annual microdata files and combines two years of data for analysis. Users can also combine data from different years to study specific populations or rare characteristics. The data is primarily used for health surveillance and population health research, benefiting federal and provincial health departments, social service agencies, government bodies, and researchers from various fields. Non-profit health organizations and the media also utilize CCHS results to raise awareness about health concerns.\nEvolution and Redesigns\nThe CCHS started collecting data in 2001, transitioning to annual data collection in 2007 with a sample size adjustment to 65,000 respondents per year. It has undergone two significant redesigns to enhance its utility. The 2015 redesign updated sampling methods, adopted a new sample frame, modernized health content, and reviewed the target population. In 2022, the survey underwent another redesign, further updating content and transitioning to an online electronic questionnaire (EQ) for direct self-reporting by selected respondents. Both redesigns involved extensive consultations with stakeholders, including federal, provincial, and territorial partners, health region authorities, and academics.\nNHANES\nThis section covers\n\nIntroduction to the NHANES dataset, highlighting its significance in evaluating the health and nutritional status of U.S. adults and children.\nSampling Procedure details, explaining the multi-stage sampling strategy and emphasizing the importance of using survey features like weights, strata, and primary sampling units for population-level estimates.\nSurvey History with a visualization representing different NHANES survey cycles.\nNHANES Data Files and Documents:\n\n\nExplains the data’s file format, mostly in SAS transport file format (.xpt).\nBreaks down the NHANES components, which include demographics, dietary, examination, laboratory, and questionnaire data.\nProvides guidelines on combining data from different cycles and handling missing data or outliers.\n\nOverview\nNational Center for Health Statistics (NCHS) conducts National Health and Nutrition Examination Survey (NHANES) (CDC,NCHS 2023). These surveys are designed to evaluate the health and nutritional status of U.S. adults and children. These surveys are being administered in two-year cycles or intervals starting from 1999-2000. Prior to 1999, a number of surveys were conducted (e.g., NHANES III), but in our discussion, we will mostly restrict our discussions to continuous NHANES (e.g., NHANES 1999-2000 to NHANES 2017-2018).\n\n\nCDC,NCHS (2023)\nSampling Procedure:\nIt is a probabilistic sample (we know probability of getting selected for all individuals). This sample is unlikely to be representative of the entire population, as some under/oversampling occurs (unlike SRS), and samples may be dependent (due to proximity of some samples). For example, household with the following characteristics may be oversampled in NHANES, e.g., African Americans, Mexican Americans, Low income White Americans, Persons age 60+ years.\n\n\nSampling Procedure:\n\nnot obtained via simple random sample\nmultistage sample designs\nA sample weight is assigned to each sample person where weight = the number of people in the target population represented by that sample person in NHANES\n\nNHANES used multistage sample designs:\n\nStage 1: PSU/clusters = geographically contiguous counties. 50 states - divided into ~3100 counties. Each PSU is assigned to a strata (e.g., urban/rural or PSU size etc.). The counties are randomly/PPS selected using a 2-per-stratum design. Complex sample variance estimation requires PSU + strata (masking involved).\nStage 2: each selected county is broken into segments (with at least ~50-100 housing units). Segments are randomly/PPS selected.\nStage 3: each selected segment is divided into households. Households are randomly selected.\nStage 4: Within each sampled household, an individual is randomly selected.\n\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nSurvey history\nOverall NHANES survey history\n\n\n\n\n\n\n\n\nNHANES datafile and documents\nFile format\nThe Continuous NHANES files are stored in the NHANES website as SAS transport file formats (.xpt). You can import this data in any statistical package that supports this file format.\nContinuous NHANES Components\nContinuous NHANES components separated to reduce the amount of time to download and documentation size:\n\n\nNHANES Tutorials\n\n\n\n\n\n\n\n\n\n\nBroadly, continuous NHANES data are available in 5 categories:\n\nDemographics\nDietary\nExamination\nLaboratory\nQuestionnaire\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHANES. However, NHANES is a cross-sectional data, and identification of the same person accross different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\n\n\nThe following data have not been released on the NHANES website as public release files due to confidentiality concerns:\n\nadolescent data on alcohol use\nsmoking\nsexual behavior\nreproductive health and drug use\n\nWithin the same cycle\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\nMissing data and outliers\nCDC (2023) recommends:\n\n\nCDC (2023)\nKey points on NHANES data analysis and missing data handling:\n\nIf less than 10% of your data for a variable are missing, it’s generally acceptable to proceed with your analysis without further evaluation or adjustment. However, when more than 10% of data is missing, assess if the missing values are evenly distributed across socio-demographic characteristics. Consider options like imputation or adjusted weights if necessary.\nIdentify and treat ‘refusal’ or ‘do not know’ responses as missing data to prevent distorted results in statistical analyses. Recode these responses as missing values, using either a period (.) for numeric variables or a blank for character variables.\nBe cautious about outliers with exceptionally large weights, as they can significantly impact your estimates. Analysts should decide whether to include or exclude these influential outliers from the analysis, taking into account their potential impact on results.\nNHANES documents\n\n\n\n\n\n\n\n\n\n\nThe following websites could be helpful: - For more information about NHANES design.\n\nVisit US CDC website and do a variable keyword search based on your research interest (e.g., arthritis).\n\nNHIS\nLike NHANES, National Health Interview Survey (NHIS) is supported by the CDC and is a large-scale multi-stage cross-sectional survey. The NHIS survey includes information on population disease prevalence, extent of disability, and use of health care services. In contrast to the NHANES that provides data every 2 years, NHIS provides data annually.\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nReferences\n\n\n\n\nCDC. 2023. “NHANES Web Tutorial Frequently Asked Questions (FAQs).” https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/faq.aspx.\n\n\nCDC,NCHS. 2023. “National Health and Nutrition Examination Survey Data.” https://wwwn.cdc.gov/nchs/nhanes/."
  },
  {
    "objectID": "accessing2.html",
    "href": "accessing2.html",
    "title": "Importing CCHS to R",
    "section": "",
    "text": "Overview\nThis section provides comprehensive instructions on how to import the Canadian Community Health Survey (CCHS) dataset from the UBC library site to the RStudio environment. The process starts with downloading the CCHS data from the UBC library site and includes step-by-step visual guides for each stage. Three primary options are provided to process and format the data:\n\nUsing the commercial software SAS.\nUtilizing the free software PSPP, an alternative to SPSS.\nDirectly processing the data in R.\n\nFor each option, users are guided on how to download, install, access, read, save, and check the dataset. The objective is to help users acquire, visualize, and manipulate the CCHS dataset seamlessly using various software applications.\nDownloading CCHS data from UBC\n\n\nStep 1: Go to dvn.library.ubc.ca, and press ‘log-in’\n\n\n\n\n\n\n\n\nStep 2: Select ‘UBC’ from the dropdown menu\n\n\n\n\n\n\n\n\nStep 3: Enter your CWL or UBC library authentication information\n\n\n\n\n\n\n\n\nStep 4: Once you log-in, search the term ‘cchs’ in the search-box\n\n\n\n\n\n\n\n\nStep 5: For illustrative purposes, let us work with the Cycle 3.1 of the CCHS dataset from the list of results. In that case, type ‘cchs 3.1’\n\n\n\n\n\n\n\n\nStep 6: CCHS Cycle 3.1 information\n\n\n\n\n\n\n\n\nStep 7: Choose the ‘Data: CD’ from the menu\n\n\n\n\n\n\n\n\nStep 8: Download the entire data (about 159 MB) as a zip file\n\n\n\n\n\n\n\n\nStep 9: Accept the ‘terms of use’\n\n\n\n\n\n\n\n\nStep 10: Select a directory to download the zip file. The path of the download directory is important (we need to use this path exactly later). For example, below we are in \"C:\\CCHS\\\" folder, but we will create a “Data” folder there, so that the download path is \"C:\\CCHS\\Data\\\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 11: Extract the zip file\n\n\n\n\n\n\n\n\nStep 12: Be patient with the extraction\n\n\n\n\n\n\n\n\nStep 13: Once extraction is complete, take a look at the folders inside. You will see that there is a folder named ‘SAS_SPSS’\n\n\n\n\n\n\nReading and Formatting the data\nOption 1: Processing data using SAS\nSAS is a commercial software. You may be able to get access to educational version. In case you don’t have access to it, later we outline how to use free packages to read these datasets.\n\n\nStep 1: Inside that ‘SAS_SPSS’ folder, find the file hs_pfe.sas. It is a long file, but we are going to work on part of it. First thing we want to do it to change all the directory names to where you have unzipped the downloaded file (for example, here the zip file was extracted to C:/CCHS/Data/cchs_cycle3-1CD/). We only need the first part of the code (as shown below; only related to data ‘hs’). Delete the rest of the codes for now. The resulting code should like like this:\n\n\n%include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_pfe.sas\";\n\ndata hs;\n        %let datafid=\"C:\\CCHS\\Data\\cchs_cycle3-1CD\\Data\\hs.txt\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_fmt.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_lbe.sas\";\nrun;\n\nOnce the modifications are done, submit the codes in SAS. Note that, the name of the data is ‘hs’.\n\n\n\n\n\n\n\nStep 2: Once you submit the code, you can check the log window in SAS to see how the code submission went. It should tell you how many observations and variables were read.\n\n\n\n\n\n\n\n\nStep 3: If you want to view the dataset, you can go to ‘Explorer’ window within SAS.\n\n\n\n\n\n\n\n\nStep 4: Generally, if you haven’t specified where to load the files, SAS will by default save the data into a library called ‘Work’\n\n\n\n\n\n\n\n\nStep 5: Open that folder, and you will be able to find the dataset ‘Hs’.\n\n\n\n\n\n\n\n\nStep 6: Right click on the data, and click ‘open’ to view the datafile.\n\n\n\n\n\n\n\n\nStep 7: To export the data into a CSV format data (so that we can read this data into other software packages), ckick ‘Menu’.\n\n\n\n\n\n\n\n\nStep 8: then press ‘Export Data’.\n\n\n\n\n\n\n\n\nStep 9: choose the library and the data.\n\n\n\n\n\n\n\n\nStep 10: choose the format in which you may want to save the existing data.\n\n\n\n\n\n\n\n\nStep 11: also specify where you want to save the csv file and the name of that file (e.g., cchs3.csv).\n\n\n\n\n\n\n\n\nStep 12: go to that directory to see the file cchs3.csv\n\n\n\n\n\n\n\n\nStep 13: If you want to save the file in SAS format, you can do so by writing the following sas code into the ‘Editor’ window. Here we are saving the data Hs within the Work library in to a data called cchs3 within the SASLib library. Note that, the directory name has to be where you want to save the output file.\n\n\nLIBNAME SASLib \"C:\\CCHS\\Data\";\nDATA SASLib.cchs3;\n    set Work.Hs;\nrun;\n\nSubmit these codes into SAS:\n\n\n\n\n\n\n\nStep 13: go to that directory to see the file cchs3.sas7dbat\n\n\n\n\n\n\nOption 2: Processing data using PSPP (Free)\nPSPP is a free package; alternative to commercial software SPSS. We can use the same SPSS codes to read the datafile into PSPP, and save.\n\n\nStep 1: Get the free PSPP software from the website: www.gnu.org/software/pspp/\n\n\nPSPP is available for GNU/Hurd, GNU/Linux, Darwin (Mac OS X), OpenBSD, NetBSD, FreeBSD, and Windows\n\n\n\n\n\nFor windows, download appropriate version.\n\n\n\n\n\nDownload the file\n\n\n\n\n\nInstall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the icon shorcut after installing\n\n\n\n\n\n\n\nStep 2: Open PSPP\n\n\n\n\n\n\n\n\nStep 3: Go to ‘file’ menu and click ‘open’\n\n\n\n\n\n\n\n\nStep 4: Specify the readfile.sps file from the ‘SAS_SPSS’ folder.\n\n\n\n\n\n\nYou will see the following file:\n\n\n\n\n\n\n\nStep 5: Similar to before, change the directories as appropriate. Get rid of the extra lines of codes. Resulting codes are as follows (you can copy and replace the code in the file with the following codes):\n\n\nfile handle infile/name = 'C:\\CCHS\\Data\\cchs_cycle3-1CD\\DATA\\hs.txt'.\ndata list file = infile notable/.\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvale.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvare.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsmiss.sps\".\nexecute.\n\n\n\n\n\n\nFor Mac users, it should be as follows (e.g., username should be your user name, if you are saving under the path \"/Users/username/CCHS/Data/\"):\n\nfile handle infile/name =\"/Users/username/CCHS/Data/cchs_cycle3-1CD/Data/hs.txt\".\ndata list file = infile notable/.\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hs_i.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvale.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvare.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsmiss.sps\".\n\nexecute.\n\n\n\nStep 6: Run the codes.\n\n\n\n\n\n\n\n\nStep 7: This is a large data, and will take some time to load the data into the PSPP data editor. Be patient.\n\n\n\n\n\n\nOnce loading is complete, it will show the ‘output’ and ‘data view’.\n\n\n\n\n\n\n\n\n\n\nNote that, you will get error message, if your files were not in the correct path. In our example, the path was \"C:\\CCHS\\Data\\\" for the zip file content (see the previous steps).\n\n\nStep 7: You can also check the ‘variable view’.\n\n\n\n\n\n\n\n\nStep 8: Save the data by clicking ‘File’ and then ‘save as …’\n\n\n\n\n\n\n\n\nStep 9: Specify the name of the datafile and the location / folder to save the data file.\n\n\n\n\n\n\n\n\nStep 10: See the SAV file saved in the directory.\n\n\n\n\n\n\n\n\nStep 11: To save CSV format data, use the following syntax.\n\n\nSAVE TRANSLATE\n  /OUTFILE=\"C:/CCHS/Data/cchs3b.csv\"  \n  /TYPE=CSV\n  /FIELDNAMES      \n  /CELLS=VALUES.\n\nNote that, for categorical data, you can either save values or labels. For our purpose, we prefer values, and hence saved with values here.\n\n\n\n\n\n\n\nStep 12: See the CSV file saved in the directory extracted from PSPP.\n\n\n\n\n\n\nOption 3: Processing data using SPSS\nLog into ubc.onthehub.com to download SPSS. With your CWL account, UBC students should be able to download it. UBC IT website for SPSS says:\nThe SPSS software license with UBC specifies that SPSS must only be used by UBC Faculty, Students, and Research Staff and only for Teaching and non-commercial Research purposes related to UBC.\nBoth network (for UBC owened devices) or standalone / home versions (for non-UBC owened devices) should be available. Once downloaded, same process of importing CCHS data in PSPP can also be applied on SPSS (same syntax files should work).\nProcessing data in R\nDownload software\n\n\nStep 1: Download either ‘R’ from CRAN www.r-project.org or ‘R open’ from Microsoft mran.microsoft.com/open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download RStudio from www.rstudio.com/\n\n\n\n\n\n\n\n\n\nStep 3: Open RStudio\n\n\n\n\n\n\nImport, export and load data into R\n\n\nStep 1: Set working directory\n\n\nsetwd(\"C:/CCHS/Data/\") # or something appropriate\n\n\n\nStep 2: Read the dataset created from PSPP with cell values. We can also do a small check to see if the cell values are visible. For example, we choose a variable ‘CCCE_05A’, and tabulate it.\n\n\nHs <- read.csv(\"cchs3b.csv\", header = TRUE)\ntable(Hs$CCCE_05A)\n\n\n\n\n\n\n\n\nStep 3: Save the RData file from R into a folder SurveyData:\n\n\nsave(Hs, file = \"SurveyData/cchs3.RData\")\n\n\n\nStep 4: See the RData file saved in the directory extracted from R.\n\n\n\n\n\n\n\n\nStep 5: Close R / RStudio and restart it. Environment window within RStudio should be empty.\n\n\n\n\n\n\n\n\nStep 6: Load the saved RData into R. Environment window within RStudio should have ‘Hs’ dataset.\n\n\nload(\"SurveyData/cchs3.RData\")"
  },
  {
    "objectID": "accessing3.html",
    "href": "accessing3.html",
    "title": "Importing NHANES to R",
    "section": "",
    "text": "This tutorial provides comprehensive instructions on accessing the National Health and Nutrition Examination Survey (NHANES) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment. It covers accessing NHANES Data:\n\nDirectly from the CDC website: A step-by-step guide with accompanying images, illustrating how to navigate the CDC website, download the data, and interpret the accompanying codebook.\nUsing R packages, specifically the nhanesA package: A concise guide on how to download and get summaries of the NHANES data using this R package.\n\n\n# Load required packages\n#devtools::install_github(\"warnes/SASxport\")\nlibrary(SASxport)\nlibrary(foreign)\nlibrary(nhanesA)\nlibrary(knitr)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nuse.saved.chche <- TRUE\n\n\n\nBefore installing a package from GitHub, it’s better to check whether you installed the right version of Rtools\nAccessing NHANES Data Directly from the CDC website\nIn the following example, we will see how to download ‘Demographics’ data, and check associated variable in that dataset.\n\n\n\n\n\n\n\nNHANES 1999-2000 and onward survey datasets are publicly available at wwwn.cdc.gov/nchs/nhanes/\n\n\nStep 1: Say, for example, we are interested about the NHANES 2015-2016 survey. Clicking the associated link in the above Figure gets us to the page for the corresponding cycle (see below).\n\n\n\n\n\n\n\n\nStep 2: There are various types of data available for this survey. Let’s explore the demographic information from this cycle. These data are mostly available in the form of SAS XPT format (see below).\n\n\n\n\n\n\n\n\nStep 3: We can download the XPT data in the local PC folder and read the data into R as as follows:\n\n\nDEMO <- read.xport(\"Data/accessing/DEMO_I.XPT\")\n\n\n\n\n\n\nStep 4: Once data is imported in RStudio, we will see the DEMO object listed under data window (see below):\n\n\n\n\n\n\n\n\nStep 5: We can also check the variable names in this DEMO dataset as follows:\n\n\nnames(DEMO)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the DEMO data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\nStep 7: There is a column name associated with each column, e.g., DMDHSEDU in the first column in the above Figure. To understand what the column names mean in this Figure, we need to take a look at the codebook. To access codebook, click the 'DEMO|Doc' link (in step 2). This will show the data documentation and associated codebook (see the next Figure).\n\n\n\n\n\n\n\n\nStep 8: We can see a link for the column or variable DMDHSEDU in the table of content (in the above Figure). Clicking that link will provide us further information about what this variable means (see the next Figure).\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count and cumulative (from the above Figure) matches with what we get from the DEMO data we just imported (particularly, for the DMDHSEDU variable):\n\n\ntable(DEMO$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(DEMO$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(DEMO$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\nAccessing NHANES Data Using R Packages\nnhanesA package\n\nlibrary(nhanesA)\n\n\n\n\n\n\n\nTip\n\n\n\nR package nhanesA provides a convenient way to download and analyze NHANES survey data.\n\n\n\n\nRNHANES (Susmann 2016) is another packages for downloading the NHANES data easily.\n\n\nStep 1: Witin the CDC website, NHANES data are available in 5 categories\n\nDemographics (DEMO)\nDietary (DIET)\nExamination (EXAM)\nLaboratory (LAB)\nQuestionnaire (Q)\n\n\n\nTo get a list of available variables within a data file, we run the following command (e.g., we check variable names within DEMO data):\n\nnhanesTables(data_group='DEMO', year=2015)\n\n\n\n  \n\n\n\n\n\nStep 2: We can obtain the summaries of the downloaded data as follows (see below):\n\n\ndemo <- nhanes('DEMO_I')\nnames(demo)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\ntable(demo$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(demo$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(demo$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\nImport data issue\nSometimes, you might see a warning message when downloading NHANES data using an R package. For example, simpleWarning in download.file(url, tf, mode = “wb”, quiet = TRUE): cannot open URL, or 404 data Not Found.\nThe possible reason could be the NHANES server was down when you tried to connect. In that case, try later with the same codes. Also, check the name of the variables carefully. The name of the same variable could be different in different survey cycles. It is also possible that some variables are not available in all cycles.\nReferences\n\n\n\n\nSusmann, Herb. 2016. RNHANES: Facilitates Analysis of CDC NHANES Data. https://CRAN.R-project.org/package=RNHANES."
  },
  {
    "objectID": "accessing4.html#references",
    "href": "accessing4.html#references",
    "title": "Reproducing NHANES results",
    "section": "References",
    "text": "References\n\n\n\n\nDhana, A. 2023. “R & Python for Data Science.” https://datascienceplus.com/.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91."
  },
  {
    "objectID": "accessing5.html",
    "href": "accessing5.html",
    "title": "Importing NHIS to R",
    "section": "",
    "text": "This tutorial provides instructions on accessing the National Health Interview Survey (NHIS) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment.\nNHIS datafile and documents\nThe NHIS files are stored in the NHIS website in different formats. You can import this data in any statistical package that supports these file formats, e.g., ASCII, CSV, SAS.\n\n\nNHIS Data, Questionnaires and Related Documentation\n\nIn the recent NHIS (2019 or later), data are available in 5 categories:\n\nInterview data for adults\nInterview data for children\nImputed income for adults\nImputed income for children\nParadata\n\n\nIn the earlier NHIS (before 2019), data are available in 8 categories:\n\nFamily file\nHousehold file\nPerson file\nChild file\nAdult file\nImputed income\nFunctioning and disability\nParadata\n\n\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHIS. Similar to NHANES, identification of the same person in NHIS across different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\nWithin the same cycle\nWithin NHIS datasets in a given cycle, each sampled person has a household number (HHX), family number (FMX), and a person number within family (FPX). We can create a unique identifier based on these three variables and merge the datasets.\nAccessing NHIS Data\n\n\nNHIS survey datasets are publicly available at https://www.cdc.gov/nchs/nhis/\nUnlike NHANES where a R package is available to download the dataset, NHIS datasets need to be downloaded directly from the CDC website. In the following example, we will see how to download ‘Adult’ data from 2021 NHIS, and check associated variable in that dataset.\n\n\n\n\n\n\n\nStep 1: Say, for example, we are interested to download the adult dataset in the CSV format:\n\n\n\n\n\n\n\n\nStep 2: We can download the data in the local PC folder, unzip it, and then read the data into R as as follows:\n\n\nadult21 <- read.csv(\"Data/accessing/adult21.csv\", header = T)\n\n\n\n\n\n\nStep 3: Once data is imported in RStudio, we will see the adult21 object listed under data window (see below):\n\n\n\n\n\n\n\n\nStep 4: We can check the variable names in this adult21 dataset using the names function.\n\n\nnames(adult21)\n\n\n\nStep 5: We can check how many unique adults are in this adult21 dataset. Note that the HHX variable in the dataset is the unique household identifier, where only one adult per household was selected for interview. We can use this HHX variable to merge adult datafile with other datafiles (e.g., child data).\n\n\nlength(unique(adult21$HHX))\n#> [1] 29482\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the adult21 data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\nStep 7: To understand what the column names mean in this Figure, we need to take a look at the codebook, which is also available on the CDC website:\n\n\n\n\n\n\n\n\nStep 8: We can see a check for the column or variables, e.g., REGION, in the codebook:\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count matches with what we get from the adult21 data we just imported (particularly, for the REGION variable):\n\n\n# Frequency table\ntable(adult21$REGION, useNA = \"always\") \n#> \n#>     1     2     3     4  <NA> \n#>  4775  6327 10731  7649     0\n\nSimilarly, we can download the child data and open it in R:\n\nchild21 <- read.csv(\"Data/accessing/child21.csv\", header = T)\n\n\n\n\n\n\n\n\n\nLet’s check how many unique children are in this child21 dataset:\n\nlength(unique(child21$HHX))\n#> [1] 8261\n\nNow let’s check for the column or variables, e.g., SEX_C, in the codebook:\n\n\n\n\n\nWe can assess if the numbers reported under count matches with what we get from the child21 data we just imported:\n\n# Frequency table\ntable(child21$SEX_C, useNA = \"always\") \n#> \n#>    1    2    7 <NA> \n#> 4257 4002    2    0\n\nMerging within the same cycle\n\n\n\n\n\n\nNote\n\n\n\nWe can use HHX variable to merge different datafiles within the same survey cycle.\n\n\nAs mentioned earlier, HHX variable in the dataset is the unique household identifier. We can use this HHX variable to merge different datafiles within the same survey cycle. Say, we are interested in merging child age (AGEP_C) and sex (SEX_C) variables with the adult datafile. We can use the merge function as follows:\n\ndat <- merge(adult21, child21[,c(\"HHX\", \"AGEP_C\", \"SEX_C\")], by = \"HHX\", all = T)\n\n\n\n\n\n\nAs we can see, there are data from 30,673 unique households, suggesting that not all children are sampled from the same household of sampled adults.\n\nlength(unique(dat$HHX))\n#> [1] 30673\n\nTable 1\nNow we will use the adult21 dataset to create Table 1 with utilizing survey features (i.e., psu, strata, and weights). For that, let us create/recode some variables:\n\n\nIn a following chapter about survey data analysis, we will explain what these survey features mean.\n\n# Heart attack\nadult21$heart.attack <- car::recode(adult21$MIEV_A, \" 2 = 'No'; 1 = 'Yes'; else = NA\", \n                                levels = c(\"No\", \"Yes\"), as.factor = T)\ntable(adult21$heart.attack, useNA = \"always\")\n#> \n#>    No   Yes  <NA> \n#> 28378  1078    26\n\n# Diabetes\nadult21$diabetes <- car::recode(adult21$DIBEV_A, \" 2 = 'No'; 1 = 'Yes'; else = NA\", \n                            levels = c(\"No\", \"Yes\"), as.factor = T)\ntable(adult21$diabetes, useNA = \"always\")\n#> \n#>    No   Yes  <NA> \n#> 26318  3134    30\n\n# Sex\nadult21$sex <- car::recode(adult21$SEX_A, \" '1'='Male'; '2'='Female'; else=NA\", \n                       levels = c(\"Female\", \"Male\"), as.factor = T)\ntable(adult21$sex, useNA = \"always\")\n#> \n#> Female   Male   <NA> \n#>  16102  13378      2\n\n# Pseudo-PSU\nadult21$psu <- adult21$PPSU\nadult21$psu <- as.factor(adult21$psu)\ntable(adult21$psu, useNA = \"always\")\n#> \n#>    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n#> 1952 1599 1141  792  669  461  549  662  629  840  521  359  491  421  405  346 \n#>   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n#>  156  221  348  531  596  656  588  593  383  647  374  294  218   57  310  334 \n#>   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n#>  269  369  410  447  180  279  263   85   70  208  175  224  302  270  396  376 \n#>   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n#>  202  240  303  253  349  249   75   67  257  203  234  327  410  364  252  247 \n#>   65   66   67   68   72   73   74   75   76   77   78   79   80   81   82   87 \n#>  199  149   96   22   65   42  108  131   37   41   31   28   50   46   64   31 \n#>   89   90   91   92   93   97   98   99  100  101  102  103  104  108  109  110 \n#>   81   28  132  170   64   86   45   32  144  128  129  171  117   63   48   13 \n#>  114  127  128  134  139  140  150  151  152  153 <NA> \n#>   54   44   10   29   69   18   46   50   49   24    0\n\n# Pseudo-stratum\nadult21$strata <- adult21$PSTRAT\nadult21$strata <- as.factor(adult21$strata)\ntable(adult21$strata, useNA = \"always\")\n#> \n#>  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115 \n#>  736  714  589  480  499  605  733  748  757  629  623  614  158  914  386  603 \n#>  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131 \n#>  192  678  661  842  549  510  606  306  517  385  633  418  265  801  449  558 \n#>  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147 \n#>  558  563  434  532  595  576  494  370  644  485  460  738  625  368  412  650 \n#>  148  149  150  151 <NA> \n#>  672  531  556 1061    0\n\n# Sampling weight\nadult21$sweight <- adult21$WTFA\nsummary(adult21$sweight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   793.2  4698.3  7402.6  8586.9 10671.1 71378.0\n\n# Drop the missing values associated with Heart attack, Diabetes, Sex\ndat.analytic <- adult21[complete.cases(adult21$heart.attack),]\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$diabetes),]\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$sex),]\ndim(dat.analytic)\n#> [1] 29435   628\n\nFirst, we will create the survey design. Second, we will report Table 1 with heart attack and sex variable, stratified by diabetes.\n\nlibrary(tableone)\nlibrary(survey)\n\n# Indicator in the full data\nadult21$indicator <- 1\nadult21$indicator[adult21$HHX %in% dat.analytic$HHX] <- 0\ntable(adult21$indicator)\n#> \n#>     0     1 \n#> 29435    47\n\n# Survey design\nw.design <- svydesign(id = ~psu, strata = ~strata, weights = ~sweight, data = adult21, nest = T)\n\n# Subset\nw.design0 <- subset(w.design, indicator == 0)\n\n# Table 1\ntab1 <- svyCreateTableOne(var = c(\"heart.attack\", \"sex\"), strata= \"diabetes\", \n                          data = w.design0, test = FALSE)\nprint(tab1)\n#>                         Stratified by diabetes\n#>                          No                  Yes               \n#>   n                      228524605.2         24325386.4        \n#>   heart.attack = Yes (%)   5335722.7 ( 2.3)   2358175.6 ( 9.7) \n#>   sex = Male (%)         109610086.2 (48.0)  12510287.1 (51.4)\n\nRegression analysis\nLet’s run a regression analysis with utilizing survey features.\n\nlibrary(Publish)\n\n# Design-adjusted logistic\nfit1 <- svyglm(I(heart.attack == \"Yes\") ~ diabetes + sex, design = w.design0, family = binomial)\npublish(fit1)\n#>  Variable  Units OddsRatio       CI.95 p-value \n#>  diabetes     No       Ref                     \n#>              Yes      4.42 [3.76;5.21]  <1e-04 \n#>       sex Female       Ref                     \n#>             Male      2.04 [1.75;2.39]  <1e-04"
  },
  {
    "objectID": "accessing7.html",
    "href": "accessing7.html",
    "title": "Linking mortality data",
    "section": "",
    "text": "This tutorial provides instructions on linking public-use US mortality data with the NHANES dataset. One can also follow the same steps to link the mortality data with the NHIS.\nDownload mortality data\nThe public-use mortality data can be downloaded directly from the CDC website. Datasets are available in .dat format, separately for each cycle of NHANES and NHIS.\n\n\n\n\n\nOn the same website, CDC also provided R, SAS, and Stata codes with instructions on how to download the datasets directly from the website.\n\n\n\n\n\nWe can click on the desired survey link to download and save the datasets on our own hard drive. The dataset will be directly downloaded to our specified download folder. Alternatively, we can right-click on the desired survey link and select Save link as...\n\n\n\n\n\nNote that the data file is saved as <survey name>_MORT_2019_PUBLIC.dat. In our example, we downloaded mortality data for the NHANES 2013-14 participants. Hence, the name of the file should be NHANES_2013_2014_MORT_2019_PUBLIC.dat.\n\n\n\n\n\nLink mortality data to NHANES\nLet us link the mortality data to the NHANES 2013-14 cycle. The steps are as follows:\n\nDownload morality data for the NHANES 2013-14 cycle\nLoad the morality data on the R environment\nLoad NHANES 2013-14 cycle\nMerge two datasets using the unique identifier\n\nDownload morality data\nWe can follow the steps described above to download the mortality dataset directly from the CDC website.\nLoad the morality data on the R environment\nTo load the dataset, we can use the read_fwf function from the readr package.\n\nlibrary(readr)\nlibrary(dplyr)\n\ndat.mort <- read_fwf(\n  file = \"Data/accessing/NHANES_2013_2014_MORT_2019_PUBLIC.dat\",\n  col_types = \"iiiiiiii\",\n  fwf_cols(SEQN = c(1,6), \n           eligstat = c(15,15),\n           mortstat = c(16,16),\n           ucod_leading = c(17,19),\n           diabetes = c(20,20),\n           hyperten = c(21,21),\n           permth_int = c(43,45),\n           permth_exm = c(46,48)),\n  na = c(\"\", \".\"))\n\nhead(dat.mort)\n\n\n\n  \n\n\n\nIn the code chuck above,\n\nSEQN: unique identifier for NHANES\n\neligstat: Eligibility Status for Mortality Follow-up\n\n1 = Eligible\n2 = Under age 18, not available for public release\n3 = Ineligible\n\n\n\nmortstat: Mortality Status\n\n0 = Assumed alive\n1 = Assumed deceased\nNA = Ineligible or under age 18\n\n\n\nucod_leading: Underlying Cause of Death\n\n1 = Diseases of heart (I00-I09, I11, I13, I20-I51)\n2 = Malignant neoplasms (C00-C97)\n3 = Chronic lower respiratory diseases (J40-J47)\n4 = Accidents (unintentional injuries) (V01-X59, Y85-Y86)\n5 = Cerebrovascular diseases (I60-I69)\n6 = Alzheimer’s disease (G30)\n7 = Diabetes mellitus (E10-E14)\n8 = Influenza and pneumonia (J09-J18)\n9 = Nephritis, nephrotic syndrome and nephrosis (N00-N07, N17-N19, N25-N27)\n10 = All other causes\nNA = Ineligible, under age 18, assumed alive, or no cause of death data available\n\n\n\ndiabetes: Diabetes Flag from Multiple Cause of Death (MCOD)\n\n0 = No - Condition not listed as a multiple cause of death\n1 = Yes - Condition listed as a multiple cause of death\nNA = Assumed alive, under age 18, ineligible for mortality follow-up, or MCOD not available\n\n\n\nhyperten: Hypertension Flag from Multiple Cause of Death (MCOD)\n\n0 = No - Condition not listed as a multiple cause of death\n1 = Yes - Condition listed as a multiple cause of death\nNA = Assumed alive, under age 18, ineligible for mortality follow-up, or MCOD not available\n\n\npermth_int: Person-Months of Follow-up from NHANES Interview date\npermth_exm: Person-Months of Follow-up from NHANES Mobile Examination Center (MEC) Date\n\nLet us see the basic summary statistics of some variables:\n\n# Mortality Status\ntable(dat.mort$mortstat, useNA = \"always\")\n#> \n#>    0    1 <NA> \n#> 5633  467 4075\n\n# Person-Months of Follow-up from NHANES Interview date\nsummary(dat.mort$permth_int)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    1.00   65.00   72.00   70.34   79.00   85.00    4075\n\n# Underlying Cause of Death\ntable(dat.mort$ucod_leading, useNA = \"always\")\n#> \n#>    1    2    3    4    5    6    7    8    9   10 <NA> \n#>  136   99   24   14   28   17   21    9   16  103 9708\n\nLoad NHANES 2013-14 cycle\nLet the open the NHANES 2013-14 dataset we created in the previous chapter on Reproducing results.\n\n# Load data\nload(\"Data/accessing/analyticNHANES2013.RData\")\nls()\n#> [1] \"analytic.data3\"  \"dat.mort\"        \"has_annotations\" \"merged.data\"\n\n# NHANES 2013-14\nhead(analytic.data3)\n\n\n\n  \n\n\ndim(analytic.data3)\n#> [1] 5455   15\n\nMerge mortality data and NHANES 2013-14 using unique identifier\nLet us merge the mortality and NHANES datasets using the SEQN variable.\n\n# Merge datasets\ndat.nhanes <- merge(analytic.data3, dat.mort, by = \"SEQN\", all.x = T)\ndim(dat.nhanes)\n#> [1] 5455   22\nhead(dat.nhanes)\n\n\n\n  \n\n\n\nTable 1\nNow we will use the dat.nhanes dataset to create Table 1 with utilizing survey features (i.e., psu, strata, and survey weights). First, we will create the survey design. Second, we will report Table 1 with age, sex, race, eligibility, all-cause mortality status, diabetes-related death, hypertension-related death, and follow-up times.\n\nlibrary(tableone)\nlibrary(survey)\n\n# Make eligibility and mortality status as factor variable\nfactor.vars <- c(\"eligstat\", \"mortstat\", \"diabetes\", \"hyperten\")\ndat.nhanes[,factor.vars] <- lapply(dat.nhanes[,factor.vars] , factor)\n\n# Survey design\nw.design <- svydesign(id = ~psu, strata = ~strata, weights = ~survey.weight, \n                      data = dat.nhanes, nest = T)\n\n# Table 1 - unweighted frequency or mean\ntab1a <- CreateTableOne(var = c(\"AgeCat\", \"Gender\", \"Race\", \"eligstat\", \"mortstat\", \n                                \"diabetes\", \"hyperten\", \"permth_int\", \"permth_exm\"),\n                        data = dat.nhanes, includeNA = T)\nprint(tab1a, showAllLevels = T, format = \"f\")\n#>                         \n#>                          level    Overall      \n#>   n                                5455        \n#>   AgeCat                 [0,20)       0        \n#>                          [20,40)   1810        \n#>                          [40,60)   1896        \n#>                          [60,Inf)  1749        \n#>   Gender                 Female    2817        \n#>                          Male      2638        \n#>   Race                   White     2343        \n#>                          Black     1115        \n#>                          Asian      623        \n#>                          Hispanic  1214        \n#>                          <NA>       160        \n#>   eligstat               1         5445        \n#>                          3           10        \n#>   mortstat               0         5030        \n#>                          1          415        \n#>                          <NA>        10        \n#>   diabetes               0          374        \n#>                          1           41        \n#>                          <NA>      5040        \n#>   hyperten               0          344        \n#>                          1           71        \n#>                          <NA>      5040        \n#>   permth_int (mean (SD))          70.40 (12.18)\n#>   permth_exm (mean (SD))          69.49 (12.20)\n\n# Table 1 - weighted percentage or mean\ntab1b <- svyCreateTableOne(var = c(\"AgeCat\", \"Gender\", \"Race\", \"eligstat\", \"mortstat\", \n                                \"diabetes\", \"hyperten\", \"permth_int\", \"permth_exm\"), \n                           data = w.design, includeNA = T)\nprint(tab1b, showAllLevels = T, format = \"p\")\n#>                         \n#>                          level    Overall             \n#>   n                                217464332.1        \n#>   AgeCat (%)             [0,20)            0.0        \n#>                          [20,40)          35.5        \n#>                          [40,60)          37.5        \n#>                          [60,Inf)         27.0        \n#>   Gender (%)             Female           51.4        \n#>                          Male             48.6        \n#>   Race (%)               White            66.1        \n#>                          Black            11.4        \n#>                          Asian             5.2        \n#>                          Hispanic         14.7        \n#>                          <NA>              2.7        \n#>   eligstat (%)           1                99.9        \n#>                          3                 0.1        \n#>   mortstat (%)           0                93.6        \n#>                          1                 6.2        \n#>                          <NA>              0.1        \n#>   diabetes (%)           0                 5.5        \n#>                          1                 0.7        \n#>                          <NA>             93.8        \n#>   hyperten (%)           0                 5.2        \n#>                          1                 1.0        \n#>                          <NA>             93.8        \n#>   permth_int (mean (SD))                 70.71 (11.28)\n#>   permth_exm (mean (SD))                 69.80 (11.31)"
  },
  {
    "objectID": "accessingF.html",
    "href": "accessingF.html",
    "title": "R Functions (A)",
    "section": "",
    "text": "The section introduces a set of R functions useful for accessing and processing complex survey data, providing their descriptions and the packages they belong to.\n\n\n\n\n\n Function_name \n    Package_name \n    Description \n  \n\n\n apply \n    base \n    Applies a function over an array or matrix. \n  \n\n cut \n    base \n    Converts a numeric variable to a factor variable. \n  \n\n merge \n    base/data.table \n    Merges multiple datasets. \n  \n\n names \n    base \n    Retrieves the names of an object. \n  \n\n nhanes \n    nhanesA \n    Downloads a NHANES datafile. \n  \n\n nhanesTables \n    nhanesA \n    Lists available variables within a datafile. \n  \n\n nhanesTranslate \n    nhanesA \n    Encodes categorical variables to match with certain standards, e.g., CDC website. \n  \n\n recode \n    car \n    Recodes a variable. \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "accessingQ.html#live-quiz",
    "href": "accessingQ.html#live-quiz",
    "title": "Quiz (A)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "accessingQ.html#download-quiz",
    "href": "accessingQ.html#download-quiz",
    "title": "Quiz (A)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here. If not downloading immediately, right-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "accessingS.html",
    "href": "accessingS.html",
    "title": "App (A)",
    "section": "",
    "text": "Below is an example of an app that utilizes NHANES demographic datasets following the tutorial materials. Users can tabulate and visualize the data summaries from the downloaded data from a selected NHANES cycle.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveA\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, nhanesA, DataExplorer, Hmisc, dplyr, and tableone packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "accessingE.html#problem-statement",
    "href": "accessingE.html#problem-statement",
    "title": "Exercise (A)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Palis, Marchand, and Oviedo-Joekes (2020), DOI: 10.1080/09638237.2018.1437602.\n\nDownload the CCHS MH topical index\n\nDownload the CCHS MH Data Dictionary"
  },
  {
    "objectID": "accessingE.html#question-1-60-grade",
    "href": "accessingE.html#question-1-60-grade",
    "title": "Exercise (A)",
    "section": "Question 1: [60% grade]",
    "text": "Question 1: [60% grade]\n1(a) Importing dataset\n\n# Importing dataset\nload(\"Data/accessing/cchsMH.RData\")\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper\n\nIdentify the variable needed for eligibility criteria\n\nHint\n\nRead the first paragraph of Analytic sample (page 2) for the eligibility criteria\nEligibility criteria was determined based on only one variable. Only work with ‘YES’ category.\n\n\n# your code here\n\n1(c) Retaining necessary variables\nIn the dataset, retain only the variables associated with outcome measure, explanatory variable, potential confounders and survey weight. There should be eight variables (one outcome, one exposure, five confounders, and one survey weight).\nHere are the steps:\n\nIdentify the outcome variable\nIdentify the explanatory variable\nIdentify the potential confounders\nIdentify the survey weight variable\n\nHint\n\nRead\n\n\nfirst and second paragraphs of Study variables for the outcome, explanatory and confounding variables\nthird paragraph of the Statistical analyses for the survey weights variable.\n\n\nThere were five potential confounders.\nPotentially useful functions for this exercise:\n\n\n%in%\nlevels\nrecode\nsubset\nas.factor\nrelevel\nor dplyr ways: filter, select\n\n\n\n\n\n# your code here\n\n1(d) Creating analytic dataset\nOutcome variable has a category ‘NOT STATED’, but for our analysis, we will omit anyone associated with this category. Similarly, for explanatory variable, we have categories such as DON’T KNOW, REFUSAL and NOT STATED. We will omit anyone with these categories.\n\nAssign missing values for categories such as DON’T KNOW, REFUSAL and NOT STATED.\nRecode the variables as shown in Table 1 in the article. You can use any function/package of your choice. Here is an example (but feel free to use other functions. In R there are many other ways to do this same task.\n\n\n## your code here\n# levels(your.data.frame$your.age.variable) <- \n#   list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n#        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n#        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n#        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n#        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n#        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n#        \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n1(e) Number of columns and variable names\nReport the number of columns in your analytic dataset, and the variable names.\n\n# your code here"
  },
  {
    "objectID": "accessingE.html#question-2-table-1-20-grade",
    "href": "accessingE.html#question-2-table-1-20-grade",
    "title": "Exercise (A)",
    "section": "Question 2: Table 1 [20% grade]",
    "text": "Question 2: Table 1 [20% grade]\nReproduce Table 1 presented in the article (or see below). Omit the ‘Main source of income’ variable from the table. The table you produce should report numbers as follows, with all columns as shown in the table. In other words, the numbers should match.\n\n\n\n\n\n\n\n\n\nSelf-rated Mental Health Variable\nTotal n(%)\nPoor or Fair n(%)\nGood n(%)\nVery good or excellent n(%)\n\n\n\nStudy sample\n2628 (100)\n1002 (38.1)\n885 (33.7)\n741 (28.2)\n\n\nCommunity belonging\n\n\n\n\n\n\n- Very weak\n480 (18.3)\n282 (28.1)\n118 (13.3)a\n80 (10.8)a\n\n\n- Somewhat weak\n857 (32.6)\n358 (35.7)\n309 (34.9)\n190 (25.6)\n\n\n- Somewhat strong\n1005 (38.2)\n288 (28.7)\n362 (40.9)\n355 (47.9)\n\n\n- Very strong\n286 (10.9)\n74 (7.4)a\n96 (10.8)a\n116 (15.7)a\n\n\nSex\n\n\n\n\n\n\n- Females\n1407 (53.5)\n616 (61.5)\n487 (55.0)\n304 (41.0)\n\n\n- Males\n1221 (46.5)\n386 (38.5)\n398 (45.0)\n437 (59.0)\n\n\nAge group\n\n\n\n\n\n\n- 15 to 24 years\n740 (28.2)\n191 (19.1)\n264 (29.8)\n285 (38.5)\n\n\n- 25 to 34 years\n475 (18.1)\n141 (14.1)\n167 (18.9)\n167 (22.5)\n\n\n- 35 to 44 years\n393 (15.0)\n185 (18.5)\n119 (13.4)a\n89 (12.0)a\n\n\n- 45 to 54 years\n438 (16.6)\n220 (22.0)\n139 (15.7)\n79 (10.7)a\n\n\n- 55 to 64 years\n379 (14.4)\n198 (19.7)\n113 (12.8)a\n68 (9.2)a\n\n\n- 65 years or older\n203 (7.7)\n67 (6.6)a\n83 (8.4)a\n53 (7.1)b\n\n\nRace/Ethnicity\n\n\n\n\n\n\n- Non-white\n458 (17.4)\n184 (18.4)\n140 (15.8)\n134 (18.1)\n\n\n- White\n2170 (82.6)\n818 (81.6)\n745 (84.2)\n607 (81.9)\n\n\nMain source of income\n\n\n\n\n\n\n- Employment Income^d\n1054 (40.1)\n289 (28.8)\n386 (43.6)\n379 (51.1)\n\n\n- Worker’s Compensation^e\n160 (6.1)\n91 (9.1)a\n44 (5.0)b\n25 (3.4)c\n\n\n- Senior Benefits^f\n134 (5.1)\n57 (5.7)a\n42 (4.7)b\n35 (4.7)\n\n\n- Other^g\n184 (7.0)\n82 (8.2)a\n60 (6.8)a\n42 (5.7)b\n\n\n- Not applicable^h\n851 (32.4)\n402 (40.1)\n263 (29.7)\n186 (25.1)\n\n\n- Not Stated^i\n245 (9.3)\n81 (8.1)a\n90 (10.2)a\n74 (10.0)\n\n\n\n\\(^a\\) Coefficient of variation between 16.6 and 25.0%. \\(^b\\) Coefficient of variation between 25.1 and 33.3%. \\(^c\\) Coefficient of variation > 33.3%. \\(^d\\) Employment Income: Wages/salaries or self-employment. \\(^e\\) Worker’s compensation: Employment insurance or worker’s compensation or social assistance/welfare. \\(^f\\) Senior Benefits: Benefits from Canada or Quebec Pension Plan or job related retirement pensions, superannuation and annuities or RRSP/RRIF of Old Age Security and Guaranteed Income Supplement. \\(^g\\) Other: Dividends/interest or child tax benefit or child support or alimony or other or no income. \\(^h\\) Not applicable: Respondents who live in a household with only one person. The income variable “main source of personal income” is applicable only to those that live in a household of more than one person. \\(^i\\) Not Stated: Question was not answered (don’t know, refusal, not stated).\n\n# your code here\nrequire(tableone)"
  },
  {
    "objectID": "accessingE.html#question-3-20-grade",
    "href": "accessingE.html#question-3-20-grade",
    "title": "Exercise (A)",
    "section": "Question 3: [20% grade]",
    "text": "Question 3: [20% grade]\n3(a) Subset\nSubset the dataset excluding ‘Very good or excellent’ responses from the self-rated mental health variable\n\n# your code here\n\n3(b) Recode\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\n# your code here\n\n3(c) Regression\nRun a logistic regression model for finding the relationship between community belonging (Reference: Very weak) and self-rated mental health (Reference: Poor) among respondents with mental or substance use disorders. Adjust the model for three confounders: sex, age, and race/ethnicity. Do not need to report summary of the model.\n\n# your code here\n\n3(d) Reporting odds ratio\nReport the odds ratios and associated confidence intervals. Publish or jtools package could be useful to report the odds ratios with confidence intervals.\n\n# your code here\n\n\n\n\n\nPalis, Heather, Kirsten Marchand, and Eugenia Oviedo-Joekes. 2020. “The Relationship Between Sense of Community Belonging and Self-Rated Mental Health Among Canadians with Mental or Substance Use Disorders.” Journal of Mental Health 29 (2): 168–75."
  },
  {
    "objectID": "researchquestion.html#background",
    "href": "researchquestion.html#background",
    "title": "Research questions",
    "section": "Background",
    "text": "Background\nWhen we are starting a research project, one of the first steps is to clearly define your research topic or question. We will primarily focus on two types of research questions:\n\npredictive (predictors predicting one outcome)\nassociational or causal (association between an outcome and an exposure, adjusting for confounders and risk factors for the outcome).\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "researchquestion.html#overview-of-tutorials",
    "href": "researchquestion.html#overview-of-tutorials",
    "title": "Research questions",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nPredictive questions\n\n\nIn the previous chapter, we learned about how to access external data. In this chapter, we will embark on a journey to understand the nuances of different research questions, laying the groundwork for the topics that lie ahead. As we move forward, the next chapter will delve deeper into the challenges associated with causal questions. We will explore the complexities of causal associations and discuss the optimal types of variables to include in adjustment models for accurate treatment effect estimation. Following that, we will transition to a chapter dedicated entirely to predictive questions, shedding light on their unique attributes and the methodologies best suited for addressing them. Join us as we navigate these intricate terrains of research inquiry.\n\nRHC Data\nThe first tutorial serves to educate the user on how to utilize the RHC dataset to answer a predictive research question: developing a prediction model for the length of stay. The tutorial equips users with the skills to clean and process raw data, transforming it into an analyzable format, and introduces concepts that will be foundational for subsequent analysis.\n\n\nData from NHANES Part 1: prepare data Part 2: work with data\nThe second tutorial (part a for downloading and part b for analyzing) provides an in-depth guide on how to build a predictive model for Diastolic blood pressure using the NHANES dataset for the years 2013-14.\n\n\n\nCausal questions\n\nData from CCHS\nThe third tutorial aims to guide a study on the relationship between Osteoarthritis (OA) and cardiovascular diseases (CVD) among Canadian adults from 2001-2005. Utilizing the Canadian Community Health Survey (CCHS) cycle 1.1-3.1, the study intends to explore whether OA increases (more accurately, whether associated with) the risk of developing CVD.\n\n\nData from NHANES\nThe NHANES dataset was analyzed in this fourth tutorial to explore the relationship between health predictors and cholesterol levels (association/causal). After refining the survey design and handling missing data, regression models were built using varying predictors. Standard error computations and p-values were derived, adjusting for the survey’s unique structure.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou will find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\nReference"
  },
  {
    "objectID": "researchquestion0.html#picot-framework",
    "href": "researchquestion0.html#picot-framework",
    "title": "Concepts (Q)",
    "section": "PICOT Framework",
    "text": "PICOT Framework\nThe PICOT framework helps to structure a specific and clear research question by focusing on five key elements:\n\n\n\n\n\n Element \n    Description \n    Example \n  \n\n\n P \n    Population of Interest: Who is the target group you are studying? \n    US adults \n  \n\n I \n    Intervention: What is the main action, treatment, or variable you're looking at? \n    Effect of having rheumatoid arthritis \n  \n\n C \n    Comparison: Are you comparing the intervention against a control group or usual care? \n    People without rheumatoid arthritis \n  \n\n O \n    Outcome of Interest: What specifically do you want to measure? \n    Rate of cardiovascular diseases \n  \n\n T \n    Time Frame: Over what time period will your study take place? \n    1999–2018 \n  \n\n\n\n\n\n\nResearch Question: “In US adults, does having rheumatoid arthritis, compared to those without rheumatoid arthritis, affect the rate of cardiovascular diseases during 1999–2018?” based on Hossain et al. (2022): DOI: 10.1016/j.annepidem.2022.03.005"
  },
  {
    "objectID": "researchquestion0.html#finer-criteria",
    "href": "researchquestion0.html#finer-criteria",
    "title": "Concepts (Q)",
    "section": "FINER Criteria",
    "text": "FINER Criteria\nOnce we have formulated your research question with the help of the PICOT elements, we should evaluate it using the FINER criteria:\n\n\n\n\nFINER Criteria\n \n Element \n    Description \n  \n\n\n F \n    Feasible: Is it possible to conduct this research with available resources? \n  \n\n I \n    Interesting: Is the research question intriguing to the scientific community? \n  \n\n N \n    Novel: Is the question original and not already thoroughly researched? \n  \n\n E \n    Ethical: Is the research ethically sound? \n  \n\n R \n    Relevant: Is the research currently needed or will it fill a gap in existing knowledge? \n  \n\n\n\n\nThe key takeaway is: Use the PICOT and FINER frameworks to guide you in framing a compelling, ethical, and achievable research question."
  },
  {
    "objectID": "researchquestion0.html#sap",
    "href": "researchquestion0.html#sap",
    "title": "Concepts (Q)",
    "section": "SAP",
    "text": "SAP\nA Statistical Analysis Plan (SAP), also referred to as a Data Analysis Plan (DAP) or Reporting Analysis Plan (RAP), is an integral part of research, particularly in randomized controlled trials (RCTs) (Kahan et al. 2020), but also in observational studies (Hiemstra et al. 2019). Here are a few reasons why it is beneficial to pre-plan the SAP for an observational study:\n\nPre-planning an SAP helps define the specific analytical strategies and methods that will be used to answer the research questions. It outlines the techniques for handling data, including\n\n\nthe treatment of missing data, outliers,\nthe use of statistical tests, and\nconfounding adjustment techniques.\n\n\nBy detailing the analysis plan before the data is examined, researchers ensure transparency and reduce the risk of data dredging or p-hacking.\nConfounding is a more pronounced issue in observational studies. Strategies for addressing confounding need to be more elaborate and explicit in observational studies.\n\n\n\nRefer to the ‘Scientific Writing for Health Research’ book chapter for more details and examples for PICOT, FINER and Statistical Analysis Plan (SAP).\n\n\n\n\n\n\nNote\n\n\n\nWe include 2 types of tutorials that emphasize the critical steps of data preparation and analysis tailored to specific research questions, cosidering the PICOT framework. They underscore the importance of refining and cleaning datasets to ensure their suitability for rigorous analytical procedures. The analyses, while rooted in distinct methodologies, converge on the common goal of deriving meaningful insights and ensuring the integrity and validity of the results obtained from the processed analytical data.\n\n\n\n\nData preparation: Merging, reformatting and recategorizing essential variables to create a dataset suitable for analysis, aligning it with the study’s objectives."
  },
  {
    "objectID": "researchquestion0.html#video-lessons",
    "href": "researchquestion0.html#video-lessons",
    "title": "Concepts (Q)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPICOT and FINER\n\n\n\nWhat is included in this Video Lesson:\n\nReferences 0:53\nHow to get an idea about a Research Question? 1:05\nWhy the question need to be good? 2:41\nA framework for defining a research question 5:17\nThink hard about the ‘Outcome’ 14:40\nIs this research doable? 17:57\nOverall Roadmap 19:57\nOther Reference (optional) 21:27\n\nThe timestamps are also included in the YouTube video description.\n\n\n\n\n\n\n\n\n\n\n\n\nSAP\n\n\n\nWhat is included in this Video Lesson:\n\nSAP 0:03\nSAP example from a RCT 1:31\nSAP example from an observational study 4:40\nCode book 15:35\n\nThe timestamps are also included in the YouTube video description."
  },
  {
    "objectID": "researchquestion0.html#video-lesson-slides",
    "href": "researchquestion0.html#video-lesson-slides",
    "title": "Concepts (Q)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "researchquestion0.html#links",
    "href": "researchquestion0.html#links",
    "title": "Concepts (Q)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "researchquestion0.html#references",
    "href": "researchquestion0.html#references",
    "title": "Concepts (Q)",
    "section": "References",
    "text": "References\n\n\n\n\nHiemstra, Bart, Frederik Keus, Jørn Wetterslev, Christian Gluud, and Iwan CC van der Horst. 2019. “DEBATE-Statistical Analysis Plans for Observational Studies.” BMC Medical Research Methodology 19 (1): 1–10.\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nKahan, Brennan C, Tahania Ahmad, Gordon Forbes, and Suzie Cro. 2020. “Public Availability and Adherence to Prespecified Statistical Analysis Approaches Was Low in Published Randomized Trials.” Journal of Clinical Epidemiology 128: 29–34.\n\n\nThabane, Lehana, Tara Thomas, Chenglin Ye, and James Paul. 2009. “Posing the Research Question: Not so Simple.” Canadian Journal of Anesthesia/Journal Canadien d’anesthésie 56 (1): 71–79."
  },
  {
    "objectID": "researchquestion1.html",
    "href": "researchquestion1.html",
    "title": "Predictive question-1",
    "section": "",
    "text": "# Load required packages\nrequire(tableone)\nrequire(Publish)\nrequire(MatchIt)\nrequire(cobalt)\nrequire(ggplot2)\n\nWorking with a Predictive question using RHC\nThis tutorial delves into processing and understanding the right heart catheterization (RHC) dataset, which pertains to patients in the intensive care unit. The dataset is particularly centered around the implications of using RHC in the early phases of care, with a focus on comparing two patient groups: those who received the RHC procedure and those who did not. The key outcome being analyzed is the 30-day survival rate. We will use this as an example to explain how to work with a predictive research question to build the analytic data.\n\n\nLink for the RHC dataset\n(Connors et al. 1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as\n\n\nlength of stay (hospital stay; measured continuously)\n\ndeath within certain period (death at any time up to 180 Days; measured as a binary variable)\n\nThe original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above.\nBut we will use this data as a case study for our prediction modelling. Traditional PICOT framework is designed primarily for clinical questions related to interventions, so when applying it to other areas like predictive modeling, some creative adaptation is needed.\n\n\n\n\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nNot applicable, as we are dealing with a prediction model here\n\n\nC\nNot applicable, as we are dealing with a prediction model here\n\n\nO\nin-hospital mortality\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\n\n\n\nWe are interested in developing a prediction model for the length of stay.\nData download\nData is freely available from Vanderbilt Biostatistics, variable list is available here, and the article is freely available from researchgate.\n\n\nRHC Data amd search for right heart catheterization dataset\n\nVariable list\n\nArticle\n\n\nLet us download the dataset and save it for later use.\n\n# Load the dataset\nObsData <- read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", \n                    header = TRUE)\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhc.RDS\")\n\nCreating analytic dataset\nNow, we show the process of preparing our analytic dataset (i.e., ready to use dataset for our analysis), so that the variables generally match with the way the authors were coded in the original article. Below we show the process of creating the analytic dataset.\nAdd column for outcome: length of stay\n\n# Length of Stay = date of discharge - study admission date\nObsData$Length.of.Stay <- ObsData$dschdte - ObsData$sadmdte\n\n# Length of Stay = date of death - study admission date if date of discharge not available\nObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] <- \n  ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - \n  ObsData$sadmdte[is.na(ObsData$Length.of.Stay)]\n\nRecoding column for outcome: death\n\n\n\n\n\n\nTip\n\n\n\nHere we use the ifelse function to create a categorical variable. Other related functions are cut, car.\n\n\nLet us recode our outcome variable as a binary variable:\n\nObsData$death <- ifelse(ObsData$death == \"Yes\", 1, 0)\n\nRemove unnecessary outcomes\nOur next task is to remove unnecessary outcomes:\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to drop variables from a dataset. E.g., without using any package and using the select function from the dplyr package.\n\n\n\nObsData <- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, \n                            t3d30, dth30, surv2md1))\n\nRemove unnecessary and problematic variables\nNow we will drop unnecessary and problematic variables:\n\nObsData <- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, \n                                     urin1, cat2))\n\nBasic data cleanup\nNow we will do some basic cleanup.\n\n\n\n\n\n\nTip\n\n\n\nWe an use the lapply function to convert all categorical variables to factors at once. Not that a similar function to lapply is sapply. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\n\n# convert all categorical variables to factors\nfactors <- c(\"cat1\", \"ca\", \"death\", \"cardiohx\", \"chfhx\", \n             \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n             \"transhx\", \"amihx\", \"sex\", \"dnr1\", \"ninsclas\", \n             \"resp\", \"card\", \"neuro\", \"gastr\", \"renal\", \"meta\", \n             \"hema\", \"seps\", \"trauma\", \"ortho\", \"race\", \n             \"income\")\nObsData[factors] <- lapply(ObsData[factors], as.factor)\n\n# convert RHC.use (RHC vs. No RHC) to a binary variable\nObsData$RHC.use <- ifelse(ObsData$swang1 == \"RHC\", 1, 0)\nObsData <- dplyr::select(ObsData, !swang1)\n\n# Categorize the variables to match with the original paper\nObsData$age <- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf),\n                   right=FALSE)\nObsData$race <- factor(ObsData$race, \n                       levels=c(\"white\",\"black\",\"other\"))\nObsData$sex <- as.factor(ObsData$sex)\nObsData$sex <- relevel(ObsData$sex, ref = \"Male\")\nObsData$cat1 <- as.factor(ObsData$cat1)\nlevels(ObsData$cat1) <- c(\"ARF\",\"CHF\",\"Other\",\"Other\",\"Other\",\n                          \"Other\",\"Other\",\"MOSF\",\"MOSF\")\nObsData$ca <- as.factor(ObsData$ca)\nlevels(ObsData$ca) <- c(\"Metastatic\",\"None\",\"Localized (Yes)\")\nObsData$ca <- factor(ObsData$ca, levels=c(\"None\", \"Localized (Yes)\",\n                                          \"Metastatic\"))\n\nRename variables\n\n# Rename the variables\nnames(ObsData) <- c(\"Disease.category\", \"Cancer\", \"Death\", \n                    \"Cardiovascular\", \"Congestive.HF\", \n                    \"Dementia\", \"Psychiatric\", \"Pulmonary\", \n                    \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\", \n                    \"Immunosupperssion\", \"Transfer.hx\", \"MI\", \n                    \"age\", \"sex\", \"edu\", \"DASIndex\", \n                    \"APACHE.score\", \"Glasgow.Coma.Score\", \n                    \"blood.pressure\", \"WBC\", \"Heart.rate\",\n                    \"Respiratory.rate\",  \"Temperature\",\n                    \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\", \n                    \"Bilirubin\", \"Creatinine\", \"Sodium\", \n                    \"Potassium\", \"PaCo2\",  \"PH\", \"Weight\", \n                    \"DNR.status\", \"Medical.insurance\", \n                    \"Respiratory.Diag\", \"Cardiovascular.Diag\", \n                    \"Neurological.Diag\", \"Gastrointestinal.Diag\",\n                    \"Renal.Diag\", \"Metabolic.Diag\", \n                    \"Hematologic.Diag\", \"Sepsis.Diag\", \n                    \"Trauma.Diag\", \"Orthopedic.Diag\", \n                    \"race\", \"income\", \n                    \"Length.of.Stay\", \"RHC.use\")\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhcAnalytic.RDS\")\n\nNotations\nlet us introduce with some notations:\n\n\nNotations\nExample in RHC study\n\n\n\n\n\\(Y_1\\): Observed outcome\nlength of stay\n\n\n\n\\(Y_2\\): Observed outcome\ndeath within 3 months\n\n\n\n\\(L\\): Covariates\nSee below\n\n\nBasic data exploration\nDimension\nLet us the how many rows and columns we have:\n\ndim(ObsData)\n#> [1] 5735   52\n\nComprehensive summary\nLet us see the summary statistics of the variables:\n\n\n\n\n\n\nTip\n\n\n\nTo see the comprehensive summary of the variables, we can use the skim function form skimr package or describe function from rms package\n\n\n\nrequire(skimr)\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nDeath\n0\n1\nFALSE\n2\n1: 3722, 0: 2013\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, > $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\nPredictive vs. causal models\nThe focus of current document is predictive models (e.g., predicting a health outcome).\n\n\n\n\n\nThe original article by Connors et al. (1996) focused on the association of\n\n\nConnors et al. (1996)\n\nright heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and\nthe health-outcomes (such as length of stay).\n\n\n\n\n\n\nThen the PICOT table changes as follows:\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nReceiving a right heart catheterization (RHC)\n\n\nC\nNot receiving a right heart catheterization (RHC)\n\n\nO\nlength of stay\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996."
  },
  {
    "objectID": "researchquestion2a.html#saving-data-for-later-use",
    "href": "researchquestion2a.html#saving-data-for-later-use",
    "title": "Predictive question-2a",
    "section": "Saving data for later use",
    "text": "Saving data for later use\nIt’s a good practice to save your data for future reference.\n\nsave(analytic.data, file=\"Data/researchquestion/Analytic2013.RData\")"
  },
  {
    "objectID": "researchquestion2a.html#exercise-try-yourself",
    "href": "researchquestion2a.html#exercise-try-yourself",
    "title": "Predictive question-2a",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nFollow the steps in the exercise section to deepen your understanding and broaden the analysis.\n\nThe following variables were not included in the above analysis, that were included in this paper: try including them and then create the new analytic data:\n\n\neducation level\npoverty income ratio\nSodium intake (mg)\nPotassium intake (mg)\n\n\nDownload the NHANES 2015-2016 and append with the NHANES 2013-2014 analytic data with same variables."
  },
  {
    "objectID": "researchquestion2a.html#references",
    "href": "researchquestion2a.html#references",
    "title": "Predictive question-2a",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion2b.html#saving-for-further-use",
    "href": "researchquestion2b.html#saving-for-further-use",
    "title": "Predictive question-2b",
    "section": "Saving for further use",
    "text": "Saving for further use\n\nsave(analytic.data1, \n     file = \"Data/researchquestion/NHANESanalytic.Rdata\")"
  },
  {
    "objectID": "researchquestion2b.html#regression-summary-optional",
    "href": "researchquestion2b.html#regression-summary-optional",
    "title": "Predictive question-2b",
    "section": "Regression summary (Optional)",
    "text": "Regression summary (Optional)\n\n\nThis is optional content for this chapter. Later in confounding and predictive factor chapters, we will learn more about adjustment.\nDifferent Generalized Linear Models (GLMs) are fit for diastolic blood pressure using variables like gender, marital status, etc. Below, we used GLMs with the Gaussian family for the continuous outcome diastolic. Note that gaussian is the default family for the glm function. Some other family options include binomial, poisson, quasibinomial, and so on.\nBivariate Regression summary (missing values included)\n\nfit1g <- glm(diastolic ~ gender, data = analytic.data1)\nsummary(fit1g)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -67.579   -7.091    0.421    6.909   50.421  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   71.5789     0.2352 304.299  < 2e-16 ***\n#> genderFemale  -2.4880     0.3278  -7.591 3.76e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 136.3911)\n#> \n#>     Null deviance: 700862  on 5082  degrees of freedom\n#> Residual deviance: 693003  on 5081  degrees of freedom\n#>   (686 observations deleted due to missingness)\n#> AIC: 39415\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\nfit1m <- glm(diastolic ~ marital, data=analytic.data1)\nsummary(fit1m)\n#> \n#> Call:\n#> glm(formula = diastolic ~ marital, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -66.750   -6.838    1.162    7.250   51.250  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                70.7500     0.2138 330.901  < 2e-16 ***\n#> maritalNever married       -1.9116     0.4316  -4.429 9.69e-06 ***\n#> maritalPreviously married  -0.3953     0.4140  -0.955     0.34    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 137.5101)\n#> \n#>     Null deviance: 700840  on 5079  degrees of freedom\n#> Residual deviance: 698139  on 5077  degrees of freedom\n#>   (689 observations deleted due to missingness)\n#> AIC: 39434\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\nstr(analytic.data1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 NA 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 NA 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 NA NA 1 1 NA 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 NA 3 NA 3 1 1 NA ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nfit13 <- glm(diastolic ~ gender + age.centred + race + \n               marital + systolic + smoke + alcohol, \n             data = analytic.data1)\nsummary(fit13)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender + age.centred + race + marital + \n#>     systolic + smoke + alcohol, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -75.142   -6.090    0.811    7.074   33.512  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               30.92372    2.44895  12.627  < 2e-16 ***\n#> genderFemale              -0.34850    0.59830  -0.582 0.560325    \n#> age.centred               -0.13638    0.02142  -6.367 2.56e-10 ***\n#> raceNon-Hispanic Black     1.44736    1.11246   1.301 0.193443    \n#> raceNon-Hispanic White     0.59565    0.96117   0.620 0.535540    \n#> raceOther Hispanic         1.07369    1.29793   0.827 0.408234    \n#> raceOther race             2.02908    1.22998   1.650 0.099216 .  \n#> maritalNever married      -2.92801    0.79123  -3.701 0.000223 ***\n#> maritalPreviously married  0.44754    0.71911   0.622 0.533804    \n#> systolic                   0.31071    0.01763  17.624  < 2e-16 ***\n#> smokeSome days            -0.42177    0.97853  -0.431 0.666513    \n#> smokeNot at all            0.01796    0.65159   0.028 0.978008    \n#> alcohol                    0.17287    0.10994   1.572 0.116060    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 117.7142)\n#> \n#>     Null deviance: 219477  on 1515  degrees of freedom\n#> Residual deviance: 176924  on 1503  degrees of freedom\n#>   (4253 observations deleted due to missingness)\n#> AIC: 11546\n#> \n#> Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "researchquestion2b.html#check-missingness-optional",
    "href": "researchquestion2b.html#check-missingness-optional",
    "title": "Predictive question-2b",
    "section": "Check missingness (optional)",
    "text": "Check missingness (optional)\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nThe plot_missing() function from the DataExplorer package is used to plot missing data.\n\nrequire(DataExplorer)\nplot_missing(analytic.data1)\n\n\n\n\n\nrequire(\"tableone\")\nvars = c(\"systolic\", \"smoke\", \"diastolic\", \"race\", \n         \"age.centred\", \"gender\", \"marital\", \"alcohol\")\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\nSetting correct variable types\nThe variables are explicitly set to either numeric or factor types.\nNote: In case any of the variables types are wrong, your table 1 output will be wrong. Better to be sure about what type of variable you want them to be (numeric or factor). For example, systolic should be numeric. Is it defined that way?\n\nmode(analytic.data1$systolic)\n#> [1] \"numeric\"\n\nIn case it wasn’t (often they can get converted to character), then here is the solution:\n\n# solution 1: one-by-one\nanalytic.data1$systolic <- \n  as.numeric(as.character(analytic.data1$systolic))\nsummary(analytic.data1$systolic)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   110.0   120.0   123.2   134.0   228.0     658\n\n\n# solution 2: fixing all variable types at once\nnumeric.names <- c(\"systolic\", \"diastolic\", \n                   \"age.centred\", \"alcohol\")\nfactor.names <- vars[!vars %in% numeric.names]\nfactor.names\n#> [1] \"smoke\"   \"race\"    \"gender\"  \"marital\"\nanalytic.data1[,factor.names] <- \n  lapply(analytic.data1[,factor.names] , factor)\nanalytic.data1[numeric.names] <- \n  apply(X = analytic.data1[numeric.names], MARGIN = 2, \n        FUN =function (x) as.numeric(as.character(x)))\nlevels(analytic.data1$marital)\n#> [1] \"Married\"            \"Never married\"      \"Previously married\"\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\nComplete case analysis\nRemoves all rows containing NA.\n\ndim(analytic.data1)\n#> [1] 5769   14\nanalytic.data2 <- as.data.frame(na.omit(analytic.data1))\ndim(analytic.data2)\n#> [1] 1516   14\nplot_missing(analytic.data2)\n\n\n\n\n\ntab1 <- CreateTableOne(data = analytic.data2, includeNA = TRUE, \n               vars = vars)\nprint(tab1)\n#>                          \n#>                           Overall       \n#>   n                         1516        \n#>   systolic (mean (SD))    123.29 (17.58)\n#>   smoke (%)                             \n#>      Every day               590 (38.9) \n#>      Some days               159 (10.5) \n#>      Not at all              767 (50.6) \n#>   diastolic (mean (SD))    70.11 (12.04)\n#>   race (%)                              \n#>      Mexican American        162 (10.7) \n#>      Non-Hispanic Black      292 (19.3) \n#>      Non-Hispanic White      778 (51.3) \n#>      Other Hispanic          126 ( 8.3) \n#>      Other race              158 (10.4) \n#>   age.centred (mean (SD))  -0.76 (16.71)\n#>   gender = Female (%)        626 (41.3) \n#>   marital (%)                           \n#>      Married                 858 (56.6) \n#>      Never married           300 (19.8) \n#>      Previously married      358 (23.6) \n#>   alcohol (mean (SD))       3.15 (2.76)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\nWe can export the tableone to a csv file as follows:\n\ntabl1p <- print(tab1)\n#>                          \n#>                           Overall       \n#>   n                         1516        \n#>   systolic (mean (SD))    123.29 (17.58)\n#>   smoke (%)                             \n#>      Every day               590 (38.9) \n#>      Some days               159 (10.5) \n#>      Not at all              767 (50.6) \n#>   diastolic (mean (SD))    70.11 (12.04)\n#>   race (%)                              \n#>      Mexican American        162 (10.7) \n#>      Non-Hispanic Black      292 (19.3) \n#>      Non-Hispanic White      778 (51.3) \n#>      Other Hispanic          126 ( 8.3) \n#>      Other race              158 (10.4) \n#>   age.centred (mean (SD))  -0.76 (16.71)\n#>   gender = Female (%)        626 (41.3) \n#>   marital (%)                           \n#>      Married                 858 (56.6) \n#>      Never married           300 (19.8) \n#>      Previously married      358 (23.6) \n#>   alcohol (mean (SD))       3.15 (2.76)\nwrite.csv(tabl1p, file = \"Data/researchquestion/table1.csv\")\n\n\nfit23 <- glm(diastolic ~ gender + age.centred + race + \n               marital + systolic + smoke + alcohol, \n             data = analytic.data2)\nrequire(Publish)\npublish(fit23)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          30.92 [26.12;35.72]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -0.35  [-1.52;0.82]   0.5603254 \n#>  age.centred                          -0.14 [-0.18;-0.09]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        1.45  [-0.73;3.63]   0.1934428 \n#>              Non-Hispanic White        0.60  [-1.29;2.48]   0.5355396 \n#>                  Other Hispanic        1.07  [-1.47;3.62]   0.4082336 \n#>                      Other race        2.03  [-0.38;4.44]   0.0992165 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.93 [-4.48;-1.38]   0.0002229 \n#>              Previously married        0.45  [-0.96;1.86]   0.5338035 \n#>     systolic                           0.31   [0.28;0.35]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.42  [-2.34;1.50]   0.6665127 \n#>                      Not at all        0.02  [-1.26;1.30]   0.9780080 \n#>      alcohol                           0.17  [-0.04;0.39]   0.1160603\n\nImputed data\nWe will learn about proper missing data analysis at a latter class. Currently, we will do a simple (but rather controversial) single imputation. In here we are simply using a random sampling to impute (probably the worst method, but we are just filling in some gaps for now).\n\nrequire(mice)\nimputation1 <- mice(analytic.data1,\n                   method = \"sample\",  \n                   m = 1, # Number of multiple imputations\n                   maxit = 1 # #iteration; mostly useful for convergence\n                   )\n#> \n#>  iter imp variable\n#>   1   1  systolic  diastolic  marital  alcohol  smoke\n#> Warning: Number of logged events: 5\nanalytic.data.imputation1 <- complete(imputation1)\ndim(analytic.data.imputation1)\n#> [1] 5769   14\nstr(analytic.data.imputation1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 180 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 84 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 2 1 1 1 2 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 3 3 3 3 1 1 1 ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nplot_missing(analytic.data.imputation1)\n\n\n\n\n\nCreateTableOne(data = analytic.data.imputation1, includeNA = TRUE,\n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.17 (18.14)\n#>   smoke (%)                             \n#>      Every day              2196 (38.1) \n#>      Some days               531 ( 9.2) \n#>      Not at all             3042 (52.7) \n#>   diastolic (mean (SD))    70.33 (11.76)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3384 (58.7) \n#>      Never married          1112 (19.3) \n#>      Previously married     1273 (22.1) \n#>   alcohol (mean (SD))       2.67 (2.36)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\nfit23i <- glm(diastolic ~ gender + age.centred + race + \n                marital + systolic + smoke + alcohol, \n              data = analytic.data.imputation1)\npublish(fit23i)\n#>     Variable              Units Coefficient         CI.95    p-value \n#>  (Intercept)                          39.78 [37.44;42.13]    < 1e-04 \n#>       gender               Male         Ref                          \n#>                          Female       -1.41 [-1.99;-0.83]    < 1e-04 \n#>  age.centred                          -0.11 [-0.13;-0.09]    < 1e-04 \n#>         race   Mexican American         Ref                          \n#>              Non-Hispanic Black        0.75  [-0.26;1.76]   0.146424 \n#>              Non-Hispanic White        0.50  [-0.39;1.39]   0.271641 \n#>                  Other Hispanic        0.51  [-0.71;1.73]   0.413715 \n#>                      Other race        1.43   [0.36;2.50]   0.008652 \n#>      marital            Married         Ref                          \n#>                   Never married       -2.73 [-3.53;-1.92]    < 1e-04 \n#>              Previously married       -0.31  [-1.05;0.43]   0.410962 \n#>     systolic                           0.25   [0.24;0.27]    < 1e-04 \n#>        smoke          Every day         Ref                          \n#>                       Some days       -0.32  [-1.35;0.72]   0.550088 \n#>                      Not at all       -0.32  [-0.93;0.28]   0.297969 \n#>      alcohol                           0.10  [-0.02;0.22]   0.101564\n\nWe see some changes in the estimates. After imputing compared to complete case analysis, any changes dramatic (e.g., changing conclusion)?\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\n\nrequire(jtools)\nrequire(ggstance)\nrequire(broom.mixed)\nrequire(huxtable)\nexport_summs(fit23, fit23i)\n\n\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n(Intercept)\n30.92 ***\n39.78 ***\n\n\n\n(2.45)   \n(1.20)   \n\n\ngenderFemale\n-0.35    \n-1.41 ***\n\n\n\n(0.60)   \n(0.29)   \n\n\nage.centred\n-0.14 ***\n-0.11 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nraceNon-Hispanic Black\n1.45    \n0.75    \n\n\n\n(1.11)   \n(0.51)   \n\n\nraceNon-Hispanic White\n0.60    \n0.50    \n\n\n\n(0.96)   \n(0.45)   \n\n\nraceOther Hispanic\n1.07    \n0.51    \n\n\n\n(1.30)   \n(0.62)   \n\n\nraceOther race\n2.03    \n1.43 ** \n\n\n\n(1.23)   \n(0.55)   \n\n\nmaritalNever married\n-2.93 ***\n-2.73 ***\n\n\n\n(0.79)   \n(0.41)   \n\n\nmaritalPreviously married\n0.45    \n-0.31    \n\n\n\n(0.72)   \n(0.38)   \n\n\nsystolic\n0.31 ***\n0.25 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nsmokeSome days\n-0.42    \n-0.32    \n\n\n\n(0.98)   \n(0.53)   \n\n\nsmokeNot at all\n0.02    \n-0.32    \n\n\n\n(0.65)   \n(0.31)   \n\n\nalcohol\n0.17    \n0.10    \n\n\n\n(0.11)   \n(0.06)   \n\n\nN\n1516       \n5769       \n\n\nAIC\n11545.85    \n43937.11    \n\n\nBIC\n11620.38    \n44030.35    \n\n\nPseudo R2\n0.19    \n0.14    \n\n *** p < 0.001;  ** p < 0.01;  * p < 0.05.\n\n\nplot_summs(fit23, fit23i)\n\n\n\n# plot_summs(fit23, fit23i, plot.distributions = TRUE)"
  },
  {
    "objectID": "researchquestion2b.html#exercise-try-yourself",
    "href": "researchquestion2b.html#exercise-try-yourself",
    "title": "Predictive question-2b",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nIn this lab, we have done multiple steps that could be improved. One of them was single imputation by random sampling. What other ad hoc method you could use to impute the factor variables?"
  },
  {
    "objectID": "researchquestion2b.html#references",
    "href": "researchquestion2b.html#references",
    "title": "Predictive question-2b",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "href": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "title": "Causal question-1",
    "section": "Naive Analysis of combined 3 cycles",
    "text": "Naive Analysis of combined 3 cycles\nIn the current analysis, we will simply consider all of the variables under consideration as ‘confounders’, and include in our analysis. Later we will perform a refined analysis.\nSummary of the analytic data\nIncluding missing values\n\ndim(c123sub3)\n#> [1] 241380     17\nanalytic <- c123sub3\ndim(analytic)\n#> [1] 241380     17\n\nrequire(\"tableone\")\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, includeNA = TRUE)\n#>                       \n#>                        Overall       \n#>   n                    241380        \n#>   CVD = event (%)        7044 ( 2.9) \n#>   age (%)                            \n#>      20-39 years       108161 (44.8) \n#>      40-49 years        59690 (24.7) \n#>      50-59 years        52685 (21.8) \n#>      60-64 years        20844 ( 8.6) \n#>   sex = Male (%)       114104 (47.3) \n#>   income (%)                         \n#>      $29,999 or less    48005 (19.9) \n#>      $30,000-$49,999    49496 (20.5) \n#>      $50,000-$79,999    61093 (25.3) \n#>      $80,000 or more    57056 (23.6) \n#>      NA                 25730 (10.7) \n#>   race (%)                           \n#>      Non-white          25840 (10.7) \n#>      White             210307 (87.1) \n#>      NA                  5233 ( 2.2) \n#>   bmicat (%)                         \n#>      Normal            103378 (42.8) \n#>      Overweight        120423 (49.9) \n#>      Underweight         8964 ( 3.7) \n#>      NA                  8615 ( 3.6) \n#>   phyact (%)                         \n#>      Active             57033 (23.6) \n#>      Inactive          117516 (48.7) \n#>      Moderate           60164 (24.9) \n#>      NA                  6667 ( 2.8) \n#>   smoke (%)                          \n#>      Current smoker     71321 (29.5) \n#>      Former smoker      97845 (40.5) \n#>      Never smoker       71397 (29.6) \n#>      NA                   817 ( 0.3) \n#>   fruit (%)                          \n#>      0-3 daily serving  56256 (23.3) \n#>      4-6 daily serving  96177 (39.8) \n#>      6+ daily serving   45861 (19.0) \n#>      NA                 43086 (17.8) \n#>   painmed (%)                        \n#>      No                 11141 ( 4.6) \n#>      Yes                25743 (10.7) \n#>      NA                204496 (84.7) \n#>   ht (%)                             \n#>      No                213432 (88.4) \n#>      Yes                27592 (11.4) \n#>      NA                   356 ( 0.1) \n#>   copd (%)                           \n#>      No                192608 (79.8) \n#>      Yes                 1353 ( 0.6) \n#>      NA                 47419 (19.6) \n#>   diab (%)                           \n#>      No                232486 (96.3) \n#>      Yes                 8811 ( 3.7) \n#>      NA                    83 ( 0.0) \n#>   edu (%)                            \n#>      < 2ndary           37775 (15.6) \n#>      2nd grad.          44376 (18.4) \n#>      Other 2nd grad.    19273 ( 8.0) \n#>      Post-2nd grad.    136031 (56.4) \n#>      NA                  3925 ( 1.6)\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control        OA            p      test\n#>   n                    221029         20351                    \n#>   CVD = event (%)        5429 ( 2.5)   1615 ( 7.9)  <0.001     \n#>   age (%)                                           <0.001     \n#>      20-39 years       106003 (48.0)   2158 (10.6)             \n#>      40-49 years        55569 (25.1)   4121 (20.2)             \n#>      50-59 years        43706 (19.8)   8979 (44.1)             \n#>      60-64 years        15751 ( 7.1)   5093 (25.0)             \n#>   sex = Male (%)       107729 (48.7)   6375 (31.3)  <0.001     \n#>   income (%)                                        <0.001     \n#>      $29,999 or less    42019 (19.0)   5986 (29.4)             \n#>      $30,000-$49,999    45090 (20.4)   4406 (21.7)             \n#>      $50,000-$79,999    56754 (25.7)   4339 (21.3)             \n#>      $80,000 or more    53637 (24.3)   3419 (16.8)             \n#>      NA                 23529 (10.6)   2201 (10.8)             \n#>   race (%)                                          <0.001     \n#>      Non-white          24681 (11.2)   1159 ( 5.7)             \n#>      White             191513 (86.6)  18794 (92.3)             \n#>      NA                  4835 ( 2.2)    398 ( 2.0)             \n#>   bmicat (%)                                        <0.001     \n#>      Normal             96697 (43.7)   6681 (32.8)             \n#>      Overweight        107871 (48.8)  12552 (61.7)             \n#>      Underweight         8490 ( 3.8)    474 ( 2.3)             \n#>      NA                  7971 ( 3.6)    644 ( 3.2)             \n#>   phyact (%)                                        <0.001     \n#>      Active             52942 (24.0)   4091 (20.1)             \n#>      Inactive          106580 (48.2)  10936 (53.7)             \n#>      Moderate           55222 (25.0)   4942 (24.3)             \n#>      NA                  6285 ( 2.8)    382 ( 1.9)             \n#>   smoke (%)                                         <0.001     \n#>      Current smoker     65398 (29.6)   5923 (29.1)             \n#>      Former smoker      88210 (39.9)   9635 (47.3)             \n#>      Never smoker       66663 (30.2)   4734 (23.3)             \n#>      NA                   758 ( 0.3)     59 ( 0.3)             \n#>   fruit (%)                                         <0.001     \n#>      0-3 daily serving  52140 (23.6)   4116 (20.2)             \n#>      4-6 daily serving  87951 (39.8)   8226 (40.4)             \n#>      6+ daily serving   41606 (18.8)   4255 (20.9)             \n#>      NA                 39332 (17.8)   3754 (18.4)             \n#>   painmed (%)                                       <0.001     \n#>      No                 10624 ( 4.8)    517 ( 2.5)             \n#>      Yes                23084 (10.4)   2659 (13.1)             \n#>      NA                187321 (84.7)  17175 (84.4)             \n#>   ht (%)                                            <0.001     \n#>      No                198550 (89.8)  14882 (73.1)             \n#>      Yes                22142 (10.0)   5450 (26.8)             \n#>      NA                   337 ( 0.2)     19 ( 0.1)             \n#>   copd (%)                                          <0.001     \n#>      No                173224 (78.4)  19384 (95.2)             \n#>      Yes                  938 ( 0.4)    415 ( 2.0)             \n#>      NA                 46867 (21.2)    552 ( 2.7)             \n#>   diab (%)                                          <0.001     \n#>      No                213910 (96.8)  18576 (91.3)             \n#>      Yes                 7046 ( 3.2)   1765 ( 8.7)             \n#>      NA                    73 ( 0.0)     10 ( 0.0)             \n#>   edu (%)                                           <0.001     \n#>      < 2ndary           32884 (14.9)   4891 (24.0)             \n#>      2nd grad.          40950 (18.5)   3426 (16.8)             \n#>      Other 2nd grad.    17808 ( 8.1)   1465 ( 7.2)             \n#>      Post-2nd grad.    125772 (56.9)  10259 (50.4)             \n#>      NA                  3615 ( 1.6)    310 ( 1.5)\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLet us investigate why pain medication has so much missing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional content respondent (cycle 3.1):\n\n\n\n\n\nIn cycle 2.1, only 21,755 out of 134,072 responded to optional medication component.\nComplete case analysis\n\ndim(c123sub3)\n#> [1] 241380     17\nanalytic2 <- as.data.frame(na.omit(c123sub3))\ndim(analytic2)\n#> [1] 21623    17\n\n\ntab1 <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, includeNA = TRUE)\nprint(tab1, showAllLevels = TRUE)\n#>              \n#>               level             Overall      \n#>   n                             21623        \n#>   CVD (%)     0 event           20917 (96.7) \n#>               event               706 ( 3.3) \n#>   age (%)     20-39 years        7119 (32.9) \n#>               40-49 years        7024 (32.5) \n#>               50-59 years        5457 (25.2) \n#>               60-64 years        2023 ( 9.4) \n#>   sex (%)     Female            10982 (50.8) \n#>               Male              10641 (49.2) \n#>   income (%)  $29,999 or less    4054 (18.7) \n#>               $30,000-$49,999    4461 (20.6) \n#>               $50,000-$79,999    6600 (30.5) \n#>               $80,000 or more    6508 (30.1) \n#>   race (%)    Non-white          2488 (11.5) \n#>               White             19135 (88.5) \n#>   bmicat (%)  Normal             8993 (41.6) \n#>               Overweight        11739 (54.3) \n#>               Underweight         891 ( 4.1) \n#>   phyact (%)  Active             5502 (25.4) \n#>               Inactive          10495 (48.5) \n#>               Moderate           5626 (26.0) \n#>   smoke (%)   Current smoker     5887 (27.2) \n#>               Former smoker      9368 (43.3) \n#>               Never smoker       6368 (29.5) \n#>   fruit (%)   0-3 daily serving  5806 (26.9) \n#>               4-6 daily serving 10730 (49.6) \n#>               6+ daily serving   5087 (23.5) \n#>   painmed (%) No                 6197 (28.7) \n#>               Yes               15426 (71.3) \n#>   ht (%)      No                19014 (87.9) \n#>               Yes                2609 (12.1) \n#>   copd (%)    No                21475 (99.3) \n#>               Yes                 148 ( 0.7) \n#>   diab (%)    No                20760 (96.0) \n#>               Yes                 863 ( 4.0) \n#>   edu (%)     < 2ndary           2998 (13.9) \n#>               2nd grad.          4605 (21.3) \n#>               Other 2nd grad.    1509 ( 7.0) \n#>               Post-2nd grad.    12511 (57.9)\ntab1b <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, strata = \"OA\", includeNA = TRUE)\nprint(tab1b, showAllLevels = TRUE)\n#>              Stratified by OA\n#>               level             Control       OA           p      test\n#>   n                             19459         2164                    \n#>   CVD (%)     0 event           18917 (97.2)  2000 (92.4)  <0.001     \n#>               event               542 ( 2.8)   164 ( 7.6)             \n#>   age (%)     20-39 years        6915 (35.5)   204 ( 9.4)  <0.001     \n#>               40-49 years        6515 (33.5)   509 (23.5)             \n#>               50-59 years        4504 (23.1)   953 (44.0)             \n#>               60-64 years        1525 ( 7.8)   498 (23.0)             \n#>   sex (%)     Female             9521 (48.9)  1461 (67.5)  <0.001     \n#>               Male               9938 (51.1)   703 (32.5)             \n#>   income (%)  $29,999 or less    3413 (17.5)   641 (29.6)  <0.001     \n#>               $30,000-$49,999    3968 (20.4)   493 (22.8)             \n#>               $50,000-$79,999    6023 (31.0)   577 (26.7)             \n#>               $80,000 or more    6055 (31.1)   453 (20.9)             \n#>   race (%)    Non-white          2370 (12.2)   118 ( 5.5)  <0.001     \n#>               White             17089 (87.8)  2046 (94.5)             \n#>   bmicat (%)  Normal             8277 (42.5)   716 (33.1)  <0.001     \n#>               Overweight        10356 (53.2)  1383 (63.9)             \n#>               Underweight         826 ( 4.2)    65 ( 3.0)             \n#>   phyact (%)  Active             4986 (25.6)   516 (23.8)   0.190     \n#>               Inactive           9417 (48.4)  1078 (49.8)             \n#>               Moderate           5056 (26.0)   570 (26.3)             \n#>   smoke (%)   Current smoker     5247 (27.0)   640 (29.6)  <0.001     \n#>               Former smoker      8363 (43.0)  1005 (46.4)             \n#>               Never smoker       5849 (30.1)   519 (24.0)             \n#>   fruit (%)   0-3 daily serving  5290 (27.2)   516 (23.8)  <0.001     \n#>               4-6 daily serving  9686 (49.8)  1044 (48.2)             \n#>               6+ daily serving   4483 (23.0)   604 (27.9)             \n#>   painmed (%) No                 5859 (30.1)   338 (15.6)  <0.001     \n#>               Yes               13600 (69.9)  1826 (84.4)             \n#>   ht (%)      No                17356 (89.2)  1658 (76.6)  <0.001     \n#>               Yes                2103 (10.8)   506 (23.4)             \n#>   copd (%)    No                19359 (99.5)  2116 (97.8)  <0.001     \n#>               Yes                 100 ( 0.5)    48 ( 2.2)             \n#>   diab (%)    No                18751 (96.4)  2009 (92.8)  <0.001     \n#>               Yes                 708 ( 3.6)   155 ( 7.2)             \n#>   edu (%)     < 2ndary           2527 (13.0)   471 (21.8)  <0.001     \n#>               2nd grad.          4173 (21.4)   432 (20.0)             \n#>               Other 2nd grad.    1364 ( 7.0)   145 ( 6.7)             \n#>               Post-2nd grad.    11395 (58.6)  1116 (51.6)"
  },
  {
    "objectID": "researchquestion3.html#save-data-for-later",
    "href": "researchquestion3.html#save-data-for-later",
    "title": "Causal question-1",
    "section": "Save data for later",
    "text": "Save data for later\n\nsave(analytic, analytic2, cc123a, file = \"Data/researchquestion/OA123CVD.RData\")"
  },
  {
    "objectID": "researchquestion3.html#references",
    "href": "researchquestion3.html#references",
    "title": "Causal question-1",
    "section": "References",
    "text": "References\n\n\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "researchquestion4.html",
    "href": "researchquestion4.html",
    "title": "Causal question-2",
    "section": "",
    "text": "Working with a causal question using NHANES\nWe are interested in exploring the relationship between diabetes (binary exposure variable defined as whether the doctor ever told the participant has diabetes) and cholesterol (binary outcome variable defined as whether total cholesterol is more than 200 mg/dL). Below is the PICOT:\n\n\nPICOT element\nDescription\n\n\n\nP\nUS adults\n\n\nI\nDiabetes\n\n\nC\nNo diabetes\n\n\nO\nTotal cholesterol > 200 mg/dL\n\n\nT\n2017–2018\n\n\n\nFirst, we will prepare the analytic dataset from NHANES 2017–2018.\nSecond, we will work with subset of data to assess the association between diabetes and cholesterol, and to get proper SE and 95% CI for the estimate. We emphasize the correct usage of the survey’s design features (correct handling of survey design elements, such as stratification, clustering, and weighting) to obtain accurate population-level estimates.\n\n# Load required packages\nrequire(SASxport)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(nhanesA)\nrequire(survey)\nrequire(Publish)\nrequire(jtools)\n\nSteps for creating analytic dataset\nWe will combine multiple components (e.g., demographic, blood pressure) using the unique identifier to create our analytic dataset.\n\n\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\n\n\n\nDownload and Subsetting to retain only the useful variables\nSearch literature for the relevant variables, and then see if some of them are available in the NHANES data.\n\n\nPeters, Fabian, and Levy (2014)\nAn an example, let us assume that variables listed in the following figures are known to be useful. Then we will try to indentify, in which NHANES component we have these variables.\n\n\nRefer to the earlier chapter to get a more detailed understanding of how we search for variables within NHANES.\n\n\n\n\n\n\n\nNHANES Data Components:\n\nDemographic (variables like age, gender, income, etc.)\nBlood Pressure (Diastolic and Systolic pressure)\nBody Measures (BMI, Waist Circumference, etc.)\nSmoking Status (Current smoker or not)\nCholesterol (Total cholesterol in different units)\nBiochemistry Profile (Triglycerides, Uric acid, etc.)\nPhysical Activity (Vigorous work and recreational activities)\nDiabetes (Whether the respondent has been told by a doctor that they have diabetes)\n\nDemographic component:\n\ndemo <- nhanes('DEMO_J') # Both males and females 0 YEARS - 150 YEARS\ndemo <- demo[c(\"SEQN\", # Respondent sequence number\n                 \"RIAGENDR\", # gender\n                 \"RIDAGEYR\", # Age in years at screening\n                 \"DMDBORN4\", # Country of birth\n                 \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                 \"DMDEDUC3\", # Education level - Children/Youth 6-19\n                 \"DMDEDUC2\", # Education level - Adults 20+\n                 \"DMDMARTL\", # Marital status: 20 YEARS - 150 YEARS\n                 \"INDHHIN2\", # Total household income\n                 \"WTMEC2YR\", \"SDMVPSU\", \"SDMVSTRA\")]\ndemo_vars <- names(demo) # nhanesTableVars('DEMO', 'DEMO_J', namesonly=TRUE)\ndemo1 <- nhanesTranslate('DEMO_J', demo_vars, data=demo)\n#> Translated columns: RIAGENDR DMDBORN4 RIDRETH3 DMDEDUC3 DMDEDUC2 DMDMARTL INDHHIN2\n\nBlood pressure component:\n\nbpx <- nhanes('BPX_J')\nbpx <- bpx[c(\"SEQN\", # Respondent sequence number\n             \"BPXDI1\", #Diastolic: Blood pres (1st rdg) mm Hg\n             \"BPXSY1\" # Systolic: Blood pres (1st rdg) mm Hg\n             )]\nbpx_vars <- names(bpx) \nbpx1 <- nhanesTranslate('BPX_J', bpx_vars, data=bpx)\n#> Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx): No columns were\n#> translated\n\nBody measure component:\n\nbmi <- nhanes('BMX_J')\nbmi <- bmi[c(\"SEQN\", # Respondent sequence number\n               \"BMXWT\", # Weight (kg) \n               \"BMXHT\", # Standing Height (cm)\n               \"BMXBMI\", # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\n               #\"BMDBMIC\", # BMI Category - Children/Youth # 2 YEARS - 19 YEARS\n               \"BMXWAIST\" # Waist Circumference (cm): 2 YEARS - 150 YEARS\n               )]\nbmi_vars <- names(bmi) \nbmi1 <- nhanesTranslate('BMX_J', bmi_vars, data=bmi)\n#> Warning in nhanesTranslate(\"BMX_J\", bmi_vars, data = bmi): No columns were\n#> translated\n\nSmoking component:\n\nsmq <- nhanes('SMQ_J')\nsmq <- smq[c(\"SEQN\", # Respondent sequence number\n               \"SMQ040\" # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\n               )]\nsmq_vars <- names(smq) \nsmq1 <- nhanesTranslate('SMQ_J', smq_vars, data=smq)\n#> Translated columns: SMQ040\n\n\n# alq <- nhanes('ALQ_J')\n# alq <- alq[c(\"SEQN\", # Respondent sequence number\n#                \"ALQ130\" # Avg # alcoholic drinks/day - past 12 mos\n#                # 18 YEARS - 150 YEARS\n#                )]\n# alq_vars <- names(alq) \n# alq1 <- nhanesTranslate('ALQ_J', alq_vars, data=alq)\n\nCholesterol component:\n\nchl <- nhanes('TCHOL_J') # 6 YEARS - 150 YEARS\nchl <- chl[c(\"SEQN\", # Respondent sequence number\n               \"LBXTC\", # Total Cholesterol (mg/dL)\n               \"LBDTCSI\" # Total Cholesterol (mmol/L)\n               )]\nchl_vars <- names(chl) \nchl1 <- nhanesTranslate('TCHOL_J', chl_vars, data=chl)\n#> Warning in nhanesTranslate(\"TCHOL_J\", chl_vars, data = chl): No columns were\n#> translated\n\nBiochemistry Profile component:\n\ntri <- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\ntri <- tri[c(\"SEQN\", # Respondent sequence number\n               \"LBXSTR\", # Triglycerides, refrig serum (mg/dL)\n               \"LBXSUA\", # Uric acid\n               \"LBXSTP\", # total Protein (g/dL)\n               \"LBXSTB\", # Total Bilirubin (mg/dL)\n               \"LBXSPH\", # Phosphorus (mg/dL)\n               \"LBXSNASI\", # Sodium (mmol/L)\n               \"LBXSKSI\", # Potassium (mmol/L)\n               \"LBXSGB\", # Globulin (g/dL)\n               \"LBXSCA\" # Total Calcium (mg/dL)\n               )]\ntri_vars <- names(tri) \ntri1 <- nhanesTranslate('BIOPRO_J', tri_vars, data=tri)\n#> Warning in nhanesTranslate(\"BIOPRO_J\", tri_vars, data = tri): No columns were\n#> translated\n\nPhysical activity component:\n\npaq <- nhanes('PAQ_J')\npaq <- paq[c(\"SEQN\", # Respondent sequence number\n               \"PAQ605\", # Vigorous work activity \n               \"PAQ650\" # Vigorous recreational activities\n               )]\npaq_vars <- names(paq) \npaq1 <- nhanesTranslate('PAQ_J', paq_vars, data=paq)\n#> Translated columns: PAQ605 PAQ650\n\nDiabetes component:\n\ndiq <- nhanes('DIQ_J')\ndiq <- diq[c(\"SEQN\", # Respondent sequence number\n               \"DIQ010\" # Doctor told you have diabetes\n               )]\ndiq_vars <- names(diq) \ndiq1 <- nhanesTranslate('DIQ_J', diq_vars, data=diq)\n#> Translated columns: DIQ010\n\nMerging all the datasets\n\n\n\n\n\n\nTip\n\n\n\nWe can use the merge or Reduce function to combine the datasets\n\n\n\nanalytic.data7 <- Reduce(function(x,y) merge(x,y,by=\"SEQN\",all=TRUE) ,\n       list(demo1,bpx1,bmi1,smq1,chl1,tri1,paq1,diq1))\ndim(analytic.data7)\n#> [1] 9254   33\n\n\n\nAll these datasets are merged into one analytic dataset using the SEQN as the key. This can be done either all at once using the Reduce function or one by one (using merge once at a time).\n\n# Merging one by one\n# analytic.data0 <- merge(demo1, bpx1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data1 <- merge(analytic.data0, bmi1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data2 <- merge(analytic.data1, smq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data3 <- merge(analytic.data2, alq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data4 <- merge(analytic.data3, chl1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data5 <- merge(analytic.data4, tri1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data6 <- merge(analytic.data5, paq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data7 <- merge(analytic.data6, diq1, by = c(\"SEQN\"), all=TRUE)\n# dim(analytic.data7)\n\nCheck Target population and avoid zero-cell cross-tabulation\n\n\nThe dataset is then filtered to only include adults (20 years and older) and avoid zero-cell cross-tabulation.\nSee that marital status variable was restricted to 20 YEARS - 150 YEARS.\n\nstr(analytic.data7)\n#> 'data.frame':    9254 obs. of  33 variables:\n#>  $ SEQN    : num  93703 93704 93705 93706 93707 ...\n#>  $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 2 1 2 1 1 2 2 2 1 1 ...\n#>  $ RIDAGEYR: num  2 2 66 18 13 66 75 0 56 18 ...\n#>  $ DMDBORN4: Factor w/ 4 levels \"Born in 50 US states or Washingt\",..: 1 1 1 1 1 2 1 1 2 2 ...\n#>  $ RIDRETH3: Factor w/ 6 levels \"Mexican American\",..: 5 3 4 5 6 5 4 3 5 1 ...\n#>  $ DMDEDUC3: Factor w/ 17 levels \"Never attended / kindergarten on\",..: NA NA NA 16 7 NA NA NA NA 13 ...\n#>  $ DMDEDUC2: Factor w/ 7 levels \"Less than 9th grade\",..: NA NA 2 NA NA 1 4 NA 5 NA ...\n#>  $ DMDMARTL: Factor w/ 7 levels \"Married\",\"Widowed\",..: NA NA 3 NA NA 1 2 NA 1 NA ...\n#>  $ INDHHIN2: Factor w/ 16 levels \"$ 0 to $ 4,999\",..: 14 14 3 NA 10 6 2 14 14 4 ...\n#>  $ WTMEC2YR: num  8540 42567 8338 8723 7065 ...\n#>  $ SDMVPSU : num  2 1 2 2 1 2 1 1 2 2 ...\n#>  $ SDMVSTRA: num  145 143 145 134 138 138 136 134 134 147 ...\n#>  $ BPXDI1  : num  NA NA NA 74 38 NA 66 NA 68 68 ...\n#>  $ BPXSY1  : num  NA NA NA 112 128 NA 120 NA 108 112 ...\n#>  $ BMXWT   : num  13.7 13.9 79.5 66.3 45.4 53.5 88.8 10.2 62.1 58.9 ...\n#>  $ BMXHT   : num  88.6 94.2 158.3 175.7 158.4 ...\n#>  $ BMXBMI  : num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>  $ BMXWAIST: num  48.2 50 101.8 79.3 64.1 ...\n#>  $ SMQ040  : Factor w/ 3 levels \"Every day\",\"Some days\",..: NA NA 3 NA NA NA 1 NA NA 2 ...\n#>  $ LBXTC   : num  NA NA 157 148 189 209 176 NA 238 182 ...\n#>  $ LBDTCSI : num  NA NA 4.06 3.83 4.89 5.4 4.55 NA 6.15 4.71 ...\n#>  $ LBXSTR  : num  NA NA 95 92 110 72 132 NA 59 124 ...\n#>  $ LBXSUA  : num  NA NA 5.8 8 5.5 4.5 6.2 NA 4.2 5.8 ...\n#>  $ LBXSTP  : num  NA NA 7.3 7.1 8 7.1 7 NA 7.1 8.1 ...\n#>  $ LBXSTB  : num  NA NA 0.6 0.7 0.7 0.5 0.3 NA 0.3 0.8 ...\n#>  $ LBXSPH  : num  NA NA 4 4 4.3 3.3 3.5 NA 3.4 5.1 ...\n#>  $ LBXSNASI: num  NA NA 141 144 137 144 141 NA 140 141 ...\n#>  $ LBXSKSI : num  NA NA 4 4.4 3.3 4.4 4.1 NA 4.9 4.3 ...\n#>  $ LBXSGB  : num  NA NA 2.9 2.7 2.8 3.2 3.3 NA 3.1 3.3 ...\n#>  $ LBXSCA  : num  NA NA 9.2 9.6 10.1 9.5 9.9 NA 9.4 9.6 ...\n#>  $ PAQ605  : Factor w/ 3 levels \"Yes\",\"No\",\"Don't know\": NA NA 2 2 NA 2 2 NA 2 1 ...\n#>  $ PAQ650  : Factor w/ 2 levels \"Yes\",\"No\": NA NA 2 2 NA 2 2 NA 1 1 ...\n#>  $ DIQ010  : Factor w/ 4 levels \"Yes\",\"No\",\"Borderline\",..: 2 2 2 2 2 3 2 NA 2 2 ...\nhead(analytic.data7)\n\n\n\n  \n\n\nsummary(analytic.data7$RIDAGEYR)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   11.00   31.00   34.33   58.00   80.00\n\n\ndim(analytic.data7)\n#> [1] 9254   33\nanalytic.data8 <- analytic.data7\nanalytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR < 20] <- NA\n#analytic.data8 <- subset(analytic.data7, RIDAGEYR >= 20)\ndim(analytic.data8)\n#> [1] 9254   33\n\nGet rid of variables where target was less than 20 years of age accordingly.\n\nanalytic.data8$DMDEDUC3 <- NULL # not relevant for adults\n#analytic.data8$BMDBMIC <- NULL # not relevant for adults\n\nGet rid of invalid responses\n\n\nVariables that have “Don’t Know” or “Refused” as responses are set to NA, effectively getting rid of invalid responses.\n\nfactor.names <- c(\"RIAGENDR\",\"DMDBORN4\",\"RIDRETH3\",\n                  \"DMDEDUC2\",\"DMDMARTL\",\"INDHHIN2\", \n                  \"SMQ040\", \"PAQ605\", \"PAQ650\", \"DIQ010\")\nnumeric.names <- c(\"SEQN\",\"RIDAGEYR\",\"WTMEC2YR\",\n                   \"SDMVPSU\", \"SDMVSTRA\",\n                   \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"BMXHT\",\n                   \"BMXBMI\", \"BMXWAIST\",\n                   \"ALQ130\", \"LBXTC\", \"LBDTCSI\", \n                   \"LBXSTR\", \"LBXSUA\", \"LBXSTP\", \"LBXSTB\", \n                   \"LBXSPH\", \"LBXSNASI\", \"LBXSKSI\",\n                   \"LBXSGB\",\"LBXSCA\")\nanalytic.data8[factor.names] <- apply(X = analytic.data8[factor.names], \n                                      MARGIN = 2, FUN = as.factor)\n# analytic.data8[numeric.names] <- apply(X = analytic.data8[numeric.names], \n#                                        MARGIN = 2, FUN = \n#                                          function (x) as.numeric(as.character(x)))\n\n\nanalytic.data9 <- analytic.data8\nanalytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == \"Don't Know\"] <- NA\n#analytic.data9 <- subset(analytic.data8, DMDBORN4 != \"Don't Know\")\ndim(analytic.data9)\n#> [1] 9254   32\n\nanalytic.data10 <- analytic.data9\nanalytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == \"Don't Know\"] <- NA\n#analytic.data10 <- subset(analytic.data9, DMDEDUC2 != \"Don't Know\")\ndim(analytic.data10)\n#> [1] 9254   32\n\nanalytic.data11 <- analytic.data10\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Don't Know\"] <- NA\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Refused\"] <- NA\n# analytic.data11 <- subset(analytic.data10, DMDMARTL != \"Don't Know\" & DMDMARTL != \"Refused\")\ndim(analytic.data11)\n#> [1] 9254   32\n\n\nanalytic.data12 <- analytic.data11\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Don't Know\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Refused\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Under $20,000\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"$20,000 and Over\"] <- NA\n# analytic.data12 <- subset(analytic.data11, INDHHIN2 != \"Don't know\" & INDHHIN2 !=  \"Refused\" & INDHHIN2 != \"Under $20,000\" & INDHHIN2 != \"$20,000 and Over\" )\ndim(analytic.data12)\n#> [1] 9254   32\n\n#analytic.data11 <- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )\n#dim(analytic.data11) # this are listed as NA anyway\n\nanalytic.data13 <- analytic.data12\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Don't know\"] <- NA\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Refused\"] <- NA\n# analytic.data13 <- subset(analytic.data12, PAQ605 != \"Don't know\" & PAQ605 != \"Refused\")\ndim(analytic.data13)\n#> [1] 9254   32\n\nanalytic.data14 <- analytic.data13\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Don't know\"] <- NA\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Refused\"] <- NA\n# analytic.data14 <- subset(analytic.data13, PAQ650 != \"Don't Know\" & PAQ650 != \"Refused\")\ndim(analytic.data14)\n#> [1] 9254   32\n\nanalytic.data15 <- analytic.data14\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Don't know\"] <- NA\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Refused\"] <- NA\n# analytic.data15 <- subset(analytic.data14, DIQ010 != \"Don't Know\" & DIQ010 != \"Refused\")\ndim(analytic.data15)\n#> [1] 9254   32\n\n\n# analytic.data15$ALQ130[analytic.data15$ALQ130 > 100] <- NA\n# summary(analytic.data15$ALQ130)\ntable(analytic.data15$SMQ040,useNA = \"always\")\n#> \n#>  Every day Not at all  Some days       <NA> \n#>        805       1338        216       6895\ntable(analytic.data15$PAQ605,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4461 1389 3404\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\n\nRecode values\nLet us recode the variables using the recode function:\n\nrequire(car)\nanalytic.data15$RIDRETH3 <- recode(analytic.data15$RIDRETH3, \n                            \"c('Mexican American','Other Hispanic')='Hispanic'; \n                            'Non-Hispanic White'='White'; \n                            'Non-Hispanic Black'='Black';\n                            c('Non-Hispanic Asian',\n                               'Other Race - Including Multi-Rac')='Other';\n                               else=NA\")\nanalytic.data15$DMDEDUC2 <- recode(analytic.data15$DMDEDUC2, \n                            \"c('Some college or AA degree',\n                             'College graduate or above')='College'; \n                            c('9-11th grade (Includes 12th grad', \n                              'High school graduate/GED or equi')\n                               ='High.School'; \n                            'Less than 9th grade'='School';\n                               else=NA\")\nanalytic.data15$DMDMARTL <- recode(analytic.data15$DMDMARTL, \n                            \"c('Divorced','Separated','Widowed')\n                                ='Previously.married'; \n                            c('Living with partner', 'Married')\n                                ='Married'; \n                            'Never married'='Never.married';\n                               else=NA\")\nanalytic.data15$INDHHIN2 <- recode(analytic.data15$INDHHIN2, \n                            \"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999', \n                                 '$10,000 to $14,999', '$15,000 to $19,999', \n                                 '$20,000 to $24,999')='<25k';\n                            c('$25,000 to $34,999', '$35,000 to $44,999', \n                                 '$45,000 to $54,999') = 'Between.25kto54k';\n                            c('$55,000 to $64,999', '$65,000 to $74,999',\n                                 '$75,000 to $99,999')='Between.55kto99k';\n                            '$100,000 and Over'= 'Over100k';\n                               else=NA\")\nanalytic.data15$SMQ040 <- recode(analytic.data15$SMQ040, \n                            \"'Every day'='Every.day';\n                            'Not at all'='Not.at.all';\n                            'Some days'='Some.days';\n                               else=NA\")\nanalytic.data15$DIQ010 <- recode(analytic.data15$DIQ010, \n                            \"'No'='No';\n                            c('Yes', 'Borderline')='Yes';\n                               else=NA\")\n\n\n\nData types for various variables are set correctly; for instance, factor variables are converted to factor data types, and numeric variables to numeric data types.\nCheck missingness\n\n\n\n\n\n\nTip\n\n\n\nWe can use the plot_missing function to plot the profile of missing values, e.g., the percentage of missing per variable\n\n\n\nrequire(DataExplorer)\nplot_missing(analytic.data15)\n\n\n\n\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nCheck data summaries\n\nnames(analytic.data15)\n#>  [1] \"SEQN\"     \"RIAGENDR\" \"RIDAGEYR\" \"DMDBORN4\" \"RIDRETH3\" \"DMDEDUC2\"\n#>  [7] \"DMDMARTL\" \"INDHHIN2\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BPXDI1\"  \n#> [13] \"BPXSY1\"   \"BMXWT\"    \"BMXHT\"    \"BMXBMI\"   \"BMXWAIST\" \"SMQ040\"  \n#> [19] \"LBXTC\"    \"LBDTCSI\"  \"LBXSTR\"   \"LBXSUA\"   \"LBXSTP\"   \"LBXSTB\"  \n#> [25] \"LBXSPH\"   \"LBXSNASI\" \"LBXSKSI\"  \"LBXSGB\"   \"LBXSCA\"   \"PAQ605\"  \n#> [31] \"PAQ650\"   \"DIQ010\"\nnames(analytic.data15) <- c(\"ID\", \"gender\", \"age\", \"born\", \"race\", \"education\", \n\"married\", \"income\", \"weight\", \"psu\", \"strata\", \"diastolicBP\", \n\"systolicBP\", \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \n\"cholesterol\", \"cholesterolM2\", \"triglycerides\", \n\"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \n\"potassium\", \"globulin\", \"calcium\", \"physical.work\", \n\"physical.recreational\",\"diabetes\")\nrequire(\"tableone\")\nCreateTableOne(data = analytic.data15, includeNA = TRUE)\n#>                                      \n#>                                       Overall            \n#>   n                                       9254           \n#>   ID (mean (SD))                      98329.50 (2671.54) \n#>   gender = Male (%)                       4557 (49.2)    \n#>   age (mean (SD))                        51.50 (17.81)   \n#>   born (%)                                               \n#>      Born in 50 US states or Washingt     7303 (78.9)    \n#>      Others                               1948 (21.1)    \n#>      Refused                                 2 ( 0.0)    \n#>      NA                                      1 ( 0.0)    \n#>   race (%)                                               \n#>      Black                                2115 (22.9)    \n#>      Hispanic                             2187 (23.6)    \n#>      Other                                1802 (19.5)    \n#>      White                                3150 (34.0)    \n#>   education (%)                                          \n#>      College                              3114 (33.7)    \n#>      High.School                          1963 (21.2)    \n#>      School                                479 ( 5.2)    \n#>      NA                                   3698 (40.0)    \n#>   married (%)                                            \n#>      Married                              3252 (35.1)    \n#>      Never.married                        1006 (10.9)    \n#>      Previously.married                   1305 (14.1)    \n#>      NA                                   3691 (39.9)    \n#>   income (%)                                             \n#>      <25k                                 1998 (21.6)    \n#>      Between.25kto54k                     2460 (26.6)    \n#>      Between.55kto99k                     1843 (19.9)    \n#>      Over100k                             1624 (17.5)    \n#>      NA                                   1329 (14.4)    \n#>   weight (mean (SD))                  34670.71 (43344.00)\n#>   psu (mean (SD))                         1.52 (0.50)    \n#>   strata (mean (SD))                    140.97 (4.20)    \n#>   diastolicBP (mean (SD))                67.84 (16.36)   \n#>   systolicBP (mean (SD))                121.33 (19.98)   \n#>   bodyweight (mean (SD))                 65.14 (32.89)   \n#>   bodyheight (mean (SD))                156.59 (22.26)   \n#>   bmi (mean (SD))                        26.58 (8.26)    \n#>   waist (mean (SD))                      89.93 (22.81)   \n#>   smoke (%)                                              \n#>      Every.day                             805 ( 8.7)    \n#>      Not.at.all                           1338 (14.5)    \n#>      Some.days                             216 ( 2.3)    \n#>      NA                                   6895 (74.5)    \n#>   cholesterol (mean (SD))               179.89 (40.60)   \n#>   cholesterolM2 (mean (SD))               4.65 (1.05)    \n#>   triglycerides (mean (SD))             137.44 (109.13)  \n#>   uric.acid (mean (SD))                   5.40 (1.48)    \n#>   protein (mean (SD))                     7.17 (0.44)    \n#>   bilirubin (mean (SD))                   0.46 (0.28)    \n#>   phosphorus (mean (SD))                  3.66 (0.59)    \n#>   sodium (mean (SD))                    140.32 (2.75)    \n#>   potassium (mean (SD))                   4.09 (0.36)    \n#>   globulin (mean (SD))                    3.09 (0.43)    \n#>   calcium (mean (SD))                     9.32 (0.37)    \n#>   physical.work (%)                                      \n#>      No                                   4461 (48.2)    \n#>      Yes                                  1389 (15.0)    \n#>      NA                                   3404 (36.8)    \n#>   physical.recreational (%)                              \n#>      No                                   4422 (47.8)    \n#>      Yes                                  1434 (15.5)    \n#>      NA                                   3398 (36.7)    \n#>   diabetes (%)                                           \n#>      No                                   7816 (84.5)    \n#>      Yes                                  1077 (11.6)    \n#>      NA                                    361 ( 3.9)\n\nCreate complete case data (for now)\n\nanalytic.with.miss <- analytic.data15\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, 1,0)\nanalytic <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic)\n#> [1] 1562   33\n\nCreating Table 1 from the complete case data\n\nrequire(\"tableone\")\nCreateTableOne(data = analytic, includeNA = TRUE)\n#>                                  \n#>                                   Overall            \n#>   n                                   1562           \n#>   ID (mean (SD))                  98344.21 (2697.76) \n#>   gender = Male (%)                    959 (61.4)    \n#>   age (mean (SD))                    53.18 (17.18)   \n#>   born = Others (%)                    299 (19.1)    \n#>   race (%)                                           \n#>      Black                             324 (20.7)    \n#>      Hispanic                          284 (18.2)    \n#>      Other                             228 (14.6)    \n#>      White                             726 (46.5)    \n#>   education (%)                                      \n#>      College                           806 (51.6)    \n#>      High.School                       658 (42.1)    \n#>      School                             98 ( 6.3)    \n#>   married (%)                                        \n#>      Married                           921 (59.0)    \n#>      Never.married                     228 (14.6)    \n#>      Previously.married                413 (26.4)    \n#>   income (%)                                         \n#>      <25k                              484 (31.0)    \n#>      Between.25kto54k                  520 (33.3)    \n#>      Between.55kto99k                  331 (21.2)    \n#>      Over100k                          227 (14.5)    \n#>   weight (mean (SD))              48538.53 (54106.24)\n#>   psu (mean (SD))                     1.48 (0.50)    \n#>   strata (mean (SD))                141.18 (4.07)    \n#>   diastolicBP (mean (SD))            72.06 (14.17)   \n#>   systolicBP (mean (SD))            127.06 (19.11)   \n#>   bodyweight (mean (SD))             85.66 (22.41)   \n#>   bodyheight (mean (SD))            168.96 (9.30)    \n#>   bmi (mean (SD))                    29.96 (7.33)    \n#>   waist (mean (SD))                 102.98 (17.15)   \n#>   smoke (%)                                          \n#>      Every.day                         530 (33.9)    \n#>      Not.at.all                        903 (57.8)    \n#>      Some.days                         129 ( 8.3)    \n#>   cholesterol (mean (SD))           188.77 (43.51)   \n#>   cholesterolM2 (mean (SD))           4.88 (1.13)    \n#>   triglycerides (mean (SD))         154.71 (123.00)  \n#>   uric.acid (mean (SD))               5.62 (1.53)    \n#>   protein (mean (SD))                 7.09 (0.43)    \n#>   bilirubin (mean (SD))               0.46 (0.27)    \n#>   phosphorus (mean (SD))              3.53 (0.54)    \n#>   sodium (mean (SD))                140.14 (2.83)    \n#>   potassium (mean (SD))               4.10 (0.38)    \n#>   globulin (mean (SD))                3.03 (0.44)    \n#>   calcium (mean (SD))                 9.29 (0.37)    \n#>   physical.work = Yes (%)              476 (30.5)    \n#>   physical.recreational = Yes (%)      290 (18.6)    \n#>   diabetes = Yes (%)                   330 (21.1)    \n#>   cholesterol.bin (mean (SD))         0.63 (0.48)\n\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\nSaving data\n\n# getwd()\nsave(analytic.with.miss, analytic, file=\"Data/researchquestion/NHANES17.RData\")\n\nReferences\n\n\n\n\nPeters, Junenette L, M Patricia Fabian, and Jonathan I Levy. 2014. “Combined Impact of Lead, Cadmium, Polychlorinated Biphenyls and Non-Chemical Risk Factors on Blood Pressure in NHANES.” Environmental Research 132: 93–99."
  },
  {
    "objectID": "researchquestionF.html",
    "href": "researchquestionF.html",
    "title": "R functions (Q)",
    "section": "",
    "text": "The list of new R functions introduced in this Research question lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.data.frame \n    base \n    To force an object to a data frame \n  \n\n as.formula \n    base/stats \n    To specify a model formula, e.g., formula for an outcome model \n  \n\n confint \n    base/stats \n    To estimate the confidence interval for model parameters \n  \n\n degf \n    survey \n    To see the degrees of freedom for a survey design object \n  \n\n describe \n    DescTools \n    To see the summary statistics of variables \n  \n\n exp \n    base \n    Exponentials \n  \n\n glm \n    base/stats \n    To run generalized linear models \n  \n\n lapply \n    base \n    To apply a function over a list, e.g., to see the summary of a list of variables or to convert a list of categorical variables to factor variables. A similar function is `sapply`. lapply and sapply have the same functionality. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list. \n  \n\n length \n    base \n    To see the length of an object, e.g., number of elements/observations of a variable \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n publish \n    Publish \n    To show/publish regression tables \n  \n\n Reduce \n    base \n    To combine multiple objects, e.g., datasets \n  \n\n round \n    base \n    To round numeric values \n  \n\n saveRDS \n    base \n    To save a single R object. Similarly, readDRS will read an R object \n  \n\n skim \n    skimr \n    To see the summary statistics of variables \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n unique \n    base \n    To see the number of unique elements \n  \n\n weights \n    base/stats \n    To extract model weights, e.g., see the weights from a pre-specified survey design \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "researchquestionQ.html#live-quiz",
    "href": "researchquestionQ.html#live-quiz",
    "title": "Quiz (Q)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "researchquestionQ.html#download-quiz",
    "href": "researchquestionQ.html#download-quiz",
    "title": "Quiz (Q)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "researchquestionS.html",
    "href": "researchquestionS.html",
    "title": "App (Q)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset from the prediction question tutorial. Users can generate regression outcomes and generate a Table 1 from it based on selected predictors.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveQ\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, Publish, jtools, ggstance, broom.mixed, huxtable, httr and tableone packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "confounding.html#background",
    "href": "confounding.html#background",
    "title": "Causal roles",
    "section": "Background",
    "text": "Background\nThis chapter delves deep into the intricate issues surrounding causal associations, particularly confounding, mediation, and other related biases. In this comprehensive series of tutorials, various aspects of confounding and bias are explored through the lens of Directed Acyclic Graphs (DAGs). Initially, the tutorials guide you through the process of generating large datasets based on these DAGs. They then delve into how the inclusion of different types of variables in adjustment models can skew estimates of treatment effects. We use the R package simcausal in these tutorials to derive empirical estimates from a large dataset.\n\n\nIn the preceding chapter, we delved into the various types of research questions, distinguishing between causal and predictive inquiries. This current chapter focuses primarily on the challenges and intricacies associated with causal questions. Specifically, we will explore which types of variables are most appropriate to incorporate into adjustment models when aiming to estimate treatment effects accurately. In contrast, the subsequent chapter will shift its emphasis towards predictive questions, providing insights into their unique characteristics and considerations.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nWe use the R package simcausal in these tutorials to derive empirical estimates from a large simulated dataset. The simulation is based on data generation based on specified DAGs.\n\n\n\n\nDirected Acyclic Graphs (DAGs) are powerful tools in the realm of causal inference (see the concepts page). They are a type of graphical representation used to depict causal relationships between variables. This visual representation of the causal structure among variables makes it easier to understand and communicate complex causal relationships. They provide a visual framework to understand, represent, and analyze complex causal relationships, ensuring that researchers make informed decisions when trying to answer causal questions."
  },
  {
    "objectID": "confounding.html#overview-of-tutorials",
    "href": "confounding.html#overview-of-tutorials",
    "title": "Causal roles",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nConfounding\nThe first tutorial provides a thorough exploration of confounding, with a particular focus on its impact on treatment effect estimates in large datasets. It emphasizes the importance of properly adjusting for confounders to arrive at accurate estimates.\n\n\nMediator\nThis tutorial focuses on the role of mediator variables in estimating treatment effects. It assesses how adjusting for the mediator influences the estimated treatment effect, exploring both scenarios where the true treatment effect is either non-null or null.\n\n\nCollider\nThis tutorial serves as a practical guide for understanding how the inclusion of colliders can affect the estimation of treatment effects in causal models.\n\n\nZ-bias\nThis tutorial explores the concept of Z-bias, a phenomenon that can lead to misleading estimates of treatment effects in observational studies. It demonstrates how failing to properly adjust or not adjust for instrumental variables can result in biased estimates and compares these with the true treatment effect.\n\n\nCollapsibility\nThis tutorial provides a detailed guide on calculating marginal probabilities and measures of association, including Risk Difference (RD), Risk Ratio (RR), and Odds Ratio (OR). It examines the impact of adjusting for various covariates on these measures, highlighting the concept of “collapsibility.”\n\n\nChange-in-estimate\nThis tutorial focuses on the “Change-in-estimate” concept to understand the impact of various variables on measures of effect. For both continuous and binary outcomes, the tutorial reveals that adding a confounder to the model alters the true treatment effect estimate. Conversely, including a variable that is not a confounder but is a pure risk factor can either change or not change the effect estimate, depending on the type of outcome involved. This nuanced approach aids in understanding how different roles of variables can influence results and interpretations in causal inference.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "confounding0.html#confounding",
    "href": "confounding0.html#confounding",
    "title": "Concepts (R)",
    "section": "Confounding",
    "text": "Confounding\nConfounding is a pervasive concern in epidemiology, especially in observational studies focusing on causality. Epidemiologists need to carefully select confounders to avoid biased results due to third factors affecting the relationship between exposure and outcome. Commonly used methods for selecting confounders, such as change-in-estimator or solely relying on p-value-based statistical methods, may be inadequate or even problematic.\nEpidemiologists need a more formalized system for confounder selection, incorporating causal diagrams and counterfactual reasoning. This includes an understanding of the underlying causal relationships and the potential impacts of different variables on the observed association. Understanding the temporal order and causal pathways is crucial for accurate confounder control.\nHowever, it is possible that epidemiologists may lack comprehensive knowledge about the causal roles of all variables and hence may need to resort to empirical criteria such as the disjunctive cause criterion, or other variable selection methods such as machine learning approaches. While these methods can provide more sophisticated analyses and help address the high dimensionality and complex structures of modern epidemiological data, epidemiologists need to understand how these approaches function, along with their benefits and limitations, to avoid introducing additional bias into the analysis."
  },
  {
    "objectID": "confounding0.html#effect-modifier",
    "href": "confounding0.html#effect-modifier",
    "title": "Concepts (R)",
    "section": "Effect modifier",
    "text": "Effect modifier\nEffect modification and interaction are two distinct concepts in epidemiology. Effect modification occurs when the causal effect of an exposure (A) on an outcome (Y) varies based on the levels of a third factor (B).\nIn this scenario, the association between the exposure and the outcome differs within the strata of a second exposure, which acts as the effect modifier. For instance, the impact of alcohol (A) on oral cancer (Y) might differ based on tobacco smoking (B).\nOn the other hand, interaction refers to the joint causal effect of two exposures (A and B) on an outcome (Y). It examines how the combination of multiple exposures influences the outcome, such as the combined effect of alcohol (A) and tobacco smoking (B) on oral cancer (Y).\nIn essence, while effect modification looks at how a third factor influences the relationship between an exposure and an outcome, interaction focuses on the combined effect of two exposures on the outcome."
  },
  {
    "objectID": "confounding0.html#table-2-fallacy",
    "href": "confounding0.html#table-2-fallacy",
    "title": "Concepts (R)",
    "section": "Table 2 fallacy",
    "text": "Table 2 fallacy\nThe “Table 2 Fallacy” in epidemiology refers to the misleading practice of presenting multiple adjusted effect estimates from a single statistical model in one table, often resulting in misinterpretation. This occurs when researchers report both the primary exposure’s effects and secondary exposures’ (often an adjustment variable for the primary exposure) effects without adequately distinguishing between the types of effects or considering the causal relationships among variables.\nThis idea highlights the potential for misunderstanding in interpreting the effects of various exposures on an outcome when they are reported together, leading to confusion over the nature and magnitude of the relationships and possibly influencing the design and interpretation of further studies (Westreich and Greenland 2013). The fallacy demonstrates the need for careful consideration of the types of effects estimated and reported in statistical models, urging researchers to be clear about the distinctions and implications of controlled direct effects, total effects, and the presence of confounding or mediating variables."
  },
  {
    "objectID": "confounding0.html#reading-list",
    "href": "confounding0.html#reading-list",
    "title": "Concepts (R)",
    "section": "Reading list",
    "text": "Reading list\nConfounding key reference: (VanderWeele 2019)\nEffect modification key reference: (VanderWeele 2009)\nTable 2 fallacy key reference: (Westreich and Greenland 2013)\nOptional reading:\n\n(Tennant et al. 2021)\n(Wright 1921)\n(Greenland, Pearl, and Robins 1999)\n(Lederer et al. 2019)\n(Etminan, Collins, and Mansournia 2020)\n(Heinze, Wallisch, and Dunkler 2018)"
  },
  {
    "objectID": "confounding0.html#video-lessons",
    "href": "confounding0.html#video-lessons",
    "title": "Concepts (R)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPotential outcome framework\n\n\n\nWhat is included in this Video Lesson:\n\n0:00 Introduction\n0:16 Notations\n2:40 Treatment Effect\n6:13 Real-world Problem of the counterfactual definition\n9:44 Real-world Solution in Observational Setting\n\nThe timestamps are also included in the YouTube video description.\n\n\n\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\nThe video lesson split into 3 parts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDAG codes:\n\n\n\nExample DAG codes can be accessed from this GitHub repository folder\n\n\n\n\n\n\n\n\nEmpirical criteria\n\n\n\nWhen complete knowledge of DAG is unavailable\n\n\n\n\n\n\n\n\n\n\n\n\nModelling criteria for variable selection\n\n\n\nMost of these modelling criteria work when we are only dealing with confounders (some important ones, and some less so), or maybe risk factors of the outcomes. But no mediators, or colliders.\n\n\n\n\n\n\n\n\n\n\n\n\nEffect modification vs. interaction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo revisit or deepen your grasp of these two concepts, consider reviewing this external tutorial.\n\n\n\n\n\n\n\n\nTable 2 fallacy"
  },
  {
    "objectID": "confounding0.html#video-lesson-slides",
    "href": "confounding0.html#video-lesson-slides",
    "title": "Concepts (R)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nConfounding\n\n\n\n\nEffect modification\n\n\nTable 2 fallacy"
  },
  {
    "objectID": "confounding0.html#links",
    "href": "confounding0.html#links",
    "title": "Concepts (R)",
    "section": "Links",
    "text": "Links\nConfounding\n\nGoogle Slides\nPDF Slides\n\nEffect modification\n\nGoogle Slides\nPDF Slides\n\nTable 2 fallacy\n\nGoogle Slides\nPDF Slides\nExternal link from dagitty"
  },
  {
    "objectID": "confounding0.html#references",
    "href": "confounding0.html#references",
    "title": "Concepts (R)",
    "section": "References",
    "text": "References\n\n\n\n\nEtminan, Mahyar, Gary S Collins, and Mohammad Ali Mansournia. 2020. “Using Causal Diagrams to Improve the Design and Interpretation of Medical Research.” Chest 158 (1): S21–28.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal Diagrams for Epidemiologic Research.” Epidemiology, 37–48.\n\n\nHeinze, Georg, Christine Wallisch, and Daniela Dunkler. 2018. “Variable Selection–a Review and Recommendations for the Practicing Statistician.” Biometrical Journal 60 (3): 431–49.\n\n\nLederer, David J, Scott C Bell, Richard D Branson, James D Chalmers, Rebecca Marshall, David M Maslove, Peter W Stewart, et al. 2019. “Control of Confounding and Reporting of Results in Causal Inference Studies: Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals.” Annals of the American Thoracic Society 16 (1): 22–28.\n\n\nTennant, Paul W, Elizabeth J Murray, Kathryn F Arnold, Leigh Berrie, Matthew P Fox, Samantha C Gadd, and George TH Ellison. 2021. “Use of Directed Acyclic Graphs (DAGs) to Identify Confounders in Applied Health Research: Review and Recommendations.” International Journal of Epidemiology 50 (2): 620–32.\n\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\n\n———. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34 (3): 211–19.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98.\n\n\nWright, Sewall. 1921. “Correlation and Causation.” J. Agric. Res 20: 557–80."
  },
  {
    "objectID": "confounding1.html",
    "href": "confounding1.html",
    "title": "Confounding",
    "section": "",
    "text": "This tutorial aims to delve into the role of confounding variables in data analysis, especially in the context of big data. We will examine each of these using simulations built on Directed Acyclic Graphs (DAGs). The objective is to understand whether a simple regression adjusting for the confounder variable can correctly estimate treatment effects in such a large sample.\n\n# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n\nBig data: What if we had 1,000,000 (one million) observations? Would that give us true result? Let’s try to answer that using DAGs.\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nAn example of A could be receiving the right heart catheterization (RHC) procedure or not, Y could be the length of hospital stay, and L could be age (see here).\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\nTo perform the lab, we’ll need the simcausal R package. This package may not be available on CRAN but can be installed from the author’s GitHub page.\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = 0.1)\nDset <- set.DAG(D)\n\nNote that rnorm and rbern generate random samples from Normal and Bernoulli distributions, respectively. In the above code chuck, we set the parameters to generate L from Normal distribution with a mean of 10 and a standard deviation of 1. Similarly, we set the parameters to generate A from the Bernoulli distribution with the probability of logit of (-10 + 1.1 \\(\\times\\) L). Finally, we set the parameters to generate Y from the Normal distribution with a mean of (0.5 \\(\\times\\) L + 1.3 \\(\\times\\) A) and a standard deviation of 0.1.\nGenerate DAG\nLet us draw a directed acyclic graph (DAG). Below we use the plotDAG function from the simcausal package. However, we can draw a DAG using DAGitty.\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nAs per the DAG, L is a confounder. When exploring the relationship between A and Y, we need to adjust our model for L to get an unbiased estimate of A on Y.\nGenerate Data\nNow, let us simulate the data using the defined parameters and the DAG. Below, we generate data for 1,000,000 participants. We set a seed 123 so that one can reproduce the same dataset.\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\nNow we will fit the generalized linear model (glm), without and with adjusting for L.\n\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        1.75\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, our true treatment effect is 1.3. When we estimate the relationship between A and Y without adjusting for L, we obtain an estimated effect of 1.75. However, this is not the true effect. The true treatment effect of 1.3 is recovered when we adjust for L.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nLet us see the results when there is no treatment effect, i.s., the true treatment effect is zero.\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 0 * A, sd = .1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        0.45\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         0.0         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this second scenario, the true treatment effect is zero. There is no arrow from A to Y in the DAG, but L remains a common cause for both. Upon analyzing the data without adjusting for L, we observe an induced correlation between A and Y. This correlation disappears, confirming the true null effect, when we adjust for L.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confounding2.html",
    "href": "confounding2.html",
    "title": "Mediator",
    "section": "",
    "text": "Mediators play a crucial role in understanding how a treatment variable affects an outcome. A mediator variable lies in the pathway between the treatment and outcome, essentially transmitting or explaining the effect of the treatment variable. In this expanded tutorial, we’ll delve into more details based on the lecture, specifically focusing on the true direct and indirect effects when a mediator is present.\n\n# Load required packages\nlibrary(simcausal)\n\nLet us consider\n\nM is continuous variable\nA is binary treatment\nY is continuous outcome\n\nAn example of A could be receiving the right heart catheterization (RHC) procedure or not, Y could be the length of hospital stay, and M could be the number of comorbidities.\nNon-null effect\n\nTrue treatment effect = 1.3\n\nOur true treatment effect is 1.3, and the mediator variable’s effect on the outcome Y is 0.5. It’s important to differentiate between these effects.\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nAs we can see, M is a mediator (mediates the effect of A on Y). When exploring the total effect of A on Y, we should not adjust our model for M.\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        1.69\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nYou might notice a total effect that could differ from the true effects. In the lecture example, a crude association showed an effect of 1.69, which is the total effect combining both direct and indirect pathways.\nUpon adjusting for M, the coefficients will show you the direct effect of A on Y and the indirect effect through M. These should align closely with our true effects: a direct effect of 1.3 and an indirect effect of 0.5.\nIn this expanded tutorial, we’ve shown how essential it is to consider mediator variables when estimating treatment effects. We’ve also illustrated how adjusting for mediators allows you to differentiate between true direct and indirect effects, thereby reducing the risk of drawing incorrect conclusions from your data.\nDetailed on the mediation analysis can be found in the Mediation analysis chapter.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M, sd = .1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        0.39\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         0.0         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nTotal Effect: If you want to measure the “total effect” of a treatment on an outcome, then you typically don’t adjust for the mediator. The reason is that the total effect captures both the direct effect of the treatment on the outcome and the indirect effect through the mediator.\nDirect and Indirect Effects: If you want to separate out the direct and indirect effects, then you would adjust for the mediator. In essence, when you control for the mediator, what remains is the direct effect of the treatment on the outcome.\nLinearity and Decomposition: In linear models with continuous outcomes, it is more straightforward to decompose total effects into direct and indirect effects. The mathematics get more complicated in non-linear models or when dealing with non-continuous outcomes."
  },
  {
    "objectID": "confounding3.html",
    "href": "confounding3.html",
    "title": "Collider",
    "section": "",
    "text": "In causal inference, understanding the role of colliders is crucial. A collider is a variable that is a common effect of two or more variables. Adjusting for a collider can introduce bias into your estimates.\n\n# Load required packages\nlibrary(simcausal)\n\nIn a DAG, a collider is a variable influenced by two or more other variables. In our case, L is a collider because it is affected by both A (the treatment) and Y (the outcome). When you adjust for a collider like L, you could introduce bias into your estimates, as demonstrated in the examples below.\nLet us consider\n\nL is continuous variable\nA is binary treatment/exposure\nY is continuous outcome\n\nAn example of A could be a genetic variable (e.g., skin color), Y could be an environmental variable (e.g., indoor air pollution), and L could be disease conditions (e.g., number of comorbidities) (detailed expamles here).\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 1.3 * A, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00        1.29\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00        0.58        0.05\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen not adjusting for L, we recover the true effect close to 1.3. Adjusting for L introduces bias, making the estimate unreliable.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00       -0.01\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00       -0.07        0.05\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen the true effect is null, not adjusting for L shows an estimate close to zero. Adjusting for L moves the estimate away from the null value, introducing bias.\n\n\nEven 1,000,000 observations were not enough to recover true treatment effect! But we are close enough."
  },
  {
    "objectID": "confounding4.html",
    "href": "confounding4.html",
    "title": "Z-bias",
    "section": "",
    "text": "Z-bias occurs in the context of causal inference, specifically when using instrumental variables to estimate causal effects. Instrumental variables (IVs) are used to isolate the variation in the treatment variable that is unrelated to the confounding factors, thus providing a pathway to estimate causal effects.\n\n# Load required packages\nlibrary(simcausal)\n\nContinuous Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rnorm\", mean = -1 + 3 * U + 1.3 * A, sd = 0.1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# True data generating mechanism (unattainable as U is unmeasured)\nfit0 <- glm(Y ~ A + U, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A           U \n#>        -1.0         1.3         3.0\n\n# Unadjusted effect (Z not controlled)\nfit1 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit1),2)\n#> (Intercept)           A \n#>        0.79        5.59\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#>      A \n#> 4.2935\n\n# Adjusted effect (Z  controlled)\nfit2 <- glm(Y ~ A + Z, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           Z \n#>        0.86        5.77       -0.12\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#>        A \n#> 4.465787\n\nBinary Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is binary outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rbern\", prob = plogis(-1 + 3 * U + 1.3 * A))\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\n# True data generating mechanism (unattainable as U is unmeasured)\nfit0 <- glm(Y ~ A + U, family=\"binomial\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A           U \n#>       -0.99        1.30        3.01\n\n# Unadjusted effect (Z not controlled)\nfit1 <- glm(Y ~ A, family=\"binomial\", data=Obs.Data)\nround(coef(fit1),2)\n#> (Intercept)           A \n#>        0.40        3.02\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#>        A \n#> 1.716482\n\n# Adjusted effect (Z  controlled)\nfit2 <- glm(Y ~ A + Z, family=\"binomial\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           Z \n#>        0.51        3.29       -0.18\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#>        A \n#> 1.991396"
  },
  {
    "objectID": "confounding5.html",
    "href": "confounding5.html",
    "title": "Collapsibility",
    "section": "",
    "text": "Let us assume we have a regression of hypertension (\\(Y\\)), smoking (\\(A\\)) and a risk factor for outcome, gender (\\(L\\)). Then let us set up 2 regression models:\n\n1st regression model is \\(Y \\sim \\beta \\times A + \\alpha \\times L\\). Here we are conditioning on gender (\\(L\\)).\n2nd regression model is \\(Y \\sim \\beta' \\times A\\)\n\n\nThen regression is collapsible for \\(\\beta\\) over \\(L\\) if \\(\\beta = \\beta'\\) from the 2nd regression omitting \\(L\\). \\(\\beta \\ne \\beta'\\) would mean non-collapsibility. A measure of association (say, risk difference) is collapsible if the marginal measure of association is equal to a weighted average of the stratum-specific measures of association. Non-collapsibility is also knows as Simpson’s Paradox (in absence of confounding of course): a statistical phenomenon where an association between two factors (say, hypertension and smoking) in a population (we are talking about marginal estimate here) is different than the associations of same relationship in subpopulations (conditional on some other factor, say, age; hence talking about conditional estimates). Risk ratio and risk difference are collapsible.\nOdds ratio can be non-collapsible. It can produce different treatment effect estimate for different covariate adjustment sets (see example below of when adjusting form age and sex vs. when adjusting none). This is true even in the absence of confounding. However, according to our definition here, OR is collapsible when we consider gender in the adjustment set.\nNote that, OR non-collapsibility is a consequence of the fact that it is estimated via a logit link function (nonlinearity of the logistic transformation).\nRef:\n\n(Greenland, Pearl, and Robins 1999)\n(Mansournia and Greenland 2015)\n\n\n# Load required packages\nlibrary(simcausal)\nlibrary(tableone)\nlibrary(Publish)\nlibrary(lawstat)\n\nData generating process\nLet us generate the data with smoking being the exposure and hypertension being the outcome.\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"gender\", distr = \"rbern\", prob = 0.7) +\n  node(\"age\", distr = \"rnorm\", mean = 2, sd = 4) +\n  node(\"smoking\", distr = \"rbern\", prob = plogis(.1)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3.5) * smoking + log(.1) * gender + log(7) * age))\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nObs.Data <- sim(DAG = Dset, n = 100000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nBalance check\n\nrequire(tableone)\nCreateTableOne(data = Obs.Data, strata = \"smoking\", vars = c(\"gender\", \"age\"))\n#>                     Stratified by smoking\n#>                      0            1            p      test\n#>   n                  47720        52280                   \n#>   gender (mean (SD))  0.70 (0.46)  0.70 (0.46)  0.403     \n#>   age (mean (SD))     2.02 (4.02)  2.01 (4.00)  0.690\n\nConditional and crude risk difference (RD)\nSince RD is a collapsible measure, the marginal and conditional estimate should be approximately the same.\nFull list of risk factors for outcome (2 variables)\n\n## RD\nrequire(Publish)\nfitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator …robust variance estimator (or bootstrap) should be used to obtain valid standard errors.”)\nStrtatum specific (2 variables)\n\nfitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nfitx3 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nfitx4 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nround(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"],\n             coef(fitx3)[\"smoking\"],\n             coef(fitx4)[\"smoking\"])),2)\n#> [1] 0.05\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 <- glm(hypertension ~ smoking + gender, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nround(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"])),2)\n#> [1] 0.05\n\nCrude (in absence of confounding)\n\nfitx0 <- glm(hypertension ~ smoking, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nConditional and crude risk ratio (RR)\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk ratio, one may use a GLM with a Poisson distribution and log link function …. one should use the robust (or sandwich) variance estimator to obtain valid standard errors (the bootstrap can also be used)”).\n\nFull list of risk factors for outcome (2 variables)\n\nfitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nThe estimate we see as a hazard ratio is basically a risk ratio.\nStrtatum specific (2 variables)\n\nfitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx3 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx4 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 1.156387\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 <- glm(hypertension ~ smoking + gender, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nfitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.077402\n\nCrude (in absence of confounding)\n\nfitx0 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nSince the marginal and conditional estimate are approximately the same, RD is a collapsible measure.\nConditional and crude OR\nFull list of risk factors for outcome (2 variables)\n\nfitx0 <- glm(hypertension ~ smoking + gender + age, family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nfitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx3 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx4 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 2.180804\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\",\n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.290429\n\nMantel-Haenszel adjusted ORs with 1 variable\n\ntabx <- xtabs( ~ hypertension + smoking + gender, data = Obs.Data)\nftable(tabx)    \n#>                      gender     0     1\n#> hypertension smoking                   \n#> 0            0               3788 12401\n#>              1               3400 11504\n#> 1            0              10547 20984\n#>              1              12178 25198\n# require(samplesizeCMH)\n# apply(tabx, 3, odds.ratio)\n\nlibrary(lawstat)\ncmh.test(tabx)\n#> \n#>  Cochran-Mantel-Haenszel Chi-square Test\n#> \n#> data:  tabx\n#> CMH statistic = NA, df = 1.0000, p-value = NA, MH Estimate = 1.2924,\n#> Pooled Odd Ratio = 1.2876, Odd Ratio of level 1 = 1.2864, Odd Ratio of\n#> level 2 = 1.2945\n# mantelhaen.test(tabx, exact = TRUE)\n\nCrude (in absence of confounding)\n\nfitx0 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nMarginal RD, RR and OR\nBelow we show a procedure for calculating marginal probabilities \\(p_1\\) (for treated) and \\(p_0\\) (for untreated).\nAdjustment of 2 variables\n\nfitx3 <- glm(hypertension ~ smoking + gender + age, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx3)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  3.37 \n#> RR (ZY)=  1.08\n\nAdjustment of 1 variable\n\nfitx2 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx2)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\nNo adjustment\n\nfitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx1)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\nBootstrap could be used to estimate confidence intervals.\nRef:\n\n\n(Kleinman and Norton 2009) (“this paper demonstrates how to move from a nonlinear model to estimates of marginal effects that are quantified as the adjusted risk ratio or adjusted risk difference”)\n\n(Austin 2010) (“clinically meaningful measures of treatment effect using logistic regression model”)\n\n(Luijken et al. 2022) (“marginal odds ratio”)\n\n(Muller and MacLehose 2014) (“marginal standardization”)\n\n(Greenland 2004) (“standardized / population-averaged”)\n\n(Bieler et al. 2010) (“standardized /population-averaged risk from the logistic model”)\nSummary\nHere are the summary of the results based on a scenario where confounding was absent:\n\n\n\n\n\n\n\n\nModelling strategy\nRD (conditional)\nRR (conditional)\nOR (conditional)\n\n\n\nage + gender in regression\n0.06 [0.05;0.06]\n1.08 [1.08;1.09]\n3.37 [3.17;3.58]\n\n\nstratified by age and gender (mean)\n0.05 (0.11, 0.1,0.01,0)\n1.16 (1.41, 1.21, 1.01,1)\n2.18 (unweighted; 1.65, 1.49, 3.45, 2.14)\n\n\ngender in regression\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.26;1.33]\n\n\nstratified by gender (mean)\n0.05 (0.6,0.5)\n1.08 (1.09, 1.06)\n1.29 (1.29, 1.29; M-H 1.29)\n\n\nMarginal estimates\n\n\n\n\n\ncrude\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.25;1.32]\n\n\nBased on marginal probabilities (any variable combination)\n0.05\n1.08\n1.29\n\n\n\nAs we can see, marginal estimate = conditional estimate for RD and RR but not for OR. Hence, RD and RR are collapsible measures, while OR is a non-collapsible measure.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walk through, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nAustin, Peter C. 2010. “Absolute Risk Reductions, Relative Risks, Relative Risk Reductions, and Numbers Needed to Treat Can Be Obtained from a Logistic Regression Model.” Journal of Clinical Epidemiology 63 (1): 2–6.\n\n\nBieler, Gayle S, G Gordon Brown, Rick L Williams, and Donna J Brogan. 2010. “Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios from Complex Survey Data.” American Journal of Epidemiology 171 (5): 618–23.\n\n\nGreenland, Sander. 2004. “Model-Based Estimation of Relative Risks and Other Epidemiologic Measures in Studies of Common Outcomes and in Case-Control Studies.” American Journal of Epidemiology 160 (4): 301–5.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Confounding and Collapsibility in Causal Inference.” Statistical Science 14 (1): 29–46.\n\n\nKleinman, Lawrence C, and Edward C Norton. 2009. “What’s the Risk? A Simple Approach for Estimating Adjusted Risk Measures from Nonlinear Models Including Logistic Regression.” Health Services Research 44 (1): 288–302.\n\n\nLuijken, Kim, Rolf HH Groenwold, Maarten van Smeden, Susanne Strohmaier, and Georg Heinze. 2022. “A Comparison of Full Model Specification and Backward Elimination of Potential Confounders When Estimating Marginal and Conditional Causal Effects on Binary Outcomes from Observational Data.” Biometrical Journal.\n\n\nMansournia, Mohammad Ali, and Sander Greenland. 2015. “The Relation of Collapsibility and Confounding to Faithfulness and Stability.” Epidemiology 26 (4): 466–72.\n\n\nMuller, Clemma J, and Richard F MacLehose. 2014. “Estimating Predicted Probabilities from Logistic Regression: Different Methods Correspond to Different Target Populations.” International Journal of Epidemiology 43 (3): 962–70.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is a confounder",
    "text": "Adjusting for a variable that is a confounder\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.85\n\nfit2 <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis( 1.1 * L + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node L, order:1\n#> node A, order:2\n#> node P, order:3\n#> node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.68\n\nfit2 <- glm(Y ~ A + L, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (simplified)",
    "text": "Adjusting for a variable that is not a confounder (simplified)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>         0.0         1.3\n\nfit2 <- glm(Y ~ A + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.06\n\nfit2 <- glm(Y ~ A + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (Complex)",
    "text": "Adjusting for a variable that is not a confounder (Complex)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\nfit2 <- glm(Y ~ A + M + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>         0.0         1.3         0.5         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>        0.00        1.06        0.41\n\nfit2 <- glm(Y ~ A + M + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>        0.00        1.29        0.50        1.10\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confoundingF.html",
    "href": "confoundingF.html",
    "title": "R functions (R)",
    "section": "",
    "text": "The list of new R functions introduced in this Confounding and bias lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n cmh.test \n    lawstat \n    To conduct the Mantel-Haenszel Chi-square test \n  \n\n DAG.empty \n    simcausal \n    To initialize an empty DAG \n  \n\n ftable \n    base/stats \n    To create a flat contingency table \n  \n\n plotDAG \n    simcausal \n    To visualize a DAG \n  \n\n set.DAG \n    simcausal \n    To create a DAG \n  \n\n sim \n    simcausal \n    To simulate data using a DAG"
  },
  {
    "objectID": "confoundingQ.html#live-quiz",
    "href": "confoundingQ.html#live-quiz",
    "title": "Quiz (R)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "confoundingQ.html#download-quiz",
    "href": "confoundingQ.html#download-quiz",
    "title": "Quiz (R)",
    "section": "Download Quiz",
    "text": "Download Quiz"
  },
  {
    "objectID": "confoundingQ.html#quiz-r",
    "href": "confoundingQ.html#quiz-r",
    "title": "Quiz (R)",
    "section": "Quiz (R)",
    "text": "Quiz (R)\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "predictivefactors.html#background",
    "href": "predictivefactors.html#background",
    "title": "Prediction ideas",
    "section": "Background",
    "text": "Background\nThe chapter provides a comprehensive guide to prediction modeling for cholesterol levels, focusing on the challenges and solutions involved in building robust prediction models. It begins by addressing the issue of collinearity among predictors and progresses to cover the intricacies of modeling both continuous and binary outcomes. Special attention is given to diagnosing and preventing model overfitting through various techniques such as data splitting and cross-validation. Advanced topics in model validation like bootstrapping are also explored. The overarching theme is to equip data analysts with the tools and methods needed to build, assess, and improve predictive models while addressing common challenges like collinearity and overfitting.\n\n\nAs we’ve journeyed through the previous chapters, we’ve gained a comprehensive understanding of various research questions, particularly distinguishing between causal and predictive inquiries. While the prior chapter delved into the intricacies of causal questions and the challenges they present, this chapter shifts the spotlight to the realm of prediction. Predictive questions have their own set of complexities and methodologies, distinct from those of causal inquiries. Here, we’ll explore the art and science of making accurate predictions, understanding the factors that influence them, and the tools and techniques best suited for predictive analysis.\nFurthermore, this chapter serves as a precursor to our upcoming exploration of machine learning. While prediction provides the foundation, machine learning offers advanced tools and algorithms to refine and enhance our predictive capabilities. By building on the foundational knowledge from the preceding chapters and setting the stage for the machine learning chapter, we aim to provide a holistic view of how prediction and machine learning intertwine in the broader landscape of research inquiry.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "predictivefactors.html#overview-of-tutorials",
    "href": "predictivefactors.html#overview-of-tutorials",
    "title": "Prediction ideas",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nIdentify collinear predictors\nThis tutorial focuses on identifying collinear predictors in a dataset related to cholesterol levels from the NHANES 2015 collection. The tutorial guides you through summarizing its structure, and applying methods for variable clustering to detect collinear predictors. The tutorial is practical for data analysts aiming to improve model accuracy by identifying and addressing redundant variables.\n\n\nExplore relationships for continuous outcome variable\nThis comprehensive tutorial walks you through the process of analyzing a dataset on cholesterol levels, focusing on exploring relationships for a continuous outcome variable. It starts by generating a correlation plot. Multiple methods for examining descriptive associations are provided, including stratification by key predictors. The tutorial also covers linear regression modeling, diagnosing data issues like outliers and leverage, and refitting the model after cleaning the data. Additionally, the tutorial delves into more complex modeling techniques like polynomial regression and multiple covariates, and addresses issues of collinearity using Variance Inflation Factors (VIF).\n\n\nExplore relationships for binary outcome variable\nA binary outcome variable is created to classify cholesterol levels as ‘healthy’ or ‘unhealthy’. This transformed variable is then modeled using logistic regression. Various predictors including demographic variables, vital statistics, and other health parameters are considered in the model. The performance of the model is evaluated using Variance Inflation Factor (VIF) for multicollinearity and Area Under the Curve (AUC) for classification accuracy. Two models are fitted, and their respective AUCs are calculated to assess the predictive power.\n\n\nOverfitting and performance\nThe tutorial focus is on addressing overfitting and assessing model performance. A linear regression model is fitted using a comprehensive set of predictors. Various statistical metrics such as the design matrix dimensions, Sum of Squares for Error (SSE), Total Sum of Squares (SST), R-squared (R2), Root Mean Square Error (RMSE), and Adjusted R2 are calculated to evaluate the model’s predictive power and fit. Functions are also created to streamline the calculation of these metrics, allowing for more dynamic and customizable performance assessment. One such function, perform, encapsulates the entire process, outputting key performance indicators including R2, adjusted R2, and RMSE, and it can be applied to new data sets for validation.\n\n\nData spliting\nThe tutorial focuses on splitting data into training and testing sets to prevent model overfitting. We allocate approximately 70% of the data to the training set and the remaining 30% to the test set. The linear regression model is then fitted using the training data. Performance metrics are extracted using the previously defined perform function, which is applied not only to the training and test sets but also to the entire dataset for comprehensive performance evaluation. This data splitting approach allows for more robust model validation by assessing how well the model generalizes to unseen data.\n\n\nCross-vaildation\nThe tutorial outlines the process of implementing k-fold cross-validation to validate a linear regression model’s performance, aiming to predict cholesterol levels. The dataset is divided into 5 folds, by turn used as training (to fit the model), and test sets (used for prediction and performance evaluation). Performance metrics such as R-squared are calculated for each fold. The process can also be automated , which helps in fitting the model across all folds and summarizing the results, including calculating the mean and standard deviation of the R-squared values to understand the model’s consistency and reliability.\n\n\nBootstrap\nThe tutorial outlines methods for implementing various bootstrapping techniques in statistical analysis. It demonstrates resampling methods using vectors and matrices. The idea of bootstrapping is emphasized as a useful technique for estimating the standard deviation (SD) of a statistic (e.g., mean), when the distribution of the data is unknown. This SD is then used to calculate confidence intervals. Different variations of bootstrap methods, such as “boot,” “boot632,” and “Optimism corrected bootstrap,” are demonstrated for linear regression and logistic regression models. They are used to obtain performance metrics like R-squared for regression models and the Receiver Operating Characteristic (ROC) curve for classification models. The tutorial also includes an example of calculating the Brier Score. The examples aim to offer various strategies for model evaluation, from the basics of resampling a vector to applying complex methods like ‘Optimism corrected bootstrap’ on real-world data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "predictivefactors0.html#research-goals",
    "href": "predictivefactors0.html#research-goals",
    "title": "Concepts (P)",
    "section": "Research goals",
    "text": "Research goals\nEpidemiologists identify four types of inferential goals in research:\n\nPrediction: Establishing models to predict future outcomes based on current or past information.\nEvaluating an exposure of primary interest: Focusing on assessing the impact or significance of a specific exposure variable within a model.\nIdentifying the important independent predictors of an outcome: Determining which variables most significantly affect the outcome and understanding the strength and nature of these relationships.\nDescriptive: A possible emphasis on describing the data and relationships within it."
  },
  {
    "objectID": "predictivefactors0.html#reading-list",
    "href": "predictivefactors0.html#reading-list",
    "title": "Concepts (P)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Vittinghoff et al. 2011)\nOptional reading:\n\n(Greenland and Pearce 2015)\n(Kuhn and Johnson 2013) (chapter 4)\n\nExercise:\nWhich type of goal does this article have?\n\n(Williamson et al. 2020)"
  },
  {
    "objectID": "predictivefactors0.html#video-lessons",
    "href": "predictivefactors0.html#video-lessons",
    "title": "Concepts (P)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nInferential goals in an epidemiological study\n\n\n\nPrediction, causal, important predictors, descriptive\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 1: Prediction model\n\n\n\ndiscrimination, calibration, overfitting, validation, model selection\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 2: Causal exploration\n\n\n\noutcome vs. exposure of primary interest\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 3: Outcome vs. multiple exposures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering and scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing reference level\n\n\n\nCriteria to determine the appropriate reference level for a categorical covariate:\n\nIf the covariate possesses at least an ordinal nature and serves primarily as an adjustment variable, it is advisable to select either the lowest or highest category as the reference level. This choice can be particularly useful in uncovering potential dose-response relationships within the data.\nConsider the specific aspect you wish to emphasize in your interpretation. For instance, if you aim to shed light on the concept of an unhealthy diet within the context, opting for the “healthy” category as the reference level can align with the typical causal motivation behind the choice.\nIn general, it is advisable to designate the category with the highest frequency as the reference level. This selection carries a statistical advantage, especially when dealing with imbalanced categories. Avoid choosing a low-frequency category as the reference level, as regression estimates may become highly unstable under such circumstances."
  },
  {
    "objectID": "predictivefactors0.html#video-lesson-slides",
    "href": "predictivefactors0.html#video-lesson-slides",
    "title": "Concepts (P)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "predictivefactors0.html#links",
    "href": "predictivefactors0.html#links",
    "title": "Concepts (P)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "predictivefactors0.html#references",
    "href": "predictivefactors0.html#references",
    "title": "Concepts (P)",
    "section": "References",
    "text": "References\n\n\n\n\nGreenland, Sander, and Neil Pearce. 2015. “Statistical Foundations for Model-Based Adjustments.” Annual Review of Public Health 36: 89–108. https://doi.org/10.1146/annurev-publhealth-031914-122531.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Vol. 26. Springer.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2011. “Predictor Selection.” In Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Springer.\n\n\nWilliamson, Elizabeth J, Alex J Walker, Krishnan Bhaskaran, Seb Bacon, Chris Bates, Caroline E Morton, Helen J Curtis, et al. 2020. “Factors Associated with COVID-19-Related Death Using OpenSAFELY.” Nature 584 (7821): 430–36."
  },
  {
    "objectID": "predictivefactors1.html",
    "href": "predictivefactors1.html",
    "title": "Collinear predictors",
    "section": "",
    "text": "In this tutorial, we’ll continue with the data analysis. We’ll focus on an analysis of an NHANES data. This data contains over 1200 observations and 33 variables. These variables come in various types: numeric, categorical, and binary. Our primary goal is to fit a linear regression model to predict cholesterol levels.\n\n# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\n\nLoad data\nLet us load the dataset and see structure of the variables:\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n#head(analytic)\nstr(analytic)\n#> 'data.frame':    1267 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83741 83747 83750 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n#>  $ age                  : num  62 53 22 46 45 30 60 69 24 70 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Others\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"Black\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"College\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Never.married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"Between.25kto54k\" \"<25k\" ...\n#>  $ weight               : num  135630 25282 39353 35674 97002 ...\n#>  $ psu                  : num  1 1 2 1 1 1 1 2 1 2 ...\n#>  $ strata               : num  125 125 128 121 125 124 128 120 130 132 ...\n#>  $ diastolicBP          : num  70 88 70 94 70 50 74 70 72 54 ...\n#>  $ systolicBP           : num  128 146 110 144 116 104 142 146 126 144 ...\n#>  $ bodyweight           : num  94.8 90.4 76.6 86.2 76.2 71.2 75.6 84 89.2 81.7 ...\n#>  $ bodyheight           : num  184 171 165 177 178 ...\n#>  $ bmi                  : num  27.8 30.8 28 27.6 24.1 26.6 35.9 31 26.9 27 ...\n#>  $ waist                : num  101.1 107.9 86.6 104.3 90.1 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Some.days\" \"Every.day\" ...\n#>  $ alcohol              : num  1 6 8 1 3 2 1 1 2 2 ...\n#>  $ cholesterol          : num  173 265 164 242 181 184 205 287 126 192 ...\n#>  $ cholesterolM2        : num  4.47 6.85 4.24 6.26 4.68 4.76 5.3 7.42 3.26 4.97 ...\n#>  $ triglycerides        : num  158 170 77 497 63 62 169 245 95 64 ...\n#>  $ uric.acid            : num  4.2 7 6 6.5 5.4 5.5 5.1 4.3 7.6 7.1 ...\n#>  $ protein              : num  7.5 7.4 7.4 6.8 7.4 6.7 7.4 6.8 7.3 7.2 ...\n#>  $ bilirubin            : num  0.5 0.6 0.2 0.5 0.7 0.8 0.4 0.6 1.2 1.2 ...\n#>  $ phosphorus           : num  4.7 4.4 5.3 3.6 3.9 3.4 3.9 4.4 3.2 3 ...\n#>  $ sodium               : num  136 140 139 138 138 136 139 140 140 139 ...\n#>  $ potassium            : num  4.3 4.55 4.16 4.27 3.91 3.97 3.99 4.25 3.8 4.63 ...\n#>  $ globulin             : num  2.9 2.9 3 2.6 2.8 2.5 3.2 2.3 2.7 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.3 9.3 9.3 9.4 9.6 9.6 9.6 9.6 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"Yes\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:3739] 3 4 5 6 8 9 13 14 15 16 ...\n#>   ..- attr(*, \"names\")= chr [1:3739] \"3\" \"4\" \"5\" \"6\" ...\n\nDescribe the data\n\nrequire(rms)\ndescribe(analytic) \n#> analytic \n#> \n#>  33  Variables      1267  Observations\n#> --------------------------------------------------------------------------------\n#> ID \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1267        1    88660     3366    84250    84687 \n#>      .25      .50      .75      .90      .95 \n#>    86019    88692    91252    92670    93089 \n#> \n#> lowest : 83732 83733 83741 83747 83750, highest: 93617 93633 93643 93659 93685\n#> --------------------------------------------------------------------------------\n#> gender \n#>        n  missing distinct \n#>     1267        0        2 \n#>                         \n#> Value      Female   Male\n#> Frequency     496    771\n#> Proportion  0.391  0.609\n#> --------------------------------------------------------------------------------\n#> age \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       61        1    49.91    19.18       24       27 \n#>      .25      .50      .75      .90      .95 \n#>       36       51       63       72       78 \n#> \n#> lowest : 20 21 22 23 24, highest: 76 77 78 79 80\n#> --------------------------------------------------------------------------------\n#> born \n#>        n  missing distinct \n#>     1267        0        2 \n#>                                                                             \n#> Value      Born in 50 US states or Washingt                           Others\n#> Frequency                               991                              276\n#> Proportion                            0.782                            0.218\n#> --------------------------------------------------------------------------------\n#> race \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                               \n#> Value         Black Hispanic    Other    White\n#> Frequency       246      337      132      552\n#> Proportion    0.194    0.266    0.104    0.436\n#> --------------------------------------------------------------------------------\n#> education \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                               \n#> Value          College High.School      School\n#> Frequency          648         523          96\n#> Proportion       0.511       0.413       0.076\n#> --------------------------------------------------------------------------------\n#> married \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                                                    \n#> Value                 Married      Never.married Previously.married\n#> Frequency                 751                226                290\n#> Proportion              0.593              0.178              0.229\n#> --------------------------------------------------------------------------------\n#> income \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                                                               \n#> Value                  <25k Between.25kto54k Between.55kto99k         Over100k\n#> Frequency               344              435              297              191\n#> Proportion            0.272            0.343            0.234            0.151\n#> --------------------------------------------------------------------------------\n#> weight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1184        1    48904    44337     9158    11549 \n#>      .25      .50      .75      .90      .95 \n#>    19540    30335    63822   121803   151546 \n#> \n#> lowest :   5470.041   5948.955   6197.660   6480.947   6703.837\n#> highest: 203562.855 207197.232 213611.345 218138.797 224891.623\n#> --------------------------------------------------------------------------------\n#> psu \n#>        n  missing distinct     Info     Mean      Gmd \n#>     1267        0        2     0.75    1.493   0.5003 \n#>                       \n#> Value          1     2\n#> Frequency    642   625\n#> Proportion 0.507 0.493\n#> --------------------------------------------------------------------------------\n#> strata \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       15    0.994    126.3    4.792      120      121 \n#>      .25      .50      .75      .90      .95 \n#>      123      126      130      132      133 \n#> \n#> lowest : 119 120 121 122 123, highest: 129 130 131 132 133\n#>                                                                             \n#> Value        119   120   121   122   123   124   125   126   127   128   129\n#> Frequency     47    74   118    63    77    66   114   104   107    65    53\n#> Proportion 0.037 0.058 0.093 0.050 0.061 0.052 0.090 0.082 0.084 0.051 0.042\n#>                                   \n#> Value        130   131   132   133\n#> Frequency     99   120    95    65\n#> Proportion 0.078 0.095 0.075 0.051\n#> --------------------------------------------------------------------------------\n#> diastolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       41    0.997    70.37    13.99       52       54 \n#>      .25      .50      .75      .90      .95 \n#>       62       70       78       86       92 \n#> \n#> lowest :   0  26  34  38  40, highest: 104 106 108 110 112\n#> --------------------------------------------------------------------------------\n#> systolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       56    0.998    126.5     19.3    102.0    106.0 \n#>      .25      .50      .75      .90      .95 \n#>    114.0    124.0    136.0    148.8    160.0 \n#> \n#> lowest :  84  88  90  92  94, highest: 194 196 206 218 236\n#> --------------------------------------------------------------------------------\n#> bodyweight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      615        1    84.95    23.56    56.29    61.10 \n#>      .25      .50      .75      .90      .95 \n#>    69.70    81.40    97.00   113.44   127.47 \n#> \n#> lowest :  39.7  39.8  40.7  42.6  42.7, highest: 161.9 166.3 175.7 175.9 178.4\n#> --------------------------------------------------------------------------------\n#> bodyheight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      376        1    169.2    10.66    153.8    157.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.6    169.3    176.2    181.1    184.2 \n#> \n#> lowest : 143.8 144.2 145.2 145.9 146.2, highest: 194.6 195.1 195.6 198.4 201.0\n#> --------------------------------------------------------------------------------\n#> bmi \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      284        1    29.58    7.403    20.60    22.06 \n#>      .25      .50      .75      .90      .95 \n#>    24.80    28.60    33.30    38.24    42.00 \n#> \n#> lowest : 16.3 17.5 17.6 17.7 17.9, highest: 57.2 57.6 59.4 60.7 64.5\n#> --------------------------------------------------------------------------------\n#> waist \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      544        1    101.8    18.47     77.1     81.4 \n#>      .25      .50      .75      .90      .95 \n#>     90.5    100.3    111.2    122.8    132.5 \n#> \n#> lowest :  65.0  65.5  66.5  68.2  68.7, highest: 159.2 159.8 160.2 160.5 161.5\n#> --------------------------------------------------------------------------------\n#> smoke \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                            \n#> Value       Every.day Not.at.all  Some.days\n#> Frequency         448        665        154\n#> Proportion      0.354      0.525      0.122\n#> --------------------------------------------------------------------------------\n#> alcohol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       14    0.952    3.109    2.419        1        1 \n#>      .25      .50      .75      .90      .95 \n#>        1        2        4        6        8 \n#> \n#> lowest :  1  2  3  4  5, highest: 10 11 12 14 15\n#>                                                                             \n#> Value          1     2     3     4     5     6     7     8     9    10    11\n#> Frequency    336   371   189   106    79    95    10    26     4    20     1\n#> Proportion 0.265 0.293 0.149 0.084 0.062 0.075 0.008 0.021 0.003 0.016 0.001\n#>                             \n#> Value         12    14    15\n#> Frequency     23     1     6\n#> Proportion 0.018 0.001 0.005\n#> --------------------------------------------------------------------------------\n#> cholesterol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    193.1    47.47    132.0    142.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.5    191.0    217.0    248.0    268.0 \n#> \n#> lowest :  81  93  97 100 101, highest: 345 348 349 358 545\n#> --------------------------------------------------------------------------------\n#> cholesterolM2 \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    4.994    1.228    3.410    3.670 \n#>      .25      .50      .75      .90      .95 \n#>    4.205    4.940    5.610    6.410    6.930 \n#> \n#> lowest :  2.09  2.40  2.51  2.59  2.61, highest:  8.92  9.00  9.03  9.26 14.09\n#> --------------------------------------------------------------------------------\n#> triglycerides \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      361        1    165.8    124.1     48.0     59.0 \n#>      .25      .50      .75      .90      .95 \n#>     84.0    127.0    201.5    309.0    396.6 \n#> \n#> lowest :   18   21   24   25   31, highest:  964 1020 1157 1253 3061\n#> --------------------------------------------------------------------------------\n#> uric.acid \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       84        1    5.598    1.626     3.43     3.80 \n#>      .25      .50      .75      .90      .95 \n#>     4.60     5.50     6.50     7.40     8.00 \n#> \n#> lowest :  1.6  2.2  2.3  2.4  2.5, highest: 10.2 10.3 11.7 12.2 18.0\n#> --------------------------------------------------------------------------------\n#> protein \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       32    0.995    7.126   0.5095      6.4      6.6 \n#>      .25      .50      .75      .90      .95 \n#>      6.8      7.1      7.4      7.7      7.9 \n#> \n#> lowest : 5.7 5.8 5.9 6.0 6.1, highest: 8.4 8.5 8.6 8.8 9.0\n#> --------------------------------------------------------------------------------\n#> bilirubin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       31    0.984   0.5467   0.2949      0.2      0.2 \n#>      .25      .50      .75      .90      .95 \n#>      0.4      0.5      0.7      0.9      1.0 \n#> \n#> lowest : 0.00 0.01 0.02 0.03 0.04, highest: 1.80 2.00 2.10 2.60 3.30\n#> --------------------------------------------------------------------------------\n#> phosphorus \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       37    0.996    3.642    0.593      2.8      3.0 \n#>      .25      .50      .75      .90      .95 \n#>      3.3      3.6      4.0      4.3      4.5 \n#> \n#> lowest : 1.8 2.0 2.2 2.3 2.4, highest: 5.2 5.3 5.4 5.6 6.1\n#> --------------------------------------------------------------------------------\n#> sodium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       20    0.977    138.5    2.383      135      136 \n#>      .25      .50      .75      .90      .95 \n#>      137      139      140      141      142 \n#> \n#> lowest : 124 126 129 130 131, highest: 142 143 144 146 148\n#>                                                                             \n#> Value        124   126   129   130   131   132   133   134   135   136   137\n#> Frequency      1     1     1     1     5     4    11    23    46    93   176\n#> Proportion 0.001 0.001 0.001 0.001 0.004 0.003 0.009 0.018 0.036 0.073 0.139\n#>                                                                 \n#> Value        138   139   140   141   142   143   144   146   148\n#> Frequency    235   260   206   112    55    29     6     1     1\n#> Proportion 0.185 0.205 0.163 0.088 0.043 0.023 0.005 0.001 0.001\n#> --------------------------------------------------------------------------------\n#> potassium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      175        1    3.985   0.3725     3.45     3.57 \n#>      .25      .50      .75      .90      .95 \n#>     3.78     3.98     4.19     4.40     4.54 \n#> \n#> lowest : 2.60 2.92 2.96 3.07 3.09, highest: 5.15 5.21 5.36 5.37 5.51\n#> --------------------------------------------------------------------------------\n#> globulin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       29    0.994    2.799   0.4536      2.2      2.3 \n#>      .25      .50      .75      .90      .95 \n#>      2.5      2.8      3.0      3.3      3.5 \n#> \n#> lowest : 1.6 1.8 1.9 2.0 2.1, highest: 4.1 4.2 4.3 4.5 5.5\n#> --------------------------------------------------------------------------------\n#> calcium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       25    0.991    9.335   0.3786      8.8      8.9 \n#>      .25      .50      .75      .90      .95 \n#>      9.1      9.3      9.6      9.7      9.9 \n#> \n#> lowest :  8.4  8.5  8.6  8.7  8.8, highest: 10.4 10.5 10.7 11.0 11.1\n#> --------------------------------------------------------------------------------\n#> physical.work \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency    895   372\n#> Proportion 0.706 0.294\n#> --------------------------------------------------------------------------------\n#> physical.recreational \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency   1002   265\n#> Proportion 0.791 0.209\n#> --------------------------------------------------------------------------------\n#> diabetes \n#>        n  missing distinct \n#>     1267        0        2 \n#>                     \n#> Value        No  Yes\n#> Frequency  1064  203\n#> Proportion 0.84 0.16\n#> --------------------------------------------------------------------------------\n\nCollinearity\nAvoiding collinear variables can result in a more interpretable, stable, and efficient predictive model. Collinearity refers to a situation in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with substantial accuracy. Collinearity poses several issues for predictive analysis:\nReduced Interpretability: When predictor variables are highly correlated, it becomes challenging to isolate the impact of individual predictors on the response variable. In other words, it is difficult to determine which predictor is genuinely influential in explaining variance in the response variable. This reduces the interpretability of the model.\nUnstable Coefficients: Collinearity can lead to inflated standard errors of the regression coefficients. This means that the coefficients can be very sensitive to small changes in the data, making the model unstable and less generalizable to new, unseen data.\nOverfitting: When predictors are collinear, the model is more likely to fit to the noise in the data rather than the actual signal. This is a manifestation of overfitting, where the model becomes too complex and captures random noise. Overfitted models will perform poorly on new, unseen data.\nInefficiency: Including redundant variables (collinear variables) does not add additional information to the model. This could be inefficient, especially when dealing with large datasets, as it increases computational costs without improving model performance.\nMulticollinearity Diagnostics\nSeveral techniques are available for diagnosing multicollinearity, including:\n\nVariance Inflation Factor (VIF): The VIF is a measure of multicollinearity, which measures how much the variance of a regression coefficient is increased because of multicollinearity. A general rule of thumb is that if VIF exceeds 4 or 5, multicollinearity is present.\nEigenvalues and Eigenvectors of the correlation/covariance matrix: The eigenvalues of the correlation matrix can also be used to measure the presence of multicollinearity, with one or more eigenvalues close to zero indicating multicollinearity.\nPairwise correlation matrices: Large correlation coefficients in the correlation matrix of predictors indicate the possibility of multicollinearity.\nRemedies\nSome common ways to handle collinearity include:\n\nRemoving one of the correlated predictors\nCombining correlated predictors into a single composite predictor\nUsing regularization techniques like Ridge Regression, which can help handle collinearity by adding a penalty term. Ridge regression adds a small amount of bias to the regression estimates, which in turn reduces the standard error, addresses the collinearity issue to some extent, and makes the results more reliable.\nIdentify collinear predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can also use hclust and varclus or variable clustering, i.e., to identify collinear predictors\n\n\n\n\nhclust is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.\n\nrequire(Hmisc)\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \n               \"married\", \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\",\n               \"alcohol\",  \"cholesterol\", \"triglycerides\", \n               \"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\",\n               \"sodium\", \"potassium\", \"globulin\", \"calcium\", \n               \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.cluster <- varclus(~., data = analytic[sel.names])\n# var.cluster\nplot(var.cluster)\n\n\n\n\nIn this plot, Spearman correlation or Spearman’s \\(\\rho\\)(rho) assesses the monotonic but nonlinear relationships predictors. We can also specify linear relationships such as Pearson correlation. A high correlation coefficient indicates predictors are highly correlated. For example, Spearman’s \\(\\rho\\) is more than 0.8 for bodyweight, BMI, and waist, indicating high collinearity between these predictors.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "predictivefactors2.html",
    "href": "predictivefactors2.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Let us focus on issues related to predictive modeling for continuous outcomes in 4 steps:\n\nDiagnosis Phase: Identifies outliers, leverage points, and residuals that could affect the model.\nCleaning Phase: Deletes problematic data based on predefined conditions.\nModeling Phase: Various models are fitted including polynomial models and a comprehensive model with multiple predictors.\nColinearity Check: A rule of thumb is used to check for multicollinearity in the comprehensive model, and problematic variables are flagged.\n\nExplore relationships for continuous outcome variable\nFirst, load several R packages for statistical modeling, data manipulation, and visualization.\n\n# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(car)\nlibrary(corrplot)\nlibrary(olsrr)\n\nLoad data\nHere, a dataset is loaded into the R environment from an RData file.\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n\nCorrelation plot\n\n\n\n\n\n\nTip\n\n\n\nWe can use the cor function to see the correlation between numeric variables and then use the corrplot function to plot the cor object. The plot helps in understanding the linear or nonlinear relationships between different numerical variables.\n\n\n\nrequire(corrplot)\nnumeric.names <- c(\"age\", \"diastolicBP\", \"systolicBP\", \"bodyweight\", \n                   \"bodyheight\", \"bmi\", \"waist\", \"alcohol\", \n                   \"cholesterol\", \"triglycerides\", \"uric.acid\", \n                   \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\",\n                   \"potassium\", \"globulin\", \"calcium\")\ncorrelationMatrix <- cor(analytic[numeric.names])\nmat.num <- round(correlationMatrix,2)\nmat.num[mat.num>0.8 & mat.num < 1]\n#> [1] 0.89 0.90 0.89 0.91 0.90 0.91\ncorrplot(correlationMatrix, method=\"number\", type=\"upper\")\n\n\n\n\nAs we can see, the correlation between bmi and waist is 0.91, suggesting a high correlation between these two predictors. Similarly, bodyweight is highly correlated with waist (correlation 0.90), bodyweight is highly correlated with bmi (correlation 0.89). Also, there is no linear relationship between age and bmi (correlation -0.02).\nExamine descriptive associations\nLet us examine the descriptive associations with the dependent variable by stratifying separately by key predictors\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to examine the descriptive associations by strata/groups, e.g., summarize, aggregate, describeBy, tapply, summary\n\n\nThe code calculates and explores various ways to describe the cholesterol levels, stratified by key predictors such as gender.\n\nmean(analytic$cholesterol)\n#> [1] 193.1002\n\n# Process 1\nmean(analytic$cholesterol[analytic$gender == \"Male\"])\n#> [1] 190.7626\nmean(analytic$cholesterol[analytic$gender == \"Female\"])\n#> [1] 196.7339\n\n# Process 2\nlibrary(dplyr)\nanalytic %>%\n  group_by(gender) %>%\n  dplyr::summarize(mean.ch=mean(cholesterol), .groups = 'drop') \n\n\n\n  \n\n\n\n# process 3\nwith(analytic, aggregate( analytic$cholesterol, by=list(gender), \n                          FUN=summary))\n\n\n\n  \n\n\n\n# process 4\npsych::describeBy(analytic$cholesterol, analytic$gender)\n#> \n#>  Descriptive statistics by group \n#> group: Female\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 496 196.73 43.26  194.5  194.44 40.77 100 358   258 0.57     0.56 1.94\n#> ------------------------------------------------------------ \n#> group: Male\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 771 190.76 43.06    188  188.54 40.03  81 545   464  1.1     5.76 1.55\n\n# process 5\ntapply(analytic$cholesterol, analytic$gender, summary)\n#> $Female\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   100.0   166.8   194.5   196.7   220.2   358.0 \n#> \n#> $Male\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   161.0   188.0   190.8   215.0   545.0\n\n# A general process\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \n               \"married\", \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \n               \"smoke\", \"alcohol\", \"cholesterol\", \n               \"triglycerides\", \"uric.acid\", \"protein\", \n               \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\",\n               \"physical.recreational\", \"diabetes\")\nvar.summ <- summary(cholesterol~ ., data = analytic[sel.names])\nvar.summ\n#> cholesterol      N= 1267  \n#> \n#> +---------------------+--------------------------------+----+-----------+\n#> |                     |                                |   N|cholesterol|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               gender|                          Female| 496|   196.7339|\n#> |                     |                            Male| 771|   190.7626|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  age|                         [20,37)| 342|   182.4854|\n#> |                     |                         [37,52)| 313|   200.1661|\n#> |                     |                         [52,64)| 315|   199.7873|\n#> |                     |                         [64,80]| 297|   190.7845|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 born|Born in 50 US states or Washingt| 991|   190.9253|\n#> |                     |                          Others| 276|   200.9094|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 race|                           Black| 246|   187.3740|\n#> |                     |                        Hispanic| 337|   193.5490|\n#> |                     |                           Other| 132|   191.8561|\n#> |                     |                           White| 552|   195.6757|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            education|                         College| 648|   192.5478|\n#> |                     |                     High.School| 523|   193.4532|\n#> |                     |                          School|  96|   194.9062|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              married|                         Married| 751|   194.0306|\n#> |                     |                   Never.married| 226|   182.8761|\n#> |                     |              Previously.married| 290|   198.6586|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               income|                            <25k| 344|   191.9564|\n#> |                     |                Between.25kto54k| 435|   191.9310|\n#> |                     |                Between.55kto99k| 297|   195.7508|\n#> |                     |                        Over100k| 191|   193.7016|\n#> +---------------------+--------------------------------+----+-----------+\n#> |          diastolicBP|                        [ 0, 64)| 336|   186.7649|\n#> |                     |                        [64, 72)| 321|   189.3458|\n#> |                     |                        [72, 80)| 319|   195.7085|\n#> |                     |                        [80,112]| 291|   201.6976|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           systolicBP|                       [ 84,116)| 340|   186.2765|\n#> |                     |                       [116,126)| 317|   190.6372|\n#> |                     |                       [126,138)| 335|   196.9881|\n#> |                     |                       [138,236]| 275|   199.6400|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyweight|                    [39.7, 69.8)| 319|   193.8903|\n#> |                     |                    [69.8, 81.5)| 316|   197.1424|\n#> |                     |                    [81.5, 97.2)| 317|   192.4984|\n#> |                     |                    [97.2,178.4]| 315|   188.8508|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyheight|                       [144,163)| 317|   198.7003|\n#> |                     |                       [163,169)| 320|   193.7750|\n#> |                     |                       [169,176)| 314|   189.8790|\n#> |                     |                       [176,201]| 316|   190.0000|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  bmi|                     [16.3,24.9)| 322|   188.8043|\n#> |                     |                     [24.9,28.7)| 315|   198.5016|\n#> |                     |                     [28.7,33.4)| 317|   197.5016|\n#> |                     |                     [33.4,64.5]| 313|   187.6262|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                waist|                   [ 65.0, 90.7)| 320|   188.9688|\n#> |                     |                   [ 90.7,100.4)| 315|   199.6413|\n#> |                     |                   [100.4,111.3)| 316|   197.3892|\n#> |                     |                   [111.3,161.5]| 316|   186.4747|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                smoke|                       Every.day| 448|   191.5938|\n#> |                     |                      Not.at.all| 665|   194.6451|\n#> |                     |                       Some.days| 154|   190.8117|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              alcohol|                               1| 336|   191.0387|\n#> |                     |                               2| 371|   192.0809|\n#> |                     |                          [3, 5)| 295|   195.9356|\n#> |                     |                          [5,15]| 265|   193.9849|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        triglycerides|                      [ 18,  85)| 320|   172.2344|\n#> |                     |                      [ 85, 128)| 319|   185.6834|\n#> |                     |                      [128, 203)| 314|   199.4140|\n#> |                     |                      [203,3061]| 314|   215.5860|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            uric.acid|                      [1.6, 4.7)| 348|   188.9310|\n#> |                     |                      [4.7, 5.6)| 305|   191.8033|\n#> |                     |                      [5.6, 6.6)| 307|   195.7720|\n#> |                     |                      [6.6,18.0]| 307|   196.4430|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              protein|                       [5.7,6.9)| 336|   189.8631|\n#> |                     |                       [6.9,7.2)| 328|   192.3201|\n#> |                     |                       [7.2,7.5)| 310|   193.4258|\n#> |                     |                       [7.5,9.0]| 293|   197.3413|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            bilirubin|                       [0.0,0.5)| 506|   195.7391|\n#> |                     |                             0.5| 212|   192.2264|\n#> |                     |                       [0.6,0.8)| 310|   192.0645|\n#> |                     |                       [0.8,3.3]| 239|   189.6318|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           phosphorus|                       [1.8,3.4)| 362|   188.0387|\n#> |                     |                       [3.4,3.7)| 309|   192.5405|\n#> |                     |                       [3.7,4.1)| 323|   195.5542|\n#> |                     |                       [4.1,6.1]| 273|   197.5421|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               sodium|                       [124,138)| 362|   191.9420|\n#> |                     |                       [138,140)| 495|   194.2929|\n#> |                     |                             140| 206|   191.7864|\n#> |                     |                       [141,148]| 204|   193.5882|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            potassium|                     [2.60,3.79)| 320|   191.5375|\n#> |                     |                     [3.79,3.99)| 328|   192.3628|\n#> |                     |                     [3.99,4.20)| 308|   196.9643|\n#> |                     |                     [4.20,5.51]| 311|   191.6592|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             globulin|                       [1.6,2.6)| 350|   189.9429|\n#> |                     |                       [2.6,2.9)| 388|   199.0052|\n#> |                     |                       [2.9,3.1)| 230|   193.3783|\n#> |                     |                       [3.1,5.5]| 299|   188.9197|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              calcium|                      [8.4, 9.2)| 371|   186.0323|\n#> |                     |                      [9.2, 9.4)| 294|   188.3605|\n#> |                     |                      [9.4, 9.7)| 395|   197.4430|\n#> |                     |                      [9.7,11.1]| 207|   204.2126|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        physical.work|                              No| 895|   194.0078|\n#> |                     |                             Yes| 372|   190.9167|\n#> +---------------------+--------------------------------+----+-----------+\n#> |physical.recreational|                              No|1002|   193.5359|\n#> |                     |                             Yes| 265|   191.4528|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             diabetes|                              No|1064|   194.8036|\n#> |                     |                             Yes| 203|   184.1724|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              Overall|                                |1267|   193.1002|\n#> +---------------------+--------------------------------+----+-----------+\nplot(var.summ)\n\n\n\n\nsummary(analytic$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   62.00   70.00   70.37   78.00  112.00\n\nanalytic$diastolicBP[analytic$diastolicBP == 0] <- NA\n\n# Bivariate Summaries Computed Separately by a Series of Predictors\nvar.summ2 <- spearman2(cholesterol~ ., data = analytic[sel.names])\nplot(var.summ2)\n\n\n\n\nRegression: Linear regression\nA linear regression model is fitted to explore the association between cholesterol levels and triglycerides. Various summary statistics are also generated for the model.\n\n\n\n\n\n\nTip\n\n\n\nWe use lm function to fit the linear regression\n\n\n\n# set up formula with just 1 variable\nformula0 <- as.formula(\"cholesterol~triglycerides\")\n\n# fitting regression on the analytic2 data\nfit0 <- lm(formula0,data = analytic2)\n\n# extract results\nsummary(fit0)\n#> \n#> Call:\n#> lm(formula = formula0, data = analytic2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -111.651  -26.157   -2.661   22.549  166.752 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.716e+02  1.127e+00  152.23   <2e-16 ***\n#> triglycerides 1.275e-01  5.456e-03   23.37   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 37.38 on 2632 degrees of freedom\n#> Multiple R-squared:  0.1718, Adjusted R-squared:  0.1715 \n#> F-statistic:   546 on 1 and 2632 DF,  p-value: < 2.2e-16\n\n# extract just the coefficients/estimates\ncoef(fit0)\n#>   (Intercept) triglycerides \n#>   171.6147531     0.1274909\n\n# extract confidence intervals\nconfint(fit0)\n#>                     2.5 %      97.5 %\n#> (Intercept)   169.4042284 173.8252779\n#> triglycerides   0.1167919   0.1381899\n\n# residual plots\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\nDiagnosis\nIdentifying problematic data\nOutliers: We can begin by plotting cholesterol against triglycerides to visualize any potential outliers. We can then identify data points where triglycerides are high.\nLeverage: It calculates and plots leverage points. Leverage points that have values greater than 0.05 are isolated for inspection.\nResiduals: Studentized residuals are computed for each data point to identify potential outliers. Those with values less than -5 are identified.\n\nrequire(olsrr)\n# Outlier\nplot(cholesterol ~ triglycerides, data = analytic2)\n\n\n\nsubset(analytic2, triglycerides > 1500)\n\n\n\n  \n\n\n\n# leverage\nols_plot_resid_lev(fit0)\n\n\n\nanalytic2$lev <- hat(model.matrix(fit0))\nplot(analytic2$lev)\n\n\n\nsummary(analytic2$lev)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.0003796 0.0004062 0.0004773 0.0007593 0.0005831 0.1800021\nwhich(analytic2$lev > 0.05)\n#> [1] 1102\nsubset(analytic2, lev > 0.05)\n\n\n\n  \n\n\n\n# Residual\nanalytic2$rstudent.values <- rstudent(fit0)\nplot(analytic2$rstudent.values)\n\n\n\nwhich(analytic2$rstudent.values < -5)\n#> integer(0)\n# Heteroskedasticity: Test for constant variance\n#ols_test_breusch_pagan(fit0, rhs = TRUE)\n\nDeleting suspicious data\nWe then delete observations based on two conditions: triglycerides > 1500 and leverage > 0.05.\n\n# condition 1: triglycerides above 1500 needs deleting\nanalytic2b <- subset(analytic2, triglycerides < 1500)\ndim(analytic2b)\n#> [1] 2632   34\n\n# condition 2: leverage above 0.05 needs deleting\nanalytic3 <- subset(analytic2b, lev < 0.05)\ndim(analytic3)\n#> [1] 2632   34\n\n# Check how many observations are deleted\nnrow(analytic2)-nrow(analytic3)\n#> [1] 2\n\nRefitting in cleaned data\nWe refit the linear model on this cleaned data, and diagnostic plots are generated.\n\n### Re-fit in data analytic3 (without problematic data)\nformula0\n#> cholesterol ~ triglycerides\nfit0 <- lm(formula0,data = analytic3)\n\nrequire(Publish)\npublish(fit0)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\nrequire(car)\n# component+residual plot or partial-residual plot\ncrPlots(fit0)\n\n\n\n\nPolynomial order 2\nWe fit polynomial models of orders 2 and 3 to explore non-linear relationships between cholesterol and triglycerides.\n\nformula1 <- as.formula(\"cholesterol~poly(triglycerides,2)\")\nformula1 <- as.formula(\"cholesterol~triglycerides^2\")\nfit1 <- lm(formula1,data = analytic3)\npublish(fit1)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit1)\n\n\n\n\nIt seems triglyceride has a quadratic effect on cholesterol. We can compare models fit0 and fit1 using the anova function:\n\n# compare fit0 and fit1 models\nres0 <- anova(fit0,fit1)\nprint(res0)\n#> Analysis of Variance Table\n#> \n#> Model 1: cholesterol ~ triglycerides\n#> Model 2: cholesterol ~ triglycerides^2\n#>   Res.Df     RSS Df Sum of Sq F Pr(>F)\n#> 1   2630 3673461                      \n#> 2   2630 3673461  0         0\n\nPolynomial order 3\n\n# Fit a polynomial of order 3\nformula2 <- as.formula(\"cholesterol~poly(triglycerides,3)\")\nformula2 <- as.formula(\"cholesterol~triglycerides^3\")\nfit2 <- lm(formula2,data = analytic3)\npublish(fit2)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit2)\n\n\n\n\n\n# compare fit1 and fit2 models\nres1 <- anova(fit1,fit2)\nprint(res1)\n#> Analysis of Variance Table\n#> \n#> Model 1: cholesterol ~ triglycerides^2\n#> Model 2: cholesterol ~ triglycerides^3\n#>   Res.Df     RSS Df Sum of Sq F Pr(>F)\n#> 1   2630 3673461                      \n#> 2   2630 3673461  0         0\n\nMultiple covariates\nWe add more covariates.\n\n# include everything!\nformula3 <- as.formula(\"cholesterol ~ gender + age + born + race + \n                       education + married + income + diastolicBP + \n                       systolicBP + bmi + bodyweight + bodyheight + \n                       waist +  triglycerides + uric.acid + protein +\n                       bilirubin + phosphorus + sodium + potassium + \n                       globulin + calcium + physical.work + \n                       physical.recreational + diabetes\")\nfit3 <- lm(formula3, data = analytic3)\npublish(fit3)\n#>               Variable                            Units Coefficient           CI.95     p-value \n#>            (Intercept)                                       280.02 [133.42;426.62]   0.0001852 \n#>                 gender                           Female         Ref                             \n#>                                                    Male      -11.99  [-16.41;-7.57]     < 1e-04 \n#>                    age                                         0.35     [0.23;0.47]     < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                             \n#>                                                  Others        7.52    [3.68;11.36]   0.0001270 \n#>                   race                            Black         Ref                             \n#>                                                Hispanic       -6.15  [-10.87;-1.44]   0.0106253 \n#>                                                   Other       -5.37   [-10.92;0.18]   0.0579281 \n#>                                                   White       -0.95    [-5.21;3.30]   0.6603698 \n#>              education                          College         Ref                             \n#>                                             High.School        2.90    [-0.28;6.08]   0.0743132 \n#>                                                  School       -2.54    [-8.61;3.54]   0.4134016 \n#>                married                          Married         Ref                             \n#>                                           Never.married       -5.72   [-9.63;-1.81]   0.0041887 \n#>                                      Previously.married        0.31    [-3.54;4.17]   0.8730460 \n#>                 income                             <25k         Ref                             \n#>                                        Between.25kto54k       -0.97    [-4.87;2.93]   0.6261315 \n#>                                        Between.55kto99k        2.29    [-1.98;6.56]   0.2928564 \n#>                                                Over100k        2.44    [-2.27;7.14]   0.3099380 \n#>            diastolicBP                                         0.38     [0.25;0.50]     < 1e-04 \n#>             systolicBP                                         0.02    [-0.08;0.12]   0.6668119 \n#>                    bmi                                        -2.55   [-4.29;-0.81]   0.0041392 \n#>             bodyweight                                         0.82     [0.19;1.45]   0.0105518 \n#>             bodyheight                                        -0.89   [-1.55;-0.24]   0.0074286 \n#>                  waist                                        -0.02    [-0.29;0.26]   0.9020424 \n#>          triglycerides                                         0.12     [0.11;0.14]     < 1e-04 \n#>              uric.acid                                         1.27     [0.08;2.47]   0.0369190 \n#>                protein                                         4.99   [-0.77;10.74]   0.0897748 \n#>              bilirubin                                        -5.43  [-10.53;-0.33]   0.0370512 \n#>             phosphorus                                        -0.18    [-2.81;2.45]   0.8939361 \n#>                 sodium                                        -0.97   [-1.66;-0.29]   0.0052516 \n#>              potassium                                         1.04    [-3.44;5.52]   0.6487979 \n#>               globulin                                        -2.25    [-8.22;3.71]   0.4591138 \n#>                calcium                                        12.02    [6.98;17.07]     < 1e-04 \n#>          physical.work                               No         Ref                             \n#>                                                     Yes       -0.45    [-3.68;2.79]   0.7858787 \n#>  physical.recreational                               No         Ref                             \n#>                                                     Yes        1.35    [-1.94;4.65]   0.4210703 \n#>               diabetes                               No         Ref                             \n#>                                                     Yes      -19.11 [-23.37;-14.85]     < 1e-04\n\nColinearity\nWe finally check for multicollinearity among predictors using the Variance Inflation Factor (VIF).\n\n\n\n\n\n\nNote\n\n\n\nRule of thumb: variables with VIF > 4 needs further investigation\n\n\n\ncar::vif(fit3)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.694171  1        1.641393\n#> age                     2.164388  1        1.471186\n#> born                    1.611478  1        1.269440\n#> race                    2.463445  3        1.162137\n#> education               1.435876  2        1.094660\n#> married                 1.481141  2        1.103187\n#> income                  1.402249  3        1.057964\n#> diastolicBP             1.271126  1        1.127442\n#> systolicBP              1.594986  1        1.262928\n#> bmi                    81.811969  1        9.044997\n#> bodyweight            101.102349  1       10.054966\n#> bodyheight             21.863188  1        4.675809\n#> waist                  11.913719  1        3.451626\n#> triglycerides           1.219331  1        1.104233\n#> uric.acid               1.603290  1        1.266211\n#> protein                 3.622385  1        1.903256\n#> bilirubin               1.185035  1        1.088593\n#> phosphorus              1.116982  1        1.056874\n#> sodium                  1.120920  1        1.058735\n#> potassium               1.178381  1        1.085533\n#> globulin                3.371211  1        1.836086\n#> calcium                 1.591677  1        1.261617\n#> physical.work           1.087315  1        1.042744\n#> physical.recreational   1.226830  1        1.107624\n#> diabetes                1.210715  1        1.100325\ncollinearity <- ols_vif_tol(fit3)\ncollinearity\n\n\n\n  \n\n\n\n# VIF > 4\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\n\nformula4 <- as.formula(\"cholesterol ~ gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + # bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit4 <- lm(formula4, data = analytic3)\npublish(fit4)\n#>               Variable                            Units Coefficient           CI.95    p-value \n#>            (Intercept)                                       136.87  [34.96;238.79]   0.008533 \n#>                 gender                           Female         Ref                            \n#>                                                    Male      -13.06  [-16.60;-9.53]    < 1e-04 \n#>                    age                                         0.35     [0.24;0.46]    < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                            \n#>                                                  Others        7.88    [4.06;11.69]    < 1e-04 \n#>                   race                            Black         Ref                            \n#>                                                Hispanic       -5.79  [-10.34;-1.24]   0.012740 \n#>                                                   Other       -4.88   [-10.33;0.57]   0.079497 \n#>                                                   White       -0.85    [-5.02;3.33]   0.690720 \n#>              education                          College         Ref                            \n#>                                             High.School        2.85    [-0.32;6.02]   0.078008 \n#>                                                  School       -2.45    [-8.49;3.60]   0.427694 \n#>                married                          Married         Ref                            \n#>                                           Never.married       -5.74   [-9.65;-1.83]   0.004088 \n#>                                      Previously.married        0.34    [-3.52;4.20]   0.861981 \n#>                 income                             <25k         Ref                            \n#>                                        Between.25kto54k       -0.87    [-4.77;3.03]   0.663123 \n#>                                        Between.55kto99k        2.46    [-1.79;6.71]   0.256585 \n#>                                                Over100k        2.63    [-2.07;7.32]   0.272886 \n#>            diastolicBP                                         0.37     [0.25;0.50]    < 1e-04 \n#>             systolicBP                                         0.03    [-0.07;0.13]   0.544971 \n#>                    bmi                                        -0.31   [-0.54;-0.08]   0.009302 \n#>          triglycerides                                         0.12     [0.11;0.14]    < 1e-04 \n#>              uric.acid                                         1.36     [0.16;2.55]   0.025926 \n#>                protein                                         4.77   [-0.98;10.51]   0.104059 \n#>              bilirubin                                        -6.06  [-11.14;-0.98]   0.019519 \n#>             phosphorus                                        -0.08    [-2.71;2.55]   0.954561 \n#>                 sodium                                        -1.03   [-1.71;-0.35]   0.003175 \n#>              potassium                                         0.89    [-3.58;5.37]   0.695615 \n#>               globulin                                        -2.20    [-8.15;3.75]   0.469150 \n#>                calcium                                        12.20    [7.16;17.25]    < 1e-04 \n#>          physical.work                               No         Ref                            \n#>                                                     Yes       -0.44    [-3.68;2.80]   0.790297 \n#>  physical.recreational                               No         Ref                            \n#>                                                     Yes        1.24    [-2.03;4.51]   0.457666 \n#>               diabetes                               No         Ref                            \n#>                                                     Yes      -19.03 [-23.26;-14.80]    < 1e-04\n\n# check if there is still any problematic variable\n# with high collinearity problem\ncollinearity <- ols_vif_tol(fit4)\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\nSave data\n\nsave.image(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")"
  },
  {
    "objectID": "predictivefactors3.html",
    "href": "predictivefactors3.html",
    "title": "Binary outcome",
    "section": "",
    "text": "We focus on statistical analysis and modeling of a binary outcome (cholesterol level) that is categorized as either “healthy” or “unhealthy.”\nExplore relationships for binary outcome variable\n\n\n\nLoad data\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")\n\nCreating binary variable\nBinary categorization: The cholesterol variable is converted into a binary outcome (“healthy” or “unhealthy”) using the ifelse function based on a threshold value of 200.\nRe-leveling: The reference category for the binary variable is changed to “unhealthy.”\n\n# Binary variable\nanalytic3$cholesterol.bin <- ifelse(analytic3$cholesterol < 200, \"healthy\", \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>      1586      1046\n\n# Changing the reference category\nanalytic3$cholesterol.bin <- as.factor(analytic3$cholesterol.bin)\nanalytic3$cholesterol.bin <- relevel(analytic3$cholesterol.bin, ref = \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#> unhealthy   healthy \n#>      1046      1586\n\nModelling data\nA logistic regression is fitted to predict the binary cholesterol outcome from multiple predictor variables.\n\n\n\n\n\n\nTip\n\n\n\nWe use the glm function to run generalized linear models. The default family is gaussian with identity link. Setting binomial family with logit link (logit link is default for binomial family) means fitting logistic regression.\n\n\n\n# Regression model\nformula5x <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\n\n# Summary\nfit5x <- glm(formula5x, family = binomial(), data = analytic3)\npublish(fit5x)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.68 [1.27;2.23]   0.000313 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.69 [0.54;0.88]   0.002636 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.29 [0.95;1.75]   0.107104 \n#>                                                   Other      1.24 [0.87;1.79]   0.234062 \n#>                                                   White      1.10 [0.83;1.44]   0.505147 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.082168 \n#>                                                  School      1.23 [0.84;1.82]   0.292070 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.67]   0.052969 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.345688 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.01 [0.78;1.29]   0.957537 \n#>                                        Between.55kto99k      0.90 [0.69;1.19]   0.462854 \n#>                                                Over100k      0.90 [0.66;1.21]   0.472137 \n#>            diastolicBP                                       0.98 [0.97;0.98]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.029513 \n#>                    bmi                                       1.11 [0.99;1.24]   0.065627 \n#>             bodyweight                                       0.96 [0.92;1.00]   0.045338 \n#>             bodyheight                                       1.05 [1.00;1.09]   0.030995 \n#>                  waist                                       1.01 [0.99;1.02]   0.464825 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.273792 \n#>                protein                                       0.61 [0.42;0.89]   0.009192 \n#>              bilirubin                                       1.19 [0.86;1.66]   0.292632 \n#>             phosphorus                                       0.96 [0.81;1.13]   0.610931 \n#>                 sodium                                       1.06 [1.02;1.11]   0.007980 \n#>              potassium                                       0.95 [0.71;1.26]   0.729218 \n#>               globulin                                       1.38 [0.94;2.01]   0.101667 \n#>                calcium                                       0.64 [0.47;0.89]   0.007026 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.392539 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.05 [0.85;1.29]   0.681388 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.68 [2.02;3.56]    < 1e-04\n\n# VIF\ncar::vif(fit5x)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.735258  1        1.653862\n#> age                     2.121098  1        1.456399\n#> born                    1.664094  1        1.289998\n#> race                    2.585539  3        1.171544\n#> education               1.458430  2        1.098933\n#> married                 1.432595  2        1.094034\n#> income                  1.426911  3        1.061043\n#> diastolicBP             1.297308  1        1.138994\n#> systolicBP              1.614374  1        1.270580\n#> bmi                    81.928815  1        9.051454\n#> bodyweight            103.125772  1       10.155086\n#> bodyheight             22.647853  1        4.758976\n#> waist                  11.493710  1        3.390237\n#> triglycerides           1.258340  1        1.121758\n#> uric.acid               1.636512  1        1.279262\n#> protein                 3.684816  1        1.919587\n#> bilirubin               1.186181  1        1.089119\n#> phosphorus              1.117915  1        1.057315\n#> sodium                  1.123193  1        1.059808\n#> potassium               1.181358  1        1.086903\n#> globulin                3.427401  1        1.851324\n#> calcium                 1.543019  1        1.242183\n#> physical.work           1.090958  1        1.044490\n#> physical.recreational   1.218558  1        1.103883\n#> diabetes                1.212365  1        1.101074\n\nAUC\nThe Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) is calculated to assess the model’s predictive performance in terms of discrimination. The AUC would tell how much the model is capable of distinguishing between healthy and unhealthy levels.\nLet us measure the accuracy for classification models fit5x.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the roc function to build a ROC curve and auc function to calculate the AUC (are under the ROC curve) value.\n\n\n\nrequire(pROC)\npred.y <- predict(fit5x, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7411\n\nauc(rocobj)\n#> Area under the curve: 0.7411\n\nRe-modelling\nLet us re-fit the model and measure the AUC. VIF is calculated again for this new model.\n\nformula5 <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\npublish(fit5)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.86 [1.48;2.33]    < 1e-04 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.67 [0.52;0.85]   0.001233 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.26 [0.94;1.69]   0.128165 \n#>                                                   Other      1.21 [0.85;1.73]   0.284083 \n#>                                                   White      1.11 [0.85;1.45]   0.460652 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.083806 \n#>                                                  School      1.22 [0.83;1.80]   0.305514 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.68]   0.049983 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.339401 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.00 [0.78;1.28]   0.999445 \n#>                                        Between.55kto99k      0.90 [0.68;1.17]   0.425427 \n#>                                                Over100k      0.89 [0.66;1.20]   0.447012 \n#>            diastolicBP                                       0.98 [0.97;0.99]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.042769 \n#>                    bmi                                       1.01 [0.99;1.02]   0.496430 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.242942 \n#>                protein                                       0.62 [0.43;0.89]   0.010343 \n#>              bilirubin                                       1.24 [0.89;1.72]   0.203993 \n#>             phosphorus                                       0.95 [0.80;1.12]   0.539847 \n#>                 sodium                                       1.06 [1.02;1.11]   0.006777 \n#>              potassium                                       0.96 [0.72;1.28]   0.790080 \n#>               globulin                                       1.37 [0.94;2.00]   0.102430 \n#>                calcium                                       0.64 [0.46;0.88]   0.005772 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.382281 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.04 [0.85;1.29]   0.682962 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.69 [2.03;3.57]    < 1e-04\n\n# VIF\ncar::vif(fit5)\n#>                           GVIF Df GVIF^(1/(2*Df))\n#> gender                1.749947  1        1.322856\n#> age                   1.850160  1        1.360206\n#> born                  1.640947  1        1.280994\n#> race                  2.345460  3        1.152669\n#> education             1.430721  2        1.093676\n#> married               1.432015  2        1.093923\n#> income                1.409064  3        1.058819\n#> diastolicBP           1.289411  1        1.135523\n#> systolicBP            1.605248  1        1.266984\n#> bmi                   1.477795  1        1.215646\n#> triglycerides         1.246395  1        1.116421\n#> uric.acid             1.624039  1        1.274378\n#> protein               3.648367  1        1.910070\n#> bilirubin             1.177643  1        1.085193\n#> phosphorus            1.114298  1        1.055603\n#> sodium                1.117463  1        1.057101\n#> potassium             1.176914  1        1.084857\n#> globulin              3.395946  1        1.842809\n#> calcium               1.542486  1        1.241969\n#> physical.work         1.089742  1        1.043907\n#> physical.recreational 1.197719  1        1.094404\n#> diabetes              1.200402  1        1.095629\n\nThe AUC for this new model is also calculated.\n\n#### AUC\npred.y <- predict(fit5, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7406\nauc(rocobj)\n#> Area under the curve: 0.7406\n\nSave data\n\nsave.image(file = \"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nReferences"
  },
  {
    "objectID": "predictivefactors4.html",
    "href": "predictivefactors4.html",
    "title": "Overfitting and performance",
    "section": "",
    "text": "The following tutorial extends the work from the previous lab and focuses on understanding overfitting, evaluating performance, and function writing in the context of linear modeling for a continuous outcome variable, cholesterol levels.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nNow we will fit the final model that we decided at the end of previous part of the lab.\n\nformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + \n             physical.recreational + diabetes\")\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4 <- lm(formula4, data = analytic3)\nsummary(fit4)\n#> \n#> Call:\n#> lm(formula = formula4, data = analytic3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -115.465  -23.695   -2.598   20.017  177.264 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               136.871606  51.998527   2.632  0.00853 ** \n#> genderMale                -13.064857   1.802099  -7.250 5.48e-13 ***\n#> age                         0.351838   0.056116   6.270 4.22e-10 ***\n#> bornOthers                  7.877420   1.947498   4.045 5.39e-05 ***\n#> raceHispanic               -5.790547   2.323010  -2.493  0.01274 *  \n#> raceOther                  -4.879882   2.781673  -1.754  0.07950 .  \n#> raceWhite                  -0.847635   2.130149  -0.398  0.69072    \n#> educationHigh.School        2.851633   1.617435   1.763  0.07801 .  \n#> educationSchool            -2.446765   3.084409  -0.793  0.42769    \n#> marriedNever.married       -5.739509   1.997152  -2.874  0.00409 ** \n#> marriedPreviously.married   0.342206   1.968165   0.174  0.86198    \n#> incomeBetween.25kto54k     -0.867063   1.990253  -0.436  0.66312    \n#> incomeBetween.55kto99k      2.462130   2.169757   1.135  0.25658    \n#> incomeOver100k              2.626046   2.394560   1.097  0.27289    \n#> diastolicBP                 0.374971   0.062238   6.025 1.93e-09 ***\n#> systolicBP                  0.029976   0.049515   0.605  0.54497    \n#> bmi                        -0.309530   0.118927  -2.603  0.00930 ** \n#> triglycerides               0.124806   0.006427  19.419  < 2e-16 ***\n#> uric.acid                   1.357242   0.609012   2.229  0.02593 *  \n#> protein                     4.767008   2.931636   1.626  0.10406    \n#> bilirubin                  -6.060791   2.593508  -2.337  0.01952 *  \n#> phosphorus                 -0.076472   1.341957  -0.057  0.95456    \n#> sodium                     -1.026686   0.347679  -2.953  0.00318 ** \n#> potassium                   0.893507   2.283488   0.391  0.69561    \n#> globulin                   -2.198037   3.036091  -0.724  0.46915    \n#> calcium                    12.202366   2.574400   4.740 2.25e-06 ***\n#> physical.workYes           -0.439108   1.651078  -0.266  0.79030    \n#> physical.recreationalYes    1.238756   1.667670   0.743  0.45767    \n#> diabetesYes               -19.032748   2.158825  -8.816  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 35.22 on 2603 degrees of freedom\n#> Multiple R-squared:  0.2415, Adjusted R-squared:  0.2334 \n#> F-statistic: 29.61 on 28 and 2603 DF,  p-value: < 2.2e-16\n\nDesign Matrix\nExpands factors to a set of dummy variables.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the model.matrix function to construct a design/model matrix, such as expand factor variables to a matrix of dummy variable\n\n\nThe dimensions of the model matrix are obtained, and the total number of model parameters (p) is calculated.\n\nhead(model.matrix(fit4))\n#>    (Intercept) genderMale age bornOthers raceHispanic raceOther raceWhite\n#> 1            1          1  62          0            0         0         1\n#> 2            1          1  53          1            0         0         1\n#> 4            1          0  56          0            0         0         1\n#> 5            1          0  42          0            0         0         0\n#> 10           1          1  22          0            0         0         0\n#> 11           1          0  32          1            1         0         0\n#>    educationHigh.School educationSchool marriedNever.married\n#> 1                     0               0                    0\n#> 2                     1               0                    0\n#> 4                     0               0                    0\n#> 5                     0               0                    0\n#> 10                    0               0                    1\n#> 11                    0               0                    0\n#>    marriedPreviously.married incomeBetween.25kto54k incomeBetween.55kto99k\n#> 1                          0                      0                      1\n#> 2                          1                      0                      0\n#> 4                          0                      0                      1\n#> 5                          1                      1                      0\n#> 10                         0                      1                      0\n#> 11                         0                      1                      0\n#>    incomeOver100k diastolicBP systolicBP  bmi triglycerides uric.acid protein\n#> 1               0          70        128 27.8           158       4.2     7.5\n#> 2               0          88        146 30.8           170       7.0     7.4\n#> 4               0          72        132 42.4            93       5.4     6.1\n#> 5               0          70        100 20.3            52       3.3     7.7\n#> 10              0          70        110 28.0            77       6.0     7.4\n#> 11              0          70        120 28.2           295       5.2     7.4\n#>    bilirubin phosphorus sodium potassium globulin calcium physical.workYes\n#> 1        0.5        4.7    136      4.30      2.9     9.8                0\n#> 2        0.6        4.4    140      4.55      2.9     9.8                0\n#> 4        0.3        3.8    141      4.08      2.3     8.9                0\n#> 5        0.3        3.2    136      3.50      3.4     9.3                0\n#> 10       0.2        5.3    139      4.16      3.0     9.3                0\n#> 11       0.4        3.1    138      4.31      2.9    10.3                0\n#>    physical.recreationalYes diabetesYes\n#> 1                         0           1\n#> 2                         0           0\n#> 4                         0           0\n#> 5                         0           0\n#> 10                        1           0\n#> 11                        0           0\n\n# Dimension of the model matrix\ndim(model.matrix(fit4))\n#> [1] 2632   29\n\n# Number of parameters = intercept + slopes\np <- dim(model.matrix(fit4))[2] \np\n#> [1] 29\n\nCheck prediction\nThe observed and predicted cholesterol values are summarized.\n\nobs.y <- analytic3$cholesterol\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   163.0   189.0   191.5   216.0   362.0\n\n# Predict the above fit on analytic3 data\npred.y <- predict(fit4, analytic3)\nsummary(pred.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   136.3   178.2   189.4   191.5   202.4   337.6\nn <- length(pred.y)\nn\n#> [1] 2632\nplot(obs.y,pred.y)\nlines(lowess(obs.y,pred.y), col = \"red\")\n\n\n\n\n# Prediction on a new data: fictitious.data\nstr(fictitious.data)\n#> 'data.frame':    4121 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83734 83735 83736 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n#>  $ age                  : num  62 53 78 56 42 72 22 32 56 46 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"High.School\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"<25k\" \"Between.55kto99k\" ...\n#>  $ weight               : num  135630 25282 12576 102079 18235 ...\n#>  $ psu                  : num  1 1 1 1 2 1 2 1 2 1 ...\n#>  $ strata               : num  125 125 131 131 126 128 128 125 126 121 ...\n#>  $ diastolicBP          : num  70 88 46 72 70 58 70 70 116 94 ...\n#>  $ systolicBP           : num  128 146 138 132 100 116 110 120 178 144 ...\n#>  $ bodyweight           : num  94.8 90.4 83.4 109.8 55.2 ...\n#>  $ bodyheight           : num  184 171 170 161 165 ...\n#>  $ bmi                  : num  27.8 30.8 28.8 42.4 20.3 28.6 28 28.2 33.6 27.6 ...\n#>  $ waist                : num  101.1 107.9 116.5 110.1 80.4 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Not.at.all\" \"Not.at.all\" ...\n#>  $ alcohol              : num  1 6 0 1 1 0 8 1 0 1 ...\n#>  $ cholesterol          : num  173 265 229 174 204 190 164 190 145 242 ...\n#>  $ cholesterolM2        : num  4.47 6.85 5.92 4.5 5.28 4.91 4.24 4.91 3.75 6.26 ...\n#>  $ triglycerides        : num  158 170 299 93 52 52 77 295 121 497 ...\n#>  $ uric.acid            : num  4.2 7 7.3 5.4 3.3 4.9 6 5.2 4.8 6.5 ...\n#>  $ protein              : num  7.5 7.4 7.3 6.1 7.7 7.1 7.4 7.4 6.9 6.8 ...\n#>  $ bilirubin            : num  0.5 0.6 0.5 0.3 0.3 0.5 0.2 0.4 0.4 0.5 ...\n#>  $ phosphorus           : num  4.7 4.4 3.6 3.8 3.2 3.7 5.3 3.1 4.1 3.6 ...\n#>  $ sodium               : num  136 140 140 141 136 140 139 138 140 138 ...\n#>  $ potassium            : num  4.3 4.55 4.7 4.08 3.5 4.2 4.16 4.31 4.5 4.27 ...\n#>  $ globulin             : num  2.9 2.9 2.8 2.3 3.4 3 3 2.9 2.9 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.7 8.9 9.3 9.3 9.3 10.3 9.5 9.3 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:885] 16 30 39 48 50 58 61 65 67 68 ...\n#>   ..- attr(*, \"names\")= chr [1:885] \"27\" \"68\" \"90\" \"112\" ...\npred.y.new1 <- predict(fit4, fictitious.data)\nsummary(pred.y.new1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   128.7   178.9   190.6   192.5   203.3   557.4\n\nMeasuring prediction error\nContinuous outcomes\nR2\nThe Sum of Squares of Errors (SSE) and the Total Sum of Squares (SST) are calculated. The proportion of variance explained by the model is then calculated as R2.\n\n\nSee Wikipedia (2023a)\n\n# Find SSE\nSSE <- sum( (obs.y - pred.y)^2 )\nSSE\n#> [1] 3228460\n\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 4256586\n\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.2415378\n\nrequire(caret)\nR2(pred.y, obs.y)\n#> [1] 0.2415378\n\nRMSE\nThe Root Mean Square Error is calculated to measure the average magnitude of the errors between predicted and observed values.\n\n\nSee Wikipedia (2023b)\n\n# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 35.21767\n\nRMSE(pred.y, obs.y)\n#> [1] 35.02311\n\nAdj R2\nIt provides a measure of how well the model generalizes and adjusts R2 based on the number of predictors.\n\n\nSee Wikipedia (2023a)\n\n# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p))\nadjR2\n#> [1] 0.2333791\n\nWriting function\nSyntax for Writing Functions\n\nfunc_name <- function (argument) {\n  A statement or multiple lines of statements\n  return(output)\n}\n\nExample of a simple function\n\nf1 <- function(a,b){\n  result <- a + b\n  return(result)\n}\nf1(a=1,b=3)\n#> [1] 4\nf1(a=1,b=6)\n#> [1] 7\n# setting default values\nf1 <- function(a=1,b=1){\n  result <- a + b\n  return(result)\n}\nf1()\n#> [1] 2\nf1(b = 10)\n#> [1] 11\n\nA bit more complicated\n\n# one argument\nmodel.fit <- function(data.for.fitting){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#> 184.3131838  -7.8095595   0.2225745  11.1557140\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#> 176.1286576  -4.8256829   0.3375009   7.7186190\n\n\n# adding one more argument: digits\nmodel.fit <- function(data.for.fitting, digits=2){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  result <- round(result,digits)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#>      184.31       -7.81        0.22       11.16\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#>      176.13       -4.83        0.34        7.72\n\nFunction that gives performance measures\nlet us create a function that will give us the performance measures:\n\nperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  \n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  \n  # sample size\n  n <- length(pred.y)\n  \n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  \n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  \n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  \n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  \n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  \n  # returning object\n  return(res)\n}\nperform(new.data = analytic3, y.name = \"cholesterol\", model.fit = fit4)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 2632 29 0.242 0.233 35.023\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "predictivefactors5.html",
    "href": "predictivefactors5.html",
    "title": "Data Splitting",
    "section": "",
    "text": "This tutorial is focused on a crucial aspect of model building: splitting your data into training and test sets to avoid overfitting. Overfitting occurs when your model learns the noise in the data rather than the underlying trend. As a result, the model performs well on the training data but poorly on new, unseen data. To mitigate this, you often split your data.\nLoad data anf files\nInitially, several libraries are loaded to facilitate data manipulation and analysis.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nThen, previously saved dataset related to cholesterol and other factors is loaded for further use.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nData splitting to avoid model overfitting\nYou start by setting a random seed to ensure that the random splitting of data is reproducible. A specified function is then used to partition the data, taking as arguments the outcome variable (cholesterol level in this case) and the percentage of data that you want to allocate to the training set (70% in this example).\n\n\nKDnuggets (2023)\n\nKuhn (2023a)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use the createDataPartition function to split a dataset into training and testing datasets. The function will return the row indices that should go into the training set. These indices are stored in a variable, and its dimensions are displayed to provide an understanding of the size of the training set that will be created. Additionally, you can calculate what 70% of your entire dataset would look like to verify the approximation of the training data size, as well as what the remaining 30% (for the test set) would look like.\n\n\n\n# Using a seed to randomize in a reproducible way \nset.seed(123)\nsplit <- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:1844, 1] 3 4 5 8 9 13 14 16 20 21 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 1844    1\n\n# Approximate train data\ndim(analytic3)*.7 \n#> [1] 1842.4   24.5\n\n# Approximate test data\ndim(analytic3)*(1-.7) \n#> [1] 789.6  10.5\n\nSplit the data\nAfter determining how to partition the data, the next step is actually creating the training and test datasets. The indices are used to subset the original dataset into these two new datasets. The dimensions of each dataset are displayed to confirm their sizes.\n\n# Create train data\ntrain.data <- analytic3[split,]\ndim(train.data)\n#> [1] 1844   35\n\n# Create test data\ntest.data <- analytic3[-split,]\ndim(test.data)\n#> [1] 788  35\n\nOur next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.\nTrain the model\nOnce the training dataset is created, you can proceed to train the model using the training data. A previously defined formula containing the predictor variables is used in a linear regression model. After fitting the model, a summary is generated to display key statistics that help in evaluating the model’s performance.\n\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4.train1 <- lm(formula4, data = train.data)\nsummary(fit4.train1)\n#> \n#> Call:\n#> lm(formula = formula4, data = train.data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -91.973 -23.719  -1.563  20.586 178.542 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                72.716792  59.916086   1.214  0.22504    \n#> genderMale                -11.293629   2.136545  -5.286 1.40e-07 ***\n#> age                         0.306235   0.066376   4.614 4.23e-06 ***\n#> bornOthers                  7.220858   2.300658   3.139  0.00172 ** \n#> raceHispanic               -6.727473   2.709718  -2.483  0.01313 *  \n#> raceOther                  -4.865771   3.237066  -1.503  0.13298    \n#> raceWhite                  -1.468522   2.494981  -0.589  0.55621    \n#> educationHigh.School        1.626097   1.920289   0.847  0.39722    \n#> educationSchool            -4.853095   3.585185  -1.354  0.17602    \n#> marriedNever.married       -5.298265   2.332033  -2.272  0.02321 *  \n#> marriedPreviously.married   1.202448   2.305191   0.522  0.60199    \n#> incomeBetween.25kto54k     -1.736495   2.360385  -0.736  0.46202    \n#> incomeBetween.55kto99k      0.170505   2.565896   0.066  0.94703    \n#> incomeOver100k              1.712359   2.860226   0.599  0.54946    \n#> diastolicBP                 0.355813   0.074380   4.784 1.86e-06 ***\n#> systolicBP                  0.037464   0.059848   0.626  0.53140    \n#> bmi                        -0.282881   0.139160  -2.033  0.04222 *  \n#> triglycerides               0.123797   0.007613  16.261  < 2e-16 ***\n#> uric.acid                   1.006499   0.712871   1.412  0.15815    \n#> protein                     1.721623   3.468969   0.496  0.61975    \n#> bilirubin                  -6.143411   3.006858  -2.043  0.04118 *  \n#> phosphorus                  0.093824   1.575489   0.060  0.95252    \n#> sodium                     -0.604286   0.400694  -1.508  0.13170    \n#> potassium                  -0.583525   2.715189  -0.215  0.82986    \n#> globulin                   -0.278970   3.614404  -0.077  0.93849    \n#> calcium                    15.679677   3.054968   5.133 3.17e-07 ***\n#> physical.workYes           -1.099540   1.960321  -0.561  0.57494    \n#> physical.recreationalYes    0.834737   1.953960   0.427  0.66928    \n#> diabetesYes               -19.932101   2.580138  -7.725 1.83e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 34.68 on 1815 degrees of freedom\n#> Multiple R-squared:  0.2433, Adjusted R-squared:  0.2316 \n#> F-statistic: 20.84 on 28 and 1815 DF,  p-value: < 2.2e-16\n\nExtract performance measures\nYou can use a saved function to measure the performance of the trained model. The function will return performance metrics like R-squared, RMSE, etc. This function is applied not just to the training data but also to the test data, the full dataset, and a separate, fictitious dataset.\n\n\n\n\n\n\nTip\n\n\n\nBelow we use the perform function that we saved to evaluate the model performances\n\n\n\nperform(new.data = train.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma   logLik      AIC\n#> [1,] 1844 29        1815 2182509 2884109 0.243 0.232 34.677 -9140.98 18341.96\n#>           BIC\n#> [1,] 18507.55\nperform(new.data = test.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>        n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 788 29         759 1057454 1372214 0.229 0.201 37.326 -3955.936 7971.873\n#>           BIC\n#> [1,] 8111.958\nperform(new.data = analytic3,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2 sigma    logLik      AIC\n#> [1,] 2632 29        2603 3239962 4256586 0.239 0.231 35.28 -13098.82 26257.64\n#>           BIC\n#> [1,] 26433.91\nperform(new.data = fictitious.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 4121 29        4092 5306559 6912485 0.232 0.227 36.011 -20601.92 41263.84\n#>           BIC\n#> [1,] 41453.55\n\nEvaluating the model’s performance on the test data provides insights into how well the model will generalize to new, unseen data. Comparing the performance metrics across different datasets can give you a robust view of your model’s predictive power and reliability.\n\n\nFor more on model training and tuning, see Kuhn (2023b)\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023a. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.\n\n\n———. 2023b. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html."
  },
  {
    "objectID": "predictivefactors6.html",
    "href": "predictivefactors6.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Cross-validation is another important technique used to assess the performance of machine learning models and mitigate the risk of overfitting. This tutorial focuses on k-fold cross-validation as a strategy to obtain a more generalized and robust assessment of the model’s performance. It shows both manual calculations for individual folds and an automated approach using the caret package. This ensures that you aren’t simply fitting your model well to a specific subset of your data but are achieving good performance in a general sense.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nk-fold cross-vaildation\n\n\nSee Wikipedia (2023)\nWe can set the number of folds to 5 (k = 5). A random seed is used for reproducibility. We use the function createFolds to create the folds. The data is divided based on the cholesterol levels, with each fold having approximately equal numbers of data points. The resulting structure contains training indices for each fold.\nWe can also examine the approximate size of training and test sets for each fold. The dimensions are displayed to understand the partitioning, and you can examine the length of indices in each fold to confirm the size of the training sets.\n\nk = 5\ndim(analytic3)\n#> [1] 2632   35\nset.seed(567)\n\n# Create folds (based on the outcome)\nfolds <- createFolds(analytic3$cholesterol, k = k, list = TRUE, \n                     returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\n\n# Approximate training data size\ndim(analytic3)*4/5\n#> [1] 2105.6   28.0\n\n# Approximate test data size\ndim(analytic3)/5  \n#> [1] 526.4   7.0\n\nlength(folds[[1]])\n#> [1] 2105\nlength(folds[[2]])\n#> [1] 2107\nlength(folds[[3]])\n#> [1] 2106\nlength(folds[[4]])\n#> [1] 2105\nlength(folds[[5]])\n#> [1] 2105\n\nstr(folds[[1]])\n#>  int [1:2105] 1 3 5 6 8 10 11 12 13 14 ...\nstr(folds[[2]])\n#>  int [1:2107] 1 2 3 4 5 6 7 8 9 12 ...\nstr(folds[[3]])\n#>  int [1:2106] 2 4 5 7 8 9 10 11 12 14 ...\nstr(folds[[4]])\n#>  int [1:2105] 1 2 3 4 6 7 8 9 10 11 ...\nstr(folds[[5]])\n#>  int [1:2105] 1 2 3 4 5 6 7 9 10 11 ...\n\nCalculation for Fold 1\nThe first fold is used as an example. The indices for the training data in the first fold are extracted and used to subset the main data set into training and test sets for that fold. Then a linear regression model is fitted using the training data, and predictions are made on the test set. The model’s performance is evaluated using the same performance function as before.\n\nfold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1]  1  3  5  6  8 10\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\nmodel.fit <- lm(formula4, data = fold1.train)\npredictions <- predict(model.fit, newdata = fold1.test)\n\nperform(new.data=fold1.test, y.name = \"cholesterol\", \n        model.fit = model.fit)\n#>        n  p df.residual      SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 527 29         498 637317.5 830983.2 0.233  0.19 35.774 -2618.471 5296.942\n#>           BIC\n#> [1,] 5424.958\n\nCalculation for Fold 2\nThe same process is repeated for the second fold. This way, you can manually evaluate how the model performs on different subsets of the data, making the performance assessment more robust.\n\nfold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 3 4 5 6\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\n\nmodel.fit <- lm(formula4, data = fold1.train)\n\npredictions <- predict(model.fit, newdata = fold1.test)\nperform(new.data=fold1.test, y.name = \"cholesterol\", \n        model.fit = model.fit)\n#>        n  p df.residual    SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 525 29         496 615243 785326.6 0.217 0.172 35.219 -2600.282 5260.564\n#>           BIC\n#> [1,] 5388.466\n\nUsing caret package to automate\n\n\nSee Kuhn (2023)\nInstead of manually running the process for each fold, the caret package can be used to automate k-fold cross-validation. A control object is set up specifying that 5-fold cross-validation should be used. Then, the train function from the caret package can be used to fit the linear regression model on each fold.\nAfter fitting, you can access summary results for each fold in the resampling results. This summary provides performance metrics such as R-squared for each fold. You can calculate the mean and standard deviation of these metrics to get an overall sense of the model’s performance.\nAdditionally, an adjusted R-squared can be calculated to consider the number of predictors in the model, giving a more accurate sense of the model’s explanatory power when you have multiple predictors.\n\n# Using Caret package\nset.seed(567)\n\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n\n# fit the model with formula = formula4\n# use training method lm\nfit4.cv<-train(formula4, trControl = ctrl,\n               data = analytic3, method = \"lm\")\nfit4.cv\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2105, 2106, 2105, 2106 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.62758  0.2194187  27.85731\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# extract results from each test data \nsummary.res <- fit4.cv$resample\nsummary.res\n\n\n\n  \n\n\nmean(fit4.cv$resample$Rsquared)\n#> [1] 0.2194187\nsd(fit4.cv$resample$Rsquared)\n#> [1] 0.02755561\n\n# # extract adj R2\n# k <- 5\n# p <- 2\n# n <- round(nrow(analytic3)/k)\n# summary.res$adjR2 <- 1-(1-fit4.cv$resample$Rsquared)*\n#  ((n-1)/(n-p))\n# summary.res\n\nReferences\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "predictivefactors7.html",
    "href": "predictivefactors7.html",
    "title": "Bootstrap",
    "section": "",
    "text": "The tutorial is on bootstrapping methods, mainly using R. Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling, with replacement (after a data point is chosen randomly from the original dataset and included in the sample, it is “replaced” back into the original dataset, making it possible for that same data point to be picked again in the same sampling process), from the observed data points. It is a way to quantify the uncertainty associated with a given estimator or statistical measure, such as the mean, median, variance, or correlation coefficient, among others. Bootstrapping is widely applicable and very straightforward to implement, which has made it a popular choice for statistical inference when analytical solutions are not available or are difficult to derive.\n\n\n\n\n\n\nImportant\n\n\n\nBootstrapping is a powerful statistical tool for making inferences by empirically estimating the sampling distribution of a statistic. It is especially useful when the underlying distribution is unknown or when an analytical solution is difficult to obtain.\n\n\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nResampling a vector\nHere, the document introduces basic resampling of a simple vector. The code creates a new sample using the sample function with replacement. It also discusses “out-of-bag” samples which are the samples not chosen during the resampling.\n\nfake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n\n\nresampled.fake.data <- sample(fake.data, size = length(fake.data), \n                              replace = TRUE)\nresampled.fake.data\n#> [1] 4 5 4 4 3\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 4 5 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 1 2\n\nThe samples not selected are known as the out-of-bag samples\n\nB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), \n                            replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 4 4 2 5 4\n#> [1] 1 1 5 3 5\n#> [1] 4 4 4 2 3\n#> [1] 2 3 2 3 1\n#> [1] 1 4 5 2 3\n#> [1] 2 1 3 4 3\n#> [1] 1 5 4 4 5\n#> [1] 1 3 3 4 1\n#> [1] 4 2 1 2 3\n#> [1] 5 2 4 3 2\n\nCalculating SD of a statistics\nWe introduce the concept of calculating confidence intervals (CIs) using bootstrapping when the distribution of data is not known. It uses resampling to create multiple bootstrap samples, then calculates means and standard deviations (SD) for those samples.\nIdea:\n\nNot sure about what distribution is appropriate to make inference?\nIf that is the case, calculating CI is hard.\nresample and get a new bootstrap sample\ncalculate a statistic (say, mean) from that sample\nfind SD of those statistic (say, means)\nUse those SD to calculate CI\n\n\nmean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 1 4 4 2 5\n#>  $ : int [1:5] 4 1 5 5 2\n#>  $ : int [1:5] 5 4 1 5 2\n#>  $ : int [1:5] 4 1 3 4 5\n#>  $ : int [1:5] 3 5 1 2 5\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 3.2 3.4 3.4 3.4 3.2\nmean(B.means)\n#> [1] 3.32\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.1095445\n\n\nmean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 2.953\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6295815\nmean(B.medians)\n#> [1] 2.935\nhist(B.means)\n\n\n\n\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 0.9723695\nhist(B.medians)\n\n\n\n\nResampling a data or matrix\nWe show how to resample a data frame or a matrix, and how to identify which rows have been selected and which haven’t, introducing the concept of “out-of-bag samples” for matrices.\n\nanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n1\n83732\nMale\n62\n\n\n2\n83733\nMale\n53\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n21\n83752\nFemale\n30\n\n\n\n\n\n\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n10.1\n83741\nMale\n22\n\n\n1\n83732\nMale\n62\n\n\n10.2\n83741\nMale\n22\n\n\n\n\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83741 83747 83750 83732\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83733 83752\n\n\nanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n16\n83747\nMale\n46\n\n\n2\n83733\nMale\n53\n\n\n16.1\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n19.1\n83750\nMale\n45\n\n\n1\n83732\nMale\n62\n\n\n\n\nselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83747 83733 83750 83732\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83741 83752\n\nThe caret package / boot\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time). We introduce the trainControl and train functions from the caret package. It sets up a linear model and demonstrates how bootstrapping can be done to estimate the variability in R-squared, a measure of goodness-of-fit for the model.\n\nset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n\n\n\n  \n\n\nmean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n\nMethod boot632\nA specific bootstrapping method called “boot632”, which aims to reduce bias but can provide unstable results if the sample size is small. Compared to the original bootstrap method, boot632 addresses the bias that is due to this the sampling with replacement.\n\n\nSee Raschka (2023)\n\nctrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n\nMethod boot632 for stepwise\nWe discuss the use of stepwise regression models in conjunction with the “boot632” method. It highlights the trade-offs and explains that models could be unstable depending on the data.\nA stable model\n\n\nSee Kuhn (2023)\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\nctrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n\nAn unstable model\n\nctrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n\nNote that SD should be higher for larger B.\nOptimism corrected bootstrap\nWe discuss a specific type of bootstrap called the “Optimism corrected bootstrap”. It’s a way to adjust performance metrics for the optimism that is often present when a model is tested on the data used to create it.\n\n\nSee Bondarenko and Consulting (2023)\nSteps:\n\nFit a model M to entire data D and estimate predictive ability R2.\nIterate from b=1 to B:\n\nTake a resample from the original data, and name it D.star\nFit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\nUse the bootstrap model M.star to get predictive ability on D, R2.fullData\n\n\nOptimism Opt is calculated as mean(R2.boot - R2.fullData)\nCalculate optimism corrected performance as R2-Opt.\n\n\nR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n\nBinary outcome\nHere, bootstrapping and cross-validation are used for a logistic regression model. It calculates the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), a measure for the performance of classification models.\n\nAUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\nAUC = 1 (perfect classification)\nAUC = 0.5 (random classification such as a coin toss)\n\n\nset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n\n\n\n  \n\n\nmean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n\nBrier Score is another metric for evaluating the performance of binary classification models. Brier Score is equivalent to the mean squared error, which we calculate for a continuous outcome. A Brier score of 0 indicates perfect accuracy and a score of 1 indicates perfect inaccuracy.\n\nrequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nBondarenko, Vadim, and FI Consulting. 2023. “The Bootstrap Approach to Managing Model Uncertainty.” https://rstudio-pubs-static.s3.amazonaws.com/90467_c70206f3dc864d53bf36072207ee011d.html.\n\n\nKuhn, Max. 2023. “Available Models.” https://topepo.github.io/caret/available-models.html.\n\n\nRaschka, Sebastian. 2023. “Bootstrap_point632_score: The .632 and .632+ Boostrap for Classifier Evaluation.” 2023. https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/."
  },
  {
    "objectID": "predictivefactorsF.html",
    "href": "predictivefactorsF.html",
    "title": "R functions (P)",
    "section": "",
    "text": "The list of new R functions introduced in this Predictive factors lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggregate \n    base/stats \n    To see summary by groups, e.g., by gender \n  \n\n anova \n    base/stats \n    To compare models \n  \n\n auc \n    pROC \n    To compute the AUC (area under the ROC curve) value \n  \n\n BrierScore \n    DescTools \n    To calculate the Brier score \n  \n\n coef \n    base/stats \n    To see the coefficients of a fitted model \n  \n\n cor \n    base/stats \n    To see the correlation between numeric variables \n  \n\n corrplot \n    corrplot \n    To visualize a correlation matrix \n  \n\n createDataPartition \n    caret \n    To split a dataset into training and testing sets \n  \n\n createFolds \n    caret \n    To create k folds based on the outcome variable \n  \n\n crPlots \n    car \n    To see partial residual plot \n  \n\n describeBy \n    psych \n    To see summary by groups, e.g., by gender \n  \n\n glm \n    base/stats \n    To run generalized linear models \n  \n\n group_by \n    dplyr \n    To group by variables \n  \n\n hat \n    base/stats \n    To return a hat matrix \n  \n\n ifelse \n    base \n    To set an condition, e.g., creating a categorical variable from a numerical variable based on a condition \n  \n\n kable \n    knitr \n    To create a nice table \n  \n\n layout \n    base/graphics \n    To specify plot arrangement \n  \n\n lines \n    base/graphics \n    To draw a line graph \n  \n\n lm \n    base/stats \n    To fit a linear regression \n  \n\n lowess \n    base/stats \n    To smooth a scatter plot \n  \n\n model.matrix \n    base/stats \n    To construct a design/model matrix, e.g., a matrix with covariate values \n  \n\n ols_plot_resid_lev \n    olsrr \n    To visualize the residuals vs leverage plot \n  \n\n ols_vif_tol \n    olsrr \n    To calculate tolerance and variance inflation factor \n  \n\n predict \n    base/stats \n    `predict` is a generic function that is used for prediction, e.g., predicting probability of an event from a model \n  \n\n R2 \n    caret \n    To calculate the R-squared value \n  \n\n RMSE \n    caret \n    To calculate the RMSE value \n  \n\n roc \n    pROC \n    To build a ROC curve \n  \n\n sample \n    base \n    To take/draw random samples with or without replacement \n  \n\n save.image \n    base \n    To save an R object \n  \n\n spearman2 \n    Hmisc \n    To compute the square of Spearman's rank correlation \n  \n\n summarize \n    dplyr \n    To see summary \n  \n\n tapply \n    base \n    To apply a function over an array, e.g., to see the summary of a variable by gender \n  \n\n train \n    caret \n    To fit the model with tuning hyperparameters \n  \n\n trainControl \n    caret \n    To tune the hyperparameters, i.e., controlling the parameters to train the model \n  \n\n varclus \n    Hmisc \n    We use the `varclus` function to identify collinear predictors with cluster analysis \n  \n\n vif \n    car \n    To calculate variance inflation factor \n  \n\n which \n    base \n    To see which indices are TRUE"
  },
  {
    "objectID": "predictivefactorsQ.html#live-quiz",
    "href": "predictivefactorsQ.html#live-quiz",
    "title": "Quiz (P)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "predictivefactorsQ.html#download-quiz",
    "href": "predictivefactorsQ.html#download-quiz",
    "title": "Quiz (P)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydata.html#background",
    "href": "surveydata.html#background",
    "title": "Survey data analysis",
    "section": "Background",
    "text": "Background\nThe chapter consists of a series of tutorials focused on conducting rigorous analyses of complex survey data, mainly using Canadian Community Health Survey (CCHS) and National Health and Nutrition Examination Survey (NHANES) datasets. The tutorials guide users through various stages of survey data analysis: formulating research questions via the PICOT framework, data preparation, quality assessment, and handling missing data. They cover both bivariate and multivariable statistical methods, such as logistic and linear regressions, emphasizing the need to account for complex survey design elements like weights, strata, and clusters to avoid biased estimates. Advanced statistical techniques like backward elimination and interaction effect assessments are also discussed. Predictive model performance is evaluated using metrics like AIC, pseudo R-squared, and ROC curves, along with specialized tests like Archer and Lemeshow Goodness of Fit. The tutorials serve as a comprehensive guide for anyone looking to delve deep into the intricacies of analyzing complex survey data effectively.\n\n\nThe foundation we’ve built in understanding various research questions, especially the distinction between causal and predictive inquiries, will be instrumental in our next phase. Survey data, with its rich and diverse information, often presents opportunities to address both causal and predictive questions. By leveraging the knowledge we’ve garnered about the intricacies of causality and the methodologies of prediction, we’ll be better equipped to extract meaningful insights from nationally representative survey data. This holistic approach ensures that we not only comprehend the underlying theories but also effectively apply them in real-world contexts, making our analysis robust, relevant, and impactful.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "surveydata.html#overview-of-tutorials",
    "href": "surveydata.html#overview-of-tutorials",
    "title": "Survey data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCCHS: Revisiting PICOT\nThe tutorial focuses on revisiting a research question concerning the relationship between osteoarthritis (OA) and cardiovascular disease (CVD) in Canadian adults, utilizing data from the Canadian Community Health Survey (CCHS) spanning from 2001 to 2005. The approach follows the PICOT framework, which specifies the target population, outcome, exposure and control groups, and timeline. The tutorial provides detailed steps for data preparation and analysis, from loading the necessary R packages to subsetting data based on a comprehensive set of variables like age, sex, marital status, and income among others. The variables are recoded into broader categories for easier analysis. The tutorial then combines different cycles of CCHS data into one comprehensive dataset. Potential confounders are also identified to better understand the relationship between OA and CVD.\n\n\nCCHS: Assessing data\nThis tutorial provides a comprehensive guide to data preparation and quality assessment. It emphasizes the importance of checking for missing data and visualizing it, creating summary tables to look for zero-cells in variables, and generating frequency tables for various variables to examine data distribution. Specific attention is given to the presence of problematic variables. Data dictionaries from different cycles are also consulted to ensure variable compatibility. After identifying and modifying problematic data, the tutorial also explains how to set appropriate reference levels for factors in the dataset and offers an option to create a new dataset that excludes missing values (although this is not generally recommended).\n\n\nCCHS: Bivariate analysis\nThis tutorial outlines how to examine relationships between two variables using R. The tutorial covers data preparation steps such as accumulating survey weights across cycles. It also highlights the handling of missing data and survey design specifications for weighted analyses. Descriptive weighted statistics are generated in tables, stratified by exposure and outcome, to provide insights for survey weighted logistic regression analysis. Additionally, proportions and design effects are calculated to account for the complex survey design’s impact on statistical estimates. The tutorial employs specialized chi-square tests, such as, Rao-Scott and Thomas-Rao modifications, to assess associations between variables, accounting for the survey’s complex design.\n\n\nCCHS: Regression analysis\nThis tutorial offers a comprehensive guide on conducting complex regression analyses on survey data using R. The tutorial starts by conducting basic data checks. It then performs both simple and multivariable logistic regression to explore the relationship between cardiovascular disease and osteoarthritis. Model fit is assessed using Akaike Information Criterion (AIC) and pseudo R-squared metrics. Variable selection techniques such as backward elimination and stepwise regression guided by AIC are applied to hone the model. The tutorial also delves into assessing interaction effects among variables like age, sex, and diabetes, incorporating significant interactions into the final model.\n\n\nCCHS: Model performance\nThe tutorial guides users through the process of evaluating logistic regression models fitted to complex survey data in R, focusing primarily on the Receiver Operating Characteristic (ROC) curves and Archer and Lemeshow Goodness of Fit tests. It introduces a specialized function for plotting ROC curves and calculating the Area Under the Curve (AUC) to gauge the model’s predictive accuracy, while taking survey weights into account. Grading guidelines for AUC values are provided for model discrimination quality. For model fit, the Archer and Lemeshow test is used. The tutorial also covers additional functionalities for dealing with strata and clusters in the survey data.\n\n\nNHANES: Predicting blood pressure\nThe tutorial provides a comprehensive guide for analyzing health survey data with a focus on how demographic factors like race, age, gender, and marital status relate to blood pressure levels using NHANES dataset. The tutorial constructs both bivariate and multivariate regression models. Additionally, the tutorial incorporates complex survey designs by creating a new survey design object that factors in sampling weight, strata, and clusters. It also generates box plots and summary statistics to visualize variations in blood pressure across different demographic groups, considering survey design. The tutorial emphasizes the importance of accounting for survey design features to avoid biased estimates and discusses the challenges of model overfitting and optimism when shifting from inference to prediction, recommending optimism-correction techniques.\n\n\nNHANES: Predicting cholesterol level\nIn the study using NHANES data, the goal was to predict cholesterol levels in adults based on various predictors such as gender, country of birth, race, education, marital status, income, BMI, and diabetes. The data was filtered to include only adults 18 years and older, and multiple statistical tests were conducted. Linear regression and logistic regression models were fitted, with results suggesting an association between gender and cholesterol level. Various statistical tests, including Wald tests and backward elimination, were employed to optimize the model. The study found that income was not a significant predictor for cholesterol levels, and interaction terms did not improve the model. Despite utilizing survey design features, the model had poor discriminatory power. However, Archer-Lemeshow Goodness of Fit test showed that the model fits the data well. The inclusion of age as an additional predictor led to different odds ratios, and the AIC value suggested that adding age improved the model.\n\n\nNHANES: Properly subsetting a design object\nThe tutorial provides a comprehensive guide on how to handle and analyze a subset of complex survey data from the NHANES study using R. It begins by checking for missing data. The focus is on subsetting data based on complete information, emphasizing the importance of accounting for the full complex survey design to obtain unbiased variance estimates. Logistic regression is then run on this subset, with the tutorial explicitly differentiating between correct and incorrect approaches to consider the survey design. Finally, variable selection methods like backward elimination are discussed to determine significant predictors, emphasizing the retention of variables deemed important based on prior research.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "surveydata0.html#survey-data-analysis",
    "href": "surveydata0.html#survey-data-analysis",
    "title": "Concepts (D)",
    "section": "Survey Data Analysis",
    "text": "Survey Data Analysis\nDesign-based analysis differs from model-based analysis in its approach to handling survey data. Design-based analysis emphasizes the importance of the survey’s sampling method and structure, focusing on representativeness and accurate variance estimation according to how the data was collected. It accounts for the complexities of the sampling design, e.g., stratification and clustering, to ensure that results are representative of the entire population. On the other hand, model-based analysis uses statistical models to understand relationships and patterns, assuming data come from a specific distribution and often relying on random sampling.\nUnderstanding survey features such as weights, strata, and clusters is crucial in complex survey data analysis. Survey weights adjust for unequal probabilities of selection and nonresponse, ensuring that the sample represents the population accurately. Stratification improves precision and representation of subgroups, while clustering, often used for practicality and cost considerations, must be accounted for to avoid underestimating standard errors. These features are vital in design-based analysis to provide unbiased, reliable estimates and are what fundamentally distinguish it from model-based approaches, which may not reflect the difficulties of complex survey structures. NHANES is used an an example to explain these ideas."
  },
  {
    "objectID": "surveydata0.html#reading-list",
    "href": "surveydata0.html#reading-list",
    "title": "Concepts (D)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Steven G. Heeringa, West, and Berglund 2017) (chapters 2 and 3)\nOptional reading: (Steven G. Heeringa, West, and Berglund 2014)\nTheoretical references (optional):\n\nF/chi-squared statistic with the Rao-Scott second-order correction (Rao and Scott 1984; Koch, Freeman Jr, and Freeman 1975; Thomas and Rao 1987)\nAIC and BIC for modeling with complex survey data (Lumley and Scott 2015)\nPseudo-R2 statistics under complex sampling (Lumley 2017)\nTests for regression models fitted to survey data (Lumley and Scott 2014)\nGoodness-of-fit test for a logistic regression model fitted using survey sample data (Archer and Lemeshow 2006)"
  },
  {
    "objectID": "surveydata0.html#video-lessons",
    "href": "surveydata0.html#video-lessons",
    "title": "Concepts (D)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nSurvey Data Analysis\n\n\n\nWhat is included in this Video Lesson:\n\nreference 00:38\ndesign-based 1:28\nexamples 3:33\nNHANES and sampling 4:54\nweights and other survey features 9:05\nestimate of interest 12:55\ndesign effect 15:52\nVariance estimation 18:13\ndesign-based analysis 25:11\nHow to make inference 29:33\ninappropriate analysis 32:08\nhow useful are sampling weights 36:15\nhow useful are psu/cluster info 37:42\nsubpopulation / subsetting 38:57\nmissingness collected to weights? 40:45\nDealing with subpopulation 41:38\n\nThe timestamps are also included in the YouTube video description."
  },
  {
    "objectID": "surveydata0.html#video-lesson-slides",
    "href": "surveydata0.html#video-lesson-slides",
    "title": "Concepts (D)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "surveydata0.html#links",
    "href": "surveydata0.html#links",
    "title": "Concepts (D)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "surveydata0.html#references",
    "href": "surveydata0.html#references",
    "title": "Concepts (D)",
    "section": "References",
    "text": "References\n\n\n\n\nArcher, K. J., and S. Lemeshow. 2006. “Goodness-of-Fit Test for a Logistic Regression Model Fitted Using Survey Sample Data.” The Stata Journal 6 (1): 97–105.\n\n\nHeeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2014. “Regression with Complex Samples.” In The SAGE Handbook of Regression Analysis and Causal Inference, edited by Henning Best and Christof Wolf. SAGE Publications.\n\n\nHeeringa, Steven G, Brady T West, and Patricia A Berglund. 2017. Applied Survey Data Analysis. Chapman; Hall/CRC.\n\n\nKoch, G. G., D. H. Freeman Jr, and J. L. Freeman. 1975. “Strategies in the Multivariate Analysis of Data from Complex Surveys.” International Statistical Review/Revue Internationale de Statistique, 59–78.\n\n\nLumley, Thomas. 2017. “Pseudo-R2 Statistics Under Complex Sampling.” Australian & New Zealand Journal of Statistics 59 (2): 187–94.\n\n\nLumley, Thomas, and Alan Scott. 2014. “Tests for Regression Models Fitted to Survey Data.” Australian & New Zealand Journal of Statistics 56 (1): 1–14.\n\n\n———. 2015. “AIC and BIC for Modeling with Complex Survey Data.” Journal of Survey Statistics and Methodology 3 (1): 1–18.\n\n\nRao, J. N. K., and A. J. Scott. 1984. “On Chi-Squared Tests for Multiway Contingency Tables with Cell Proportions Estimated from Survey Data.” The Annals of Statistics, 46–60.\n\n\nThomas, D. R., and J. N. K. Rao. 1987. “Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics Under Cluster Sampling.” Journal of the American Statistical Association 82 (398): 630–36."
  },
  {
    "objectID": "surveydata1.html#references",
    "href": "surveydata1.html#references",
    "title": "CCHS: Revisiting PICOT",
    "section": "References",
    "text": "References\n\n\n\n\nCanada, Statistics. 2005. “Canadian Community Health Survey (CCHS), Cycle 3.1.” Author Ottawa.\n\n\nKarim, Ehsan. 2023. “Case Study 2: Risk of Cardiovascular Disease Among Osteoarthritis Patients.” https://ssc.ca/en/case-study/case-study-2-risk-cardiovascular-disease-among-osteoarthritis-patients.\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "surveydata2.html",
    "href": "surveydata2.html",
    "title": "CCHS: Assessing data",
    "section": "",
    "text": "Let us load all the necessary packages for data manipulation, statistical analysis, and plotting.\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\n\nLoad data\nData loading that we saved earlier:\n\nload(\"Data/surveydata/cchs123.RData\")\nls()\n#> [1] \"analytic\" \"cc123a\"\n\nChecking\nCheck the data for missingness\nChecks the dimensions of the data and runs functions to explore missing data, stratifying by some variables. Additionally, it plots the missing data for visualization.\n\ndim(analytic)\n#> [1] 397173     24\nrequire(\"tableone\")\n#CreateTableOne(data = analytic, includeNA = TRUE)\nCreateTableOne(data = analytic, strata = \"CVD\", includeNA = TRUE)\n#>                       Stratified by CVD\n#>                        event                 no event              p      test\n#>   n                        25524                371121                        \n#>   CVD (%)                                                             NaN     \n#>      event                 25524 (100.0)             0 (  0.0)                \n#>      no event                  0 (  0.0)        371121 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years             330 (  1.3)         48293 ( 13.0)                \n#>      30-39 years             580 (  2.3)         63194 ( 17.0)                \n#>      40-49 years            1498 (  5.9)         63549 ( 17.1)                \n#>      50-59 years            3635 ( 14.2)         57300 ( 15.4)                \n#>      60-64 years            2720 ( 10.7)         22497 (  6.1)                \n#>      65 years and over     16496 ( 64.6)         64198 ( 17.3)                \n#>      teen                    265 (  1.0)         52090 ( 14.0)                \n#>   sex = Male (%)           12506 ( 49.0)        169776 ( 45.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single            13287 ( 52.1)        188687 ( 50.8)                \n#>      single                12207 ( 47.8)        181811 ( 49.0)                \n#>      NA                       30 (  0.1)           623 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white              1276 (  5.0)         37323 ( 10.1)                \n#>      White                 23629 ( 92.6)        325178 ( 87.6)                \n#>      NA                      619 (  2.4)          8620 (  2.3)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              11547 ( 45.2)        112678 ( 30.4)                \n#>      2nd grad.              3310 ( 13.0)         61355 ( 16.5)                \n#>      Other 2nd grad.        1323 (  5.2)         27643 (  7.4)                \n#>      Post-2nd grad.         8744 ( 34.3)        163052 ( 43.9)                \n#>      NA                      600 (  2.4)          6393 (  1.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       11664 ( 45.7)         89506 ( 24.1)                \n#>      $30,000-$49,999        4871 ( 19.1)         72994 ( 19.7)                \n#>      $50,000-$79,999        3193 ( 12.5)         81861 ( 22.1)                \n#>      $80,000 or more        1905 (  7.5)         73768 ( 19.9)                \n#>      NA                     3891 ( 15.2)         52992 ( 14.3)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight             504 (  2.0)          9600 (  2.6)                \n#>      healthy weight         7176 ( 28.1)        141200 ( 38.0)                \n#>      Overweight            12104 ( 47.4)        153887 ( 41.5)                \n#>      NA                     5740 ( 22.5)         66434 ( 17.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                 3642 ( 14.3)         94844 ( 25.6)                \n#>      Inactive              15494 ( 60.7)        174976 ( 47.1)                \n#>      Moderate               4928 ( 19.3)         88480 ( 23.8)                \n#>      NA                     1460 (  5.7)         12821 (  3.5)                \n#>   doctor (%)                                                       <0.001     \n#>      No                     1134 (  4.4)         57425 ( 15.5)                \n#>      Yes                   24384 ( 95.5)        313282 ( 84.4)                \n#>      NA                        6 (  0.0)           414 (  0.1)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed      20041 ( 78.5)        266358 ( 71.8)                \n#>      stressed               5184 ( 20.3)         76986 ( 20.7)                \n#>      NA                      299 (  1.2)         27777 (  7.5)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker         4481 ( 17.6)         93253 ( 25.1)                \n#>      Former smoker         13927 ( 54.6)        143421 ( 38.6)                \n#>      Never smoker           6981 ( 27.4)        132891 ( 35.8)                \n#>      NA                      135 (  0.5)          1556 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker       15852 ( 62.1)        279583 ( 75.3)                \n#>      Former driker          6820 ( 26.7)         48373 ( 13.0)                \n#>      Never drank            2421 (  9.5)         38195 ( 10.3)                \n#>      NA                      431 (  1.7)          4970 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving      4284 ( 16.8)         79088 ( 21.3)                \n#>      4-6 daily serving     10527 ( 41.2)        148684 ( 40.1)                \n#>      6+ daily serving       5047 ( 19.8)         73729 ( 19.9)                \n#>      NA                     5666 ( 22.2)         69620 ( 18.8)                \n#>   bp (%)                                                           <0.001     \n#>      No                    12611 ( 49.4)        315344 ( 85.0)                \n#>      Yes                   12857 ( 50.4)         55037 ( 14.8)                \n#>      NA                       56 (  0.2)           740 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                    23378 ( 91.6)        267481 ( 72.1)                \n#>      Yes                    1449 (  5.7)          3043 (  0.8)                \n#>      NA                      697 (  2.7)        100597 ( 27.1)                \n#>   diab (%)                                                         <0.001     \n#>      No                    20461 ( 80.2)        353817 ( 95.3)                \n#>      Yes                    5038 ( 19.7)         17138 (  4.6)                \n#>      NA                       25 (  0.1)           166 (  0.0)                \n#>   province = South (%)     25271 ( 99.0)        363659 ( 98.0)     <0.001     \n#>   weight (mean (SD))      152.58 (181.69)       203.40 (244.28)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                     7968 ( 31.2)        122798 ( 33.1)                \n#>      21                     9027 ( 35.4)        124838 ( 33.6)                \n#>      31                     8529 ( 33.4)        123485 ( 33.3)                \n#>   ID (mean (SD))       199839.07 (114705.35) 198466.74 (114661.51)  0.064     \n#>   OA (%)                                                           <0.001     \n#>      Control               12655 ( 49.6)        301675 ( 81.3)                \n#>      OA                     6522 ( 25.6)         34346 (  9.3)                \n#>      NA                     6347 ( 24.9)         35100 (  9.5)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years             2295 (  9.0)         24409 (  6.6)                \n#>      not immigrant         21342 ( 83.6)        316353 ( 85.2)                \n#>      recent                  159 (  0.6)         10476 (  2.8)                \n#>      NA                     1728 (  6.8)         19883 (  5.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND            519 (  2.0)          7398 (  2.0)                \n#>      PEI                     567 (  2.2)          7172 (  1.9)                \n#>      NOVA SCOTIA            1308 (  5.1)         14015 (  3.8)                \n#>      NEW BRUNSWICK          1223 (  4.8)         13786 (  3.7)                \n#>      QU\\xc9BEC              1380 (  5.4)         20625 (  5.6)                \n#>      ONTARIO                8596 ( 33.7)        115053 ( 31.0)                \n#>      MANITOBA               1339 (  5.2)         22074 (  5.9)                \n#>      SASKATCHEWAN           1542 (  6.0)         21782 (  5.9)                \n#>      ALBERTA                1837 (  7.2)         38238 ( 10.3)                \n#>      BRITISH COLUMBIA       2847 ( 11.2)         46834 ( 12.6)                \n#>      YUKON/NWT/NUNAVT        173 (  0.7)          4884 (  1.3)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                 3839 ( 15.0)         52850 ( 14.2)                \n#>      NFLD & LAB.             274 (  1.1)          3832 (  1.0)                \n#>      YUKON/NWT/NUNA.          80 (  0.3)          2578 (  0.7)\nCreateTableOne(data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control               OA                    p      test\n#>   n                       314542                 40943                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 12655 (  4.0)          6522 ( 15.9)                \n#>      no event             301675 ( 95.9)         34346 ( 83.9)                \n#>      NA                      212 (  0.1)            75 (  0.2)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           46805 ( 14.9)           537 (  1.3)                \n#>      30-39 years           59233 ( 18.8)          1622 (  4.0)                \n#>      40-49 years           55598 ( 17.7)          4128 ( 10.1)                \n#>      50-59 years           43746 ( 13.9)          8994 ( 22.0)                \n#>      60-64 years           15772 (  5.0)          5100 ( 12.5)                \n#>      65 years and over     41661 ( 13.2)         20436 ( 49.9)                \n#>      teen                  51727 ( 16.4)           126 (  0.3)                \n#>   sex = Male (%)          153889 ( 48.9)         11627 ( 28.4)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           158065 ( 50.3)         21794 ( 53.2)                \n#>      single               155952 ( 49.6)         19099 ( 46.6)                \n#>      NA                      525 (  0.2)            50 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             34028 ( 10.8)          1803 (  4.4)                \n#>      White                273378 ( 86.9)         38241 ( 93.4)                \n#>      NA                     7136 (  2.3)           899 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              92831 ( 29.5)         14539 ( 35.5)                \n#>      2nd grad.             52077 ( 16.6)          6291 ( 15.4)                \n#>      Other 2nd grad.       24099 (  7.7)          2484 (  6.1)                \n#>      Post-2nd grad.       140400 ( 44.6)         16887 ( 41.2)                \n#>      NA                     5135 (  1.6)           742 (  1.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       68530 ( 21.8)         16233 ( 39.6)                \n#>      $30,000-$49,999       61697 ( 19.6)          8360 ( 20.4)                \n#>      $50,000-$79,999       72657 ( 23.1)          6348 ( 15.5)                \n#>      $80,000 or more       67458 ( 21.4)          4191 ( 10.2)                \n#>      NA                    44200 ( 14.1)          5811 ( 14.2)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8660 (  2.8)           715 (  1.7)                \n#>      healthy weight       123416 ( 39.2)         12631 ( 30.9)                \n#>      Overweight           123898 ( 39.4)         20715 ( 50.6)                \n#>      NA                    58568 ( 18.6)          6882 ( 16.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                84269 ( 26.8)          6968 ( 17.0)                \n#>      Inactive             143058 ( 45.5)         23604 ( 57.7)                \n#>      Moderate              75703 ( 24.1)          9176 ( 22.4)                \n#>      NA                    11512 (  3.7)          1195 (  2.9)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53335 ( 17.0)          2221 (  5.4)                \n#>      Yes                  260802 ( 82.9)         38717 ( 94.6)                \n#>      NA                      405 (  0.1)             5 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     223212 ( 71.0)         31769 ( 77.6)                \n#>      stressed              63923 ( 20.3)          8998 ( 22.0)                \n#>      NA                    27407 (  8.7)           176 (  0.4)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        79521 ( 25.3)          8087 ( 19.8)                \n#>      Former smoker        117745 ( 37.4)         20267 ( 49.5)                \n#>      Never smoker         116006 ( 36.9)         12428 ( 30.4)                \n#>      NA                     1270 (  0.4)           161 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      239223 ( 76.1)         28622 ( 69.9)                \n#>      Former driker         37042 ( 11.8)          8668 ( 21.2)                \n#>      Never drank           34185 ( 10.9)          3128 (  7.6)                \n#>      NA                     4092 (  1.3)           525 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     68629 ( 21.8)          6571 ( 16.0)                \n#>      4-6 daily serving    125177 ( 39.8)         17214 ( 42.0)                \n#>      6+ daily serving      62121 ( 19.7)          9123 ( 22.3)                \n#>      NA                    58615 ( 18.6)          8035 ( 19.6)                \n#>   bp (%)                                                           <0.001     \n#>      No                   275443 ( 87.6)         25551 ( 62.4)                \n#>      Yes                   38442 ( 12.2)         15341 ( 37.5)                \n#>      NA                      657 (  0.2)            51 (  0.1)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213719 ( 67.9)         39007 ( 95.3)                \n#>      Yes                    2131 (  0.7)          1214 (  3.0)                \n#>      NA                    98692 ( 31.4)           722 (  1.8)                \n#>   diab (%)                                                         <0.001     \n#>      No                   301943 ( 96.0)         36211 ( 88.4)                \n#>      Yes                   12442 (  4.0)          4705 ( 11.5)                \n#>      NA                      157 (  0.0)            27 (  0.1)                \n#>   province = South (%)    307761 ( 97.8)         40507 ( 98.9)     <0.001     \n#>   weight (mean (SD))      211.50 (251.46)       159.00 (188.84)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106231 ( 33.8)         12052 ( 29.4)                \n#>      21                   104530 ( 33.2)         14750 ( 36.0)                \n#>      31                   103781 ( 33.0)         14141 ( 34.5)                \n#>   ID (mean (SD))       197003.20 (115147.95) 204459.43 (113014.25) <0.001     \n#>   OA (%)                                                              NaN     \n#>      Control              314542 (100.0)             0 (  0.0)                \n#>      OA                        0 (  0.0)         40943 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            19385 (  6.2)          3622 (  8.8)                \n#>      not immigrant        268962 ( 85.5)         34509 ( 84.3)                \n#>      recent                10187 (  3.2)           151 (  0.4)                \n#>      NA                    16008 (  5.1)          2661 (  6.5)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6315 (  2.0)           725 (  1.8)                \n#>      PEI                    5892 (  1.9)           817 (  2.0)                \n#>      NOVA SCOTIA           11081 (  3.5)          1880 (  4.6)                \n#>      NEW BRUNSWICK         11517 (  3.7)          1693 (  4.1)                \n#>      QU\\xc9BEC             19111 (  6.1)          2035 (  5.0)                \n#>      ONTARIO               95651 ( 30.4)         13669 ( 33.4)                \n#>      MANITOBA              18050 (  5.7)          2272 (  5.5)                \n#>      SASKATCHEWAN          17941 (  5.7)          2166 (  5.3)                \n#>      ALBERTA               32207 ( 10.2)          3608 (  8.8)                \n#>      BRITISH COLUMBIA      40034 ( 12.7)          4873 ( 11.9)                \n#>      YUKON/NWT/NUNAVT       4446 (  1.4)           273 (  0.7)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                46817 ( 14.9)          6366 ( 15.5)                \n#>      NFLD & LAB.            3145 (  1.0)           403 (  1.0)                \n#>      YUKON/NWT/NUNA.        2335 (  0.7)           163 (  0.4)\n\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLook for zero-cells\nCreates two new variables based on age groups and generates summary tables. It also comments on the presence of ‘zero cells’ in one of the variables, which might require further handling.\n\nanalytic$age.65p <- analytic$age.teen <- 0\nanalytic$age.teen[analytic$age == \"teen\"] <- 1\nanalytic$age.65p[analytic$age == \"65 years and over\"] <- 1\nCreateTableOne(data = analytic, strata = \"age.teen\", includeNA = TRUE)\n#>                       Stratified by age.teen\n#>                        0                     1                     p      test\n#>   n                       344786                 52387                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 25259 ( 7.3)            265 (  0.5)                \n#>      no event             319031 (92.5)          52090 ( 99.4)                \n#>      NA                      496 ( 0.1)             32 (  0.1)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (14.1)              0 (  0.0)                \n#>      30-39 years           63810 (18.5)              0 (  0.0)                \n#>      40-49 years           65111 (18.9)              0 (  0.0)                \n#>      50-59 years           61035 (17.7)              0 (  0.0)                \n#>      60-64 years           25265 ( 7.3)              0 (  0.0)                \n#>      65 years and over     80913 (23.5)              0 (  0.0)                \n#>      teen                      0 ( 0.0)          52387 (100.0)                \n#>   sex = Male (%)          155980 (45.2)          26543 ( 50.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           201528 (58.5)            685 (  1.3)                \n#>      single               142639 (41.4)          51660 ( 98.6)                \n#>      NA                      619 ( 0.2)             42 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             31107 ( 9.0)           7534 ( 14.4)                \n#>      White                305497 (88.6)          43725 ( 83.5)                \n#>      NA                     8182 ( 2.4)           1128 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              83649 (24.3)          40776 ( 77.8)                \n#>      2nd grad.             59205 (17.2)           5548 ( 10.6)                \n#>      Other 2nd grad.       24580 ( 7.1)           4420 (  8.4)                \n#>      Post-2nd grad.       170707 (49.5)           1265 (  2.4)                \n#>      NA                     6645 ( 1.9)            378 (  0.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       93630 (27.2)           7701 ( 14.7)                \n#>      $30,000-$49,999       69798 (20.2)           8142 ( 15.5)                \n#>      $50,000-$79,999       73596 (21.3)          11512 ( 22.0)                \n#>      $80,000 or more       63697 (18.5)          12018 ( 22.9)                \n#>      NA                    44065 (12.8)          13014 ( 24.8)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            7277 ( 2.1)           2839 (  5.4)                \n#>      healthy weight       138611 (40.2)           9922 ( 18.9)                \n#>      Overweight           163701 (47.5)           2520 (  4.8)                \n#>      NA                    35197 (10.2)          37106 ( 70.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                74738 (21.7)          23833 ( 45.5)                \n#>      Inactive             176573 (51.2)          14166 ( 27.0)                \n#>      Moderate              82158 (23.8)          11349 ( 21.7)                \n#>      NA                    11317 ( 3.3)           3039 (  5.8)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    49874 (14.5)           8749 ( 16.7)                \n#>      Yes                  294763 (85.5)          43342 ( 82.7)                \n#>      NA                      149 ( 0.0)            296 (  0.6)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     265391 (77.0)          21353 ( 40.8)                \n#>      stressed              78044 (22.6)           4253 (  8.1)                \n#>      NA                     1351 ( 0.4)          26781 ( 51.1)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88986 (25.8)           8866 ( 16.9)                \n#>      Former smoker        150004 (43.5)           7566 ( 14.4)                \n#>      Never smoker         104332 (30.3)          35685 ( 68.1)                \n#>      NA                     1464 ( 0.4)            270 (  0.5)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      268269 (77.8)          27464 ( 52.4)                \n#>      Former driker         50929 (14.8)           4370 (  8.3)                \n#>      Never drank           20754 ( 6.0)          19916 ( 38.0)                \n#>      NA                     4834 ( 1.4)            637 (  1.2)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72392 (21.0)          11049 ( 21.1)                \n#>      4-6 daily serving    139753 (40.5)          19627 ( 37.5)                \n#>      6+ daily serving      66831 (19.4)          12026 ( 23.0)                \n#>      NA                    65810 (19.1)           9685 ( 18.5)                \n#>   bp (%)                                                           <0.001     \n#>      No                   276318 (80.1)          51846 ( 99.0)                \n#>      Yes                   67763 (19.7)            308 (  0.6)                \n#>      NA                      705 ( 0.2)            233 (  0.4)                \n#>   copd (%)                                                         <0.001     \n#>      No                   291191 (84.5)              0 (  0.0)                \n#>      Yes                    4508 ( 1.3)              0 (  0.0)                \n#>      NA                    49087 (14.2)          52387 (100.0)                \n#>   diab (%)                                                         <0.001     \n#>      No                   322448 (93.5)          52141 ( 99.5)                \n#>      Yes                   22032 ( 6.4)            199 (  0.4)                \n#>      NA                      306 ( 0.1)             47 (  0.1)                \n#>   province = South (%)    338450 (98.2)          51001 ( 97.4)     <0.001     \n#>   weight (mean (SD))      201.76 (245.97)       189.09 (205.24)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   113323 (32.9)          17557 ( 33.5)                \n#>      21                   115548 (33.5)          18524 ( 35.4)                \n#>      31                   115915 (33.6)          16306 ( 31.1)                \n#>   ID (mean (SD))       199143.77 (114810.36) 194922.59 (113553.38) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              262815 (76.2)          51727 ( 98.7)                \n#>      OA                    40817 (11.8)            126 (  0.2)                \n#>      NA                    41154 (11.9)            534 (  1.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            25976 ( 7.5)            770 (  1.5)                \n#>      not immigrant        289651 (84.0)          48427 ( 92.4)                \n#>      recent                 8710 ( 2.5)           1934 (  3.7)                \n#>      NA                    20449 ( 5.9)           1256 (  2.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6646 ( 1.9)           1278 (  2.4)                \n#>      PEI                    6802 ( 2.0)            942 (  1.8)                \n#>      NOVA SCOTIA           13337 ( 3.9)           2004 (  3.8)                \n#>      NEW BRUNSWICK         13057 ( 3.8)           1968 (  3.8)                \n#>      QU\\xc9BEC             19186 ( 5.6)           2826 (  5.4)                \n#>      ONTARIO              107768 (31.3)          16053 ( 30.6)                \n#>      MANITOBA              20362 ( 5.9)           3092 (  5.9)                \n#>      SASKATCHEWAN          20160 ( 5.8)           3201 (  6.1)                \n#>      ALBERTA               34293 ( 9.9)           5834 ( 11.1)                \n#>      BRITISH COLUMBIA      43431 (12.6)           6336 ( 12.1)                \n#>      YUKON/NWT/NUNAVT       4170 ( 1.2)            894 (  1.7)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                49806 (14.4)           6958 ( 13.3)                \n#>      NFLD & LAB.            3602 ( 1.0)            509 (  1.0)                \n#>      YUKON/NWT/NUNA.        2166 ( 0.6)            492 (  0.9)                \n#>   age.teen (mean (SD))      0.00 (0.00)           1.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.23 (0.42)           0.00 (0.00)      <0.001\n# copd has zero cells\n# analytic$age[analytic$age == 'teen'] <- NA (will set this if we use copd)\n\n\nCreateTableOne(data = analytic, strata = \"age.65p\", includeNA = TRUE)\n#>                       Stratified by age.65p\n#>                        0                     1                     p      test\n#>   n                       316260                 80913                        \n#>   CVD (%)                                                          <0.001     \n#>      event                  9028 ( 2.9)          16496 ( 20.4)                \n#>      no event             306923 (97.0)          64198 ( 79.3)                \n#>      NA                      309 ( 0.1)            219 (  0.3)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (15.4)              0 (  0.0)                \n#>      30-39 years           63810 (20.2)              0 (  0.0)                \n#>      40-49 years           65111 (20.6)              0 (  0.0)                \n#>      50-59 years           61035 (19.3)              0 (  0.0)                \n#>      60-64 years           25265 ( 8.0)              0 (  0.0)                \n#>      65 years and over         0 ( 0.0)          80913 (100.0)                \n#>      teen                  52387 (16.6)              0 (  0.0)                \n#>   sex = Male (%)          150152 (47.5)          32371 ( 40.0)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           163660 (51.7)          38553 ( 47.6)                \n#>      single               152077 (48.1)          42222 ( 52.2)                \n#>      NA                      523 ( 0.2)            138 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             35329 (11.2)           3312 (  4.1)                \n#>      White                274000 (86.6)          75222 ( 93.0)                \n#>      NA                     6931 ( 2.2)           2379 (  2.9)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              84832 (26.8)          39593 ( 48.9)                \n#>      2nd grad.             53974 (17.1)          10779 ( 13.3)                \n#>      Other 2nd grad.       25305 ( 8.0)           3695 (  4.6)                \n#>      Post-2nd grad.       147385 (46.6)          24587 ( 30.4)                \n#>      NA                     4764 ( 1.5)           2259 (  2.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       62513 (19.8)          38818 ( 48.0)                \n#>      $30,000-$49,999       62296 (19.7)          15644 ( 19.3)                \n#>      $50,000-$79,999       77283 (24.4)           7825 (  9.7)                \n#>      $80,000 or more       72566 (22.9)           3149 (  3.9)                \n#>      NA                    41602 (13.2)          15477 ( 19.1)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8588 ( 2.7)           1528 (  1.9)                \n#>      healthy weight       124932 (39.5)          23601 ( 29.2)                \n#>      Overweight           136225 (43.1)          29996 ( 37.1)                \n#>      NA                    46515 (14.7)          25788 ( 31.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                85384 (27.0)          13187 ( 16.3)                \n#>      Inactive             144019 (45.5)          46720 ( 57.7)                \n#>      Moderate              76602 (24.2)          16905 ( 20.9)                \n#>      NA                    10255 ( 3.2)           4101 (  5.1)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53972 (17.1)           4651 (  5.7)                \n#>      Yes                  261866 (82.8)          76239 ( 94.2)                \n#>      NA                      422 ( 0.1)             23 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     215454 (68.1)          71290 ( 88.1)                \n#>      stressed              73402 (23.2)           8895 ( 11.0)                \n#>      NA                    27404 ( 8.7)            728 (  0.9)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88068 (27.8)           9784 ( 12.1)                \n#>      Former smoker        115111 (36.4)          42459 ( 52.5)                \n#>      Never smoker         111879 (35.4)          28138 ( 34.8)                \n#>      NA                     1202 ( 0.4)            532 (  0.7)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      245156 (77.5)          50577 ( 62.5)                \n#>      Former driker         35401 (11.2)          19898 ( 24.6)                \n#>      Never drank           31888 (10.1)           8782 ( 10.9)                \n#>      NA                     3815 ( 1.2)           1656 (  2.0)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72908 (23.1)          10533 ( 13.0)                \n#>      4-6 daily serving    124621 (39.4)          34759 ( 43.0)                \n#>      6+ daily serving      61855 (19.6)          17002 ( 21.0)                \n#>      NA                    56876 (18.0)          18619 ( 23.0)                \n#>   bp (%)                                                           <0.001     \n#>      No                   282174 (89.2)          45990 ( 56.8)                \n#>      Yes                   33346 (10.5)          34725 ( 42.9)                \n#>      NA                      740 ( 0.2)            198 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213221 (67.4)          77970 ( 96.4)                \n#>      Yes                    1791 ( 0.6)           2717 (  3.4)                \n#>      NA                   101248 (32.0)            226 (  0.3)                \n#>   diab (%)                                                         <0.001     \n#>      No                   305027 (96.4)          69562 ( 86.0)                \n#>      Yes                   10974 ( 3.5)          11257 ( 13.9)                \n#>      NA                      259 ( 0.1)             94 (  0.1)                \n#>   province = South (%)    309016 (97.7)          80435 ( 99.4)     <0.001     \n#>   weight (mean (SD))      215.36 (255.33)       140.38 (160.88)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106647 (33.7)          24233 ( 29.9)                \n#>      21                   105506 (33.4)          28566 ( 35.3)                \n#>      31                   104107 (32.9)          28114 ( 34.7)                \n#>   ID (mean (SD))       197072.98 (115035.66) 204504.77 (112956.66) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              272881 (86.3)          41661 ( 51.5)                \n#>      OA                    20507 ( 6.5)          20436 ( 25.3)                \n#>      NA                    22872 ( 7.2)          18816 ( 23.3)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            17607 ( 5.6)           9139 ( 11.3)                \n#>      not immigrant        273622 (86.5)          64456 ( 79.7)                \n#>      recent                10325 ( 3.3)            319 (  0.4)                \n#>      NA                    14706 ( 4.6)           6999 (  8.7)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6665 ( 2.1)           1259 (  1.6)                \n#>      PEI                    5993 ( 1.9)           1751 (  2.2)                \n#>      NOVA SCOTIA           11896 ( 3.8)           3445 (  4.3)                \n#>      NEW BRUNSWICK         11856 ( 3.7)           3169 (  3.9)                \n#>      QU\\xc9BEC             18128 ( 5.7)           3884 (  4.8)                \n#>      ONTARIO               97660 (30.9)          26161 ( 32.3)                \n#>      MANITOBA              17967 ( 5.7)           5487 (  6.8)                \n#>      SASKATCHEWAN          17507 ( 5.5)           5854 (  7.2)                \n#>      ALBERTA               33445 (10.6)           6682 (  8.3)                \n#>      BRITISH COLUMBIA      39394 (12.5)          10373 ( 12.8)                \n#>      YUKON/NWT/NUNAVT       4765 ( 1.5)            299 (  0.4)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                45226 (14.3)          11538 ( 14.3)                \n#>      NFLD & LAB.            3279 ( 1.0)            832 (  1.0)                \n#>      YUKON/NWT/NUNA.        2479 ( 0.8)            179 (  0.2)                \n#>   age.teen (mean (SD))      0.17 (0.37)           0.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.00 (0.00)           1.00 (0.00)      <0.001\nanalytic$age.65p <- analytic$age.teen <- NULL\n\nProduces frequency tables for multiple variable combinations to check the distribution of the data and identify issues.\n\ntable(analytic$province.check,analytic$fruit)\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  2991              3401             1237\n#>   PEI                           2280              3654             1506\n#>   NOVA SCOTIA                   3089              4804             1974\n#>   NEW BRUNSWICK                 2989              4730             1880\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      28752             59466            30746\n#>   MANITOBA                      4561              7669             3095\n#>   SASKATCHEWAN                  4173              7390             3003\n#>   ALBERTA                      10828             18901             8251\n#>   BRITISH COLUMBIA             10726             24422            12390\n#>   YUKON/NWT/NUNAVT              1829              2045             1023\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\ntable(analytic$age)\n#> \n#>       20-29 years       30-39 years       40-49 years       50-59 years \n#>             48652             63810             65111             61035 \n#>       60-64 years 65 years and over              teen \n#>             25265             80913             52387\ntable(analytic$copd, analytic$age)\n#>      \n#>       20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   No            0       63645       64735       60203       24638\n#>   Yes           0         117         320         768         586\n#>      \n#>       65 years and over  teen\n#>   No              77970     0\n#>   Yes              2717     0\ntable(analytic$stress, analytic$age) \n#>                   \n#>                    20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   Not too stressed       37117       45494       45316       45226       20948\n#>   stressed               11472       18197       19639       15623        4218\n#>                   \n#>                    65 years and over  teen\n#>   Not too stressed             71290 21353\n#>   stressed                      8895  4253\n\n\nuniverse 15 + is not an issue for stress as age starts from 20\n\ncopd is problematic!\n\nCreates tables to look at the distribution of a specific variable across different cycles (time periods) of the survey. Notes differences and issues.\n\n\nfruit variable measured in an optional component (not available in all cycles)\n\n\ntable(analytic$province.check[analytic$cycle==11],\n      analytic$fruit[analytic$cycle==11])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1512              1643              689\n#>   PEI                           1084              1738              773\n#>   NOVA SCOTIA                   1732              2500             1036\n#>   NEW BRUNSWICK                 1663              2363              934\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      10437             19478             8809\n#>   MANITOBA                      2604              4214             1526\n#>   SASKATCHEWAN                  2386              3957             1387\n#>   ALBERTA                       4391              7050             2664\n#>   BRITISH COLUMBIA              4321              9350             4278\n#>   YUKON/NWT/NUNAVT               999              1014              448\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n\n\ntable(analytic$province.check[analytic$cycle==21],\n      analytic$fruit[analytic$cycle==21])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1479              1758              548\n#>   PEI                            615               947              344\n#>   NOVA SCOTIA                   1357              2304              938\n#>   NEW BRUNSWICK                 1326              2367              946\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       9365             20356            10933\n#>   MANITOBA                      1957              3455             1569\n#>   SASKATCHEWAN                  1787              3433             1616\n#>   ALBERTA                       3326              6376             3046\n#>   BRITISH COLUMBIA              3186              7727             4224\n#>   YUKON/NWT/NUNAVT               830              1031              575\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# a different QUEBEC spelling used\n\n\ntable(analytic$province.check[analytic$cycle==31],\n      analytic$fruit[analytic$cycle==31])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                     0                 0                0\n#>   PEI                            581               969              389\n#>   NOVA SCOTIA                      0                 0                0\n#>   NEW BRUNSWICK                    0                 0                0\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       8950             19632            11004\n#>   MANITOBA                         0                 0                0\n#>   SASKATCHEWAN                     0                 0                0\n#>   ALBERTA                       3111              5475             2541\n#>   BRITISH COLUMBIA              3219              7345             3888\n#>   YUKON/NWT/NUNAVT                 0                 0                0\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# The real problem!\n\n\nLook at data dictionaries in all cycles\n\ncycle 1.1 FVCADTOT Universe: All respondents\ncycle 2.1 FVCCDTOT Universe: All respondents\ncycle 3.1 FVCEDTOT Universe: Respondents with FVCEFOPT = 1\n\n\n\nBelow we delete or modify problematic data, and removes unnecessary variables. Checks the dimensions before and after data cleanup.\n\ndim(analytic)\n#> [1] 397173     24\nanalytic1 <- analytic\n# analytic1$South[analytic1$province.check == \"NFLD & LAB.\"] <- NA\n# analytic1$South[analytic1$province.check == \"YUKON/NWT/NUNA.\"] <- NA\n# analytic1 <- subset(analytic, province.check != \"NFLD & LAB.\" & \n#                       province.check != \"YUKON/NWT/NUNA.\" )\ndim(analytic1)\n#> [1] 397173     24\n\nanalytic1$copd <- NULL # will bring this later for missing data analysis\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n# analytic1 <- droplevels.data.frame(analytic1)\nanalytic1$province.check <- NULL # we already have simplified province variable\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n\nSet appropriate reference\nSave the original data (with missing values)!\n\nanalytic.miss <- analytic1\n\nRelevels factors in the dataset so that a specific level is set as the reference level. This is often needed for statistical analysis.\n\nanalytic.miss$smoke <- relevel(as.factor(analytic.miss$smoke), ref='Never smoker')\nanalytic.miss$drink <- relevel(as.factor(analytic.miss$drink), ref='Never drank')\nanalytic.miss$province <- relevel(as.factor(analytic.miss$province), ref='South')\nanalytic.miss$immigrate <- relevel(as.factor(analytic.miss$immigrate), ref='not immigrant')\n\nComplete data options\nCreates a new dataset that omits all rows containing any missing values. This is generally not recommended for most data analysis, as it can introduce bias.\n\n# Wrong thing to do for survey data analysis!!\nanalytic2 <- as.data.frame(na.omit(analytic1)) \ndim(analytic2)\n#> [1] 185613     22\n# tab1 <- CreateTableOne(data = analytic2, strata = \"OA\", includeNA = TRUE)\n# print(tab1, test=FALSE, showAllLevels = TRUE)\n\nSaving dataset\nLet us check the dimensions of multiple data objects and then save them to a file for future use.\n\ndim(cc123a)\n#> [1] 397173     25\ndim(analytic)\n#> [1] 397173     24\ndim(analytic.miss)\n#> [1] 397173     22\ndim(analytic2)\n#> [1] 185613     22\nsave(analytic.miss, analytic2, file = \"Data/surveydata/cchs123b.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata3.html",
    "href": "surveydata3.html",
    "title": "CCHS: Bivariate analysis",
    "section": "",
    "text": "The following tutorial is performing bivariate analysis on our CCHS analytic dataset to examine relationships between two variables (association question).\nWe load several R packages required for bivariate analysis, statistical tests, and data visualization.\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\n\nLoad data\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/surveydata/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"\n\nPreparing data\nWeights\nHere, the weights of survey respondents are accumulated, to account for the combination of different cycles of the data.\n\nanalytic.miss$weight <- analytic.miss$weight/3 # 3 cycles combined\n\nFixing variable types\nWe convert several variables to categorical or “factor” types, which are better suited for some statistical analysis when variables have categories.\n\nvar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"OA\", \"immigrate\")\nanalytic.miss[var.names] <- lapply(analytic.miss[var.names] , factor)\nstr(analytic.miss)\n#> 'data.frame':    397173 obs. of  22 variables:\n#>  $ CVD      : Factor w/ 2 levels \"event\",\"no event\": 1 2 2 2 2 2 2 2 2 2 ...\n#>  $ age      : Factor w/ 7 levels \"20-29 years\",..: 6 6 2 6 1 6 3 7 1 1 ...\n#>  $ sex      : Factor w/ 2 levels \"Female\",\"Male\": 1 1 2 1 1 2 2 2 1 2 ...\n#>  $ married  : Factor w/ 2 levels \"not single\",\"single\": 2 2 1 2 2 1 1 2 2 2 ...\n#>  $ race     : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ edu      : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 2 4 4 4 4 4 4 1 4 4 ...\n#>  $ income   : Factor w/ 4 levels \"$29,999 or less\",..: 1 1 4 1 2 2 1 1 NA 4 ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#>  $ phyact   : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 2 2 2 2 2 1 1 2 3 ...\n#>  $ doctor   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ stress   : Factor w/ 2 levels \"Not too stressed\",..: 1 1 2 1 1 1 1 NA 1 1 ...\n#>  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#>  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#>  $ bp       : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ diab     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ weight   : num  47.6 23.8 56.1 23.8 65.4 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ OA       : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\nThe code identifies rows where data is missing and labels them for later analyses.\n\nanalytic.miss$miss <- 1\nhead(analytic.miss$ID) # full data\n#> [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#> [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#> [1]  3  5  7 10 11 13\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] <- 0\ntable(analytic.miss$miss)\n#> \n#>      0      1 \n#> 185613 211560\n\nSetting Design\nThe code sets up the survey design, specifying weights (but no specific clustering and stratification, as they are unavailable for CCHS public access data), for use in survey-weighted analyses.\n\nrequire(survey)\nsummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#> [1] 80.34263\n\nThis creates a subset of the data where there are no missing values. Note that subset was done to the design object w.design0, not the data analytic.miss.\n\nw.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsd(weights(w.design))\n#> [1] 84.97819\n\nBivariate analysis\nTable 1 (weighted)\nStratified by exposure\nThese tables contain descriptive statistics, stratified by different categories. They can be useful for understanding how variables relate to the exposure or outcome in the data.\n\nrequire(tableone)\nvar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"OA\"\n# tab1 <- CreateTableOne(var = var.names, strata= \"OA\", data=analytic.miss, test = TRUE)\n# print(tab1)\ntab2 <- svyCreateTableOne(var = var.names, strata= \"OA\", \n                          data=w.design, test = TRUE)\nprint(tab2)\n#>                        Stratified by OA\n#>                         Control            OA                p      test\n#>   n                     12124961.5         1153392.2                    \n#>   CVD = no event (%)    11786450.6 (97.2)  1020965.5 (88.5)  <0.001     \n#>   age (%)                                                    <0.001     \n#>      20-29 years         2668880.8 (22.0)    28317.5 ( 2.5)             \n#>      30-39 years         3009426.7 (24.8)    77159.3 ( 6.7)             \n#>      40-49 years         3108300.9 (25.6)   211515.1 (18.3)             \n#>      50-59 years         1900845.3 (15.7)   350264.7 (30.4)             \n#>      60-64 years          546041.8 ( 4.5)   163724.7 (14.2)             \n#>      65 years and over    624706.7 ( 5.2)   322033.4 (27.9)             \n#>      teen                 266759.3 ( 2.2)      377.4 ( 0.0)             \n#>   sex = Male (%)         6374765.5 (52.6)   379850.5 (32.9)  <0.001     \n#>   married = single (%)   4120611.0 (34.0)   367647.7 (31.9)  <0.001     \n#>   race = White (%)      10312228.2 (85.0)  1081778.6 (93.8)  <0.001     \n#>   edu (%)                                                    <0.001     \n#>      < 2ndary            1752318.3 (14.5)   309652.8 (26.8)             \n#>      2nd grad.           2314713.1 (19.1)   203437.5 (17.6)             \n#>      Other 2nd grad.     1078645.2 ( 8.9)    79255.1 ( 6.9)             \n#>      Post-2nd grad.      6979284.9 (57.6)   561046.8 (48.6)             \n#>   income (%)                                                 <0.001     \n#>      $29,999 or less     2051640.6 (16.9)   353862.9 (30.7)             \n#>      $30,000-$49,999     2436063.7 (20.1)   272484.1 (23.6)             \n#>      $50,000-$79,999     3495902.5 (28.8)   275115.8 (23.9)             \n#>      $80,000 or more     4141354.6 (34.2)   251929.4 (21.8)             \n#>   bmi (%)                                                    <0.001     \n#>      Underweight          346004.9 ( 2.9)    22064.6 ( 1.9)             \n#>      healthy weight      6019004.1 (49.6)   431570.2 (37.4)             \n#>      Overweight          5759952.5 (47.5)   699757.4 (60.7)             \n#>   phyact (%)                                                 <0.001     \n#>      Active              3037314.2 (25.1)   216879.5 (18.8)             \n#>      Inactive            5982492.3 (49.3)   647856.2 (56.2)             \n#>      Moderate            3105154.9 (25.6)   288656.5 (25.0)             \n#>   doctor = Yes (%)      10087473.8 (83.2)  1090763.9 (94.6)  <0.001     \n#>   stress = stressed (%)  3123770.9 (25.8)   301895.2 (26.2)   0.420     \n#>   smoke (%)                                                  <0.001     \n#>      Never smoker        4043479.9 (33.3)   320323.7 (27.8)             \n#>      Current smoker      3219168.6 (26.5)   275835.7 (23.9)             \n#>      Former smoker       4862313.0 (40.1)   557232.7 (48.3)             \n#>   drink (%)                                                  <0.001     \n#>      Never drank          678435.7 ( 5.6)    66085.0 ( 5.7)             \n#>      Current drinker    10297713.4 (84.9)   887808.2 (77.0)             \n#>      Former driker       1148812.3 ( 9.5)   199498.9 (17.3)             \n#>   fruit (%)                                                  <0.001     \n#>      0-3 daily serving   3214156.0 (26.5)   236483.8 (20.5)             \n#>      4-6 daily serving   6001124.3 (49.5)   588323.8 (51.0)             \n#>      6+ daily serving    2909681.1 (24.0)   328584.5 (28.5)             \n#>   bp = Yes (%)           1212548.2 (10.0)   347269.8 (30.1)  <0.001     \n#>   diab = Yes (%)          377876.2 ( 3.1)   104541.0 ( 9.1)  <0.001     \n#>   province = North (%)     27124.3 ( 0.2)     1825.1 ( 0.2)  <0.001     \n#>   immigrate (%)                                              <0.001     \n#>      not immigrant       9898636.6 (81.6)   994682.5 (86.2)             \n#>      > 10 years          1384672.6 (11.4)   146879.8 (12.7)             \n#>      recent               841652.3 ( 6.9)    11829.8 ( 1.0)\n\nStratified by outcome\nThis table is generally useful for logistic regression analysis\n\nvar.names <- c(\"OA\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"CVD\"\ntab3 <- svyCreateTableOne(var = var.names, strata= \"CVD\", data=w.design, test = TRUE)\nprint(tab3)\n#>                        Stratified by CVD\n#>                         event            no event           p      test\n#>   n                     470937.5         12807416.1                    \n#>   OA = OA (%)           132426.7 (28.1)   1020965.5 ( 8.0)  <0.001     \n#>   age (%)                                                   <0.001     \n#>      20-29 years         14966.0 ( 3.2)   2682232.3 (20.9)             \n#>      30-39 years         24105.5 ( 5.1)   3062480.5 (23.9)             \n#>      40-49 years         63520.1 (13.5)   3256296.0 (25.4)             \n#>      50-59 years        122613.0 (26.0)   2128497.0 (16.6)             \n#>      60-64 years         72328.3 (15.4)    637438.1 ( 5.0)             \n#>      65 years and over  172742.2 (36.7)    773997.8 ( 6.0)             \n#>      teen                  662.3 ( 0.1)    266474.4 ( 2.1)             \n#>   sex = Male (%)        267743.8 (56.9)   6486872.2 (50.6)  <0.001     \n#>   married = single (%)  143747.0 (30.5)   4344511.8 (33.9)  <0.001     \n#>   race = White (%)      434591.6 (92.3)  10959415.2 (85.6)  <0.001     \n#>   edu (%)                                                   <0.001     \n#>      < 2ndary           147338.4 (31.3)   1914632.6 (14.9)             \n#>      2nd grad.           77705.6 (16.5)   2440445.0 (19.1)             \n#>      Other 2nd grad.     30921.3 ( 6.6)   1126979.0 ( 8.8)             \n#>      Post-2nd grad.     214972.2 (45.6)   7325359.4 (57.2)             \n#>   income (%)                                                <0.001     \n#>      $29,999 or less    164929.4 (35.0)   2240574.2 (17.5)             \n#>      $30,000-$49,999    109988.2 (23.4)   2598559.6 (20.3)             \n#>      $50,000-$79,999    103091.1 (21.9)   3667927.1 (28.6)             \n#>      $80,000 or more     92928.7 (19.7)   4300355.3 (33.6)             \n#>   bmi (%)                                                   <0.001     \n#>      Underweight          8844.4 ( 1.9)    359225.0 ( 2.8)             \n#>      healthy weight     173475.1 (36.8)   6277099.2 (49.0)             \n#>      Overweight         288617.9 (61.3)   6171091.9 (48.2)             \n#>   phyact (%)                                                <0.001     \n#>      Active              85140.3 (18.1)   3169053.4 (24.7)             \n#>      Inactive           274968.8 (58.4)   6355379.7 (49.6)             \n#>      Moderate           110828.4 (23.5)   3282983.0 (25.6)             \n#>   doctor = Yes (%)      445493.3 (94.6)  10732744.5 (83.8)  <0.001     \n#>   stress = stressed (%) 113282.5 (24.1)   3312383.7 (25.9)   0.023     \n#>   smoke (%)                                                 <0.001     \n#>      Never smoker       119434.6 (25.4)   4244368.9 (33.1)             \n#>      Current smoker      97328.0 (20.7)   3397676.3 (26.5)             \n#>      Former smoker      254174.9 (54.0)   5165370.9 (40.3)             \n#>   drink (%)                                                 <0.001     \n#>      Never drank         29444.3 ( 6.3)    715076.4 ( 5.6)             \n#>      Current drinker    344405.1 (73.1)  10841116.6 (84.6)             \n#>      Former driker       97088.1 (20.6)   1251223.1 ( 9.8)             \n#>   fruit (%)                                                  0.001     \n#>      0-3 daily serving  111803.5 (23.7)   3338836.3 (26.1)             \n#>      4-6 daily serving  233403.5 (49.6)   6356044.7 (49.6)             \n#>      6+ daily serving   125730.4 (26.7)   3112535.1 (24.3)             \n#>   bp = Yes (%)          209257.0 (44.4)   1350561.0 (10.5)  <0.001     \n#>   diab = Yes (%)         78762.9 (16.7)    403654.4 ( 3.2)  <0.001     \n#>   province = North (%)     702.8 ( 0.1)     28246.6 ( 0.2)   0.005     \n#>   immigrate (%)                                             <0.001     \n#>      not immigrant      389553.0 (82.7)  10503766.2 (82.0)             \n#>      > 10 years          69008.0 (14.7)   1462544.4 (11.4)             \n#>      recent              12376.5 ( 2.6)    841105.5 ( 6.6)\n\nHow did they calculate the p-values? Hint: svychisq (see below).\nProportions and Design Effect\nThis part computes proportions and design effects, which help understand the influence of the sampling design on the estimated statistics.\n\nrequire(survey)\n# Computing survey statistics on subsets of a survey defined by factor(s).\nfit0a <- svyby(~CVD,~OA,design=w.design, svymean,deff=TRUE)\nfit0a\n\n\n\n  \n\n\nconfint(fit0a)\n#>                          2.5 %    97.5 %\n#> Control:CVDevent    0.02681661 0.0290204\n#> OA:CVDevent         0.10847000 0.1211599\n#> Control:CVDno event 0.97097960 0.9731834\n#> OA:CVDno event      0.87884010 0.8915300\n# 7.45% OA patients estimated to have CVD event.\n# 95% CI:  (0.067, 0.0816)\n\nLet\n\n\n\\(\\theta\\) = parameter (population slope) and\n\n\n\\(\\hat(\\theta)\\) = statistic (estimated slope).\n\n\\(b = \\frac{\\sum[w (y_i-\\bar{y}) (x_i-\\bar{x})]}{\\sum[w (x_i-\\bar{x})^2]}\\)\nDE = Effect of complex survey on the SEs, relative to a SRS of equal size.\n\n\\(D^2(\\hat{\\theta}) = \\frac{Var(\\hat{\\theta})_{Complex Survey}}{Var(\\hat{\\theta})_{SRS}}\\)\n\\(D^2(\\hat{\\theta}) = \\frac{SE(\\hat{\\theta})^2_{Complex Survey}}{SE(\\hat{\\theta})^2_{SRS}}\\)\n\nNote:\n\nSE increases as value of weight increases (CCHS).\nNHANES has more things to worry about (strata, PSU)\n\nDEFF = 2 means that the variance of the sample proportion, when choosing the sample by complex survey sampling, is nearly 2 times as large as the variance of the same estimator under simple random sampling/SRS.\n\nfit0b <- svyby(~CVD,~diab,design=w.design, svymean,deff=TRUE)\nfit0b\n\n\n\n  \n\n\nconfint(fit0b)\n#>                     2.5 %     97.5 %\n#> No:CVDevent     0.0295633 0.03173344\n#> Yes:CVDevent    0.1505917 0.17594247\n#> No:CVDno event  0.9682666 0.97043670\n#> Yes:CVDno event 0.8240575 0.84940825\n\nTesting association\nHere, Chi-square tests are conducted to test the association between different variables. Two variants of the test are used: Rao-Scott and Thomas-Rao modifications. These adaptations are used when the data come from a complex survey design.\n\nTests for hypothesis\n\nRao-Scott modifications (chi-sq)\nThomas-Rao modifications (F)\n\n\n\n\n# Rao-Scott modifications (chi-sq)\nsvychisq(~CVD+OA,design=w.design, statistic=\"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"Chisq\")\n#> X-squared = 3249.7, df = 1, p-value < 2.2e-16\n\n# Thomas-Rao modifications (F)\nsvychisq(~CVD+OA,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"F\")\n#> F = 1863, ndf = 1, ddf = 185612, p-value < 2.2e-16\n\n# Both provide strong evidence to reject the null hypothesis.\n# Conclusion: there is a significant (at 5%) association \n# between CVD prevalence and OA.\nsvychisq(~CVD+fruit,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + fruit, design = w.design, statistic = \"F\")\n#> F = 7.1241, ndf = 1.9758e+00, ddf = 3.6673e+05, p-value = 0.0008503\nsvychisq(~CVD+province,design=w.design, statistic=\"Chisq\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + province, design = w.design, statistic = \"Chisq\")\n#> X-squared = 1.4848, df = 1, p-value = 0.00492\n\nSaving data\nFinally, the dataset, along with any new variables or subsets created during the analysis, is saved for future use.\n\nsave(w.design, analytic.miss, analytic2, file = \"Data/surveydata/cchs123w.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata4.html",
    "href": "surveydata4.html",
    "title": "CCHS: Regression",
    "section": "",
    "text": "This tutorial is for a complex data analysis, specifically using regression techniques to analyze survey data.\nLet us load necessary R packages for the analysis:\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(MASS)\n\nLoad data\nLoads a dataset and provides some quick data checks, like the dimensions and summary of weights.\n\nload(\"Data/surveydata/cchs123w.RData\")\nls()\n#> [1] \"analytic.miss\"   \"analytic2\"       \"has_annotations\" \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\n\nLogistic for complex survey\nPerforms a simple logistic regression using the complex survey data, focusing on the relationship between cardiovascular disease and osteoarthritis.\n\nformula0 <- as.formula(I(CVD==\"event\") ~ OA)\n\n## Crude regression\nfit2 <- svyglm(formula0, \n              design = w.design, \n              family = binomial(logit))\nrequire(Publish)\npublish(fit2)\n#>  Variable   Units OddsRatio       CI.95 p-value \n#>        OA Control       Ref                     \n#>                OA      4.52 [4.19;4.87]  <1e-04\n\nMultivariable analysis\nRuns a more complex logistic regression model, adding multiple covariates to better understand the relationship.\n\nformula1 <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race + \n              edu + income + bmi + phyact + doctor + stress + \n              smoke + drink + fruit + bp + diab + province + immigrate)\n\nfit3 <- svyglm(formula1, \n              design = w.design, \n              family = binomial(logit))\npublish(fit3)\n#>   Variable             Units OddsRatio         CI.95     p-value \n#>         OA           Control       Ref                           \n#>                           OA      1.52   [1.40;1.66]     < 1e-04 \n#>        age       20-29 years       Ref                           \n#>                  30-39 years      1.29   [0.98;1.69]   0.0707636 \n#>                  40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                  50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                  60-64 years      9.71  [7.68;12.29]     < 1e-04 \n#>            65 years and over     15.85 [12.57;20.00]     < 1e-04 \n#>                         teen      0.46   [0.20;1.07]   0.0707295 \n#>        sex            Female       Ref                           \n#>                         Male      1.73   [1.60;1.88]     < 1e-04 \n#>    married        not single       Ref                           \n#>                       single      0.97   [0.90;1.05]   0.5209444 \n#>       race         Non-white       Ref                           \n#>                        White      1.44   [1.19;1.75]   0.0002219 \n#>        edu          < 2ndary       Ref                           \n#>                    2nd grad.      0.90   [0.80;1.00]   0.0512330 \n#>              Other 2nd grad.      0.97   [0.83;1.13]   0.6737665 \n#>               Post-2nd grad.      0.93   [0.85;1.02]   0.1016562 \n#>     income   $29,999 or less       Ref                           \n#>              $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>              $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>              $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>        bmi       Underweight       Ref                           \n#>               healthy weight      0.86   [0.67;1.10]   0.2350526 \n#>                   Overweight      0.88   [0.69;1.12]   0.3033213 \n#>     phyact            Active       Ref                           \n#>                     Inactive      1.21   [1.10;1.34]   0.0001345 \n#>                     Moderate      1.08   [0.97;1.21]   0.1771985 \n#>     doctor                No       Ref                           \n#>                          Yes      1.75   [1.49;2.06]     < 1e-04 \n#>     stress  Not too stressed       Ref                           \n#>                     stressed      1.30   [1.18;1.42]     < 1e-04 \n#>      smoke      Never smoker       Ref                           \n#>               Current smoker      1.18   [1.05;1.32]   0.0050518 \n#>                Former smoker      1.21   [1.11;1.33]     < 1e-04 \n#>      drink       Never drank       Ref                           \n#>              Current drinker      0.82   [0.68;0.98]   0.0290605 \n#>                Former driker      1.13   [0.93;1.36]   0.2133779 \n#>      fruit 0-3 daily serving       Ref                           \n#>            4-6 daily serving      0.94   [0.86;1.03]   0.1758214 \n#>             6+ daily serving      1.09   [0.97;1.23]   0.1311029 \n#>         bp                No       Ref                           \n#>                          Yes      2.35   [2.16;2.55]     < 1e-04 \n#>       diab                No       Ref                           \n#>                          Yes      1.86   [1.66;2.07]     < 1e-04 \n#>   province             South       Ref                           \n#>                        North      1.21   [0.90;1.62]   0.2103030 \n#>  immigrate     not immigrant       Ref                           \n#>                   > 10 years      1.02   [0.89;1.16]   0.8057243 \n#>                       recent      1.06   [0.73;1.53]   0.7651069\n\nModel fit assessment\nVariability explained\nPseudo-R-square values indicate how much of the total variability in the outcomes is explainable by the fitted model (analogous to R-square). For a continuous outcome, we can compute R-square, while R-square cannot be calculated when the outcome variable is categorical, nominal or ordinal. We use pseudo-R-squared measures when the dependent variable is not continuous but a likelihood function is used to fit a model. Popular Pseudo-R-square measures are:\n\n\nCox/Snell (never reaches max 1)\n\nNagelkerke R-square (scaled to max 1)\n\n\nThe larger Cox & Snell estimate is the better the model.\nThese Pseudo-R-square values should be interpreted with caution (if not ignored).\nThey offer little confidence in interpreting the model fit.\nSurvey weighted version of them are available.\nNot trivial to decide which statistic to use under complex surveys.\n\nEvaluates the model fit using Akaike Information Criterion (AIC) and pseudo R-squared metrics.\n\nfit3 <- svyglm(formula1, \n              design = w.design, \n              family = quasibinomial(logit)) # publish does not work\nAIC(fit3) \n#>        eff.p          AIC     deltabar \n#>    67.752064 45362.706032     2.053093\n\n# AIC for survey weighted regressions\npsrsq(fit3, method = \"Cox-Snell\")\n#> [1] 0.06091896\npsrsq(fit3, method = \"Nagelkerke\")\n#> [1] 0.2307586\n# Nagelkerke and Cox-Snell pseudo-rsquared statistics\n\nBackward Elimination\n\nModel comparisons\n\nLRT-aprroximation\nWald-based\n\n\n\nChecking one by one\nChecks the significance of each variable one by one and removes those that are not statistically significant.\n\nround(sort(summary(fit3)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.                ageteen \n#>                   0.03                   0.05                   0.07 \n#>         age30-39 years      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.18                   0.18                   0.21 \n#>     drinkFormer driker      bmihealthy weight          bmiOverweight \n#>                   0.21                   0.24                   0.30 \n#>          marriedsingle     eduOther 2nd grad.        immigraterecent \n#>                   0.52                   0.67                   0.77 \n#>    immigrate> 10 years \n#>                   0.81\n# bmiOverweight is associated with largest p-value\n# but what about other categories?\n\nregTermTest(fit3,~bmi) # coef of all bmi cat = 0\n#> Wald test for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> F =  0.7591291  on  2  and  185579  df: p= 0.46808\nfit4 <- update(fit3, .~. -bmi) \n\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> Working 2logLR =  1.424634 p= 0.49071 \n#> (scale factors:  1.1 0.93 );  denominator df= 185579\n# high p-value (in both wald and Anova) makes it more likely that you should exclude bmi\nAIC(fit3,fit4) \n#>         eff.p      AIC deltabar\n#> [1,] 67.75206 45362.71 2.053093\n#> [2,] 64.30460 45358.26 2.074342\nround(sort(summary(fit4)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.00 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.17                   0.17                   0.21 \n#>     drinkFormer driker          marriedsingle     eduOther 2nd grad. \n#>                   0.21                   0.53                   0.67 \n#>        immigraterecent    immigrate> 10 years \n#>                   0.76                   0.81\n\nUsing AIC to automate\nUses stepwise regression guided by AIC to automatically select the most important variables.\n\nrequire(MASS)\nformula1b <- as.formula(I(CVD==\"event\") ~ OA + age + sex)\nfit1b <- svyglm(formula1b, \n              design = w.design, \n              family = binomial(logit))\nfit5 <- stepAIC(fit1b, direction = \"backward\")\n#> Start:  AIC=47384.51\n#> I(CVD == \"event\") ~ OA + age + sex\n#> \n#>        Df Deviance   AIC\n#> <none>       47353 47385\n#> - sex   1    47634 47658\n#> - OA    1    47679 47702\n#> - age   6    54414 54291\n\n\npublish(fit5)\n#>  Variable             Units OddsRatio         CI.95   p-value \n#>        OA           Control       Ref                         \n#>                          OA      1.81   [1.66;1.97]   < 1e-04 \n#>       age       20-29 years       Ref                         \n#>                 30-39 years      1.40   [1.07;1.84]   0.01374 \n#>                 40-49 years      3.39   [2.69;4.27]   < 1e-04 \n#>                 50-59 years      9.42  [7.55;11.76]   < 1e-04 \n#>                 60-64 years     17.78 [14.22;22.23]   < 1e-04 \n#>           65 years and over     33.82 [27.26;41.97]   < 1e-04 \n#>                        teen      0.45   [0.20;1.02]   0.05462 \n#>       sex            Female       Ref                         \n#>                        Male      1.57   [1.46;1.69]   < 1e-04\nround(sort(summary(fit5)$coef[,\"Pr(>|t|)\"]),2)\n#>          (Intercept) age65 years and over       age60-64 years \n#>                 0.00                 0.00                 0.00 \n#>       age50-59 years                 OAOA              sexMale \n#>                 0.00                 0.00                 0.00 \n#>       age40-49 years       age30-39 years              ageteen \n#>                 0.00                 0.01                 0.05\n\nUsing AIC, but keeping importants\nSimilar to the previous step but ensures certain important variables remain in the model.\n\nformula1c <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate)\nscope <- list(upper = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate,\n              lower = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab)\n\nfit1c <- svyglm(formula1c, design = w.design, family = binomial(logit))\n\nfitstep <- step(fit1c, scope = scope, trace = FALSE, k = 2, direction = \"backward\")\n# k = 2 gives the genuine AIC\n\n\npublish(fitstep)\n#>  Variable             Units OddsRatio         CI.95     p-value \n#>        OA           Control       Ref                           \n#>                          OA      1.52   [1.40;1.66]     < 1e-04 \n#>       age       20-29 years       Ref                           \n#>                 30-39 years      1.29   [0.98;1.70]   0.0697699 \n#>                 40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                 50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                 60-64 years      9.71  [7.68;12.28]     < 1e-04 \n#>           65 years and over     15.85 [12.57;19.98]     < 1e-04 \n#>                        teen      0.46   [0.20;1.07]   0.0705660 \n#>       sex            Female       Ref                           \n#>                        Male      1.73   [1.60;1.88]     < 1e-04 \n#>   married        not single       Ref                           \n#>                      single      0.97   [0.90;1.05]   0.5042496 \n#>      race         Non-white       Ref                           \n#>                       White      1.41   [1.18;1.70]   0.0001986 \n#>       edu          < 2ndary       Ref                           \n#>                   2nd grad.      0.90   [0.80;1.00]   0.0524940 \n#>             Other 2nd grad.      0.97   [0.83;1.13]   0.6732446 \n#>              Post-2nd grad.      0.93   [0.85;1.02]   0.1045975 \n#>    income   $29,999 or less       Ref                           \n#>             $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>             $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>             $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>       bmi       Underweight       Ref                           \n#>              healthy weight      0.86   [0.67;1.10]   0.2316915 \n#>                  Overweight      0.88   [0.69;1.12]   0.2982852 \n#>    phyact            Active       Ref                           \n#>                    Inactive      1.22   [1.10;1.34]   0.0001227 \n#>                    Moderate      1.08   [0.97;1.21]   0.1754422 \n#>     fruit 0-3 daily serving       Ref                           \n#>           4-6 daily serving      0.94   [0.86;1.03]   0.1807666 \n#>            6+ daily serving      1.09   [0.97;1.23]   0.1295281 \n#>        bp                No       Ref                           \n#>                         Yes      2.35   [2.16;2.55]     < 1e-04 \n#>      diab                No       Ref                           \n#>                         Yes      1.85   [1.66;2.07]     < 1e-04 \n#>    doctor                No       Ref                           \n#>                         Yes      1.75   [1.49;2.05]     < 1e-04 \n#>    stress  Not too stressed       Ref                           \n#>                    stressed      1.30   [1.18;1.42]     < 1e-04 \n#>     smoke      Never smoker       Ref                           \n#>              Current smoker      1.17   [1.05;1.31]   0.0053412 \n#>               Former smoker      1.21   [1.10;1.33]     < 1e-04 \n#>     drink       Never drank       Ref                           \n#>             Current drinker      0.82   [0.68;0.98]   0.0254942 \n#>               Former driker      1.12   [0.93;1.36]   0.2205315\nround(sort(summary(fitstep)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#>         phyactModerate fruit4-6 daily serving     drinkFormer driker \n#>                   0.18                   0.18                   0.22 \n#>      bmihealthy weight          bmiOverweight          marriedsingle \n#>                   0.23                   0.30                   0.50 \n#>     eduOther 2nd grad. \n#>                   0.67\n\nAssess interactions\nCheck biologically interesting ones.\nCheck one by one\nChecks if there is a significant interaction effect between ‘age’ and ‘sex’.\n\nfit8a <- update(fitstep, .~. + interaction(age,sex))\nanova(fitstep, fit8a) # keep interaction\n#> Working (Rao-Scott+F) LRT for interaction(age, sex)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(age, sex), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  40.16528 p= 1.2167e-06 \n#> (scale factors:  1.3 1.2 1.2 0.93 0.78 0.71 );  denominator df= 185576\n\nChecks if there is a significant interaction effect between ‘sex’ and ‘diabetes’.\n\nfit8b <- update(fitstep, .~. + interaction(sex,diab))\nanova(fitstep, fit8b) # Do not keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(sex, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(sex, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  0.4591456 p= 0.49597 \n#> df=1;  denominator df= 185581\n\nChecks if there is a significant interaction effect between ‘BMI’ and ‘diabetes’.\n\nfit8c <- update(fitstep, .~. + interaction(bmi,diab))\nanova(fitstep, fit8c) # keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(bmi, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(bmi, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  7.92727 p= 0.02533 \n#> (scale factors:  1.4 0.6 );  denominator df= 185580\n\nAdd all significant interactions in 1 model\nUpdates the model to include significant interaction terms.\nNote that we have 0 effect modifier, 2 interactions\n\nfit9 <- update(fitstep, .~. + interaction(age,sex) + interaction(bmi,diab))\nrequire(jtools)\nsumm(fit9, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    185613 \n  \n\n Dependent variable \n    I(CVD == \"event\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.233 \n  \n\n Pseudo-R² (McFadden) \n    0.207 \n  \n\n AIC \n    41548.160 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    -5.590 \n    -6.046 \n    -5.135 \n    -24.069 \n    0.000 \n  \n\n OAOA \n    0.438 \n    0.352 \n    0.523 \n    10.043 \n    0.000 \n  \n\n age30-39 years \n    0.299 \n    -0.118 \n    0.717 \n    1.405 \n    0.160 \n  \n\n age40-49 years \n    1.158 \n    0.794 \n    1.522 \n    6.236 \n    0.000 \n  \n\n age50-59 years \n    2.181 \n    1.831 \n    2.531 \n    12.212 \n    0.000 \n  \n\n age60-64 years \n    2.619 \n    2.263 \n    2.976 \n    14.395 \n    0.000 \n  \n\n age65 years and over \n    3.033 \n    2.680 \n    3.387 \n    16.833 \n    0.000 \n  \n\n ageteen \n    -0.695 \n    -1.704 \n    0.314 \n    -1.350 \n    0.177 \n  \n\n sexMale \n    -0.028 \n    -0.447 \n    0.390 \n    -0.133 \n    0.895 \n  \n\n marriedsingle \n    -0.016 \n    -0.096 \n    0.064 \n    -0.382 \n    0.703 \n  \n\n raceWhite \n    0.348 \n    0.165 \n    0.531 \n    3.724 \n    0.000 \n  \n\n edu2nd grad. \n    -0.107 \n    -0.217 \n    0.003 \n    -1.906 \n    0.057 \n  \n\n eduOther 2nd grad. \n    -0.039 \n    -0.192 \n    0.114 \n    -0.498 \n    0.618 \n  \n\n eduPost-2nd grad. \n    -0.081 \n    -0.173 \n    0.010 \n    -1.746 \n    0.081 \n  \n\n income$30,000-$49,999 \n    -0.304 \n    -0.400 \n    -0.208 \n    -6.215 \n    0.000 \n  \n\n income$50,000-$79,999 \n    -0.444 \n    -0.552 \n    -0.336 \n    -8.034 \n    0.000 \n  \n\n income$80,000 or more \n    -0.555 \n    -0.684 \n    -0.426 \n    -8.424 \n    0.000 \n  \n\n bmihealthy weight \n    -1.406 \n    -2.677 \n    -0.135 \n    -2.167 \n    0.030 \n  \n\n bmiOverweight \n    -1.110 \n    -2.375 \n    0.154 \n    -1.721 \n    0.085 \n  \n\n phyactInactive \n    0.194 \n    0.094 \n    0.293 \n    3.817 \n    0.000 \n  \n\n phyactModerate \n    0.077 \n    -0.034 \n    0.187 \n    1.353 \n    0.176 \n  \n\n fruit4-6 daily serving \n    -0.062 \n    -0.155 \n    0.031 \n    -1.313 \n    0.189 \n  \n\n fruit6+ daily serving \n    0.094 \n    -0.023 \n    0.211 \n    1.579 \n    0.114 \n  \n\n bpYes \n    0.861 \n    0.778 \n    0.944 \n    20.354 \n    0.000 \n  \n\n diabYes \n    1.745 \n    0.462 \n    3.027 \n    2.667 \n    0.008 \n  \n\n doctorYes \n    0.535 \n    0.372 \n    0.697 \n    6.447 \n    0.000 \n  \n\n stressstressed \n    0.256 \n    0.164 \n    0.347 \n    5.483 \n    0.000 \n  \n\n smokeCurrent smoker \n    0.152 \n    0.039 \n    0.266 \n    2.628 \n    0.009 \n  \n\n smokeFormer smoker \n    0.177 \n    0.082 \n    0.272 \n    3.654 \n    0.000 \n  \n\n drinkCurrent drinker \n    -0.205 \n    -0.381 \n    -0.029 \n    -2.277 \n    0.023 \n  \n\n drinkFormer driker \n    0.116 \n    -0.072 \n    0.303 \n    1.210 \n    0.226 \n  \n\n interaction(age, sex)30-39 years.Female \n    -0.083 \n    -0.622 \n    0.457 \n    -0.300 \n    0.764 \n  \n\n interaction(age, sex)40-49 years.Female \n    -0.302 \n    -0.766 \n    0.161 \n    -1.278 \n    0.201 \n  \n\n interaction(age, sex)50-59 years.Female \n    -0.810 \n    -1.258 \n    -0.362 \n    -3.543 \n    0.000 \n  \n\n interaction(age, sex)60-64 years.Female \n    -0.787 \n    -1.237 \n    -0.337 \n    -3.428 \n    0.001 \n  \n\n interaction(age, sex)65 years and over.Female \n    -0.603 \n    -1.035 \n    -0.170 \n    -2.733 \n    0.006 \n  \n\n interaction(age, sex)teen.Female \n    -0.129 \n    -1.806 \n    1.548 \n    -0.151 \n    0.880 \n  \n\n interaction(bmi, diab)healthy weight.No \n    1.380 \n    0.085 \n    2.675 \n    2.089 \n    0.037 \n  \n\n interaction(bmi, diab)Overweight.No \n    1.085 \n    -0.204 \n    2.373 \n    1.650 \n    0.099 \n  \n\n\n Standard errors: Robust\n\n\n\n\nfit9 <- update(fitstep, .~. + age:sex + bmi:diab)\npublish(fit9)\n#>                                            Variable             Units OddsRatio         CI.95     p-value \n#>                                                  OA           Control       Ref                           \n#>                                                                    OA      1.55   [1.42;1.69]     < 1e-04 \n#>                                             married        not single       Ref                           \n#>                                                                single      0.98   [0.91;1.07]   0.7026897 \n#>                                                race         Non-white       Ref                           \n#>                                                                 White      1.42   [1.18;1.70]   0.0001960 \n#>                                                 edu          < 2ndary       Ref                           \n#>                                                             2nd grad.      0.90   [0.81;1.00]   0.0566536 \n#>                                                       Other 2nd grad.      0.96   [0.83;1.12]   0.6183383 \n#>                                                        Post-2nd grad.      0.92   [0.84;1.01]   0.0807393 \n#>                                              income   $29,999 or less       Ref                           \n#>                                                       $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>                                                       $50,000-$79,999      0.64   [0.58;0.71]     < 1e-04 \n#>                                                       $80,000 or more      0.57   [0.50;0.65]     < 1e-04 \n#>                                              phyact            Active       Ref                           \n#>                                                              Inactive      1.21   [1.10;1.34]   0.0001349 \n#>                                                              Moderate      1.08   [0.97;1.21]   0.1760803 \n#>                                               fruit 0-3 daily serving       Ref                           \n#>                                                     4-6 daily serving      0.94   [0.86;1.03]   0.1892549 \n#>                                                      6+ daily serving      1.10   [0.98;1.23]   0.1142759 \n#>                                                  bp                No       Ref                           \n#>                                                                   Yes      2.37   [2.18;2.57]     < 1e-04 \n#>                                              doctor                No       Ref                           \n#>                                                                   Yes      1.71   [1.45;2.01]     < 1e-04 \n#>                                              stress  Not too stressed       Ref                           \n#>                                                              stressed      1.29   [1.18;1.42]     < 1e-04 \n#>                                               smoke      Never smoker       Ref                           \n#>                                                        Current smoker      1.16   [1.04;1.30]   0.0085960 \n#>                                                         Former smoker      1.19   [1.09;1.31]   0.0002579 \n#>                                               drink       Never drank       Ref                           \n#>                                                       Current drinker      0.81   [0.68;0.97]   0.0227934 \n#>                                                         Former driker      1.12   [0.93;1.35]   0.2264702 \n#>               age(20-29 years): sex(Male vs Female)                        0.97   [0.64;1.48]   0.8945322 \n#>               age(30-39 years): sex(Male vs Female)                        1.06   [0.75;1.49]   0.7583565 \n#>               age(40-49 years): sex(Male vs Female)                        1.32   [1.07;1.62]   0.0091527 \n#>               age(50-59 years): sex(Male vs Female)                        2.18   [1.85;2.58]     < 1e-04 \n#>               age(60-64 years): sex(Male vs Female)                        2.14   [1.80;2.53]     < 1e-04 \n#>         age(65 years and over): sex(Male vs Female)                        1.78   [1.58;1.99]     < 1e-04 \n#>                      age(teen): sex(Male vs Female)                        1.11   [0.22;5.62]   0.9033924 \n#>        sex(Female): age(30-39 years vs 20-29 years)                        1.24   [0.87;1.77]   0.2276609 \n#>        sex(Female): age(40-49 years vs 20-29 years)                        2.35   [1.75;3.16]     < 1e-04 \n#>        sex(Female): age(50-59 years vs 20-29 years)                        3.94   [2.95;5.27]     < 1e-04 \n#>        sex(Female): age(60-64 years vs 20-29 years)                        6.25   [4.66;8.39]     < 1e-04 \n#>  sex(Female): age(65 years and over vs 20-29 years)                       11.37  [8.60;15.02]     < 1e-04 \n#>               sex(Female): age(teen vs 20-29 years)                        0.44   [0.11;1.69]   0.2320840 \n#>          sex(Male): age(30-39 years vs 20-29 years)                        1.35   [0.89;2.05]   0.1599938 \n#>          sex(Male): age(40-49 years vs 20-29 years)                        3.18   [2.21;4.58]     < 1e-04 \n#>          sex(Male): age(50-59 years vs 20-29 years)                        8.85  [6.24;12.56]     < 1e-04 \n#>          sex(Male): age(60-64 years vs 20-29 years)                       13.73  [9.61;19.61]     < 1e-04 \n#>    sex(Male): age(65 years and over vs 20-29 years)                       20.77 [14.59;29.57]     < 1e-04 \n#>                 sex(Male): age(teen vs 20-29 years)                        0.50   [0.18;1.37]   0.1769155 \n#>                   bmi(Underweight): diab(Yes vs No)                        5.72  [1.59;20.63]   0.0076561 \n#>                bmi(healthy weight): diab(Yes vs No)                        1.44   [1.19;1.75]   0.0002221 \n#>                    bmi(Overweight): diab(Yes vs No)                        1.93   [1.70;2.20]     < 1e-04 \n#>        diab(No): bmi(healthy weight vs Underweight)                        0.97   [0.76;1.25]   0.8394972 \n#>            diab(No): bmi(Overweight vs Underweight)                        0.97   [0.76;1.25]   0.8400722 \n#>       diab(Yes): bmi(healthy weight vs Underweight)                        0.25   [0.07;0.87]   0.0302066 \n#>           diab(Yes): bmi(Overweight vs Underweight)                        0.33   [0.09;1.17]   0.0852377\n\n\nbasic.model <- eval(fit5$call[[2]])\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\n\naic.int.model <- eval(fit9$call[[2]])\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\n\nSaving data\nSaves the final regression models for future use.\n\nsave(basic.model, aic.int.model, file = \"Data/surveydata/cchs123w2.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata5.html",
    "href": "surveydata5.html",
    "title": "CCHS: Performance",
    "section": "",
    "text": "The tutorial outlines the process for evaluating the performance of logistic regression models fitted to complex survey data using R. It focuses on two major aspects: creating Receiver Operating Characteristic (ROC) curves and conducting Archer and Lemeshow Goodness of Fit tests. Here AUC is a measure to evaluate the predictive accuracy of the model, and Archer and Lemeshow test is a statistical test to evaluate how well your model fits the observed data.\nWe start by importing the required R packages.\n\n# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\nLoad data\nIt loads two datasets from the specified paths.\n\nload(\"Data/surveydata/cchs123w.RData\")\nload(\"Data/surveydata/cchs123w2.RData\")\nls()\n#> [1] \"aic.int.model\"   \"analytic.miss\"   \"analytic2\"       \"basic.model\"    \n#> [5] \"has_annotations\" \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\n\nThree different logistic regression models are fitted to the data:\n\nSimple model: Model with only OA as a predictor\nBasic model: Model with OA, age and sex\nComplex model: Model with many predictors and some interaction terms\n\n\n# Formula for Simple model\nsimple.model <- as.formula(I(CVD==\"event\") ~ OA)\nsimple.model\n#> I(CVD == \"event\") ~ OA\n\n# Formula for Basic model\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\n\n# Formula for the Complex model with interactions\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\n\n\nlibrary(survey)\n\n# Simple model\nfit0 <- svyglm(simple.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# Basic model\nfit5 <- svyglm(basic.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# Complex model with interactions\nfit9 <- svyglm(aic.int.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nModel performance\nROC curve\nThis section defines a function, svyROCw, to plot the ROC curves and calculate the area under the curve (AUC). The function can handle both weighted and unweighted survey data.\n\nThe appropriateness of the fitted logistic regression model needs to be examined before it is accepted for use.\nPlotting the pairs of - sensitivities vs - 1-specificities on a scatter plot provides a Receiver Operating Characteristic (ROC) curve.\nThe area under the ROC curve = AUC / C-statistic.\nROC/AUC should consider weights for complex surveys.\n\nGrading Guidelines for AUC values:\n\n0.90-1.0 excellent discrimination (unusual)\n0.80-0.90 good discrimination\n0.70-0.80 fair discrimination\n0.60-0.70 poor discrimination\n0.50-0.60 failed discrimination\n\n\nrequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit=fit,outcome=analytic2$CVD==\"event\", weight = NULL){\n  # ROC curve for\n  # Survey Data with Logistic Regression\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { # library(WeightedROC)\n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\n\n\nsummary(analytic2$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   71.56  137.95  214.61  261.91 7154.95\nanalytic2$corrected.weight <- weights(w.design)\nsummary(analytic2$corrected.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsvyROCw(fit=fit0,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nsvyROCw(fit=fit5,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nsvyROCw(fit=fit9,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\n# This function does not take in to account of strata/cluster\n\nArcher and Lemeshow test\nThis test helps to evaluate how well the model fits the data. A Goodness of Fit (GOF) function AL.gof is defined. If the p-value from this test is greater than a certain threshold (e.g., 0.05), the model fit is considered acceptable.\n\nHosmer Lemeshow-type tests are most useful as a very crude way to screen for fit problems, and should not be taken as a definitive diagnostic of a ‘good’ fit.\n\nproblem in small sample size\nDependent on G (group)\n\n\nArcher and Lemeshow (2006) extended the standard Hosmer and Lemeshow GOF test for complex surveys.\nAfter fitting the survey weighted logistic regression, the F-adjusted mean residual goodness-of-fit test could suggest\n\nno evidence of lack of fit (if P-value > a reasonable cut-point, e.g., 0.05)\nevidence of lack of fit (if P-value < a reasonable cut-point, e.g., 0.05)\n\n\n\n\nAL.gof <- function(fit=fit, data = analytic2, \n                   weight = \"corrected.weight\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=~1, \n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\nAL.gof(fit0, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.819403e-22  on  1  and  185611  df: p= 1\nAL.gof(fit5, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.795204  on  8  and  185604  df: p= 0.0042898\nAL.gof(fit9, analytic2, weight = \"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.650332  on  9  and  185603  df: p= 0.0045417\n\nAdditional function\nIf the survey data contains strata and cluster, then the following function will be useful:\n\nAL.gof2 <- function(fit=fit7, data = analytic, \n                   weight = \"corrected.weight\", psu = \"psu\", strata= \"strata\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata6.html#a-note-about-predictive-models",
    "href": "surveydata6.html#a-note-about-predictive-models",
    "title": "NHANES: Blood Pressure",
    "section": "A note about Predictive models",
    "text": "A note about Predictive models\nIn statistical analyses involving survey data, it’s crucial to account for the survey’s design features. These features can include sampling weights, stratification, and clustering, among others. Ignoring these could lead to biased estimates and incorrect conclusions. Considering such survey design features make the analysis more robust and reliable in terms of inference.\nHowever, when the goal shifts from inference to prediction, additional challenges come into play. Specifically, the model may perform well on the data used to fit it (the “training” data) but not generalize well to new, unseen data. This discrepancy between training performance and generalization to new data is often referred to as “overfitting,” and the optimism of the model refers to the extent to which it overestimates its predictive performance on new data based on its performance on the training data.\nOptimism-correction techniques are methodologies designed to address this issue. They allow you to evaluate how well your model is likely to perform on new data, not just the data you used to build it. Methods for optimism correction often involve techniques like cross-validation, bootstrapping, or specialized types of model validation that help in estimating the ‘true’ predictive performance of the model. Some of these techniques were discussed in the predictive modelling chapter.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata7.html",
    "href": "surveydata7.html",
    "title": "NHANES: Cholesterol",
    "section": "",
    "text": "# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(tableone)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(dplyr)\n\nPreprocessing\nAnalytic data set\nWe will use cholesterolNHANES15part1.RData in this prediction goal question (predicting cholesterol in adults).\nFor this exercise, we are assuming that:\n\noutcome: cholesterol\n\npredictors:\n\ngender\nwhether born in US\nrace\neducation\nwhether married\nincome level\nBMI\nwhether has diabetes\n\n\n\nsurvey features:\n\nsurvey weights\nstrata\ncluster/PSU; where strata is nested within clusters\n\n– restrict to those participants who are of 18 years of age or older\n\n\n\nload(\"Data/surveydata/cholesterolNHANES15part1.rdata\") #Loading the dataset\nls()\n#>  [1] \"analytic\"           \"analytic.with.miss\" \"analytic1\"         \n#>  [4] \"analytic2\"          \"analytic2b\"         \"analytic3\"         \n#>  [7] \"collinearity\"       \"correlationMatrix\"  \"diff.boot\"         \n#> [10] \"extract.boot.fun\"   \"extract.fit\"        \"extract.lm.fun\"    \n#> [13] \"fictitious.data\"    \"fit0\"               \"fit1\"              \n#> [16] \"fit2\"               \"fit3\"               \"fit4\"              \n#> [19] \"fit5\"               \"formula0\"           \"formula1\"          \n#> [22] \"formula2\"           \"formula3\"           \"formula4\"          \n#> [25] \"formula5\"           \"k.folds\"            \"numeric.names\"     \n#> [28] \"perform\"            \"pred.y\"             \"rocobj\"            \n#> [31] \"sel.names\"          \"var.cluster\"        \"var.summ\"          \n#> [34] \"var.summ2\"\n\nRetaining only useful variables\n\n# Data dimensions\ndim(analytic)\n#> [1] 1267   33\n\n# Variable names\nnames(analytic)\n#>  [1] \"ID\"                    \"gender\"                \"age\"                  \n#>  [4] \"born\"                  \"race\"                  \"education\"            \n#>  [7] \"married\"               \"income\"                \"weight\"               \n#> [10] \"psu\"                   \"strata\"                \"diastolicBP\"          \n#> [13] \"systolicBP\"            \"bodyweight\"            \"bodyheight\"           \n#> [16] \"bmi\"                   \"waist\"                 \"smoke\"                \n#> [19] \"alcohol\"               \"cholesterol\"           \"cholesterolM2\"        \n#> [22] \"triglycerides\"         \"uric.acid\"             \"protein\"              \n#> [25] \"bilirubin\"             \"phosphorus\"            \"sodium\"               \n#> [28] \"potassium\"             \"globulin\"              \"calcium\"              \n#> [31] \"physical.work\"         \"physical.recreational\" \"diabetes\"\n\n#Subsetting dataset with variables needed:\nrequire(dplyr)\nanadata <- select(analytic, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\n# new data sizes\ndim(anadata)\n#> [1] 1267   13\n\n# retained variable names\nnames(anadata)\n#>  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#>  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#> [11] \"weight\"      \"psu\"         \"strata\"\n\n#Restricting to participants who are 18 or older\nsummary(anadata$age) #The age range is already 20-80\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   20.00   36.00   51.00   49.91   63.00   80.00\n\n#Recoding the born variable\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born in 50 US states or Washingt                           Others \n#>                              991                              276 \n#>                             <NA> \n#>                                0\nlevels(anadata$born)\n#> NULL\nanadata$born <- car::recode(anadata$born,\n                            \"'Born in 50 US states or Washingt' = 'Born.in.US';\n                            'Others' = 'Others';\n                            else=NA\")\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born.in.US     Others       <NA> \n#>        991        276          0\n\nChecking the data for missing\n\nrequire(DataExplorer)\nplot_missing(anadata) #no missing data\n\n\n\n\nPreparing factor and continuous variables appropriately\n\nvars = c(\"cholesterol\", \"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\nnumeric.names <- c(\"cholesterol\", \"bmi\")\nfactor.names <- vars[!vars %in% numeric.names] \n\nanadata[factor.names] <- apply(X = anadata[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanadata[numeric.names] <- apply(X = anadata[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\n\nTable 1\n\nlibrary(tableone)\ntab1 <- CreateTableOne(data = anadata, includeNA = TRUE, vars = vars)\nprint(tab1, showAllLevels = TRUE,  varLabels = TRUE)\n#>                          \n#>                           level              Overall       \n#>   n                                            1267        \n#>   cholesterol (mean (SD))                    193.10 (43.22)\n#>   gender (%)              Female                496 (39.1) \n#>                           Male                  771 (60.9) \n#>   born (%)                Born.in.US            991 (78.2) \n#>                           Others                276 (21.8) \n#>   race (%)                Black                 246 (19.4) \n#>                           Hispanic              337 (26.6) \n#>                           Other                 132 (10.4) \n#>                           White                 552 (43.6) \n#>   education (%)           College               648 (51.1) \n#>                           High.School           523 (41.3) \n#>                           School                 96 ( 7.6) \n#>   married (%)             Married               751 (59.3) \n#>                           Never.married         226 (17.8) \n#>                           Previously.married    290 (22.9) \n#>   income (%)              <25k                  344 (27.2) \n#>                           Between.25kto54k      435 (34.3) \n#>                           Between.55kto99k      297 (23.4) \n#>                           Over100k              191 (15.1) \n#>   bmi (mean (SD))                             29.58 (6.84) \n#>   diabetes (%)            No                   1064 (84.0) \n#>                           Yes                   203 (16.0)\n\nLinear regression when cholesterol is continuous\nFit a linear regression, and report the VIFs.\n\n#Fitting initial regression\n\nfit0 <- lm(cholesterol ~ gender + born + race + education +\n              married + income + bmi + diabetes,\n            data = anadata)\n\nlibrary(Publish)\npublish(fit0)\n#>     Variable              Units Coefficient           CI.95    p-value \n#>  (Intercept)                         198.90 [184.82;212.97]    < 1e-04 \n#>       gender             Female         Ref                            \n#>                            Male       -6.82  [-11.76;-1.89]   0.006854 \n#>         born         Born.in.US         Ref                            \n#>                          Others       15.65    [8.54;22.75]    < 1e-04 \n#>         race              Black         Ref                            \n#>                        Hispanic       -2.75   [-10.61;5.10]   0.492333 \n#>                           Other       -3.95   [-13.61;5.72]   0.423740 \n#>                           White        5.36   [-1.20;11.92]   0.109403 \n#>    education            College         Ref                            \n#>                     High.School        3.51    [-1.61;8.63]   0.179871 \n#>                          School        0.31   [-9.63;10.24]   0.951841 \n#>      married            Married         Ref                            \n#>                   Never.married      -11.05  [-17.67;-4.44]   0.001082 \n#>              Previously.married        4.72   [-1.43;10.86]   0.132468 \n#>       income               <25k         Ref                            \n#>                Between.25kto54k       -0.48    [-6.72;5.75]   0.879480 \n#>                Between.55kto99k        3.41   [-3.60;10.43]   0.340491 \n#>                        Over100k        2.24   [-6.02;10.51]   0.595131 \n#>          bmi                          -0.21    [-0.56;0.15]   0.257105 \n#>     diabetes                 No         Ref                            \n#>                             Yes      -10.61  [-17.21;-4.02]   0.001652\n\n#Checking VIFs\ncar::vif(fit0) \n#>               GVIF Df GVIF^(1/(2*Df))\n#> gender    1.065810  1        1.032381\n#> born      1.578258  1        1.256288\n#> race      1.684064  3        1.090753\n#> education 1.280113  2        1.063683\n#> married   1.225520  2        1.052156\n#> income    1.277005  3        1.041595\n#> bmi       1.086953  1        1.042570\n#> diabetes  1.073619  1        1.036156\n\nAll VIFs are small.\nTest of association when cholesterol is binary\nDichotomize the outcome such that cholesterol<200 is labeled as ‘healthy’; otherwise label it as ‘unhealthy’, and name it ‘cholesterol.bin’. Test the association between this binary variable and gender.\n\n#Creating binary variable for cholesterol\nanadata$cholesterol.bin <- ifelse(anadata$cholesterol <200, \"healthy\", \"unhealthy\")\n#If cholesterol is <200, then \"healthy\", if not, \"unhealthy\"\n\ntable(anadata$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>       738       529\nanadata$cholesterol.bin <- as.factor(anadata$cholesterol.bin)\nanadata$cholesterol.bin <- relevel(anadata$cholesterol.bin, ref = \"unhealthy\")\n\nTest of association between cholesterol and gender (no survey features)\n\n# Simple Chi-square testing\nchisq.chol.gen <- chisq.test(anadata$cholesterol.bin, anadata$gender)\nchisq.chol.gen\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  anadata$cholesterol.bin and anadata$gender\n#> X-squared = 5.1321, df = 1, p-value = 0.02349\n\nSetting up survey design\n\nrequire(survey)\nsummary(anadata$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\nw.design <- svydesign(id = ~psu, weights = ~weight, strata = ~strata,\n                      nest = TRUE, data = anadata)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\n\nTest of association accounting for survey design\n\n#Rao-Scott modifications (chi-sq)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> X-squared = 11.092, df = 1, p-value = 0.02365\n\n#Thomas-Rao modifications (F)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\")\n#> F = 5.1205, ndf = 1, ddf = 15, p-value = 0.03891\n\nAll three tests indicate strong evidence to reject the H0. There seems to be an association between gender and cholesterol level (healthy/unhealthy)\nTable 1\nCreate a Table 1 (summarizing the covariates) stratified by the binary outcome: cholesterol.bin, utilizing the above survey features.\n\n# Creating Table 1 stratified by binary outcome (cholesterol)\n# Using the survey features\n\nvars2 = c(\"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\n\n\nkableone <- function(x, ...) {\n  capture.output(x <- print(x, showAllLevels= TRUE, padColnames = TRUE, insertLevel = TRUE))\n  knitr::kable(x, ...)\n}\nkableone(svyCreateTableOne(var = vars2, strata= \"cholesterol.bin\", data=w.design, test = TRUE)) \n\n\n\n\n\n\n\n\n\n\n\n\nlevel\nunhealthy\nhealthy\np\ntest\n\n\n\nn\n\n27369732.3\n34591444.0\n\n\n\n\ngender (%)\nFemale\n13573865.5 (49.6)\n13917447.5 (40.2)\n0.039\n\n\n\n\nMale\n13795866.8 (50.4)\n20673996.5 (59.8)\n\n\n\n\nborn (%)\nBorn.in.US\n23772751.7 (86.9)\n31532673.3 (91.2)\n0.028\n\n\n\n\nOthers\n3596980.6 (13.1)\n3058770.7 ( 8.8)\n\n\n\n\nrace (%)\nBlack\n1832118.3 ( 6.7)\n3696893.4 (10.7)\n0.015\n\n\n\n\nHispanic\n3263992.3 (11.9)\n3921344.6 (11.3)\n\n\n\n\n\nOther\n1887156.6 ( 6.9)\n2601870.3 ( 7.5)\n\n\n\n\n\nWhite\n20386465.2 (74.5)\n24371335.7 (70.5)\n\n\n\n\neducation (%)\nCollege\n15855712.5 (57.9)\n20945710.7 (60.6)\n0.522\n\n\n\n\nHigh.School\n10615218.7 (38.8)\n12434827.2 (35.9)\n\n\n\n\n\nSchool\n898801.1 ( 3.3)\n1210906.1 ( 3.5)\n\n\n\n\nmarried (%)\nMarried\n17489306.2 (63.9)\n21170020.0 (61.2)\n0.005\n\n\n\n\nNever.married\n3086474.4 (11.3)\n7175237.2 (20.7)\n\n\n\n\n\nPreviously.married\n6793951.8 (24.8)\n6246186.8 (18.1)\n\n\n\n\nincome (%)\n<25k\n4760281.8 (17.4)\n6364208.6 (18.4)\n0.915\n\n\n\n\nBetween.25kto54k\n8682481.6 (31.7)\n10786198.6 (31.2)\n\n\n\n\n\nBetween.55kto99k\n6939847.0 (25.4)\n9190388.2 (26.6)\n\n\n\n\n\nOver100k\n6987121.9 (25.5)\n8250648.6 (23.9)\n\n\n\n\nbmi (mean (SD))\n\n29.35 (6.13)\n29.64 (7.05)\n0.593\n\n\n\ndiabetes (%)\nNo\n25080412.0 (91.6)\n30006523.6 (86.7)\n0.012\n\n\n\n\nYes\n2289320.3 ( 8.4)\n4584920.4 (13.3)\n\n\n\n\n\n\n\nLogistic regression model\nRun a logistic regression model using the same variables, utilizing the survey features. Report the corresponding odds ratios and the 95% confidence intervals.\n\nformula1 <- as.formula(I(cholesterol.bin==\"unhealthy\") ~ gender + born +\n                         race + education + married + income + bmi +\n                         diabetes)\n\nfit1 <- svyglm(formula1,\n               design = w.design, \n               family = binomial(link = \"logit\"))\n\npublish(fit1)\n#>   Variable              Units OddsRatio       CI.95  p-value \n#>     gender             Female       Ref                      \n#>                          Male      0.70 [0.49;0.98]   0.2866 \n#>       born         Born.in.US       Ref                      \n#>                        Others      2.10 [1.41;3.13]   0.1707 \n#>       race              Black       Ref                      \n#>                      Hispanic      1.15 [0.80;1.67]   0.5871 \n#>                         Other      1.11 [0.69;1.80]   0.7406 \n#>                         White      1.46 [1.00;2.14]   0.3003 \n#>  education            College       Ref                      \n#>                   High.School      1.21 [0.96;1.52]   0.3563 \n#>                        School      0.86 [0.52;1.43]   0.6712 \n#>    married            Married       Ref                      \n#>                 Never.married      0.54 [0.32;0.90]   0.2526 \n#>            Previously.married      1.31 [0.92;1.87]   0.3704 \n#>     income               <25k       Ref                      \n#>              Between.25kto54k      1.03 [0.61;1.73]   0.9408 \n#>              Between.55kto99k      1.02 [0.66;1.56]   0.9525 \n#>                      Over100k      1.12 [0.73;1.72]   0.6920 \n#>        bmi                         1.00 [0.97;1.03]   0.9361 \n#>   diabetes                 No       Ref                      \n#>                           Yes      0.62 [0.41;0.95]   0.2720\n\nWald test (survey version)\nPerform a Wald test (survey version) to test the null hypothesis that all coefficients associated with the income variable are zero, and interpret.\n\n#Testing the H0 that all coefficients associated with the income variable are zero\nregTermTest(fit1, ~income)\n#> Wald test for income\n#>  in svyglm(formula = formula1, design = w.design, family = binomial(link = \"logit\"))\n#> F =  0.1050099  on  3  and  1  df: p= 0.94611\n\nThe Wald test here gives a large p-value; We do not have evidence to reject the H0 of coefficient being 0. If the coefficient for income variable is 0, this means that the outcome in the model (cholesterol) is not affected by income. This suggests that removing income from the model does not statistically improve the model fit. So we can remove income variable from the model.\nBackward elimination\nRun a backward elimination (using the AIC criteria) on the above logistic regression fit (keeping important variables gender, race, bmi, diabetes in the model), and report the odds ratios and the 95% confidence intervals from the resulting final logistic regression fit.\n\n#Running backward elimination based on AIC\nrequire(MASS)\nscope <- list(upper = ~ gender + born + race + education + \n                married + income + bmi + diabetes,\n              lower = ~ gender + race + bmi + diabetes)\n\nfit3 <- step(fit1, scope = scope, trace = FALSE,\n                k = 2, direction = \"backward\")\n\n#Odds Ratios\npublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\n\nBorn and married are also found to be useful on top of gender + race + bmi + diabetes.\nInteraction terms\nChecking interaction terms\n– gender and whether married\n– gender and whether born in the US\n– gender and diabetes\n– whether married and diabetes\n\n#gender and married\nfit4 <- update(fit3, .~. + interaction(gender, married))\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for interaction(gender, married)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     married), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.7461308 p= 0.70903 \n#> (scale factors:  1.1 0.93 );  denominator df= 4\n\nDo not include interaction term\n\n#gender and born in us\nfit5 <- update(fit3, .~. + interaction(gender, born))\nanova(fit3, fit5)\n#> Working (Rao-Scott+F) LRT for interaction(gender, born)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     born), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.4635299 p= 0.52441 \n#> df=1;  denominator df= 5\n\nDo not include interaction term\n\n#gender and diabetes\nfit6 <- update(fit3, .~. + interaction(gender, diabetes))\nanova(fit3, fit6)\n#> Working (Rao-Scott+F) LRT for interaction(gender, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  1.222596 p= 0.32211 \n#> df=1;  denominator df= 5\n\nDo not include interaction term\n\n#married and diabetes\nfit7 <- update(fit3, .~. + interaction(married, diabetes))\nanova(fit3, fit7)\n#> Working (Rao-Scott+F) LRT for interaction(married, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(married, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.3207507 p= 0.84547 \n#> (scale factors:  1.4 0.62 );  denominator df= 4\n\nDo not include interaction term\nNone of the interaction terms are improving the model fit.\nAUC\nReport AUC of the final model (only using weight argument) and interpret.\nAUC of the final model (only using weight argument) and interpret\n\nrequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight){\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { \n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\nsvyROCw(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight)\n\n\n\n\nThe area under the curve in the final model is 0.611, using the survey weighted ROC. The AUC of 0.611 indicates that this model has poor discrimination.\nArcher-Lemeshow Goodness of fit\nReport Archer-Lemeshow Goodness of fit test and interpret (utilizing all the survey features).\n\n#Archer-Lemeshow Goodness of fit test utilizing all survey features\nAL.gof2 <- function(fit = fit3, data = anadata, \n                   weight = \"weight\", psu = \"psu\", strata = \"strata\"){\n  r <- residuals(fit, type = \"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f, (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                         data=data2g, nest = TRUE)\n  decilemodel <- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nAL.gof2(fit3, anadata, weight = \"weight\", psu = \"psu\", strata = \"strata\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  0.7569326  on  9  and  6  df: p= 0.66075\n\nArcher and Lemeshow GoF test was used to test the fit of this model. The p-value of 0.3043, which is greater than 0.05. This means that there is no evidence of lack of fit to this model.\nAdd age as a predictor for linear regression\nFit another logistic regression (similar to Q1) with the above-mentioned predictors (as obtained in Q7) and age, utilizing the survey features. What difference do you see from the previous fit results?\n\naic.int.model <- eval(fit3$call[[2]])\naic.int.model\n#> I(cholesterol.bin == \"unhealthy\") ~ gender + born + race + married + \n#>     bmi + diabetes\n\nformula3 <- as.formula(cholesterol.bin ~ gender + born + race + married + bmi + diabetes + age)\nfit9 <- svyglm(formula3,\n               design = w.design,\n               family = binomial(link=\"logit\"))\nsummary(fit9)\n#> \n#> Call:\n#> svyglm(formula = formula3, design = w.design, family = binomial(link = \"logit\"))\n#> \n#> Survey design:\n#> svydesign(id = ~psu, weights = ~weight, strata = ~strata, nest = TRUE, \n#>     data = anadata)\n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)                1.0657919  0.6260889   1.702   0.1494  \n#> genderMale                 0.3821902  0.1703394   2.244   0.0749 .\n#> bornOthers                -0.6912102  0.2026407  -3.411   0.0190 *\n#> raceHispanic              -0.2442019  0.1823190  -1.339   0.2381  \n#> raceOther                 -0.1570271  0.2306208  -0.681   0.5262  \n#> raceWhite                 -0.3638735  0.2029676  -1.793   0.1330  \n#> marriedNever.married       0.4029107  0.2637962   1.527   0.1872  \n#> marriedPreviously.married -0.2096009  0.1620478  -1.293   0.2524  \n#> bmi                       -0.0002237  0.0134117  -0.017   0.9873  \n#> diabetesYes                0.6534019  0.2456333   2.660   0.0449 *\n#> age                       -0.0151364  0.0042038  -3.601   0.0155 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1.000456)\n#> \n#> Number of Fisher Scoring iterations: 4\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\n\nComparing with previous model fit\n\npublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\nAIC(fit3)\n#>       eff.p         AIC    deltabar \n#>   11.121795 1706.792543    1.235755\nAIC(fit9)\n#>      eff.p        AIC   deltabar \n#>   12.14310 1694.15974    1.21431\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata8.html",
    "href": "surveydata8.html",
    "title": "NHANES: Subsetting",
    "section": "",
    "text": "The tutorial demonstrates how to work with subset of complex survey data, specifically focusing on an NHANES example.\nThe required packages are loaded.\n\n# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\n\nLoad data\nSurvey data is loaded into the R environment.\n\nload(\"Data/surveydata/NHANES17.RData\")\nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\n\nCheck missingness\nA subset of variables is selected, and the presence of missing data is visualized.\n\nVars <- c(\"ID\", \n          \"weight\", \n          \"psu\", \n          \"strata\", \n          \"gender\", \n          \"born\", \n          \"race\", \n          \"bmi\", \n          \"cholesterol\", \n          \"diabetes\")\nanalytic.full.data <- analytic.with.miss[,Vars]\n\nA new variable is also created to categorize cholesterol levels as “healthy” or “unhealthy.”\n\nanalytic.full.data$cholesterol.bin <- ifelse(analytic.full.data$cholesterol <200, \"healthy\", \"unhealthy\")\nanalytic.full.data$cholesterol <- NULL\n\nrequire(DataExplorer)\nplot_missing(analytic.full.data)\n\n\n\n\nSubsetting Complex Survey data\nWe are subsetting based on whether the subjects have missing observation (e.g., only retaining those with complete information). This is often an eligibility criteria in studies. In missing data analysis, we will learn more about more appropriate approaches.\n\ndim(analytic.full.data)\n#> [1] 9254   10\nhead(analytic.full.data$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\nanalytic.complete.case.only <- as.data.frame(na.omit(analytic.full.data))\ndim(analytic.complete.case.only)\n#> [1] 6636   10\nhead(analytic.complete.case.only$ID) # complete case\n#> [1] 93705 93706 93707 93708 93709 93711\nhead(analytic.full.data$ID[analytic.full.data$ID %in% analytic.complete.case.only$ID])\n#> [1] 93705 93706 93707 93708 93709 93711\n\nBelow we show how to identify who has missing observations vs not based on full (analytic.full.data) and complete case (analytic.complete.case.only) data. See Heeringa et al (2010) book page 114 (section 4.5.3 “Preparation for Subclass analyses”) and also page 218 (section 7.5.4 “appropriate analysis: incorporating all Sample Design Features”). This is done for 2 reasons:\n\nfull complex survey design structure is taken into account, so that variance estimation is done correctly. If one or more PSU were excluded because none of the complete cases were observed in those PSU, the sub-population (complete cases) will not have complete information of how many PSU were actually present in the original complex design. Then in the population, a reduced number of PSUs would be used to calculate variance (number of SPU is a component of the variance calculation formula, see equation (5.2) in Heeringa et al (2010) textbook. Same is true for strata.), and will result in a wrong/biased variance estimate. Also see West et al. doi: 10.1177/1536867X0800800404\nsize of sub-population (here, those with complete cases) is recognized as a random variable; not just a fixed size.\n\n\n# assign missing indicator\nanalytic.full.data$miss <- 1 \n# assign missing indicator = 0 if the observation is available\nanalytic.full.data$miss[analytic.full.data$ID %in% analytic.complete.case.only$ID] <- 0\n\n\ntable(analytic.full.data$miss)\n#> \n#>    0    1 \n#> 6636 2618\n# IDs not in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==1])\n#> [1] 93703 93704 93710 93720 93724 93725\n# IDs in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==0])\n#> [1] 93705 93706 93707 93708 93709 93711\n\nLogistic regression on sub-population\nA logistic regression model is run on the subset of data that has no missing values. Here, it distinguishes between correct and incorrect approaches to account for the complex survey design.\n\nrequire(survey)\nrequire(Publish)\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nWrong approach\n\nw.design.wrong <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.complete.case.only, #wrong!!\n                       nest = TRUE)\n\nCorrect approach\n\nw.design0 <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.full.data, \n                       nest = TRUE)\n\n# retain only those that have complete observation / no missing\nw.design <- subset(w.design0, miss == 0)# this is the subset design\n\nFull model\n\nfit <- svyglm(model.formula, family = quasibinomial, \n              design = w.design) # subset design\npublish(fit)\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\nVariable selection\nFinally, we discuss variable selection methods. We employ backward elimination to determine which variables are significant predictors while retaining an important variable in the model. If unsure about usefulness of some (gender, born, race, bmi) variables in predicting the outcome, check via backward elimination while keeping important variable (diabetes, say, that has been established in the literature) in the model\n\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nscope <- list(upper = ~ diabetes+gender+born+race+bmi, lower = ~ diabetes)\n\nfit <- svyglm(model.formula, design=w.design, # subset design\n              family=quasibinomial)\n\nfitstep <- step(fit,  scope = scope, trace = FALSE, direction = \"backward\")\npublish(fitstep) # final model\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\nAlso see (Stata 2023) for further details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nStata. 2023. “How Can i Analyze a Subpopulation of My Survey Data in Stata?” https://stats.oarc.ucla.edu/stata/faq/how-can-i-analyze-a-subpopulation-of-my-survey-data-in-stata/."
  },
  {
    "objectID": "surveydataF.html",
    "href": "surveydataF.html",
    "title": "R functions (D)",
    "section": "",
    "text": "The list of new R functions introduced in this Survey data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n AIC \n    base/stats \n    To extract the AIC value of a model \n  \n\n as.character \n    base \n    To create a character vector \n  \n\n as.numeric \n    base \n    To create a numeric vector \n  \n\n eval \n    base \n    To evaluate an expression \n  \n\n fitted \n    base/stats \n    To extract fitted values of a model \n  \n\n ls \n    base \n    To see the list of objects \n  \n\n psrsq \n    survey \n    To compute the Nagelkerke and Cox-Snell pseudo R-squared statistics for survey data \n  \n\n regTermTest \n    survey \n    To test for an additional variable in a regression model \n  \n\n residuals \n    base/stats \n    To extract residuals of a model \n  \n\n stepAIC \n    MASS \n    To choose a model by stepwise AIC \n  \n\n step \n    base/stats \n    To choose a model by stepwise AIC but it can keep the pre-specified variables in the model \n  \n\n summ \n    jtools \n    To show/publish regression tables \n  \n\n svyboxplot \n    survey \n    To produce a box plot for survey data \n  \n\n svyby \n    survey \n    To see the summary statistics for a survey design \n  \n\n svychisq \n    survey \n    To test the bivariate assocaition between two categorical variables for survey data \n  \n\n svyCreateTableOne \n    tableone \n    To create a frequency table with a survey design \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n update \n    base/stats \n    To update and re-fit a regression model"
  },
  {
    "objectID": "surveydataQ.html#live-quiz",
    "href": "surveydataQ.html#live-quiz",
    "title": "Quiz (D)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "surveydataQ.html#download-quiz",
    "href": "surveydataQ.html#download-quiz",
    "title": "Quiz (D)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydataS.html",
    "href": "surveydataS.html",
    "title": "App (D)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can choose variables for bivariate analysis, and decide whether to apply survey features or not.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveD\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, survey and publish packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "surveydataE.html#problem-statement",
    "href": "surveydataE.html#problem-statement",
    "title": "Exercise (D)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will revisit the article by Flegal et al. (2016). Our primary aim this time is to execute the survey data analysis more rigorously, specifically by incorporating essential survey features into our analysis.\n\n\nThis is the same article that we discussed in our data access chapter!\nWe will reproduce some results from the article. The authors used NHANES 2013-14 dataset to create their main analytic dataset. The dataset contains 10,175 subjects with 12 relevant variables:\n\nSEQN: Respondent sequence number\nRIDAGEYR: Age in years at screening\nRIAGENDR: Gender\nDMDEDUC2: Education level\nRIDRETH3: Race/ethnicity\nRIDEXPRG: Pregnancy status at exam\nWTINT2YR: Full sample 2 year weights\nSDMVPSU: Masked variance pseudo-PSU\nSDMVSTRA: Masked variance pseudo-stratum\nBMXBMI: Body mass index in kg/m**2\nSMQ020: Whether smoked at least 100 cigarettes in life\nSMQ040: Current status of smoking (Do you now smoke cigarettes?)"
  },
  {
    "objectID": "surveydataE.html#question-1-creating-data-and-table",
    "href": "surveydataE.html#question-1-creating-data-and-table",
    "title": "Exercise (D)",
    "section": "Question 1: Creating data and table",
    "text": "Question 1: Creating data and table\n1(a) Importing dataset\n\n# you have to download the data in the same folder\nload(\"Data/surveydata/Flegal2016.RData\")\nls()\n#> [1] \"dat.full\"\nnames(dat.full)\n#>  [1] \"SEQN\"     \"RIDAGEYR\" \"RIAGENDR\" \"DMDEDUC2\" \"RIDRETH3\" \"RIDEXPRG\"\n#>  [7] \"WTINT2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BMXBMI\"   \"SMQ020\"   \"SMQ040\"\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria described in the second paragraph of the Methods section.\n\nHint: The authors restricted their study to\n\nadults aged 20 years and more,\nnon-missing body mass index, and\nnon-pregnant.\n\n\n\nYour analytic sample size should be 5,455, as described in the first sentence in the Results section.\n\n# 20+\ndat.analytic <- subset(dat.full, RIDAGEYR>=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic <- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ndat.analytic <- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\n\ndim(dat.analytic)\n#> [1] 5455   12\n\n1(c) Reproduce Table 1\nReproduce Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Please be advised to order the categories as shown in the table. tableone package could be helpful.\nHint 2: the authors did not show the results for the Other race category. But in your table, you could include all race categories.\n\n\nlibrary(tableone)\n\ndat <- dat.analytic\n\n# Age\ndat$age <- cut(dat$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat$gender <- dat$RIAGENDR\n\n# Race/Hispanic origin group\ndat$race <- dat$RIDRETH3\ndat$race <- car::recode(dat$race, \" 'Non-Hispanic White'='White'; 'Non-Hispanic Black'=\n                        'Black'; 'Non-Hispanic Asian'='Asian'; c('Mexican American',\n                        'Other Hispanic')='Hispanic'; 'Other Race - Including Multi-Rac'=\n                        'Other'; else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                      \"Hispanic\", \"Other\"))\n\n# Table 1: Overall \ntab11 <- CreateTableOne(vars = \"age\", strata = \"race\", data = dat, test = F, \n                        addOverall = T)\n\n# Table 1: Male\ntab12 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(dat, gender == \"Male\"))\n\n# Table 1: Female\ntab13 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(dat, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1a <- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1a, format = \"f\") # Showing only frequencies \n#> $Overall\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           5455    2343  1115  623   1214     160  \n#>   age                                                 \n#>      [20,40)  1810     734   362  216    412      86  \n#>      [40,60)  1896     759   383  251    449      54  \n#>      [60,Inf) 1749     850   370  156    353      20  \n#> \n#> $Male\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2638    1130  556   300   573      79   \n#>   age                                                 \n#>      [20,40)   909     386  182   106   189      46   \n#>      [40,60)   897     360  179   120   215      23   \n#>      [60,Inf)  832     384  195    74   169      10   \n#> \n#> $Female\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2817    1213  559   323   641      81   \n#>   age                                                 \n#>      [20,40)   901     348  180   110   223      40   \n#>      [40,60)   999     399  204   131   234      31   \n#>      [60,Inf)  917     466  175    82   184      10"
  },
  {
    "objectID": "surveydataE.html#question-2",
    "href": "surveydataE.html#question-2",
    "title": "Exercise (D)",
    "section": "Question 2",
    "text": "Question 2\n2(a) Reproduce Table 1 with survey features [15% grade]\nNot in this article but in many other articles, you would see n comes from the analytic sample and % comes from the survey design that accounts for survey features such as strata, clusters and survey weights. In Question 1, you see how n comes from the analytic sample. Your task for Question 2(a) is to create % part of the Table 1 with survey features, i.e., % should come from the survey design that accounts for strata, clusters and survey weights.\n\nHint 1: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 2: Generate age, gender, and race variable in your full data (codes shown in Question 1 could be helpful).\nHint 3: Subset the design.\nHint 4: Reproduce Table 1 with the design. svyCreateTableOne could be a helpful function.\n\n\n## Create all variables in the full data\n# Age\ndat.full$age <- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat.full$gender <- dat.full$RIAGENDR\n\n# Race/Hispanic origin group\ndat.full$race <- dat.full$RIDRETH3\ndat.full$race <- car::recode(dat.full$race, \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black'='Black'; 'Non-Hispanic Asian'='Asian'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             'Other Race - Including Multi-Rac'='Other'; \n                             else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                  \"Hispanic\", \"Other\"))\n\n## Subset the design\n# your codes here\n\n\n## Table 1 \n# your codes here\n\n#print(tab1b, format = \"p\") # Showing only percentages  \n\n2(b) Reproduce Table 3 [50% grade]\nReproduce the first column of Table 3 of the article (i.e., among men, explore the relationship between obesity and four predictors shown in the table).\n\nHint 1: If necessary, re-level or re-order the levels. Use Publish package to report the estimates.\nHint 2: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 3: The authors used SAS to produce the results vs. We are using R. The estimates could be slightly different (in second decimal point) from the estimates presented in Table 3, but they should be approximately similar.\nHint 4: You need to generate two variables, smoking status and education. The unweighted frequencies should be matched with the frequencies in eTable 1 and eTable 2.\n\nYour odds ratios could be look like as follows:\n\n\n\n\n\n\n## Recode Obese, Smoking status, Education - work on full data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Reproduce Table 3 - column 1\n# your codes here\n\n2(c) Model selection [25% grade]\nFrom the literature, you know that age and race needs to be adjusted in the model, but you are not sure about smoking and education. Run an AIC based backward selection process to figure out whether you want to add smoking or education, or both in the final model in 2(b). What is your conclusion [Expected answer: one short sentence]?\n\nHint 1: You need to make sure your design (that is based on eligibility) is free from missing values. Even after applying eligibility criteria, you may have some missing values on multiple variables (see eTable 1 and eTable 2). This is especially important for model selection process.\nHint 2: Work with the analytic data, keep only the relevant variables, and then remove missing values. Finally, subset the design and then select your final model.\n\n\n## Recode Obese, Smoking status, Education - work on analytic data\n# your codes here\n\n\n## Remove missing values - work on analytic data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Model selection \n# your codes here\n\n2(d) Testing for interactions [10% grade]\nCheck whether the interaction between age and smoking should be added in the 2(b) model (yes or no answer required, along with the code and p-value):\n\n# your codes here"
  },
  {
    "objectID": "missingdata.html#background",
    "href": "missingdata.html#background",
    "title": "Missing data analysis",
    "section": "Background",
    "text": "Background\nThe chapter provides a comprehensive guide on missing data analysis, emphasizing various imputation techniques to address data gaps. It begins by introducing the concept of imputation and the different types of missing data: MCAR, MAR, and MNAR. The tutorial then delves into multiple imputation methods for complex survey data, highlighting the importance of visualizing missing data patterns, creating multiple imputed datasets, and pooling results for a consolidated analysis. The challenges of imputing dependent and exposure variables are addressed, with a focus on the benefits of using auxiliary variables. The guide also explores the estimation of model performance in datasets with missing values, using metrics like the AUC and the Archer-Lemeshow test. Special attention is given to handling subpopulations with missing observations, testing the MCAR assumption empirically, and understanding effect modification within multiple imputation.\n\n\nIn the vast landscape of survey data analysis, one challenge consistently emerges as both a hurdle and an opportunity: missing data. As we delve into this chapter, we’ll confront the often-encountered issue of incomplete or absent data points in survey datasets. Missing data isn’t just a challenge; it’s an invitation to refine our analytical techniques. This chapter will equip you with the tools and methodologies to handle missing data adeptly, ensuring that our survey data analysis remains robust and reliable.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "missingdata.html#overview-of-tutorials",
    "href": "missingdata.html#overview-of-tutorials",
    "title": "Missing data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nMissing data and imputation\nImputation is a technique used to replace missing data with substituted values. In health research, missing data is a common issue, and imputation helps in ensuring datasets are complete, leading to more accurate analyses. There are three types of missing data: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR). The type of missingness determines how the missing data should be handled. Various single imputation methods, such as mean imputation, regression imputation, and predictive mean matching, are used based on the nature of the missing data. Multiple imputation is a process where the incomplete dataset is imputed multiple times, and the results are pooled together for more accurate analyses. Variable selection is crucial when analyzing missing data, and methods like majority, stack, and Wald are used to determine the best model. It’s also essential to assess the impact of missing values on model fitting (convergence and diagnostics) to ensure the reliability of the results.\n\n\nMultiple imputation in complex survey data\nIn the tutorial involve understanding how to assess the missing data patterns and visualize to understand the extent of missingness. Multiple imputations are then performed to address the missing data, creating multiple versions of the dataset with varying imputations. After imputation, new variables are created or modified for analysis, and the integrity of the imputed data is checked both visually. The tutorial also emphasizes the importance of combining multiple imputed datasets for analysis. Logistic regression is applied to the imputed datasets, and the results are pooled to get a single set of estimates. The tutorial concludes with a variable selection process to identify the most relevant variables for the model.\n\n\nMultiple imputation then deletion (MID)\nThis tutorial emphasizes the challenges of imputing dependent and exposure variables. The tutorial underscores the potential benefits of using auxiliary variables in the imputation process. While traditional Multiple Imputation (MI) and MID can yield similar results, MID is particularly advantageous when there’s a significant percentage of missing values in the outcome variable. The tutorial walks through the process of data loading, identifying missing values, performing standard imputations, and adding missing indicators. Subsequent steps involve structuring the data for survey design, fitting statistical models to each imputed dataset, and pooling the results for a consolidated analysis. The final stages focus on calculating and presenting odds ratios to interpret the relationships between variables.\n\n\nModel performance from multiple imputed datasets\nIn the context of survey data analysis, the provided tutorial outlines the process of estimating model performance, particularly when dealing with weighted data that has missing values. The focus is about estimating treatment effects, both individually and in a pooled manner. Model performance is gauged using the Area Under the Curve (AUC) and the Archer-Lemeshow (AL) test. This is done for models with and without interactions. The results provide insights into the model’s accuracy and fit, with the AUC offering a measure of the model’s discriminative ability and the AL test indicating the model’s goodness of fit to the data. The appendix provides a closer look at the user-defined functions used throughout the analysis.\n\n\nDealing with subpopulations with missing observations\nThe primary objective is to showcase how to handle missing data analysis with multiple imputation in the backdrop of complex surveys, particularly when we are interested in subpopulations. The process involves working with the analytic data, imputing missing values from this dataset, accounting for ineligible subjects from the complete data, and reincorporating these ineligible subjects into the imputed datasets. This ensures that the survey’s features can be utilized and the design subsetted accordingly. After importing and inspecting the dataset, the analysis subsets the data based on eligibility criteria, imputes missing values, and prepares the survey design. The subsequent steps involve design-adjusted logistic regression and pooling of estimates using Rubin’s rule.\n\n\nTesting MCAR assumption empirically in the data\nThe tutorial discusses the process of testing for Missing Completely At Random (MCAR) in datasets. Initially, essential packages are loaded to facilitate the analysis. A DAG is defined to represent the causal relationships between dataset variables, and this DAG is used to simulate a dataset. The DAG is then visualized, and the simulated dataset undergoes random data omission to mimic MCAR scenarios. Various visualizations, such as margin plots, are employed to understand the distribution of missing values in relation to other variables. Little’s MCAR test, a statistical method, is applied to determine if the data is indeed MCAR. The test’s limitations are also discussed. Additionally, tests for multivariate normality and homoscedasticity are conducted. In a subsequent section, data is intentionally set to missing based on a specific rule, and similar analyses and visualizations are performed to understand the nature of this missingness.\n\n\nEffect modification within multiple imputation\nThe tutorial delves into the intricacies of effect modification within the realm of survey data analysis. A dataset is comprising several imputed datasets is used. The primary objective is to investigate how two specific variables interact in predicting a particular outcome. To this end, logistic regression models are constructed for each level of a categorical variable. Emphasis is placed on the significance of Odds Ratios (ORs) in interpreting these interactions. Subsequently, simple slopes analyses are performed for each imputed dataset, shedding light on the relationship between the predictor and the outcome at distinct levels of a moderating variable. The outcomes from each imputed dataset are then pooled to offer a comprehensive understanding of the effect modification.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "missingdata0.html#missing-data-analysis",
    "href": "missingdata0.html#missing-data-analysis",
    "title": "Concepts (M)",
    "section": "Missing Data Analysis",
    "text": "Missing Data Analysis\nThis section is about understanding, categorizing, and addressing missing data in clinical and epidemiological research. It highlights the prevalence of missing data in these fields, the common use of complete case analysis without considering the implications, and the types of missingness: Missing Completely at Random (MCAR), Missing at Random (MAR), and Not Missing at Random (NMAR), each requiring different approaches and considerations. The consequences of not properly addressing missing data are detailed as bias, incorrect standard errors/precision, and a substantial loss of power.\nThis section also delves into strategies for addressing missing data, focusing on ad-hoc approaches and imputation methods. Ad-hoc approaches, such as ignoring missing data or using a missing category indicator, are generally dismissed as statistically invalid. In contrast, imputation, particularly multiple imputation (MI), is presented as a more robust and statistically sound method. Multiple imputation involves creating multiple complete datasets by predicting missing values and pooling the results to address the uncertainty associated with missing data. The section further discusses the types of imputation, the necessity of including a sufficient number of predictive variables, and the use of subject-area knowledge in building imputation models, providing a nuanced understanding of the challenges and solutions associated with missing data in research.\nReporting Guideline section delves into the complexities of handling missing data in statistical analysis, primarily through MI methods, especially Multiple Imputation by Chained Equations (MICE). It lays out the assumptions necessary for these methods (MCAR, MAR, MNAR). The guide also details how MICE works, using sequential regression imputation to create multiple imputed datasets, thereby allowing for more accurate and robust statistical inferences. Additionally, it provides comprehensive instructions on reporting MICE analysis, including detailing the missingness rates, the reasons for missing data, the assumptions made, and the specifics of the imputation and pooling methods used, ensuring transparency and reproducibility in research."
  },
  {
    "objectID": "missingdata0.html#reading-list",
    "href": "missingdata0.html#reading-list",
    "title": "Concepts (M)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Sterne and al. 2009)\nOptional reading: (Van Buuren 2018)\nFurther optional readings: (Lumley 2011; Granger, Sergeant, and Lunt 2019; Hughes et al. 2019)"
  },
  {
    "objectID": "missingdata0.html#video-lessons",
    "href": "missingdata0.html#video-lessons",
    "title": "Concepts (M)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMissing Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReporting guidelines when missing data is present"
  },
  {
    "objectID": "missingdata0.html#video-lesson-slides",
    "href": "missingdata0.html#video-lesson-slides",
    "title": "Concepts (M)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nMissing data\n\n\n\n\nReporting guideline"
  },
  {
    "objectID": "missingdata0.html#links",
    "href": "missingdata0.html#links",
    "title": "Concepts (M)",
    "section": "Links",
    "text": "Links\nVideo Lessons\n\nGoogle Slides\nPDF Slides\n\nReporting guideline\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "missingdata0.html#references",
    "href": "missingdata0.html#references",
    "title": "Concepts (M)",
    "section": "References",
    "text": "References\n\n\n\n\nGranger, Elizabeth, Jamie C. Sergeant, and Mark Lunt. 2019. “Avoiding Pitfalls When Combining Multiple Imputation and Propensity Scores.” Statistics in Medicine 38 (26): 5120–32.\n\n\nHughes, Rachael A., Jon Heron, Jonathan A. Sterne, and Kate Tilling. 2019. “Accounting for Missing Data in Statistical Analyses: Multiple Imputation Is Not Always the Answer.” International Journal of Epidemiology 1: 11.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using r. Vol. 565. John Wiley & Sons.\n\n\nSterne, Jonathan A., and et al. 2009. “Multiple Imputation for Missing Data in Epidemiological and Clinical Research: Potential and Pitfalls.” BMJ 338: b2393.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC."
  },
  {
    "objectID": "missingdata1.html",
    "href": "missingdata1.html",
    "title": "Imputation",
    "section": "",
    "text": "What is imputation?\nImputation is the process of replacing missing data with substituted values. In health research, it’s common to have missing data. This tutorial teaches you how to handle and replace these missing values using the mice package in R.\nWhy is imputation important?\nMissing data can lead to biased or incorrect results. Imputation helps in making the dataset complete, which can lead to more accurate analyses.\nKey reference\nIn this discussion, our primary guide and source of information is the work titled “Flexible Imputation of Missing Data” by Stef van Buuren, denoted here as (Van Buuren 2018). This book is an invaluable resource for anyone looking to delve deeper into the intricacies of handling missing data, especially in the context of statistical analyses. Below we also cited the relevant section numbers.\nFirst, you need to load the necessary libraries:\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(mitools)\n\nType of missing data\n\nRef: (Van Buuren 2018), Section 1.2\n\nIn this section, we are going to introduce three types of missing data that we will encounter in data analysis.\n\n\nMissing Completely at Random (MCAR):\n\nThe reason data is missing is completely random and not related to any measured or unmeasured variables. It’s often an unrealistic assumption.\n\n\nMissing at Random (MAR):\n\nThe missing data is related to variables that are observed.\n\n\nMissing Not at Random (MNAR):\n\nThe missing data is related to variables that are not observed.\nWhy does missingness type matter?\nThe type of missingness affects how you handle the missing data:\n\nIf data is MCAR, you can still analyze the complete cases without introducing bias.\nIf data is MAR, you can use imputation to replace the missing values.\nIf data is MNAR, it’s challenging to address, and estimates will likely be biased. We could do some sensitivity analyses to check the impact.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nData imputation\nGetting to know the data\nBefore imputing, you should understand the data. The tutorial uses the analytic.with.miss dataset from NHANES. Various plots and functions are used to inspect the missing data pattern and relationships between variables.\n\n\n\n\n\n\nImportant\n\n\n\n\nTake a look here for those who are interested in how the analytic data was created.\nFor the purposes of this lab, we are just going to treat the data as SRS, and not going to deal with intricacies of survey data analysis.\n\n\n\n\nrequire(VIM)\nload(\"Data/missingdata/NHANES17.RData\")\nNHANES17s <- analytic.with.miss[1:30,c(\"age\", \"bmi\", \"cholesterol\",\"diastolicBP\")]\nNHANES17s\n\n\n\n  \n\n\nNHANES17s[complete.cases(NHANES17s),]\n\n\n\n  \n\n\nmd.pattern(NHANES17s) \n\n\n\n#>    bmi cholesterol age diastolicBP   \n#> 15   1           1   1           1  0\n#> 4    1           1   1           0  1\n#> 4    1           1   0           1  1\n#> 1    1           0   1           1  1\n#> 4    1           0   0           0  3\n#> 2    0           0   0           0  4\n#>      2           7  10          10 29\n# Inspect the missing data pattern (each row = pattern)\n# possible missingness (0,1) pattern and counts\n# last col = missing counts for each variables\n# last row = how many variable values missing in the row\n# First col: Frequency of the pattern \n# e,g, 2 cases missing for bmi\n\nrequire(DataExplorer)\nplot_missing(NHANES17s)\n\n\n\n# check the missingness\n\nrequire(VIM)\nmarginplot(NHANES17s[, c(\"diastolicBP\", \"bmi\")])\n\n\n\nmarginplot(NHANES17s[, c(\"diastolicBP\", \"cholesterol\")])\n\n\n\nmarginplot(NHANES17s[, c(\"cholesterol\", \"bmi\")])\n\n\n\n# distribution of observed data given the other variable is observed\n# for MCAR, blue and red box plots should be similar\n\nSingle imputation\n\nRef: (Van Buuren 2018), Section 1.3\n\nImpute NA only once. Below are some examples (Van Buuren and Groothuis-Oudshoorn 2011):\nMean imputation\n\nRef: (Van Buuren 2018), Section 1.3.3, and (Buuren and Groothuis-Oudshoorn 2010)\n\n\nMean imputation is a straightforward method where missing values in a dataset are replaced with the mean of the observed values. While it’s simple and intuitive, this approach can reduce the overall variability of the data, leading to an underestimation of variance. This can be problematic in statistical analyses where understanding data spread is crucial.\n\n# Replace missing values by mean \nimputation1 <- mice(NHANES17s, \n                   method = \"mean\", # Replace by mean of the other values\n                   m = 1, # Number of multiple imputations. \n                   maxit = 1) # Number of iteration; mostly useful for convergence\n#> \n#>  iter imp variable\n#>   1   1  age  bmi  cholesterol  diastolicBP\nimputation1$imp\n#> $age\n#>       1\n#> 1  57.4\n#> 2  57.4\n#> 4  57.4\n#> 5  57.4\n#> 8  57.4\n#> 10 57.4\n#> 17 57.4\n#> 18 57.4\n#> 22 57.4\n#> 23 57.4\n#> \n#> $bmi\n#>           1\n#> 8  25.12857\n#> 18 25.12857\n#> \n#> $cholesterol\n#>           1\n#> 1  178.8261\n#> 2  178.8261\n#> 8  178.8261\n#> 18 178.8261\n#> 22 178.8261\n#> 23 178.8261\n#> 30 178.8261\n#> \n#> $diastolicBP\n#>       1\n#> 1  67.6\n#> 2  67.6\n#> 3  67.6\n#> 6  67.6\n#> 8  67.6\n#> 12 67.6\n#> 18 67.6\n#> 22 67.6\n#> 23 67.6\n#> 25 67.6\ncomplete(imputation1, action = 1) # this is a function from mice\n\n\n\n  \n\n\n# there is another function in tidyr with the same name!\n# use mice::complete() to avoid conflict\n## the imputed dataset\n\nRegression Imputation\n\nRef: (Van Buuren 2018), Section 1.3.4\n\nRegression imputation offers a more nuanced approach, especially when dealing with interrelated variables. By building a regression model using observed data, missing values are predicted based on the relationships between variables. This method can provide more accurate estimates for missing values by leveraging the inherent correlations within the data, making it a preferred choice in many scenarios over mean imputation.\n\\(Y \\sim X\\)\n\\(age \\sim bmi + cholesterol + diastolicBP\\)\n\nimputation2 <- mice(NHANES17s, \n            method = \"norm.predict\", # regression imputation\n            seed = 1,\n            m = 1, \n            print = FALSE)\n\n# look at all imputed values\nimputation2$imp\n#> $age\n#>           1\n#> 1  55.32215\n#> 2  54.99604\n#> 4  55.65437\n#> 5  53.68539\n#> 8  56.70424\n#> 10 55.79329\n#> 17 54.38372\n#> 18 56.70422\n#> 22 54.81486\n#> 23 55.06851\n#> \n#> $bmi\n#>           1\n#> 8  25.12857\n#> 18 25.12857\n#> \n#> $cholesterol\n#>           1\n#> 1  183.3772\n#> 2  184.2347\n#> 8  179.7431\n#> 18 179.7431\n#> 22 184.7111\n#> 23 184.0442\n#> 30 183.4399\n#> \n#> $diastolicBP\n#>           1\n#> 1  66.48453\n#> 2  66.24254\n#> 3  68.82463\n#> 6  67.47980\n#> 8  67.51011\n#> 12 68.91318\n#> 18 67.51011\n#> 22 66.10810\n#> 23 66.29631\n#> 25 67.93401\n\n# examine the correlation between age and bmi before and after imputation\nfit1 <- lm(age ~ bmi, NHANES17s) \n\nsummary(fit1) ## original data\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = NHANES17s)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.383  -5.194   3.168   9.444  15.965 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#> bmi           0.1462     0.6063   0.241  0.81219   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.51 on 18 degrees of freedom\n#>   (10 observations deleted due to missingness)\n#> Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#> F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nsqrt(summary(fit1)$r.squared)\n#> [1] 0.05674047\n\nfit2 <- lm(age ~ bmi, mice::complete(imputation2)) \nsummary(fit2) ## imputed complete data\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = mice::complete(imputation2))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.152  -1.407   0.000   8.026  15.989 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  52.1516     9.2485   5.639 4.86e-06 ***\n#> bmi           0.1812     0.3568   0.508    0.616    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.45 on 28 degrees of freedom\n#> Multiple R-squared:  0.009127,   Adjusted R-squared:  -0.02626 \n#> F-statistic: 0.2579 on 1 and 28 DF,  p-value: 0.6155\nsqrt(summary(fit2)$r.squared)\n#> [1] 0.09553283\n## Relationship become stronger before imputation. \n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation2), cor(age, bmi))\n#> [1] 0.09553283\n\nStochastic regression imputation\n\nRef: (Van Buuren 2018), Section 1.3.5\n\nRegression imputation, while powerful, has an inherent limitation. When it employs the fitted model to predict missing values, it does so without incorporating the error terms. This means that the imputed values are precisely on the regression line, leading to an overly perfect fit. As a result, the natural variability present in real-world data is not captured, causing the imputed dataset to exhibit biased correlations and reduced variance. Essentially, the data becomes too “clean,” and this lack of variability can mislead subsequent analyses, making them overly optimistic or even erroneous.\nRecognizing this limitation, stochastic regression imputation was introduced as an enhancement. Instead of merely using the fitted model, it adds a randomly drawn error term during the imputation process. This error term reintroduces the natural variability that the original regression imputation method missed. By doing so, the imputed values are scattered around the regression line, more accurately reflecting the true correlations and distributions in the dataset. This method, therefore, offers a more realistic representation of the data, ensuring that subsequent analyses are grounded in a dataset that mirrors genuine variability and relationships.\n\\(Y \\sim X + e\\)\n\\(age \\sim bmi + cholesterol + diastolicBP + error\\)\n\nimputation3 <- mice(NHANES17s, method = \"norm.nob\", # stochastic regression imputation\n                    m = 1, maxit = 1, seed = 504, print = FALSE)\n\n# look at all imputed values\nimputation3$imp\n#> $age\n#>           1\n#> 1  79.59513\n#> 2  53.94601\n#> 4  73.76486\n#> 5  43.69817\n#> 8  49.70112\n#> 10 43.81002\n#> 17 29.72590\n#> 18 61.15172\n#> 22 58.75506\n#> 23 78.39545\n#> \n#> $bmi\n#>           1\n#> 8  27.53270\n#> 18 31.52568\n#> \n#> $cholesterol\n#>           1\n#> 1  252.2928\n#> 2  209.7680\n#> 8  169.2450\n#> 18 107.7585\n#> 22 181.8617\n#> 23 239.9008\n#> 30 131.8489\n#> \n#> $diastolicBP\n#>           1\n#> 1  75.02181\n#> 2  44.45935\n#> 3  86.69637\n#> 6  60.54256\n#> 8  63.80884\n#> 12 60.03311\n#> 18 73.94575\n#> 22 36.70323\n#> 23 73.95647\n#> 25 65.84012\n#mice::complete(imputation3)\n\n# examine the correlation between age and bmi before and after imputation\nfit1 <- lm(age ~ bmi, NHANES17s) \nsummary(fit1) \n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = NHANES17s)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.383  -5.194   3.168   9.444  15.965 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#> bmi           0.1462     0.6063   0.241  0.81219   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.51 on 18 degrees of freedom\n#>   (10 observations deleted due to missingness)\n#> Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#> F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nfit3 <- lm(age ~ bmi, mice::complete(imputation3)) \nsummary(fit3)\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = mice::complete(imputation3))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.089  -6.691   3.183  10.104  21.288 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  60.4173    11.4671   5.269 1.33e-05 ***\n#> bmi          -0.1206     0.4371  -0.276    0.785    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.53 on 28 degrees of freedom\n#> Multiple R-squared:  0.002712,   Adjusted R-squared:  -0.03291 \n#> F-statistic: 0.07614 on 1 and 28 DF,  p-value: 0.7846\n## Fitted coefficients of bmi are much closer before and after imputation\n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation3), cor(age, bmi))\n#> [1] -0.05207502\n# see the direction change?\n\nPredictive mean matching\nPredictive Mean Matching (PMM) is an advanced imputation technique that aims to provide more realistic imputations for missing data. Let’s break it down:\nIn this context, we’re trying to fill in missing values for the variable ‘age’. To do this, we use other variables like ‘bmi’, ‘cholesterol’, and ‘diastolicBP’ to predict ‘age’. First, a regression model is run using the available data to estimate the relationship between ‘age’ and the predictor variables. From this model, we get a coefficient, which is then adjusted slightly to introduce some randomness. Using this adjusted coefficient, we predict the missing ‘age’ values for all subjects. For example, if ‘subject 19’ has a missing age value, we might predict it to be 45.5 years. Instead of using this predicted value directly, we look for other subjects who have actual age values and whose predicted ages are close to 45.5 years. From these subjects, one is randomly chosen, and their real age is used as the imputed value for ‘subject 19’. In this way, PMM ensures that the imputed values are based on real, observed data from the dataset.\n\n\n\n\n\n\nTip\n\n\n\n\nAssume \\(Y\\) = age, a variable with some missing values. \\(X\\) (say, bmi, cholesterol, diastolicBP) are predictors of \\(Y\\).\nEstimate beta coef \\(\\beta\\) from complete case running \\(Y \\sim X + e\\)\n\ngenerate new \\(\\beta* \\sim Normal(b,se_b)\\).\nusing \\(\\beta*\\), predict new \\(\\hat{Y}\\) predicted age for all subjects (those with missing and observed age):\n\nIf subject 19 (say) has missing values in age variable, find out his predicted age \\(\\hat{Y}\\) (say, 45.5).\nFind others subjects, subjects 2, 15, 24 (say) who has their age measured and their predicted age \\(\\hat{Y}\\) (say, predicted ages 43.9,45.7,46.1 with real ages 43,45,46 respectively) are close to subject 19 (predicted age 45.5).\nRandomly select subject 2 with real/observed age 43, and impute 43 for subject 19’s missing age.\n\n\n\n\n\nThe strength of PMM lies in its approach. Instead of imputing a potentially artificial value based on a prediction, it imputes a real, observed value from the dataset. This ensures that the imputed data retains the original data’s characteristics and doesn’t introduce any unrealistic values. It offers a safeguard against extrapolation, ensuring that the imputed values are always within the plausible range of the dataset.\n\nimputation3b <- mice(NHANES17s, method = \"pmm\", \n                    m = 1, maxit = 1,\n                    seed = 504, print = FALSE)\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation3b), cor(age, bmi))\n#> [1] -0.08029207\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nMultiple imputation and workflow\n\nRef: (Van Buuren 2018), Sections 1.4 and 5.1\nRef: (Buuren and Groothuis-Oudshoorn 2010)\n\n\nWe have learned different methods of imputation. In this section, we will introduce how to incorporate the data imputation into data analysis. In multiple imputation data analysis, three steps will be taken:\n\nStep 0: Set imputation model: Before starting the imputation process, it’s crucial to determine the appropriate imputation model based on the nature of the missing data and the relationships between variables. This model will guide how the missing values are estimated. For instance, if the data is missing at random, a linear regression model might be used for continuous data, while logistic regression might be used for binary data. The choice of model can significantly impact the quality of the imputed data, so it’s essential to understand the underlying mechanisms causing the missingness and select a model accordingly.\nStep 1: The incomplete dataset will be imputed \\(m\\) times: In this step, the incomplete dataset is imputed multiple times, resulting in \\(m\\) different “complete” datasets. The reason for creating multiple datasets is to capture the uncertainty around the missing values. Each of these datasets will have slightly different imputed values, reflecting the variability and uncertainty in the imputation process. The number of imputations, \\(m\\), is typically chosen based on the percentage of missing data and the desired level of accuracy. Common choices for \\(m\\) range from 5 to 50, but more imputations provide more accurate results, especially when the percentage of missing data is high.\nStep 2: Each \\(m\\) complete datasets will be analyzed separately by standard analysis (e.g., regression model): Once the \\(m\\) complete datasets are generated, each one is analyzed separately using standard statistical methods. For example, if the research question involves understanding the relationship between two variables, a regression model might be applied to each dataset. This step produces \\(m\\) sets of analysis results, one for each imputed dataset.\nStep 3: The analysis results will be pooled / aggregated together by Rubin’s rules (1987): The final step involves combining the results from the \\(m\\) separate analyses into a single set of results. This is done using Rubin’s rules (1987) (Little and Rubin 1987), which provide a way to aggregate the estimates and adjust for the variability between the imputed datasets. The pooled results give a more accurate and robust estimate than analyzing a single imputed dataset. Rubin’s rules ensure that the combined results reflect both the within-imputation variability (the variability in results from analyzing each dataset separately) and the between-imputation variability (the differences in results across the imputed datasets).\n\nStep 0\nSet imputation model:\n\nini <- mice(data = NHANES17s, maxit = 0, print = FALSE)\npred <- ini$pred\npred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           1   0           1           1\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n# A value of 1 indicates that column variables (say, bmi, cholesterol, diastolicBP) \n# are used as a predictor to impute the a row variable (say, age).\npred[,\"diastolicBP\"] <- 0 \n# if you believe 'diastolicBP' should not be a predictor in any imputation model\npred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           0\n#> bmi           1   0           1           0\n#> cholesterol   1   1           0           0\n#> diastolicBP   1   1           1           0\n# for cholesterol: bmi and age used to predict cholesterol (diastolicBP is not a predictor)\n# for diastolicBP: bmi, age and cholesterol used to predict diastolicBP \n# (diastolicBP itself is not a predictor) \n\nSet imputation method:\nSee Table 1 of (Van Buuren and Groothuis-Oudshoorn 2011).\n\nmeth <- ini$meth\nmeth\n#>         age         bmi cholesterol diastolicBP \n#>       \"pmm\"       \"pmm\"       \"pmm\"       \"pmm\"\n# pmm is generally a good method, \n# but let's see how to work with other methods\n# just as an example.\n# Specifying imputation method:\nmeth[\"bmi\"] <- \"mean\" \n# for BMI: no predictor used in mean method \n# (only average of observed bmi)\nmeth[\"cholesterol\"] <- \"norm.predict\" \nmeth[\"diastolicBP\"] <- \"norm.nob\"\nmeth\n#>            age            bmi    cholesterol    diastolicBP \n#>          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\"\n\nSet imputation model based on correlation alone:\n\npredictor.selection <- quickpred(NHANES17s, \n                                 mincor=0.1, # absolute correlation \n                                 minpuc=0.1) # proportion of usable cases\npredictor.selection\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n\nStep 1\n\n# Step 1 Impute the incomplete data m=10 times\nimputation4 <- mice(data=NHANES17s, \n                    seed=504,\n                    method = meth,\n                    predictorMatrix = predictor.selection,\n                    m=10, # imputation will be done 10 times (i.e., 10 imputed datasets)\n                    maxit=3)\n#> \n#>  iter imp variable\n#>   1   1  age  bmi  cholesterol  diastolicBP\n#>   1   2  age  bmi  cholesterol  diastolicBP\n#>   1   3  age  bmi  cholesterol  diastolicBP\n#>   1   4  age  bmi  cholesterol  diastolicBP\n#>   1   5  age  bmi  cholesterol  diastolicBP\n#>   1   6  age  bmi  cholesterol  diastolicBP\n#>   1   7  age  bmi  cholesterol  diastolicBP\n#>   1   8  age  bmi  cholesterol  diastolicBP\n#>   1   9  age  bmi  cholesterol  diastolicBP\n#>   1   10  age  bmi  cholesterol  diastolicBP\n#>   2   1  age  bmi  cholesterol  diastolicBP\n#>   2   2  age  bmi  cholesterol  diastolicBP\n#>   2   3  age  bmi  cholesterol  diastolicBP\n#>   2   4  age  bmi  cholesterol  diastolicBP\n#>   2   5  age  bmi  cholesterol  diastolicBP\n#>   2   6  age  bmi  cholesterol  diastolicBP\n#>   2   7  age  bmi  cholesterol  diastolicBP\n#>   2   8  age  bmi  cholesterol  diastolicBP\n#>   2   9  age  bmi  cholesterol  diastolicBP\n#>   2   10  age  bmi  cholesterol  diastolicBP\n#>   3   1  age  bmi  cholesterol  diastolicBP\n#>   3   2  age  bmi  cholesterol  diastolicBP\n#>   3   3  age  bmi  cholesterol  diastolicBP\n#>   3   4  age  bmi  cholesterol  diastolicBP\n#>   3   5  age  bmi  cholesterol  diastolicBP\n#>   3   6  age  bmi  cholesterol  diastolicBP\n#>   3   7  age  bmi  cholesterol  diastolicBP\n#>   3   8  age  bmi  cholesterol  diastolicBP\n#>   3   9  age  bmi  cholesterol  diastolicBP\n#>   3   10  age  bmi  cholesterol  diastolicBP\nimputation4$pred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n## look at the variables used for imputation\nmice::complete(imputation4, action = 1) # 1 imputed data  \n\n\n\n  \n\n\nall <- mice::complete(imputation4, action=\"long\") # combine all 5 imputed datasets\ndim(all)\n#> [1] 300   6\nhead(all)\n\n\n\n  \n\n\n## you can change the way of displaying the data\ndata_hori <- mice::complete(imputation4, action=\"broad\") # display five imputations horizontally\n#> New names:\n#> • `age` -> `age...1`\n#> • `bmi` -> `bmi...2`\n#> • `cholesterol` -> `cholesterol...3`\n#> • `diastolicBP` -> `diastolicBP...4`\n#> • `age` -> `age...5`\n#> • `bmi` -> `bmi...6`\n#> • `cholesterol` -> `cholesterol...7`\n#> • `diastolicBP` -> `diastolicBP...8`\n#> • `age` -> `age...9`\n#> • `bmi` -> `bmi...10`\n#> • `cholesterol` -> `cholesterol...11`\n#> • `diastolicBP` -> `diastolicBP...12`\n#> • `age` -> `age...13`\n#> • `bmi` -> `bmi...14`\n#> • `cholesterol` -> `cholesterol...15`\n#> • `diastolicBP` -> `diastolicBP...16`\n#> • `age` -> `age...17`\n#> • `bmi` -> `bmi...18`\n#> • `cholesterol` -> `cholesterol...19`\n#> • `diastolicBP` -> `diastolicBP...20`\n#> • `age` -> `age...21`\n#> • `bmi` -> `bmi...22`\n#> • `cholesterol` -> `cholesterol...23`\n#> • `diastolicBP` -> `diastolicBP...24`\n#> • `age` -> `age...25`\n#> • `bmi` -> `bmi...26`\n#> • `cholesterol` -> `cholesterol...27`\n#> • `diastolicBP` -> `diastolicBP...28`\n#> • `age` -> `age...29`\n#> • `bmi` -> `bmi...30`\n#> • `cholesterol` -> `cholesterol...31`\n#> • `diastolicBP` -> `diastolicBP...32`\n#> • `age` -> `age...33`\n#> • `bmi` -> `bmi...34`\n#> • `cholesterol` -> `cholesterol...35`\n#> • `diastolicBP` -> `diastolicBP...36`\n#> • `age` -> `age...37`\n#> • `bmi` -> `bmi...38`\n#> • `cholesterol` -> `cholesterol...39`\n#> • `diastolicBP` -> `diastolicBP...40`\n\ndim(data_hori)\n#> [1] 30 40\nhead(data_hori)\n\n\n\n  \n\n\n\n## Compare means of each imputed dataset\ncolMeans(data_hori)\n#>          age.1          bmi.1  cholesterol.1  diastolicBP.1          age.2 \n#>       61.06667       25.12857      179.47152       68.06998       58.20000 \n#>          bmi.2  cholesterol.2  diastolicBP.2          age.3          bmi.3 \n#>       25.12857      179.71210       66.61289       57.40000       25.12857 \n#>  cholesterol.3  diastolicBP.3          age.4          bmi.4  cholesterol.4 \n#>      180.28681       68.77854       54.86667       25.12857      179.58873 \n#>  diastolicBP.4          age.5          bmi.5  cholesterol.5  diastolicBP.5 \n#>       66.13461       52.80000       25.12857      179.35478       66.74254 \n#>          age.6          bmi.6  cholesterol.6  diastolicBP.6          age.7 \n#>       60.00000       25.12857      179.22499       66.76592       58.43333 \n#>          bmi.7  cholesterol.7  diastolicBP.7          age.8          bmi.8 \n#>       25.12857      178.99900       66.82760       56.20000       25.12857 \n#>  cholesterol.8  diastolicBP.8          age.9          bmi.9  cholesterol.9 \n#>      179.63656       67.31160       59.06667       25.12857      179.58295 \n#>  diastolicBP.9         age.10         bmi.10 cholesterol.10 diastolicBP.10 \n#>       66.37078       55.63333       25.12857      179.69042       67.60832\n\nStep 2\n\nimputation4\n#> Class: mids\n#> Number of multiple imputations:  10 \n#> Imputation methods:\n#>            age            bmi    cholesterol    diastolicBP \n#>          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\" \n#> PredictorMatrix:\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n\n\nimputation4[[1]]\n\n\n\n  \n\n\n\n\nmice::complete(imputation4, action = 1)\n\n\n\n  \n\n\n\n\nmice::complete(imputation4, action = 10)\n\n\n\n  \n\n\n\n\n# Step 2 Analyze the imputed data\nfit4 <- with(data = imputation4, exp = lm(cholesterol ~ age + bmi + diastolicBP))\n## fit model with each of 10 datasets separately\nfit4\n#> call :\n#> with.mids(data = imputation4, expr = lm(cholesterol ~ age + bmi + \n#>     diastolicBP))\n#> \n#> call1 :\n#> mice(data = NHANES17s, m = 10, method = meth, predictorMatrix = predictor.selection, \n#>     maxit = 3, seed = 504)\n#> \n#> nmis :\n#>         age         bmi cholesterol diastolicBP \n#>          10           2           7          10 \n#> \n#> analyses :\n#> [[1]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   210.68650     -0.19085     -0.52112     -0.09498  \n#> \n#> \n#> [[2]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   185.56395      0.06366     -0.49154      0.04196  \n#> \n#> \n#> [[3]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   188.01460     -0.07922     -0.52325      0.14493  \n#> \n#> \n#> [[4]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    210.2814       0.1096      -0.4602      -0.3802  \n#> \n#> \n#> [[5]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    167.8965       0.7171      -0.7613      -0.1090  \n#> \n#> \n#> [[6]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    187.4324       0.1727      -0.3452      -0.1482  \n#> \n#> \n#> [[7]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   203.45344     -0.22713     -0.37039     -0.02806  \n#> \n#> \n#> [[8]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>  213.721570    -0.003491    -0.517870    -0.310132  \n#> \n#> \n#> [[9]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   205.74248     -0.16224     -0.46983     -0.07188  \n#> \n#> \n#> [[10]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    181.2751       0.0698      -0.5425       0.1208\n\nStep 3\nUnderstanding the pooled results\nWe will show the result of entire pool later. First we want to show the pooled results for the age variable only an an example.\n\nrequire(dplyr)\nres10 <- summary(fit4) %>% as_tibble %>% print(n=40)\n#> # A tibble: 40 × 6\n#>    term         estimate std.error statistic   p.value  nobs\n#>    <chr>           <dbl>     <dbl>     <dbl>     <dbl> <int>\n#>  1 (Intercept) 211.         53.6     3.93    0.000561     30\n#>  2 age          -0.191       0.452  -0.422   0.676        30\n#>  3 bmi          -0.521       0.943  -0.553   0.585        30\n#>  4 diastolicBP  -0.0950      0.563  -0.169   0.867        30\n#>  5 (Intercept) 186.         47.9     3.88    0.000644     30\n#>  6 age           0.0637      0.459   0.139   0.891        30\n#>  7 bmi          -0.492       0.939  -0.524   0.605        30\n#>  8 diastolicBP   0.0420      0.533   0.0787  0.938        30\n#>  9 (Intercept) 188.         48.9     3.85    0.000694     30\n#> 10 age          -0.0792      0.470  -0.169   0.867        30\n#> 11 bmi          -0.523       0.926  -0.565   0.577        30\n#> 12 diastolicBP   0.145       0.547   0.265   0.793        30\n#> 13 (Intercept) 210.         38.6     5.44    0.0000105    30\n#> 14 age           0.110       0.372   0.295   0.771        30\n#> 15 bmi          -0.460       0.950  -0.485   0.632        30\n#> 16 diastolicBP  -0.380       0.534  -0.712   0.483        30\n#> 17 (Intercept) 168.         39.6     4.24    0.000253     30\n#> 18 age           0.717       0.299   2.39    0.0241       30\n#> 19 bmi          -0.761       0.898  -0.848   0.404        30\n#> 20 diastolicBP  -0.109       0.500  -0.218   0.829        30\n#> 21 (Intercept) 187.         57.1     3.28    0.00294      30\n#> 22 age           0.173       0.437   0.395   0.696        30\n#> 23 bmi          -0.345       0.943  -0.366   0.717        30\n#> 24 diastolicBP  -0.148       0.569  -0.260   0.797        30\n#> 25 (Intercept) 203.         48.5     4.20    0.000278     30\n#> 26 age          -0.227       0.390  -0.583   0.565        30\n#> 27 bmi          -0.370       0.921  -0.402   0.691        30\n#> 28 diastolicBP  -0.0281      0.536  -0.0523  0.959        30\n#> 29 (Intercept) 214.         51.5     4.15    0.000313     30\n#> 30 age          -0.00349     0.450  -0.00776 0.994        30\n#> 31 bmi          -0.518       0.927  -0.559   0.581        30\n#> 32 diastolicBP  -0.310       0.525  -0.590   0.560        30\n#> 33 (Intercept) 206.         47.8     4.30    0.000213     30\n#> 34 age          -0.162       0.392  -0.414   0.682        30\n#> 35 bmi          -0.470       0.921  -0.510   0.614        30\n#> 36 diastolicBP  -0.0719      0.523  -0.137   0.892        30\n#> 37 (Intercept) 181.         44.4     4.08    0.000379     30\n#> 38 age           0.0698      0.353   0.198   0.845        30\n#> 39 bmi          -0.543       0.970  -0.559   0.581        30\n#> 40 diastolicBP   0.121       0.558   0.216   0.830        30\nm10 <- res10[res10$term == \"age\",]\nm10\n\n\n\n  \n\n\n\nLet us describe the components of a pool for the age variable only:\n\nm.number <- 10\n# estimate = pooled estimate \n# = sum of (m “beta-hat” estimates) / m (mean of m estimated statistics)\nestimate <- mean(m10$estimate)\nestimate\n#> [1] 0.04699243\n# ubar = sum of (m variance[beta] estimates) / m \n# = within-imputation variance (mean of estimated variances)\nubar.var <- mean(m10$std.error^2)\nubar.var\n#> [1] 0.1686837\n# b =  variance of (m “beta-hat” estimates) \n# = between-imputation variance \n# (degree to which estimated statistic / \n# “beta-hat” varies across m imputed datasets). \n# This b is not available for single imputation when m = 1.\nb.var <- var(m10$estimate)\nb.var\n#> [1] 0.07372796\n# t = ubar + b + b/m = total variance according to Rubin’s rules \n# (within-imputation & between imputation variation)\nt.var <- ubar.var + b.var + b.var/m.number\nt.var\n#> [1] 0.2497845\n# riv = relative increase in variance\nriv = (b.var + b.var/m.number)/ubar.var\nriv\n#> [1] 0.4807859\n# lambda = proportion of variance to due nonresponse\nlambda = (b.var + b.var/m.number)/t.var\nlambda\n#> [1] 0.3246829\n# df (approximate for large sample without correction)\ndf.large.sample <- (m.number - 1)/lambda^2\ndf.large.sample\n#> [1] 85.37359\n# df (hypothetical complete data)\ndfcom <- m10$nobs[1] - 4 # n = 30, # parameters = 4\ndfcom\n#> [1] 26\n# df (Barnard-Rubin correction)\ndf.obs <- (dfcom + 1)/(dfcom + 3) * dfcom * (1 - lambda)\ndf.c <- df.large.sample * df.obs/(df.large.sample + df.obs)\ndf.c\n#> [1] 13.72019\n# fmi = fraction of missing information per parameter\nfmi = (riv + 2/(df.large.sample +3)) / (1 + riv)\nfmi # based on large sample approximation\n#> [1] 0.3399662\nfmi = (riv + 2/(df.c +3)) / (1 + riv)\nfmi # Barnard-Rubin correction\n#> [1] 0.4054616\n\nPooled estimate\nCompare above results with the pooled table from mice below. Note that df is based on Barnard-Rubin correction and fmi value is calculated based on that corrected df.\n\n# Step 3 pool the analysis results\nest1 <- mice::pool(fit4)\n## pool all estimated together using Rubin's rule \nest1\n\nClass: mipo    m = 10 (transposed version to accommodate space)\n\n\nTerm\n(Intercept)\nage\nbmi\ndiastolicBP\n\n\n\nm\n10\n10\n10\n10\n\n\nEstimate\n195.40679314\n0.04699243\n-0.50032666\n-0.08347279\n\n\n\\(\\bar{u}\\)\n2313.6362339\n0.1686837\n0.8722547\n0.2909291\n\n\nb\n237.04075365\n0.07372796\n0.01274940\n0.02857675\n\n\nt\n2574.3810629\n0.2497845\n0.8862790\n0.3223635\n\n\ndf_com\n26\n26\n26\n26\n\n\ndf\n21.22870\n13.72019\n23.80807\n21.35356\n\n\nRIV\n0.11269915\n0.48078595\n0.01607826\n0.10804843\n\n\n\\(\\lambda\\)\n0.10128447\n0.32468295\n0.01582384\n0.09751237\n\n\nFMI\n0.17547051\n0.40546159\n0.08924771\n0.17162783\n\n\n\nHere:\n\ndfcom = df for complete data\ndf = df with Barnard-Rubin correction\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSpecial case: Variable selection\nVariable selection in analyzing missing data\n\nRef: (Van Buuren 2018), Section 5.4\n\nThe common workflow for analyzing missing data are (as mentioned above):\n\nImputing the data \\(m\\) times\nAnalyzing the \\(m\\) dataset\nPool all analysis together\n\nWe could apply variable selection in step 2, especially when we have no idea what is the best model to analyze the data. Howevere, it may become challenging when we pull all data together. With different dataset, the final model may or may not be the same.\nWe present the three method of variable selection on each imputed dataset presented by Buuren:\n\nMajority: perform the model selection separately with m dataset and choose the variables that appears at least m/2 times\nStack: combine m datasets into a single dataset, and perform variable selection on this dataset\nWald (Rubin’s rule): model selection was performed at model fitting step and combine the estimates using Rubin’s rules. This is considered as gold standard.\n\nMajority using NHANES17s\n\ndata <- NHANES17s\nimp <- mice(data, seed = 504, m = 100, print = FALSE)\n## Multiple imputation with 100 imputations, resulting in 100 imputed datasets\nscope0 <- list(upper = ~ age + bmi + cholesterol, lower = ~1)\nexpr <- expression(f1 <- lm(diastolicBP ~ age),\n                   f2 <- step(f1, scope = scope0, trace = FALSE))\nfit5 <- with(imp, expr)\n\n## apply stepwise on each of the imputed dataset separately\nformulas <- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms <- lapply(formulas, terms)\nvotes <- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#> votes\n#>         age         bmi cholesterol \n#>           6          12           1\n\n\n## Set up the stepwise variable selection, from null model to full model\nscope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\n## Set up the stepwise variable selection, from important only model to full model\nexpr <- expression(f1 <- lm(diastolicBP ~ age),\n                   f2 <- step(f1, scope = scope, trace = FALSE))\nfit5 <- with(imp, expr)\n## apply stepwise on each of the imputed dataset separately\nformulas <- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms <- lapply(formulas, terms)\nvotes <- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#> votes\n#>         age         bmi cholesterol \n#>         100          11           1\n\nStack using NHANES17s\n\nStack.data <- mice::complete(imp, action=\"long\")\nhead(Stack.data)\n\n\n\n  \n\n\ntail(Stack.data)\n\n\n\n  \n\n\nfitx <- lm(diastolicBP ~ age + bmi + cholesterol, data = Stack.data)\nfity <- step(fitx, scope = scope0, trace = FALSE)\nrequire(Publish)\npublish(fity)\n#>     Variable Units Coefficient         CI.95 p-value \n#>  (Intercept)             63.70 [62.12;65.28] < 1e-04 \n#>          bmi              0.14   [0.08;0.20] < 1e-04\n\nWald using NHANES17s\n\n# m = 100\nfit7 <- with(data=imp, expr=lm(diastolicBP ~ 1))\nnames(fit7)\n#> [1] \"call\"     \"call1\"    \"nmis\"     \"analyses\"\nfit7$analyses[[1]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ 1)\n#> \n#> Coefficients:\n#> (Intercept)  \n#>       68.47\nfit7$analyses[[100]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ 1)\n#> \n#> Coefficients:\n#> (Intercept)  \n#>       65.47\nfit8 <- with(data=imp, expr=lm(diastolicBP ~ bmi))\nfit8$analyses[[45]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ bmi)\n#> \n#> Coefficients:\n#> (Intercept)          bmi  \n#>    63.93092      0.09209\nfit8$analyses[[99]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ bmi)\n#> \n#> Coefficients:\n#> (Intercept)          bmi  \n#>    68.34740      0.01797\n# The D1-statistics is the multivariate Wald test.\nstat <- D1(fit8, fit7)\n## use Wald test to see if we should add bmi into the model\nstat\n#>    test statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.1215668   1 22.70437    28 0.7305539 0.5550013\n# which indicates that adding bmi into our model might not be useful\n\n\nfit9 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi))\nstat <- D1(fit9, fit8)\n## use Wald test to see if we should add age into the model\nstat\n#>    test   statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.006608523   1 22.46746    27 0.9359289 0.4545242\n# which indicates that adding age into our model might not be useful\n\n\nfit10 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi + cholesterol))\nstat <- D1(fit10, fit9)\n## use Wald test to see if we should add cholesterol into the model\nstat\n#>    test    statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.0003547819   1 22.14158    26 0.9851409 0.3615345\n# which indicates that adding cholesterol into our model might not be useful\n\nTry method=\"likelihood\" as well.\n\nstat <- pool.compare(fit10, fit7, method = \"likelihood\", data=imp)\n## test to see if we should add all 3 variables into the model\nstat$pvalue\n#> [1] 0.9432629\n# which indicates that adding none of the variables into our model might be useful\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nAssess the impact of missing values in model fitting\nWhen working with datasets, missing values are a common challenge. These gaps in data can introduce biases and uncertainties, especially when we try to fit models to the data. To address this, researchers often use imputation methods to fill in the missing values based on the observed information. However, imputation itself can introduce uncertainties. Therefore, it’s essential to assess the impact of these missing values on model fitting. Buuren, as referenced in (Van Buuren 2018) Section 5.4.3, provides methods to do this. Out of the four methods presented by Buuren, the following two are the most commonly used:\n\nMultiple imputation with more number of imputations (i.e., 200). Perform variable selection on each imputed dataset. The differences are attributed to the missing values\nBootstrapping the data from a single imputed dataset and do variable selection for each bootstrapping sample. We could evaluate sampling variation using this method\n\nBootstrap using NHANES17s\n\nimpx <- mice(NHANES17s, seed = 504, m=1, print=FALSE)\ncompletedata <- mice::complete(impx)\n  \nset.seed(504) \nvotes <-c()\nformula0 <- as.formula(\"diastolicBP ~ age + bmi + cholesterol\")\nscope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\nfor (i in 1:200){\n     ind <- sample(1:nrow(completedata),replace = TRUE)\n     newdata <- completedata[ind,]\n     full.model <- glm(formula0, data = newdata)\n     f2 <- MASS::stepAIC(full.model, \n                   scope = scope, trace = FALSE)\n     formulas <- as.formula(f2)\n     temp <- unlist(labels(terms(formulas)))\n     votes <- c(votes,temp)\n }\n table(votes)\n#> votes\n#>         age         bmi cholesterol \n#>         200          59          17\n ## among 200 bootstrap samples how many times that each \n ## variable appears in the final model. Models have different\n ## variables are attributed to sampling variation\n\nConvergence and diagnostics\nConvergence\nWhen an algorithm converges, it means that the sequence of estimates generated by the algorithm stabilizes and reaches a distribution that does not depend on the initial values. This is crucial for imputation and other statistical analyses because it indicates that the estimates are representative of the true underlying statistical properties and are not biased by initial assumptions or specific starting conditions.\n\nRef: (Van Buuren 2018), Section 6.5.2\nMCMC Algorithm in MICE: The MICE package implements a MCMC algorithm for imputation. The coefficients should be converged and irrelevant to the order which variable is imputed first.\nUnderstanding pattern: For convergence to be achieved, these chains should mix well with each other, meaning their paths should overlap and crisscross freely. If they show distinct, separate trends or paths, it indicates a lack of convergence, suggesting that the imputation may not be reliable.\nVisualizing Convergence: We could plot the imputation object to see the streams.\n\n\n## Recall the imputation we have done before\nimputation5 <- mice(NHANES17s, seed = 504, \n                   m=10,\n                   maxit = 5,\n                   print=FALSE) \nplot(imputation5)\n\n\n\n\n\n\n## We hope to see no pattern in the trace lines\n## Sometimes to comfirm this we may want to run with more iterations\nimputation5_2 <- mice(NHANES17s, seed = 504, \n                    m=10,\n                    maxit = 50,\n                    print=FALSE) \nplot(imputation5_2)\n\n\n\n\n\n\n\nDiagnostics\nModel diagnostics plays a pivotal role in ensuring the robustness and accuracy of model fitting. Particularly in the realm of missing value imputations, where observed data serves as the foundation for estimating absent values, it becomes imperative to rigorously assess the imputation process. A straightforward diagnostic technique involves comparing the distributions of the observed data with the imputed values, especially when segmented or conditioned based on the variables that originally had missing entries. This comparison helps in discerning any discrepancies or biases introduced during the imputation, ensuring that the filled values align well with the inherent patterns of the observed data.\n\nRef: (Van Buuren 2018), Section 6.6\n\n\n## We could compare the imputed and observed data using Density plots\ndensityplot(imputation5, layout = c(2, 2))\n\n\n\nimputation5_3 <- mice(NHANES17s, seed = 504, \n                    m=50,\n                    maxit = 50,\n                    print=FALSE)\ndensityplot(imputation5_3)\n\n\n\n## a subjective judgment on whether you think if there is significant discrepancy\nbwplot(imputation5, age + bmi + cholesterol +diastolicBP ~ .imp, layout = c(2, 2))\n\n\n\nbwplot(imputation5_3)\n\n\n\n## Plot a box plot to compare the imputed and observed values\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nLittle, RJA, and DB Rubin. 1987. “Multiple Imputation for Nonresponse in Surveys.” John Wiley & Sons, Inc.. Doi 10: 9780470316696.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45: 1–67."
  },
  {
    "objectID": "missingdata2.html",
    "href": "missingdata2.html",
    "title": "Imputation in NHANES",
    "section": "",
    "text": "This tutorial provides a comprehensive guide on handling and analyzing complex survey data with missing values. In analyzing complex survey data, a distinct approach is required compared to handling regular datasets. Specifically, the intricacies of survey design necessitate the consideration of primary sampling units/cluster, sampling weights, and stratification factors. These elements ensure that the analysis accurately reflects the survey’s design and the underlying population structure. Recognizing and incorporating these factors is crucial for obtaining valid and representative insights from the data. As we delve into this tutorial, we’ll explore how to effectively integrate these components into our missing data analysis process.\nComplex Survey data\nIn the initial chunk, we load all the necessary libraries that will be used throughout the tutorial. These libraries provide functions and tools for data exploration, visualization, imputation, and analysis.\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\nNext, we load a dataset that contains survey data with some missing values. We then select specific columns or variables from this dataset that we are interested in analyzing. To understand the extent and pattern of missingness in our data, we visualize it and display the missing data pattern.\n\nload(\"Data/missingdata/NHANES17.RData\")\n\nVars <- c(\"ID\", \"weight\", \"psu\", \"strata\", \n          \"gender\", \"born\", \"race\", \n          \"bmi\", \"cholesterol\", \"diabetes\")\nanalyticx <- analytic.with.miss[,Vars]\nplot_missing(analyticx)\n\n\n\nmd.pattern(analyticx)\n\n\n\n#>      ID weight psu strata gender race born diabetes  bmi cholesterol     \n#> 6636  1      1   1      1      1    1    1        1    1           1    0\n#> 1364  1      1   1      1      1    1    1        1    1           0    1\n#> 97    1      1   1      1      1    1    1        1    0           1    1\n#> 795   1      1   1      1      1    1    1        1    0           0    2\n#> 4     1      1   1      1      1    1    1        0    1           1    1\n#> 357   1      1   1      1      1    1    1        0    0           0    3\n#> 1     1      1   1      1      1    1    0        1    1           1    1\n#>       0      0   0      0      0    0    1      361 1249        2516 4127\n\nImputing\nIn the following chunk, we address the missing data by performing multiple imputations. This means that instead of filling in each missing value with a single estimate, we create multiple versions (datasets) where each missing value is filled in differently based on a specified algorithm. This helps in capturing the uncertainty around the missing values. The chunk sets up the parameters for multiple imputations, ensuring reproducibility and efficiency, and then performs the imputations on the dataset with missing values.\n\n# imputation <- mice(analyticx, m=5, maxit=5, seed = 504007)\nset.seed(504)\nimputation <- parlmice(analyticx, m=5, maxit=5, cluster.seed=504007)\n#> Warning: 'parlmice' is deprecated.\n#> Use 'futuremice' instead.\n#> See help(\"Deprecated\")\n\n\n\nData Input: The primary input for the imputation function is the dataset with missing values. This dataset is what we aim to impute.\n\nNumber of Imputations: The option m=5 indicates that we want to create 5 different imputed datasets. Each of these datasets will have the missing values filled in slightly differently, based on the underlying imputation algorithm and the randomness introduced.\n\nMaximum Iterations: The imputation process is iterative, meaning it refines its estimates over several rounds. The option maxit=5 specifies that the algorithm should run for a maximum of 5 iterations. This helps in achieving more accurate imputations, especially when the missing data mechanism is complex.\n\nSetting Seed: In computational processes that involve randomness, it’s often useful to set a “seed” value. This ensures that the random processes are reproducible. If you run the imputation multiple times with the same seed, you’ll get the same results each time. Two seed values are set in the chunk: one using the general set.seed() function and another specifically for the imputation function as cluster.seed.\n\nParallel Processing: The function parlmice used for imputation indicates that the imputations are done in parallel. This means that instead of imputing one dataset after the other, the function tries to impute multiple datasets simultaneously (if the computational resources allow). This can speed up the process, especially when dealing with large datasets or when creating many imputed datasets.\nCreate new variable\nAfter imputation, we might want to create new variables or modify existing ones to better suit our analysis. Here, we transform one of the variables into a binary category based on a threshold. The choice of thresholds should be based on the data (i.e. median, quintiles, etc.) or on the literature (clinically relevant). This can help in simplifying the analysis or making the results more interpretable.\n\nimpdata <- complete(imputation, action=\"long\") #stacked data\nimpdata$cholesterol.bin <- ifelse(impdata$cholesterol < 200, \"healthy\", \"unhealthy\")\nimpdata$cholesterol.bin <- as.factor(impdata$cholesterol.bin)\ndim(impdata)\n#> [1] 46270    13\nhead(impdata)\n\n\n\n  \n\n\n\nChecking the data\nAfter imputation, it’s crucial to ensure that the imputed data maintains the integrity and structure of the original dataset. The following chunks are designed to help you visually and programmatically inspect the imputed data.\nVisual Inspection of Missing Data:\nIn this chunk, we visually inspect the imputed datasets to see if there are any remaining missing values. We specifically look at the first two imputed datasets. Visualization tools like these can quickly show if the imputation process was successful in filling all missing values.\n\nplot_missing(subset(impdata, subset=.imp==1))\n\n\n\nplot_missing(subset(impdata, subset=.imp==2))\n\n\n\n\nComparing Original and Imputed Data (First Imputed Dataset):\n\nIn this chunk, we focus on the first imputed dataset. We extract this dataset and display the initial entries to get a sense of the data.\nWe then remove any remaining missing values (if any) and display the initial entries of this complete dataset.\nNext, we compare the IDs (or unique identifiers) of the entries in the complete dataset with the original dataset to see which entries had missing values.\nWe then create a new variable that indicates whether an entry had missing values or not and tabulate this information.\n\n\nanalytic.miss1 <- subset(impdata, subset=.imp==1)\nhead(analytic.miss1$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic1 <- as.data.frame(na.omit(analytic.miss1))\nhead(analytic1$ID) # complete case\n#> [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss1$ID[analytic.miss1$ID %in% analytic1$ID])\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss1$miss <- 1\nanalytic.miss1$miss[analytic.miss1$ID %in% analytic1$ID] <- 0\ntable(analytic.miss1$miss)\n#> \n#>    0    1 \n#> 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 102840 102862 102919 102927 102928 102942\n\nComparing Original and Imputed Data (Second Imputed Dataset):\nThe this chunk is similar to the above but focuses on the second imputed dataset. We perform the same steps: extracting the dataset, removing missing values, comparing IDs, and creating a variable to indicate missingness.\n\nanalytic.miss2 <- subset(impdata, subset=.imp==2)\nhead(analytic.miss2$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic2 <- as.data.frame(na.omit(analytic.miss2))\nhead(analytic2$ID) # complete case\n#> [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss2$ID[analytic.miss2$ID %in% analytic2$ID])\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss2$miss <- 1\nanalytic.miss2$miss[analytic.miss2$ID %in% analytic2$ID] <- 0\ntable(analytic.miss2$miss)\n#> \n#>    0    1 \n#> 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 102840 102862 102919 102927 102928 102942\n\nAggregating Missingness Information Across All Imputed Datasets:\n\nIn the fourth chunk, we aim to consolidate the missingness information across all imputed datasets. We initialize a variable in the main dataset to indicate missingness.\nWe then loop through each of the imputed datasets and update the main dataset’s missingness variable based on the missingness information from each imputed dataset. This gives us a consolidated view of which entries had missing values across all imputed datasets.\n\n\nimpdata$miss<-1\nm <- 5\nfor (i in 1:m){\n  impdata$miss[impdata$.imp == i] <- analytic.miss2$miss\n  print(table(impdata$miss[impdata$.imp == i]))\n}\n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362\n\nCombining data\nSince we have multiple versions of the imputed dataset, we need a way to combine them for analysis. In the next chunks, we use a method to merge these datasets into a single list, making it easier to apply subsequent analyses on all datasets simultaneously.\n\nlibrary(mitools) \nallImputations <- imputationList(list(\n  subset(impdata, subset=.imp==1),\n  subset(impdata, subset=.imp==2),\n  subset(impdata, subset=.imp==3),\n  subset(impdata, subset=.imp==4), \n  subset(impdata, subset=.imp==5)))\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\nCombining data efficiently\n\nm <- 5\nset.seed(123)\nallImputations <-  imputationList(lapply(1:m, \n                                         function(n)  \n                                           subset(impdata, subset=.imp==n)))\n                                           #mice::complete(imputation, action = n)))\nsummary(allImputations)\n#>             Length Class  Mode\n#> imputations 5      -none- list\n#> call        2      -none- call\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\nLogistic regression\nWith our imputed datasets ready, we proceed to fit a statistical model. Here, we use logistic regression as an example. We fit the model to each imputed dataset separately and then extract relevant statistics like odds ratios and confidence intervals.\n\nrequire(jtools)\nrequire(survey)\ndata.list <- vector(\"list\", m)\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~diabetes+gender+born+race+bmi\")\n\n\nsummary(allImputations$imputations[[1]]$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12347   21060   34671   37562  419763\nsum(allImputations$imputations[[1]]$weight==0)\n#> [1] 550\nw.design0 <- svydesign(ids=~psu, weights=~weight, strata=~strata,\n                           data = allImputations, nest = TRUE)\nw.design <- subset(w.design0, miss == 0)\nfits <- with(w.design, svyglm(model.formula, family=quasibinomial))\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n# Estimate from first data\nexp(coef(fits[[1]]))[2]\n#> diabetesYes \n#>    1.409246\nexp(confint(fits[[1]]))[2,]\n#>    2.5 %   97.5 % \n#> 1.129997 1.757503\n# Estimate from second data\nexp(coef(fits[[2]]))[2]\n#> diabetesYes \n#>    1.437951\nexp(confint(fits[[2]]))[2,]\n#>    2.5 %   97.5 % \n#> 1.171160 1.765518\n\nPooled / averaged estimates\nAfter analyzing each imputed dataset separately, we need to combine the results to get a single set of estimates. This is done using a method that pools the results, taking into account the variability between the different imputed datasets.\n\npooled.estimates <- MIcombine(fits)\nsum.pooled <- summary(pooled.estimates)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(fits)\n#>                   results          se        (lower       upper) missInfo\n#> (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#> diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#> genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#> bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#> bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#> raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#> raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#> raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#> bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nexp(sum.pooled[,1])\n#> [1] 8.458294e+00 1.422037e+00 1.193362e+00 5.507777e-01 4.405626e-06\n#> [6] 1.149279e+00 8.407373e-01 6.989885e-01 9.606814e-01\nOR <- round(exp(pooled.estimates$coefficients),2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)),2)\nsig <- (CI[,1] < 1 & CI[,2] > 1)\nsig <- ifelse(sig==FALSE, \"*\", \"\")\nOR <- cbind(OR,CI,sig)\nOR\n\n\n\n  \n\n\n\nStep-by-step example\nThis segment offers a hands-on approach to understanding the imputation process. Here’s a breakdown:\n\n\nFitting Models to Individual Imputed Datasets:\n\nA list is initialized to store the results of models fitted to each imputed dataset.\nFor every dataset, the specific imputed data is extracted.\nA survey design is established, considering factors like primary sampling units, stratification, and weights. This ensures the analysis aligns with the survey’s design.\nThis design is then refined to only consider complete data entries.\nA logistic regression model is then applied to this refined data.\nThe results of this modeling are stored and displayed for review.\n\n\n\nPooling Results from All Models:\n\nAfter individual analysis, the next step is to combine or ‘pool’ these results.\nA special function is used to merge the results from all the models. This function accounts for variations between datasets and offers a combined estimate.\nA summary of this combined data is then displayed, offering insights like coefficients, standard errors, and more. Another version of this summary, focusing on log-effects, is also presented for deeper insights.\n\n\n\n\nfits2 <- vector(\"list\", m)\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design0.i <- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i <- subset(w.design0.i, miss == 0)\n  fit <- svyglm(model.formula, design=w.design.i, \n                family = quasibinomial(\"logit\"))\n  print(summ(fit))\n  fits2[[i]] <- fit\n}\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.26   0.20    11.32   0.00\n#> diabetesYes            0.34   0.09     3.67   0.01\n#> genderMale             0.17   0.09     1.88   0.10\n#> bornOthers            -0.62   0.10    -6.36   0.00\n#> bornRefused          -12.35   0.70   -17.63   0.00\n#> raceHispanic           0.18   0.14     1.27   0.25\n#> raceOther             -0.20   0.11    -1.77   0.12\n#> raceWhite             -0.38   0.13    -2.83   0.03\n#> bmi                   -0.04   0.01    -7.92   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.08   0.23     9.17   0.00\n#> diabetesYes            0.36   0.09     4.19   0.00\n#> genderMale             0.20   0.08     2.49   0.04\n#> bornOthers            -0.63   0.08    -7.41   0.00\n#> bornRefused          -12.33   0.71   -17.25   0.00\n#> raceHispanic           0.13   0.14     0.95   0.37\n#> raceOther             -0.16   0.10    -1.57   0.16\n#> raceWhite             -0.37   0.14    -2.71   0.03\n#> bmi                   -0.04   0.01    -6.05   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 1\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.04\n#> Pseudo-R² (McFadden) = 0.02\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.03   0.22     9.22   0.00\n#> diabetesYes            0.35   0.08     4.30   0.00\n#> genderMale             0.17   0.09     1.88   0.10\n#> bornOthers            -0.56   0.08    -7.21   0.00\n#> bornRefused          -12.30   0.70   -17.49   0.00\n#> raceHispanic           0.10   0.13     0.76   0.47\n#> raceOther             -0.19   0.10    -1.81   0.11\n#> raceWhite             -0.34   0.13    -2.59   0.04\n#> bmi                   -0.04   0.01    -6.58   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.18   0.21    10.57   0.00\n#> diabetesYes            0.36   0.09     3.75   0.01\n#> genderMale             0.16   0.09     1.78   0.12\n#> bornOthers            -0.57   0.10    -5.98   0.00\n#> bornRefused          -12.35   0.71   -17.32   0.00\n#> raceHispanic           0.12   0.14     0.83   0.43\n#> raceOther             -0.16   0.10    -1.61   0.15\n#> raceWhite             -0.34   0.13    -2.55   0.04\n#> bmi                   -0.04   0.01    -7.11   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.12   0.22     9.67   0.00\n#> diabetesYes            0.35   0.09     4.02   0.01\n#> genderMale             0.19   0.08     2.30   0.06\n#> bornOthers            -0.60   0.10    -6.06   0.00\n#> bornRefused          -12.33   0.71   -17.40   0.00\n#> raceHispanic           0.17   0.14     1.20   0.27\n#> raceOther             -0.17   0.10    -1.64   0.14\n#> raceWhite             -0.36   0.13    -2.78   0.03\n#> bmi                   -0.04   0.01    -6.65   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n\n\npooled.estimates <- MIcombine(fits2)\nsummary(pooled.estimates)\n#> Multiple imputation results:\n#>       MIcombine.default(fits2)\n#>                   results          se        (lower       upper) missInfo\n#> (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#> diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#> genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#> bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#> bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#> raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#> raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#> raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#> bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nsummary(pooled.estimates,logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fits2)\n#>              results      se  (lower  upper) missInfo\n#> (Intercept)  8.5e+00 2.0e+00 5.3e+00 1.3e+01     19 %\n#> diabetesYes  1.4e+00 1.3e-01 1.2e+00 1.7e+00      1 %\n#> genderMale   1.2e+00 1.1e-01 1.0e+00 1.4e+00      5 %\n#> bornOthers   5.5e-01 5.3e-02 4.6e-01 6.7e-01     11 %\n#> bornRefused  4.4e-06 3.1e-06 1.1e-06 1.8e-05      0 %\n#> raceHispanic 1.1e+00 1.6e-01 8.7e-01 1.5e+00      6 %\n#> raceOther    8.4e-01 8.9e-02 6.8e-01 1.0e+00      5 %\n#> raceWhite    7.0e-01 9.4e-02 5.4e-01 9.1e-01      2 %\n#> bmi          9.6e-01 6.3e-03 9.5e-01 9.7e-01     20 %\n\nVariable selection\nSometimes, not all variables in the dataset are relevant for our analysis. In the final chunks, we apply a method to select the most relevant variables for our model. This can help in simplifying the model and improving its interpretability.\n\nrequire(jtools)\nrequire(survey)\ndata.list <- vector(\"list\", m)\nmodel.formula <- as.formula(\"cholesterol~diabetes+gender+born+race+bmi\")\nscope <- list(upper = ~ diabetes+gender+born+race+bmi,\n              lower = ~ diabetes)\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design0.i <- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i <- subset(w.design0.i, miss == 0)\n  fit <- svyglm(model.formula, design=w.design.i)\n  fitstep <- step(fit, scope = scope, trace = FALSE,\n                              direction = \"backward\")\n  data.list[[i]] <- fitstep\n}\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n\nCheck out the variables selected\n\nx <- all.vars(formula(fit))\nfor (i in 1:m) x <- c(x, all.vars(formula(data.list[[i]])))\ntable(x)-1\n#> x\n#>         bmi        born cholesterol    diabetes      gender        race \n#>           5           5           5           5           5           5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "missingdata3.html",
    "href": "missingdata3.html",
    "title": "Missing in outcome",
    "section": "",
    "text": "This section provides a theoretical background on the concept of Multiple Imputation and then Deletion (MID). It highlights the challenges of imputing dependent and exposure variables and introduces the idea of using auxiliary variables to aid imputation. The section also contrasts the results of traditional MI with MID, especially when the number of imputed datasets is high.\n\nOften researchers are reluctant to impute values in the dependent variable (and exposure variable). Particularly, for dependent variable, imputation might not help too much.\nHowever, if you have a good auxiliary variable (e.g., strongly correlated predictor, that are not used in the main analysis), often multiple imputation method can help. Use of auxiliary variables is one of the greatest strengths of MI methods.\nMI algorithm generally do not have any special treatment for dependent variable in its original form, and hence ignoring dependent variable completely may not be a good idea in many scenarios.\nMultiple imputation followed by deletion of imputed outcomes is known as MID. This is very popular, especially when you have high percentage missing values in the outcome variable (e.g., 20%-50%). For low missing % in outcome, the advantage can be minimal.\nWe are extending this idea to deletion of imputed exposures as well (researchers are often reluctant to impute primary exposure of interest).\nOriginal MI and MID may result in similar results when m (number of imputed datasets) is higher.\n\nData\nIn the initial chunk, we load several packages that provide functions and tools necessary for the subsequent analysis. These packages facilitate multiple imputation, data visualization, and statistical modeling among other tasks.\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\nWe load the necessary data:\n\nload(\"Data/missingdata/NHANES17.RData\")\n\nThe data is briefly inspected to understand its structure. An identifier column is added to uniquely identify each row or observation in the dataset.\n\nrequire(mice)\nnhanes2\n\n\n\n  \n\n\nnhanes2$id <- 1:nrow(nhanes2)\nnhanes2\n\n\n\n  \n\n\n\nOutcome and exposure has missing\nThis chunk focuses on identifying which rows have missing values in both the outcome and exposure variables. The outcome and exposure variables are crucial for the analysis, so understanding where they are missing is essential.\n\n# assume outcome = bmi and exposure = chl \nnhanes2.excludingYA <- subset(nhanes2, !is.na(bmi) & !is.na(chl) )\nnhanes2.excludingYA # data without missing A and Y\n\n\n\n  \n\n\n# identify ids of subjects with missing A & Y \nnhanes2.excludingYA$id\n#>  [1]  2  5  7  8  9 13 14 17 18 19 22 23 25\n\nImpute as usual\nUsing the entire dataset, missing values are imputed. This is done by first initializing an imputation model and then performing the imputation to create multiple datasets where missing values are filled in. The result is a list of datasets with imputed values. That means, we impute Y and A for now, as well as other covariates with missing values.\n\n# use full data to impute \nini <- mice(nhanes2, pri = FALSE)\nini$method\n#>      age      bmi      hyp      chl       id \n#>       \"\"    \"pmm\" \"logreg\"    \"pmm\"       \"\"\npred <- ini$predictorMatrix\npred\n#>     age bmi hyp chl id\n#> age   0   1   1   1  1\n#> bmi   1   0   1   1  1\n#> hyp   1   1   0   1  1\n#> chl   1   1   1   0  1\n#> id    1   1   1   1  0\npred[,\"id\"] <- 0 # as this is not a predictor\nm <- 5\nimp <- mice(data=nhanes2, m=m, maxit=3, seed=504007)\n#> \n#>  iter imp variable\n#>   1   1  bmi  hyp  chl\n#>   1   2  bmi  hyp  chl\n#>   1   3  bmi  hyp  chl\n#>   1   4  bmi  hyp  chl\n#>   1   5  bmi  hyp  chl\n#>   2   1  bmi  hyp  chl\n#>   2   2  bmi  hyp  chl\n#>   2   3  bmi  hyp  chl\n#>   2   4  bmi  hyp  chl\n#>   2   5  bmi  hyp  chl\n#>   3   1  bmi  hyp  chl\n#>   3   2  bmi  hyp  chl\n#>   3   3  bmi  hyp  chl\n#>   3   4  bmi  hyp  chl\n#>   3   5  bmi  hyp  chl\n# list format in m data\nimpdata <- mice::complete(imp, action = \"all\")\nimpdata # all IDs are present\n#> $`1`\n#>      age  bmi hyp chl id\n#> 1  20-39 20.4  no 187  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 27.4  no 187  3\n#> 4  60-99 25.5 yes 204  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.5 yes 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 20.4  no 199 10\n#> 11 20-39 27.5  no 199 11\n#> 12 40-59 26.3  no 284 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 187 15\n#> 16 20-39 35.3 yes 204 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 284 20\n#> 21 20-39 35.3 yes 184 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`2`\n#>      age  bmi hyp chl id\n#> 1  20-39 28.7 yes 238  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 30.1  no 187  3\n#> 4  60-99 22.5 yes 218  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 20.4 yes 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 22.5 yes 238 10\n#> 11 20-39 26.3 yes 118 11\n#> 12 40-59 20.4  no 199 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 187 15\n#> 16 20-39 24.9  no 238 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 218 20\n#> 21 20-39 27.5  no 187 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`3`\n#>      age  bmi hyp chl id\n#> 1  20-39 25.5  no 229  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 22.0  no 187  3\n#> 4  60-99 20.4  no 199  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.7  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 27.4  no 184 10\n#> 11 20-39 30.1  no 238 11\n#> 12 40-59 22.0  no 186 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 229 15\n#> 16 20-39 22.0  no 118 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 218 20\n#> 21 20-39 29.6  no 131 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`4`\n#>      age  bmi hyp chl id\n#> 1  20-39 24.9  no 187  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 27.4  no 187  3\n#> 4  60-99 24.9 yes 206  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.5  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 26.3 yes 187 10\n#> 11 20-39 29.6  no 229 11\n#> 12 40-59 27.4  no 204 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 186 15\n#> 16 20-39 35.3  no 184 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 199 20\n#> 21 20-39 27.5  no 187 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 284 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`5`\n#>      age  bmi hyp chl id\n#> 1  20-39 27.2  no 238  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 24.9  no 187  3\n#> 4  60-99 20.4  no 229  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 21.7  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 20.4 yes 187 10\n#> 11 20-39 25.5  no 118 11\n#> 12 40-59 21.7  no 187 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 199 15\n#> 16 20-39 27.5  no 187 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 206 20\n#> 21 20-39 33.2  no 206 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 204 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> attr(,\"class\")\n#> [1] \"mild\" \"list\"\n\nInclude a missing indicator (Y & A)\nFor each imputed dataset, a new column is added to indicate whether the outcome and exposure variables were originally missing. This “missing indicator” column will be used later to subset the data.\n\n# Define formula (making binary Y)\nformula <- as.formula(\"I(bmi>25) ~ chl + hyp\")\ndata.list <- vector(\"list\", m)\n# subset the data without Y and A's that had missing values\n# and record those subset data\nfor (i in 1:m) {\n  analytic.i <- impdata[[i]]\n  analytic.i$miss <- 1\n  analytic.i$miss[analytic.i$id %in% nhanes2.excludingYA$id] <- 0\n  data.list[[i]] <- analytic.i\n}\ndata.list  # only relevant IDs are present\n#> [[1]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 20.4  no 187  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 27.4  no 187  3    1\n#> 4  60-99 25.5 yes 204  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.5 yes 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 20.4  no 199 10    1\n#> 11 20-39 27.5  no 199 11    1\n#> 12 40-59 26.3  no 284 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 187 15    1\n#> 16 20-39 35.3 yes 204 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 284 20    1\n#> 21 20-39 35.3 yes 184 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[2]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 28.7 yes 238  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 30.1  no 187  3    1\n#> 4  60-99 22.5 yes 218  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 20.4 yes 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 22.5 yes 238 10    1\n#> 11 20-39 26.3 yes 118 11    1\n#> 12 40-59 20.4  no 199 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 187 15    1\n#> 16 20-39 24.9  no 238 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 218 20    1\n#> 21 20-39 27.5  no 187 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[3]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 25.5  no 229  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 22.0  no 187  3    1\n#> 4  60-99 20.4  no 199  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.7  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 27.4  no 184 10    1\n#> 11 20-39 30.1  no 238 11    1\n#> 12 40-59 22.0  no 186 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 229 15    1\n#> 16 20-39 22.0  no 118 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 218 20    1\n#> 21 20-39 29.6  no 131 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[4]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 24.9  no 187  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 27.4  no 187  3    1\n#> 4  60-99 24.9 yes 206  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.5  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 26.3 yes 187 10    1\n#> 11 20-39 29.6  no 229 11    1\n#> 12 40-59 27.4  no 204 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 186 15    1\n#> 16 20-39 35.3  no 184 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 199 20    1\n#> 21 20-39 27.5  no 187 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 284 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[5]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 27.2  no 238  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 24.9  no 187  3    1\n#> 4  60-99 20.4  no 229  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 21.7  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 20.4 yes 187 10    1\n#> 11 20-39 25.5  no 118 11    1\n#> 12 40-59 21.7  no 187 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 199 15    1\n#> 16 20-39 27.5  no 187 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 206 20    1\n#> 21 20-39 33.2  no 206 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 204 24    1\n#> 25 40-59 27.4  no 186 25    0\n# record the fits from each data\n\nDesign, subset and fit\nFor each imputed dataset, a statistical model is fitted. Before fitting, the data is structured to account for survey design features. Only rows without originally missing outcome and exposure values are used for model fitting. The results of the model fitting for each dataset are stored for later analysis.\n\nrequire(survey)\nfit.list <- vector(\"list\", 5)\nfor (i in 1:m) {\n  analytic.i <- data.list[[i]]\n  # assigning survey features = 1\n  w.design0 <- svydesign(id=~1, weights=~1,\n                        data=analytic.i)\n  w.design <- subset(w.design0, miss == 0)\n  fit <- svyglm(formula, design=w.design, family=binomial)\n  fit.list[[i]] <-  fit\n}\n\nPooled results\nAfter fitting models to each imputed dataset, the results are combined or “pooled”. This pooled result provides a more robust estimate by considering the variability across the imputed datasets.\n\nrequire(mitools)\npooled.estimates <- MIcombine(fit.list)\npooled.estimates\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>                  results         se\n#> (Intercept) -1.769918773 2.73723080\n#> chl          0.009747869 0.01499608\n#> hypyes      18.128613035 1.05775401\n\n# or you can do it this way\nbetas<-MIextract(fit.list,fun=coef)\nvars<-MIextract(fit.list, fun=vcov)\nsummary(MIcombine(betas,vars))\n#> Multiple imputation results:\n#>       MIcombine.default(betas, vars)\n#>                  results         se      (lower      upper) missInfo\n#> (Intercept) -1.769918773 2.73723080 -7.13479255  3.59495501      0 %\n#> chl          0.009747869 0.01499608 -0.01964391  0.03913964      0 %\n#> hypyes      18.128613035 1.05775401 16.05545326 20.20177281      0 %\n\n# report beta coef\nsum.pooled <- summary(pooled.estimates, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>             results    se (lower upper) missInfo\n#> (Intercept) -1.7699 2.737  -7.13  3.595      0 %\n#> chl          0.0097 0.015  -0.02  0.039      0 %\n#> hypyes      18.1286 1.058  16.06 20.202      0 %\nsum.pooled\n\n\n\n  \n\n\n\nReport OR\nThe pooled results are further processed to calculate and report odds ratios, which provide insights into the relationships between variables in the context of logistic regression.\n\nsum.pooled.OR <- summary(pooled.estimates, logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>             results      se  (lower  upper) missInfo\n#> (Intercept) 1.7e-01 4.7e-01 8.0e-04 3.6e+01      0 %\n#> chl         1.0e+00 1.5e-02 9.8e-01 1.0e+00      0 %\n#> hypyes      7.5e+07 7.9e+07 9.4e+06 5.9e+08      0 %\nsum.pooled.OR\n\n\n\n  \n\n\n\nUsing publish package may be possible, but requires complicated process. Look for publish.MIresult (but this can be complicated)."
  },
  {
    "objectID": "missingdata4.html",
    "href": "missingdata4.html",
    "title": "Performance with NA",
    "section": "",
    "text": "This is a tutorial of how to estimate model performance while analyzing survey data with missing values (for predictive goals).\n\n# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\nUseful functions\nThe functions below could be helpful in\n\n\nsvyROCw3: calculating area under the ROC curve (AUC) value with survey data with logistic regression\n\n\nAL.gof3: testing Archer-Lemeshow goodness of fit for survey data with logistic regression\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# ROC curve for Survey Data with Logistic Regression\nsvyROCw3 <- function(fit=fit7,outcome=analytic2$CVD==\"event\", weight = NULL,plot=FALSE){\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  if (plot == TRUE){\n    roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n    with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  }\n  } else { # library(WeightedROC)\n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  if (plot == TRUE){\n    with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n  }\n  return(auc)\n}\n\n# Archer-Lemeshow Goodness of Fit Test for Survey Data with Logistic Regression\nAL.gof3 <- function(fit=fit7, data = analytic2, weight = \"weight\", psu = \"psu\", \n                    strata= \"strata\"){\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  if (is.null(psu)){\n    newdesign <- svydesign(id=~1,\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  if (!is.null(psu)) {\n    newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(as.numeric(res$p)) \n}\n\nLoad imputed 5 sets of data\nSaved at the end of Lab 6 part 2.\n\n# Saved from last lab\nload(\"Data/missingdata/missOA123CVDnorth.RData\")\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 2 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 1 2 1 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\nEstimating treatment effect\nIndividual beta estimates\n\nlibrary(survey)\nw.design <- svydesign(ids=~1, weights=~weight,\n                           data = allImputations)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nestimates <- with(w.design, svyglm(model.formula, family=quasibinomial))\nestimates\n#> [[1]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6809                  1.1063                  0.7911  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6233                  2.0076                  0.6278  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4347                 -0.6031                 -0.6805  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2136                  0.8277                  1.0344  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9143                  0.8166                 -0.8183  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1404                 -0.7286                 -0.9360  \n#>        sexMale:copdYes  \n#>                 0.6167  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31100     AIC: NA\n#> \n#> [[2]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.4837                  0.8766                  0.7781  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6090                  1.9901                  0.6083  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4415                 -0.5969                 -0.6840  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2374                  0.5877                  1.0413  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.8837                  0.8054                 -0.5388  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1303                 -0.6944                 -0.8545  \n#>        sexMale:copdYes  \n#>                 0.5857  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31260     AIC: NA\n#> \n#> [[3]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6365                  1.0389                  0.7815  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6140                  1.9986                  0.6180  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4334                 -0.5945                 -0.6843  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2042                  0.7963                  1.0386  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9468                  0.8020                 -0.7330  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1185                 -0.7775                 -0.9414  \n#>        sexMale:copdYes  \n#>                 0.5548  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31130     AIC: NA\n#> \n#> [[4]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6811                  1.2937                  0.7838  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6248                  2.0097                  0.6285  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4431                 -0.6012                 -0.6877  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2325                  0.8179                  1.0401  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9368                  0.8022                 -1.0624  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.0381                 -0.7407                 -0.9356  \n#>        sexMale:copdYes  \n#>                 0.5422  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31100     AIC: NA\n#> \n#> [[5]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.4487                  0.7569                  0.7752  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6021                  1.9847                  0.6107  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4393                 -0.5977                 -0.6788  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2351                  0.5457                  1.0421  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9235                  0.8057                 -0.3884  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1275                 -0.7370                 -0.9152  \n#>        sexMale:copdYes  \n#>                 0.5871  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31290     AIC: NA\n#> \n#> attr(,\"call\")\n#> with(w.design, svyglm(model.formula, family = quasibinomial))\n\nPooled / averaged estimates for beta and OR\n\nlibrary(\"mitools\")\npooled.estimates <- MIcombine(estimates)\npooled.estimates\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(estimates)\n#>                           results         se\n#> (Intercept)            -5.5861842 0.19239878\n#> OAOA                    1.0144758 0.27097789\n#> age40-49 years          0.7819429 0.10165218\n#> age50-59 years          1.6146502 0.09905167\n#> age60-64 years          1.9981358 0.10380099\n#> sexMale                 0.6186653 0.05484262\n#> income$30,000-$49,999  -0.4383959 0.06787453\n#> income$50,000-$79,999  -0.5987066 0.06593903\n#> income$80,000 or more  -0.6830429 0.06907307\n#> raceWhite               0.2245784 0.10567063\n#> painmedYes              0.7150386 0.16508180\n#> htYes                   1.0393147 0.05522682\n#> copdYes                 1.9210219 0.65240500\n#> diabYes                 0.8063590 0.08169615\n#> OAOA:painmedYes        -0.7081707 0.32575000\n#> age40-49 years:copdYes -1.1109564 0.69441469\n#> age50-59 years:copdYes -0.7356335 0.67352566\n#> age60-64 years:copdYes -0.9165474 0.65626790\n#> sexMale:copdYes         0.5772949 0.30888502\n\nsum.pooled <- summary(pooled.estimates,logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(estimates)\n#>                        results      se (lower  upper) missInfo\n#> (Intercept)             0.0037 0.00072 0.0025  0.0056     45 %\n#> OAOA                    2.7579 0.74733 1.4778  5.1469     76 %\n#> age40-49 years          2.1857 0.22218 1.7909  2.6676      0 %\n#> age50-59 years          5.0261 0.49785 4.1392  6.1031      1 %\n#> age60-64 years          7.3753 0.76556 6.0175  9.0394      1 %\n#> sexMale                 1.8564 0.10181 1.6672  2.0672      4 %\n#> income$30,000-$49,999   0.6451 0.04378 0.5647  0.7369      0 %\n#> income$50,000-$79,999   0.5495 0.03623 0.4829  0.6253      0 %\n#> income$80,000 or more   0.5051 0.03489 0.4411  0.5783      0 %\n#> raceWhite               1.2518 0.13228 1.0176  1.5399      2 %\n#> painmedYes              2.0443 0.33747 1.3628  3.0664     86 %\n#> htYes                   2.8273 0.15614 2.5372  3.1505      0 %\n#> copdYes                 6.8279 4.45458 1.9009 24.5255      0 %\n#> diabYes                 2.2397 0.18298 1.9083  2.6287      1 %\n#> OAOA:painmedYes         0.4925 0.16045 0.2275  1.0663     81 %\n#> age40-49 years:copdYes  0.3292 0.22863 0.0844  1.2841      0 %\n#> age50-59 years:copdYes  0.4792 0.32275 0.1280  1.7940      0 %\n#> age60-64 years:copdYes  0.3999 0.26244 0.1105  1.4473      0 %\n#> sexMale:copdYes         1.7812 0.55019 0.9723  3.2632      1 %\nsum.pooled\n\n\n\n  \n\n\n\nEstimating model performance (AUC and AL)\nIndividual AUC estimates (with interactions)\n\nlibrary(ROCR)\nlibrary(WeightedROC)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nAL.scalar <- AUC.scalar <- vector(\"list\", 5)\nfor (i in 1:5) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design <- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit <- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc <- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL <- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] <- AL\n  AUC.scalar[[i]] <- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#> AUC calculated for data 1 \n#> AUC calculated for data 2 \n#> AUC calculated for data 3 \n#> AUC calculated for data 4 \n#> AUC calculated for data 5\nstr(AUC.scalar)\n#> List of 5\n#>  $ : num 0.8\n#>  $ : num 0.795\n#>  $ : num 0.798\n#>  $ : num 0.8\n#>  $ : num 0.794\nAL.scalar\n#> [[1]]\n#> [1] 0.01243738\n#> \n#> [[2]]\n#> [1] 0.5471927\n#> \n#> [[3]]\n#> [1] 0.13081\n#> \n#> [[4]]\n#> [1] 0.644286\n#> \n#> [[5]]\n#> [1] 0.6049137\n\nAveraged estimates for AUC (with interactions)\n\n# summary of AUC\nmean(unlist(AUC.scalar))\n#> [1] 0.7973746\nsd(unlist(AUC.scalar))\n#> [1] 0.002688964\nround(range(unlist(AUC.scalar)),3)\n#> [1] 0.794 0.800\n# p-values (from AL) by majority\nsum(AL.scalar>0.05)\n#> [1] 4\n\nModel performance without interactions\nIndividual AUC estimates / AL p-values\n\nlibrary(ROCR)\nlibrary(WeightedROC)\nAL.scalar <- AUC.scalar <- vector(\"list\", 5)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab\")\nfor (i in 1:5) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design <- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit <- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc <- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL <- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] <- AL\n  AUC.scalar[[i]] <- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#> AUC calculated for data 1 \n#> AUC calculated for data 2 \n#> AUC calculated for data 3 \n#> AUC calculated for data 4 \n#> AUC calculated for data 5\nstr(AUC.scalar)\n#> List of 5\n#>  $ : num 0.798\n#>  $ : num 0.794\n#>  $ : num 0.797\n#>  $ : num 0.798\n#>  $ : num 0.794\nAL.scalar\n#> [[1]]\n#> [1] 0.01839624\n#> \n#> [[2]]\n#> [1] 0.2836256\n#> \n#> [[3]]\n#> [1] 0.02439659\n#> \n#> [[4]]\n#> [1] 0.8333214\n#> \n#> [[5]]\n#> [1] 0.2050434\n\nAveraged estimates for AUC / majority of AL p-values\n\n# summary of AUC\nmean(unlist(AUC.scalar))\n#> [1] 0.7964061\nsd(unlist(AUC.scalar))\n#> [1] 0.002002636\nround(range(unlist(AUC.scalar)),3)\n#> [1] 0.794 0.798\n# p-values (from AL) by majority\nsum(AL.scalar>0.05)\n#> [1] 3\n\nAppendix\nUser-written svyROCw3 and AL.gof3 functions\n\nsvyROCw3\n#> function(fit=fit7,outcome=analytic2$CVD==\"event\", weight = NULL,plot=FALSE){\n#>   if (is.null(weight)){ # require(ROCR)\n#>     prob <- predict(fit, type = \"response\")\n#>   pred <- prediction(as.vector(prob), outcome)\n#>   perf <- performance(pred, \"tpr\", \"fpr\")\n#>   auc <- performance(pred, measure = \"auc\")\n#>   auc <- auc@y.values[[1]]\n#>   if (plot == TRUE){\n#>     roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n#>       model = \"Logistic\")\n#>     with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n#>      xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n#>      main = paste(\"AUC = \", round(auc,3))))\n#>   mtext(\"Unweighted ROC\")\n#>   abline(0,1, lty=2)\n#>   }\n#>   } else { # library(WeightedROC)\n#>     outcome <- as.numeric(outcome)\n#>   pred <- predict(fit, type = \"response\")\n#>   tp.fp <- WeightedROC(pred, outcome, weight)\n#>   auc <- WeightedAUC(tp.fp)\n#>   if (plot == TRUE){\n#>     with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n#>      xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n#>      main = paste(\"AUC = \", round(auc,3))))\n#>   abline(0,1, lty=2)\n#>   mtext(\"Weighted ROC\")\n#>   }\n#>   }\n#>   return(auc)\n#> }\n#> <bytecode: 0x000002492d933ec0>\nAL.gof3\n#> function(fit=fit7, data = analytic2, weight = \"weight\", psu = \"psu\", \n#>                     strata= \"strata\"){\n#>   r <- residuals(fit, type=\"response\") \n#>   f<-fitted(fit) \n#>   breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n#>   breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n#>   g<- cut(f, breaks.g)\n#>   data2g <- cbind(data,r,g)\n#>   if (is.null(psu)){\n#>     newdesign <- svydesign(id=~1,\n#>                          weights=as.formula(paste0(\"~\",weight)), \n#>                         data=data2g, nest = TRUE)\n#>   }\n#>   if (!is.null(psu)) {\n#>     newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n#>                          strata=as.formula(paste0(\"~\",strata)),\n#>                          weights=as.formula(paste0(\"~\",weight)), \n#>                         data=data2g, nest = TRUE)\n#>   }\n#>   decilemodel<- svyglm(r~g, design=newdesign) \n#>   res <- regTermTest(decilemodel, ~g)\n#>   return(as.numeric(res$p)) \n#> }\n#> <bytecode: 0x000002492dae1c48>"
  },
  {
    "objectID": "missingdata5.html",
    "href": "missingdata5.html",
    "title": "Subpopulations",
    "section": "",
    "text": "This tutorial demonstrates how to manage missing data in complex surveys using multiple imputation, focusing on specific subpopulations defined by the study’s eligibility criteria.\nPurpose\nLet us we are interested in exploring the relationship between rheumatoid arthritis and cardiovascular disease (CVD) among US adults aged 20 years or more. For that, we will use NHANES 2017–2018 dataset, which follows a complex survey design.\n\n\nIn this tutorial, we used a similar approach to the one in a published article by Hossain et al. (2022), but we used less data (restricted to only 2017–2018) to speed up the analysis. Ref link.\nThe purpose of this example is to demonstrate how to do the missing data analysis with multiple imputation in the context of complex surveys.\nThe main idea is:\n\nworking with the analytic data\nimputing missing values based on that analytic dataset\nkeep count of the ineligible subjects from the full data who are not included in the analytic data\nadding those ineligible subjects back in the imputed datasets, so that we can utilize the survey features and subset the design.\n\n\n# Load required packages\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tableone)\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\nlibrary(mice)\nlibrary(mitools)\n\nLet us import the dataset:\n\nload(\"Data/missingdata/MIexample.RData\")\nls()\n#> [1] \"dat.full\"        \"has_annotations\"\n\n\ndim(dat.full)\n#> [1] 9254   15\nhead(dat.full)\n\n\n\n  \n\n\n\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nAnalytic dataset\nSubsetting according to eligibility\nLet us create an analytic dataset for\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\n# Drop < 20 years\ndat.with.miss <- subset(dat.full, age >= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#> \n#>   No  Yes <NA> \n#> 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#> \n#>   No  Yes <NA> \n#> 3857  337 1375\n\n# Drop missing in outcome and exposure \n# i.e., dataset with missing values only in covariates\ndat.analytic <- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#> [1] 4191\n\nAs we can see, we have 4,191 participants aged 20 years or more without missing values in outcome or exposure. Let us count the ineligible subjects from the full data and create an indicator variable.\n\ndat.full$ineligible <- 1\ndat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] <- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#> \n#>    0    1 <NA> \n#> 4191 5063    0\n\nWe have 4,191 eligible and 5,063 ineligible subjects based on the eligibility criteria.\nGeneral strategy of solution:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nlater we will include 5,063 ineligible subjects in the data so that we can utilize survey features.\nTable 1\nLet us see the summary statistics, i.e., create Table 1 stratified by outcome (cvd). Before that, we will categorize age and recode rheumatoid:\n\n# Categorical age\ndat.analytic$age.cat <- \n  with(dat.analytic, ifelse(age >= 20 & age < 50, \"20-49\", \n                            ifelse(age >= 50 & age < 65, \n                                   \"50-64\", \"65+\")))\ndat.analytic$age.cat <- factor(dat.analytic$age.cat, \n                               levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.analytic$age.cat, useNA = \"always\")\n#> \n#> 20-49 50-64   65+  <NA> \n#>  2280  1097   814     0\n\n# Recode rheumatoid to arthritis\ndat.analytic$arthritis <- \ncar::recode(dat.analytic$rheumatoid, \" 'No' = 'No arthritis';\n            'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.analytic$arthritis, useNA = \"always\")\n#> \n#>         No arthritis Rheumatoid arthritis                 <NA> \n#>                 3854                  337                    0\n\n# Keep only relevant variables\nvars <-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\",\n           \"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\",\n           \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 <- dat.analytic[, vars]\n\n\n# Create Table 1\nvars <- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 <- CreateTableOne(vars = vars, strata = \"cvd\", \n                       data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#>                  Stratified by cvd\n#>                   level                     Overall      No          \n#>   n                                          4191         3823       \n#>   arthritis       No arthritis               3854         3580       \n#>                   Rheumatoid arthritis        337          243       \n#>   age.cat         20-49                      2280         2240       \n#>                   50-64                      1097          979       \n#>                   65+                         814          604       \n#>   sex             Male                       2126         1884       \n#>                   Female                     2065         1939       \n#>   education       Less than high school       828          728       \n#>                   High school                2292         2094       \n#>                   College graduate or above  1063          993       \n#>   race            White                      1275         1113       \n#>                   Black                       998          898       \n#>                   Hispanic                   1015          958       \n#>                   Others                      903          854       \n#>   income          less than $20,000           659          557       \n#>                   $20,000 to $74,999         1967         1796       \n#>                   $75,000 and Over           1143         1079       \n#>   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#>   smoking         Never smoker               2570         2427       \n#>                   Previous smoker             882          726       \n#>                   Current smoker              739          670       \n#>   htn             No                         1424         1380       \n#>                   Yes                        2415         2107       \n#>   diabetes        No                         3622         3396       \n#>                   Yes                         566          424       \n#>                  Stratified by cvd\n#>                   Yes         \n#>   n                 368       \n#>   arthritis         274       \n#>                      94       \n#>   age.cat            40       \n#>                     118       \n#>                     210       \n#>   sex               242       \n#>                     126       \n#>   education         100       \n#>                     198       \n#>                      70       \n#>   race              162       \n#>                     100       \n#>                      57       \n#>                      49       \n#>   income            102       \n#>                     171       \n#>                      64       \n#>   bmi (mean (SD)) 30.09 (7.29)\n#>   smoking           143       \n#>                     156       \n#>                      69       \n#>   htn                44       \n#>                     308       \n#>   diabetes          226       \n#>                     142\n\nCheck missingness using a plot\nNow we will see the percentage of missing values in the variables.\n\nDataExplorer::plot_missing(dat.analytic2)\n\n\n\n\nWe have about 10% missing values in income, followed by hypertension (8.4%), bmi (6.8%), education (0.2%), and diabetes (0.1%).\nDealing with missing values in covariates\n\nNow we will perform multiple imputation to deal with missing values only in covariates. We will use the dat.analytic2 dataset that contains missing values in the covariates but no missing values in the outcome or exposure.\nFor this exercise, we will consider 5 imputed datasets, 3 iterations, and the predictive mean matching method for bmi and income.\n\nWe have already set up the data such that the variables are of appropriate types, e.g., numeric bmi, factor age, sex, and so on.\nWe will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nNow we will set up the initial model and set up the methods and predictor matrix before imputing 5 datasets.\n\n\n\nStep 0: Set up the imputation model\n\n# Step 0: Set imputation model\nini <- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred <- ini$pred\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",] <- 0\n\n# Do not use survey weight or PSU variable as auxiliary variables\npred[,\"studyid\"] <- pred[\"studyid\",] <- 0\npred[,\"psu\"] <- pred[\"psu\",] <- 0\npred[,\"survey.weight\"] <- pred[\"survey.weight\",] <- 0\n\n# Set imputation method\nmeth <- ini$meth\nmeth[\"bmi\"] <- \"pmm\"\nmeth[\"income\"] <- \"pmm\"\nmeth\n#>       studyid survey.weight           psu        strata           cvd \n#>            \"\"            \"\"            \"\"            \"\"            \"\" \n#>     arthritis       age.cat           sex     education          race \n#>            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#>        income           bmi       smoking           htn      diabetes \n#>         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\n\nThere is no missing for studyid, survey.weight, psu, strata, cvd, arthritis, age, sex, race, smoking. Hence, no method is assigned for these variables.\nFor education, polyreg (Polytomous logistic regression) will be used.\nSimilarly, we will use pmm (Predictive mean matching) for bmi, income and used logreg (Logistic regression) for htn, diabetes. See the Imputation chapter to see how the PMM method works.\nStep 1: Imputing missing values using mice\n1.1 Imputing dataset for eligible subjects\n\n# Step 1: impute the incomplete data\nimputation <- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nimpdata <- mice::complete(imputation, action=\"long\")\n\ntable(impdata$age.cat)\n#> \n#> 20-49 50-64   65+ \n#> 11400  5485  4070\n\nNote that age has no missing values, and everyone is above 20.\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id <- NULL\n\n# Create an indicator of eligible subjects \nimpdata$ineligible <- 0\n\n# Number of subjects\nnrow(impdata)\n#> [1] 20955\n\nLet’s see whether there is any missing value after imputation:\n\nDataExplorer::plot_missing(impdata)\n\n\n\n\n\nThere is no missing value after imputation.\nAs we can see, there is an additional variable (.imp) in the imputed dataset. This .imp goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\n1.2 Preparing dataset for ineligible subjects\nThe next task is adding the ineligible subjects in the imputed datasets, so that we can set up the survey design on the full dataset and then subset the design.\n\n# Number of ineligible subjects\n#dat.full$ineligible <- 1\n#dat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] <- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#> \n#>    0    1 <NA> \n#> 4191 5063    0\n\nNow we will subset the data for ineligible subjects and create m = 5 copies.\n\n# Subset for ineligible\ndat.ineligible <- subset(dat.full, ineligible == 1)\n\n# Create m = 5 datasets with .imp 1 to m = 5\ndat31 <- dat.ineligible; dat31$.imp <- 1\ndat32 <- dat.ineligible; dat32$.imp <- 2\ndat33 <- dat.ineligible; dat33$.imp <- 3\ndat34 <- dat.ineligible; dat34$.imp <- 4\ndat35 <- dat.ineligible; dat35$.imp <- 5\n\nThe next step is combining ineligible datasets. Before merging the stacked dataset for ineligible subjects to the imputed stacked dataset for eligible subjects, we must ensure the variable names are the same.\n\n# Stacked data for ineligible subjects\ndat.ineligible.stacked <- rbind(dat31, dat32, dat33, dat34, dat35)\n\nWe should have missing value in this ineligible part of the data:\n\nDataExplorer::plot_missing(dat.ineligible.stacked)\n\n\n\n\n1.3 Combining eligible (imputed) and ineligible (unimputed) subjects\n\nnames(impdata)\n#>  [1] \".imp\"          \"studyid\"       \"survey.weight\" \"psu\"          \n#>  [5] \"strata\"        \"cvd\"           \"arthritis\"     \"age.cat\"      \n#>  [9] \"sex\"           \"education\"     \"race\"          \"income\"       \n#> [13] \"bmi\"           \"smoking\"       \"htn\"           \"diabetes\"     \n#> [17] \"ineligible\"\n\n\nnames(dat.ineligible.stacked)\n#>  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#>  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#>  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#> [13] \"smoking\"       \"htn\"           \"diabetes\"      \"ineligible\"   \n#> [17] \".imp\"\n\nAs we can see, the variable names are different in the two datasets. Particularly, arthritis and age.cat variables are not available in the dat.ineligible.stacked dataset. Now we will recode these variables in the same format as done for impdata:\n\n# Categorical age\nsummary(dat.ineligible.stacked$age)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00    5.00   12.00   23.25   41.00   80.00\ndat.ineligible.stacked$age.cat <- \n  with(dat.ineligible.stacked, \n       ifelse(age >= 20 & age < 50, \"20-49\", \n              ifelse(age >= 50 & age < 65, \"50-64\", \n                     ifelse(age >= 65, \"65+\", NA))))\ndat.ineligible.stacked$age.cat <- \n  factor(dat.ineligible.stacked$age.cat, \n         levels = c(\"20-49\", \"50-64\", \"65+\"))\n\nNote that, we are assigning anyone with less than 20 age as missing.\n\ntable(dat.ineligible.stacked$age.cat, useNA = \"always\")\n#> \n#> 20-49 50-64   65+  <NA> \n#>  1100  2360  3430 18425\n# Recode arthritis\ndat.ineligible.stacked$arthritis <- \n  car::recode(dat.ineligible.stacked$rheumatoid, \n  \" 'No' = 'No arthritis'; 'Yes' = 'Rheumatoid arthritis' \", \n  as.factor = T)\n\nNote: In the above step, we could also create two variables with missing values rather than recoding. The reason is that we will subset the design; no matter whether we recode or create missing values for ineligible, the only information we need from ineligible subjects is their survey features when creating the design.\nThe next step is to combine these two datasets (impdata and dat.ineligible.stacked).\n\n# Variable names in the imputed dataset\nvars <- names(impdata) \n\n# Set up the dataset for ineligible - same variables as impdata\ndat.ineligible.stacked <- dat.ineligible.stacked[, vars]\n\nNow we will merge ineligible and eligible subjects to make the full dataset of 5 \\(\\times\\) 9,254 = 46,270 subjects.\n\nimpdata2 <- rbind(impdata, dat.ineligible.stacked)\nimpdata2 <- impdata2[order(impdata2$.imp, impdata2$studyid),]\ndim(impdata2)\n#> [1] 46270    17\n\n1.4 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on full dataset [with eligible (imputed) and ineligible (unimputed) subjects] of 5 \\(\\times\\) 9,254 = 46,270 subjects and subset the design for 5 \\(\\times\\) 4,716 = 23,580 subjects.\n\nm <- 5\nallImputations <- imputationList(lapply(1:m, function(n) \n  subset(impdata2, subset=.imp==n)))\n\n# Step 2: Survey data analysis\nw.design0 <- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) # Design on full data\nw.design <- subset(w.design0, ineligible == 0) # Subset the design\n\nWe can see the length of the subsetted design:\n\ndim(w.design)\n#> [1] 4191   17    5\n\nThe subsetted design contains 4,191 subjects with 17 variables and 5 imputed datasets. Now we will run the design-adjusted logistic regression on and pool the estimate using Rubin’s rule:\nStep 2: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit <- with(w.design, \n            svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + \n                     sex + education + race + income + bmi + \n                     smoking + htn + diabetes, \n                   family = quasibinomial))\nres <- exp(as.data.frame(cbind(coef(fit[[1]]),\n      coef(fit[[2]]),\n      coef(fit[[3]]),\n      coef(fit[[4]]),\n      coef(fit[[5]]))))\nnames(res) <- paste(\"OR from m =\", 1:5)\nround(t(res),2)\n#>               (Intercept) arthritisRheumatoid arthritis age.cat50-64 age.cat65+\n#> OR from m = 1        0.02                          2.03         4.71      13.08\n#> OR from m = 2        0.02                          2.04         4.74      13.21\n#> OR from m = 3        0.02                          2.12         4.58      11.88\n#> OR from m = 4        0.02                          2.10         4.59      12.45\n#> OR from m = 5        0.02                          2.07         4.47      12.25\n#>               sexFemale educationHigh school educationCollege graduate or above\n#> OR from m = 1      0.46                 0.75                               0.75\n#> OR from m = 2      0.45                 0.76                               0.79\n#> OR from m = 3      0.47                 0.71                               0.70\n#> OR from m = 4      0.46                 0.70                               0.71\n#> OR from m = 5      0.45                 0.75                               0.77\n#>               raceBlack raceHispanic raceOthers income$20,000 to $74,999\n#> OR from m = 1      0.96         0.54       0.86                     0.48\n#> OR from m = 2      0.97         0.54       0.88                     0.48\n#> OR from m = 3      0.98         0.54       0.85                     0.54\n#> OR from m = 4      0.96         0.53       0.86                     0.51\n#> OR from m = 5      0.99         0.55       0.87                     0.53\n#>               income$75,000 and Over  bmi smokingPrevious smoker\n#> OR from m = 1                   0.33 1.02                   1.55\n#> OR from m = 2                   0.32 1.03                   1.55\n#> OR from m = 3                   0.36 1.01                   1.59\n#> OR from m = 4                   0.34 1.02                   1.56\n#> OR from m = 5                   0.34 1.02                   1.55\n#>               smokingCurrent smoker htnYes diabetesYes\n#> OR from m = 1                  1.43   1.35        3.01\n#> OR from m = 2                  1.47   1.30        3.01\n#> OR from m = 3                  1.45   1.48        3.13\n#> OR from m = 4                  1.44   1.46        3.03\n#> OR from m = 5                  1.45   1.47        3.05\n\nStep 3: Pooling estimates\n\n# Step 3: Pooled estimates\npooled.estimates <- MIcombine(fit)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nConclusion\nAmong US adults aged 20 years or more, the odds of CVD was approximately twice among those with rheumatoid arthritis than no arthritis.\nReferences\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30."
  },
  {
    "objectID": "missingdata6.html",
    "href": "missingdata6.html",
    "title": "MCAR tests",
    "section": "",
    "text": "MCAR tests are essential tools in the data analysis process. They help researchers understand the nature of missingness in their datasets, guide appropriate imputation strategies, and ensure the validity and reliability of statistical analyses.\nMCAR data\nIn the initial chunk, several packages are loaded. These packages provide functions and tools necessary for the subsequent analysis, including multiple imputation, statistical modeling, and data visualization.\n\n# Load required packages\nrequire(mice)\nrequire(mitools)\nrequire(survey)\nrequire(remotes)\nrequire(simcausal)\n\nData generating process\nA Directed Acyclic Graph (DAG) is defined, which represents a causal model of how different variables in the dataset relate to each other. This DAG is used to simulate data based on the relationships defined. We generate L as a function of P and B.\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"B\", distr = \"rnorm\", mean = 0, sd = 1) +\n  node(\"P\", distr = \"rnorm\", mean = 0, sd = .7) +\n  node(\"L\", distr = \"rnorm\", mean = 2 + 2 * P + 3 * B, sd = 3) + \n  node(\"A\", distr = \"rnorm\", mean = 0.5 + L + 2 * P, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A + B + 2 * P, sd = .5)\nDset <- set.DAG(D)\n\nGenerate DAG\nThe previously defined DAG is visualized, providing a graphical representation of the relationships between variables.\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\nUsing the DAG, a dataset is simulated. This dataset will be used for subsequent analysis.\n\nObs.Data <- sim(DAG = Dset, n = 10000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\nObs.Data.original <- Obs.Data\n\nRandomly set some data to missing\nSome values in the dataset are randomly set to missing (i.e., randomly assign some L values to missing). This simulates a scenario where data might be missing completely at random (MCAR).\n\nset.seed(123)\nObs.Data$L[sample(1:length(Obs.Data$L), size = 1000)] <- NA\nsummary(Obs.Data$L)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#> -17.116  -1.096   1.896   1.968   4.996  20.129    1000\n\nThe missing data patterns in the dataset are visualized. This helps in understanding which variables have missing values and how they might be related.\n\nrequire(VIM)\nres <- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\nVisualize via margin plots\nMargin plots are used to compare the distributions of variables when a particular variable is missing versus when it is observed. This provides insights into how missingness might be related to the values of other variables.\n\nThe red boxplot depicts the distribution of a variable in the data where L has a missing value.\nThe blue boxplot depicts the distribution of the values of a variable in the data where L has an observed value.\n\nSame median and spread (range) may mean no difference in the distribution.\n\n\nmarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\nLittle’s MCAR test\nA statistical test is conducted to determine if data is missing entirely at random (MCAR). The outcome of this test offers a deeper understanding of the reasons behind the missing data.\nLittle’s 1988 chi-squared test evaluates if data is MCAR by checking for significant differences in the means of various missing-value patterns (Little 1988). Based on the test’s statistic and p-value, we can infer if the data is MCAR. The null hypothesis for this test is that the data is MCAR.\n\nThe essence of this test is to compare means across groups with different missing data patterns. It uses a likelihood ratio test and assumes the data follows a multivariate normal distribution.\nIf this test is rejected (indicated by a low p-value or a high statistic), it suggests the data might not be MCAR.\n\nHowever, this test has several limitations (rdrr 2023):\n\nThe test doesn’t pinpoint which specific variables don’t adhere to MCAR, meaning it doesn’t highlight potential variables associated with missingness.\nIt assumes multivariate normality. If this assumption isn’t met, especially with non-normal or categorical variables, the test might not be reliable unless there’s a large sample size.\nThe test assumes that all missing data patterns have the same covariance matrix. This means it can’t detect deviations from MCAR that are based on covariance, especially if the data is Missing at Random (MAR) or Missing Not at Random (MNAR).\nResearch has shown that Little’s MCAR test might have low power, especially when few variables don’t follow MCAR, the association between the data and its missingness is weak, or if the data is MNAR.\nThe test can only reject the MCAR assumption but can’t confirm it. A non-significant result doesn’t necessarily confirm that the data is MCAR.\nEven if the test result is significant, it doesn’t rule out the possibility of the data being MNAR.\n\n\nrequire(naniar)\nmcar_test(Obs.Data)\n\n\n\n  \n\n\n\nrequire(misty)\nna.test(Obs.Data)\n#>  Jamshidian and Jalal's Approach for Testing MCAR\n#> \n#>   Imputation:         Non-Parametric \n#>   Nr. of Imputations: 20 \n#>   Pooling:            Median \n#> \n#>   Hawkins Test\n#> \n#>    H0: Multivariate Normality and Homogeneity of Covariances (MCAR)\n#>    H1: Multivariate Non-Normality or Heterogeneity of Covariances (MCAR Violation)\n#> \n#>         n nIncomp nPattern  chi2 df  pval \n#>     10000    1000        2 75.37  4 0.000\n#> \n#>   Anderson-Darling Non-Parametric Test\n#> \n#>    H0: Multivariate Non-Normality, but Homogeneity of Covariances (MCAR)\n#>    H1: Heterogeneity of Covariances (MCAR Violation)\n#> \n#>         n nIncomp nPattern    T pval \n#>     10000    1000        2 0.61   NA\n\nMCAR and normality test\nHawkins, in 1981, introduced a method to assess both multivariate normality and the consistency in variances, known as homoscedasticity. This method not only checks for consistent variances but also for mean equality (Hawkins 1981).\nIn 2010, Jamshidian and Jalal suggested a technique to compare covariances among groups with the same missing data patterns. They utilized the Hawkins test for data assumed to be normal and a non-parametric approach for other data types (Jamshidian and Jalal 2010).\nThe following package (Jamshidian and Jalal 2010) tests multivariate normality and homoscedasticity in the context of missing data.\n\n#library(devtools)\n#install_github(\"cran/MissMech\")\nlibrary(MissMech)\ntest.result <- TestMCARNormality(data = Obs.Data)\ntest.result\n#> Call:\n#> TestMCARNormality(data = Obs.Data)\n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1   NA   1   1              1000\n#> group.2    1   1   1    1   1   1              9000\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#> \n#>     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#>     Provided that normality can be assumed, the hypothesis of MCAR is \n#>     rejected at 0.05 significance level. \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0.4019219 \n#> \n#>     Reject Normality at 0.05 significance level.\n#>     There is not sufficient evidence to reject MCAR at 0.05 significance level.\nsummary(test.result)\n#> \n#> Number of imputation:  1 \n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1   NA   1   1              1000\n#> group.2    1   1   1    1   1   1              9000\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0.4019219\n\n\npng(\"E:/GitHub/EpiMethods/Images/missingdata/boxplot.png\", width = 600, height = 600)\nboxplot(test.result)\ndev.off()\n\n\nboxplot(test.result)\n\n\n\n\n\n\nNon-MCAR data\nSet some data to missing based on a rule\nIn this section, the original dataset is restored. Then, for a specific column, any value greater than a certain threshold is set to ‘missing’. A summary of this column is then provided to understand the distribution of missing values.\n\nObs.Data <- Obs.Data.original\nObs.Data$L[Obs.Data$L > 7.79] <- NA\nsummary(Obs.Data$L)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#> -17.116  -1.482   1.330   1.050   3.954   7.787    1035\n\nThe dataset’s missing data patterns are visualized. This visualization helps in understanding which data points are missing and their distribution across the dataset.\n\nres <- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\nVisualize via margin plots\nMargin plots are used to visualize the relationship between two variables, especially when one of them has missing values. Here, the relationship of a specific column with missing values is visualized against other columns in the dataset. This helps in understanding how the missingness in one variable might relate to other variables.\n\nmarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\nLittle’s MCAR test\nLittle’s MCAR test is applied to the dataset to check if the data is missing completely at random. This test provides a statistic and a p-value to determine the nature of missingness in the data.\n\nmcar_test(Obs.Data)\n\n\n\n  \n\n\nna.test(Obs.Data)\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n\n#> Warning in (n - 1L) * (n - 2L) * (n - 3L): NAs produced by integer overflow\n#>  Jamshidian and Jalal's Approach for Testing MCAR\n#> \n#>   Imputation:         Non-Parametric \n#>   Nr. of Imputations: 20 \n#>   Pooling:            Median \n#> \n#>   Hawkins Test\n#> \n#>    H0: Multivariate Normality and Homogeneity of Covariances (MCAR)\n#>    H1: Multivariate Non-Normality or Heterogeneity of Covariances (MCAR Violation)\n#> \n#>         n nIncomp nPattern   chi2 df  pval \n#>     10000    1035        2 145.93  4 0.000\n#> \n#>   Anderson-Darling Non-Parametric Test\n#> \n#>    H0: Multivariate Non-Normality, but Homogeneity of Covariances (MCAR)\n#>    H1: Heterogeneity of Covariances (MCAR Violation)\n#> \n#>         n nIncomp nPattern     T pval \n#>     10000    1035        2 42.89   NA\n\nMCAR and normality test\nA test is conducted to check if the data follows a multivariate normal distribution and if the variances across different groups are consistent (homoscedasticity). The results of this test, along with a summary and a boxplot visualization, are provided to understand the distribution and characteristics of the data.\n\ntest.result <- TestMCARNormality(data = Obs.Data)\ntest.result\n#> Call:\n#> TestMCARNormality(data = Obs.Data)\n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1    1   1   1              8965\n#> group.2    1   1   1   NA   1   1              1035\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#> \n#>     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#>     Provided that normality can be assumed, the hypothesis of MCAR is \n#>     rejected at 0.05 significance level. \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0 \n#> \n#>     Hypothesis of MCAR is rejected at  0.05 significance level.\n#>     The multivariate normality test is inconclusive.\nsummary(test.result)\n#> \n#> Number of imputation:  1 \n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1    1   1   1              8965\n#> group.2    1   1   1   NA   1   1              1035\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0\n\n\npng(\"E:/GitHub/EpiMethods/Images/missingdata/boxplot1.png\", width = 600, height = 600)\nboxplot(test.result)\ndev.off()\n\n\nboxplot(test.result)\n\n\n\n\n\n\n\n\n\n\nHawkins, Douglas M. 1981. “A New Test for Multivariate Normality and Homoscedasticity.” Technometrics 23 (1): 105–10.\n\n\nJamshidian, Mortaza, and Siavash Jalal. 2010. “Tests of Homoscedasticity, Normality, and Missing Completely at Random for Incomplete Multivariate Data.” Psychometrika 75 (4): 649–74.\n\n\nLittle, Roderick JA. 1988. “A Test of Missing Completely at Random for Multivariate Data with Missing Values.” Journal of the American Statistical Association 83 (404): 1198–1202.\n\n\nrdrr. 2023. “Na.test: Little’s Missing Completely at Random (MCAR) Test.” https://rdrr.io/cran/misty/man/na.test.html."
  },
  {
    "objectID": "missingdata7.html",
    "href": "missingdata7.html",
    "title": "Effect modification",
    "section": "",
    "text": "In this tutorial, we delve into the concept of effect modification.\n\n\n\n\n\n\nImportant\n\n\n\nWe discussed about effect modification in an earlier discussion.\n\n\nWe start by loading the necessary packages that will aid in the analysis. We also define a function tidy.pool_mi to streamline the pooling process for multiple imputation results.\n\n# Load required packages\nlibrary(survey)\nrequire(interactions)\nrequire(mitools)\nrequire(mice)\nrequire(miceadds)\nlibrary(modelsummary)\n\ntidy.pool_mi <- function(x, ...) {\n  msg <- capture.output(out <- summary(x, ...))\n  out$term <- row.names(out)\n  colnames(out) <- c(\"estimate\", \"std.error\", \"statistic\", \"p.value\",\n                     \"conf.low\", \"conf.high\", \"miss\", \"term\")\n  return(out)\n}\n\nData\nWe load a dataset named smi. This dataset is a list of multiple imputed datasets, which is evident from the structure and the way we access its elements.\n\nrequire(mitools)\ndata(smi)\nlength(smi)\n#> [1] 2\nlength(smi[[1]])\n#> [1] 5\nhead(smi[[1]][[1]])\n#>       id wave mmetro parsmk         drkfre         alcdos alcdhi           smk\n#> 1 920006    1      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 2 920006    2      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 3 920006    3      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 4 920006    4      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 5 920006    5      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 6 920006    6      1      0 not in last wk not in last wk      0 non/ex-smoker\n#>   cistot mdrkfre sex drinkreg\n#> 1      0       0   1    FALSE\n#> 2      0       0   1    FALSE\n#> 3      0       0   1    FALSE\n#> 4      1       0   1    FALSE\n#> 5      4       0   1    FALSE\n#> 6      3       0   1    FALSE\n\nModel with interaction and ORs\nWe’re interested in understanding how the variable wave interacts with sex in predicting drinkreg. We fit two logistic regression models, one for each level of the sex variable (0 males, 1 females), to understand this interaction. wave is the exposure variable here.\nFor effect modifier sex = 0 (males)\n\nmodels <- with(smi, glm(drinkreg~ wave + sex + wave*sex, family = binomial()))\nsummary(pool(models, rule = \"rubin1987\"), conf.int = TRUE, exponentiate = TRUE)[2,]\n#>   term estimate  std.error statistic       df      p.value   2.5 %   97.5 %\n#> 2 wave 1.271952 0.06587423  3.651693 234.4974 0.0003212903 1.11714 1.448217\n\nFor effect modifier sex = 1 (females) (just changing reference)\n\nmodels2<-with(smi, glm(drinkreg~ wave + I(sex==0) + wave*I(sex==0),family=binomial()))\nsummary(pool(models2, rule = \"rubin1987\"),conf.int = TRUE, exponentiate = TRUE)[2,]\n#>   term estimate  std.error statistic       df      p.value    2.5 %   97.5 %\n#> 2 wave 1.225438 0.05876369   3.45959 240.3053 0.0006399352 1.091487 1.375828\n\n\nNotice the ORs for wave in the above 2 analyses. These are basically our target.\nFor proper survey data analysis, you will have to work with design and make sure you subset your subpopulation (those eligible) appropriately.\nSimple slopes analyses\nWe perform a simple slopes analysis for each imputed dataset. This analysis helps in understanding the relationship between the predictor and the outcome at specific levels of the moderator.\n\na1 <- sim_slopes(models[[1]], pred = wave, modx = sex)\na2 <- sim_slopes(models[[2]], pred = wave, modx = sex)\na3 <- sim_slopes(models[[3]], pred = wave, modx = sex)\na4 <- sim_slopes(models[[4]], pred = wave, modx = sex)\na5 <- sim_slopes(models[[5]], pred = wave, modx = sex)\n\nAfter obtaining the results from each imputed dataset, we pool them to get a consolidated result. This is done separately for each level of the sex variable.\nPooled results for sex = 0\n\n# For sex = 0\nef.lev <- 1\nest <- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse <- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr <- se^2\nOR <- exp(est)\nOR.se <- OR * se\nOR.v <- OR.se^2\n\nmod_pooled <- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n#>   estimate  std.error statistic      p.value conf.low conf.high   miss term\n#> 1 1.272164 0.08386552  15.16909 6.987679e-39 1.107118   1.43721 12.2 %    1\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#> Multiple imputation results:\n#>       MIcombine.default(as.list(OR), as.list(OR.v))\n#>    results         se   (lower  upper) missInfo\n#> 1 1.272164 0.08386552 1.107118 1.43721     12 %\n\nPooled results for sex = 1\n\n# For sex = 1\nef.lev <- 2\nest <- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse <- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr <- se^2\nOR <- exp(est)\nOR.se <- OR * se\nOR.v <- OR.se^2\n\nmod_pooled <- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n#>   estimate  std.error statistic      p.value conf.low conf.high   miss term\n#> 1 1.225598 0.07204679  17.01113 2.282427e-46 1.083838  1.367357 11.9 %    1\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#> Multiple imputation results:\n#>       MIcombine.default(as.list(OR), as.list(OR.v))\n#>    results         se   (lower   upper) missInfo\n#> 1 1.225598 0.07204679 1.083838 1.367357     12 %"
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R functions (M)",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggr \n    VIM \n    To calculate/plot the missing values in the variables \n  \n\n boxplot \n    base/graphics \n    To produce a box plot \n  \n\n bwplot \n    mice \n    To produce box plot to compare the imputed and observed values \n  \n\n colMeans \n    base \n    To compute the column-wise mean, i.e., mean for each variable/column \n  \n\n complete \n    mice \n    To extract the imputed dataset \n  \n\n complete.cases \n    base/stats \n    To select the complete cases, i.e., observations without missing values \n  \n\n D1 \n    mice \n    To conduct the multivariate Wald test with D1-statistic \n  \n\n densityplot \n    mice \n    To produce desnsity plots \n  \n\n expression \n    base \n    To set/create an expression \n  \n\n imputationList \n    mice \n    To combine multiple imputed datasets \n  \n\n marginplot \n    VIM \n    To draw a scatterplot with additional information when there are missing values \n  \n\n mcar_test \n    naniar \n    To conduct Little's MCAR test \n  \n\n md.pattern \n    mice \n    To see the pattern of the missing data \n  \n\n mice \n    mice \n    To impute missing data where the argument m represents the number of multiple imputation \n  \n\n MIcombine \n    mitools \n    To combine/pool the results using Rubin's rule \n  \n\n MIextract \n    mitools \n    To extract parameters from a list of outputs \n  \n\n na.test \n    misty \n    To conduct Little's MCAR test \n  \n\n parlmice \n    mice \n    To run `mice` function in parallel, i.e., parallel computing of mice \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n pool \n    mice \n    To pool the results using Rubin's rule \n  \n\n pool.compare \n    mice \n    To compare two nested models \n  \n\n pool_mi \n    miceadds \n    To combine/pool the results using Rubin's rule \n  \n\n quickpred \n    mice \n    To set imputation model based on the correlation \n  \n\n sim_slopes \n    interactions \n    To perform simple slope analyses \n  \n\n TestMCARNormality \n    MissMech \n    To test multivariate normality and homoscedasticity in the context of missing data \n  \n\n unlist \n    base \n    To convert a list to a vector"
  },
  {
    "objectID": "missingdataQ.html#live-quiz",
    "href": "missingdataQ.html#live-quiz",
    "title": "Quiz (M)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "missingdataQ.html#download-quiz",
    "href": "missingdataQ.html#download-quiz",
    "title": "Quiz (M)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "missingdataS.html",
    "href": "missingdataS.html",
    "title": "App (M)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from various imputations as well as pooled results from multiple imputation.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveM\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, and ggplot2 packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "missingdataE.html#problem-statement",
    "href": "missingdataE.html#problem-statement",
    "title": "Exercise (M)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n\nid: Respondent sequence number\nsurvey.weight: Full sample 4 year interview weight\npsu: Masked pseudo PSU\nstrata: Masked pseudo strata (strata is nested within PSU)\n\n4 Outcome variables\n\nweight.loss.behavior: doing lifestyle behavior changes - controlling or losing weight\nexercise.behavior: doing lifestyle behavior changes - increasing exercise\nsalt.behavior: doing lifestyle behavior changes - reducing salt in diet\nfat.behavior: doing lifestyle behavior changes - reducing fat in diet\n\n4 predictors (i.e., exposure variables)\n\nweight.loss.advice: told by a doctor or health professional - to control/lose weight\nexercise.advice: told by a doctor or health professional - to exercise\nsalt.advice: told by a doctor or health professional - to reduce salt in diet\nfat.advice: told by a doctor or health professional - to reduce fat/calories\n\nConfounders and other variables\n\ngender: Gender\nage: Age in years at screening\nincome: The ratio of family income to federal poverty level\nrace: Race/Ethnicity\nbmi: Body Mass Index in kg/m\\(^2\\)\n\ncomorbidity: Comorbidity index\nDIQ010: Self-report to have been informed by a provider to have diabetes\nBPQ020: Self-report to have been informed by a provider to have hypertension"
  },
  {
    "objectID": "missingdataE.html#question-1-analytic-dataset",
    "href": "missingdataE.html#question-1-analytic-dataset",
    "title": "Exercise (M)",
    "section": "Question 1: Analytic dataset",
    "text": "Question 1: Analytic dataset\n1(a) Importing dataset\n\n# download the data in the same folder\nload(\"Data/missingdata/Williams2021.RData\")\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\n# Drop < 18 years\ndat <- dat.full\ndat <- dat[dat$age >= 18,] \n\n# Eligibility\ndat <- dat[dat$DIQ010==\"Yes\" | dat$BPQ020==\"Yes\",] \n\n# Dataset with missing values in outcomes, predictors, and confounders\ndat.with.miss <- dat\nnrow(dat.with.miss) # N = 4,746\n#> [1] 4746\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the “Self-reported behavior change and receipt of advice” paragraph.\n\n\ndat <- dat.with.miss\n\n# Drop missing or don't know outcomes \ndat <- dat[complete.cases(dat$weight.loss.behavior),]\ndat <- dat[complete.cases(dat$exercise.behavior),]\ndat <- dat[complete.cases(dat$salt.behavior),]\ndat <- dat[complete.cases(dat$fat.behavior),]\n\n# Drop missing or don't know predictors\ndat <- dat[complete.cases(dat$weight.loss.advice),]\ndat <- dat[complete.cases(dat$exercise.advice),]\ndat <- dat[complete.cases(dat$salt.advice),]\ndat <- dat[complete.cases(dat$fat.advice),] \n\n# Dataset without missing in outcomes and predictors but missing in confounders \ndat.with.miss2 <- dat\nnrow(dat.with.miss2) # N = 4,716\n#> [1] 4716\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nHint 2: You may need to generate the Condition variable.\nHint 3: age and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\ndat <- dat.with.miss2\n\n# Create the condition variable\ndat$condition <- NA\ndat$condition[dat$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat$condition[dat$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat$condition[dat$BPQ020 == \"Yes\" & dat$DIQ010 == \"Yes\"] <- \"Both\"\ndat$condition <- factor(dat$condition, levels=c(\"Hypertension Only\", \"Diabetes Only\",\n                                                \"Both\"))\ntable(dat$condition, useNA = \"always\")\n#> \n#> Hypertension Only     Diabetes Only              Both              <NA> \n#>              3004               533              1179                 0\n\n\n# First column of Table 1\nvars <- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 <- CreateTableOne(vars = vars, data = dat, includeNA = F)\nprint(tab1, format = \"f\")\n#>                          \n#>                           Overall      \n#>   n                        4716        \n#>   gender = Male            2332        \n#>   age (mean (SD))         59.94 (14.96)\n#>   income                               \n#>      <100%                  881        \n#>      100-199%              1193        \n#>      200-299%               672        \n#>      300-399%               424        \n#>      400+%                  930        \n#>   race                                 \n#>      Hispanic              1161        \n#>      Non-Hispanic white    1630        \n#>      Non-Hispanic black    1239        \n#>      Others                 686        \n#>   bmi                                  \n#>      Reference              753        \n#>      Overweight            1372        \n#>      Obese                 2287        \n#>   condition                            \n#>      Hypertension Only     3004        \n#>      Diabetes Only          533        \n#>      Both                  1179        \n#>   comorbidity (mean (SD))  1.29 (1.45)"
  },
  {
    "objectID": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "href": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "title": "Exercise (M)",
    "section": "Question 2: Dealing with missing values in confoudners [100% grade]",
    "text": "Question 2: Dealing with missing values in confoudners [100% grade]\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nHint 1: There are four outcome variables and four predictor variables used in the study.\nHint 2: The authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\n# Create the condition variable in the analytic dataset\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat.with.miss2$condition[dat.with.miss2$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\" & \n                          dat.with.miss2$DIQ010 == \"Yes\"] <- \"Both\"\ndat.with.miss2$condition <- factor(dat.with.miss2$condition, \n                                  levels=c(\"Hypertension Only\", \"Diabetes Only\", \"Both\"))\n\n# Variables of interest\nvars <- c(\n  # Outcome\n  \"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\",\n  \n  #Predictors\n  \"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\",\n   \n  # Confounders       \n  \"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\n\n# Plot missing values using DataExplorer\nplot_missing(dat.with.miss2[,vars])\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nPerform multiple imputations to deal with missing values only in confounders. Use the dataset created in Dataset with missing values only in confounders (dat.with.miss2). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to lose weights, i.e., create only the first column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types. lapply function could be helpful.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 5: Set your seed to 123.\nHint 6: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 7: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 8: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Setup the data such that the variables are of appropriate types\nfactor.names <- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \n                  \"fat.behavior\", \"weight.loss.advice\", \"exercise.advice\", \n                  \"salt.advice\", \"fat.advice\", \"gender\", \"income\", \"race\", \"bmi\", \n                  \"condition\")\n# your codes \n\n\n## Change the reference categories\n# your codes\n\n\n## Imputation model set up\n# your codes\n\n\n## Regression analysis\n# your codes\n\n\n## Pooled estimates\n# your codes"
  },
  {
    "objectID": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "href": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "title": "Exercise (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Include all 4 outcomes and 4 predictors in your imputation model.\nHint 5: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 6: Set your seed to 123.\nHint 7: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 8: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 9: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Create a missing indicator so that MID can be applied\n# your codes here\n\n## MID\n# your codes here"
  },
  {
    "objectID": "propensityscore.html#background",
    "href": "propensityscore.html#background",
    "title": "Propensity score",
    "section": "Background",
    "text": "Background\nThis chapter provides a comprehensive set of tutorials that guide readers through various methodologies of Propensity Score Matching (PSM) and Multiple Imputation (MI) using R, with practical applications using datasets like the Canadian Community Health Survey (CCHS) and the National Health and Nutrition Examination Survey (NHANES). The tutorials explore different scenarios and methodologies in handling and analyzing data, particularly focusing on estimating treatment effects and managing missing data. They delve into specific examples, such as exploring the relationship between Osteoarthritis (OA) and Cardiovascular Disease (CVD), and between Body Mass Index (BMI) and diabetes, while emphasizing the importance of accurate data handling, variable management, and robust analysis through PSM and MI. The tutorials are meticulously structured, providing step-by-step guides, code snippets, and thorough explanations, ensuring that readers can comprehend and replicate the processes in their research, thereby enhancing the reliability and robustness of their analyses, especially in the presence of missing data.\n\n\nStepping into this chapter, we are diving deeper into the world of survey data analysis, exploring how to combine propensity score matching (PSM) and strategies for handling missing data. PSM helps us balance our data, making sure our study groups are comparable, while managing missing data ensures our results are as accurate as possible. In the upcoming tutorials, we will weave through the steps of using PSM while also dealing with the gaps in our data, ensuring our analyses are solid and dependable. So, this chapter is not just a next step, but a leap into a more advanced exploration, blending matching methods with careful data handling strategies.\n\n\n\n\n\n\nNote\n\n\n\nShould you find yourself seeking a refresher on PSM, we invite you to revisit our dedicated/external tutorial, which elucidates PSM within a non-survey data analysis context. This resource not only provides a foundational understanding but also serves as a comprehensive guide through the nuanced steps of PSM. Additionally, our external discussion page offers a succinct summary of the tutorial and thoughtfully extends the conversation into more intricate directions, exploring the complexities and advanced applications of PSM (propensity score weighting, categorical and continuous exposure). Both resources are crafted to enhance your understanding and application of PSM, ensuring a robust and informed approach to your data analysis journey\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "propensityscore.html#overview-of-tutorials",
    "href": "propensityscore.html#overview-of-tutorials",
    "title": "Propensity score",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCovariate matching using CCHS: example of OA-CVD\nThe tutorial illustrate a comprehensive data analysis workflow using R, focusing on matching methods to estimate treatment effects with the CCHS data. Initially, we conduct data pre-processing steps to handle categorical variables and missing data. Subsequent sections delve into setting up design objects for survey-weighted analyses and conducting preliminary analyses to explore variable distributions and treatment effects. The core of the analysis involves implementing matching techniques, starting with a single variable and progressively including more variables to refine the matching. Various matching scenarios are explored, each followed by logistic regression models to estimate treatment effects.\n\n\nPropensity score matching using CCHS: revisiting example of OA-CVD\nThe tutorial provides a thorough walkthrough of implementing Propensity Score Matching (PSM) in R, specifically in the context of an OA - CVD health study from the CCHS. PSM is utilized to mitigate bias from confounding variables in observational studies by pairing treated and control units with analogous propensity scores. The guide underscores that PSM is iterative, often requiring refinement of the matching strategy to achieve satisfactory covariate balance in the matched sample. Various strategies for estimating treatment effects in the matched sample are explored, each with distinct assumptions and implications. The tutorial also delves into different matching strategies, such as nearest-neighbor matching with and without calipers, matching with different ratios, and matching with replacement, all while emphasizing the importance of assessing and re-assessing covariate balance at each step using both graphical and numerical methods.\n\n\nPropensity score matching using NHANES: example of OA - CVD\nThe provided text outlines methodologies for conducting PSM using the NHANES dataset, with a particular emphasis on handling survey design and weights in the analysis. Three distinct approaches, attributed to Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), are delineated, each with a structured four-step process: 1) specifying the propensity score model, 2) matching treated and untreated subjects based on estimated propensity scores, 3) comparing baseline characteristics between matched groups, and 4) estimating treatment effects using the matched sample. The procedures utilize various R packages and functions to manipulate data, visualize missing data patterns, format variables, and perform analyses, ensuring that survey weights and design are appropriately considered to avoid bias in population-level effect estimates. The text underscores the importance of incorporating survey design into at least propensity score outcome analysis (e.g., during step 4: treatment effect estimation), as neglecting survey weights can significantly impact the estimates of population-level effects.\n\n\nPropensity score matching using NHANES: example of BMI - diabetes\nThe tutorial provides a comprehensive guide on implementing PSM in R, utilizing the NHANES dataset, with a specific focus on diabetes as an outcome and body mass index (BMI) as an exposure variable. The methodology encompasses ensuring accurate and reproducible results in PSM. The tutorial, again, meticulously follows three distinct approaches for PSM, as recommended by Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), each providing a unique perspective on handling and analyzing variables within the propensity score model. Notably, the tutorial introduces a nuanced approach to variable handling, model specifications, and matching steps, ensuring a thorough understanding of implementing PSM with varied methodologies. Furthermore, the tutorial introduces a “double adjustment” step in each approach, providing a robust estimate of the treatment effect while adjusting for covariates, thereby offering readers a holistic view on conducting PSM with a different set of variables and methodologies in the analysis steps.\n\n\nPropensity score matching using NHANES when some variables have missing observations\nThis tutorial offers a clear and straightforward guide on how to use Propensity Score Matching (PSM) and Multiple Imputation (MI) in R, using the NHANES dataset for practical illustration. The main goal is to explore the relationship between “diabetes” (outcome) and being “born in the US” (exposure), while effectively managing missing data through MI. The first part of the tutorial, focusing on logistic regression, explains how to perform multiple imputations, fit a logistic regression model to all imputed datasets, and then obtain pooled Odds Ratios (OR) and 95% confidence intervals. Following this, the PSM analysis section carefully applies the PSM method, following Zanutto E. L. (2006), to all imputed datasets, and presents the pooled OR estimates and 95% confidence intervals. The tutorial emphasizes the crucial role of managing missing data through multiple imputation and provides a detailed, step-by-step guide, including code and thorough explanations, to ensure a deep understanding and ability to replicate the PSM with MI process in epidemiological research. This resource is invaluable for researchers and data analysts looking to strengthen their analyses when dealing with missing data."
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting",
    "href": "propensityscore.html#propensity-score-weighting",
    "title": "Propensity score",
    "section": "Propensity score weighting",
    "text": "Propensity score weighting\nIn this section, propensity score weighting with different methods is employed to estimate treatment effects in a modified NHANES dataset. Three different propensity score weighting approaches are applied: Zanutto’s (2006), DuGoff et al.’s (2014), and Austin et al.’s (2018). Each approach involves multiple steps, including the estimation of propensity scores, the calculation of weights (both unstabilized and stabilized), balance checking to ensure covariate balancing, and fitting outcome models using the weighted data. The double adjustment technique is also considered in each approach."
  },
  {
    "objectID": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score matching with multiple imputation in subpopulations",
    "text": "Propensity score matching with multiple imputation in subpopulations\nIn this section, the goal is to use propensity score matching (PSM) with multiple imputation (MI) to analyze a modified dataset from NHANES 2017-2018. The analysis focuses on specific subpopulations defined by eligibility criteria. This analysis provides insights into how to handle complex survey data with missing values and perform PSM with MI for subpopulations defined by eligibility criteria."
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score weighting with multiple imputation in subpopulations",
    "text": "Propensity score weighting with multiple imputation in subpopulations\nIn this chapter, we employ propensity score (PS) weighting with MI to analyze a modified dataset from NHANES 2017-2018, focusing on specific subpopulations defined by eligibility criteria. Here we demonstrate how to perform PS weighting with MI for subpopulations defined by eligibility criteria."
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "href": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "title": "Propensity score",
    "section": "Propensity score weighting for multiple treatment categories",
    "text": "Propensity score weighting for multiple treatment categories\nIn this chapter, we use propensity score weighting for multiple treatment categories using CCHS data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "propensityscore0.html#propensity-score-analysis",
    "href": "propensityscore0.html#propensity-score-analysis",
    "title": "Concepts (S)",
    "section": "Propensity Score Analysis",
    "text": "Propensity Score Analysis\nThis section provides a comprehensive exploration into various facets of propensity score (PS) methods and their application in observational studies and surveys. Beginning with an in-depth look into key concepts and calculations related to ATE and ATT, the content navigates through the practical application and diagnostic checks of covariate balance using the SMD. It further elucidates the methodology and application of PS, particularly focusing on matching and weighting to mitigate bias and create comparable groups for causal inference. The intricacies of employing PS methods within surveys are explored, highlighting different approaches and the incorporation of design variables in PS and outcome models. Fundamental assumptions for causal inference, namely Conditional Exchangeability, Positivity, and Causal Consistency, are dissected to form a foundational understanding for conducting robust causal analyses. Additionally, the content optionally delves into the nuances of implementing IPW in surveys. Lastly, additional optional content features an insightful workshop, offering more explanations of PS method implementations in research contexts."
  },
  {
    "objectID": "propensityscore0.html#reading-list",
    "href": "propensityscore0.html#reading-list",
    "title": "Concepts (S)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Peter C. Austin 2011)\nOptional reading:\n\nPropensity score introduction (Karim 2021) External link\nExtensions of Propensity score approaches External link: prepared for Guest Lecture in SPPH 500/007 (Analytical Methods in Epidemiological Research)\nPropensity score for complex surveys External link: Uses the same lectures here, with some added text descriptions. This also includes a a structured framework for reporting analyses using PS methods in research manuscripts.\nReporting guideline (Stuart 2018; Simoneau et al. 2022)\nAssumptions (Hernán and Robins 2020)\n\nTheoretical references for propensity score analyses in complex surveys:\n(Peter C. Austin, Jembere, and Chiu 2018; DuGoff, Schuler, and Stuart 2014; Zanutto 2006; Leite, Stapleton, and Bettini 2018; Lenis, Ackerman, and Stuart 2018; Lenis et al. 2017; Ridgeway et al. 2015)"
  },
  {
    "objectID": "propensityscore0.html#video-lessons",
    "href": "propensityscore0.html#video-lessons",
    "title": "Concepts (S)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nTarget parameters\n\n\n\nAverage Treatment Effect (ATE) vs. Average Treatment effect on the Treated (ATT)\n\n\n\n\n\n\n\n\n\n\n\n\nBalance\n\n\n\nBalance and standardized mean difference (SMD) in observational studies\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score weighting in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConference Workshop (Optional)\n\n\n\nPost Conference Workshop for 2021 Conference - Canadian Society for Epidemiology and Biostatistics (CSEB)"
  },
  {
    "objectID": "propensityscore0.html#video-lesson-slides",
    "href": "propensityscore0.html#video-lesson-slides",
    "title": "Concepts (S)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nTarget parameters\n\n\nBalance\n\n\nPropensity score matching\n\n\nPropensity score matching in complex survey\n\n\nPropensity score weighting in complex survey\n\n\nCausal Assumptions\n\n\nFAQ"
  },
  {
    "objectID": "propensityscore0.html#links",
    "href": "propensityscore0.html#links",
    "title": "Concepts (S)",
    "section": "Links",
    "text": "Links\nTarget parameters\n\nGoogle Slides\nPDF Slides\n\nBalance\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching in complex survey\n\nGoogle Slides\nPDF Slides\n\nPropensity score weighting in complex survey\n\nGoogle Slides\nPDF Slides\n\nCausal Assumptions\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "propensityscore0.html#references",
    "href": "propensityscore0.html#references",
    "title": "Concepts (S)",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C. 2011. “A Tutorial and Case Study in Propensity Score Analysis: An Application to Estimating the Effect of in-Hospital Smoking Cessation Counseling on Mortality.” Multivariate Behavioral Research 46 (1): 119–51.\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nHernán, Miguel A., and James M. Robins. 2020. “Chapter 3.” In Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\n\n\nKarim, ME. 2021. “Understanding Propensity Score Matching.” 2021. https://ehsanx.github.io/psw/.\n\n\nLeite, Walter L., Laura M. Stapleton, and Eduardo F. Bettini. 2018. “Propensity Score Analysis of Complex Survey Data with Structural Equation Modeling: A Tutorial with Mplus.” Structural Equation Modeling: A Multidisciplinary Journal, 1–22.\n\n\nLenis, Diego, Benjamin Ackerman, and Elizabeth A. Stuart. 2018. “Measuring Model Misspecification: Application to Propensity Score Methods with Complex Survey Data.” Computational Statistics & Data Analysis.\n\n\nLenis, Diego, Thuan Quoc Nguyen, Dong, and Elizabeth A. Stuart. 2017. “It’s All about Balance: Propensity Score Matching in the Context of Complex Survey Data.” Biostatistics.\n\n\nRidgeway, Greg, Stephanie A. Kovalchik, Beth Ann Griffin, and Mohammed U. Kabeto. 2015. “Propensity Score Analysis with Survey Weighted Data.” Journal of Causal Inference 3 (2): 237–49.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette, Johanna Muñoz, Robert W Platt, John Petkau, et al. 2022. “Recommendations for the Use of Propensity Score Methods in Multiple Sclerosis Research.” Multiple Sclerosis Journal 28 (9): 1467–80.\n\n\nStuart, Elizabeth A. 2018. “Chapter 28. Propensity Scores and Matching Methods.” In The Reviewer’s Guide to Quantitative Methods in the Social Sciences, Second Edition, edited by Gregory R. Hancock, Ralph O. Mueller, and Laura M. Stapleton. Routledge.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore1.html",
    "href": "propensityscore1.html",
    "title": "Exact Matching (CCHS)",
    "section": "",
    "text": "In the following code chunk, we load the necessary R libraries for our analysis. MatchIt is used for matching methods to find comparable control units, tableone for creating Table 1 to describe baseline characteristics, Publish for generating readable output of regression analysis, and survey for analyzing complex survey samples.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(Publish)\nrequire(survey)\n\nLoad data\nIn the following code chunk, we load the CCHS dataset which is related to the Canadian Community Health Survey (CCHS). We then use ls() to list all objects in the workspace and str to display the structure of the data frame, providing a quick overview of the data and checking for any character variables.\n\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\"   \"analytic2\"       \"has_annotations\"\nstr(analytic.miss) # is there any character variable?\n#> 'data.frame':    397173 obs. of  22 variables:\n#>  $ CVD      : chr  \"event\" \"no event\" \"no event\" \"no event\" ...\n#>  $ age      : chr  \"65 years and over\" \"65 years and over\" \"30-39 years\" \"65 years and over\" ...\n#>  $ sex      : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n#>  $ married  : chr  \"single\" \"single\" \"not single\" \"single\" ...\n#>  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" ...\n#>  $ income   : chr  \"$29,999 or less\" \"$29,999 or less\" \"$80,000 or more\" \"$29,999 or less\" ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#>  $ phyact   : chr  \"Inactive\" \"Inactive\" \"Inactive\" \"Inactive\" ...\n#>  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n#>  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"stressed\" \"Not too stressed\" ...\n#>  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#>  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#>  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ weight   : num  142.8 71.4 168.3 71.4 196.1 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ OA       : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\nData pre-pocessing\nIn the following code chunk, we define a vector containing the names of variables of interest that needs to be converted to factor variables. We then convert these variables to factors, ensuring they are treated as categorical in subsequent analyses. We also recode the Osteoarthritis (OA) variable into a numeric binary format and display the frequency table of OA before and after the transformation.\n\nvar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"doctor\", \"drink\", \"bp\", \"province\",\n               \"immigrate\", \"fruit\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic.miss[var.names] <- lapply(analytic.miss[var.names] , factor)\ntable(analytic.miss$OA)\n#> \n#> Control      OA \n#>  314542   40943\nanalytic.miss$OA <- as.numeric(analytic.miss$OA==\"OA\") \ntable(analytic.miss$OA)\n#> \n#>      0      1 \n#> 314542  40943\n\nIdentify subjects with missing\nIn the following code chunk, we create a new variable miss and initially assign all its values to 1 in the full dataset (that contains some missing observations). We then adjust this assignment by setting miss to 0 for observations that are also present in another complete case dataset. That means any row with miss equal to 0 means that row has no missing observations. Finally, we display the frequency table of the miss variable to check the number of missing and non-missing observations.\n\nanalytic.miss$miss <- 1\nhead(analytic.miss$ID) # full data\n#> [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#> [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#> [1]  3  5  7 10 11 13\n# if associated with complete case, assign miss <- 0\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] <- 0\ntable(analytic.miss$miss)\n#> \n#>      0      1 \n#> 185613 211560\n\nSetting Design\nUnconditional design\nIn the following code chunk, we explore the summary of the weight variable and establish an unconditional survey design object w.design0 using the svydesign function, which will be used for subsequent survey-weighted analyses. We then explore the summary, standard deviation, and sum of the weights within our design object.\n\nsummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   65.28  126.63  200.09  243.21 7154.95\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   65.28  126.63  200.09  243.21 7154.95\nsd(weights(w.design0))\n#> [1] 241.0279\nsum(weights(w.design0))\n#> [1] 79468929\n\nConditioning the design\nIn the following code chunk, we create a new survey design object w.design by subsetting w.design0 to only include observations without missing data (miss == 0). We then explore the summary, standard deviation, and sum of the weights within this new design object.\n\nw.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   71.56  137.95  214.61  261.91 7154.95\nsd(weights(w.design))\n#> [1] 254.9346\nsum(weights(w.design))\n#> [1] 39835061\n\nSubset data (more!)\nWe subset the data for fast results (less computation). We will only work with cycle 1.1, and the people from Northern provinces in Canada.\n\nw.design1 <- subset(w.design, cycle == 11 & province == \"North\")\nsum(weights(w.design1))\n#> [1] 42786.28\n\nPreliminary analysis\nTable 1\nIn the following code chunk, we define a new variable vector var.names and create a categorical table using svyCreateCatTable to explore the distribution of age and sex across strata of OA within our subsetted design object w.design1. We then print the table with standardized mean differences (SMD) to assess the balance of these variables across strata.\n\nvar.names <- c(\"age\", \"sex\")\ntab0 <- svyCreateCatTable(var = var.names, strata= \"OA\", data=w.design1,test=FALSE)\nprint(tab0, smd = TRUE)\n#>                       Stratified by OA\n#>                        0               1              SMD   \n#>   n                    40691.2         2095.1               \n#>   age (%)                                              1.084\n#>      20-29 years       10889.4 (26.8)   120.9 ( 5.8)        \n#>      30-39 years       12251.7 (30.1)   237.8 (11.3)        \n#>      40-49 years       11094.0 (27.3)   572.7 (27.3)        \n#>      50-59 years        5346.6 (13.1)  1092.4 (52.1)        \n#>      60-64 years        1109.4 ( 2.7)    71.4 ( 3.4)        \n#>      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#>      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#>   sex = Male (%)       20824.6 (51.2)  1050.8 (50.2)   0.020\n\nTreatment effect\nIn the following code chunk, we fit a logistic regression model using svyglm to estimate the effect of OA and other covariates on the binary outcome CVD (cardiovascular disease). We then use publish to display the results in a readable format.\n\nfit.outcome <- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design1,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#>   Variable             Units OddsRatio         CI.95    p-value \n#>         OA                        0.89   [0.17;4.59]   0.887411 \n#>        age       20-29 years       Ref                          \n#>                  30-39 years      2.62  [0.29;23.43]   0.389521 \n#>                  40-49 years      4.89  [0.59;40.73]   0.142280 \n#>                  50-59 years     17.95 [2.59;124.68]   0.003550 \n#>                  60-64 years     23.95 [3.41;168.27]   0.001439 \n#>        sex            Female       Ref                          \n#>                         Male      1.32   [0.64;2.71]   0.456222 \n#>     stress  Not too stressed       Ref                          \n#>                     stressed      0.54   [0.21;1.39]   0.198815 \n#>    married        not single       Ref                          \n#>                       single      0.75   [0.31;1.80]   0.513807 \n#>     income   $29,999 or less       Ref                          \n#>              $30,000-$49,999      0.72   [0.24;2.16]   0.556703 \n#>              $50,000-$79,999      0.95   [0.27;3.40]   0.939104 \n#>              $80,000 or more      0.47   [0.10;2.15]   0.332557 \n#>       race         Non-white       Ref                          \n#>                        White      0.33   [0.11;0.94]   0.038131 \n#>        bmi       Underweight       Ref                          \n#>               healthy weight      0.29   [0.03;3.20]   0.310237 \n#>                   Overweight      0.44   [0.04;4.77]   0.503130 \n#>     phyact            Active       Ref                          \n#>                     Inactive      0.84   [0.30;2.40]   0.751345 \n#>                     Moderate      1.02   [0.32;3.27]   0.979528 \n#>      smoke      Never smoker       Ref                          \n#>               Current smoker      0.98   [0.26;3.76]   0.981454 \n#>                Former smoker      0.71   [0.18;2.71]   0.612518 \n#>  immigrate     not immigrant       Ref                          \n#>                   > 10 years      0.14   [0.03;0.78]   0.025010 \n#>                       recent      0.00   [0.00;0.00]    < 1e-04 \n#>      fruit 0-3 daily serving       Ref                          \n#>            4-6 daily serving      1.15   [0.52;2.56]   0.725722 \n#>             6+ daily serving      0.68   [0.17;2.71]   0.583752 \n#>       diab                No       Ref                          \n#>                          Yes      3.08  [0.93;10.23]   0.066677 \n#>        edu          < 2ndary       Ref                          \n#>                    2nd grad.      4.12  [0.87;19.43]   0.074178 \n#>              Other 2nd grad.      3.04  [0.63;14.67]   0.167135 \n#>               Post-2nd grad.      3.00  [0.82;10.98]   0.096939\n\nMatching: Estimating treatment effect\nGoing back to the data (not working on design here while matching)\nIn the following code chunk, we create a new dataset by omitting NA values from analytic.miss and converting it to a data frame. We then create a subset analytic11n which includes only observations from cycle 1.1 and the Northern provinces. We display the dimensions of this subset, as well as frequency tables of OA and a cross-tabulation of OA and age to understand the distribution of our target variable and a key covariate.\n\n# Create the dataset without design features\nanalytic2 <- as.data.frame(na.omit(analytic.miss))\nanalytic11n <- subset(analytic2, cycle == 11 & province == \"North\")\ndim(analytic11n)\n#> [1] 1424   23\ntable(analytic11n$OA)\n#> \n#>    0    1 \n#> 1357   67\ntable(analytic11n$OA,analytic11n$age)\n#>    \n#>     20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   0         345         432         358         177          45\n#>   1           4          11          18          31           3\n#>    \n#>     65 years and over teen\n#>   0                 0    0\n#>   1                 0    0\n\nMatching by 1 matching variable\nIn the following code chunk, we perform exact matching using a single variable, age. We define the matching formula and apply the matchit function to create matched sets of treated and control units. The resulting matching.obj object is displayed to summarize the matching results.\n\nmatch.formula <- as.formula(\"OA ~ age\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1424 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age\n\nMatching by 2 matching variables\nIn the following code chunk, we extend the matching to include two variables, age and sex. We create a new variable var.comb that concatenates these two variables and display its frequency table and the number of unique combinations. We then perform exact matching using both variables and display the resulting object.\n\nvar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex')])\ntable(var.comb)\n#> var.comb\n#> 20-29 yearsFemale   20-29 yearsMale 30-39 yearsFemale   30-39 yearsMale \n#>               184               165               220               223 \n#> 40-49 yearsFemale   40-49 yearsMale 50-59 yearsFemale   50-59 yearsMale \n#>               187               189               101               107 \n#> 60-64 yearsFemale   60-64 yearsMale \n#>                24                24\nlength(table(var.comb))\n#> [1] 10\nmatch.formula <- as.formula(\"OA ~ age + sex\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1424 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex\n\nMatching by 3 matching variables\nIn the following code chunk, we further extend the matching to include three variables: age, sex, and stress. We explore the unique combinations of these variables and their distribution across levels of OA. We then perform exact matching using these three variables and display the resulting object.\n\nvar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex', 'stress')])\ntable(var.comb)\n#> var.comb\n#> 20-29 yearsFemaleNot too stressed         20-29 yearsFemalestressed \n#>                               157                                27 \n#>   20-29 yearsMaleNot too stressed           20-29 yearsMalestressed \n#>                               147                                18 \n#> 30-39 yearsFemaleNot too stressed         30-39 yearsFemalestressed \n#>                               170                                50 \n#>   30-39 yearsMaleNot too stressed           30-39 yearsMalestressed \n#>                               183                                40 \n#> 40-49 yearsFemaleNot too stressed         40-49 yearsFemalestressed \n#>                               142                                45 \n#>   40-49 yearsMaleNot too stressed           40-49 yearsMalestressed \n#>                               141                                48 \n#> 50-59 yearsFemaleNot too stressed         50-59 yearsFemalestressed \n#>                                72                                29 \n#>   50-59 yearsMaleNot too stressed           50-59 yearsMalestressed \n#>                                78                                29 \n#> 60-64 yearsFemaleNot too stressed         60-64 yearsFemalestressed \n#>                                18                                 6 \n#>   60-64 yearsMaleNot too stressed           60-64 yearsMalestressed \n#>                                20                                 4\nlength(table(var.comb))\n#> [1] 20\ntable(var.comb,analytic11n$OA)\n#>                                    \n#> var.comb                              0   1\n#>   20-29 yearsFemaleNot too stressed 156   1\n#>   20-29 yearsFemalestressed          27   0\n#>   20-29 yearsMaleNot too stressed   144   3\n#>   20-29 yearsMalestressed            18   0\n#>   30-39 yearsFemaleNot too stressed 168   2\n#>   30-39 yearsFemalestressed          49   1\n#>   30-39 yearsMaleNot too stressed   178   5\n#>   30-39 yearsMalestressed            37   3\n#>   40-49 yearsFemaleNot too stressed 130  12\n#>   40-49 yearsFemalestressed          42   3\n#>   40-49 yearsMaleNot too stressed   138   3\n#>   40-49 yearsMalestressed            48   0\n#>   50-59 yearsFemaleNot too stressed  65   7\n#>   50-59 yearsFemalestressed          22   7\n#>   50-59 yearsMaleNot too stressed    67  11\n#>   50-59 yearsMalestressed            23   6\n#>   60-64 yearsFemaleNot too stressed  17   1\n#>   60-64 yearsFemalestressed           5   1\n#>   60-64 yearsMaleNot too stressed    19   1\n#>   60-64 yearsMalestressed             4   0\nmatch.formula <- as.formula(\"OA ~ age + sex + stress\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1327 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress\n\nMatching by 4 matching variables\nThe process of matching by 4 variables involves creating combinations of the 4 variables, exploring their distributions, and performing exact matching.\n\nvar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income')])\n#table(var.comb)\nlength(table(var.comb))\n#> [1] 76\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 900 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income\n\nMatching by 5 matching variables\n\nvar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race')])\nlength(table(var.comb))\n#> [1] 146\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income + race\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 616 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income, race\n\nMatching by 6 matching variables\n\nvar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race','edu')])\nlength(table(var.comb))\n#> [1] 354\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income + race + edu\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 399 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income, race, edu\nOACVD.match.11n <- match.data(matching.obj)\nvar.names <- c(\"age\", \"sex\", \"stress\", \"income\", \"race\", \"edu\")\ntab1 <- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n,test=FALSE)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     337         62               \n#>   age (%)                                       0.565\n#>      20-29 years         63 (18.7)   4 ( 6.5)        \n#>      30-39 years         61 (18.1)  11 (17.7)        \n#>      40-49 years        127 (37.7)  17 (27.4)        \n#>      50-59 years         82 (24.3)  28 (45.2)        \n#>      60-64 years          4 ( 1.2)   2 ( 3.2)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        211 (62.6)  29 (46.8)   0.322\n#>   stress = stressed (%)  42 (12.5)  17 (27.4)   0.381\n#>   income (%)                                    0.115\n#>      $29,999 or less     69 (20.5)  11 (17.7)        \n#>      $30,000-$49,999     57 (16.9)  13 (21.0)        \n#>      $50,000-$79,999     69 (20.5)  12 (19.4)        \n#>      $80,000 or more    142 (42.1)  26 (41.9)        \n#>   race = White (%)      242 (71.8)  43 (69.4)   0.054\n#>   edu (%)                                       0.146\n#>      < 2ndary            73 (21.7)  11 (17.7)        \n#>      2nd grad.            5 ( 1.5)   2 ( 3.2)        \n#>      Other 2nd grad.      0 ( 0.0)   0 ( 0.0)        \n#>      Post-2nd grad.     259 (76.9)  49 (79.0)\n\nTreatment effect\nConvert data to design\nIn the following code chunk, we create a new variable matched in the analytic.miss dataset to indicate whether an observation was included in the matched dataset OACVD.match.11n. We then create a new survey design object w.design.m that includes only the matched observations for subsequent analyses.\n\nanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match.11n$ID) # matched data\n#> [1] 399\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n$ID])\n#> [1] 399\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match.11n$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396774    399\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\nOutcome analysis\nThe subsequent code chunks involve fitting logistic regression models to estimate the treatment effect, both in a crude and adjusted manner, respectively. The models are fitted using the matched survey design object and the results are displayed in a readable format.\nCrude\n\nfit.outcome <- svyglm(I(CVD==\"event\") ~ OA,\n                   design = w.design.m,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#>  Variable Units OddsRatio        CI.95  p-value \n#>        OA            3.14 [0.80;12.40]   0.1025\n\nAdjusted\n\nfit.outcome <- svyglm(I(CVD==\"event\") ~ OA + \n                        age + sex + stress + income + race + edu,\n                   design = w.design.m,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\npublish(fit.outcome)\n#>  Variable            Units    OddsRatio                        CI.95   p-value \n#>        OA                          2.04                 [0.34;12.16]   0.43593 \n#>       age      20-29 years          Ref                                        \n#>                30-39 years         0.54                  [0.08;3.51]   0.51962 \n#>                40-49 years  30148597.83    [7758796.64;117149345.85]   < 1e-04 \n#>                50-59 years  63290825.92   [12589874.04;318170669.03]   < 1e-04 \n#>                60-64 years         0.31                  [0.01;9.33]   0.49735 \n#>       sex           Female          Ref                                        \n#>                       Male         1.58                  [0.29;8.62]   0.59729 \n#>    stress Not too stressed          Ref                                        \n#>                   stressed         0.15                  [0.01;1.80]   0.13666 \n#>    income  $29,999 or less          Ref                                        \n#>            $30,000-$49,999         0.20                  [0.01;3.84]   0.28640 \n#>            $50,000-$79,999         0.20                  [0.02;1.95]   0.16543 \n#>            $80,000 or more         0.08                  [0.01;0.68]   0.02122 \n#>      race        Non-white          Ref                                        \n#>                      White         1.02                  [0.11;9.45]   0.98723 \n#>       edu         < 2ndary          Ref                                        \n#>                  2nd grad. 845233463.16 [44642866.24;16002995941.97]   < 1e-04 \n#>             Post-2nd grad.  69867459.11    [9660580.05;505296971.72]   < 1e-04\n\nQuestions for the students\n\nLook at all the ORs. Some of them are VERY high. Why?\nLook at the CI in the above table. Some of them are Inf. Why?\nShould we match matching variables in the regression?\nMatching by a lot of variables\nThe code chunks involve performing matching using a large number of variables and estimating the treatment effect using the matched data. The process involves creating matched datasets, converting them to survey design objects, and fitting logistic regression models.\nMatching part in data\n\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu\")\nmatching.obj2 <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj2\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 22 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, immigrate, fruit, diab, edu\nOACVD.match.11n2 <- match.data(matching.obj2)\nvar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"immigrate\", \"fruit\", \"diab\", \"edu\")\ntab2 <- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n2,test=FALSE)\nprint(tab2, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     11         11               \n#>   age (%)                                     <0.001\n#>      20-29 years         3 (27.3)   3 (27.3)        \n#>      30-39 years         1 ( 9.1)   1 ( 9.1)        \n#>      40-49 years         4 (36.4)   4 (36.4)        \n#>      50-59 years         3 (27.3)   3 (27.3)        \n#>      60-64 years         0 ( 0.0)   0 ( 0.0)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)         6 (54.5)   6 (54.5)  <0.001\n#>   stress = stressed (%)  1 ( 9.1)   1 ( 9.1)  <0.001\n#>   married = single (%)   3 (27.3)   3 (27.3)  <0.001\n#>   income (%)                                  <0.001\n#>      $29,999 or less     1 ( 9.1)   1 ( 9.1)        \n#>      $30,000-$49,999     2 (18.2)   2 (18.2)        \n#>      $50,000-$79,999     2 (18.2)   2 (18.2)        \n#>      $80,000 or more     6 (54.5)   6 (54.5)        \n#>   race = White (%)      10 (90.9)  10 (90.9)  <0.001\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      4 (36.4)   4 (36.4)        \n#>      Overweight          7 (63.6)   7 (63.6)        \n#>   phyact (%)                                  <0.001\n#>      Active              3 (27.3)   3 (27.3)        \n#>      Inactive            5 (45.5)   5 (45.5)        \n#>      Moderate            3 (27.3)   3 (27.3)        \n#>   smoke (%)                                   <0.001\n#>      Never smoker        3 (27.3)   3 (27.3)        \n#>      Current smoker      2 (18.2)   2 (18.2)        \n#>      Former smoker       6 (54.5)   6 (54.5)        \n#>   immigrate (%)                               <0.001\n#>      not immigrant      10 (90.9)  10 (90.9)        \n#>      > 10 years          1 ( 9.1)   1 ( 9.1)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                   <0.001\n#>      0-3 daily serving   3 (27.3)   3 (27.3)        \n#>      4-6 daily serving   6 (54.5)   6 (54.5)        \n#>      6+ daily serving    2 (18.2)   2 (18.2)        \n#>   diab = Yes (%)         0 ( 0.0)   0 ( 0.0)  <0.001\n#>   edu (%)                                     <0.001\n#>      < 2ndary            1 ( 9.1)   1 ( 9.1)        \n#>      2nd grad.           0 ( 0.0)   0 ( 0.0)        \n#>      Other 2nd grad.     0 ( 0.0)   0 ( 0.0)        \n#>      Post-2nd grad.     10 (90.9)  10 (90.9)\n\nTreatment effect estimation in design\nCreate design\n\nanalytic.miss$matched2 <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match.11n2$ID) # matched data\n#> [1] 22\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n2$ID])\n#> [1] 22\nanalytic.miss$matched2[analytic.miss$ID %in% OACVD.match.11n2$ID] <- 1\ntable(analytic.miss$matched2)\n#> \n#>      0      1 \n#> 397151     22\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m2 <- subset(w.design0, matched2 == 1)\n\noutcome analysis\n\nfit.outcome <- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married +\n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design.m2,\n                   family = binomial(logit))\npublish(fit.outcome)\n# Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : \n#   contrasts can be applied only to factors with 2 or more levels\n\nQuestions for the students\n\nWhy the above model not fitting?\nSave data for later use\n\nsave(analytic11n, analytic2, analytic.miss, file=\"Data/propensityscore/cchs123c.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "propensityscore2.html",
    "href": "propensityscore2.html",
    "title": "PSM in OA-CVD (CCHS)",
    "section": "",
    "text": "This tutorial is a comprehensive guide on implementing Propensity Score Matching (PSM) using R, particularly focusing on a OA - CVD health study from the Canadian Community Health Survey (CCHS). This PSM method is used to reduce bias due to confounding variables in observational studies by matching treated and control units with similar propensity scores. The tutorial illustrates that PSM is an iterative process, where researchers may need to refine their matching strategy to achieve satisfactory balance in the matched sample. Different strategies for estimating the treatment effect in the matched sample are explored, each with its own assumptions and implications.\nLoad packages\nAt first, various R packages are loaded to utilize their functions for data manipulation, statistical analysis, and visualization.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\n\nLoad data\nThe dataset is loaded into the R environment. Variables are renamed to avoid conflicts in subsequent analyses.\n\nload(file=\"Data/propensityscore/cchs123c.RData\")\nhead(analytic11n)\n\n\n\n  \n\n\n\n# later we will create another variable called weights\n# hence to avoid any conflict/ambiguity,\n# renaming weight variable to survey.weight\nanalytic.miss$survey.weight <- analytic.miss$weight\nanalytic11n$survey.weight <- analytic11n$weight\nanalytic.miss$weight <- analytic11n$weight <- NULL\n\nAnalysis\nWe are going to apply propensity score analysis (Matching) in our OA - CVD problem from CCHS. For computation considerations, we will only work with cycle 1.1, and the people from Northern provinces in Canada (analytic11n data).\nStep 1\nSpecify PS\nA logistic regression model formula is specified to calculate the propensity scores (PS), which is the probability of receiving the treatment given the observed covariates.\n\nps.formula <- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                        doctor + drink + bp + \n                         immigrate + fruit + diab + edu\")\nvar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \n               \"income\", \"race\", \"bmi\", \"phyact\", \"smoke\", \n               \"doctor\", \"drink\", \"bp\", \n               \"immigrate\", \"fruit\", \"diab\", \"edu\")\n\nFit model\nThe software fits the PS model using a logistic regression by default. This package actually performs step 1 and 2 with one command matchit.\nLook at the website for arguments of matchit (RDocumentation 2023)]. It looks like this\n\nmatchit(formula, data, model=\"logit\", discard=0, reestimate=FALSE, nearest=TRUE,\n                 replace=FALSE, m.order=2, ratio=1, caliper=0, calclosest=FALSE,\n                 subclass=0, sub.by=\"treat\", mahvars=NULL, exact=FALSE, counter=TRUE, full=FALSE, full.options=list(),...)\n\n\n\n\n\n\n\nTip\n\n\n\nNearest-Neighbor Matching:\nNearest-neighbor matching is a widely used technique in PSM to pair treated and control units based on the proximity of their propensity scores. It is straightforward and computationally efficient, making it a popular choice in many applications of PSM. Nearest-neighbor matching is often termed a “greedy” algorithm because it matches units in order, without considering the global distribution of propensity scores. Once a match is made, it is not revisited, even if a later unit would have been a better match. The method seeks to minimize bias by creating closely matched pairs but can increase variance if the pool of potential matches is reduced too much (e.g., using a very narrow caliper). It is essential to ensure that there is a common support region where the distributions of propensity scores for treated and control units overlap, ensuring comparability.\n\n\nStep 2\nMatch subjects by PS\nWe are going to match using a Nearest neighbor algorithm. This is a greedy matching algorithm. Note that we are not even defining any caliper.\n\n\n\n\n\n\nTip\n\n\n\nCaliper:\nIn the context of PSM, a caliper is a predefined maximum allowable difference between the propensity scores of matched units. Essentially, it sets a threshold for how dissimilar matched units can be in terms of their propensity scores. When a caliper is used, a treated unit is only matched with a control unit if the absolute difference in their propensity scores is less than or equal to the specified caliper width. The caliper is used to avoid bad matches and thereby minimize bias in the estimated treatment effect. The size of the caliper is crucial. Too wide a caliper may allow poor matches, while too narrow a caliper may result in many units going unmatched. Implementing a caliper involves a trade-off between bias and efficiency. Using a caliper reduces bias by avoiding poor matches but may increase variance by reducing the number of matched pairs available for analysis. Therefore, the use of a caliper in PSM is a strategic decision to enhance the quality of matches and thereby improve the validity of causal inferences drawn from observational data. It is a practical tool to ensure that matched units are sufficiently similar in terms of their propensity scores, reducing the likelihood of bias due to poor matches.\n\n\n\n# set seed\nset.seed(123)\n# match\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 1424 (original), 134 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\n# create the \"matched\"\" data\nOACVD.match <- match.data(matching.obj)\n# see the dimension\ndim(analytic11n)\n#> [1] 1424   23\ndim(OACVD.match)\n#> [1] 134  26\n\nLet’s try to understand how this is working.\nExtract matched IDs\n\nm.mat<-matching.obj$match.matrix\nhead(m.mat)\n#>       [,1]    \n#> 17864 \"96719\" \n#> 17921 \"17846\" \n#> 18191 \"97168\" \n#> 18256 \"111999\"\n#> 18264 \"17989\" \n#> 18383 \"108197\"\n\nExtract the matched treated IDs\n\ntreated.id<-as.numeric(row.names(m.mat))\ntreated.id # basically row names\n#>  [1]  17864  17921  18191  18256  18264  18383  18389  18475  39105  96344\n#> [11]  96364  96407  96424  96460  96484  96571  96582  96625  96632  96641\n#> [21]  96657  96686  96693  96696  96705  96734  96795  96840  96913  97027\n#> [31]  97065  97125 108178 108183 108185 108192 111809 111813 111856 111859\n#> [41] 111895 111896 111920 111942 112014 112026 112046 112083 112086 112114\n#> [51] 112122 112151 112167 112189 112197 112215 112232 112245 112275 112284\n#> [61] 112289 112290 112300 112325 112375 126477 126522\n\nExtract the matched untreated IDs\n\nuntreated.id <- as.numeric(m.mat)\nuntreated.id # basically row names\n#>  [1]  96719  17846  97168 111999  17989 108197 112384  17909 126561 111880\n#> [11] 112184  18117  96865  18120  97023 112379  97017 126562  96356 126470\n#> [21] 126385  96374  18203  18262  96972 111924  96354  96983  18235  96882\n#> [31] 112054  18321 112349  18426  38996 126516 111814 112087  96569 111932\n#> [41] 126539  18315  96665  18225 112052 112324 112165  18329  96609 126376\n#> [51]  96474 126570 126547 126343  96680  96558 111931  96718  96533 111823\n#> [61] 112177  17953  17904 111908 111962  96644  96576\n\nExtract the matched treated data\n\ntx <- analytic11n[rownames(analytic11n) %in% treated.id,]\nhead(tx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n\n  \n\n\n\nExtract the matched untreated data\n\nutx <- analytic11n[rownames(analytic11n) %in% untreated.id,]\nhead(utx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n\n  \n\n\n\nExtract the matched data altogether\nSimply using match.data is enough (as done earlier).\n\nOACVD.match <- match.data(matching.obj)\n\nAssign match ID\n\nOACVD.match$match.ID <- NA\nOACVD.match$match.ID[rownames(OACVD.match) %in% treated.id] <- 1:length(treated.id)\nOACVD.match$match.ID[rownames(OACVD.match) %in% untreated.id] <- 1:length(untreated.id)\ntable(OACVD.match$match.ID)\n#> \n#>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#> 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#> 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n\nTake a look at individual matches for the first match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 1,])\n\n\n\n  \n\n\n\nTake a look at individual matches for the second match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 2,])\n\n\n\n  \n\n\n\nStep 3\nBoth graphical and numerical methods are used to assess the quality of the matches and the balance of covariates in the matched sample.\nExamining PS graphically\nVisually inspect the PS and assess the balance of covariates in the matched sample. Various plots are generated to visualize the distribution of PS across treatment groups and to check the balance of covariates before and after matching.\nmatchit package\n\n# plot(matching.obj) # covariate balance\nplot(matching.obj, type = \"jitter\") # propensity score locations\n\n\n\n#> To identify the units, use first mouse button; to stop, use second.\nplot(matching.obj, type = \"hist\") #check matched treated vs matched control\n\n\n\nsummrize.output <- summary(matching.obj, standardize = TRUE)\nplot(summrize.output)\n\n\n\n\nOveralp check\n\n# plot propensity scores by exposure group\nplot(density(OACVD.match$distance[OACVD.match$OA==1]), \n     col = \"red\", main = \"\")\nlines(density(OACVD.match$distance[OACVD.match$OA==0]), \n      col = \"blue\", lty = 2)\nlegend(\"topright\", c(\"Non-arthritis\",\"OA\"), \n       col = c(\"red\", \"blue\"), lty=1:2)\n\n\n\n\ncobalt package\nOverlap check in a more convenient way\n\n# different badwidth\nbal.plot(matching.obj, var.name = \"distance\")\n\n\n\n\nLook at the data\n\n# what is distance variable here?\nhead(OACVD.match)\n\n\n\n  \n\n\n\nNumerical values of PS\n\nsummary(OACVD.match$distance)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.044834 0.099094 0.138969 0.200576 0.611166\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.047344 0.099215 0.135042 0.199279 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.047346 0.098973 0.142897 0.198741 0.611166\n\nQuestion for the students\n\nAre you happy with the matching after reviewing the plots?\nCovariate balance in matched sample\nCovariate balance is assessed numerically using standardized mean differences (SMD).\n\n\n\n\n\n\nTip\n\n\n\nStandardized mean differences: SMD is a versatile and widely used statistical measure that facilitates the comparison of groups in research by providing a scale-free metric of difference and balance. In the context of propensity score matching, achieving low SMD values for covariates after matching is crucial to ensuring the validity of causal inferences drawn from the matched sample.\nBenifits:\n\nSMD is not influenced by the scale of the measured variable, making it suitable for comparing the balance of different variables measured on different scales.\nUnlike hypothesis testing, SMD is not affected by sample size, making it a reliable measure for assessing balance in matched samples.\n\n\n\n\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     67         67               \n#>   age (%)                                      0.190\n#>      20-29 years         4 ( 6.0)   4 ( 6.0)        \n#>      30-39 years        16 (23.9)  11 (16.4)        \n#>      40-49 years        16 (23.9)  18 (26.9)        \n#>      50-59 years        28 (41.8)  31 (46.3)        \n#>      60-64 years         3 ( 4.5)   3 ( 4.5)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        28 (41.8)  32 (47.8)   0.120\n#>   stress = stressed (%) 20 (29.9)  21 (31.3)   0.032\n#>   married = single (%)  22 (32.8)  23 (34.3)   0.032\n#>   income (%)                                   0.183\n#>      $29,999 or less    11 (16.4)  13 (19.4)        \n#>      $30,000-$49,999    19 (28.4)  15 (22.4)        \n#>      $50,000-$79,999    14 (20.9)  12 (17.9)        \n#>      $80,000 or more    23 (34.3)  27 (40.3)        \n#>   race = White (%)      43 (64.2)  44 (65.7)   0.031\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight     18 (26.9)  18 (26.9)        \n#>      Overweight         49 (73.1)  49 (73.1)        \n#>   phyact (%)                                   0.041\n#>      Active             16 (23.9)  16 (23.9)        \n#>      Inactive           40 (59.7)  39 (58.2)        \n#>      Moderate           11 (16.4)  12 (17.9)        \n#>   smoke (%)                                    0.258\n#>      Never smoker       14 (20.9)   9 (13.4)        \n#>      Current smoker     20 (29.9)  27 (40.3)        \n#>      Former smoker      33 (49.3)  31 (46.3)        \n#>   doctor = Yes (%)      51 (76.1)  47 (70.1)   0.135\n#>   drink (%)                                    0.116\n#>      Never drank         2 ( 3.0)   2 ( 3.0)        \n#>      Current drinker    54 (80.6)  51 (76.1)        \n#>      Former driker      11 (16.4)  14 (20.9)        \n#>   bp = Yes (%)           7 (10.4)   5 ( 7.5)   0.105\n#>   immigrate (%)                                0.180\n#>      not immigrant      64 (95.5)  61 (91.0)        \n#>      > 10 years          3 ( 4.5)   6 ( 9.0)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                    0.146\n#>      0-3 daily serving  19 (28.4)  19 (28.4)        \n#>      4-6 daily serving  28 (41.8)  32 (47.8)        \n#>      6+ daily serving   20 (29.9)  16 (23.9)        \n#>   diab = Yes (%)         1 ( 1.5)   4 ( 6.0)   0.238\n#>   edu (%)                                      0.105\n#>      < 2ndary           14 (20.9)  13 (19.4)        \n#>      2nd grad.           1 ( 1.5)   2 ( 3.0)        \n#>      Other 2nd grad.     1 ( 1.5)   1 ( 1.5)        \n#>      Post-2nd grad.     51 (76.1)  51 (76.1)\n\nQuestion for the students\n\nAll SMD < 0.20?\nOther balance measures\nIndividual categories\nIf you want to check balance at each category (not very useful in general situations). We are generally interested if the variables are balanced or not (not categories).\n\nbaltab <- bal.tab(matching.obj)\nbaltab\n#> Balance Measures\n#>                             Type Diff.Adj\n#> distance                Distance   0.0597\n#> age_20-29 years           Binary   0.0000\n#> age_30-39 years           Binary  -0.0746\n#> age_40-49 years           Binary   0.0299\n#> age_50-59 years           Binary   0.0448\n#> age_60-64 years           Binary   0.0000\n#> sex_Male                  Binary   0.0597\n#> stress_stressed           Binary   0.0149\n#> married_single            Binary   0.0149\n#> income_$29,999 or less    Binary   0.0299\n#> income_$30,000-$49,999    Binary  -0.0597\n#> income_$50,000-$79,999    Binary  -0.0299\n#> income_$80,000 or more    Binary   0.0597\n#> race_White                Binary   0.0149\n#> bmi_Underweight           Binary   0.0000\n#> bmi_healthy weight        Binary   0.0000\n#> bmi_Overweight            Binary   0.0000\n#> phyact_Active             Binary   0.0000\n#> phyact_Inactive           Binary  -0.0149\n#> phyact_Moderate           Binary   0.0149\n#> smoke_Never smoker        Binary  -0.0746\n#> smoke_Current smoker      Binary   0.1045\n#> smoke_Former smoker       Binary  -0.0299\n#> doctor_Yes                Binary  -0.0597\n#> drink_Never drank         Binary   0.0000\n#> drink_Current drinker     Binary  -0.0448\n#> drink_Former driker       Binary   0.0448\n#> bp_Yes                    Binary  -0.0299\n#> immigrate_not immigrant   Binary  -0.0448\n#> immigrate_> 10 years      Binary   0.0448\n#> immigrate_recent          Binary   0.0000\n#> fruit_0-3 daily serving   Binary   0.0000\n#> fruit_4-6 daily serving   Binary   0.0597\n#> fruit_6+ daily serving    Binary  -0.0597\n#> diab_Yes                  Binary   0.0448\n#> edu_< 2ndary              Binary  -0.0149\n#> edu_2nd grad.             Binary   0.0149\n#> edu_Other 2nd grad.       Binary   0.0000\n#> edu_Post-2nd grad.        Binary   0.0000\n#> \n#> Sample sizes\n#>           Control Treated\n#> All          1357      67\n#> Matched        67      67\n#> Unmatched    1290       0\n\nIndividual plots\nYou could plot each variables individually\n\nbal.plot(matching.obj, var.name = \"income\")\n\n\n\nbal.plot(matching.obj, var.name = \"age\")\n\n\n\nbal.plot(matching.obj, var.name = \"race\")\n\n\n\nbal.plot(matching.obj, var.name = \"diab\")\n\n\n\nbal.plot(matching.obj, var.name = \"immigrate\")\n\n\n\n\nLove plot\n\n# Individual categories again\nlove.plot(baltab, threshold = .2)\n#> Warning: Unadjusted values are missing. This can occur when `un = FALSE` and\n#> `quick = TRUE` in the original call to `bal.tab()`.\n#> Warning: Standardized mean differences and raw mean differences are present in the same plot. \n#> Use the `stars` argument to distinguish between them and appropriately label the x-axis.\n\n\n\n\nRepeat of Step 1-3 again\nCovariate balance is reassessed in each step to ensure the quality of the match.\nAdd caliper\nThe matching process is repeated, this time introducing a caliper to ensure that matches are only made within a specified range of PS.\n\nlogitPS <-  -log(1/OACVD.match$distance - 1) \n# logit of the propensity score\n.2*sd(logitPS) # suggested in the literature\n#> [1] 0.2334615\n\n\n# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1,\n                        caliper = .2*sd(logitPS))\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.015)\n#>  - number of obs.: 1424 (original), 128 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n\n\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041740 0.094963 0.124665 0.184103 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     64         64               \n#>   age (%)                                      0.196\n#>      20-29 years         4 ( 6.2)   4 ( 6.2)        \n#>      30-39 years        16 (25.0)  11 (17.2)        \n#>      40-49 years        16 (25.0)  18 (28.1)        \n#>      50-59 years        25 (39.1)  28 (43.8)        \n#>      60-64 years         3 ( 4.7)   3 ( 4.7)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        27 (42.2)  30 (46.9)   0.094\n#>   stress = stressed (%) 18 (28.1)  18 (28.1)  <0.001\n#>   married = single (%)  21 (32.8)  23 (35.9)   0.066\n#>   income (%)                                   0.204\n#>      $29,999 or less    11 (17.2)  13 (20.3)        \n#>      $30,000-$49,999    19 (29.7)  14 (21.9)        \n#>      $50,000-$79,999    13 (20.3)  12 (18.8)        \n#>      $80,000 or more    21 (32.8)  25 (39.1)        \n#>   race = White (%)      40 (62.5)  42 (65.6)   0.065\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight     18 (28.1)  18 (28.1)        \n#>      Overweight         46 (71.9)  46 (71.9)        \n#>   phyact (%)                                   0.096\n#>      Active             14 (21.9)  16 (25.0)        \n#>      Inactive           39 (60.9)  36 (56.2)        \n#>      Moderate           11 (17.2)  12 (18.8)        \n#>   smoke (%)                                    0.267\n#>      Never smoker       14 (21.9)   9 (14.1)        \n#>      Current smoker     19 (29.7)  26 (40.6)        \n#>      Former smoker      31 (48.4)  29 (45.3)        \n#>   doctor = Yes (%)      48 (75.0)  44 (68.8)   0.139\n#>   drink (%)                                    0.123\n#>      Never drank         2 ( 3.1)   2 ( 3.1)        \n#>      Current drinker    52 (81.2)  49 (76.6)        \n#>      Former driker      10 (15.6)  13 (20.3)        \n#>   bp = Yes (%)           7 (10.9)   5 ( 7.8)   0.107\n#>   immigrate (%)                                0.260\n#>      not immigrant      62 (96.9)  58 (90.6)        \n#>      > 10 years          2 ( 3.1)   6 ( 9.4)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                    0.116\n#>      0-3 daily serving  19 (29.7)  19 (29.7)        \n#>      4-6 daily serving  27 (42.2)  30 (46.9)        \n#>      6+ daily serving   18 (28.1)  15 (23.4)        \n#>   diab = Yes (%)         0 ( 0.0)   3 ( 4.7)   0.314\n#>   edu (%)                                      0.108\n#>      < 2ndary           14 (21.9)  13 (20.3)        \n#>      2nd grad.           1 ( 1.6)   2 ( 3.1)        \n#>      Other 2nd grad.     1 ( 1.6)   1 ( 1.6)        \n#>      Post-2nd grad.     48 (75.0)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable for pair matching?\nhead(OACVD.match)\n\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       1       1       1       1       1       1\n\nStep 4\nEstimate treatment effect for matched data\nDifferent models (e.g., unconditional logistic regression, survey design) are fitted to estimate the treatment effect in the matched sample.\nUnconditional logistic\n\n# Wrong model for population!!\noutcome.model <- glm(CVD ~ OA, data = OACVD.match, family = binomial())\npublish(outcome.model)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.48 [0.09;2.74]   0.4119\n\nSurvey design\nConvert data to design\nThe matched data is converted to a survey design object to account for the matched pairs in the analysis.\n\nanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 128\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 128\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 397045    128\nw.design0 <- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\nBalance in matched population?\n\ntab1 <- svyCreateTableOne(strata = \"OA\", data = w.design.m, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0              1              SMD   \n#>   n                     1783.6         2002.1               \n#>   age (%)                                              0.307\n#>      20-29 years         131.0 ( 7.3)   120.9 ( 6.0)        \n#>      30-39 years         388.0 (21.8)   237.8 (11.9)        \n#>      40-49 years         518.3 (29.1)   572.7 (28.6)        \n#>      50-59 years         680.1 (38.1)   999.3 (49.9)        \n#>      60-64 years          66.1 ( 3.7)    71.4 ( 3.6)        \n#>      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#>      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#>   sex = Male (%)         852.4 (47.8)   985.1 (49.2)   0.028\n#>   stress = stressed (%)  544.6 (30.5)   531.8 (26.6)   0.088\n#>   married = single (%)   419.8 (23.5)   427.5 (21.4)   0.052\n#>   income (%)                                           0.222\n#>      $29,999 or less     266.6 (14.9)   352.4 (17.6)        \n#>      $30,000-$49,999     462.8 (25.9)   348.8 (17.4)        \n#>      $50,000-$79,999     298.6 (16.7)   315.7 (15.8)        \n#>      $80,000 or more     755.5 (42.4)   985.2 (49.2)        \n#>   race = White (%)      1129.2 (63.3)  1364.9 (68.2)   0.103\n#>   bmi (%)                                              0.045\n#>      Underweight           0.0 ( 0.0)     0.0 ( 0.0)        \n#>      healthy weight      483.3 (27.1)   583.3 (29.1)        \n#>      Overweight         1300.2 (72.9)  1418.8 (70.9)        \n#>   phyact (%)                                           0.054\n#>      Active              448.0 (25.1)   493.9 (24.7)        \n#>      Inactive           1075.2 (60.3)  1176.6 (58.8)        \n#>      Moderate            260.3 (14.6)   331.5 (16.6)        \n#>   smoke (%)                                            0.288\n#>      Never smoker        400.2 (22.4)   265.8 (13.3)        \n#>      Current smoker      548.8 (30.8)   836.6 (41.8)        \n#>      Former smoker       834.6 (46.8)   899.6 (44.9)        \n#>   doctor = Yes (%)      1376.0 (77.1)  1430.1 (71.4)   0.131\n#>   drink (%)                                            0.194\n#>      Never drank          44.0 ( 2.5)   112.6 ( 5.6)        \n#>      Current drinker    1464.1 (82.1)  1510.6 (75.5)        \n#>      Former driker       275.4 (15.4)   378.9 (18.9)        \n#>   bp = Yes (%)           166.6 ( 9.3)   153.5 ( 7.7)   0.060\n#>   immigrate (%)                                        0.235\n#>      not immigrant      1694.8 (95.0)  1774.1 (88.6)        \n#>      > 10 years           88.8 ( 5.0)   228.0 (11.4)        \n#>      recent                0.0 ( 0.0)     0.0 ( 0.0)        \n#>   fruit (%)                                            0.293\n#>      0-3 daily serving   426.1 (23.9)   480.8 (24.0)        \n#>      4-6 daily serving   748.1 (41.9)  1082.3 (54.1)        \n#>      6+ daily serving    609.4 (34.2)   439.0 (21.9)        \n#>   diab = Yes (%)           0.0 ( 0.0)    83.6 ( 4.2)   0.295\n#>   edu (%)                                              0.172\n#>      < 2ndary            324.9 (18.2)   342.8 (17.1)        \n#>      2nd grad.            18.8 ( 1.1)    47.6 ( 2.4)        \n#>      Other 2nd grad.      15.2 ( 0.9)    52.2 ( 2.6)        \n#>      Post-2nd grad.     1424.7 (79.9)  1559.4 (77.9)\n\nOutcome analysis\n\nfit.design <- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.50 [0.08;3.09]   0.4535\n\nMatched data with increase ratio\nThe matching process is repeated with a different ratio (e.g., 1:5) to explore how changing the ratio affects the covariate balance and treatment effect estimation.\n\n# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 5:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.013)\n#>  - number of obs.: 1424 (original), 349 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004643 0.039181 0.084421 0.101576 0.146403 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     285         64               \n#>   age (%)                                       0.217\n#>      20-29 years         24 ( 8.4)   4 ( 6.2)        \n#>      30-39 years         59 (20.7)  11 (17.2)        \n#>      40-49 years         94 (33.0)  18 (28.1)        \n#>      50-59 years         98 (34.4)  28 (43.8)        \n#>      60-64 years         10 ( 3.5)   3 ( 4.7)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        132 (46.3)  30 (46.9)   0.011\n#>   stress = stressed (%)  81 (28.4)  18 (28.1)   0.007\n#>   married = single (%)   91 (31.9)  23 (35.9)   0.085\n#>   income (%)                                    0.065\n#>      $29,999 or less     64 (22.5)  13 (20.3)        \n#>      $30,000-$49,999     65 (22.8)  14 (21.9)        \n#>      $50,000-$79,999     51 (17.9)  12 (18.8)        \n#>      $80,000 or more    105 (36.8)  25 (39.1)        \n#>   race = White (%)      169 (59.3)  42 (65.6)   0.131\n#>   bmi (%)                                       0.097\n#>      Underweight          0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      68 (23.9)  18 (28.1)        \n#>      Overweight         217 (76.1)  46 (71.9)        \n#>   phyact (%)                                    0.136\n#>      Active              57 (20.0)  16 (25.0)        \n#>      Inactive           178 (62.5)  36 (56.2)        \n#>      Moderate            50 (17.5)  12 (18.8)        \n#>   smoke (%)                                     0.097\n#>      Never smoker        46 (16.1)   9 (14.1)        \n#>      Current smoker     123 (43.2)  26 (40.6)        \n#>      Former smoker      116 (40.7)  29 (45.3)        \n#>   doctor = Yes (%)      183 (64.2)  44 (68.8)   0.096\n#>   drink (%)                                     0.051\n#>      Never drank         10 ( 3.5)   2 ( 3.1)        \n#>      Current drinker    212 (74.4)  49 (76.6)        \n#>      Former driker       63 (22.1)  13 (20.3)        \n#>   bp = Yes (%)           22 ( 7.7)   5 ( 7.8)   0.003\n#>   immigrate (%)                                 0.100\n#>      not immigrant      266 (93.3)  58 (90.6)        \n#>      > 10 years          19 ( 6.7)   6 ( 9.4)        \n#>      recent               0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                     0.149\n#>      0-3 daily serving  104 (36.5)  19 (29.7)        \n#>      4-6 daily serving  124 (43.5)  30 (46.9)        \n#>      6+ daily serving    57 (20.0)  15 (23.4)        \n#>   diab = Yes (%)         12 ( 4.2)   3 ( 4.7)   0.023\n#>   edu (%)                                       0.137\n#>      < 2ndary            67 (23.5)  13 (20.3)        \n#>      2nd grad.            9 ( 3.2)   2 ( 3.1)        \n#>      Other 2nd grad.      9 ( 3.2)   1 ( 1.6)        \n#>      Post-2nd grad.     200 (70.2)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.8906  0.8906  0.8906  1.0000  0.8906  4.4531\n\nCombining matching weights\nDifferent approaches to incorporating weights (e.g., matching weights, survey weights) are explored.\nNot incorporating matching weights\n\nanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396824    349\nw.design0 <- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nfit.design <- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95 p-value \n#>        OA            0.80 [0.23;2.80]   0.722\n\nIncorporating matching weights\n\nanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396824    349\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight <- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] <-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 <- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nfit.design <- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            1.14 [0.32;4.07]   0.8419\n\nMatched with replacement\nMatching is performed with replacement, allowing control units to be used in more than one match.\n\n# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2,\n                        replace = TRUE)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 5:1 nearest neighbor matching with replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.013)\n#>  - number of obs.: 1424 (original), 308 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004643 0.034244 0.067224 0.100845 0.148958 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.042322 0.097877 0.129743 0.189255 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     243         65               \n#>   age (%)                                       0.266\n#>      20-29 years         22 ( 9.1)   4 ( 6.2)        \n#>      30-39 years         57 (23.5)  11 (16.9)        \n#>      40-49 years         74 (30.5)  18 (27.7)        \n#>      50-59 years         82 (33.7)  29 (44.6)        \n#>      60-64 years          8 ( 3.3)   3 ( 4.6)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        113 (46.5)  31 (47.7)   0.024\n#>   stress = stressed (%)  67 (27.6)  19 (29.2)   0.037\n#>   married = single (%)   75 (30.9)  23 (35.4)   0.096\n#>   income (%)                                    0.079\n#>      $29,999 or less     53 (21.8)  13 (20.0)        \n#>      $30,000-$49,999     57 (23.5)  14 (21.5)        \n#>      $50,000-$79,999     44 (18.1)  12 (18.5)        \n#>      $80,000 or more     89 (36.6)  26 (40.0)        \n#>   race = White (%)      146 (60.1)  42 (64.6)   0.094\n#>   bmi (%)                                       0.088\n#>      Underweight          0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      58 (23.9)  18 (27.7)        \n#>      Overweight         185 (76.1)  47 (72.3)        \n#>   phyact (%)                                    0.160\n#>      Active              45 (18.5)  16 (24.6)        \n#>      Inactive           155 (63.8)  37 (56.9)        \n#>      Moderate            43 (17.7)  12 (18.5)        \n#>   smoke (%)                                     0.095\n#>      Never smoker        40 (16.5)   9 (13.8)        \n#>      Current smoker     101 (41.6)  26 (40.0)        \n#>      Former smoker      102 (42.0)  30 (46.2)        \n#>   doctor = Yes (%)      152 (62.6)  45 (69.2)   0.141\n#>   drink (%)                                     0.059\n#>      Never drank          9 ( 3.7)   2 ( 3.1)        \n#>      Current drinker    181 (74.5)  50 (76.9)        \n#>      Former driker       53 (21.8)  13 (20.0)        \n#>   bp = Yes (%)           19 ( 7.8)   5 ( 7.7)   0.005\n#>   immigrate (%)                                 0.115\n#>      not immigrant      228 (93.8)  59 (90.8)        \n#>      > 10 years          15 ( 6.2)   6 ( 9.2)        \n#>      recent               0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                     0.147\n#>      0-3 daily serving   87 (35.8)  19 (29.2)        \n#>      4-6 daily serving  109 (44.9)  31 (47.7)        \n#>      6+ daily serving    47 (19.3)  15 (23.1)        \n#>   diab = Yes (%)         11 ( 4.5)   3 ( 4.6)   0.004\n#>   edu (%)                                       0.175\n#>      < 2ndary            58 (23.9)  13 (20.0)        \n#>      2nd grad.            8 ( 3.3)   2 ( 3.1)        \n#>      Other 2nd grad.      9 ( 3.7)   1 ( 1.5)        \n#>      Post-2nd grad.     168 (69.1)  49 (75.4)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.7477  0.7477  0.7477  1.0000  1.0000  7.4769\n\nSurvey design\nThe matched data is converted into a survey design object, and the treatment effect is estimated while accounting for the complex survey design.\n\nanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 308\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 308\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396865    308\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight <- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] <-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 <- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nfit.design <- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.99 [0.26;3.72]   0.9909\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nRDocumentation. 2023. “Matchit: Matchit: Matching Software for Causal Inference.” https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit."
  },
  {
    "objectID": "propensityscore3.html",
    "href": "propensityscore3.html",
    "title": "PSM in OA-CVD (US)",
    "section": "",
    "text": "Pre-processing\nLoad data\nLoad the dataset and inspect its structure and variables.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") \nls()\n#> [1] \"analytic\"           \"analytic.with.miss\" \"has_annotations\"\n\nVisualize missing data patterns.\n\nlibrary(dplyr)\nanalytic.with.miss <- dplyr::select(analytic.with.miss, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, \n                  married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\ndim(analytic.with.miss)\n#> [1] 9254   13\nstr(analytic.with.miss)\n#> 'data.frame':    9254 obs. of  13 variables:\n#>  $ cholesterol: 'labelled' int  NA NA 157 148 189 209 176 NA 238 182 ...\n#>   ..- attr(*, \"label\")= chr \"Total Cholesterol (mg/dL)\"\n#>  $ gender     : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>  $ age        : 'labelled' int  NA NA 66 NA NA 66 75 NA 56 NA ...\n#>   ..- attr(*, \"label\")= chr \"Age in years at screening\"\n#>  $ born       : chr  \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race       : chr  \"Other\" \"White\" \"Black\" \"Other\" ...\n#>  $ education  : chr  NA NA \"High.School\" NA ...\n#>  $ married    : chr  NA NA \"Previously.married\" NA ...\n#>  $ income     : chr  \"Over100k\" \"Over100k\" \"<25k\" NA ...\n#>  $ bmi        : 'labelled' num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>   ..- attr(*, \"label\")= chr \"Body Mass Index (kg/m**2)\"\n#>  $ diabetes   : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ weight     : 'labelled' num  8540 42567 8338 8723 7065 ...\n#>   ..- attr(*, \"label\")= chr \"Full sample 2 year MEC exam weight\"\n#>  $ psu        : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>  $ strata     : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\nnames(analytic.with.miss)\n#>  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#>  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#> [11] \"weight\"      \"psu\"         \"strata\"\n\nlibrary(DataExplorer)\nplot_missing(analytic.with.miss)\n\n\n\n\nFormatting variables\nRename variables to avoid conflicts. Recode variables into binary or categorical as needed. Ensure variable types (factor, numeric) are appropriate.\n\n# to avaoid any confusion later\n# rename weight variable as weights \n# is reserved for matching weights\nanalytic.with.miss$survey.weight <- analytic.with.miss$weight\nanalytic.with.miss$weight <- NULL\n\n#Creating binary variable for cholesterol\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, \n                                             1, #\"healthy\",\n                                             0) #\"unhealthy\")\n# exposure recoding\nanalytic.with.miss$diabetes <- ifelse(analytic.with.miss$diabetes == \"Yes\", 1, 0)\n\n# ID\nanalytic.with.miss$ID <- 1:nrow(analytic.with.miss)\n\n# covariates\nanalytic.with.miss$born <- ifelse(analytic.with.miss$born == \"Other\", \n                                             0,\n                                             1)\n\nvars = c(\"gender\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\")\n\nnumeric.names <- c(\"cholesterol\", \"bmi\")\nfactor.names <- vars[!vars %in% numeric.names] \n\nanalytic.with.miss[factor.names] <- apply(X = analytic.with.miss[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanalytic.with.miss[numeric.names] <- apply(X = analytic.with.miss[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\nanalytic.with.miss$income <- factor(analytic.with.miss$income, \n                                    ordered = TRUE, \n                                levels = c(\"<25k\", \"Between.25kto54k\", \n                                           \"Between.55kto99k\", \n                                           \"Over100k\"))\n\n# features\ntable(analytic.with.miss$strata)\n#> \n#> 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#> 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\ntable(analytic.with.miss$psu)\n#> \n#>    1    2 \n#> 4464 4790\ntable(analytic.with.miss$strata,analytic.with.miss$psu)\n#>      \n#>         1   2\n#>   134 215 295\n#>   135 316 322\n#>   136 320 375\n#>   137 306 248\n#>   138 308 297\n#>   139 278 375\n#>   140 315 297\n#>   141 282 411\n#>   142 349 386\n#>   143 232 319\n#>   144 351 338\n#>   145 339 270\n#>   146 277 327\n#>   147 335 261\n#>   148 241 269\n\n# impute\n# require(mice)\n# imputation1 <- mice(analytic.with.miss, seed = 123,\n#                    m = 1, # Number of multiple imputations. \n#                    maxit = 10 # Number of iteration; mostly useful for convergence\n#                    )\n# analytic.with.miss <- complete(imputation1)\n# plot_missing(analytic.with.miss)\n\nComplete case data\nCreate a dataset (analytic.data) without NA values for analysis. This is done for simplified analysis, but this approach has it’s own challenges. In a next tutorial, we will appropriately deal with missing observations in a propensity score modelling.\n\ndim(analytic.with.miss)\n#> [1] 9254   15\nanalytic.data <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic.data) # complete case\n#> [1] 4167   15\n\nZanutto (2006)\n\nRef: Zanutto (2006)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSet seed\n\nset.seed(123)\n\n\n“it is not necessary to use survey-weighted estimation for the propensity score model”\n\nPropensity score analysis in 4 steps:\n\nStep 1: PS model specification\nStep 2: Matching based on the estimated propensity scores\nStep 3: Balance checking\nStep 4: Outcome modelling\nStep 1\nSpecify the propensity score model to estimate propensity scores\n\nps.formula <- as.formula(diabetes ~ gender + born +\n                         race + education + married + income + bmi)\n\nStep 2\nMatch treated and untreated subjects based on the estimated propensity scores. Perform nearest-neighbor matching using the propensity scores. Visualize the distribution of propensity scores before and after matching.\n\nrequire(MatchIt)\nset.seed(123)\n# This function fits propensity score model (using logistic \n# regression as above) when specified distance = 'logit'\n# performs nearest-neighbor (NN) matching, \n# without replacement \n# with caliper = .2*SD of propensity score  \n# within which to draw control units \n# with 1:1 ratio (pair-matching)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\n# see matchit function options here\n# https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02901 0.12255 0.17658 0.18982 0.23876 0.82164\nplot(match.obj, type = \"jitter\")\n\n\n\n#> [1] \"To identify the units, use first mouse button; to stop, use second.\"\n#> integer(0)\nplot(match.obj, type = \"hist\")\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02901 0.11509 0.16687 0.17949 0.22816 0.75968 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.04793 0.16489 0.21300 0.23395 0.27768 0.82164\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.019)\n#>  - number of obs.: 4167 (original), 1564 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data <- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nrequire(tableone)\nbaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1 <- CreateTableOne(strata = \"diabetes\", vars = baselinevars,\n                       data = analytic.data, test = FALSE)\nprint(tab1, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                      3376           791               \n#>   gender = Male (%)      1578 (46.7)    434 (54.9)   0.163\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.060\n#>      Black                728 (21.6)    183 (23.1)        \n#>      Hispanic             727 (21.5)    170 (21.5)        \n#>      Other                642 (19.0)    159 (20.1)        \n#>      White               1279 (37.9)    279 (35.3)        \n#>   education (%)                                      0.185\n#>      College             1992 (59.0)    415 (52.5)        \n#>      High.School         1174 (34.8)    290 (36.7)        \n#>      School               210 ( 6.2)     86 (10.9)        \n#>   married (%)                                        0.316\n#>      Married             2027 (60.0)    488 (61.7)        \n#>      Never.married        631 (18.7)     70 ( 8.8)        \n#>      Previously.married   718 (21.3)    233 (29.5)        \n#>   income (%)                                         0.092\n#>      <25k                 830 (24.6)    225 (28.4)        \n#>      Between.25kto54k    1064 (31.5)    244 (30.8)        \n#>      Between.55kto99k     778 (23.0)    173 (21.9)        \n#>      Over100k             704 (20.9)    149 (18.8)        \n#>   bmi (mean (SD))       29.29 (7.11)  32.31 (8.03)   0.399\n\n\ntab1m <- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                       782           782               \n#>   gender = Male (%)       422 (54.0)    430 (55.0)   0.021\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.068\n#>      Black                180 (23.0)    179 (22.9)        \n#>      Hispanic             151 (19.3)    170 (21.7)        \n#>      Other                171 (21.9)    156 (19.9)        \n#>      White                280 (35.8)    277 (35.4)        \n#>   education (%)                                      0.077\n#>      College              441 (56.4)    411 (52.6)        \n#>      High.School          262 (33.5)    286 (36.6)        \n#>      School                79 (10.1)     85 (10.9)        \n#>   married (%)                                        0.044\n#>      Married              502 (64.2)    486 (62.1)        \n#>      Never.married         63 ( 8.1)     69 ( 8.8)        \n#>      Previously.married   217 (27.7)    227 (29.0)        \n#>   income (%)                                         0.065\n#>      <25k                 202 (25.8)    218 (27.9)        \n#>      Between.25kto54k     236 (30.2)    244 (31.2)        \n#>      Between.55kto99k     187 (23.9)    171 (21.9)        \n#>      Over100k             157 (20.1)    149 (19.1)        \n#>   bmi (mean (SD))       32.12 (8.29)  32.05 (7.58)   0.009\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample. Use the matched sample to estimate the treatment effect, considering survey design.\nIncorporating the survey design into both linear regression and propensity score analysis is crucial. Neglecting the survey weights can significantly impact the estimates, altering the representation of population-level effects.\n\nrequire(survey)\n# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data$ID) # matched data\n#> [1] 1564\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#> [1] 1564\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7690 1564\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial(logit), design = w.design.m)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1564 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.02 \n  \n\n Pseudo-R² (McFadden) \n    0.01 \n  \n\n AIC \n    1924.27 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.34 \n    0.98 \n    1.83 \n    1.84 \n    0.09 \n  \n\n diabetes \n    1.67 \n    1.17 \n    2.37 \n    2.84 \n    0.01 \n  \n\n\n Standard errors: Robust\n\n\n\nDuGoff et al. (2014)\n\nRef: DuGoff, Schuler, and Stuart (2014)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Similar to Zanutto but includes additional covariates in the model.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula <- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi+\n                           psu+strata+survey.weight)\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.00363 0.11341 0.17509 0.18982 0.24765 0.80853\nplot(match.obj, type = \"jitter\")\n\n\n\n#> [1] \"To identify the units, use first mouse button; to stop, use second.\"\n#> integer(0)\nplot(match.obj, type = \"hist\")\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.00363 0.10394 0.16243 0.17690 0.23461 0.73143 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01182 0.17245 0.22748 0.24500 0.29948 0.80853\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4167 (original), 1570 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi, psu, strata, survey.weight\n# extract matched data\nmatched.data <- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nrequire(tableone)\nbaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\", \n                  \"psu\", \"strata\", \"survey.weight\")\nmatched.data$survey.weight <- as.numeric(as.character(matched.data$survey.weight))\nmatched.data$strata <- as.numeric(as.character(matched.data$strata))\ntab1m <- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                            Stratified by diabetes\n#>                             0                   1                   SMD   \n#>   n                              785                 785                  \n#>   gender = Male (%)              433 (55.2)          431 (54.9)      0.005\n#>   born (mean (SD))              1.00 (0.00)         1.00 (0.00)     <0.001\n#>   race (%)                                                           0.048\n#>      Black                       193 (24.6)          180 (22.9)           \n#>      Hispanic                    163 (20.8)          170 (21.7)           \n#>      Other                       163 (20.8)          158 (20.1)           \n#>      White                       266 (33.9)          277 (35.3)           \n#>   education (%)                                                      0.032\n#>      College                     403 (51.3)          412 (52.5)           \n#>      High.School                 300 (38.2)          288 (36.7)           \n#>      School                       82 (10.4)           85 (10.8)           \n#>   married (%)                                                        0.030\n#>      Married                     473 (60.3)          484 (61.7)           \n#>      Never.married                71 ( 9.0)           70 ( 8.9)           \n#>      Previously.married          241 (30.7)          231 (29.4)           \n#>   income (%)                                                         0.035\n#>      <25k                        232 (29.6)          222 (28.3)           \n#>      Between.25kto54k            236 (30.1)          242 (30.8)           \n#>      Between.55kto99k            176 (22.4)          173 (22.0)           \n#>      Over100k                    141 (18.0)          148 (18.9)           \n#>   bmi (mean (SD))              31.92 (8.33)        32.09 (7.52)      0.020\n#>   psu = 2 (%)                    382 (48.7)          394 (50.2)      0.031\n#>   strata (mean (SD))          140.84 (4.24)       140.97 (4.23)      0.031\n#>   survey.weight (mean (SD)) 35647.01 (37699.98) 35596.81 (45212.82)  0.001\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample\n\n# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data$ID) # matched data\n#> [1] 1570\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#> [1] 1570\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7684 1570\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial, design = w.design.m)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1570 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.01 \n  \n\n Pseudo-R² (McFadden) \n    0.00 \n  \n\n AIC \n    1918.09 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.69 \n    1.33 \n    2.15 \n    4.24 \n    0.00 \n  \n\n diabetes \n    1.32 \n    0.99 \n    1.77 \n    1.86 \n    0.08 \n  \n\n\n Standard errors: Robust\n\n\n\nAustin et al. (2018)\n\nRef: Austin, Jembere, and Chiu (2018)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Use survey logistic regression to account for survey design in propensity score estimation.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula <- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi)\nrequire(survey)\nanalytic.design <- svydesign(id=~psu,weights=~survey.weight, \n                             strata=~strata,\n                             data=analytic.data, nest=TRUE)\nps.fit <- svyglm(ps.formula, design=analytic.design, family=quasibinomial)\nanalytic.data$PS <- fitted(ps.fit)\nsummary(analytic.data$PS)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores. Two methods are explored: using the Matching package and the MatchIt package.\n\nrequire(Matching)\nmatch.obj2 <- Match(Y=analytic.data$cholesterol, \n                    Tr=analytic.data$diabetes, \n                    X=analytic.data$PS, \n                    M=1, \n                    estimand = \"ATT\",\n                    replace=FALSE, \n                    caliper = 0.2)\nsummary(match.obj2)\n#> \n#> Estimate...  -15.287 \n#> SE.........  2.118 \n#> T-stat.....  -7.2175 \n#> p.val......  5.2958e-13 \n#> \n#> Original number of observations..............  4167 \n#> Original number of treated obs...............  791 \n#> Matched number of observations...............  781 \n#> Matched number of observations  (unweighted).  781 \n#> \n#> Caliper (SDs)........................................   0.2 \n#> Number of obs dropped by 'exact' or 'caliper'  10\nmatched.data2 <- analytic.data[c(match.obj2$index.treated, \n                                 match.obj2$index.control),]\ndim(matched.data2)\n#> [1] 1562   16\n\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = analytic.data$PS, \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\nplot(match.obj, type = \"jitter\")\n\n\n\n#> [1] \"To identify the units, use first mouse button; to stop, use second.\"\n#> integer(0)\nplot(match.obj, type = \"hist\")\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08182 0.12644 0.14119 0.18267 0.75047 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02352 0.12362 0.16998 0.19389 0.23171 0.86375\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.019)\n#>  - number of obs.: 4167 (original), 1568 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data2 <- match.data(match.obj)\ndim(matched.data2)\n#> [1] 1568   19\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nbaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1m <- CreateTableOne(strata = \"diabetes\", \n                           vars = baselinevars,\n                           data = matched.data2, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                       784           784               \n#>   gender = Male (%)       405 (51.7)    431 (55.0)   0.067\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.134\n#>      Black                163 (20.8)    182 (23.2)        \n#>      Hispanic             139 (17.7)    170 (21.7)        \n#>      Other                171 (21.8)    157 (20.0)        \n#>      White                311 (39.7)    275 (35.1)        \n#>   education (%)                                      0.040\n#>      College              428 (54.6)    413 (52.7)        \n#>      High.School          274 (34.9)    288 (36.7)        \n#>      School                82 (10.5)     83 (10.6)        \n#>   married (%)                                        0.070\n#>      Married              509 (64.9)    485 (61.9)        \n#>      Never.married         59 ( 7.5)     70 ( 8.9)        \n#>      Previously.married   216 (27.6)    229 (29.2)        \n#>   income (%)                                         0.063\n#>      <25k                 220 (28.1)    220 (28.1)        \n#>      Between.25kto54k     226 (28.8)    242 (30.9)        \n#>      Between.55kto99k     192 (24.5)    173 (22.1)        \n#>      Over100k             146 (18.6)    149 (19.0)        \n#>   bmi (mean (SD))       31.93 (8.16)  32.11 (7.61)   0.023\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample.\n\n# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data2$ID) # matched data\n#> [1] 1568\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data2$ID])\n#> [1] 1568\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data2$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7686 1568\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m2 <- subset(w.design0, matched == 1)\n\n\nout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial, design = w.design.m2)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1568 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.01 \n  \n\n Pseudo-R² (McFadden) \n    0.01 \n  \n\n AIC \n    1918.99 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.47 \n    1.13 \n    1.91 \n    2.88 \n    0.01 \n  \n\n diabetes \n    1.51 \n    1.20 \n    1.89 \n    3.58 \n    0.00 \n  \n\n\n Standard errors: Robust\n\n\n\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore4.html#references",
    "href": "propensityscore4.html#references",
    "title": "PSM in BMI-diabetes",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore5.html",
    "href": "propensityscore5.html",
    "title": "PSM with MI",
    "section": "",
    "text": "The tutorial provides a detailed walkthrough of implementing Propensity Score Matching (PSM) combined with Multiple Imputation (MI) in a statistical analysis, focusing on handling missing data and mitigating bias in observational studies.\nThe initial chunk is dedicated to loading various R packages that will be utilized throughout the tutorial. These libraries provide functions and tools that facilitate data manipulation, statistical modeling, visualization, and more.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\nrequire(data.table)\nrequire(jtools)\nrequire(ggstance)\nrequire(DataExplorer)\nrequire(mitools)\nlibrary(kableExtra)\nlibrary(mice)\n\nProblem Statement\nLogistic regression\n\nPerform multiple imputation to deal with missing values; with 3 imputed datasets, 5 iterations,\nfit survey featured logistic regression in all of the 3 imputed datasets, and\nobtain the pooled OR (adjusted) and the corresponding 95% confidence intervals.\n\nHints\n\nUse the covariates (listed below) in the imputation model.\n\nImputation model covariates can be different than the original analysis covariates. You are encouraged to use variables in the imputation model that can be predictive of the variables with missing observations. In this example, we use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nAlso the imputation model specification can be modified. For example, we use pmm method for bmi in the imputation model.\nRemove any subject ID variable from the imputation model, if created in an intermediate step. Indeed ID variables should not be in the imputation model, if they are not predictive of the variables with missing observations.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPredictive Mean Matching:\nThe “Predictive Mean Matching” (PMM) method in Multiple Imputation (MI) is a widely used technique to handle missing data, particularly well-suited for continuous variables. PMM operates by first creating a predictive model for the variable with missing data, using observed values from other variables in the dataset. For each missing value, PMM identifies a set of observed values with predicted scores that are close to the predicted score for the missing value, derived from the predictive model. Then, instead of imputing a predicted score directly, PMM randomly selects one of the observed values from this set and assigns it as the imputed value. This method retains the original distribution of the imputed variable since it only uses observed values for imputation, and it also tends to preserve relationships between variables. PMM is particularly advantageous when the normality assumption of the imputed variable is questionable, providing a robust and practical approach to managing missing data in various research contexts.\n\n\nPropensity score matching (Zanutto, 2006)\n\nUse the propensity score matching as per Zanutto E. L. (2006)’s recommendation in all of the imputed datasets.\nReport the pooled OR estimates (adjusted) and corresponding 95% confidence intervals (adjusted OR).\nData and variables\nAnalytic data\nThe analytic dataset is saved as NHANES17.RData.\nVariables\nWe are primarily interested in outcome diabetes and exposure whether born in the US (born).\nVariables under consideration:\n\nsurvey features\n\nPSU\nstrata\nsurvey weight\n\n\nCovariates\n\nrace\nage\nmarriage\neducation\ngender\nBMI\nsystolic blood pressure\n\n\nPre-processing\nThe data is loaded and variables of interest are identified.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") # read data\nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\ndim(analytic.with.miss)\n#> [1] 9254   34\nvars <- c(\"ID\", # ID\n          \"psu\", \"strata\", \"weight\", # Survey features \n          \"race\", \"age\", \"married\",\"education\",\"gender\",\"bmi\",\"systolicBP\", # Covariates\n          \"born\", # Exposure\n          \"diabetes\") # Outcome\n\nSubset the dataset\nThe dataset is then subsetted to retain only the relevant variables, ensuring that subsequent analyses are focused and computationally efficient.\n\ndat.with.miss <- analytic.with.miss[,vars]\ndim(analytic.with.miss)\n#> [1] 9254   34\n\nInspect weights\nThe weights of the observations are inspected and adjusted to avoid issues in subsequent analyses.\n\nsummary(dat.with.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12347   21060   34671   37562  419763\n# weight = 0 would create problem in the analysis\n# ad-hoc solution to 0 weight problem\ndat.with.miss$weight[dat.with.miss$weight == 0] <- 0.00000001\n\nRecode the exposure variable\nThe exposure variable is recoded for clarity and ease of interpretation in results.\n\ndat.with.miss$born <- car::recode(dat.with.miss$born, \nrecodes = \" 'Born in 50 US states or Washingt' = \n'Born in US'; 'Others' = 'Others'; else = NA \" )\ndat.with.miss$born <- factor(dat.with.miss$born, levels = c(\"Born in US\", \"Others\"))\n\nvariable types\nVariable types are set, ensuring that each variable is treated appropriately in the analyses.\n\nfactor.names <- c(\"race\", \"married\", \"education\", \"gender\", \"diabetes\")\ndat.with.miss[,factor.names] <- lapply(dat.with.miss[,factor.names], factor)\n\nInspect extent of missing data problem\nA visualization is generated to explore the extent and pattern of missing data in the dataset, which informs the strategy for handling them.\n\nrequire(DataExplorer)\nplot_missing(dat.with.miss)\n\n\n\n\nNote that, multiple imputation then delete (MID) approach can be applied if the outcome had some missing values. Due to the small number of missingness, MICE may not impute the outcomes BTW.\n\n\n\n\n\n\nTip\n\n\n\nMultiple imputation then delete (MID):\nMID is a specific approach used in the context of multiple imputation (MI) when dealing with missing outcome data. All missing values, including those in the outcome variable, are imputed to create several complete datasets. In subsequent analyses, the imputed values for the outcome variable are deleted, so that only observed outcome values are analyzed. Each dataset (with observed outcome values and imputed predictor values) is analyzed separately, and results are pooled to provide a single estimate.\n\n\nLogistic regression\nInitialization\nThe MI process is initialized, setting up the framework for subsequent imputations.\n\nimputation <- mice(data = dat.with.miss, maxit = 0, print = FALSE)\n\nSetting imputation model covariates\nThe predictor matrix is adjusted to specify which variables will be used to predict missing values in the imputation model. Setting strata as auxiliary variable:\n\npred <- imputation$pred\npred\n#>            ID psu strata weight race age married education gender bmi\n#> ID          0   1      1      1    1   1       1         1      1   1\n#> psu         1   0      1      1    1   1       1         1      1   1\n#> strata      1   1      0      1    1   1       1         1      1   1\n#> weight      1   1      1      0    1   1       1         1      1   1\n#> race        1   1      1      1    0   1       1         1      1   1\n#> age         1   1      1      1    1   0       1         1      1   1\n#> married     1   1      1      1    1   1       0         1      1   1\n#> education   1   1      1      1    1   1       1         0      1   1\n#> gender      1   1      1      1    1   1       1         1      0   1\n#> bmi         1   1      1      1    1   1       1         1      1   0\n#> systolicBP  1   1      1      1    1   1       1         1      1   1\n#> born        1   1      1      1    1   1       1         1      1   1\n#> diabetes    1   1      1      1    1   1       1         1      1   1\n#>            systolicBP born diabetes\n#> ID                  1    1        1\n#> psu                 1    1        1\n#> strata              1    1        1\n#> weight              1    1        1\n#> race                1    1        1\n#> age                 1    1        1\n#> married             1    1        1\n#> education           1    1        1\n#> gender              1    1        1\n#> bmi                 1    1        1\n#> systolicBP          0    1        1\n#> born                1    0        1\n#> diabetes            1    1        0\npred[,\"ID\"] <- pred[\"ID\",] <- 0\npred[,\"psu\"] <- pred[\"psu\",] <- 0\npred[,\"weight\"] <- pred[\"weight\",] <- 0\npred[\"strata\",] <- 0\npred\n#>            ID psu strata weight race age married education gender bmi\n#> ID          0   0      0      0    0   0       0         0      0   0\n#> psu         0   0      0      0    0   0       0         0      0   0\n#> strata      0   0      0      0    0   0       0         0      0   0\n#> weight      0   0      0      0    0   0       0         0      0   0\n#> race        0   0      1      0    0   1       1         1      1   1\n#> age         0   0      1      0    1   0       1         1      1   1\n#> married     0   0      1      0    1   1       0         1      1   1\n#> education   0   0      1      0    1   1       1         0      1   1\n#> gender      0   0      1      0    1   1       1         1      0   1\n#> bmi         0   0      1      0    1   1       1         1      1   0\n#> systolicBP  0   0      1      0    1   1       1         1      1   1\n#> born        0   0      1      0    1   1       1         1      1   1\n#> diabetes    0   0      1      0    1   1       1         1      1   1\n#>            systolicBP born diabetes\n#> ID                  0    0        0\n#> psu                 0    0        0\n#> strata              0    0        0\n#> weight              0    0        0\n#> race                1    1        1\n#> age                 1    1        1\n#> married             1    1        1\n#> education           1    1        1\n#> gender              1    1        1\n#> bmi                 1    1        1\n#> systolicBP          0    1        1\n#> born                1    0        1\n#> diabetes            1    1        0\n\nSetting imputation model specification\nThe method for imputing a particular variable is specified (e.g., using Predictive Mean Matching). Here, we add pmm for bmi:\n\nmeth <- imputation$meth\nmeth[\"bmi\"] <- \"pmm\"\n\nImpute incomplete data\nMultiple datasets are imputed, each providing a different “guess” at the missing values, based on observed data. We are imputing m = 3 times.\n\nimputation <- mice(data = dat.with.miss, \n                   seed = 123, \n                   predictorMatrix = pred,\n                   method = meth, \n                   m = 3, \n                   maxit = 5, \n                   print = FALSE)\nimpdata <- mice::complete(imputation, action=\"long\")\nimpdata$.id <- NULL\nm <- 3\nset.seed(123)\nallImputations <-  imputationList(lapply(1:m, \n                                         function(n)\n                                           subset(impdata, \n                                                  subset=.imp==n)))\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 3\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 57 46 66 50 23 66 75 49 56 36 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 2 1 3 3 1 1 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 2 2 1 1 3 1 2 1 3 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 31.2 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 108 96 200 112 128 124 120 122 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 24 49 66 32 34 66 75 80 56 28 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 2 1 3 2 1 1 3 3 1 2 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 2 3 1 1 1 1 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 24.9 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 102 104 136 112 128 120 120 120 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 47 71 66 71 45 66 75 37 56 47 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 1 1 3 1 1 1 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 1 3 1 1 1 2 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 15.9 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 100 114 162 112 128 166 120 116 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\nDesign\nA survey design object is created, ensuring that subsequent analyses appropriately account for the survey design.\n\nw.design <- svydesign(ids = ~psu, weights = ~weight, strata = ~strata,\n                      data = allImputations, nest = TRUE)\n\nSurvey data analysis\nA logistic regression model is fitted to each imputed dataset.\n\nmodel.formula <- as.formula(I(diabetes == 'Yes') ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\nfit.from.logistic <- with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPooled estimates\nResults from models across all imputed datasets are pooled to provide a single estimate, accounting for the uncertainty due to missing data.\n\npooled.estimates <- MIcombine(fit.from.logistic)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#>       MIcombine.default(fit.from.logistic)\n#>                           results      se  (lower  upper) missInfo\n#> (Intercept)               0.00013 7.5e-05 3.9e-05 0.00041     22 %\n#> bornOthers                1.44729 2.7e-01 1.0e+00 2.07384      0 %\n#> raceHispanic              0.81619 1.1e-01 6.3e-01 1.05882      0 %\n#> raceOther                 1.43817 2.6e-01 1.0e+00 2.04954      3 %\n#> raceWhite                 0.86411 1.3e-01 6.5e-01 1.14994      3 %\n#> age                       1.06157 3.6e-03 1.1e+00 1.06874      6 %\n#> marriedNever.married      0.83242 1.6e-01 5.7e-01 1.20809     10 %\n#> marriedPreviously.married 0.88401 1.1e-01 6.8e-01 1.14163     11 %\n#> educationHigh.School      1.16331 1.9e-01 8.4e-01 1.60803      0 %\n#> educationSchool           1.41943 2.4e-01 1.0e+00 1.98397      7 %\n#> genderMale                1.53458 1.8e-01 1.2e+00 1.94217      3 %\n#> bmi                       1.10597 1.2e-02 1.1e+00 1.12956      1 %\n#> systolicBP                1.00325 3.2e-03 1.0e+00 1.01001     39 %\nOR <- round(exp(pooled.estimates$coefficients), 2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR[2,]\n\n\n\n  \n\n\n\nPropensity score matching analysis\nInitialization\nThe MI process is re-initialized to facilitate PSM in the context of MI.\n\nimputation <- mice(data = dat.with.miss, maxit = 0, print = FALSE)\nimpdata <- mice::complete(imputation, action=\"long\")\nm <- 3\nallImputations <- imputationList(lapply(1:m, \n                                        function(n) \n                                          subset(impdata, \n                                                 subset=.imp==n)))\n\nZanutto E. L. (2006) under multiple imputation\n\n\n\n\n\n\nTip\n\n\n\nAn iterative process is performed within each imputed dataset, which involves:\n\nEstimating propensity scores.\nMatching treated and untreated subjects based on these scores.\nExtracting matched data and checking the balance of covariates across matched groups.\nFitting outcome models to the survey weighted matched data and estimating treatment effects.\n\n\n\nNotice that we are performing multi-step process within MI\n\nmatch.statm <- SMDm <- tab1m <- vector(\"list\", m) \nfit.from.PS <- vector(\"list\", m)\n\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  # Rename the weight variable into survey.weight\n  names(analytic.i)[names(analytic.i) == \"weight\"] <- \"survey.weight\"\n  \n  # Specify the PS model to estimate propensity scores\n  ps.formula <- as.formula(I(born==\"Others\") ~ \n                             race + age + married + education + \n                             gender + bmi + systolicBP)\n\n  # Propensity scores\n  ps.fit <- glm(ps.formula, data = analytic.i, family = binomial(\"logit\"))\n  analytic.i$PS <- fitted(ps.fit)\n  \n  # Match exposed and unexposed subjects \n  set.seed(123)\n  match.obj <- matchit(ps.formula, data = analytic.i, \n                       distance = analytic.i$PS, \n                       method = \"nearest\", \n                       replace = FALSE,\n                       caliper = 0.2, \n                       ratio = 1)\n  match.statm[[i]] <- match.obj\n  analytic.i$PS <- match.obj$distance\n  \n  # Extract matched data\n  matched.data <- match.data(match.obj) \n  \n  # Balance checking\n  cov <- c(\"race\", \"age\", \"married\", \"education\", \"gender\", \"bmi\", \"systolicBP\")\n  \n  tab1m[[i]] <- CreateTableOne(strata = \"born\", \n                               vars = cov, data = matched.data, \n                               test = FALSE, smd = TRUE)\n  SMDm[[i]] <- ExtractSmd(tab1m[[i]])\n  \n  # Setup the design with survey features\n  analytic.i$matched <- 0\n  analytic.i$matched[analytic.i$ID %in% matched.data$ID] <- 1\n  \n  # Survey setup for full data\n  w.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                         data = analytic.i, nest = TRUE)\n  \n  # Subset matched data\n  w.design.m <- subset(w.design0, matched == 1)\n  \n  # Outcome model (double adjustment)\n  out.formula <- as.formula(I(diabetes == \"Yes\") ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\n  fit.from.PS[[i]] <- svyglm(out.formula, design = w.design.m, \n                     family = quasibinomial(\"logit\"))\n}\n\nCheck matched data\nThe matched data is inspected to ensure that matching was successful and appropriate.\n\nmatch.statm\n#> [[1]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3590 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n#> \n#> [[2]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3598 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n#> \n#> [[3]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3594 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n\nCheck balance in matched data\nThe balance of covariates across matched groups is assessed to ensure that matching has successfully reduced bias.\n\nSMDm\n#> [[1]]\n#>                 1 vs 2\n#> race       0.028883793\n#> age        0.033614763\n#> married    0.007318561\n#> education  0.117536503\n#> gender     0.040145831\n#> bmi        0.043350560\n#> systolicBP 0.054549772\n#> \n#> [[2]]\n#>                 1 vs 2\n#> race       0.019901420\n#> age        0.016267050\n#> married    0.017196043\n#> education  0.128811588\n#> gender     0.003338016\n#> bmi        0.057014434\n#> systolicBP 0.071553721\n#> \n#> [[3]]\n#>                1 vs 2\n#> race       0.04490482\n#> age        0.01959377\n#> married    0.03687394\n#> education  0.13301810\n#> gender     0.01225625\n#> bmi        0.03697878\n#> systolicBP 0.10025529\n\nPooled estimate\nFinally, the treatment effect estimates from the matched analyses across all imputed datasets are pooled to provide a single, overall estimate, ensuring that the final result appropriately accounts for the uncertainty due to both the matching process and the imputation of missing data.\n\npooled.estimates <- MIcombine(fit.from.PS)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.from.PS)\n#>                           results      se  (lower  upper) missInfo\n#> (Intercept)               8.9e-05 4.9e-05 0.00003 0.00026      8 %\n#> bornOthers                2.0e+00 3.1e-01 1.47719 2.73325     16 %\n#> raceHispanic              7.0e-01 1.7e-01 0.42593 1.15504     27 %\n#> raceOther                 1.4e+00 4.1e-01 0.77209 2.53278     26 %\n#> raceWhite                 4.9e-01 2.7e-01 0.15853 1.52308     28 %\n#> age                       1.1e+00 4.6e-03 1.04472 1.06298      8 %\n#> marriedNever.married      5.9e-01 2.0e-01 0.28824 1.18926     38 %\n#> marriedPreviously.married 1.0e+00 2.5e-01 0.62417 1.64438     11 %\n#> educationHigh.School      1.4e+00 3.0e-01 0.89403 2.10852      2 %\n#> educationSchool           1.3e+00 3.4e-01 0.81042 2.21438      7 %\n#> genderMale                1.3e+00 2.3e-01 0.86695 1.83880     31 %\n#> bmi                       1.1e+00 1.1e-02 1.08062 1.12285      5 %\n#> systolicBP                1.0e+00 3.0e-03 1.00314 1.01494      3 %\nOR <- round(exp(pooled.estimates$coefficients), 2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR[2,]"
  },
  {
    "objectID": "propensityscore6.html",
    "href": "propensityscore6.html",
    "title": "PS Weighting (US)",
    "section": "",
    "text": "Propensity analysis problem\nIn this chapter, we will use propensity score weighting (using SMD cut-point 0.2, may adjust for imbalanced and/or all covariates in the outcome model, if any) analysis as per the following recommendations - (Zanutto 2006) - (DuGoff, Schuler, and Stuart 2014) - (Austin, Jembere, and Chiu 2018)\nDataset\n\nThe following modified NHANES dataset\n\nNHANES15lab5.RData\n\n\nuse the data set “analytic.with.miss” within this file.\n\nfor obtaining the final treatment effect estimates, you can omit missing values, but only after creating the design (e.g., subset the design, not the data itself directly).\n\n\nThe same dataset was used for propensity score weighting\n\nVariables\n\nOutcome: diabetes\n\n‘No’ as the reference category\n\n\nExposure: bmi\n\nconvert to binary with >25 vs. <= 25,\nwith > 25 as the reference category\n\n\nConfounder list:\n\ngender\nage\n\nassume continuous\n\n\nrace\nincome\neducation\nmarried\ncholesterol\ndiastolicBP\nsystolicBP\n\n\nMediator:\n\nphysical.work\n\n‘No’ as the reference category\n\n\n\n\nSurvey features\n\npsu\nstrata\nweight\n\n\nPre-processing\nLoad data\n\nload(file=\"Data/propensityscore/NHANES15lab5.RData\")\n\nVariable summary\n\n# Full data\ndat.full <- analytic.with.miss\n\n# Exposure\ndat.full$bmi <- with(dat.full, ifelse(bmi>25, \"Overweight\", \n                                      ifelse(bmi<=25, \"Not overweight\", NA)))\ndat.full$bmi <- as.factor(dat.full$bmi)\ndat.full$bmi <- relevel(dat.full$bmi, ref = \"Overweight\")\n\n# Drop unnecessary variables \ndat.full$born <- NULL\ndat.full$physical.work <- NULL\n\n# Rename the weight variable into interview.weight\nnames(dat.full)[names(dat.full) == \"weight\"] <- \"interview.weight\"\n\nComplete case data\nWe will use the complete case data to perform the analysis.\n\n# Complete case data \nanalytic.data <- dat.full[complete.cases(dat.full),]\ndim(analytic.data)\n#> [1] 6316   15\n\nReproducibility\n\nset.seed(504)\n\nApproach by Zanutto (2006)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula <- as.formula(I(bmi==\"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\nStep 2\nFor the second step, we will calculate the both unstabilized and stabilized weights. However, stabilized inverse probability weight is often recommended to prevent from extreme weights (Hernán and Robins 2006).\n\n# Propensity scores\nps.fit <- glm(ps.formula, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps <- predict(ps.fit, type = \"response\", newdata = analytic.data)\n\n# Unstabilized weight\nanalytic.data$usweight <- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                     1/ps, 1/(1-ps)))\n\n# Unstabilized weight summary\nround(summary(analytic.data$usweight), 2)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.01    1.33    1.63    2.10    2.14   62.31\n\n# Stabilized weight\nanalytic.data$sweight <- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps)))\n\n# Stabilized weight summary\nround(summary(analytic.data$sweight), 2)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.44    0.70    0.84    1.04    1.08   25.40\n\nWe can see that the mean of stabilized weights is 1, while it is approximately 2.1 for unstabilized weights. For both unstabilized and stabilized weights, it seems there are extreme weights, particularly for the unstabilized weights. Extreme weights could be dealt with weight truncation, typically truncated at the 1st and 99th percentiles (Cole and Hernán 2008).\nLet us truncate the weights at the 1st and 99th percentiles\n\n# Truncating unstabilized weight\nanalytic.data <- analytic.data %>% \n  mutate(usweight_t = pmin(pmax(usweight, quantile(usweight, 0.01)), \n                           quantile(usweight, 0.99)))\nsummary(analytic.data$usweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.055   1.335   1.629   2.020   2.140  10.912\n\n# Truncating stabilized weight\nanalytic.data <- analytic.data %>% \n  mutate(sweight_t = pmin(pmax(sweight, quantile(sweight, 0.01)), \n                          quantile(sweight, 0.99)))\nsummary(analytic.data$sweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.4861  0.7035  0.8400  1.0044  1.0833  4.4901\n\nStep 3\nNow we will check the distribution of the covariates by the exposure status on the pseudo population in terms of pre-specified SMD.\n\n# Covariates\nvars <- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated unstabilized weight\ndesign.unstab <- svydesign(ids = ~ID, weights = ~usweight_t, data = analytic.data)\n\n# Design with truncated stabilized weight\ndesign.stab <- svydesign(ids = ~ID, weights = ~sweight_t, data = analytic.data)\n\n# Balance checking with truncated unstabilized weight\ntab.unstab <- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.unstab, test = F)\nprint(tab.unstab, smd = T)\n#>                          Stratified by bmi\n#>                           Overweight      Not overweight  SMD   \n#>   n                        6199.3          6559.7               \n#>   gender = Male (%)        2999.3 (48.4)   3126.6 (47.7)   0.014\n#>   age (mean (SD))           48.38 (17.38)   49.83 (18.11)  0.082\n#>   race (%)                                                 0.047\n#>      Black                 1304.7 (21.0)   1407.8 (21.5)        \n#>      Hispanic              1901.9 (30.7)   1873.7 (28.6)        \n#>      Other                  956.4 (15.4)   1029.3 (15.7)        \n#>      White                 2036.2 (32.8)   2249.0 (34.3)        \n#>   income (%)                                               0.033\n#>      <25k                  1664.3 (26.8)   1829.5 (27.9)        \n#>      Between.25kto54k      1986.5 (32.0)   2133.5 (32.5)        \n#>      Between.55kto99k      1449.1 (23.4)   1466.6 (22.4)        \n#>      Over100k              1099.4 (17.7)   1130.0 (17.2)        \n#>   education (%)                                            0.019\n#>      College               3469.5 (56.0)   3610.4 (55.0)        \n#>      High.School           2047.7 (33.0)   2217.9 (33.8)        \n#>      School                 682.1 (11.0)    731.4 (11.1)        \n#>   married (%)                                              0.040\n#>      Married               3728.2 (60.1)   3853.6 (58.7)        \n#>      Never.married         1101.9 (17.8)   1147.0 (17.5)        \n#>      Previously.married    1369.2 (22.1)   1559.1 (23.8)        \n#>   cholesterol (mean (SD))  181.58 (40.93)  183.13 (43.59)  0.037\n#>   diastolicBP (mean (SD))   66.31 (14.64)   66.87 (14.78)  0.038\n#>   systolicBP (mean (SD))   121.04 (16.61)  123.60 (23.62)  0.125\n\n# Balance checking with truncated stabilized weight\ntab.stab <- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab, smd = T)\n#>                          Stratified by bmi\n#>                           Overweight      Not overweight  SMD   \n#>   n                        3665.9          2677.9               \n#>   gender = Male (%)        1771.7 (48.3)   1276.5 (47.7)   0.013\n#>   age (mean (SD))           48.40 (17.38)   49.84 (18.12)  0.081\n#>   race (%)                                                 0.048\n#>      Black                  772.6 (21.1)    575.0 (21.5)        \n#>      Hispanic              1126.3 (30.7)    764.6 (28.6)        \n#>      Other                  561.1 (15.3)    420.5 (15.7)        \n#>      White                 1206.0 (32.9)    917.9 (34.3)        \n#>   income (%)                                               0.033\n#>      <25k                   982.8 (26.8)    747.3 (27.9)        \n#>      Between.25kto54k      1176.4 (32.1)    870.8 (32.5)        \n#>      Between.55kto99k       857.6 (23.4)    598.5 (22.3)        \n#>      Over100k               649.2 (17.7)    461.3 (17.2)        \n#>   education (%)                                            0.019\n#>      College               2051.4 (56.0)   1473.3 (55.0)        \n#>      High.School           1210.5 (33.0)    905.9 (33.8)        \n#>      School                 404.0 (11.0)    298.7 (11.2)        \n#>   married (%)                                              0.040\n#>      Married               2205.9 (60.2)   1572.9 (58.7)        \n#>      Never.married          650.0 (17.7)    468.0 (17.5)        \n#>      Previously.married     810.1 (22.1)    637.1 (23.8)        \n#>   cholesterol (mean (SD))  181.62 (40.91)  183.15 (43.61)  0.036\n#>   diastolicBP (mean (SD))   66.37 (14.53)   66.87 (14.81)  0.034\n#>   systolicBP (mean (SD))   121.06 (16.58)  123.63 (23.65)  0.126\n\nAs we can see, all SMDs are less than our specified cut-point of 0.2, indicating that there is good covariate balancing. next, we will fit the outcome model on the pseudo population (i.e., weighted data). Note that we must utilize the survey feature as the design for the population-level estimate. For this step, we will multiply propensity score weight and survey weight and create a new weight variable.\nStep 4 - with unstabilized weight\n\nrequire(survey)\nrequire(jtools)\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] <- 1\n\n# New weight = interview weight * unstabilized weight \nanalytic.data$new.usweight_t <- with(analytic.data, interview.weight * usweight_t)\nsummary(analytic.data$new.usweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    6437   26567   42862   77768   88106 1831548\n\n#  New weight variable in the full dataset\ndat.full$new.usweight_t <- 0\ndat.full$new.usweight_t[dat.full$ID %in% analytic.data$ID] <- \n  analytic.data$new.usweight_t\nsummary(dat.full$new.usweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0   24068   49261   55161 1831548\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~psu, strata = ~strata, weights = ~new.usweight_t, \n                      data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s <- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula <- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit <- svyglm(out.formula, design = w.design.s, family = binomial(\"logit\"))\nsumm(fit, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.062 \n  \n\n Pseudo-R² (McFadden) \n    0.047 \n  \n\n AIC \n    3371.907 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.164 \n    0.141 \n    0.190 \n    -24.213 \n    0.000 \n  \n\n bmiNot overweight \n    0.280 \n    0.217 \n    0.361 \n    -9.814 \n    0.000 \n  \n\n\n Standard errors: Robust\n\n\n\nStep 4 - with stabilized weight\nSimilarly, we can fit the outcome model with stabilized weights.\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] <- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight_t <- with(analytic.data, interview.weight * sweight_t)\nsummary(analytic.data$new.sweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    2985   13311   22494   39174   43896  753678\n\n#  New weight variable in the full dataset\ndat.full$new.sweight_t <- 0\ndat.full$new.sweight_t[dat.full$ID %in% analytic.data$ID] <- analytic.data$new.sweight_t\nsummary(dat.full$new.sweight_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0   12115   24814   28232  753678\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 <- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula <- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab <- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.055 \n  \n\n Pseudo-R² (McFadden) \n    0.041 \n  \n\n AIC \n    3712.451 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.164 \n    0.141 \n    0.190 \n    -24.235 \n    0.000 \n  \n\n bmiNot overweight \n    0.280 \n    0.217 \n    0.361 \n    -9.815 \n    0.000 \n  \n\n\n Standard errors: Robust\n\n\n\nDouble adjustment\n\nlibrary(survey)\n# Outcome model with covariates adjustment\nfit.DA <- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.194 \n  \n\n Pseudo-R² (McFadden) \n    0.149 \n  \n\n AIC \n    3331.238 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.045 \n    0.015 \n    0.133 \n    -5.608 \n    NA \n  \n\n bmiNot overweight \n    0.234 \n    0.175 \n    0.312 \n    -9.826 \n    NA \n  \n\n genderMale \n    1.402 \n    1.190 \n    1.652 \n    4.038 \n    NA \n  \n\n age \n    1.050 \n    1.041 \n    1.058 \n    11.793 \n    NA \n  \n\n raceHispanic \n    0.790 \n    0.600 \n    1.040 \n    -1.677 \n    NA \n  \n\n raceOther \n    0.849 \n    0.428 \n    1.683 \n    -0.470 \n    NA \n  \n\n raceWhite \n    0.541 \n    0.371 \n    0.789 \n    -3.192 \n    NA \n  \n\n incomeBetween.25kto54k \n    0.723 \n    0.478 \n    1.093 \n    -1.540 \n    NA \n  \n\n incomeBetween.55kto99k \n    0.647 \n    0.411 \n    1.018 \n    -1.883 \n    NA \n  \n\n incomeOver100k \n    0.476 \n    0.309 \n    0.733 \n    -3.371 \n    NA \n  \n\n educationHigh.School \n    0.937 \n    0.725 \n    1.211 \n    -0.498 \n    NA \n  \n\n educationSchool \n    0.930 \n    0.568 \n    1.524 \n    -0.286 \n    NA \n  \n\n marriedNever.married \n    0.895 \n    0.628 \n    1.275 \n    -0.615 \n    NA \n  \n\n marriedPreviously.married \n    0.791 \n    0.529 \n    1.184 \n    -1.137 \n    NA \n  \n\n cholesterol \n    0.993 \n    0.990 \n    0.997 \n    -3.760 \n    NA \n  \n\n diastolicBP \n    1.007 \n    0.999 \n    1.014 \n    1.744 \n    NA \n  \n\n systolicBP \n    1.002 \n    0.993 \n    1.012 \n    0.529 \n    NA \n  \n\n\n Standard errors: Robust\n\n\n\n# Log odds ratio with p-values\nsummary(fit.DA, df.resid = degf(w.design.s2))\n#> \n#> Call:\n#> svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#>     race + income + education + married + cholesterol + diastolicBP + \n#>     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#> \n#> Survey design:\n#> subset(w.design0, ind == 1)\n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               -3.095813   0.552073  -5.608 4.99e-05 ***\n#> bmiNot overweight         -1.454514   0.148024  -9.826 6.29e-08 ***\n#> genderMale                 0.338112   0.083733   4.038  0.00107 ** \n#> age                        0.048468   0.004110  11.793 5.48e-09 ***\n#> raceHispanic              -0.235285   0.140287  -1.677  0.11422    \n#> raceOther                 -0.164132   0.349277  -0.470  0.64517    \n#> raceWhite                 -0.613643   0.192233  -3.192  0.00606 ** \n#> incomeBetween.25kto54k    -0.324911   0.211048  -1.540  0.14451    \n#> incomeBetween.55kto99k    -0.435472   0.231288  -1.883  0.07927 .  \n#> incomeOver100k            -0.742995   0.220399  -3.371  0.00420 ** \n#> educationHigh.School      -0.065157   0.130721  -0.498  0.62540    \n#> educationSchool           -0.072034   0.251806  -0.286  0.77874    \n#> marriedNever.married      -0.110932   0.180496  -0.615  0.54803    \n#> marriedPreviously.married -0.233875   0.205646  -1.137  0.27327    \n#> cholesterol               -0.006873   0.001828  -3.760  0.00189 ** \n#> diastolicBP                0.006728   0.003859   1.744  0.10169    \n#> systolicBP                 0.002438   0.004605   0.529  0.60430    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero or negative residual df; p-values not defined\n#> \n#> (Dispersion parameter for binomial family taken to be 0.9452878)\n#> \n#> Number of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\nTip\n\n\n\nDouble adjustment:\nAs explained in the previous chapter, double adjustment should be applied thoughtfully, with careful consideration of model specification, covariate selection, and underlying assumptions to ensure valid and reliable results. Always consider the specific context of the study and consult statistical guidelines or experts when applying advanced methods like double adjustment in propensity score analysis.\n\n\nApproach by DuGoff et al. (2014)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula2 <- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP + \n                           psu + strata + interview.weight)\n\nStep 2\n\n# Propensity scores\nps.fit2 <- glm(ps.formula2, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps2 <- fitted(ps.fit2)\n\n# Stabilized weight\nanalytic.data$sweight.dug <- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps2, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps2)))\nsummary(analytic.data$sweight.dug)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.4349  0.6942  0.8296  1.0307  1.0859 23.0226\n\n# Truncating stabilized weight\nanalytic.data <- analytic.data %>% \n  mutate(sweight.dug_t = pmin(pmax(sweight.dug, quantile(sweight.dug, 0.01)), \n                          quantile(sweight.dug, 0.99)))\nsummary(analytic.data$sweight.dug_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.4802  0.6942  0.8296  1.0019  1.0859  4.4041\n\nStep 3\n\n# Balance checking\ncov2 <- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab <- svydesign(ids = ~ID, weights = ~sweight.dug_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab2 <- svyCreateTableOne(vars = cov2, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab2, smd = T)\n#>                          Stratified by bmi\n#>                           Overweight      Not overweight  SMD   \n#>   n                        3652.9          2675.0               \n#>   gender = Male (%)        1766.5 (48.4)   1276.8 (47.7)   0.013\n#>   age (mean (SD))           48.47 (17.42)   49.80 (17.99)  0.075\n#>   race (%)                                                 0.056\n#>      Black                  769.1 (21.1)    568.7 (21.3)        \n#>      Hispanic              1121.5 (30.7)    755.4 (28.2)        \n#>      Other                  553.0 (15.1)    417.0 (15.6)        \n#>      White                 1209.4 (33.1)    933.9 (34.9)        \n#>   income (%)                                               0.023\n#>      <25k                   982.1 (26.9)    734.0 (27.4)        \n#>      Between.25kto54k      1172.1 (32.1)    871.1 (32.6)        \n#>      Between.55kto99k       846.1 (23.2)    597.0 (22.3)        \n#>      Over100k               652.6 (17.9)    472.9 (17.7)        \n#>   education (%)                                            0.012\n#>      College               2049.1 (56.1)   1484.2 (55.5)        \n#>      High.School           1202.5 (32.9)    893.7 (33.4)        \n#>      School                 401.3 (11.0)    297.1 (11.1)        \n#>   married (%)                                              0.035\n#>      Married               2201.3 (60.3)   1581.2 (59.1)        \n#>      Never.married          647.4 (17.7)    465.6 (17.4)        \n#>      Previously.married     804.2 (22.0)    628.2 (23.5)        \n#>   cholesterol (mean (SD))  181.74 (40.94)  182.97 (43.25)  0.029\n#>   diastolicBP (mean (SD))   66.46 (14.42)   66.88 (14.79)  0.029\n#>   systolicBP (mean (SD))   121.14 (16.59)  123.39 (23.38)  0.111\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] <- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.dug_t <- with(analytic.data, interview.weight * sweight.dug_t)\nsummary(analytic.data$new.sweight.dug_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    2996   13343   22260   39436   43914  855145\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.dug_t <- 0\ndat.full$new.sweight.dug_t[dat.full$ID %in% analytic.data$ID] <- analytic.data$new.sweight.dug_t\nsummary(dat.full$new.sweight.dug_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0   12058   24980   28225  855145\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.dug_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 <- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula <- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.dug <- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.dug, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.060 \n  \n\n Pseudo-R² (McFadden) \n    0.045 \n  \n\n AIC \n    3518.691 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.161 \n    0.140 \n    0.185 \n    -26.027 \n    0.000 \n  \n\n bmiNot overweight \n    0.272 \n    0.203 \n    0.364 \n    -8.727 \n    0.000 \n  \n\n\n Standard errors: Robust\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2.DA <- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit2.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.196 \n  \n\n Pseudo-R² (McFadden) \n    0.152 \n  \n\n AIC \n    3154.781 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.043 \n    0.014 \n    0.131 \n    -5.537 \n    NA \n  \n\n bmiNot overweight \n    0.235 \n    0.169 \n    0.325 \n    -8.730 \n    NA \n  \n\n genderMale \n    1.393 \n    1.150 \n    1.688 \n    3.381 \n    NA \n  \n\n age \n    1.049 \n    1.042 \n    1.056 \n    13.564 \n    NA \n  \n\n raceHispanic \n    0.813 \n    0.624 \n    1.061 \n    -1.524 \n    NA \n  \n\n raceOther \n    0.841 \n    0.454 \n    1.558 \n    -0.549 \n    NA \n  \n\n raceWhite \n    0.535 \n    0.369 \n    0.774 \n    -3.316 \n    NA \n  \n\n incomeBetween.25kto54k \n    0.720 \n    0.492 \n    1.053 \n    -1.693 \n    NA \n  \n\n incomeBetween.55kto99k \n    0.659 \n    0.440 \n    0.985 \n    -2.035 \n    NA \n  \n\n incomeOver100k \n    0.491 \n    0.338 \n    0.713 \n    -3.735 \n    NA \n  \n\n educationHigh.School \n    0.930 \n    0.753 \n    1.150 \n    -0.667 \n    NA \n  \n\n educationSchool \n    0.994 \n    0.625 \n    1.580 \n    -0.026 \n    NA \n  \n\n marriedNever.married \n    0.982 \n    0.671 \n    1.439 \n    -0.092 \n    NA \n  \n\n marriedPreviously.married \n    0.783 \n    0.534 \n    1.150 \n    -1.245 \n    NA \n  \n\n cholesterol \n    0.993 \n    0.989 \n    0.996 \n    -3.913 \n    NA \n  \n\n diastolicBP \n    1.006 \n    0.998 \n    1.013 \n    1.426 \n    NA \n  \n\n systolicBP \n    1.004 \n    0.995 \n    1.012 \n    0.853 \n    NA \n  \n\n\n Standard errors: Robust\n\n\n\n# Log odds ratio with p-values\nsummary(fit2.DA, df.resid = degf(w.design.s2))\n#> \n#> Call:\n#> svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#>     race + income + education + married + cholesterol + diastolicBP + \n#>     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#> \n#> Survey design:\n#> subset(w.design0, ind == 1)\n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               -3.143943   0.567807  -5.537 5.70e-05 ***\n#> bmiNot overweight         -1.450047   0.166099  -8.730 2.89e-07 ***\n#> genderMale                 0.331561   0.098056   3.381  0.00411 ** \n#> age                        0.047790   0.003523  13.564 7.97e-10 ***\n#> raceHispanic              -0.206657   0.135568  -1.524  0.14822    \n#> raceOther                 -0.172590   0.314341  -0.549  0.59105    \n#> raceWhite                 -0.625821   0.188746  -3.316  0.00471 ** \n#> incomeBetween.25kto54k    -0.328863   0.194223  -1.693  0.11107    \n#> incomeBetween.55kto99k    -0.417687   0.205223  -2.035  0.05989 .  \n#> incomeOver100k            -0.711500   0.190509  -3.735  0.00199 ** \n#> educationHigh.School      -0.072053   0.108040  -0.667  0.51496    \n#> educationSchool           -0.006136   0.236588  -0.026  0.97965    \n#> marriedNever.married      -0.017832   0.194660  -0.092  0.92822    \n#> marriedPreviously.married -0.244058   0.196041  -1.245  0.23226    \n#> cholesterol               -0.007067   0.001806  -3.913  0.00138 ** \n#> diastolicBP                0.005516   0.003869   1.426  0.17440    \n#> systolicBP                 0.003739   0.004381   0.853  0.40682    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero or negative residual df; p-values not defined\n#> \n#> (Dispersion parameter for binomial family taken to be 0.9373219)\n#> \n#> Number of Fisher Scoring iterations: 6\n\nApproach by Austin et al. (2018)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula3 <- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\n# Survey design\nrequire(survey)\nanalytic.design <- svydesign(id = ~psu, weights = ~interview.weight, strata = ~strata,\n                             data = analytic.data, nest = TRUE)\n\nStep 2\n\n# Propensity scores\nps.fit3 <- svyglm(ps.formula3, design = analytic.design, family = binomial(\"logit\"))\nanalytic.data$ps3 <- fitted(ps.fit3)\n\n# Stabilized weight\nanalytic.data$sweight.aus <- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps3, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps3)))\nsummary(analytic.data$sweight.aus)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.4436  0.7138  0.8410  1.0392  1.0645 26.2776\n\n# Truncating stabilized weight\nanalytic.data <- analytic.data %>% \n  mutate(sweight.aus_t = pmin(pmax(sweight.aus, quantile(sweight.aus, 0.01)), \n                          quantile(sweight.aus, 0.99)))\nsummary(analytic.data$sweight.aus_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.5198  0.7138  0.8410  1.0071  1.0645  4.8830\n\nStep 3\n\n# Balance checking\nvars <- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n          \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab <- svydesign(ids = ~ID, weights = ~sweight.aus_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab.aus <- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab.aus, smd = T)\n#>                          Stratified by bmi\n#>                           Overweight      Not overweight  SMD   \n#>   n                        3434.3          2926.8               \n#>   gender = Male (%)        1609.8 (46.9)   1458.6 (49.8)   0.059\n#>   age (mean (SD))           48.51 (17.32)   49.95 (18.14)  0.081\n#>   race (%)                                                 0.099\n#>      Black                  737.1 (21.5)    623.8 (21.3)        \n#>      Hispanic              1085.0 (31.6)    826.0 (28.2)        \n#>      Other                  471.2 (13.7)    489.7 (16.7)        \n#>      White                 1141.0 (33.2)    987.3 (33.7)        \n#>   income (%)                                               0.041\n#>      <25k                   933.7 (27.2)    813.0 (27.8)        \n#>      Between.25kto54k      1108.2 (32.3)    946.8 (32.3)        \n#>      Between.55kto99k       776.7 (22.6)    685.4 (23.4)        \n#>      Over100k               615.7 (17.9)    481.6 (16.5)        \n#>   education (%)                                            0.039\n#>      College               1908.2 (55.6)   1601.1 (54.7)        \n#>      High.School           1129.9 (32.9)   1011.4 (34.6)        \n#>      School                 396.2 (11.5)    314.3 (10.7)        \n#>   married (%)                                              0.030\n#>      Married               2063.9 (60.1)   1737.6 (59.4)        \n#>      Never.married          605.8 (17.6)    501.4 (17.1)        \n#>      Previously.married     764.6 (22.3)    687.8 (23.5)        \n#>   cholesterol (mean (SD))  182.67 (41.11)  182.70 (43.38)  0.001\n#>   diastolicBP (mean (SD))   66.76 (14.28)   66.85 (14.85)  0.006\n#>   systolicBP (mean (SD))   121.39 (16.74)  123.98 (23.72)  0.126\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] <- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.aus_t <- with(analytic.data, interview.weight * sweight.aus_t)\nsummary(analytic.data$new.sweight.aus_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    3255   13604   22527   38953   43663  819622\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.aus_t <- 0\ndat.full$new.sweight.aus_t[dat.full$ID %in% analytic.data$ID] <- analytic.data$new.sweight.aus_t\nsummary(dat.full$new.sweight.aus_t)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0   12356   24675   28169  819622\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.aus_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 <- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula <- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.aus <- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.aus, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.055 \n  \n\n Pseudo-R² (McFadden) \n    0.041 \n  \n\n AIC \n    3622.951 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.162 \n    0.140 \n    0.187 \n    -24.912 \n    0.000 \n  \n\n bmiNot overweight \n    0.291 \n    0.225 \n    0.377 \n    -9.397 \n    0.000 \n  \n\n\n Standard errors: Robust\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit3.DA <- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit3.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    6316 \n  \n\n Dependent variable \n    I(diabetes == \"Yes\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.193 \n  \n\n Pseudo-R² (McFadden) \n    0.149 \n  \n\n AIC \n    3247.730 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    0.045 \n    0.015 \n    0.129 \n    -5.735 \n    NA \n  \n\n bmiNot overweight \n    0.233 \n    0.175 \n    0.311 \n    -9.886 \n    NA \n  \n\n genderMale \n    1.416 \n    1.186 \n    1.690 \n    3.850 \n    NA \n  \n\n age \n    1.049 \n    1.041 \n    1.057 \n    12.066 \n    NA \n  \n\n raceHispanic \n    0.773 \n    0.585 \n    1.022 \n    -1.808 \n    NA \n  \n\n raceOther \n    0.867 \n    0.451 \n    1.666 \n    -0.428 \n    NA \n  \n\n raceWhite \n    0.520 \n    0.353 \n    0.765 \n    -3.316 \n    NA \n  \n\n incomeBetween.25kto54k \n    0.700 \n    0.461 \n    1.062 \n    -1.677 \n    NA \n  \n\n incomeBetween.55kto99k \n    0.650 \n    0.407 \n    1.039 \n    -1.801 \n    NA \n  \n\n incomeOver100k \n    0.467 \n    0.303 \n    0.721 \n    -3.437 \n    NA \n  \n\n educationHigh.School \n    0.948 \n    0.744 \n    1.209 \n    -0.428 \n    NA \n  \n\n educationSchool \n    0.976 \n    0.598 \n    1.592 \n    -0.099 \n    NA \n  \n\n marriedNever.married \n    0.896 \n    0.629 \n    1.276 \n    -0.609 \n    NA \n  \n\n marriedPreviously.married \n    0.784 \n    0.525 \n    1.169 \n    -1.195 \n    NA \n  \n\n cholesterol \n    0.993 \n    0.990 \n    0.997 \n    -3.653 \n    NA \n  \n\n diastolicBP \n    1.005 \n    0.998 \n    1.013 \n    1.412 \n    NA \n  \n\n systolicBP \n    1.004 \n    0.995 \n    1.012 \n    0.826 \n    NA \n  \n\n\n Standard errors: Robust\n\n\n\n# Log odds ratio with p-values\nsummary(fit3.DA, df.resid = degf(w.design.s2))\n#> \n#> Call:\n#> svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#>     race + income + education + married + cholesterol + diastolicBP + \n#>     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#> \n#> Survey design:\n#> subset(w.design0, ind == 1)\n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               -3.109181   0.542121  -5.735 3.94e-05 ***\n#> bmiNot overweight         -1.456869   0.147369  -9.886 5.81e-08 ***\n#> genderMale                 0.347655   0.090292   3.850  0.00157 ** \n#> age                        0.047457   0.003933  12.066 4.01e-09 ***\n#> raceHispanic              -0.257364   0.142343  -1.808  0.09069 .  \n#> raceOther                 -0.142755   0.333175  -0.428  0.67440    \n#> raceWhite                 -0.653902   0.197190  -3.316  0.00470 ** \n#> incomeBetween.25kto54k    -0.357122   0.212996  -1.677  0.11432    \n#> incomeBetween.55kto99k    -0.430289   0.238964  -1.801  0.09190 .  \n#> incomeOver100k            -0.761311   0.221498  -3.437  0.00367 ** \n#> educationHigh.School      -0.052990   0.123816  -0.428  0.67475    \n#> educationSchool           -0.024734   0.249884  -0.099  0.92246    \n#> marriedNever.married      -0.109989   0.180532  -0.609  0.55148    \n#> marriedPreviously.married -0.243725   0.203990  -1.195  0.25072    \n#> cholesterol               -0.006605   0.001808  -3.653  0.00235 ** \n#> diastolicBP                0.005253   0.003721   1.412  0.17842    \n#> systolicBP                 0.003618   0.004380   0.826  0.42172    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero or negative residual df; p-values not defined\n#> \n#> (Dispersion parameter for binomial family taken to be 0.9416613)\n#> \n#> Number of Fisher Scoring iterations: 6\n\nReferences\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nCole, Stephen R, and Miguel A Hernán. 2008. “Constructing Inverse Probability Weights for Marginal Structural Models.” American Journal of Epidemiology 168 (6): 656–64.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nHernán, Miguel A, and James M Robins. 2006. “Estimating Causal Effects from Epidemiological Data.” Journal of Epidemiology and Community Health 60 (7): 578.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore7.html",
    "href": "propensityscore7.html",
    "title": "PSM with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score matching (PSM) with multiple imputation, focusing on specific subpopulations defined by the study’s eligibility criteria. We will use PSM as per DuGoff, Schuler, and Stuart (2014) recommendation, with SMD cut-point 0.2 and adjust for imbalanced and/or all covariates in the outcome model, if any.\nThe modified dataset from NHANES 2017- 2018, which was also used in missing data subpopulations chapter, will be used. This example aims to demonstrate how to do the missing data analysis using multiple imputation with PSM in the context of complex surveys.\nPre-processing\nLoad data\nLet us import the dataset:\n\nload(\"Data/missingdata/MIexample.RData\")\nls()\n#> [1] \"dat.full\"\n\nVariables\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nData pre-processng\n\n# Categorical age\ndat.full$age.cat <- with(dat.full, ifelse(age >= 20 & age < 50, \"20-49\", \n                                  ifelse(age >= 50 & age < 65, \"50-64\", \"65+\")))\ndat.full$age.cat <- factor(dat.full$age.cat, levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.full$age.cat, useNA = \"always\")\n#> \n#> 20-49 50-64   65+  <NA> \n#>  2500  1569  5185     0\n\n# Recode rheumatoid to arthritis\ndat.full$arthritis <- car::recode(dat.full$rheumatoid, \" 'No' = 'No arthritis';\n                                      'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.full$arthritis, useNA = \"always\")\n#> \n#>         No arthritis Rheumatoid arthritis                 <NA> \n#>                 3857                  337                 5060\n\nSubsetting according to eligibility\nWe will create the analytic dataset with\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\n# Drop < 20 years\ndat.with.miss <- subset(dat.full, age >= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#> \n#>   No  Yes <NA> \n#> 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#> \n#>   No  Yes <NA> \n#> 3857  337 1375\n\n# Drop missing in outcome and exposure - dataset with missing values only in covariates\ndat.analytic <- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#> [1] 4191\n\nWe have 4,191 participants in our analytic dataset. The general strategy of solution to implement PSM with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nApply PSM on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin’s rule\nvariable summary\nLet us see the summary statistics as we did in the missing data analysis:\n\n# Keep only relevant variables\nvars <-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\", \"arthritis\", \"age.cat\", \n           \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 <- dat.analytic[, vars]\n\n# Create Table 1\nvars <- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 <- CreateTableOne(vars = vars, strata = \"cvd\", data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#>                  Stratified by cvd\n#>                   level                     Overall      No          \n#>   n                                          4191         3823       \n#>   arthritis       No arthritis               3854         3580       \n#>                   Rheumatoid arthritis        337          243       \n#>   age.cat         20-49                      2280         2240       \n#>                   50-64                      1097          979       \n#>                   65+                         814          604       \n#>   sex             Male                       2126         1884       \n#>                   Female                     2065         1939       \n#>   education       Less than high school       828          728       \n#>                   High school                2292         2094       \n#>                   College graduate or above  1063          993       \n#>   race            White                      1275         1113       \n#>                   Black                       998          898       \n#>                   Hispanic                   1015          958       \n#>                   Others                      903          854       \n#>   income          less than $20,000           659          557       \n#>                   $20,000 to $74,999         1967         1796       \n#>                   $75,000 and Over           1143         1079       \n#>   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#>   smoking         Never smoker               2570         2427       \n#>                   Previous smoker             882          726       \n#>                   Current smoker              739          670       \n#>   htn             No                         1424         1380       \n#>                   Yes                        2415         2107       \n#>   diabetes        No                         3622         3396       \n#>                   Yes                         566          424       \n#>                  Stratified by cvd\n#>                   Yes         \n#>   n                 368       \n#>   arthritis         274       \n#>                      94       \n#>   age.cat            40       \n#>                     118       \n#>                     210       \n#>   sex               242       \n#>                     126       \n#>   education         100       \n#>                     198       \n#>                      70       \n#>   race              162       \n#>                     100       \n#>                      57       \n#>                      49       \n#>   income            102       \n#>                     171       \n#>                      64       \n#>   bmi (mean (SD)) 30.09 (7.29)\n#>   smoking           143       \n#>                     156       \n#>                      69       \n#>   htn                44       \n#>                     308       \n#>   diabetes          226       \n#>                     142\n\n\n# missingness\nDataExplorer::plot_missing(dat.analytic2)\n\n\n\n\nDealing with missing values in covariates\nSimilar to the previous exercise, we will create 5 imputed datasets with 3 iterations, and the predictive mean matching method for bmi and income. We will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nStep 0: Set up the imputation model\n\n# Step 0: Set imputation model\nini <- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred <- ini$pred\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",] <- 0\n\n# Do not use survey weight or PSU variable as auxiliary variables\npred[,\"studyid\"] <- pred[\"studyid\",] <- 0\npred[,\"psu\"] <- pred[\"psu\",] <- 0\npred[,\"survey.weight\"] <- pred[\"survey.weight\",] <- 0\n\n# Set imputation method\nmeth <- ini$meth\nmeth[\"bmi\"] <- \"pmm\"\nmeth[\"income\"] <- \"pmm\"\nmeth\n#>       studyid survey.weight           psu        strata           cvd \n#>            \"\"            \"\"            \"\"            \"\"            \"\" \n#>     arthritis       age.cat           sex     education          race \n#>            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#>        income           bmi       smoking           htn      diabetes \n#>         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\nStep 1: Imputing missing values using mice for eligible subjects\n\n\n\n\n\n\n\n# Step 1: impute the incomplete data\nimputation <- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\nLet us save the datasets.\n\nsave(dat.full, dat.analytic, dat.analytic2, imputation, \n     file = \"Data/propensityscore/analytic_imputed.RData\")\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nimpdata <- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#> [1] 20955    17\n\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id <- NULL\n\n# Number of subjects\nnrow(impdata)\n#> [1] 20955\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n\n\n\n\nThere is no missing value after imputation. There is an additional variable (.imp) in the imputed dataset, which goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\nStep 2: PSM steps 1-3 by DuGoff et al. (2014)\nOur next step is to use steps 1-3 of the PSM analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Match an exposed subject without replacement within the caliper of 0.2 times the standard deviation of the logit of PS.\nStep 2.3: Balance checking using SMD. Consider SMD <0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula <- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and matching for each imputed dataset\n\n# Null vector or list to store values\ncaliper <- NULL\ndat.matched <- match.obj <- list(NULL)\n\nm <- 5 # 5 imputed dataset\n\n# PSM on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed <- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit <- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps <- fitted(ps.fit)\n  \n  # Caliper fixing to 0.2*sd(logit of PS)\n  caliper[ii] <- 0.2*sd(log(dat.imputed$ps/(1-dat.imputed$ps)))\n  \n  # 1:1 PS matching  \n  set.seed(504)\n  match.obj[[ii]] <- matchit(ps.formula, data = dat.imputed,\n                        distance = dat.imputed$ps, \n                        method = \"nearest\", \n                        replace = FALSE,\n                        caliper = caliper[ii], \n                        ratio = 1)\n  dat.imputed$ps <- match.obj[[ii]]$distance\n  \n  # Extract matched data\n  dat.matched[[ii]] <- match.data(match.obj[[ii]]) \n}\nmatch.obj\n#> [[1]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4191 (original), 668 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#> \n#> [[2]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.02)\n#>  - number of obs.: 4191 (original), 666 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#> \n#> [[3]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4191 (original), 672 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#> \n#> [[4]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4191 (original), 664 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#> \n#> [[5]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4191 (original), 662 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n\n\n# Dimension of each of the matched dataset\nlapply(dat.matched, dim)\n#> [[1]]\n#> [1] 668  20\n#> \n#> [[2]]\n#> [1] 666  20\n#> \n#> [[3]]\n#> [1] 672  20\n#> \n#> [[4]]\n#> [1] 664  20\n#> \n#> [[5]]\n#> [1] 662  20\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m <- list(NULL)\nfor (ii in 1:m) {\n  # Matched data\n  dat <- dat.matched[[ii]]\n  \n  # Covariates\n  vars <- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Balance checking \n  tab1m[[ii]] <- CreateTableOne(strata = \"arthritis\", vars = vars, data = dat, test = F)\n}\nprint(tab1m, smd = TRUE)\n#> [[1]]\n#>                               Stratified by arthritis\n#>                                No arthritis  Rheumatoid arthritis SMD   \n#>   n                              334           334                      \n#>   age.cat (%)                                                      0.018\n#>      20-49                        51 (15.3)     52 (15.6)               \n#>      50-64                       131 (39.2)    133 (39.8)               \n#>      65+                         152 (45.5)    149 (44.6)               \n#>   sex = Female (%)               172 (51.5)    178 (53.3)          0.036\n#>   education (%)                                                    0.034\n#>      Less than high school        80 (24.0)     84 (25.1)               \n#>      High school                 204 (61.1)    203 (60.8)               \n#>      College graduate or above    50 (15.0)     47 (14.1)               \n#>   race (%)                                                         0.049\n#>      White                       105 (31.4)    103 (30.8)               \n#>      Black                       107 (32.0)    112 (33.5)               \n#>      Hispanic                     67 (20.1)     69 (20.7)               \n#>      Others                       55 (16.5)     50 (15.0)               \n#>   income (%)                                                       0.073\n#>      less than $20,000            95 (28.4)    100 (29.9)               \n#>      $20,000 to $74,999          162 (48.5)    167 (50.0)               \n#>      $75,000 and Over             77 (23.1)     67 (20.1)               \n#>   bmi (mean (SD))              30.62 (8.16)  30.54 (7.34)          0.011\n#>   smoking (%)                                                      0.052\n#>      Never smoker                158 (47.3)    161 (48.2)               \n#>      Previous smoker             102 (30.5)    106 (31.7)               \n#>      Current smoker               74 (22.2)     67 (20.1)               \n#>   htn = Yes (%)                  283 (84.7)    279 (83.5)          0.033\n#>   diabetes = Yes (%)             106 (31.7)    100 (29.9)          0.039\n#> \n#> [[2]]\n#>                               Stratified by arthritis\n#>                                No arthritis  Rheumatoid arthritis SMD   \n#>   n                              333           333                      \n#>   age.cat (%)                                                      0.055\n#>      20-49                        50 (15.0)     52 (15.6)               \n#>      50-64                       142 (42.6)    133 (39.9)               \n#>      65+                         141 (42.3)    148 (44.4)               \n#>   sex = Female (%)               172 (51.7)    176 (52.9)          0.024\n#>   education (%)                                                    0.068\n#>      Less than high school        92 (27.6)     84 (25.2)               \n#>      High school                 191 (57.4)    202 (60.7)               \n#>      College graduate or above    50 (15.0)     47 (14.1)               \n#>   race (%)                                                         0.146\n#>      White                        91 (27.3)    104 (31.2)               \n#>      Black                       100 (30.0)    110 (33.0)               \n#>      Hispanic                     79 (23.7)     69 (20.7)               \n#>      Others                       63 (18.9)     50 (15.0)               \n#>   income (%)                                                       0.131\n#>      less than $20,000           115 (34.5)     96 (28.8)               \n#>      $20,000 to $74,999          166 (49.8)    175 (52.6)               \n#>      $75,000 and Over             52 (15.6)     62 (18.6)               \n#>   bmi (mean (SD))              30.50 (7.88)  30.51 (7.46)          0.001\n#>   smoking (%)                                                      0.015\n#>      Never smoker                160 (48.0)    161 (48.3)               \n#>      Previous smoker             104 (31.2)    105 (31.5)               \n#>      Current smoker               69 (20.7)     67 (20.1)               \n#>   htn = Yes (%)                  288 (86.5)    278 (83.5)          0.084\n#>   diabetes = Yes (%)              97 (29.1)     99 (29.7)          0.013\n#> \n#> [[3]]\n#>                               Stratified by arthritis\n#>                                No arthritis  Rheumatoid arthritis SMD   \n#>   n                              336           336                      \n#>   age.cat (%)                                                      0.077\n#>      20-49                        46 (13.7)     52 (15.5)               \n#>      50-64                       145 (43.2)    133 (39.6)               \n#>      65+                         145 (43.2)    151 (44.9)               \n#>   sex = Female (%)               176 (52.4)    179 (53.3)          0.018\n#>   education (%)                                                    0.076\n#>      Less than high school        80 (23.8)     86 (25.6)               \n#>      High school                 215 (64.0)    203 (60.4)               \n#>      College graduate or above    41 (12.2)     47 (14.0)               \n#>   race (%)                                                         0.084\n#>      White                       114 (33.9)    105 (31.2)               \n#>      Black                       114 (33.9)    112 (33.3)               \n#>      Hispanic                     59 (17.6)     69 (20.5)               \n#>      Others                       49 (14.6)     50 (14.9)               \n#>   income (%)                                                       0.118\n#>      less than $20,000            87 (25.9)    104 (31.0)               \n#>      $20,000 to $74,999          183 (54.5)    166 (49.4)               \n#>      $75,000 and Over             66 (19.6)     66 (19.6)               \n#>   bmi (mean (SD))              30.56 (7.71)  30.54 (7.53)          0.003\n#>   smoking (%)                                                      0.127\n#>      Never smoker                180 (53.6)    161 (47.9)               \n#>      Previous smoker              89 (26.5)    107 (31.8)               \n#>      Current smoker               67 (19.9)     68 (20.2)               \n#>   htn = Yes (%)                  280 (83.3)    281 (83.6)          0.008\n#>   diabetes = Yes (%)             104 (31.0)    102 (30.4)          0.013\n#> \n#> [[4]]\n#>                               Stratified by arthritis\n#>                                No arthritis  Rheumatoid arthritis SMD   \n#>   n                              332           332                      \n#>   age.cat (%)                                                      0.062\n#>      20-49                        48 (14.5)     52 (15.7)               \n#>      50-64                       127 (38.3)    133 (40.1)               \n#>      65+                         157 (47.3)    147 (44.3)               \n#>   sex = Female (%)               174 (52.4)    175 (52.7)          0.006\n#>   education (%)                                                    0.059\n#>      Less than high school        79 (23.8)     85 (25.6)               \n#>      High school                 200 (60.2)    200 (60.2)               \n#>      College graduate or above    53 (16.0)     47 (14.2)               \n#>   race (%)                                                         0.034\n#>      White                       100 (30.1)    105 (31.6)               \n#>      Black                       111 (33.4)    108 (32.5)               \n#>      Hispanic                     71 (21.4)     69 (20.8)               \n#>      Others                       50 (15.1)     50 (15.1)               \n#>   income (%)                                                       0.067\n#>      less than $20,000            93 (28.0)     95 (28.6)               \n#>      $20,000 to $74,999          165 (49.7)    172 (51.8)               \n#>      $75,000 and Over             74 (22.3)     65 (19.6)               \n#>   bmi (mean (SD))              30.06 (7.85)  30.46 (7.49)          0.052\n#>   smoking (%)                                                      0.053\n#>      Never smoker                166 (50.0)    161 (48.5)               \n#>      Previous smoker              97 (29.2)    105 (31.6)               \n#>      Current smoker               69 (20.8)     66 (19.9)               \n#>   htn = Yes (%)                  281 (84.6)    278 (83.7)          0.025\n#>   diabetes = Yes (%)              93 (28.0)     98 (29.5)          0.033\n#> \n#> [[5]]\n#>                               Stratified by arthritis\n#>                                No arthritis  Rheumatoid arthritis SMD   \n#>   n                              331           331                      \n#>   age.cat (%)                                                      0.026\n#>      20-49                        50 (15.1)     52 (15.7)               \n#>      50-64                       137 (41.4)    133 (40.2)               \n#>      65+                         144 (43.5)    146 (44.1)               \n#>   sex = Female (%)               172 (52.0)    175 (52.9)          0.018\n#>   education (%)                                                    0.025\n#>      Less than high school        86 (26.0)     84 (25.4)               \n#>      High school                 196 (59.2)    200 (60.4)               \n#>      College graduate or above    49 (14.8)     47 (14.2)               \n#>   race (%)                                                         0.026\n#>      White                        99 (29.9)    103 (31.1)               \n#>      Black                       111 (33.5)    109 (32.9)               \n#>      Hispanic                     70 (21.1)     69 (20.8)               \n#>      Others                       51 (15.4)     50 (15.1)               \n#>   income (%)                                                       0.074\n#>      less than $20,000            84 (25.4)     94 (28.4)               \n#>      $20,000 to $74,999          181 (54.7)    170 (51.4)               \n#>      $75,000 and Over             66 (19.9)     67 (20.2)               \n#>   bmi (mean (SD))              30.17 (7.58)  30.49 (7.86)          0.042\n#>   smoking (%)                                                      0.027\n#>      Never smoker                165 (49.8)    161 (48.6)               \n#>      Previous smoker             104 (31.4)    105 (31.7)               \n#>      Current smoker               62 (18.7)     65 (19.6)               \n#>   htn = Yes (%)                  276 (83.4)    277 (83.7)          0.008\n#>   diabetes = Yes (%)             102 (30.8)     97 (29.3)          0.033\n\nFor each of the datasets, all SMDs are less than our specified cut-point of 0.2.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Remember, we must utilize survey features to correctly estimate the standard error.\n3.1 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction and unmatched) with the matched datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible <- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat <- dat.matched[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible <- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] <- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] <- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp <- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#> [[1]]\n#> [1] 8586   19\n#> \n#> [[2]]\n#> [1] 8588   19\n#> \n#> [[3]]\n#> [1] 8582   19\n#> \n#> [[4]]\n#> [1] 8590   19\n#> \n#> [[5]]\n#> [1] 8592   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.matched[[3]])\n#>  [1] \".imp\"          \"studyid\"       \"survey.weight\" \"psu\"          \n#>  [5] \"strata\"        \"cvd\"           \"arthritis\"     \"age.cat\"      \n#>  [9] \"sex\"           \"education\"     \"race\"          \"income\"       \n#> [13] \"bmi\"           \"smoking\"       \"htn\"           \"diabetes\"     \n#> [17] \"ps\"            \"distance\"      \"weights\"       \"subclass\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#>  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#>  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#>  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#> [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#> [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\nFour variables (ps, distance, weights, and subclass) are unavailable in our full, analytic, or ineligible datasets but in the matched datasets. We need to create these 4 variables in the ineligible datasets.\n\ndat.ineligible2 <- list(NULL)\n\nfor (ii in 1:m) {\n  dat <- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible <- NULL\n  \n  # Create ps, distance, weights, and subclass\n  dat$ps <- NA\n  dat$distance <- NA\n  dat$weights <- NA\n  dat$subclass <- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars <- names(dat.matched[[1]])\n  dat <- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] <- dat\n}\n\nWe created ps, distance, weights, and subclass with missing values for the ineligible participants. Note that it doesn’t matter whether there are missing covariate values for ineligible. Since we will create the design on the full dataset and subset the design for only eligible (i.e., matched participants), missing covariate values for ineligible will not impact our analysis.\n3.2 Combining eligible (matched) and ineligible (unimputed + unmatched) subjects\nNow, we will merge matched eligible and unimputed and unmatched ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 <- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 <- data.frame(dat.matched[[ii]])\n  d1$eligible <- 1\n  \n  # Ineligible\n  d2 <- data.frame(dat.ineligible2[[ii]])\n  d2$eligible <- 0\n  \n  # Full data\n  d3 <- rbind(d1, d2)\n  dat.full2[[ii]] <- d3\n}\nlapply(dat.full2, dim)\n#> [[1]]\n#> [1] 9254   21\n#> \n#> [[2]]\n#> [1] 9254   21\n#> \n#> [[3]]\n#> [1] 9254   21\n#> \n#> [[4]]\n#> [1] 9254   21\n#> \n#> [[5]]\n#> [1] 9254   21\n\n# Stacked dataset\ndat.stacked <- rbindlist(dat.full2)\ndim(dat.stacked)\n#> [1] 46270    21\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset.\n\nallImputations <- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 <- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design <- subset(w.design0, eligible == 1) \n#> Warning in subset.svyimputationList(w.design0, eligible == 1): subset differed\n#> between imputations\n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#> [[1]]\n#> [1] 668  21\n#> \n#> [[2]]\n#> [1] 666  21\n#> \n#> [[3]]\n#> [1] 672  21\n#> \n#> [[4]]\n#> [1] 664  21\n#> \n#> [[5]]\n#> [1] 662  21\n\nNow we will run the design-adjusted logistic regression on and pool the estimate using Rubin’s rule:\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit <- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis, family = quasibinomial))\nres <- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) <- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#>               (Intercept) arthritisRheumatoid arthritis\n#> OR from m = 1        0.18                          1.53\n#> OR from m = 2        0.24                          1.09\n#> OR from m = 3        0.21                          1.28\n#> OR from m = 4        0.17                          1.61\n#> OR from m = 5        0.12                          2.32\n\nStep 3.5: Pooling estimates\n\n# Pooled estimate\npooled.estimates <- MIcombine(fit)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 <- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates <- MIcombine(fit2)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nReferences\n\n\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303."
  },
  {
    "objectID": "propensityscore8.html",
    "href": "propensityscore8.html",
    "title": "PSW with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score (PS) weighting with multiple imputation (MI), focusing on specific subpopulations defined by the study’s eligibility criteria. Similar to the previous chapter on PSM with MI for subpopulation, the modified dataset from NHANES 2017- 2018, will be used.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/analytic_imputed.RData\")\nls()\n#> [1] \"dat.analytic\"  \"dat.analytic2\" \"dat.full\"      \"imputation\"\n\n\n\ndat.full: Full dataset of 9,254 subjects\n\ndat.analytic and dat.analytic2: Analytic dataset of 4,191 participants with only missing in the covariates. There are no missing values for the exposure or outcomes.\n\nimputation: m = 5 imputed datasets from dat.analytic2 using MI.\n\nThe general strategy of solution to implement PS weighting with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects.\nApply PS weghting on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin’s rule\nDealing with missing values in covariates\nStep 1: Imputing missing values using mice for eligible subjects\nWe already completed this step in the previous chapter, where we imputed m = 5 datasets using MI.\nNow we will combine 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\n# Stacked imputed dataset\nimpdata <- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#> [1] 20955    17\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id <- NULL\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n\n\n\n\nStep 2: PS weighting steps 1-3 by DuGoff et al. (2014)\nOur next step is to use steps 1-3 of the PS weighting analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Calculate PS weights\nStep 2.3: Balance checking using SMD. Consider SMD <0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula <- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and calculating weights\n\ndat.ps <- list(NULL)\n\nm <- 5 # 5 imputed dataset\n\n# PS weighting on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed <- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit <- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps <- fitted(ps.fit)\n  \n  # Stabilized weight\n  dat.imputed$sweight <- with(dat.imputed, \n                              ifelse(I(arthritis == \"Rheumatoid arthritis\"), \n                                     mean(I(arthritis == \"Rheumatoid arthritis\"))/ps, \n                                     (1-mean(I(arthritis == \"Rheumatoid arthritis\")))/(1-ps)))\n\n  # Dataset\n  dat.ps[[ii]] <- dat.imputed\n}\n\n# Weight summary\npurrr::map_df(dat.ps, function(df){summary(df$sweight)})\n\n\n\n  \n\n\n\n\n# Dimension of each of the imputed dataset\nlapply(dat.ps, dim)\n#> [[1]]\n#> [1] 4191   18\n#> \n#> [[2]]\n#> [1] 4191   18\n#> \n#> [[3]]\n#> [1] 4191   18\n#> \n#> [[4]]\n#> [1] 4191   18\n#> \n#> [[5]]\n#> [1] 4191   18\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m <- list(NULL)\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat <- dat.ps[[ii]]\n  \n  # Covariates\n  vars <- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Design with truncated stabilized weight\n  wdesign <- svydesign(ids = ~studyid, weights = ~sweight, data = dat)\n  \n  # Balance checking \n  tab1m[[ii]] <- svyCreateTableOne(vars = vars, strata = \"arthritis\", data = wdesign,\n                                   test = F)\n}\nprint(tab1m, smd = TRUE)\n#> [[1]]\n#>                               Stratified by arthritis\n#>                                No arthritis    Rheumatoid arthritis SMD   \n#>   n                             3856.0          316.3                     \n#>   age.cat (%)                                                        0.077\n#>      20-49                      2096.4 (54.4)   159.8 (50.5)              \n#>      50-64                      1010.5 (26.2)    89.2 (28.2)              \n#>      65+                         749.1 (19.4)    67.3 (21.3)              \n#>   sex = Female (%)              1900.0 (49.3)   144.1 (45.6)         0.075\n#>   education (%)                                                      0.053\n#>      Less than high school       764.6 (19.8)    60.9 (19.3)              \n#>      High school                2109.7 (54.7)   180.9 (57.2)              \n#>      College graduate or above   981.8 (25.5)    74.4 (23.5)              \n#>   race (%)                                                           0.074\n#>      White                      1173.6 (30.4)   107.0 (33.8)              \n#>      Black                       917.9 (23.8)    70.1 (22.2)              \n#>      Hispanic                    933.5 (24.2)    73.4 (23.2)              \n#>      Others                      831.0 (21.6)    65.8 (20.8)              \n#>   income (%)                                                         0.045\n#>      less than $20,000           685.9 (17.8)    59.3 (18.7)              \n#>      $20,000 to $74,999         2015.6 (52.3)   158.2 (50.0)              \n#>      $75,000 and Over           1154.5 (29.9)    98.8 (31.3)              \n#>   bmi (mean (SD))                29.34 (7.29)   29.93 (6.90)         0.082\n#>   smoking (%)                                                        0.211\n#>      Never smoker               2362.5 (61.3)   162.7 (51.4)              \n#>      Previous smoker             814.5 (21.1)    91.9 (29.1)              \n#>      Current smoker              679.1 (17.6)    61.6 (19.5)              \n#>   htn = Yes (%)                 2403.5 (62.3)   211.4 (66.8)         0.094\n#>   diabetes = Yes (%)             523.3 (13.6)    50.9 (16.1)         0.071\n#> \n#> [[2]]\n#>                               Stratified by arthritis\n#>                                No arthritis    Rheumatoid arthritis SMD   \n#>   n                             3855.7          317.6                     \n#>   age.cat (%)                                                        0.079\n#>      20-49                      2096.4 (54.4)   160.2 (50.4)              \n#>      50-64                      1010.3 (26.2)    89.8 (28.3)              \n#>      65+                         749.0 (19.4)    67.6 (21.3)              \n#>   sex = Female (%)              1899.4 (49.3)   144.0 (45.3)         0.079\n#>   education (%)                                                      0.050\n#>      Less than high school       765.1 (19.8)    61.3 (19.3)              \n#>      High school                2113.3 (54.8)   181.6 (57.2)              \n#>      College graduate or above   977.2 (25.3)    74.7 (23.5)              \n#>   race (%)                                                           0.076\n#>      White                      1173.6 (30.4)   107.8 (33.9)              \n#>      Black                       917.9 (23.8)    71.0 (22.4)              \n#>      Hispanic                    933.2 (24.2)    72.0 (22.7)              \n#>      Others                      831.0 (21.6)    66.8 (21.0)              \n#>   income (%)                                                         0.039\n#>      less than $20,000           688.0 (17.8)    57.8 (18.2)              \n#>      $20,000 to $74,999         2015.9 (52.3)   160.1 (50.4)              \n#>      $75,000 and Over           1151.8 (29.9)    99.7 (31.4)              \n#>   bmi (mean (SD))                29.33 (7.22)   29.90 (6.95)         0.081\n#>   smoking (%)                                                        0.213\n#>      Never smoker               2362.4 (61.3)   162.6 (51.2)              \n#>      Previous smoker             813.9 (21.1)    91.7 (28.9)              \n#>      Current smoker              679.3 (17.6)    63.3 (19.9)              \n#>   htn = Yes (%)                 2394.0 (62.1)   213.6 (67.3)         0.108\n#>   diabetes = Yes (%)             522.8 (13.6)    50.3 (15.8)         0.065\n#> \n#> [[3]]\n#>                               Stratified by arthritis\n#>                                No arthritis    Rheumatoid arthritis SMD   \n#>   n                             3856.6          313.7                     \n#>   age.cat (%)                                                        0.088\n#>      20-49                      2096.4 (54.4)   156.9 (50.0)              \n#>      50-64                      1010.1 (26.2)    88.9 (28.3)              \n#>      65+                         750.1 (19.4)    67.9 (21.6)              \n#>   sex = Female (%)              1900.1 (49.3)   142.2 (45.3)         0.079\n#>   education (%)                                                      0.069\n#>      Less than high school       765.0 (19.8)    57.8 (18.4)              \n#>      High school                2112.6 (54.8)   182.5 (58.2)              \n#>      College graduate or above   979.0 (25.4)    73.4 (23.4)              \n#>   race (%)                                                           0.082\n#>      White                      1174.1 (30.4)   106.8 (34.1)              \n#>      Black                       918.2 (23.8)    69.9 (22.3)              \n#>      Hispanic                    933.4 (24.2)    69.8 (22.2)              \n#>      Others                      830.9 (21.5)    67.2 (21.4)              \n#>   income (%)                                                         0.095\n#>      less than $20,000           698.8 (18.1)    63.2 (20.1)              \n#>      $20,000 to $74,999         2003.2 (51.9)   148.1 (47.2)              \n#>      $75,000 and Over           1154.6 (29.9)   102.4 (32.7)              \n#>   bmi (mean (SD))                29.29 (7.21)   29.80 (6.97)         0.072\n#>   smoking (%)                                                        0.216\n#>      Never smoker               2362.7 (61.3)   160.3 (51.1)              \n#>      Previous smoker             814.6 (21.1)    91.0 (29.0)              \n#>      Current smoker              679.2 (17.6)    62.5 (19.9)              \n#>   htn = Yes (%)                 2398.6 (62.2)   206.3 (65.8)         0.074\n#>   diabetes = Yes (%)             523.7 (13.6)    50.9 (16.2)         0.075\n#> \n#> [[4]]\n#>                               Stratified by arthritis\n#>                                No arthritis    Rheumatoid arthritis SMD   \n#>   n                             3856.2          312.9                     \n#>   age.cat (%)                                                        0.098\n#>      20-49                      2096.3 (54.4)   154.7 (49.5)              \n#>      50-64                      1010.8 (26.2)    91.2 (29.1)              \n#>      65+                         749.1 (19.4)    67.0 (21.4)              \n#>   sex = Female (%)              1900.1 (49.3)   144.4 (46.2)         0.063\n#>   education (%)                                                      0.064\n#>      Less than high school       763.7 (19.8)    58.4 (18.7)              \n#>      High school                2115.2 (54.9)   181.5 (58.0)              \n#>      College graduate or above   977.2 (25.3)    72.9 (23.3)              \n#>   race (%)                                                           0.078\n#>      White                      1173.7 (30.4)   106.5 (34.0)              \n#>      Black                       918.0 (23.8)    70.1 (22.4)              \n#>      Hispanic                    933.5 (24.2)    71.2 (22.8)              \n#>      Others                      831.0 (21.5)    65.0 (20.8)              \n#>   income (%)                                                         0.096\n#>      less than $20,000           694.1 (18.0)    63.9 (20.4)              \n#>      $20,000 to $74,999         2009.3 (52.1)   148.3 (47.4)              \n#>      $75,000 and Over           1152.9 (29.9)   100.7 (32.2)              \n#>   bmi (mean (SD))                29.28 (7.21)   29.86 (7.13)         0.081\n#>   smoking (%)                                                        0.209\n#>      Never smoker               2362.7 (61.3)   161.3 (51.6)              \n#>      Previous smoker             813.8 (21.1)    90.7 (29.0)              \n#>      Current smoker              679.7 (17.6)    60.9 (19.5)              \n#>   htn = Yes (%)                 2394.5 (62.1)   206.6 (66.0)         0.082\n#>   diabetes = Yes (%)             523.0 (13.6)    50.2 (16.0)         0.069\n#> \n#> [[5]]\n#>                               Stratified by arthritis\n#>                                No arthritis    Rheumatoid arthritis SMD   \n#>   n                             3855.7          316.8                     \n#>   age.cat (%)                                                        0.080\n#>      20-49                      2096.4 (54.4)   159.6 (50.4)              \n#>      50-64                      1010.3 (26.2)    89.8 (28.4)              \n#>      65+                         749.1 (19.4)    67.4 (21.3)              \n#>   sex = Female (%)              1899.6 (49.3)   143.9 (45.4)         0.077\n#>   education (%)                                                      0.060\n#>      Less than high school       767.4 (19.9)    62.1 (19.6)              \n#>      High school                2110.3 (54.7)   181.8 (57.4)              \n#>      College graduate or above   978.0 (25.4)    72.9 (23.0)              \n#>   race (%)                                                           0.074\n#>      White                      1173.5 (30.4)   107.1 (33.8)              \n#>      Black                       917.9 (23.8)    70.1 (22.1)              \n#>      Hispanic                    933.4 (24.2)    73.1 (23.1)              \n#>      Others                      830.9 (21.5)    66.5 (21.0)              \n#>   income (%)                                                         0.047\n#>      less than $20,000           685.0 (17.8)    60.6 (19.1)              \n#>      $20,000 to $74,999         2022.6 (52.5)   159.2 (50.3)              \n#>      $75,000 and Over           1148.1 (29.8)    97.0 (30.6)              \n#>   bmi (mean (SD))                29.31 (7.20)   29.77 (7.04)         0.064\n#>   smoking (%)                                                        0.206\n#>      Never smoker               2362.7 (61.3)   163.1 (51.5)              \n#>      Previous smoker             813.3 (21.1)    90.0 (28.4)              \n#>      Current smoker              679.7 (17.6)    63.6 (20.1)              \n#>   htn = Yes (%)                 2409.4 (62.5)   210.2 (66.3)         0.080\n#>   diabetes = Yes (%)             522.5 (13.6)    50.3 (15.9)         0.066\n\nFor each of the datasets, all SMDs except for smoking are less than our specified cut-point of 0.2. We will adjust our outcome model for smoking.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Note that, we must utilize survey features to correctly estimate the standard error. For this step, we will multiply PS weight and survey weight and create a new weight variable.\n3.1 Calculating new weights\n\ndat.ps2 <- list(NULL)\n\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat <- dat.ps[[ii]]\n  \n  # New weight = survey weight * PS weight \n  dat$new_weight <- with(dat, survey.weight * sweight)\n  \n  dat.ps2[[ii]] <- dat\n}\n\n3.2 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction) with the PS weighted datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible <- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat <- dat.ps[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible <- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] <- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] <- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp <- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#> [[1]]\n#> [1] 5063   19\n#> \n#> [[2]]\n#> [1] 5063   19\n#> \n#> [[3]]\n#> [1] 5063   19\n#> \n#> [[4]]\n#> [1] 5063   19\n#> \n#> [[5]]\n#> [1] 5063   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.ps2[[3]])\n#>  [1] \".imp\"          \"studyid\"       \"survey.weight\" \"psu\"          \n#>  [5] \"strata\"        \"cvd\"           \"arthritis\"     \"age.cat\"      \n#>  [9] \"sex\"           \"education\"     \"race\"          \"income\"       \n#> [13] \"bmi\"           \"smoking\"       \"htn\"           \"diabetes\"     \n#> [17] \"ps\"            \"sweight\"       \"new_weight\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#>  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#>  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#>  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#> [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#> [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\n\ndat.ineligible2 <- list(NULL)\n\nfor (ii in 1:m) {\n  dat <- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible <- NULL\n  \n  # Create ps and sweight\n  dat$ps <- NA\n  dat$sweight <- NA\n  dat$new_weight <- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars <- names(dat.ps2[[1]])\n  dat <- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] <- dat\n}\n\n3.2 Combining eligible (imputed and PS weighted) and ineligible (unimputed) subjects\nNow, we will merge imputed eligible and unimputed ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 <- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 <- data.frame(dat.ps2[[ii]])\n  d1$eligible <- 1\n  \n  # Ineligible\n  d2 <- data.frame(dat.ineligible2[[ii]])\n  d2$eligible <- 0\n  \n  # Full data\n  d3 <- rbind(d1, d2)\n  \n  #  New weight variable in the full dataset\n  d3$new_weight <- 0\n  d3$new_weight[d3$studyid %in% d1$studyid] <- d1$new_weight\n  \n  # Full data in list format\n  dat.full2[[ii]] <- d3\n}\nlapply(dat.full2, dim)\n#> [[1]]\n#> [1] 9254   20\n#> \n#> [[2]]\n#> [1] 9254   20\n#> \n#> [[3]]\n#> [1] 9254   20\n#> \n#> [[4]]\n#> [1] 9254   20\n#> \n#> [[5]]\n#> [1] 9254   20\n\n# Stacked dataset\ndat.stacked <- rbindlist(dat.full2)\ndim(dat.stacked)\n#> [1] 46270    20\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset. Make sure to use the new weight variable that combines survey weights and PS weights.\n\nallImputations <- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 <- svydesign(ids = ~psu, \n                       weights = ~new_weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design <- subset(w.design0, eligible == 1) \n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#> [[1]]\n#> [1] 4191   20\n#> \n#> [[2]]\n#> [1] 4191   20\n#> \n#> [[3]]\n#> [1] 4191   20\n#> \n#> [[4]]\n#> [1] 4191   20\n#> \n#> [[5]]\n#> [1] 4191   20\n\nNow we will run the design-adjusted logistic regression, adjusting for smoking since smoking was not balanced in terms of SMD.\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit <- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + smoking, family = quasibinomial))\nres <- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) <- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#>               (Intercept) arthritisRheumatoid arthritis smokingPrevious smoker\n#> OR from m = 1        0.04                          1.21                   2.93\n#> OR from m = 2        0.04                          1.24                   2.93\n#> OR from m = 3        0.04                          1.23                   2.93\n#> OR from m = 4        0.04                          1.22                   2.92\n#> OR from m = 5        0.04                          1.22                   2.92\n#>               smokingCurrent smoker\n#> OR from m = 1                  1.55\n#> OR from m = 2                  1.56\n#> OR from m = 3                  1.54\n#> OR from m = 4                  1.55\n#> OR from m = 5                  1.55\n\nStep 3.5: Pooling estimates\nNow, we will pool the estimate using Rubin’s rule:\n\n# Pooled estimate\npooled.estimates <- MIcombine(fit)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 <- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates <- MIcombine(fit2)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nReferences"
  },
  {
    "objectID": "propensityscore9.html",
    "href": "propensityscore9.html",
    "title": "PSW with multiple tx",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score weighting (PSW) for multiple treatment categories. We will use CCHS data that was used in the previous chapter on exact matching with CCHS.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"\n\n\n\nanalytic.miss: Full dataset of 397,173 participants from CCHS cycles 1.1, 2.1, and 3.1 with missing values in some covariates\n\nanalytic2: Analytic dataset of 185,613 participants without missing in the covariates.\nPre-processing\nLet us create the full and analytic datasets for only CCHS 3.1.\n\n# Full dataset with missing\ndat.full <- subset(analytic.miss, cycle == \"31\")\ndim(dat.full)\n#> [1] 132221     22\n\n# Analytic dataset without missing\ndat.analytic <- subset(analytic2, cycle == \"31\")\ndim(dat.analytic)\n#> [1] 39634    22\n\nWe will use the analytic dataset (dat.analytic) to run our PSW analysis with the following variables: - Outcome: CVD - Exposure: phyact (3-level physical activity) - Covariates: age, sex, married (marital status), race, edu (education), income, bmi (body mass index), doctor (whether visited to a doctor), stress, smoke, drink (drink alcohol or not), fruit (fruit consumption), bp (blood pressure), diab (diabetes), OA (osteoarthritis), immigrate (immigrant or not)\n- Sampling weight: weight\n\n# Is there any character variable?\nstr(dat.analytic) \n#> 'data.frame':    39634 obs. of  22 variables:\n#>  $ CVD      : chr  \"no event\" \"no event\" \"no event\" \"no event\" ...\n#>  $ age      : chr  \"20-29 years\" \"65 years and over\" \"20-29 years\" \"20-29 years\" ...\n#>  $ sex      : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n#>  $ married  : chr  \"not single\" \"single\" \"single\" \"single\" ...\n#>  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"2nd grad.\" \"Other 2nd grad.\" ...\n#>  $ income   : chr  \"$50,000-$79,999\" \"$29,999 or less\" \"$29,999 or less\" \"$50,000-$79,999\" ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: 3 2 2 2 2 3 2 3 3 2 ...\n#>  $ phyact   : chr  \"Inactive\" \"Moderate\" \"Active\" \"Moderate\" ...\n#>  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n#>  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" ...\n#>  $ smoke    : chr  \"Former smoker\" \"Never smoker\" \"Never smoker\" \"Never smoker\" ...\n#>  $ drink    : chr  \"Current drinker\" \"Former driker\" \"Never drank\" \"Current drinker\" ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 1 1 3 2 2 2 2 3 ...\n#>  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ province : chr  \"South\" \"South\" \"South\" \"South\" ...\n#>  $ weight   : num  93.5 111.4 120.4 328.2 810.6 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ ID       : int  264953 264954 264961 264962 264963 264964 264969 264971 264975 264976 ...\n#>  $ OA       : chr  \"Control\" \"OA\" \"Control\" \"Control\" ...\n#>  $ immigrate: chr  \"not immigrant\" \"not immigrant\" \"not immigrant\" \"not immigrant\" ...\n\n# Make all variables (except for ID and weight) as factor\nvar.names <- c(\"CVD\", \"phyact\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \n               \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"province\", \"OA\", \"immigrate\")\ndat.full[var.names] <- lapply(dat.full[var.names] , factor)\ndat.analytic[var.names] <- lapply(dat.analytic[var.names], factor)\n\n# Outcome - CVD\ntable(dat.analytic$CVD, useNA = \"always\")\n#> \n#>    event no event     <NA> \n#>     1931    37703        0\ndat.full$CVD <- car::recode(dat.full$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.full$CVD <- factor(dat.full$CVD, levels = c(\"No\", \"Yes\"))\n\ndat.analytic$CVD <- car::recode(dat.analytic$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.analytic$CVD <- factor(dat.analytic$CVD, levels = c(\"No\", \"Yes\"))\ntable(dat.analytic$CVD, useNA = \"always\")\n#> \n#>    No   Yes  <NA> \n#> 37703  1931     0\n\n# Exposure - physical activity\ntable(dat.analytic$phyact, useNA = \"always\")\n#> \n#>   Active Inactive Moderate     <NA> \n#>    11508    17569    10557        0\ndat.full$phyact <- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\ndat.analytic$phyact <- factor(dat.analytic$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\n\n# Table 1\nvars <- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\ntab1 <- CreateTableOne(vars = vars, strata = \"phyact\", data = dat.analytic, test = F)\nprint(tab1, smd = T, showAllLevels = T)\n#>                Stratified by phyact\n#>                 level             Inactive      Moderate      Active       \n#>   n                               17569         10557         11508        \n#>   age (%)       20-29 years        2537 (14.4)   1709 (16.2)   2316 (20.1) \n#>                 30-39 years        3526 (20.1)   2265 (21.5)   2276 (19.8) \n#>                 40-49 years        3270 (18.6)   1939 (18.4)   2037 (17.7) \n#>                 50-59 years        2901 (16.5)   1659 (15.7)   1480 (12.9) \n#>                 60-64 years        1130 ( 6.4)    665 ( 6.3)    570 ( 5.0) \n#>                 65 years and over  3310 (18.8)   1568 (14.9)   1183 (10.3) \n#>                 teen                895 ( 5.1)    752 ( 7.1)   1646 (14.3) \n#>   sex (%)       Female             9403 (53.5)   5709 (54.1)   5526 (48.0) \n#>                 Male               8166 (46.5)   4848 (45.9)   5982 (52.0) \n#>   married (%)   not single         9600 (54.6)   5920 (56.1)   5637 (49.0) \n#>                 single             7969 (45.4)   4637 (43.9)   5871 (51.0) \n#>   race (%)      Non-white          1757 (10.0)    886 ( 8.4)   1066 ( 9.3) \n#>                 White             15812 (90.0)   9671 (91.6)  10442 (90.7) \n#>   edu (%)       < 2ndary           3690 (21.0)   1635 (15.5)   2039 (17.7) \n#>                 2nd grad.          3246 (18.5)   1749 (16.6)   1845 (16.0) \n#>                 Other 2nd grad.    1566 ( 8.9)    969 ( 9.2)   1150 (10.0) \n#>                 Post-2nd grad.     9067 (51.6)   6204 (58.8)   6474 (56.3) \n#>   income (%)    $29,999 or less    4480 (25.5)   1929 (18.3)   1906 (16.6) \n#>                 $30,000-$49,999    4018 (22.9)   2059 (19.5)   2097 (18.2) \n#>                 $50,000-$79,999    4512 (25.7)   2974 (28.2)   3095 (26.9) \n#>                 $80,000 or more    4559 (25.9)   3595 (34.1)   4410 (38.3) \n#>   bmi (%)       Underweight         532 ( 3.0)    243 ( 2.3)    340 ( 3.0) \n#>                 healthy weight     7349 (41.8)   5023 (47.6)   6233 (54.2) \n#>                 Overweight         9688 (55.1)   5291 (50.1)   4935 (42.9) \n#>   doctor (%)    No                 2322 (13.2)   1272 (12.0)   1520 (13.2) \n#>                 Yes               15247 (86.8)   9285 (88.0)   9988 (86.8) \n#>   stress (%)    Not too stressed  13544 (77.1)   8371 (79.3)   9314 (80.9) \n#>                 stressed           4025 (22.9)   2186 (20.7)   2194 (19.1) \n#>   smoke (%)     Current smoker     5032 (28.6)   2386 (22.6)   2488 (21.6) \n#>                 Former smoker      6900 (39.3)   4562 (43.2)   4672 (40.6) \n#>                 Never smoker       5637 (32.1)   3609 (34.2)   4348 (37.8) \n#>   drink (%)     Current drinker   13913 (79.2)   9016 (85.4)   9863 (85.7) \n#>                 Former driker      2582 (14.7)   1102 (10.4)   1063 ( 9.2) \n#>                 Never drank        1074 ( 6.1)    439 ( 4.2)    582 ( 5.1) \n#>   fruit (%)     0-3 daily serving  5610 (31.9)   2270 (21.5)   1902 (16.5) \n#>                 4-6 daily serving  8827 (50.2)   5445 (51.6)   5481 (47.6) \n#>                 6+ daily serving   3132 (17.8)   2842 (26.9)   4125 (35.8) \n#>   bp (%)        No                14188 (80.8)   8976 (85.0)  10349 (89.9) \n#>                 Yes                3381 (19.2)   1581 (15.0)   1159 (10.1) \n#>   diab (%)      No                16393 (93.3)  10114 (95.8)  11165 (97.0) \n#>                 Yes                1176 ( 6.7)    443 ( 4.2)    343 ( 3.0) \n#>   OA (%)        Control           14864 (84.6)   9310 (88.2)  10565 (91.8) \n#>                 OA                 2705 (15.4)   1247 (11.8)    943 ( 8.2) \n#>   immigrate (%) not immigrant     16557 (94.2)  10150 (96.1)  11098 (96.4) \n#>                 recent             1012 ( 5.8)    407 ( 3.9)    410 ( 3.6) \n#>                Stratified by phyact\n#>                 SMD   \n#>   n                   \n#>   age (%)        0.285\n#>                       \n#>                       \n#>                       \n#>                       \n#>                       \n#>                       \n#>   sex (%)        0.081\n#>                       \n#>   married (%)    0.095\n#>                       \n#>   race (%)       0.037\n#>                       \n#>   edu (%)        0.119\n#>                       \n#>                       \n#>                       \n#>   income (%)     0.213\n#>                       \n#>                       \n#>                       \n#>   bmi (%)        0.173\n#>                       \n#>                       \n#>   doctor (%)     0.023\n#>                       \n#>   stress (%)     0.063\n#>                       \n#>   smoke (%)      0.129\n#>                       \n#>                       \n#>   drink (%)      0.133\n#>                       \n#>                       \n#>   fruit (%)      0.323\n#>                       \n#>                       \n#>   bp (%)         0.175\n#>                       \n#>   diab (%)       0.116\n#>                       \n#>   OA (%)         0.150\n#>                       \n#>   immigrate (%)  0.070\n#> \n\nPSW for multiple tx\nNominal categories (option 1)\nFor this part (option 1), we consider physical activity as a nominal variable.\nEstimating Propensity score\nLet us fit the PS model by considering physical activity as a nominal variable and estimate the propensity scores:\n\n# Formula\nps.formula <- formula(phyact ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\n#> Warning: package 'VGAM' was built under R version 4.2.3\n#> Loading required package: stats4\n#> Loading required package: splines\n#> \n#> Attaching package: 'VGAM'\n#> The following object is masked from 'package:survey':\n#> \n#>     calibrate\nps.fit <- vglm(ps.formula, weights = weight, data = dat.analytic, \n               family = multinomial(parallel = FALSE))\n\n# Propensity scores\nps <- data.frame(fitted(ps.fit))\nhead(ps)\n\n\n\n  \n\n\n\n# Summary\napply(ps, 2, summary)\n#>           Inactive   Moderate    Active\n#> Min.    0.06879258 0.07957314 0.0285961\n#> 1st Qu. 0.34233334 0.23371928 0.1867680\n#> Median  0.44756388 0.27020295 0.2634537\n#> Mean    0.44778510 0.26768926 0.2845256\n#> 3rd Qu. 0.55452938 0.30446807 0.3616572\n#> Max.    0.89058237 0.39864744 0.7524817\n\nCreating weights\nLet us create PS weights. For subject \\(i\\), PS weight is calculated as\n\\[w_i = \\frac{1}{P(A_i = a|L)}, \\] where \\(A\\) is the exposure with levels \\(a\\) (Inactive, Moderate, and Active in our case), and \\(L\\) is the list of covariates.\n\n# IPW\ndat.analytic$ipw <- ifelse(dat.analytic$phyact==\"Active\", 1/ps$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps$Moderate, \n                                  1/ps$Inactive))\nwith(dat.analytic, by(ipw, phyact, summary))\n#> phyact: Inactive\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.123   1.671   1.994   2.233   2.505  14.536 \n#> ------------------------------------------------------------ \n#> phyact: Moderate\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   2.508   3.206   3.576   3.743   4.087  11.695 \n#> ------------------------------------------------------------ \n#> phyact: Active\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.329   2.310   3.086   3.534   4.225  25.049\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars <- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design <- svydesign(id = ~1, weights = ~ipw, data = dat.analytic)\n\n# Table 1\ntabw <- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#>                Stratified by phyact\n#>                 level             Inactive        Moderate       \n#>   n                               39231.2         39519.8        \n#>   age (%)       20-29 years        6477.4 (16.5)   6856.1 (17.3) \n#>                 30-39 years        7706.8 (19.6)   8174.8 (20.7) \n#>                 40-49 years        7098.2 (18.1)   7163.0 (18.1) \n#>                 50-59 years        5922.4 (15.1)   5901.2 (14.9) \n#>                 60-64 years        2414.8 ( 6.2)   2265.4 ( 5.7) \n#>                 65 years and over  6187.1 (15.8)   5843.8 (14.8) \n#>                 teen               3424.6 ( 8.7)   3315.4 ( 8.4) \n#>   sex (%)       Female            20188.3 (51.5)  20693.4 (52.4) \n#>                 Male              19042.9 (48.5)  18826.3 (47.6) \n#>   married (%)   not single        20665.9 (52.7)  20907.0 (52.9) \n#>                 single            18565.3 (47.3)  18612.8 (47.1) \n#>   race (%)      Non-white          3471.9 ( 8.8)   4095.5 (10.4) \n#>                 White             35759.3 (91.2)  35424.2 (89.6) \n#>   edu (%)       < 2ndary           7267.3 (18.5)   7288.2 (18.4) \n#>                 2nd grad.          6902.6 (17.6)   6901.3 (17.5) \n#>                 Other 2nd grad.    3783.0 ( 9.6)   3671.6 ( 9.3) \n#>                 Post-2nd grad.    21278.3 (54.2)  21658.7 (54.8) \n#>   income (%)    $29,999 or less    8432.3 (21.5)   8425.7 (21.3) \n#>                 $30,000-$49,999    8115.6 (20.7)   8109.8 (20.5) \n#>                 $50,000-$79,999   10169.1 (25.9)  10602.5 (26.8) \n#>                 $80,000 or more   12514.2 (31.9)  12381.9 (31.3) \n#>   bmi (%)       Underweight        1092.7 ( 2.8)   1110.6 ( 2.8) \n#>                 healthy weight    18191.1 (46.4)  18613.7 (47.1) \n#>                 Overweight        19947.4 (50.8)  19795.5 (50.1) \n#>   doctor (%)    No                 5073.1 (12.9)   5127.6 (13.0) \n#>                 Yes               34158.1 (87.1)  34392.2 (87.0) \n#>   stress (%)    Not too stressed  30921.0 (78.8)  31245.6 (79.1) \n#>                 stressed           8310.3 (21.2)   8274.2 (20.9) \n#>   smoke (%)     Current smoker    10018.8 (25.5)  10121.1 (25.6) \n#>                 Former smoker     15860.1 (40.4)  15737.2 (39.8) \n#>                 Never smoker      13352.4 (34.0)  13661.5 (34.6) \n#>   drink (%)     Current drinker   32257.4 (82.2)  32836.4 (83.1) \n#>                 Former driker      4857.5 (12.4)   4621.5 (11.7) \n#>                 Never drank        2116.4 ( 5.4)   2061.9 ( 5.2) \n#>   fruit (%)     0-3 daily serving  9738.2 (24.8)   9786.2 (24.8) \n#>                 4-6 daily serving 19523.6 (49.8)  19803.8 (50.1) \n#>                 6+ daily serving   9969.4 (25.4)   9929.8 (25.1) \n#>   bp (%)        No                33045.9 (84.2)  33662.1 (85.2) \n#>                 Yes                6185.3 (15.8)   5857.7 (14.8) \n#>   diab (%)      No                37227.0 (94.9)  37572.5 (95.1) \n#>                 Yes                2004.2 ( 5.1)   1947.3 ( 4.9) \n#>   OA (%)        Control           34297.7 (87.4)  34835.2 (88.1) \n#>                 OA                 4933.5 (12.6)   4684.6 (11.9) \n#>   immigrate (%) not immigrant     37503.8 (95.6)  37532.8 (95.0) \n#>                 recent             1727.4 ( 4.4)   1987.0 ( 5.0) \n#>                Stratified by phyact\n#>                 Active          SMD   \n#>   n             40667.9               \n#>   age (%)        6467.5 (15.9)   0.046\n#>                  8588.7 (21.1)        \n#>                  7538.2 (18.5)        \n#>                  6327.6 (15.6)        \n#>                  2420.1 ( 6.0)        \n#>                  5989.1 (14.7)        \n#>                  3336.8 ( 8.2)        \n#>   sex (%)       21026.2 (51.7)   0.012\n#>                 19641.7 (48.3)        \n#>   married (%)   22651.2 (55.7)   0.040\n#>                 18016.8 (44.3)        \n#>   race (%)       3834.0 ( 9.4)   0.034\n#>                 36834.0 (90.6)        \n#>   edu (%)        7395.4 (18.2)   0.023\n#>                  6825.8 (16.8)        \n#>                  3765.2 ( 9.3)        \n#>                 22681.6 (55.8)        \n#>   income (%)     8029.8 (19.7)   0.041\n#>                  8544.1 (21.0)        \n#>                 11407.3 (28.0)        \n#>                 12686.7 (31.2)        \n#>   bmi (%)        1241.6 ( 3.1)   0.017\n#>                 19120.5 (47.0)        \n#>                 20305.8 (49.9)        \n#>   doctor (%)     5272.7 (13.0)   0.001\n#>                 35395.3 (87.0)        \n#>   stress (%)    32100.6 (78.9)   0.004\n#>                  8567.4 (21.1)        \n#>   smoke (%)      9760.6 (24.0)   0.032\n#>                 17024.0 (41.9)        \n#>                 13883.4 (34.1)        \n#>   drink (%)     33927.5 (83.4)   0.022\n#>                  4634.1 (11.4)        \n#>                  2106.3 ( 5.2)        \n#>   fruit (%)     10170.0 (25.0)   0.007\n#>                 20200.6 (49.7)        \n#>                 10297.3 (25.3)        \n#>   bp (%)        34372.4 (84.5)   0.017\n#>                  6295.6 (15.5)        \n#>   diab (%)      38850.3 (95.5)   0.020\n#>                  1817.7 ( 4.5)        \n#>   OA (%)        35666.9 (87.7)   0.015\n#>                  5001.1 (12.3)        \n#>   immigrate (%) 38662.6 (95.1)   0.020\n#>                  2005.4 ( 4.9)\n\nAll covariates are balanced in terms of SMD (SMD \\(\\le\\) 0.20).\nOutcome analysis\nNow, we will fit the outcome model. To get the correct estimate of the standard error, we will set up the design with full data and subset the design.\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] <- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight <- with(dat.analytic, ipw * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight <- 0\ndat.full$ATEweight[dat.full$ID %in% dat.analytic$ID] <- dat.analytic$ATEweight\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~1, weights = ~ATEweight, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 <- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop <- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n\n  \n\n\n\n# Outcome model\nfit <- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit, confint.method = \"robust\", pvalue.method = \"robust\")\n#>  Variable    Units OddsRatio       CI.95  p-value \n#>    phyact Inactive       Ref                      \n#>           Moderate      0.94 [0.64;1.38]   0.7693 \n#>             Active      0.83 [0.53;1.29]   0.4133\n\nOrdinal categories (for comparison)\nFor comparison, let us consider physical activity as a ordinal variable (option 2).\nDefine ordinal variable\n\n# Exposure - ordinal physical activity\ndat.full$phyact.ord <- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"), \n                              ordered = T)\ndat.analytic$phyact.ord <- factor(dat.analytic$phyact, \n                                  levels = c(\"Inactive\", \"Moderate\", \"Active\"), ordered = T)\nhead(dat.analytic$phyact.ord)\n#> [1] Inactive Moderate Active   Moderate Active   Active  \n#> Levels: Inactive < Moderate < Active\n\nEstimating Propensity score\n\n# Formula\nps.formula2 <- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\nps.fit2 <- vglm(ps.formula2, weights = weight, data = dat.analytic, family = propodds)\n\n# Propensity scores\nps2 <- data.frame(fitted(ps.fit2))\nhead(ps2)\n\n\n\n  \n\n\n\n# Summary\napply(ps2, 2, summary)\n#>           Inactive   Moderate     Active\n#> Min.    0.07517679 0.07505615 0.03456101\n#> 1st Qu. 0.34108922 0.25002048 0.18907940\n#> Median  0.44711595 0.28026693 0.26446898\n#> Mean    0.44780744 0.26682382 0.28536874\n#> 3rd Qu. 0.55497781 0.29473347 0.35967960\n#> Max.    0.89038285 0.29934482 0.78152250\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw2 <- ifelse(dat.analytic$phyact==\"Active\", 1/ps2$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps2$Moderate, \n                                  1/ps2$Inactive))\nwith(dat.analytic, by(ipw2, phyact.ord, summary))\n#> phyact.ord: Inactive\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.123   1.668   2.001   2.237   2.516  13.302 \n#> ------------------------------------------------------------ \n#> phyact.ord: Moderate\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   3.341   3.382   3.525   3.748   3.875  11.248 \n#> ------------------------------------------------------------ \n#> phyact.ord: Active\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.280   2.319   3.073   3.504   4.211  19.768\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars <- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design <- svydesign(id = ~1, weights = ~ipw2, data = dat.analytic)\n\n# Table 1\ntabw2 <- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw2, smd = T, showAllLevels = T)\n#>                Stratified by phyact\n#>                 level             Inactive        Moderate       \n#>   n                               39305.5         39566.3        \n#>   age (%)       20-29 years        6679.4 (17.0)   6125.6 (15.5) \n#>                 30-39 years        7635.7 (19.4)   8357.8 (21.1) \n#>                 40-49 years        7085.9 (18.0)   7124.1 (18.0) \n#>                 50-59 years        5820.8 (14.8)   6307.9 (15.9) \n#>                 60-64 years        2339.1 ( 6.0)   2494.0 ( 6.3) \n#>                 65 years and over  6108.9 (15.5)   6264.9 (15.8) \n#>                 teen               3635.8 ( 9.3)   2892.0 ( 7.3) \n#>   sex (%)       Female            19987.1 (50.9)  21622.5 (54.6) \n#>                 Male              19318.4 (49.1)  17943.8 (45.4) \n#>   married (%)   not single        20342.7 (51.8)  22151.4 (56.0) \n#>                 single            18962.9 (48.2)  17414.9 (44.0) \n#>   race (%)      Non-white          3595.2 ( 9.1)   3507.2 ( 8.9) \n#>                 White             35710.4 (90.9)  36059.1 (91.1) \n#>   edu (%)       < 2ndary           7467.4 (19.0)   6739.9 (17.0) \n#>                 2nd grad.          7027.3 (17.9)   6644.2 (16.8) \n#>                 Other 2nd grad.    3815.2 ( 9.7)   3628.7 ( 9.2) \n#>                 Post-2nd grad.    20995.6 (53.4)  22553.5 (57.0) \n#>   income (%)    $29,999 or less    8596.0 (21.9)   7724.2 (19.5) \n#>                 $30,000-$49,999    8182.3 (20.8)   7888.9 (19.9) \n#>                 $50,000-$79,999   10126.1 (25.8)  11034.9 (27.9) \n#>                 $80,000 or more   12401.1 (31.6)  12918.3 (32.6) \n#>   bmi (%)       Underweight        1134.1 ( 2.9)    960.4 ( 2.4) \n#>                 healthy weight    18284.0 (46.5)  18426.4 (46.6) \n#>                 Overweight        19887.4 (50.6)  20179.5 (51.0) \n#>   doctor (%)    No                 5183.0 (13.2)   4741.2 (12.0) \n#>                 Yes               34122.5 (86.8)  34825.0 (88.0) \n#>   stress (%)    Not too stressed  31032.0 (79.0)  31233.5 (78.9) \n#>                 stressed           8273.6 (21.0)   8332.8 (21.1) \n#>   smoke (%)     Current smoker    10263.8 (26.1)   9253.9 (23.4) \n#>                 Former smoker     15572.0 (39.6)  16822.9 (42.5) \n#>                 Never smoker      13469.8 (34.3)  13489.4 (34.1) \n#>   drink (%)     Current drinker   32206.8 (81.9)  33304.9 (84.2) \n#>                 Former driker      4898.9 (12.5)   4438.8 (11.2) \n#>                 Never drank        2199.8 ( 5.6)   1822.5 ( 4.6) \n#>   fruit (%)     0-3 daily serving  9841.8 (25.0)   9509.3 (24.0) \n#>                 4-6 daily serving 19586.3 (49.8)  19922.3 (50.4) \n#>                 6+ daily serving   9877.5 (25.1)  10134.7 (25.6) \n#>   bp (%)        No                33233.3 (84.6)  33115.0 (83.7) \n#>                 Yes                6072.2 (15.4)   6451.2 (16.3) \n#>   diab (%)      No                37292.2 (94.9)  37648.3 (95.2) \n#>                 Yes                2013.4 ( 5.1)   1918.0 ( 4.8) \n#>   OA (%)        Control           34458.3 (87.7)  34493.7 (87.2) \n#>                 OA                 4847.3 (12.3)   5072.6 (12.8) \n#>   immigrate (%) not immigrant     37534.6 (95.5)  37816.7 (95.6) \n#>                 recent             1770.9 ( 4.5)   1749.5 ( 4.4) \n#>                Stratified by phyact\n#>                 Active          SMD   \n#>   n             40328.5               \n#>   age (%)        6789.1 (16.8)   0.080\n#>                  8504.7 (21.1)        \n#>                  7562.1 (18.8)        \n#>                  6079.6 (15.1)        \n#>                  2283.6 ( 5.7)        \n#>                  5614.4 (13.9)        \n#>                  3495.0 ( 8.7)        \n#>   sex (%)       20379.8 (50.5)   0.055\n#>                 19948.8 (49.5)        \n#>   married (%)   21840.5 (54.2)   0.057\n#>                 18488.0 (45.8)        \n#>   race (%)       4125.6 (10.2)   0.031\n#>                 36202.9 (89.8)        \n#>   edu (%)        7564.8 (18.8)   0.052\n#>                  6886.8 (17.1)        \n#>                  3777.5 ( 9.4)        \n#>                 22099.4 (54.8)        \n#>   income (%)     8371.7 (20.8)   0.056\n#>                  8581.4 (21.3)        \n#>                 11013.7 (27.3)        \n#>                 12361.7 (30.7)        \n#>   bmi (%)        1294.7 ( 3.2)   0.037\n#>                 19134.4 (47.4)        \n#>                 19899.5 (49.3)        \n#>   doctor (%)     5467.9 (13.6)   0.031\n#>                 34860.6 (86.4)        \n#>   stress (%)    31816.9 (78.9)   0.001\n#>                  8511.6 (21.1)        \n#>   smoke (%)     10153.3 (25.2)   0.048\n#>                 16300.6 (40.4)        \n#>                 13874.6 (34.4)        \n#>   drink (%)     33415.6 (82.9)   0.043\n#>                  4714.1 (11.7)        \n#>                  2198.9 ( 5.5)        \n#>   fruit (%)     10190.6 (25.3)   0.020\n#>                 19953.3 (49.5)        \n#>                 10184.6 (25.3)        \n#>   bp (%)        34557.1 (85.7)   0.037\n#>                  5771.5 (14.3)        \n#>   diab (%)      38519.4 (95.5)   0.020\n#>                  1809.1 ( 4.5)        \n#>   OA (%)        35641.0 (88.4)   0.024\n#>                  4687.5 (11.6)        \n#>   immigrate (%) 38153.4 (94.6)   0.030\n#>                  2175.2 ( 5.4)\n\nAgain, all covariates are balanced in terms of SMD.\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] <- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight2 <- with(dat.analytic, ipw2 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight2 <- 0\ndat.full$ATEweight2[dat.full$ID %in% dat.analytic$ID] <- dat.analytic$ATEweight2\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~1, weights = ~ATEweight2, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 <- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop2 <- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop2\n\n\n\n  \n\n\n\n# Outcome model\nfit2 <- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit2, confint.method = \"robust\", pvalue.method = \"robust\")\n#>  Variable    Units OddsRatio       CI.95  p-value \n#>    phyact Inactive       Ref                      \n#>           Moderate      1.01 [0.71;1.43]   0.9775 \n#>             Active      0.81 [0.52;1.27]   0.3645\n\nMachine learning / GBM (option 3)\nIn this part, we will use Gradient Boosting as one of the machine learning techniques to estimate the propensity scores.\nEstimating Propensity score\n\n# Formula\nps.formula3 <- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\npacman::p_load(twang)\nset.seed(123)\nps.fit3 <- mnps(ps.formula3, data = dat.analytic, estimand = \"ATE\", verbose = FALSE,\n                stop.method = c(\"es.max\"), n.trees = 200, sampw = dat.analytic$weight)\nsummary(ps.fit3)\n#> Summary of pairwise comparisons:\n#>   max.std.eff.sz min.p max.ks min.ks.pval stop.method\n#> 1      0.4134866     0      1           0         unw\n#> 2      0.1880231     0      1           0      es.max\n#> \n#> Sample sizes and effective sample sizes:\n#>   treatment     n ESS:es.max\n#> 1  Inactive 17569   7361.137\n#> 2  Moderate 10557   4875.807\n#> 3    Active 11508   5376.500\n\n\n# Propensity scores\nps3 <- data.frame(Active = ps.fit3$psList$Active$ps,\n                 Moderate = ps.fit3$psList$Moderate$ps,\n                 Inactive = ps.fit3$psList$Inactive$ps)\nnames(ps3) <- c(\"Active\",\"Moderate\",\"Inactive\")\nhead(ps3)\n\n\n\n  \n\n\n\n# Summary\napply(ps3, 2, summary)\n#>            Active  Moderate  Inactive\n#> Min.    0.1827539 0.1884745 0.2759573\n#> 1st Qu. 0.2316300 0.2478217 0.3726945\n#> Median  0.2664345 0.2666218 0.4485734\n#> Mean    0.2894693 0.2679683 0.4420585\n#> 3rd Qu. 0.3343953 0.2859762 0.5015505\n#> Max.    0.5330670 0.3255325 0.6389846\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw3 <- ifelse(dat.analytic$phyact==\"Active\", 1/ps3$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps3$Moderate, \n                                  1/ps3$Inactive))\nwith(dat.analytic, by(ipw3, phyact, summary))\n#> phyact: Inactive\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.565   1.877   2.086   2.206   2.484   3.624 \n#> ------------------------------------------------------------ \n#> phyact: Moderate\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   3.072   3.415   3.652   3.709   3.947   5.306 \n#> ------------------------------------------------------------ \n#> phyact: Active\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.876   2.716   3.229   3.320   3.952   5.465\n\nWeights are not large compared to options 1 and 2.\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars <- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design <- svydesign(id = ~1, weights = ~ipw3, data = dat.analytic)\n\n# Table 1\ntabw <- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#>                Stratified by phyact\n#>                 level             Inactive        Moderate       \n#>   n                               38763.2         39159.6        \n#>   age (%)       20-29 years        6086.5 (15.7)   6641.5 (17.0) \n#>                 30-39 years        7624.6 (19.7)   8171.6 (20.9) \n#>                 40-49 years        7057.4 (18.2)   7073.0 (18.1) \n#>                 50-59 years        6267.5 (16.2)   5894.9 (15.1) \n#>                 60-64 years        2399.6 ( 6.2)   2390.7 ( 6.1) \n#>                 65 years and over  6883.8 (17.8)   5859.8 (15.0) \n#>                 teen               2443.7 ( 6.3)   3128.1 ( 8.0) \n#>   sex (%)       Female            20928.4 (54.0)  21097.6 (53.9) \n#>                 Male              17834.8 (46.0)  18062.0 (46.1) \n#>   married (%)   not single        21244.6 (54.8)  21279.5 (54.3) \n#>                 single            17518.5 (45.2)  17880.1 (45.7) \n#>   race (%)      Non-white          3683.4 ( 9.5)   3666.8 ( 9.4) \n#>                 White             35079.7 (90.5)  35492.8 (90.6) \n#>   edu (%)       < 2ndary           7610.9 (19.6)   6692.3 (17.1) \n#>                 2nd grad.          7088.9 (18.3)   6693.3 (17.1) \n#>                 Other 2nd grad.    3558.1 ( 9.2)   3619.6 ( 9.2) \n#>                 Post-2nd grad.    20505.3 (52.9)  22154.2 (56.6) \n#>   income (%)    $29,999 or less    9138.3 (23.6)   7750.6 (19.8) \n#>                 $30,000-$49,999    8400.8 (21.7)   7956.3 (20.3) \n#>                 $50,000-$79,999    9946.0 (25.7)  10696.8 (27.3) \n#>                 $80,000 or more   11278.0 (29.1)  12755.9 (32.6) \n#>   bmi (%)       Underweight        1195.7 ( 3.1)    967.7 ( 2.5) \n#>                 healthy weight    16766.4 (43.3)  18791.6 (48.0) \n#>                 Overweight        20801.0 (53.7)  19400.2 (49.5) \n#>   doctor (%)    No                 5070.5 (13.1)   4827.3 (12.3) \n#>                 Yes               33692.7 (86.9)  34332.3 (87.7) \n#>   stress (%)    Not too stressed  29885.7 (77.1)  31099.5 (79.4) \n#>                 stressed           8877.5 (22.9)   8060.0 (20.6) \n#>   smoke (%)     Current smoker    10737.3 (27.7)   9401.4 (24.0) \n#>                 Former smoker     15204.3 (39.2)  16211.3 (41.4) \n#>                 Never smoker      12821.5 (33.1)  13546.9 (34.6) \n#>   drink (%)     Current drinker   31123.7 (80.3)  33043.2 (84.4) \n#>                 Former driker      5322.0 (13.7)   4301.7 (11.0) \n#>                 Never drank        2317.5 ( 6.0)   1814.6 ( 4.6) \n#>   fruit (%)     0-3 daily serving 10529.0 (27.2)   8996.8 (23.0) \n#>                 4-6 daily serving 19466.5 (50.2)  19851.5 (50.7) \n#>                 6+ daily serving   8767.6 (22.6)  10311.3 (26.3) \n#>   bp (%)        No                31712.5 (81.8)  33335.5 (85.1) \n#>                 Yes                7050.7 (18.2)   5824.1 (14.9) \n#>   diab (%)      No                36326.4 (93.7)  37499.3 (95.8) \n#>                 Yes                2436.8 ( 6.3)   1660.2 ( 4.2) \n#>   OA (%)        Control           33060.8 (85.3)  34579.5 (88.3) \n#>                 OA                 5702.3 (14.7)   4580.0 (11.7) \n#>   immigrate (%) not immigrant     36812.8 (95.0)  37425.9 (95.6) \n#>                 recent             1950.3 ( 5.0)   1733.7 ( 4.4) \n#>                Stratified by phyact\n#>                 Active          SMD   \n#>   n             38211.2               \n#>   age (%)        6567.9 (17.2)   0.145\n#>                  8208.6 (21.5)        \n#>                  7313.3 (19.1)        \n#>                  5555.3 (14.5)        \n#>                  2186.9 ( 5.7)        \n#>                  4539.2 (11.9)        \n#>                  3840.0 (10.0)        \n#>   sex (%)       18441.4 (48.3)   0.077\n#>                 19769.8 (51.7)        \n#>   married (%)   20012.7 (52.4)   0.033\n#>                 18198.5 (47.6)        \n#>   race (%)       3397.2 ( 8.9)   0.014\n#>                 34814.0 (91.1)        \n#>   edu (%)        6186.6 (16.2)   0.081\n#>                  6159.6 (16.1)        \n#>                  3630.7 ( 9.5)        \n#>                 22234.2 (58.2)        \n#>   income (%)     6758.9 (17.7)   0.120\n#>                  7420.8 (19.4)        \n#>                 10612.9 (27.8)        \n#>                 13418.7 (35.1)        \n#>   bmi (%)        1041.5 ( 2.7)   0.112\n#>                 19648.4 (51.4)        \n#>                 17521.2 (45.9)        \n#>   doctor (%)     5001.8 (13.1)   0.015\n#>                 33209.4 (86.9)        \n#>   stress (%)    30845.7 (80.7)   0.059\n#>                  7365.5 (19.3)        \n#>   smoke (%)      8531.3 (22.3)   0.083\n#>                 16200.7 (42.4)        \n#>                 13479.2 (35.3)        \n#>   drink (%)     32817.1 (85.9)   0.101\n#>                  3703.9 ( 9.7)        \n#>                  1690.1 ( 4.4)        \n#>   fruit (%)      7924.2 (20.7)   0.120\n#>                 19284.4 (50.5)        \n#>                 11002.5 (28.8)        \n#>   bp (%)        33698.1 (88.2)   0.120\n#>                  4513.1 (11.8)        \n#>   diab (%)      36930.8 (96.6)   0.092\n#>                  1280.4 ( 3.4)        \n#>   OA (%)        34640.5 (90.7)   0.110\n#>                  3570.7 ( 9.3)        \n#>   immigrate (%) 36764.3 (96.2)   0.040\n#>                  1446.8 ( 3.8)\n\nAll covariates are balanced in terms of SMD.\n\nplot(ps.fit3, color = TRUE, plots = 2, figureRows = 1)\n\n\n\n#> [[1]]\n\n\n\n#> \n#> [[2]]\n\n\n\n#> \n#> [[3]]\n\n\n\n\n\nplot(ps.fit3, plots = 3, color=TRUE, pairwiseMax = FALSE)\n\n\n\n#> [[1]]\n\n\n\n#> \n#> [[2]]\n\n\n\n#> \n#> [[3]]\n\n\n\n\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind <- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] <- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight3 <- with(dat.analytic, ipw3 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight3 <- 0\ndat.full$ATEweight3[dat.full$ID %in% dat.analytic$ID] <- dat.analytic$ATEweight3\n\n# Survey setup with full data \nw.design0 <- svydesign(id = ~1, weights = ~ATEweight3, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 <- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop <- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n\n  \n\n\n\n# Outcome model\nfit3 <- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit3, confint.method = \"robust\", pvalue.method = \"robust\")\n#>  Variable    Units OddsRatio       CI.95   p-value \n#>    phyact Inactive       Ref                       \n#>           Moderate      0.82 [0.56;1.21]   0.32243 \n#>             Active      0.65 [0.44;0.96]   0.03204\n\nOther approches for multiple treatments\nNot covered here, but possible to do in a multiple treatments context:\n\nPropensity score matching\nPropensity score stratification\nMarginal mean weighting"
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R functions (S)",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n bal.plot \n    cobalt \n    To produce a overalp/balance plot for propensity scoes \n  \n\n bal.tab \n    cobalt \n    To check the balance at each category of covariates \n  \n\n CreateCatTable \n    tableone \n    To create a frequency table with categorical variables only \n  \n\n do.call \n    base \n    To execute a function call \n  \n\n love.plot \n    cobalt \n    To plot the standardized mean differences at each category of covariates \n  \n\n match.data \n    MatchIt \n    To extract the matched dataste from a matchit object \n  \n\n matchit \n    MatchIt \n    To match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m. \n  \n\n rownames \n    base \n    Names of the rows"
  },
  {
    "objectID": "propensityscoreQ.html#live-quiz",
    "href": "propensityscoreQ.html#live-quiz",
    "title": "Quiz (S)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "propensityscoreQ.html#download-quiz",
    "href": "propensityscoreQ.html#download-quiz",
    "title": "Quiz (S)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "propensityscoreS.html",
    "href": "propensityscoreS.html",
    "title": "App (S)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from a propensity score analysis using complex survey data, and implications of choosing different list of covariates for the propensity score model fit in each step of the analysis.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveS\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, ggplot2, survey, tableone, tibble, dplyr, tidyr, broom.mixed and jtools packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app."
  },
  {
    "objectID": "propensityscoreE.html#problem-statement",
    "href": "propensityscoreE.html#problem-statement",
    "title": "Exercise (S)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Moon et al. (2021):\nWe will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\n\nSEQN: Respondent sequence number\nstrata: Masked pseudo strata (strata is nested within PSU)\npsu: Masked pseudo PSU\nsurvey.weight: Full sample 8 year interview weight divided by 4\nsurvey.cycle: NHANES cycle\n\nOutcome variable\n\ncvd: Cardiovascular disease\n\nExposure\n\nnocturia: Binary nocturia\n\nConfounders and other variables\n\nage: Age in years at screening\ngender: Gender\nrace: Race/Ethnicity\nsmoking: 100+ cigarettes in life\nalcohol: Alcohol consumption (12+ drinks in 1 year)\nsleep: Sleep duration, h\nbmi: Body Mass Index in kg/m\\(^2\\)\n\nsystolic: Systolic blood pressure, mmHg\ndiastolic: Diastolic blood pressure, mmHg\ntcholesterol: Total cholesterol, mg/dl\ntriglycerides: Triglycerides, mg/dl\nhdl: HDL‐cholesterol, mg/dl\ndiabetes: Diabetes mellitus\nhypertension: Hypertension\n\nTwo important warnings before we start:\n\nIn this paper, there is insufficient information to create the analytic dataset. This is mainly because of not sufficiently defining the covariates and not explicitly explaining the inclusion/exclusion criteria.\nThe authors did incorrect analyses. For example, they didn’t consider survey features. Since we will utilize survey features in our analysis, our results will likely be different than the results shown by the authors in Table 2."
  },
  {
    "objectID": "propensityscoreE.html#question-1-0-grade",
    "href": "propensityscoreE.html#question-1-0-grade",
    "title": "Exercise (S)",
    "section": "Question 1: [0% grade]",
    "text": "Question 1: [0% grade]\n1(a) Importing dataset\n\nload(file = \"Data/propensityscore/Moon2021.RData\")\n\n1(b) Subsetting according to eligibility\n\n# Age 20+\ndat.analytic <- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic <- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars <- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic <- dat.analytic[,vars]\n\n# Complete case\ndat.analytic <- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#> [1] 15404    21\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, hypertension, diabetes mellitus, and survey cycles.\n\nHint 1: the authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nHint 2: Adjust the model for age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, and survey cycle.\nHint 3: Use Publish package to report the odds ratio with the 95% CI and p-value.\n\n\n# Create an indicator variable in the full data\ndat.full$miss <- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] <- 0\n\n# Design setup\nsvy.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design <- subset(svy.design0, miss == 0)\n\n# Design-adjusted logistic\nfit.logit <- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#>       Variable              Units OddsRatio         CI.95     p-value \n#>       nocturia                 <2       Ref                           \n#>                                2+      1.44   [1.21;1.71]   0.0001496 \n#>            age            [20,40)       Ref                           \n#>                           [40,60)      4.21   [3.05;5.82]     < 1e-04 \n#>                           [60,80)     11.46  [7.89;16.64]     < 1e-04 \n#>                          [80,Inf)     25.28 [17.51;36.50]     < 1e-04 \n#>         gender               Male       Ref                           \n#>                            Female      0.68   [0.58;0.79]     < 1e-04 \n#>           race          Hispanics       Ref                           \n#>                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#>                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#>                       Other races      1.55   [1.05;2.30]   0.0319116 \n#>            bmi                         1.02   [1.01;1.03]   0.0003273 \n#>        smoking                 No       Ref                           \n#>                               Yes      1.74   [1.46;2.07]     < 1e-04 \n#>        alcohol                 No       Ref                           \n#>                               Yes      0.92   [0.59;1.45]   0.7273627 \n#>          sleep                         0.96   [0.90;1.01]   0.1146287 \n#>   tcholesterol                         0.99   [0.99;0.99]     < 1e-04 \n#>  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#>            hdl                         0.99   [0.98;1.00]   0.0416900 \n#>   hypertension                 No       Ref                           \n#>                               Yes      2.73   [2.27;3.29]     < 1e-04 \n#>       diabetes                 No       Ref                           \n#>                               Yes      1.83   [1.51;2.22]     < 1e-04 \n#>   survey.cycle            2005-06       Ref                           \n#>                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#>                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#>                           2011-11      0.82   [0.68;0.99]   0.0398975"
  },
  {
    "objectID": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "href": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "title": "Exercise (S)",
    "section": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]",
    "text": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Question 1) using the propensity score 1:1 matching analysis as per DuGoff et al. (2014) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia <2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare your results with the results reported by the authors. [Expected answer: 2-3 sentences]\n\n\n# your codes here"
  },
  {
    "objectID": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "href": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "title": "Exercise (S)",
    "section": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]",
    "text": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Questions 1 and 2) using the propensity score 1:4 matching analysis as per Austin et al. (2018) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia <2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing. Remember, you need to multiply matching weights and survey weights to get survey-based estimates.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare the results with Question 2. What’s the overall conclusion? [Expected answer: 2-3 sentences]\n\n\n# your codes here"
  },
  {
    "objectID": "machinelearning.html#background",
    "href": "machinelearning.html#background",
    "title": "Machine learning (ML)",
    "section": "Background",
    "text": "Background\nThe chapter encompasses a series of instructional content that sequentially explores various facets of predictive modeling and machine learning, connecting them with a previous chapter. Beginning with a tutorial that revisits the application of regression for predicting continuous outcomes, it underscores the importance of understanding prediction error and overfitting, and introduces foundational machine learning concepts. The subsequent tutorial emphasizes the pivotal role of data splitting in predictive modeling, illustrating how to partition data into training and test sets and evaluate model performance across different data scenarios. Moving forward, the concept of cross-validation is explored, detailing the k-fold cross-validation method and demonstrating its implementation both manually and using the caret package. Another tutorial navigates through predicting binary outcomes using logistic regression, evaluating model performance using various metrics, and employing k-fold cross-validation.\nThe series then delves into supervised learning, exploring regularization techniques, decision trees, and ensemble methods, while employing various model evaluation metrics and cross-validation techniques. Lastly, unsupervised learning is introduced with a focus on the k-means clustering algorithm, discussing its implementation, determining the optimal number of clusters, and addressing associated challenges. Throughout, the tutorials provide practical examples, code snippets, and visual aids, offering a comprehensive and applied exploration of predictive modeling and machine learning concepts.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "machinelearning.html#overview-of-tutorials",
    "href": "machinelearning.html#overview-of-tutorials",
    "title": "Machine learning (ML)",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nBuilding upon the foundational concepts introduced in a previous chapter about prediction research, this chapter takes our understanding to the next level. We’ve already explored the fundamentals of propensity score methods, and now it is time to harness the power of machine learning and prediction techniques within a causal inference context, a journey that will ultimately lead us to the concept of double robust estimation methods, such as TMLE.\nBefore we embark on this exciting journey, we will bridge the gap by dedicating this chapter to a deeper exploration of machine learning. By discussing various types of machine learning algorithms and diving into the intricacies of predictive modeling, we aim to provide you with a robust foundation.\n\nRevisiting: Explore Relationships for Continuous Outcomes\nIn this tutorial, the focus is on utilizing regression to predict continuous outcomes, specifically employing multiple linear regression to construct an initial prediction model. The tutorial revisits fundamental concepts related to prediction and introduces foundational ideas pertinent to machine learning, all while using a distinct dataset compared to previous tutorials. The process involves loading a dataset, defining variables, and fitting a model using linear regression. Subsequent sections delve into the creation of a design matrix, obtaining predictions, and measuring prediction error through various metrics like R^2 and RMSE. The tutorial also addresses the critical concept of overfitting, discussing its causes, consequences, and potential solutions, such as internal and external validation methods.\n\n\nRevisiting: Data Spliting\nThis tutorial emphasizes the crucial concept of data splitting in the context of predictive modeling and machine learning, utilizing a different dataset than previous tutorials. The process begins with loading a dataset and then strategically splitting it into training and test subsets, ensuring a robust approach to model validation. A model is trained using the training data, and its performance is evaluated using various metrics, such as R^2 and RMSE, through a custom function that extracts these performance measures. This function facilitates the evaluation of the model’s predictive accuracy and fit by applying it to different datasets (training, test, and the entire dataset), thereby enabling a comprehensive understanding of the model’s performance across different data scenarios.\n\n\nRevisiting: Cross-vaildation\nThis tutorial delves into the concept of cross-validation, a pivotal technique in predictive modeling and machine learning, using a distinct dataset for illustrative purposes. The process of k-fold cross-validation is explored, wherein the data is partitioned into ‘k’ subsets, and the model is trained ‘k’ times, each time using a different subset as the test set and the remaining data as the training set. This method is employed to assess the model’s predictive performance and to mitigate the risk of results being dependent on the initial data split. The tutorial demonstrates both manual calculations for individual folds and the utilization of the caret package to automate the cross-validation process, thereby providing a comprehensive overview of the method.\n\n\nRevisiting: Explore Relationships for Binary Outcomes\nThe tutorial navigates through the concept of predicting binary outcomes using logistic regression, emphasizing the application of various model evaluation metrics and methodologies in a machine learning context. It begins by ensuring that the outcome variable is treated as a factor and then proceeds to model fitting, where logistic regression is applied to predict a binary outcome. The model’s predictive performance is evaluated using metrics like the Area Under the Curve (AUC) and the Brier Score, which respectively assess the model’s classification accuracy and the mean squared difference between predicted probabilities and the actual outcomes. Furthermore, the tutorial explores k-fold cross-validation using the caret package, providing a robust method to assess the model’s predictive performance while avoiding overfitting. It also touches upon variable selection using stepwise regression with the Akaike Information Criterion (AIC) as a selection criterion.\n\n\nSupervised Learning\nThis tutorial delves into the realm of supervised learning, exploring beyond statistical regression and introducing various machine learning methods tailored for both continuous and binary outcomes. The tutorial explores different regularization techniques, such as LASSO, Ridge, and Elastic Net, which are used to prevent overfitting by penalizing large coefficients in regression models. It also introduces decision trees (CART), which provide a flexible, hierarchical approach to modeling data, and can automatically incorporate non-linear effects and interactions. The tutorial further explores ensemble methods, which combine predictions from multiple models to improve predictive accuracy. Two types of ensemble methods are discussed: Type I, which trains the same model on different samples of the data (e.g., bagging and boosting), and Type II, which trains different models on the same data (e.g., Super Learner). Various model evaluation metrics and cross-validation techniques are utilized throughout to assess and enhance the predictive performance of the models.\n\n\nUnsupervised Learning\nThe tutorial introduces unsupervised learning, with a focus on clustering, a technique that categorizes data into distinct groups based on similarity without using predefined labels. The k-means clustering algorithm is highlighted, which partitions data into k groups by minimizing within-cluster variation, typically using the sum of squares of Euclidean distances. The algorithm iteratively assigns data points to clusters based on the mean of the data points in each cluster and recalculates the cluster means until the cluster assignments no longer change. Various examples illustrate how to apply k-means clustering to different datasets and variable combinations. The tutorial also discusses determining the optimal number of clusters, k, and addresses challenges such as the influence of outliers and the sensitivity to the initial assignment of cluster means.\n\n\nMachine Learning for Health Survey Data using NHIS data\nThis tutorial provides a step-by-step guide to fitting machine learning models with health survey data, specifically using the National Health Interview Survey (NHIS) 2016 dataset to predict high impact chronic pain (HICP) among adults aged 65 years or older. The tutorial covers the use of (1) LASSO and (2) random forest models with sampling weights for population-level predictions. It also discusses the split-sample approach for internal validation, though it acknowledges that cross-validation and bootstrapping may be better alternatives. The tutorial includes the exploration of the analytic dataset, weight normalization, split-sample creation, defining regression formulas, fitting LASSO models with optimal lambda, and evaluating model performance with metrics such as AUC, calibration slope, and Brier score. The same process is then repeated for fitting random forest models, and a variable importance plot is generated to identify influential predictors. Finally, a performance comparison table is provided.\n\n\nReplicate Results from a Published Article\nThe tutorial guides users on implementing machine learning techniques using health survey data, specifically for predicting high impact chronic pain (HICP). It seeks to replicate results from a 2023 article, which utilized the National Health Interview Survey (NHIS) 2016 dataset. For simplicity, complete case data was used in this tutorial. In the original research, the authors developed prediction models for HICP and evaluated their performance within specific sociodemographic groups, such as gender, age groups, and race/ethnicity. They adopted LASSO and random forest models, applying 5-fold cross-validation. They also factored in survey weights in both models to achieve population-level predictions.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences"
  },
  {
    "objectID": "machinelearning0.html#machine-learning",
    "href": "machinelearning0.html#machine-learning",
    "title": "Concepts (L)",
    "section": "Machine learning",
    "text": "Machine learning\nMachine learning focuses on developing algorithms and models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. In epidemiology, machine learning has several important uses and applications. This section is a very basic introduction to machine learning for Epidemiology."
  },
  {
    "objectID": "machinelearning0.html#reading-list",
    "href": "machinelearning0.html#reading-list",
    "title": "Concepts (L)",
    "section": "Reading list",
    "text": "Reading list\nKey reference\n\n(Bi et al. 2019)\n\nFollowing ate optional but useful references\n\n(Karim 2021)\n(Liu et al. 2019)\n(Kuhn et al. 2013)\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)"
  },
  {
    "objectID": "machinelearning0.html#video-lessons",
    "href": "machinelearning0.html#video-lessons",
    "title": "Concepts (L)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning Reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning Terminologies\n\n\n\nWhat is included in this Video Lesson:\n\nReference 0:08\nTypes of Epidemiological models 0:32\nAnalyzing Epidemiological Study data 2:44\nMachine learning 5:42\nTerminologies 9:15\nClassification of Machine learning 12:55\nClassification of Supervised learning 17:30\nOther classifications 18:42\nPopular algorithms 20:06\nDecision tree 20:50\nShrinkage Methods 26:37\nEnsemble methods 37:04\nVariable Importance measure 40:20\nEpidemiologic applications 44:29\nFuture Reading 53:17\n\nThe timestamps are also included in the YouTube video description."
  },
  {
    "objectID": "machinelearning0.html#video-lesson-slides",
    "href": "machinelearning0.html#video-lesson-slides",
    "title": "Concepts (L)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "machinelearning0.html#links",
    "href": "machinelearning0.html#links",
    "title": "Concepts (L)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "machinelearning0.html#references",
    "href": "machinelearning0.html#references",
    "title": "Concepts (L)",
    "section": "References",
    "text": "References\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. “What Is Machine Learning? A Primer for the Epidemiologist.” American Journal of Epidemiology 188 (12): 2222–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKarim, Ehsan. 2021. “Understanding Basics and Usage of Machine Learning in Medical Literature.” 2021. https://ehsanx.github.io/into2ML/.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. “Over-Fitting and Model Tuning.” Applied Predictive Modeling, 61–92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. “How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature.” Jama 322 (18): 1806–16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. “Predictor Selection.” Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395–429."
  },
  {
    "objectID": "machinelearning1.html",
    "href": "machinelearning1.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction model.\nLoad dataset\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n\n  \n\n\n\nPrediction for length of stay\nNow, we show the regression fitting when outcome is continuous (length of stay).\nVariables\n\nbaselinevars <- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#>  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#>  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#>  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#> [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#> [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#> [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#> [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#> [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#> [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#> [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#> [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#> [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#> [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#> [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#> [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#> [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#> [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula1 <- as.formula(paste(\"Length.of.Stay~ \", \n                               paste(baselinevars, \n                                     collapse = \"+\")))\nsaveRDS(out.formula1, file = \"Data/machinelearning/form1.RDS\")\nfit1 <- lm(out.formula1, data = ObsData)\nrequire(Publish)\nadj.fit1 <- publish(fit1, digits=1)$regressionTable\n\n\nout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nadj.fit1\n\n\n\n  \n\n\n\nDesign Matrix\n\nNotations\n\nn is number of observations\np is number of covariates\n\n\n\nExpands factors to a set of dummy variables.\n\ndim(ObsData)\n#> [1] 5735   52\nlength(attr(terms(out.formula1), \"term.labels\"))\n#> [1] 50\n\n\nhead(model.matrix(fit1))\n#>   (Intercept) Disease.categoryCHF Disease.categoryOther Disease.categoryMOSF\n#> 1           1                   0                     1                    0\n#> 2           1                   0                     0                    1\n#> 3           1                   0                     0                    1\n#> 4           1                   0                     0                    0\n#> 5           1                   0                     0                    1\n#> 6           1                   0                     1                    0\n#>   CancerLocalized (Yes) CancerMetastatic Cardiovascular1 Congestive.HF1\n#> 1                     1                0               0              0\n#> 2                     0                0               1              1\n#> 3                     1                0               0              0\n#> 4                     0                0               0              0\n#> 5                     0                0               0              0\n#> 6                     0                0               0              1\n#>   Dementia1 Psychiatric1 Pulmonary1 Renal1 Hepatic1 GI.Bleed1 Tumor1\n#> 1         0            0          1      0        0         0      1\n#> 2         0            0          0      0        0         0      0\n#> 3         0            0          0      0        0         0      1\n#> 4         0            0          0      0        0         0      0\n#> 5         0            0          0      0        0         0      0\n#> 6         0            0          1      0        0         0      0\n#>   Immunosupperssion1 Transfer.hx1 MI1 age[50,60) age[60,70) age[70,80)\n#> 1                  0            0   0          0          0          1\n#> 2                  1            1   0          0          0          1\n#> 3                  1            0   0          0          0          0\n#> 4                  1            0   0          0          0          1\n#> 5                  0            0   0          0          1          0\n#> 6                  0            0   0          0          0          0\n#>   age[80, Inf) sexFemale       edu DASIndex APACHE.score Glasgow.Coma.Score\n#> 1            0         0 12.000000 23.50000           46                  0\n#> 2            0         1 12.000000 14.75195           50                  0\n#> 3            0         1 14.069916 18.13672           82                  0\n#> 4            0         1  9.000000 22.92969           48                  0\n#> 5            0         0  9.945259 21.05078           72                 41\n#> 6            1         1  8.000000 17.50000           38                  0\n#>   blood.pressure         WBC Heart.rate Respiratory.rate Temperature\n#> 1             41 22.09765620        124               10    38.69531\n#> 2             63 28.89843750        137               38    38.89844\n#> 3             57  0.04999542        130               40    36.39844\n#> 4             55 23.29687500         58               26    35.79688\n#> 5             65 29.69921880        125               27    34.79688\n#> 6            115 18.00000000        134               36    39.19531\n#>   PaO2vs.FIO2  Albumin Hematocrit Bilirubin Creatinine Sodium Potassium PaCo2\n#> 1     68.0000 3.500000   58.00000 1.0097656  1.1999512    145  4.000000    40\n#> 2    218.3125 2.599609   32.50000 0.6999512  0.5999756    137  3.299805    34\n#> 3    275.5000 3.500000   21.09766 1.0097656  2.5996094    146  2.899902    16\n#> 4    156.6562 3.500000   26.29688 0.3999634  1.6999512    117  5.799805    30\n#> 5    478.0000 3.500000   24.00000 1.0097656  3.5996094    126  5.799805    17\n#> 6    184.1875 3.099609   30.50000 1.0097656  1.3999023    138  5.399414    68\n#>         PH   Weight DNR.statusYes Medical.insuranceMedicare\n#> 1 7.359375 64.69995             0                         1\n#> 2 7.329102 45.69998             0                         0\n#> 3 7.359375  0.00000             0                         0\n#> 4 7.459961 54.59998             0                         0\n#> 5 7.229492 78.39996             1                         1\n#> 6 7.299805 54.89999             0                         1\n#>   Medical.insuranceMedicare & Medicaid Medical.insuranceNo insurance\n#> 1                                    0                             0\n#> 2                                    0                             0\n#> 3                                    0                             0\n#> 4                                    0                             0\n#> 5                                    0                             0\n#> 6                                    0                             0\n#>   Medical.insurancePrivate Medical.insurancePrivate & Medicare\n#> 1                        0                                   0\n#> 2                        0                                   1\n#> 3                        1                                   0\n#> 4                        0                                   1\n#> 5                        0                                   0\n#> 6                        0                                   0\n#>   Respiratory.DiagYes Cardiovascular.DiagYes Neurological.DiagYes\n#> 1                   1                      1                    0\n#> 2                   0                      0                    0\n#> 3                   0                      1                    0\n#> 4                   1                      0                    0\n#> 5                   0                      1                    0\n#> 6                   1                      0                    0\n#>   Gastrointestinal.DiagYes Renal.DiagYes Metabolic.DiagYes Hematologic.DiagYes\n#> 1                        0             0                 0                   0\n#> 2                        0             0                 0                   0\n#> 3                        0             0                 0                   0\n#> 4                        0             0                 0                   0\n#> 5                        0             0                 0                   0\n#> 6                        0             0                 0                   0\n#>   Sepsis.DiagYes Trauma.DiagYes Orthopedic.DiagYes raceblack raceother\n#> 1              0              0                  0         0         0\n#> 2              1              0                  0         0         0\n#> 3              0              0                  0         0         0\n#> 4              0              0                  0         0         0\n#> 5              0              0                  0         0         0\n#> 6              0              0                  0         0         0\n#>   income$25-$50k income> $50k incomeUnder $11k RHC.use\n#> 1              0            0                1       0\n#> 2              0            0                1       1\n#> 3              1            0                0       1\n#> 4              0            0                0       0\n#> 5              0            0                1       1\n#> 6              0            0                1       0\ndim(model.matrix(fit1))\n#> [1] 5735   64\np <- dim(model.matrix(fit1))[2] # intercept + slopes\np\n#> [1] 64\n\nObtain prediction\n\nobs.y <- ObsData$Length.of.Stay\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    2.00    7.00   14.00   21.56   25.00  394.00\n# Predict the above fit on ObsData data\npred.y1 <- predict(fit1, ObsData)\nsummary(pred.y1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  -32.76   16.62   21.96   21.56   26.73   42.67\nn <- length(pred.y1)\nn\n#> [1] 5735\nplot(obs.y,pred.y1)\nlines(lowess(obs.y,pred.y1), col = \"red\")\n\n\n\n\nMeasuring prediction error\nPrediction error measures how well the model can predict the outcome for new data that were not used in developing the prediction model.\n\nBias reduced for models with more variables\nUnimportant variables lead to noise / variability\nBias variance trade-off / need penalization\n\nR2\nThe provided information describes a statistical context involving a dataset of n values, \\(y_1, ..., y_n\\) (referred to as \\(y_i\\) or as a vector \\(y = [y_1,...,y_n]^T\\)), each paired with a fitted value \\(f_1,...,f_n\\) (denoted as \\(f_i\\) or sometimes \\(\\hat{y_i}\\), and as a vector \\(f\\)). The residuals, represented as \\(e_i\\), are defined as the differences between the observed and the fitted values: $ e_i = y_i − f_i$\nThe mean of the observed data is denoted by \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i \\]\nThe variability of the dataset can be quantified using two sums of squares formulas: 1. Residual Sum of Squares (SSres) or SSE: It quantifies the variance remaining in the data after fitting a model, calculated as: \\[ SS_{res} = \\sum_{i}(y_i - f_i)^2 = \\sum_{i}e_i^2 \\] 2. Total Sum of Squares (SStot) or SST: It represents the total variance in the observed data, calculated as: \\[ SS_{tot} = \\sum_{i}(y_i - \\bar{y})^2 \\]\nThe Coefficient of Determination (R²) or R.2, which provides a measure of how well the model’s predictions match the observed data, is defined as: \\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nIn the ideal scenario where the model fits the data perfectly, we have \\(SS_{res} = 0\\) and thus \\(R^2 = 1\\). Conversely, a baseline model, which always predicts the mean \\(\\bar{y}\\) of the observed data, would yield \\(R^2 = 0\\). Models performing worse than this baseline model would result in a negative R² value. This metric is widely utilized in regression analysis to evaluate model performance, where a higher R² indicates a better fit of the model to the data.\n\n# Find SSE\nSSE <- sum( (obs.y - pred.y1)^2 )\nSSE\n#> [1] 3536398\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 3836690\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.07826832\nrequire(caret)\ncaret::R2(pred.y1, obs.y)\n#> [1] 0.07826832\n\nref\nRMSE\n\n# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 24.97185\ncaret::RMSE(pred.y1, obs.y)\n#> [1] 24.83212\n\nSee (Wikipedia 2023b)\nAdj R2\nThe Adjusted R² statistic modifies the \\(R^2\\) value to counteract the automatic increase of \\(R^2\\) when extra explanatory variables are added to a model, even if they do not improve the model fit. This adjustment is crucial for ensuring that the metric offers a reliable indication of the explanatory power of the model, especially in multiple regression where several predictors are involved.\nThe commonly used formula is defined as:\n\\[\n\\bar{R}^{2} = 1 - \\frac{SS_{\\text{res}} / df_{\\text{res}}}{SS_{\\text{tot}} / df_{\\text{tot}}}\n\\]\nWhere:\n\n\n\\(SS_{\\text{res}}\\) and \\(SS_{\\text{tot}}\\) represent the residual and total sums of squares respectively.\n\n\\(df_{\\text{res}}\\) and \\(df_{\\text{tot}}\\) refer to the degrees of freedom of the residual and total sums of squares. Usually, \\(df_{\\text{res}} = n - p\\) and \\(df_{\\text{tot}} = n - 1\\), where:\n\n\n\\(n\\) signifies the sample size.\n\n\\(p\\) denotes the number of variables in the model.\n\n\n\nThis metric plays a vital role in model selection and safeguards against overfitting by penalizing the inclusion of non-informative variables\nThe alternate formula is:\n\\[\n\\bar{R}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n\\]\nThis formula modifies the \\(R^2\\) value, accounting for the number of predictors and offering a more parsimonious model fit measure.\n\n# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p-1))\nadjR2\n#> [1] 0.06786429\n\nSee (Wikipedia 2023a)\nOverfitting and Optimism\n\nModel usually performs very well in the empirical data where the model was fitted in the same data (optimistic)\nModel performs poorly in the new data (generalization is not as good)\n\nCauses\n\nModel determined by data at hand without expert opinion\nToo many model parameters (\\(age\\), \\(age^2\\), \\(age^3\\)) / predictors\nToo small dataset (training) / data too noisy\nConsequences\n\nOverestimation of effects of predictors\nReduction in model performance in new observations\nProposed solutions\nWe generally use procedures such as\n\nInternal validation\n\nsample splitting\ncross-validation\nbootstrap\n\n\nExternal validation\n\nTemporal\nGeographical\nDifferent data source to calculate same variable\nDifferent disease\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "machinelearning2.html",
    "href": "machinelearning2.html",
    "title": "Data spliting",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nLoad dataset\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n\n  \n\n\n\nSee (KDnuggets 2023; Kuhn 2023)\n\n# Using a seed to randomize in a reproducible way \nset.seed(123)\nrequire(caret)\nsplit<-createDataPartition(y = ObsData$Length.of.Stay, \n                           p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:4017, 1] 1 2 3 4 5 6 7 8 9 10 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 4017    1\ndim(ObsData)*.7 # approximate train data\n#> [1] 4014.5   36.4\ndim(ObsData)*(1-.7) # approximate train data\n#> [1] 1720.5   15.6\n\nSplit the data\n\n# create train data\ntrain.data<-ObsData[split,]\ndim(train.data)\n#> [1] 4017   52\n# create test data\ntest.data<-ObsData[-split,]\ndim(test.data)\n#> [1] 1718   52\n\nTrain the model\n\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nfit.train1<-lm(out.formula1, data = train.data)\n# summary(fit.train1)\n\nFunction that gives performance measures\n\nperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  # sample size\n  n <- length(pred.y)\n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  # returning object\n  return(res)\n}\n\nExtract performance measures\n\nperform(new.data=train.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 4017 64 0.081 0.067 24.647\nperform(new.data=test.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 1718 64 0.056  0.02 25.488\nperform(new.data=ObsData,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 5735 64 0.073 0.063 24.902\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html."
  },
  {
    "objectID": "machinelearning3.html",
    "href": "machinelearning3.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nNow, we will describe the ideas of cross-validation.\nLoad previously saved data\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\n\nk-fold cross-vaildation\nSee (Wikipedia 2023)\n\n\n\n\n\n\nk = 5\ndim(ObsData)\n#> [1] 5735   52\nset.seed(567)\n# create folds (based on outcome)\nfolds <- createFolds(ObsData$Length.of.Stay, k = k, \n                     list = TRUE, returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\ndim(ObsData)*4/5 # approximate training data size\n#> [1] 4588.0   41.6\ndim(ObsData)/5  # approximate test data size\n#> [1] 1147.0   10.4\nlength(folds[[1]])\n#> [1] 4588\nlength(folds[[5]])\n#> [1] 4587\nstr(folds[[1]])\n#>  int [1:4588] 1 2 4 6 7 8 9 10 11 13 ...\nstr(folds[[5]])\n#>  int [1:4587] 1 3 5 6 7 8 10 11 12 13 ...\n\nCalculation for Fold 1\n\nfold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 4 6 7 8\nfold1.train <- ObsData[fold1.train.ids,]\nfold1.test <- ObsData[-fold1.train.ids,]\nout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nmodel.fit <- lm(out.formula1, data = fold1.train)\npredictions <- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#>         n  p    R2  adjR2  RMSE\n#> [1,] 1147 64 0.051 -0.004 24.86\n\nCalculation for Fold 2\n\nfold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 2 3 4 5 6 7\nfold1.train <- ObsData[fold1.train.ids,]\nfold1.test <- ObsData[-fold1.train.ids,]\nmodel.fit <- lm(out.formula1, data = fold1.train)\npredictions <- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 1147 64 0.066 0.011 24.714\n\nUsing caret package to automate\nSee (Kuhn 2023)\n\n# Using Caret package\nset.seed(504)\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n# fit the model with formula = out.formula1\n# use training method lm\nfit.cv<-train(out.formula1, trControl = ctrl,\n               data = ObsData, method = \"lm\")\nfit.cv\n#> Linear Regression \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.05478  0.05980578  15.19515\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n# extract results from each test data \nsummary.res <- fit.cv$resample\nsummary.res\n\n\n\n  \n\n\nmean(fit.cv$resample$Rsquared)\n#> [1] 0.05980578\nsd(fit.cv$resample$Rsquared)\n#> [1] 0.01204451\nmean(fit.cv$resample$RMSE)\n#> [1] 25.05478\nsd(fit.cv$resample$RMSE)\n#> [1] 2.240366\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "machinelearning4.html",
    "href": "machinelearning4.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of binary outcomes. We will use logistic regression to build the first prediction model.\nRead previously saved data\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nOutcome levels (factor)\n\nLabel\n\nPossible values of outcome\n\n\n\n\nlevels(ObsData$Death)=c(\"No\",\"Yes\") # this is useful for caret\n# ref: https://tinyurl.com/caretbin\nclass(ObsData$Death)\n#> [1] \"factor\"\ntable(ObsData$Death)\n#> \n#>   No  Yes \n#> 2013 3722\n\nMeasuring prediction error\n\n\nBrier score\n\nBrier score 0 means perfect prediction, and\nclose to zero means better prediction,\n1 being the worst prediction.\nLess accurate forecasts get higher score in Brier score.\n\n\n\nAUC\n\nThe area under a ROC curve is called as a c statistics.\nc being 0.5 means random prediction and\n1 indicates perfect prediction\n\n\nPrediction for death\nIn this section, we show the regression fitting when outcome is binary (death).\nVariables\n\nbaselinevars <- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#>  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#>  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#>  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#> [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#> [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#> [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#> [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#> [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#> [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#> [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#> [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#> [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#> [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#> [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#> [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#> [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#> [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula2 <- as.formula(paste(\"Death~ \", paste(baselinevars, collapse = \"+\")))\nsaveRDS(out.formula2, file = \"Data/machinelearning/form2.RDS\")\nfit2 <- glm(out.formula2, data = ObsData, \n            family = binomial(link = \"logit\"))\nrequire(Publish)\nadj.fit2 <- publish(fit2, digits=1)$regressionTable\n\n\nout.formula2\n#> Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#>     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#>     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#>     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#>     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#>     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#>     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nadj.fit2\n\n\n\n  \n\n\n\nMeasuring prediction error\nAUC\n\nrequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- predict(fit2, type = \"response\")\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.7682\nplot(rocobj)\n\n\n\nauc(rocobj)\n#> Area under the curve: 0.7682\n\nBrier Score\n\nrequire(DescTools)\nBrierScore(fit2)\n#> [1] 0.1812502\n\nCross-validation using caret\nBasic setup\n\n# Using Caret package\nset.seed(504)\n\n# make a 5-fold CV\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n\n# fit the model with formula = out.formula2\n# use training method glm (have to specify family)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\")\nfit.cv.bin\n#> Generalized Linear Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7545115  0.4659618  0.8535653\n\nExtract results from each test data\n\nsummary.res <- fit.cv.bin$resample\nsummary.res\n\n\n\n  \n\n\nmean(fit.cv.bin$resample$ROC)\n#> [1] 0.7545115\nsd(fit.cv.bin$resample$ROC)\n#> [1] 0.01651437\n\nMore options\n\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\",\n              preProc = c(\"center\", \"scale\"))\nfit.cv.bin\n#> Generalized Linear Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> Pre-processing: centered (63), scaled (63) \n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7548047  0.4629717  0.8530367\n\nVariable selection\nWe can also use stepwise regression that uses AIC as a criterion.\n\nset.seed(504)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin.aic<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmStepAIC\",\n               direction =\"backward\",\n              family = binomial(),\n              metric=\"ROC\")\n\n\nfit.cv.bin.aic\n#> Generalized Linear Model with Stepwise Feature Selection \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens      Spec     \n#>   0.7540424  0.464468  0.8562535\nsummary(fit.cv.bin.aic)\n#> \n#> Call:\n#> NULL\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.8626  -0.9960   0.5052   0.8638   1.9578  \n#> \n#> Coefficients:\n#>                                          Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)                             1.0783624  0.7822168   1.379 0.168019\n#> Disease.categoryOther                   0.4495099  0.0919860   4.887 1.03e-06\n#> `CancerLocalized (Yes)`                 1.8942512  0.5501880   3.443 0.000575\n#> CancerMetastatic                        3.2703316  0.5858715   5.582 2.38e-08\n#> Cardiovascular1                         0.2386749  0.0939617   2.540 0.011081\n#> Congestive.HF1                          0.4539010  0.0971624   4.672 2.99e-06\n#> Dementia1                               0.2380213  0.1162903   2.047 0.040679\n#> Hepatic1                                0.3593093  0.1541762   2.331 0.019779\n#> Tumor1                                 -1.2455123  0.5542624  -2.247 0.024630\n#> Immunosupperssion1                      0.2174294  0.0730803   2.975 0.002928\n#> Transfer.hx1                           -0.1849029  0.0945679  -1.955 0.050555\n#> `age[50,60)`                            0.3621248  0.0984288   3.679 0.000234\n#> `age[60,70)`                            0.6941924  0.0968434   7.168 7.60e-13\n#> `age[70,80)`                            0.6804939  0.1126637   6.040 1.54e-09\n#> `age[80, Inf)`                          0.9833851  0.1410563   6.972 3.13e-12\n#> sexFemale                              -0.2805950  0.0653527  -4.294 1.76e-05\n#> DASIndex                               -0.0429272  0.0062191  -6.902 5.11e-12\n#> APACHE.score                            0.0174907  0.0020017   8.738  < 2e-16\n#> Glasgow.Coma.Score                      0.0093657  0.0012563   7.455 9.00e-14\n#> WBC                                     0.0044518  0.0030090   1.479 0.139009\n#> Temperature                            -0.0524703  0.0192757  -2.722 0.006487\n#> PaO2vs.FIO2                             0.0004741  0.0003054   1.552 0.120548\n#> Hematocrit                             -0.0154796  0.0041593  -3.722 0.000198\n#> Bilirubin                               0.0313087  0.0094004   3.331 0.000867\n#> Weight                                 -0.0031548  0.0011213  -2.813 0.004902\n#> DNR.statusYes                           0.9347360  0.1326924   7.044 1.86e-12\n#> Medical.insuranceMedicare               0.4764895  0.1257582   3.789 0.000151\n#> `Medical.insuranceMedicare & Medicaid`  0.3364916  0.1584757   2.123 0.033729\n#> `Medical.insuranceNo insurance`         0.3711345  0.1568820   2.366 0.017996\n#> Medical.insurancePrivate                0.2632637  0.1139805   2.310 0.020903\n#> `Medical.insurancePrivate & Medicare`   0.2819715  0.1313101   2.147 0.031764\n#> Respiratory.DiagYes                     0.1393974  0.0769026   1.813 0.069886\n#> Cardiovascular.DiagYes                  0.1804967  0.0836679   2.157 0.030982\n#> Neurological.DiagYes                    0.4320266  0.1189357   3.632 0.000281\n#> Gastrointestinal.DiagYes                0.2819563  0.1092206   2.582 0.009836\n#> Hematologic.DiagYes                     0.9734424  0.1651363   5.895 3.75e-09\n#> Sepsis.DiagYes                          0.1539651  0.0943235   1.632 0.102614\n#> `incomeUnder $11k`                      0.2151437  0.0689392   3.121 0.001804\n#> RHC.use                                 0.3552053  0.0713632   4.977 6.44e-07\n#>                                           \n#> (Intercept)                               \n#> Disease.categoryOther                  ***\n#> `CancerLocalized (Yes)`                ***\n#> CancerMetastatic                       ***\n#> Cardiovascular1                        *  \n#> Congestive.HF1                         ***\n#> Dementia1                              *  \n#> Hepatic1                               *  \n#> Tumor1                                 *  \n#> Immunosupperssion1                     ** \n#> Transfer.hx1                           .  \n#> `age[50,60)`                           ***\n#> `age[60,70)`                           ***\n#> `age[70,80)`                           ***\n#> `age[80, Inf)`                         ***\n#> sexFemale                              ***\n#> DASIndex                               ***\n#> APACHE.score                           ***\n#> Glasgow.Coma.Score                     ***\n#> WBC                                       \n#> Temperature                            ** \n#> PaO2vs.FIO2                               \n#> Hematocrit                             ***\n#> Bilirubin                              ***\n#> Weight                                 ** \n#> DNR.statusYes                          ***\n#> Medical.insuranceMedicare              ***\n#> `Medical.insuranceMedicare & Medicaid` *  \n#> `Medical.insuranceNo insurance`        *  \n#> Medical.insurancePrivate               *  \n#> `Medical.insurancePrivate & Medicare`  *  \n#> Respiratory.DiagYes                    .  \n#> Cardiovascular.DiagYes                 *  \n#> Neurological.DiagYes                   ***\n#> Gastrointestinal.DiagYes               ** \n#> Hematologic.DiagYes                    ***\n#> Sepsis.DiagYes                            \n#> `incomeUnder $11k`                     ** \n#> RHC.use                                ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 7433.3  on 5734  degrees of freedom\n#> Residual deviance: 6198.0  on 5696  degrees of freedom\n#> AIC: 6276\n#> \n#> Number of Fisher Scoring iterations: 5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "machinelearning5.html",
    "href": "machinelearning5.html",
    "title": "Supervised learning",
    "section": "",
    "text": "In this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods.\nIn the first code chunk, we load necessary R libraries that will be utilized throughout the chapter for various machine learning methods and data visualization.\nRead previously saved data\nThe second chunk is dedicated to reading previously saved data and formulas from specified file paths, ensuring that the dataset and predefined formulas are available for subsequent analyses.\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nlevels(ObsData$Death)=c(\"No\",\"Yes\")\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula2 <- readRDS(file = \"Data/machinelearning/form2.RDS\")\n\nRidge, LASSO, and Elastic net\nThe traditional regression models (e.g., linear regression, logistic regression) show poor model performance when\n\npredictors are highly correlated or\nthere are many predictors\n\nIn both cases, the variance of the estimated regression coefficients could be highly variable. Hence, the model often results in poor predictions. The general solution to this problem is to reduce the variance at the cost of introducing some bias in the coefficients. This approach is called regularization or shrinking. Since we are interested in overall prediction rather than individual regression coefficients in a prediction context, this shrinkage approach is almost always beneficial for the model’s predictive performance. Ridge, LASSO, and Elastic Net are shrinkage machine learning techniques.\n\nRidge: Penalizes the sum of squared regression coefficients (the so-called \\(L_2\\) penalty). This approach does not remove irrelevant predictors, but minimizes the impact of the irrelevant predictors. There is a hyperparameter called \\(\\lambda\\) (lambda) that determines the amount of shrinkage of the coefficients. The larger \\(\\lambda\\) indicates more penalization of the coefficients.\nLASSO: The Least Absolute Shrinkage and Selection Operator (LASSO) is quite similar conceptually to the ridge regression. However, lasso penalizes the sum of the absolute values of regression coefficients (so-called \\(L_1\\) penalty). As a result, a high \\(\\lambda\\) value forces many coefficients to be exactly zero in lasso regression, suggesting a reduced model with fewer predictors, which is never the case in ridge regression.\nElastic Net: The elastic net combines the penalties of ridge regression and lasso to get the best of both methods. Two hyperparameters in the elastic net are \\(\\alpha\\) (alpha) and \\(\\lambda\\).\n\nWe can use the glmnet function in R to fit these there models.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nContinuous outcome\nCross-validation LASSO\n\n\n\n\n\nIn this code chunk, we implement a machine learning model training process with a focus on utilizing cross-validation and tuning parameters to optimize the model. Cross-validation is a technique used to assess how well the model will generalize to an independent dataset by partitioning the original dataset into a training set to train the model, and a test set to evaluate it. Here, we specify that we are using a particular type of cross-validation, denoted as “cv”, and that we will be creating 5 folds (or partitions) of the data, as indicated by number = 5.\nThe model being trained is specified to use a method known as “glmnet”, which is capable of performing lasso, ridge, and elastic net regularization regressions. Tuning parameters are crucial in controlling the behavior of our learning algorithm. In this instance, we specify lambda and alpha as our tuning parameters, which control the amount of regularization applied to the model and the mixing percentage between lasso and ridge regression, respectively. The tuneGrid argument is used to specify the exact values of alpha and lambda that the model should consider during training. The verbose = FALSE argument ensures that additional model training details are not printed during the training process. Finally, the trained model is stored in an object for further examination and use.\n\nctrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <- train(out.formula1, \n                    trControl = ctrl,\n                    data = ObsData, method = \"glmnet\",\n                    lambda= 0,\n                    tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                    verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4589, 4589, 4586, 4587, 4589 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.12778  0.05773001  15.22013\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Ridge\nSubsequent code chunks explore Ridge regression and Elastic Net, employing similar methodologies but adjusting tuning parameters accordingly.\n\nctrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <-train(out.formula1, \n                   trControl = ctrl,\n                   data = ObsData, method = \"glmnet\",\n                   lambda= 0,\n                   tuneGrid = expand.grid(alpha = 0, lambda = 0),\n                   verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4588, 4587, 4589, 4589 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.12456  0.05882567  15.20355\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\nBinary outcome\nCross-validation LASSO\nWe then shift to binary outcomes, exploring LASSO and Ridge regression with similar implementations but adjusting for the binary nature of the outcome variable.\n\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, \n                  trControl = ctrl,\n                  data = ObsData, \n                  method = \"glmnet\",\n                  lambda= 0,\n                  tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                  verbose = FALSE,\n                  metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4589, 4588, 4587 \n#> Resampling results:\n#> \n#>   ROC        Sens      Spec     \n#>   0.7559632  0.466479  0.8543891\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\n\nNot okay to select variables from a shrinkage model, and then use them in a regular regression\nCross-validation Ridge\n\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               lambda= 0,\n               tuneGrid = expand.grid(alpha = 0,  \n                                      lambda = 0),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4588, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7525711  0.4605064  0.8533099\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Elastic net\n\nAlpha = mixing parameter\nLambda = regularization or tuning parameter\nWe can use expand.grid for model tuning\n\n\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  \n                                      lambda = seq(0.05,0.3,by = 0.05)),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4589, 4587, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   alpha  lambda  ROC        Sens          Spec     \n#>   0.10   0.05    0.7528442  0.3750700591  0.9000559\n#>   0.10   0.10    0.7496720  0.2737182573  0.9363268\n#>   0.10   0.15    0.7448441  0.1728800168  0.9685679\n#>   0.10   0.20    0.7388067  0.0879337802  0.9868352\n#>   0.10   0.25    0.7322999  0.0218633878  0.9959710\n#>   0.10   0.30    0.7252141  0.0004975124  1.0000000\n#>   0.15   0.05    0.7522050  0.3551942521  0.9086537\n#>   0.15   0.10    0.7455831  0.2170968976  0.9513740\n#>   0.15   0.15    0.7359529  0.1023431231  0.9838811\n#>   0.15   0.20    0.7247676  0.0198758071  0.9957025\n#>   0.15   0.25    0.7163649  0.0000000000  1.0000000\n#>   0.15   0.30    0.7102519  0.0000000000  1.0000000\n#>   0.20   0.05    0.7506461  0.3338259077  0.9153698\n#>   0.20   0.10    0.7395549  0.1694023678  0.9691062\n#>   0.20   0.15    0.7243135  0.0367665395  0.9930158\n#>   0.20   0.20    0.7136920  0.0000000000  0.9997315\n#>   0.20   0.25    0.7065455  0.0000000000  1.0000000\n#>   0.20   0.30    0.6970550  0.0000000000  1.0000000\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were alpha = 0.1 and lambda = 0.05.\nplot(fit.cv.bin)\n\n\n\n\nDecision tree\nDecision trees are then introduced and implemented, with visualizations and evaluation metrics provided to assess their performance.\n\nDecision tree\n\nReferred to as Classification and regression trees or CART\nCovers\n\nClassification (categorical outcome)\nRegression (continuous outcome)\n\n\nFlexible to incorporate non-linear effects automatically\n\nNo need to specify higher order terms / interactions\n\n\nUnstable, prone to overfitting, suffers from high variance\n\n\n\nSimple CART\n\nrequire(rpart)\nsummary(ObsData$DASIndex) # Duke Activity Status Index\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   11.00   16.06   19.75   20.50   23.43   33.00\ncart.fit <- rpart(Death~DASIndex, data = ObsData)\npar(mfrow = c(1,1), xpd = NA)\nplot(cart.fit)\ntext(cart.fit, use.n = TRUE)\n\n\n\nprint(cart.fit)\n#> n= 5735 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 5735 2013 Yes (0.3510026 0.6489974)  \n#>   2) DASIndex>=24.92383 1143  514 No (0.5503062 0.4496938)  \n#>     4) DASIndex>=29.14648 561  199 No (0.6452763 0.3547237) *\n#>     5) DASIndex< 29.14648 582  267 Yes (0.4587629 0.5412371) *\n#>   3) DASIndex< 24.92383 4592 1384 Yes (0.3013937 0.6986063) *\nrequire(rattle)\nrequire(rpart.plot)\nrequire(RColorBrewer)\nfancyRpartPlot(cart.fit, caption = NULL)\n\n\n\n\nAUC\n\nrequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5912\nplot(rocobj)\n\n\n\nauc(rocobj)\n#> Area under the curve: 0.5912\n\nComplex CART\nMore variables\n\nout.formula2\n#> Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#>     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#>     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#>     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#>     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#>     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#>     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nrequire(rpart)\ncart.fit <- rpart(out.formula2, data = ObsData)\n\nCART Variable importance\n\ncart.fit$variable.importance\n#>            DASIndex              Cancer               Tumor                 age \n#>         123.2102455          33.4559400          32.5418433          24.0804860 \n#>   Medical.insurance                 WBC                 edu Cardiovascular.Diag \n#>          14.5199953           5.6673997           3.7441554           3.6449371 \n#>          Heart.rate      Cardiovascular         Trauma.Diag               PaCo2 \n#>           3.4059248           3.1669125           0.5953098           0.2420672 \n#>           Potassium              Sodium             Albumin \n#>           0.2420672           0.2420672           0.1984366\n\nAUC\n\nrequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5981\nplot(rocobj)\n\n\n\nauc(rocobj)\n#> Area under the curve: 0.5981\n\nCross-validation CART\n\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"rpart\",\n              metric=\"ROC\")\nfit.cv.bin\n#> CART \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   cp           ROC        Sens       Spec     \n#>   0.007203179  0.6304911  0.2816488  0.9086574\n#>   0.039741679  0.5725283  0.2488649  0.8981807\n#>   0.057128664  0.5380544  0.1287804  0.9473284\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final value used for the model was cp = 0.007203179.\n# extract results from each test data \nsummary.res <- fit.cv.bin$resample\nsummary.res\n\n\n\n  \n\n\n\nEnsemble methods (Type I)\nWe explore ensemble methods, specifically bagging and boosting, through implementation and evaluation in the context of binary outcomes.\nTraining same model to different samples (of the same data)\nCross-validation bagging\n\nBagging or bootstrap aggregation\n\nindependent bootstrap samples (sampling with replacement, B times),\napplies CART on each i (no prunning)\nAverage the resulting predictions\nReduces variance as a result of using bootstrap\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"bag\",\n               bagControl = bagControl(fit = ldaBag$fit, \n                                       predict = ldaBag$pred, \n                                       aggregate = ldaBag$aggregate),\n               metric=\"ROC\")\n#> Warning: executing %dopar% sequentially: no parallel backend registered\nfit.cv.bin\n#> Bagged Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7506666  0.4485809  0.8602811\n#> \n#> Tuning parameter 'vars' was held constant at a value of 63\n\n\nBagging improves prediction accuracy\n\nover prediction using a single tree\n\n\nLooses interpretability\n\nas this is an average of many diagrams now\n\n\nBut we can get a summary of the importance of each variable\n\nBagging Variable importance\n\ncaret::varImp(fit.cv.bin, scale = FALSE)\n#> ROC curve variable importance\n#> \n#>   only 20 most important variables shown (out of 50)\n#> \n#>                    Importance\n#> age                    0.6159\n#> APACHE.score           0.6140\n#> DASIndex               0.5962\n#> Cancer                 0.5878\n#> Creatinine             0.5835\n#> Tumor                  0.5807\n#> blood.pressure         0.5697\n#> Glasgow.Coma.Score     0.5656\n#> Disease.category       0.5641\n#> Temperature            0.5584\n#> DNR.status             0.5572\n#> Hematocrit             0.5525\n#> Weight                 0.5424\n#> Bilirubin              0.5397\n#> income                 0.5319\n#> Immunosupperssion      0.5278\n#> RHC.use                0.5263\n#> Dementia               0.5252\n#> Congestive.HF          0.5250\n#> Hematologic.Diag       0.5250\n\nCross-validation boosting\n\nBoosting\n\nsequentially updated/weighted bootstrap based on previous learning\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"gbm\",\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> Stochastic Gradient Boosting \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  ROC        Sens       Spec     \n#>   1                   50      0.7218938  0.2145970  0.9505647\n#>   1                  100      0.7410292  0.2980581  0.9234228\n#>   1                  150      0.7483014  0.3487142  0.9030028\n#>   2                   50      0.7414513  0.2960631  0.9263816\n#>   2                  100      0.7534264  0.3869684  0.8917212\n#>   2                  150      0.7575826  0.4187512  0.8777477\n#>   3                   50      0.7496078  0.3626125  0.9070358\n#>   3                  100      0.7579645  0.4078244  0.8764076\n#>   3                  150      0.7637074  0.4445909  0.8702298\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were n.trees = 150, interaction.depth =\n#>  3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nplot(fit.cv.bin)\n\n\n\n\nEnsemble methods (Type II)\nWe introduce the concept of Super Learner, providing external resources for further exploration.\nTraining different models on the same data\nSuper Learner\n\nLarge number of candidate learners (CL) with different strengths\n\nParametric (logistic)\nNon-parametric (CART)\n\n\nCross-validation: CL applied on training data, prediction made on test data\nFinal prediction uses a weighted version of all predictions\n\nWeights = coef of Observed outcome ~ prediction from each CL\n\n\nSteps\nRefer to this tutorial for steps and examples! Refer to the next chapter for more details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following is a brief exercise of super learners in the propensity score context, but we will explore more about this topic in the next chapter."
  },
  {
    "objectID": "machinelearning6.html",
    "href": "machinelearning6.html",
    "title": "Unsupervised learning",
    "section": "",
    "text": "In this chapter, we will talk about unsupervised learning.\nIn the initial code chunk, we load a specific library that will be utilized for publishing-related functionality throughout the chapter.\nClustering\nClustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity.\nGroup characteristics include (to the extent that is possible)\n\nlow inter-class similarity: observation from different clusters would be dissimilar\nhigh intra-class similarity: observation from the same cluster would be similar\n\nWithin-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances (Wikipedia 2023a)\n\n\n\n\n\nK-means\nK-means is a very popular clustering algorithm, that partitions the data into \\(k\\) groups.\nAlgorithm:\n\nDetermine a number \\(k\\) (e.g., could be 3)\nrandomly select \\(k\\) subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster.\nBy Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong.\ncompute new mean value for each cluster.\nbased on this new mean, try to determine again in which cluster the data points belong.\nprocess continues until the data points do not change cluster membership.\nRead previously saved data\nWe read a previously saved dataset from a specified file path.\n\nObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nIn the next few code chunks, we implement k-means clustering on various subsets of the data, visualizing the results and displaying the cluster centers. The first example uses two variables, the second example uses three, and in the third example, a larger subset of variables is selected but not immediately utilized in the clustering. In the subsequent code chunk, we apply k-means clustering to the larger subset of variables, displaying various results and aggregating data by cluster to display mean and standard deviation values for each variable within each cluster.\nExample 1\n\ndatax0 <- ObsData[c(\"Heart.rate\", \"edu\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   Heart.rate      edu\n#> 1  134.96277 11.75466\n#> 2   54.55138 11.44494\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\nExample 2\n\ndatax0 <- ObsData[c(\"blood.pressure\", \"Heart.rate\", \"Respiratory.rate\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   blood.pressure Heart.rate Respiratory.rate\n#> 1       80.10812  135.08956         29.85267\n#> 2       73.71684   54.95789         22.76723\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\nExample with many variables\n\ndatax <- ObsData[c(\"edu\", \"blood.pressure\", \"Heart.rate\", \n                   \"Respiratory.rate\" , \"Temperature\",\n                   \"PH\", \"Weight\", \"Length.of.Stay\")]\n\n\nkres <- kmeans(datax, centers = 3)\n#kres\nhead(kres$cluster)\n#> [1] 3 3 1 2 3 3\nkres$size\n#> [1]  331 1406 3998\nkres$centers\n#>        edu blood.pressure Heart.rate Respiratory.rate Temperature       PH\n#> 1 11.87912       74.13142   133.5257         28.06042    38.11417 7.389388\n#> 2 11.45185       73.77809    54.7532         22.76693    37.04930 7.381912\n#> 3 11.74154       80.55103   134.9102         29.96748    37.77743 7.390619\n#>     Weight Length.of.Stay\n#> 1 70.73593       98.97885\n#> 2 66.89213       17.34922\n#> 3 67.91611       16.63032\naggregate(datax, by = list(cluster = kres$cluster), mean)\n\n\n\n  \n\n\naggregate(datax, by = list(cluster = kres$cluster), sd)\n\n\n\n  \n\n\n\nOptimal number of clusters\nNext, we explore determining the optimal number of clusters, visualizing the total within-cluster sum of squares for different values of k and indicating a chosen value of k with a vertical line on the plot.\n\nrequire(factoextra)\nfviz_nbclust(datax, kmeans, method = \"wss\")+\n  geom_vline(xintercept=3,linetype=3)\n\n\n\n\nHere the vertical line is chosen based on elbow method (Wikipedia 2023b).\nDiscussion\n\nWe need to supply a number, \\(k\\): but we can test different \\(k\\)s to identify optimal value\nClustering can be influenced by outliners, so median based clustering is possible\nmere ordering can influence clustering, hence we should choose different initial means (e.g., nstart should be greater than 1).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n\n\n———. 2023b. “Elbow Method (Clustering).” https://en.wikipedia.org/wiki/Elbow_method_(clustering)."
  },
  {
    "objectID": "machinelearning6a.html",
    "href": "machinelearning6a.html",
    "title": "NHIS Example",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning (ML) techniques with health survey data. We will use the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP) among adults aged 65 years or older. We will use LASSO and random forest models with sampling weights to obtain population-level predictions. In this tutorial, the split-sample approach as an internal validation technique will be used. You can review the earlier tutorial on data splitting technique. Note that this split-sample approach is flagged as a problematic approach in the literature (Steyerberg et al. 2001). The better approach could be cross-validation and bootstrapping Steyerberg and Steyerberg (2019). In the next tutorial, we will apply the ML techniques for survey data with cross-validation.\n\n\nSteyerberg EW, Harrell Jr FE, Borsboom GJ, Eijkemans MJ, Vergouwe Y, Habbema JD. Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of Clinical Epidemiology. 2001; 54(8):774-81. DOI: 10.1016/S0895-4356(01)00341-9\nSteyerberg EW, Steyerberg EW. Overfitting and optimism in prediction models. Clinical prediction models: A practical approach to development, validation, and updating. 2019:95-112. DOI: 10.1007/978-3-030-16399-0_5\n\n\n\n\n\n\nNote\n\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/nhis2016.RData\")\nls()\n#> [1] \"dat.analytic\"\n\n\ndim(dat.analytic)\n#> [1] 7828   14\n\nhead(dat.analytic)\n\n\n\n  \n\n\n\nThe dataset contains 7,828 complete case participants (i.e., no missing) with 14 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (high impact chronic pain, the binary outcome variable)\n\nsex: Sex\n\nmarital: Marital status\n\nrace: Race/ethnicity\n\npoverty.status: Poverty status\n\ndiabetes: Diabetes\n\nhigh.cholesterol: High cholesterol\n\nstroke: Stroke\n\narthritis: Arthritis and rheumatism\n\ncurrent.smoker: Current smoker\n\nLet’s see the descriptive statistics of the predictors stratified by the outcome variable (HICP).\nDescriptive statistics\n\n# Predictors\npredictors <- c(\"sex\", \"marital\", \"race\", \"poverty.status\", \n                \"diabetes\", \"high.cholesterol\", \"stroke\",\n                \"arthritis\", \"current.smoker\")\n\n# Table 1 - Unweighted \n#tab1 <- CreateTableOne(vars = predictors, strata = \"HICP\", \n#                       data = dat.analytic, test = F)\n#print(tab1, showAllLevels = T)\n\ntbl_summary(data = dat.analytic, include = predictors, \n            by = HICP, missing = \"no\") %>% \n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\nCharacteristic\n      \n        HICP\n      \n    \n\n\n0, N = 6,8471\n\n      \n1, N = 9811\n\n    \n\n\n\nsex\n\n\n\n\n    Female\n3,841 (56%)\n623 (64%)\n\n\n    Male\n3,006 (44%)\n358 (36%)\n\n\nmarital\n\n\n\n\n    Never married\n444 (6.5%)\n60 (6.1%)\n\n\n    Married/with partner\n3,183 (46%)\n379 (39%)\n\n\n    Divorced/separated\n1,252 (18%)\n198 (20%)\n\n\n    Widowed\n1,968 (29%)\n344 (35%)\n\n\nrace\n\n\n\n\n    White\n5,455 (80%)\n756 (77%)\n\n\n    Black\n606 (8.9%)\n99 (10%)\n\n\n    Hispanic\n431 (6.3%)\n79 (8.1%)\n\n\n    Others\n355 (5.2%)\n47 (4.8%)\n\n\npoverty.status\n\n\n\n\n    <100% FPL\n543 (7.9%)\n176 (18%)\n\n\n    100-200% FPL\n1,520 (22%)\n307 (31%)\n\n\n    200-400% FPL\n2,309 (34%)\n309 (31%)\n\n\n    400%+ FPL\n2,475 (36%)\n189 (19%)\n\n\ndiabetes\n1,275 (19%)\n315 (32%)\n\n\nhigh.cholesterol\n3,602 (53%)\n633 (65%)\n\n\nstroke\n527 (7.7%)\n146 (15%)\n\n\narthritis\n3,226 (47%)\n769 (78%)\n\n\ncurrent.smoker\n619 (9.0%)\n123 (13%)\n\n\n\n\n1 n (%)\n    \n\n\n\n\nWeight normalization\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Note that we are not interested in the statistical significance of the \\(\\beta\\) coefficients. Hence, not utilizing PSU and strata should not be an issue in this prediction problem. However, we still need to use sampling weights to get population-level predictions. Large weights are usually problematic, particularly with model evaluation. One way to solve the problem is weight normalization (Bruin 2023).\n\n# Normalize weight\ndat.analytic$wgt <- dat.analytic$weight * \n  nrow(dat.analytic)/sum(dat.analytic$weight)\n\n# Weight summary\nsummary(dat.analytic$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     243    1521    2604    2914    3791   14662\nsummary(dat.analytic$wgt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.08339 0.52198 0.89365 1.00000 1.30109 5.03175\n\n# The weighted and unweighted n are equal\nnrow(dat.analytic)\n#> [1] 7828\nsum(dat.analytic$wgt)\n#> [1] 7828\n\nSplit-sample\nLet us create our training and test data using the split-sample approach. We created 70% training and 30% test data for our example.\n\nset.seed(604001)\ndat.analytic$datasplit <- rbinom(nrow(dat.analytic), \n                                 size = 1, prob = 0.7) \ntable(dat.analytic$datasplit)\n#> \n#>    0    1 \n#> 2343 5485\n\n# Training data\ndat.train <- dat.analytic[dat.analytic$datasplit == 1,]\ndim(dat.train)\n#> [1] 5485   16\n\n# Test data\ndat.test <- dat.analytic[dat.analytic$datasplit == 0,]\ndim(dat.test)\n#> [1] 2343   16\n\nRegression formula\nLet’s us define the regression formula:\n\nFormula <- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n\nLASSO for Surveys\nNow, we will fit the LASSO model for our survey data. Here are the steps:\n\nWe will fit 5-fold cross-validation on the training data to find the value of lambda that gives minimum prediction error. We will incorporate sampling weights in the model to account for survey data.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nData in matrix\nTo perform LASSO with the glmnet package, we need to set the predictors in the data.matrix format and outcome variable as a vector.\n\n# Training data - X: predictor, y: outcome\nX.train <- model.matrix(Formula, dat.train)[,-1] \ny.train <- as.matrix(dat.train$HICP) \n\n# Test data - X: predictor, y: outcome\nX.test <- model.matrix(Formula, dat.test)[,-1] \ny.test <- as.matrix(dat.test$HICP) \n\nLet us see the few rows of the data:\n\nhead(X.train)\n#>    sexMale maritalMarried/with partner maritalDivorced/separated maritalWidowed\n#> 12       1                           1                         0              0\n#> 13       1                           0                         1              0\n#> 16       0                           0                         1              0\n#> 42       0                           0                         0              1\n#> 63       0                           0                         0              1\n#> 65       0                           0                         0              1\n#>    raceBlack raceHispanic raceOthers poverty.status100-200% FPL\n#> 12         0            0          0                          0\n#> 13         0            0          0                          0\n#> 16         1            0          0                          0\n#> 42         0            0          0                          0\n#> 63         0            0          0                          0\n#> 65         0            1          0                          1\n#>    poverty.status200-400% FPL poverty.status400%+ FPL diabetesYes\n#> 12                          1                       0           0\n#> 13                          0                       1           0\n#> 16                          1                       0           0\n#> 42                          0                       1           0\n#> 63                          1                       0           0\n#> 65                          0                       0           1\n#>    high.cholesterolYes strokeYes arthritisYes current.smokerYes\n#> 12                   0         0            1                 0\n#> 13                   0         0            1                 0\n#> 16                   1         0            1                 0\n#> 42                   1         0            0                 0\n#> 63                   0         0            0                 0\n#> 65                   1         0            1                 0\n\nAs we can see, factor predictors are coded into dummy variables. It is important to note that the continuous predictors should be standardized. glmnet does this by default. Next, we will use the glmnet function to fit the LASSO model.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nFind best lambda\nNow, we will use k-fold cross-validation with the cv.glmnet function to find the best lambda value. In this example, we choose k = 5. Note that we must incorporate sampling weight to account for survey data.\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso <- cv.glmnet(x = X.train, y = y.train, \n                          nfolds = 5, alpha = 1, \n                          family = \"binomial\", \n                          weights = dat.train$wgt)\nfit.cv.lasso\n#> \n#> Call:  cv.glmnet(x = X.train, y = y.train, weights = dat.train$wgt,      nfolds = 5, alpha = 1, family = \"binomial\") \n#> \n#> Measure: Binomial Deviance \n#> \n#>       Lambda Index Measure      SE Nonzero\n#> min 0.001355    43  0.6856 0.01905      12\n#> 1se 0.020128    14  0.7036 0.01446       5\n\nWe can also plot all the lambda values against the deviance (i.e., prediction error).\n\nplot(fit.cv.lasso)\n\n\n\n\n\n# Best lambda\nfit.cv.lasso$lambda.min\n#> [1] 0.001355482\n\nThe lambda value that has the lowest deviance is 0.0013555. Our next step is to fit the LASSO model with the best lambda. Again, we must incorporate sampling weight to account for survey data.\nLASSO with best lambda\n\n# Fit the model on the training set with optimum lambda\nfit.lasso <- glmnet(x = X.train, y = y.train, \n                    alpha = 1, family = \"binomial\",\n                    lambda = fit.cv.lasso$lambda.min, \n                    weights = dat.train$wgt)\nfit.lasso\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df %Dev   Lambda\n#> 1 12 9.68 0.001355\n\nLet’s check the coefficients from the model:\n\n# Intercept \nfit.lasso$a0\n#>        s0 \n#> -2.491484\n\n# Beta coefficients\nfit.lasso$beta\n#> 15 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                       s0\n#> sexMale                     -0.009832818\n#> maritalMarried/with partner  .          \n#> maritalDivorced/separated    .          \n#> maritalWidowed               0.041111308\n#> raceBlack                   -0.064853067\n#> raceHispanic                -0.037985160\n#> raceOthers                   .          \n#> poverty.status100-200% FPL  -0.268494032\n#> poverty.status200-400% FPL  -0.602097419\n#> poverty.status400%+ FPL     -1.052635980\n#> diabetesYes                  0.369667918\n#> high.cholesterolYes          0.299475863\n#> strokeYes                    0.449050679\n#> arthritisYes                 1.241288036\n#> current.smokerYes            0.253439388\n\nAs we can see, the coefficient is not shown for some predictors. This is because the LASSO model shrunk the coefficient to zero. In other words, these predictors were dropped entirely from the model because they were not contributing enough to predict the outcome. next, we will use the final model to make predictions on new observations or our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.lasso <- predict(fit.lasso, \n                               newx = X.test, \n                               type = \"response\")\nhead(dat.test$pred.lasso)\n#>            s0\n#> 3  0.05711170\n#> 11 0.03716633\n#> 28 0.14049527\n#> 30 0.05764351\n#> 31 0.11408034\n#> 59 0.32540267\n\nModel performance\nNow, we will calculate the model performance measures such as AUC, calibration slope, and Brier score Christodoulou et al. (2019). We will incorporate sampling weights to get population-level estimates.\n\n\nSteyerberg EW, Vickers AJ, Cook NR, Gerds T, Gonen M, Obuchowski N, Pencina MJ, Kattan MW. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.). 2010;21(1):128. DOI: 10.1097/EDE.0b013e3181c30fb2\nSteyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European heart journal. 2014;35(29):1925-31. DOI: 10.1093/eurheartj/ehu207\nChristodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology. 2019;110:12-22. DOI: 10.1016/j.jclinepi.2019.02.004\n\n\n\n\n\n\nNote\n\n\n\n\nArea under the curve (AUC) is a measure of discrimination or accuracy of a model. A higher AUC is better. An AUC value of 1 is considered a perfect prediction, while an AUC value of 0.50 is no better than a coin toss. In practice, AUC values of 0.70 to 0.80 are considered good, and those \\(>0.80\\) are considered very good.\nCalibration is defined as the agreement between observed and predicted probability of the outcome. In this exercise, we will estimate the calibration slope as a measure of calibration. A calibration slope of 1 reflects a well-calibrated model, a calibration slope less than 1 indicates overfitting and greater than 1 indicates underfitting of the model.\nThe Brier score is a measure of overall performance. The Brier score can range from 0 to 1 and is similar to the mean squared error. A lower Brier score value (closer to 0) indicates a better model.\n\n\n\n\n# AUC on the test set with sampling weights\nauc.lasso <- WeightedAUC(WeightedROC(dat.test$pred.lasso, \n                                     dat.test$HICP, \n                                     weight = dat.test$wgt))\nauc.lasso\n#> [1] 0.7662941\n\n\n# Logit of the predicted probability\ndat.test$pred.lasso.logit <- Logit(dat.test$pred.lasso)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.lasso.logit, data = dat.test, \n               family = binomial, weights = wgt)\ncal.slope.lasso <- summary(mod.cal)$coef[2,1]\ncal.slope.lasso\n#> [1] 1.244645\n\n\n# Weighted Brier Score\nbrier.lasso <- mean(brierscore(HICP ~ dat.test$pred.lasso, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier.lasso\n#> [1] 0.09978551\n\nRandom Forest for Surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model:\n\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey data.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nFormula\nWe will use the same formula defined above.\n\nFormula\n#> HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#>     stroke + arthritis + current.smoker\n\nHyperparameter tuning\nFor tuning the hyperparameters, let’s use the grid search approach.\n\n# Grid with 1000 models - huge time consuming\n#grid.search <- expand.grid(mtry = 1:10, node.size = 1:10, \n#                           num.trees = seq(50,500,50), \n#                           oob_error = 0)\n  \n# Grid with 36 models as an exercise\ngrid.search <- expand.grid(mtry = 5:7, node.size = 1:3, \n                           num.trees = seq(200,500,100), \n                           oob_error = 0)\nhead(grid.search)\n\n\n\n  \n\n\n\nNow, we will fit the random forest model with the selected grids. We will incorporate sampling weight as the case weight in the ranger function.\n\n## Calculate prediction error for each grid \nfor(ii in 1:nrow(grid.search)) {\n  # Model on training set with grid\n  fit.rf.tune <- ranger(formula = Formula, \n                        data = dat.train, \n                        num.trees = grid.search$num.trees[ii],\n                        mtry = grid.search$mtry[ii], \n                        min.node.size = grid.search$node.size[ii],\n                        importance = 'impurity', \n                        case.weights = dat.train$wgt)\n  \n  # Add Out-of-bag (OOB) error to each grid\n  grid.search$oob_error[ii] <- sqrt(fit.rf.tune$prediction.error)\n}\nhead(grid.search)\n\n\n\n  \n\n\n\nLet’s check which combination of hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) gives minimum prediction error.\n\nposition <- which.min(grid.search$oob_error)\ngrid.search[position,]\n\n\n\n  \n\n\n\nModel after tuning\nNow, we will fit the random forest model with the tuned hyperparameters.\n\n# Fit the model on the training set \nfit.rf <- ranger(formula = Formula, \n                 data = dat.train, \n                 case.weights = dat.train$wgt, \n                 probability = T,\n                 num.trees = grid.search$num.trees[position], \n                 min.node.size = grid.search$node.size[position], \n                 mtry = grid.search$mtry[position], \n                 importance = 'impurity')\n\n# Fitted random forest model\nfit.rf\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  300 \n#> Sample size:                      5485 \n#> Number of independent variables:  9 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1110941\n\nNow, we can use the model to make predictions on our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.rf <- predict(fit.rf, \n                            data = dat.test)$predictions[,2]\nhead(dat.test$pred.rf)\n#> [1] 0.031093101 0.003712312 0.129992168 0.044802288 0.025764960 0.307125058\n\nModel performance\nThe same as the LASSO model, we can calculate the AUC, calibration slope, and Brier score.\n\n# AUC on the test set with sampling weights\nauc.rf <- WeightedAUC(WeightedROC(dat.test$pred.rf, \n                                  dat.test$HICP, \n                                  weight = dat.test$wgt))\nauc.rf\n#> [1] 0.6941022\n\n\n# Logit of the predicted probability\ndat.test$pred.rf[dat.test$pred.rf == 0] <- 0.00001\ndat.test$pred.rf.logit <- Logit(dat.test$pred.rf)\n\n# Weighted calibration slope\nmod.cal <- glm(HICP ~ pred.rf.logit, \n               data = dat.test, \n               family = binomial, \n               weights = wgt)\ncal.slope.rf <- summary(mod.cal)$coef[2,1]\ncal.slope.rf\n#> [1] 0.4977901\n\n\n# Weighted Brier Score\nbrier.rf <- mean(brierscore(HICP ~ dat.test$pred.rf, \n                            data = dat.test,\n                            wt = dat.test$wgt))\nbrier.rf\n#> [1] 0.1095384\n\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\nggplot(\n  enframe(fit.rf$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") + \n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\nAs per the figure, marital status, poverty status, sex, and arthritis are the most influential predictors in predicting HICP, while stroke is the least important predictor.\nPerformance comparison\n\n\n\n\n\n\n\n\nModel\nAUC\nCalibration slope\nBrier score\n\n\n\nLASSO\n0.7662941\n1.2446448\n0.0997855\n\n\nRandom forest\n0.6941022\n0.4977901\n0.1095384\n\n\nReferences\n\n\n\n\nBruin, J. 2023. “Advanced Topics in Survey Data Analysis.” 2023. https://stats.oarc.ucla.edu/other/mult-pkg/seminars/advanced-topics-in-survey-data-analysis/.\n\n\nChristodoulou, Evangelia, Jie Ma, Gary S Collins, Ewout W Steyerberg, Jan Y Verbakel, and Ben Van Calster. 2019. “A Systematic Review Shows No Performance Benefit of Machine Learning over Logistic Regression for Clinical Prediction Models.” Journal of Clinical Epidemiology 110: 12–22.\n\n\nSteyerberg, Ewout W, Frank E Harrell Jr, Gerard JJM Borsboom, MJC Eijkemans, Yvonne Vergouwe, and J Dik F Habbema. 2001. “Internal Validation of Predictive Models: Efficiency of Some Procedures for Logistic Regression Analysis.” Journal of Clinical Epidemiology 54 (8): 774–81.\n\n\nSteyerberg, Ewout W, and Ewout W Steyerberg. 2019. “Overfitting and Optimism in Prediction Models.” Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating, 95–112.\n\n\nSteyerberg, Ewout W, and Yvonne Vergouwe. 2014. “Towards Better Clinical Prediction Models: Seven Steps for Development and an ABCD for Validation.” European Heart Journal 35 (29): 1925–31.\n\n\nSteyerberg, Ewout W, Andrew J Vickers, Nancy R Cook, Thomas Gerds, Mithat Gonen, Nancy Obuchowski, Michael J Pencina, and Michael W Kattan. 2010. “Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures.” Epidemiology (Cambridge, Mass.) 21 (1): 128."
  },
  {
    "objectID": "machinelearning6b.html",
    "href": "machinelearning6b.html",
    "title": "Replicate Results",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning techniques with health survey data. We will replicate some of the results of this article by Falasinnu et al. (2023).\n\n\nFalasinnu T, Hossain MB, Weber II KA, Helmick CG, Karim ME, Mackey S. The Problem of Pain in the United States: A Population-Based Characterization of Biopsychosocial Correlates of High Impact Chronic Pain Using the National Health Interview Survey. The Journal of Pain. 2023;24(6):1094-103. DOI: 10.1016/j.jpain.2023.03.008\nThe authors used the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP). They also evaluated the predictive performances of the models within sociodemographic subgroups, such as sex (male, female), age (\\(<65\\), \\(\\ge 65\\)), and race/ethnicity (White, Black, Hispanic). They used LASSO and random forest models with 5-fold cross-validation as an internal validation. To obtain population-level predictions, they account for survey weights in both models.\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\n\n\n\n\nNote\n\n\n\nTo handle missing data in the predictors, they used multiple imputation technique. However, for simplicity, this tutorial focuses on a complete case dataset. We will also only focus on predicting HICP for people aged 65 years or older (a dataset of ~8,800 participants compared to the dataset of 33,000 participants aged 18 years or older).\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/Falasinnu2023.RData\")\nls()\n#> [1] \"dat\"\n\n\ndim(dat)\n#> [1] 8881   49\n\nThe dataset contains 8,881 participants aged 65 years or older with 49 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (binary outcome variable)\n\nage: Age\n\nsex: Sex\n\nhhsize: Number of people in household\n\nborn: Citizenship\n\nmarital: Marital status\n\nregion: Region\n\nrace: Race/ethnicity\n\neducation: Education\n\nemployment.status: Employment status\n\npoverty.status: Poverty status\n\nveteran: Veteran\n\ninsurance: Health insurance coverage\n\nsex.orientation: Sexual orientation\n\nworried.money: Worried about money\n\ngood.neighborhood: Good neighborhood\n\npsy.symptom: Psychological symptoms\n\nvisit.ED: Number of times in ER/ED\n\nsurgery: Number of surgeries in past 12 months\n\ndr.visit: Time since doctor visits\n\ncancer: Cancer\n\nasthma: Asthma\n\nhtn: Hypertension\n\nliver.disease: Liver disease\n\ndiabetes: Diabetes\n\nulcer: Ulcer\n\nstroke: Stroke\n\nemphysema: Emphysema\n\ncopd: COPD\n\nhigh.cholesterol: High cholesterol\n\ncoronary.heart.disease: Coronary heart disease\n\nangina: Angina pectoris\n\nheart.attack: Heart attack\n\nheart.disease: Heart condition/disease\n\narthritis: Arthritis and rheumatism\n\ncrohns.disease: Crohn’s disease\n\nplace.routine.care: Usual place for routine care\n\ntrouble.asleep: Trouble falling asleep\n\nobese: Obesity\n\ncurrent.smoker: Current smoker\n\nheavy.drinker: Heavy drinker\n\nhospitalization: Hospital stay days\n\nbetter.health.status: Better health status\n\nphysical.activity: Physical activity\n\nSee the NHIS 2016 dataset and the article for better understanding of the variables.\nComplete case data\n\n# Age\ntable(dat$age, useNA = \"always\")\n#> \n#>  <65  65+ <NA> \n#>    0 8881    0\n\nLet us consider a complete case dataset\n\ndat.complete <- na.omit(dat)\ndim(dat.complete)\n#> [1] 7280   49\n\nAs we can see, there are 7,280 participants with complete case information. Let’s see the descriptive statistics of the predictors stratified by HICP.\nDescriptive statistics\n\n# Predictors\npredictors <- c(\"sex\", \"hhsize\", \"born\", \"marital\", \n                \"region\", \"race\", \"education\", \n                \"employment.status\", \"poverty.status\",\n                \"veteran\", \"insurance\", \n                \"sex.orientation\", \"worried.money\", \n                \"good.neighborhood\", \n                \"psy.symptom\", \"visit.ED\", \"surgery\", \n                \"dr.visit\", \"cancer\", \n                \"asthma\", \"htn\", \"liver.disease\", \n                \"diabetes\", \"ulcer\", \"stroke\",\n                \"emphysema\", \"copd\", \"high.cholesterol\",\n                \"coronary.heart.disease\", \n                \"angina\", \"heart.attack\", \n                \"heart.disease\", \"arthritis\", \n                \"crohns.disease\", \"place.routine.care\", \n                \"trouble.asleep\", \"obese\", \n                \"current.smoker\", \"heavy.drinker\",\n                \"hospitalization\", \n                \"better.health.status\", \n                \"physical.activity\")\n\n# Table 1 - Unweighted \ntbl_summary(data = dat.complete, \n            include = predictors, \n            by = HICP, missing = \"no\") %>% \n  modify_spanning_header(c(\"stat_1\", \n                           \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\nCharacteristic\n      \n        HICP\n      \n    \n\n\n0, N = 6,3891\n\n      \n1, N = 8911\n\n    \n\n\n\nsex\n\n\n\n\n    Female\n3,587 (56%)\n569 (64%)\n\n\n    Male\n2,802 (44%)\n322 (36%)\n\n\nhhsize\n2 (1, 2)\n2 (1, 2)\n\n\nborn\n\n\n\n\n    Born in US\n5,775 (90%)\n802 (90%)\n\n\n    Other place\n614 (9.6%)\n89 (10.0%)\n\n\nmarital\n\n\n\n\n    Never married\n411 (6.4%)\n57 (6.4%)\n\n\n    Married/with partner\n2,990 (47%)\n349 (39%)\n\n\n    Divorced/separated\n1,161 (18%)\n179 (20%)\n\n\n    Widowed\n1,827 (29%)\n306 (34%)\n\n\nregion\n\n\n\n\n    Northeast\n1,172 (18%)\n143 (16%)\n\n\n    Midwest\n1,451 (23%)\n189 (21%)\n\n\n    South\n2,203 (34%)\n331 (37%)\n\n\n    West\n1,563 (24%)\n228 (26%)\n\n\nrace\n\n\n\n\n    White\n5,090 (80%)\n694 (78%)\n\n\n    Black\n564 (8.8%)\n88 (9.9%)\n\n\n    Hispanic\n406 (6.4%)\n70 (7.9%)\n\n\n    Others\n329 (5.1%)\n39 (4.4%)\n\n\neducation\n\n\n\n\n    Less than high school\n954 (15%)\n220 (25%)\n\n\n    High school/GED\n1,863 (29%)\n269 (30%)\n\n\n    Some college\n1,716 (27%)\n248 (28%)\n\n\n    Bachelors degree or higher\n1,856 (29%)\n154 (17%)\n\n\nemployment.status\n\n\n\n\n    Employed hourly\n578 (9.0%)\n22 (2.5%)\n\n\n    Employed non-hourly\n608 (9.5%)\n27 (3.0%)\n\n\n    Worked previously\n4,923 (77%)\n777 (87%)\n\n\n    Never worked\n280 (4.4%)\n65 (7.3%)\n\n\npoverty.status\n\n\n\n\n    <100% FPL\n496 (7.8%)\n154 (17%)\n\n\n    100-200% FPL\n1,401 (22%)\n276 (31%)\n\n\n    200-400% FPL\n2,157 (34%)\n283 (32%)\n\n\n    400%+ FPL\n2,335 (37%)\n178 (20%)\n\n\nveteran\n1,482 (23%)\n163 (18%)\n\n\ninsurance\n\n\n\n\n    Uninsured\n32 (0.5%)\n6 (0.7%)\n\n\n    Medicaid/Medicare\n2,995 (47%)\n478 (54%)\n\n\n    Privately Insured\n2,847 (45%)\n311 (35%)\n\n\n    Other\n515 (8.1%)\n96 (11%)\n\n\nsex.orientation\n\n\n\n\n    Heterosexual\n6,224 (97%)\n861 (97%)\n\n\n    Other\n165 (2.6%)\n30 (3.4%)\n\n\nworried.money\n2,351 (37%)\n491 (55%)\n\n\ngood.neighborhood\n5,988 (94%)\n781 (88%)\n\n\npsy.symptom\n694 (11%)\n353 (40%)\n\n\nvisit.ED\n\n\n\n\n    None\n5,050 (79%)\n531 (60%)\n\n\n    One\n949 (15%)\n187 (21%)\n\n\n    2-3\n313 (4.9%)\n123 (14%)\n\n\n    4+\n77 (1.2%)\n50 (5.6%)\n\n\nsurgery\n\n\n\n\n    None\n5,218 (82%)\n650 (73%)\n\n\n    One\n898 (14%)\n176 (20%)\n\n\n    Two\n206 (3.2%)\n44 (4.9%)\n\n\n    3+\n67 (1.0%)\n21 (2.4%)\n\n\ndr.visit\n\n\n\n\n    <6 months\n5,390 (84%)\n837 (94%)\n\n\n    6-12 months\n591 (9.3%)\n41 (4.6%)\n\n\n    1-5 years\n281 (4.4%)\n10 (1.1%)\n\n\n    >5 years/never\n127 (2.0%)\n3 (0.3%)\n\n\ncancer\n1,566 (25%)\n271 (30%)\n\n\nasthma\n645 (10%)\n176 (20%)\n\n\nhtn\n3,953 (62%)\n689 (77%)\n\n\nliver.disease\n122 (1.9%)\n50 (5.6%)\n\n\ndiabetes\n1,179 (18%)\n278 (31%)\n\n\nulcer\n545 (8.5%)\n161 (18%)\n\n\nstroke\n485 (7.6%)\n130 (15%)\n\n\nemphysema\n235 (3.7%)\n76 (8.5%)\n\n\ncopd\n496 (7.8%)\n145 (16%)\n\n\nhigh.cholesterol\n3,373 (53%)\n577 (65%)\n\n\ncoronary.heart.disease\n814 (13%)\n211 (24%)\n\n\nangina\n298 (4.7%)\n100 (11%)\n\n\nheart.attack\n552 (8.6%)\n154 (17%)\n\n\nheart.disease\n1,083 (17%)\n210 (24%)\n\n\narthritis\n3,017 (47%)\n700 (79%)\n\n\ncrohns.disease\n102 (1.6%)\n29 (3.3%)\n\n\nplace.routine.care\n\n\n\n\n    No place\n235 (3.7%)\n26 (2.9%)\n\n\n    Doctor's office\n4,634 (73%)\n621 (70%)\n\n\n    Hospital/Clinic\n1,403 (22%)\n221 (25%)\n\n\n    Other place\n117 (1.8%)\n23 (2.6%)\n\n\ntrouble.asleep\n1,970 (31%)\n424 (48%)\n\n\nobese\n1,757 (28%)\n397 (45%)\n\n\ncurrent.smoker\n565 (8.8%)\n111 (12%)\n\n\nheavy.drinker\n308 (4.8%)\n25 (2.8%)\n\n\nhospitalization\n\n\n\n\n    None\n5,009 (78%)\n503 (56%)\n\n\n    1-2 days\n592 (9.3%)\n57 (6.4%)\n\n\n    3-5 days\n360 (5.6%)\n63 (7.1%)\n\n\n    6+ days\n428 (6.7%)\n268 (30%)\n\n\nbetter.health.status\n890 (14%)\n119 (13%)\n\n\nphysical.activity\n\n\n\n\n    Less\n3,734 (58%)\n743 (83%)\n\n\n    Moderate\n1,794 (28%)\n108 (12%)\n\n\n    High\n861 (13%)\n40 (4.5%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nLASSO for surveys\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Similar to the previous chapter, we will normalize the weight.\nWeight normalization\n\n# Normalize weight\ndat.complete$wgt <- dat.complete$weight * \n  nrow(dat.complete)/sum(dat.complete$weight)\n\n# Weight summary\nsummary(dat.complete$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     243    1503    2583    2886    3747   14662\nsummary(dat.complete$wgt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.0842  0.5208  0.8950  1.0000  1.2983  5.0804\n\n# The weighted and unweighted n are equal\nnrow(dat.complete)\n#> [1] 7280\nsum(dat.complete$wgt)\n#> [1] 7280\n\nFolds\nLet’s create five random folds and specify the regression formula.\n\nk <- 5\nset.seed(604)\nnfolds <- sample(1:k, \n                 size = nrow(dat.complete), \n                 replace = T)\ntable(nfolds)\n#> nfolds\n#>    1    2    3    4    5 \n#> 1451 1457 1496 1468 1408\n\nFormula\n\nFormula <- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#> HICP ~ sex + hhsize + born + marital + region + race + education + \n#>     employment.status + poverty.status + veteran + insurance + \n#>     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#>     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#>     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#>     coronary.heart.disease + angina + heart.attack + heart.disease + \n#>     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#>     obese + current.smoker + heavy.drinker + hospitalization + \n#>     better.health.status + physical.activity\n\n5-fold CV LASSO\nNow, we will fit the LASSO model with 5-fold cross-validation (CV). Here are the steps:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit 5-fold cross-validation on the training set to find the value of lambda that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\n\nfit.lasso <- list(NULL)\nauc.lasso <- NULL\ncal.slope.lasso <- NULL\nbrier.lasso <- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train <- dat.complete[nfolds != fold, ]\n  X.train <- model.matrix(Formula, dat.train)[,-1]\n  y.train <- as.matrix(dat.train$HICP)\n  \n  # Test data\n  dat.test <- dat.complete[nfolds == fold, ]\n  X.test <- model.matrix(Formula, dat.test)[,-1]\n  y.test <- as.matrix(dat.test$HICP)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso <- cv.glmnet(x = X.train, \n                            y = y.train, \n                            nfolds = 5, \n                            alpha = 1, \n                            family = \"binomial\", \n                            weights = dat.train$wgt)\n  \n  # Fit the model on the training set with optimum lambda\n  fit.lasso[[fold]] <- glmnet(\n    x = X.train, \n    y = y.train, \n    alpha = 1, \n    family = \"binomial\",\n    lambda = fit.cv.lasso$lambda.min,\n    weights = dat.train$wgt)\n  \n  # Prediction on the test set\n  dat.test$pred.lasso <- predict(fit.lasso[[fold]], \n                                 newx = X.test, \n                                 type = \"response\")\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] <- WeightedAUC(\n    WeightedROC(dat.test$pred.lasso,\n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  mod.cal <- glm(\n    HICP ~ Logit(dat.test$pred.lasso), \n    data = dat.test, \n    family = binomial, \n    weights = wgt)\n  cal.slope.lasso[fold] <- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.lasso[fold] <- mean(\n    brierscore(HICP ~ dat.test$pred.lasso,\n               data = dat.test, \n               wt = dat.test$wgt))\n}\n\nModel performance\nLet’s check how prediction worked.\n\n# Fitted LASSO models\nfit.lasso[[1]]\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df  %Dev   Lambda\n#> 1 46 23.91 0.002972\nfit.lasso[[2]]\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df  %Dev   Lambda\n#> 1 47 25.32 0.002559\nfit.lasso[[3]]\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df %Dev   Lambda\n#> 1 47 24.3 0.002724\nfit.lasso[[4]]\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df  %Dev   Lambda\n#> 1 34 25.27 0.004726\nfit.lasso[[5]]\n#> \n#> Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#> \n#>   Df  %Dev   Lambda\n#> 1 39 24.32 0.003525\n\n\n# Intercept from the LASSO models in different folds\nfit.lasso[[1]]$a0\n#>        s0 \n#> -3.733405\nfit.lasso[[2]]$a0\n#>        s0 \n#> -3.534898\nfit.lasso[[3]]$a0\n#>        s0 \n#> -3.486223\nfit.lasso[[4]]$a0\n#>        s0 \n#> -3.800206\nfit.lasso[[5]]$a0\n#>       s0 \n#> -3.68776\n\n\n# Beta coefficients from the LASSO models in different folds\nfit.lasso[[1]]$beta\n#> 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                                 s0\n#> sexMale                               .           \n#> hhsize                                0.0092060605\n#> bornOther place                       .           \n#> maritalMarried/with partner           .           \n#> maritalDivorced/separated             .           \n#> maritalWidowed                        0.0926365970\n#> regionMidwest                         .           \n#> regionSouth                           .           \n#> regionWest                            0.1470219542\n#> raceBlack                            -0.0436983700\n#> raceHispanic                          .           \n#> raceOthers                            .           \n#> educationHigh school/GED              .           \n#> educationSome college                 .           \n#> educationBachelors degree or higher   .           \n#> employment.statusEmployed non-hourly  .           \n#> employment.statusWorked previously    0.5412332930\n#> employment.statusNever worked         0.8300536966\n#> poverty.status100-200% FPL            0.0636289868\n#> poverty.status200-400% FPL           -0.0697612513\n#> poverty.status400%+ FPL              -0.2887583235\n#> veteranYes                           -0.0300714331\n#> insuranceMedicaid/Medicare            .           \n#> insurancePrivately Insured           -0.0924641963\n#> insuranceOther                        0.1503323815\n#> sex.orientationOther                  0.0765093892\n#> worried.moneyYes                      0.2812674935\n#> good.neighborhoodYes                 -0.2491796538\n#> psy.symptomYes                        0.9726200151\n#> visit.EDOne                           0.0674459188\n#> visit.ED2-3                           0.1375781535\n#> visit.ED4+                            0.4225671575\n#> surgeryOne                            .           \n#> surgeryTwo                            .           \n#> surgery3+                            -0.0839622285\n#> dr.visit6-12 months                  -0.2120063637\n#> dr.visit1-5 years                    -0.4068474570\n#> dr.visit>5 years/never               -0.1453128227\n#> cancerYes                             0.0993613856\n#> asthmaYes                             0.2997325771\n#> htnYes                                0.2082877598\n#> liver.diseaseYes                      0.8058966864\n#> diabetesYes                           0.0007357323\n#> ulcerYes                              0.4180133887\n#> strokeYes                             .           \n#> emphysemaYes                         -0.0509549802\n#> copdYes                               0.1002374105\n#> high.cholesterolYes                   0.1021030868\n#> coronary.heart.diseaseYes             0.0220558291\n#> anginaYes                             0.0849595261\n#> heart.attackYes                       0.2476656673\n#> heart.diseaseYes                      .           \n#> arthritisYes                          0.9746692568\n#> crohns.diseaseYes                     .           \n#> place.routine.careDoctor's office    -0.0583977619\n#> place.routine.careHospital/Clinic     .           \n#> place.routine.careOther place         .           \n#> trouble.asleepYes                     0.2030296869\n#> obeseYes                              0.3879895531\n#> current.smokerYes                     0.3374178965\n#> heavy.drinkerYes                     -0.1268971235\n#> hospitalization1-2 days               0.1192556336\n#> hospitalization3-5 days               .           \n#> hospitalization6+ days                1.0866087297\n#> better.health.statusYes              -0.1013785074\n#> physical.activityModerate            -0.7523741651\n#> physical.activityHigh                -0.6865233686\nfit.lasso[[2]]$beta\n#> 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                                s0\n#> sexMale                               .          \n#> hhsize                                0.018493189\n#> bornOther place                       .          \n#> maritalMarried/with partner           .          \n#> maritalDivorced/separated            -0.001903136\n#> maritalWidowed                        .          \n#> regionMidwest                        -0.077656662\n#> regionSouth                           0.066061919\n#> regionWest                            0.198988965\n#> raceBlack                            -0.313705357\n#> raceHispanic                         -0.160881871\n#> raceOthers                            .          \n#> educationHigh school/GED             -0.073948767\n#> educationSome college                 .          \n#> educationBachelors degree or higher  -0.092601336\n#> employment.statusEmployed non-hourly  .          \n#> employment.statusWorked previously    0.662301617\n#> employment.statusNever worked         0.990458783\n#> poverty.status100-200% FPL            .          \n#> poverty.status200-400% FPL           -0.253983648\n#> poverty.status400%+ FPL              -0.485846823\n#> veteranYes                           -0.095649252\n#> insuranceMedicaid/Medicare            .          \n#> insurancePrivately Insured           -0.005530756\n#> insuranceOther                        0.249278523\n#> sex.orientationOther                  .          \n#> worried.moneyYes                      0.301627606\n#> good.neighborhoodYes                 -0.554793692\n#> psy.symptomYes                        0.975043141\n#> visit.EDOne                           0.058872693\n#> visit.ED2-3                           0.255785667\n#> visit.ED4+                            0.422172434\n#> surgeryOne                            0.002831717\n#> surgeryTwo                            0.099101540\n#> surgery3+                             .          \n#> dr.visit6-12 months                  -0.105087667\n#> dr.visit1-5 years                    -0.381233762\n#> dr.visit>5 years/never               -0.459656177\n#> cancerYes                             0.281041121\n#> asthmaYes                             0.427654297\n#> htnYes                                0.219625784\n#> liver.diseaseYes                      0.580991873\n#> diabetesYes                           .          \n#> ulcerYes                              0.348542693\n#> strokeYes                             0.070369016\n#> emphysemaYes                          .          \n#> copdYes                               .          \n#> high.cholesterolYes                   0.150862244\n#> coronary.heart.diseaseYes             0.009536678\n#> anginaYes                             .          \n#> heart.attackYes                       0.405053759\n#> heart.diseaseYes                      .          \n#> arthritisYes                          0.954063592\n#> crohns.diseaseYes                     .          \n#> place.routine.careDoctor's office    -0.045764606\n#> place.routine.careHospital/Clinic     .          \n#> place.routine.careOther place         0.207253975\n#> trouble.asleepYes                     0.212898902\n#> obeseYes                              0.389340100\n#> current.smokerYes                     0.332982403\n#> heavy.drinkerYes                     -0.135194457\n#> hospitalization1-2 days               .          \n#> hospitalization3-5 days               .          \n#> hospitalization6+ days                1.018420971\n#> better.health.statusYes              -0.001173805\n#> physical.activityModerate            -0.703703292\n#> physical.activityHigh                -0.677811398\nfit.lasso[[3]]$beta\n#> 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                               s0\n#> sexMale                              -0.03555390\n#> hhsize                                0.01276707\n#> bornOther place                       .         \n#> maritalMarried/with partner          -0.00226132\n#> maritalDivorced/separated             .         \n#> maritalWidowed                        .         \n#> regionMidwest                         .         \n#> regionSouth                           .         \n#> regionWest                            0.25300309\n#> raceBlack                            -0.21629021\n#> raceHispanic                          .         \n#> raceOthers                            0.07458935\n#> educationHigh school/GED             -0.07527970\n#> educationSome college                 .         \n#> educationBachelors degree or higher  -0.08524865\n#> employment.statusEmployed non-hourly  .         \n#> employment.statusWorked previously    0.66693373\n#> employment.statusNever worked         0.96274685\n#> poverty.status100-200% FPL            .         \n#> poverty.status200-400% FPL           -0.11543598\n#> poverty.status400%+ FPL              -0.31422327\n#> veteranYes                           -0.08849244\n#> insuranceMedicaid/Medicare            .         \n#> insurancePrivately Insured           -0.14856615\n#> insuranceOther                        0.14368311\n#> sex.orientationOther                  .         \n#> worried.moneyYes                      0.28107756\n#> good.neighborhoodYes                 -0.37214169\n#> psy.symptomYes                        1.02129791\n#> visit.EDOne                           0.11427725\n#> visit.ED2-3                           0.23654896\n#> visit.ED4+                            0.53011900\n#> surgeryOne                            0.06030144\n#> surgeryTwo                            .         \n#> surgery3+                            -0.28355643\n#> dr.visit6-12 months                  -0.04219568\n#> dr.visit1-5 years                    -0.83131719\n#> dr.visit>5 years/never               -0.48832578\n#> cancerYes                             0.18366379\n#> asthmaYes                             0.13556093\n#> htnYes                                0.12622357\n#> liver.diseaseYes                      0.62123990\n#> diabetesYes                           .         \n#> ulcerYes                              0.39650846\n#> strokeYes                             .         \n#> emphysemaYes                          .         \n#> copdYes                               0.16563326\n#> high.cholesterolYes                   0.10541633\n#> coronary.heart.diseaseYes             0.08229669\n#> anginaYes                             .         \n#> heart.attackYes                       0.37154172\n#> heart.diseaseYes                      .         \n#> arthritisYes                          0.91902311\n#> crohns.diseaseYes                    -0.01478240\n#> place.routine.careDoctor's office    -0.04968533\n#> place.routine.careHospital/Clinic     .         \n#> place.routine.careOther place         0.20094580\n#> trouble.asleepYes                     0.07725267\n#> obeseYes                              0.40542559\n#> current.smokerYes                     0.27207489\n#> heavy.drinkerYes                     -0.06932537\n#> hospitalization1-2 days              -0.13525647\n#> hospitalization3-5 days               .         \n#> hospitalization6+ days                1.02580763\n#> better.health.statusYes               .         \n#> physical.activityModerate            -0.74686507\n#> physical.activityHigh                -0.90554222\nfit.lasso[[4]]$beta\n#> 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                                s0\n#> sexMale                               .          \n#> hhsize                                .          \n#> bornOther place                       .          \n#> maritalMarried/with partner           .          \n#> maritalDivorced/separated             .          \n#> maritalWidowed                        .          \n#> regionMidwest                         .          \n#> regionSouth                           .          \n#> regionWest                            0.151732262\n#> raceBlack                            -0.063747960\n#> raceHispanic                          .          \n#> raceOthers                            .          \n#> educationHigh school/GED              .          \n#> educationSome college                 .          \n#> educationBachelors degree or higher   .          \n#> employment.statusEmployed non-hourly  .          \n#> employment.statusWorked previously    0.521879242\n#> employment.statusNever worked         0.671265401\n#> poverty.status100-200% FPL            .          \n#> poverty.status200-400% FPL           -0.014851029\n#> poverty.status400%+ FPL              -0.229122884\n#> veteranYes                            .          \n#> insuranceMedicaid/Medicare            .          \n#> insurancePrivately Insured           -0.092128877\n#> insuranceOther                        .          \n#> sex.orientationOther                  0.150490732\n#> worried.moneyYes                      0.282333603\n#> good.neighborhoodYes                 -0.104236603\n#> psy.symptomYes                        1.132555465\n#> visit.EDOne                           0.009230339\n#> visit.ED2-3                           0.288663967\n#> visit.ED4+                            0.673934923\n#> surgeryOne                            .          \n#> surgeryTwo                            .          \n#> surgery3+                             .          \n#> dr.visit6-12 months                  -0.018831608\n#> dr.visit1-5 years                    -0.496368171\n#> dr.visit>5 years/never               -0.415165313\n#> cancerYes                             .          \n#> asthmaYes                             0.240957776\n#> htnYes                                0.125845727\n#> liver.diseaseYes                      0.672282863\n#> diabetesYes                           .          \n#> ulcerYes                              0.352645788\n#> strokeYes                             .          \n#> emphysemaYes                          .          \n#> copdYes                               0.054275559\n#> high.cholesterolYes                   0.057735457\n#> coronary.heart.diseaseYes             0.070550439\n#> anginaYes                             .          \n#> heart.attackYes                       0.234807175\n#> heart.diseaseYes                      .          \n#> arthritisYes                          1.043934915\n#> crohns.diseaseYes                     0.086038829\n#> place.routine.careDoctor's office     .          \n#> place.routine.careHospital/Clinic     .          \n#> place.routine.careOther place         .          \n#> trouble.asleepYes                     0.177916442\n#> obeseYes                              0.363088024\n#> current.smokerYes                     0.339985811\n#> heavy.drinkerYes                      .          \n#> hospitalization1-2 days               .          \n#> hospitalization3-5 days               0.073321609\n#> hospitalization6+ days                1.066617646\n#> better.health.statusYes               .          \n#> physical.activityModerate            -0.699175264\n#> physical.activityHigh                -0.642594150\nfit.lasso[[5]]$beta\n#> 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                                               s0\n#> sexMale                               .         \n#> hhsize                                0.01585559\n#> bornOther place                       .         \n#> maritalMarried/with partner           .         \n#> maritalDivorced/separated            -0.04982466\n#> maritalWidowed                        0.03837620\n#> regionMidwest                        -0.25040909\n#> regionSouth                           .         \n#> regionWest                            0.08288425\n#> raceBlack                            -0.07020324\n#> raceHispanic                          .         \n#> raceOthers                            .         \n#> educationHigh school/GED              .         \n#> educationSome college                 .         \n#> educationBachelors degree or higher   .         \n#> employment.statusEmployed non-hourly  .         \n#> employment.statusWorked previously    0.64497666\n#> employment.statusNever worked         0.83560645\n#> poverty.status100-200% FPL            .         \n#> poverty.status200-400% FPL           -0.03113577\n#> poverty.status400%+ FPL              -0.22313059\n#> veteranYes                           -0.20269803\n#> insuranceMedicaid/Medicare            .         \n#> insurancePrivately Insured           -0.14170986\n#> insuranceOther                        0.19604683\n#> sex.orientationOther                  .         \n#> worried.moneyYes                      0.31572735\n#> good.neighborhoodYes                 -0.31850186\n#> psy.symptomYes                        1.15194136\n#> visit.EDOne                           0.05961452\n#> visit.ED2-3                           0.34349175\n#> visit.ED4+                            0.25637410\n#> surgeryOne                            .         \n#> surgeryTwo                            .         \n#> surgery3+                             .         \n#> dr.visit6-12 months                   .         \n#> dr.visit1-5 years                    -0.32790023\n#> dr.visit>5 years/never                .         \n#> cancerYes                             0.21372688\n#> asthmaYes                             0.21206722\n#> htnYes                                0.13025900\n#> liver.diseaseYes                      0.44892643\n#> diabetesYes                           0.05119678\n#> ulcerYes                              0.27325347\n#> strokeYes                             .         \n#> emphysemaYes                          .         \n#> copdYes                               0.04954730\n#> high.cholesterolYes                   0.14824572\n#> coronary.heart.diseaseYes             .         \n#> anginaYes                             .         \n#> heart.attackYes                       0.27924438\n#> heart.diseaseYes                      .         \n#> arthritisYes                          1.00382986\n#> crohns.diseaseYes                     .         \n#> place.routine.careDoctor's office    -0.01943640\n#> place.routine.careHospital/Clinic     .         \n#> place.routine.careOther place         0.07273335\n#> trouble.asleepYes                     0.14684831\n#> obeseYes                              0.38029871\n#> current.smokerYes                     0.14111139\n#> heavy.drinkerYes                      .         \n#> hospitalization1-2 days              -0.02310396\n#> hospitalization3-5 days               .         \n#> hospitalization6+ days                1.08731840\n#> better.health.statusYes               .         \n#> physical.activityModerate            -0.76639891\n#> physical.activityHigh                -0.38546251\n\n\n# AUCs from different folds\nauc.lasso\n#> [1] 0.8396619 0.8129805 0.8465035 0.7896878 0.8342721\n\n# Calibration slope from different folds\ncal.slope.lasso\n#> [1] 1.1287166 0.9985467 1.1224240 0.8934633 1.0131154\n\n# Brier score from different folds\nbrier.lasso\n#> [1] 0.08524781 0.08476661 0.08666820 0.09071329 0.08911735\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.lasso)\n#> [1] 0.8246212\n\n# Average calibration slope\nmean(cal.slope.lasso)\n#> [1] 1.031253\n\n# Average Brier score\nmean(brier.lasso)\n#> [1] 0.08730265\n\nAlthough the authors used multiple imputation, our AUC from the LASSO model with complete case data analysis is not that different. Note: the authors reported the AUC values in Table 2.\nRandom forest for surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model with 5-fold CV:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\nFolds\n\nk <- 5\ntable(nfolds)\n#> nfolds\n#>    1    2    3    4    5 \n#> 1451 1457 1496 1468 1408\n\nFormula\n\nFormula\n#> HICP ~ sex + hhsize + born + marital + region + race + education + \n#>     employment.status + poverty.status + veteran + insurance + \n#>     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#>     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#>     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#>     coronary.heart.disease + angina + heart.attack + heart.disease + \n#>     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#>     obese + current.smoker + heavy.drinker + hospitalization + \n#>     better.health.status + physical.activity\n\n5-fold CV random forest\n\nfit.rf <- list(NULL)\nauc.rf <- NULL\ncal.slope.rf <- brier.rf <- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train <- dat.complete[nfolds != fold, ]\n  \n  # Test data\n  dat.test <- dat.complete[nfolds == fold, ]\n  \n  # Tuning the hyperparameters \n  ## Grid with 1000 models - huge time consuming\n  #grid.search <- expand.grid(mtry = 1:10, node.size = 1:10, \n  #                          num.trees = seq(50,500,50), \n  #                           OOB_RMSE = 0)\n  \n  ## Grid with 36 models as an exercise\n  grid.search <- expand.grid(\n    mtry = 5:7, \n    node.size = 1:3, \n    num.trees = seq(200,500,100),\n    OOB_RMSE = 0) \n  \n  ## Model with grids \n  for(ii in 1:nrow(grid.search)) {\n    # Model on training set with grid\n    fit.rf.tune <- ranger(\n      formula = Formula,\n      data = dat.train, \n      num.trees = grid.search$num.trees[ii],\n      mtry = grid.search$mtry[ii], \n      min.node.size = grid.search$node.size[ii],\n      importance = 'impurity', \n      case.weights = dat.train$wgt)\n    \n    # Add Out-of-bag (OOB) error to grid\n    grid.search$OOB_RMSE[ii] <- \n      sqrt(fit.rf.tune$prediction.error)\n  }\n  # Position of the tuned hyperparameters\n  position <- which.min(grid.search$OOB_RMSE)\n  \n  # Fit the model on the training set with tuned hyperparameters\n  fit.rf[[fold]] <- ranger(\n    formula = Formula,\n    data = dat.train, \n    case.weights = dat.train$wgt, \n    probability = T,\n    num.trees = grid.search$num.trees[position],\n    min.node.size = grid.search$node.size[position], \n    mtry = grid.search$mtry[position], \n    importance = 'impurity')\n  \n  # Prediction on the test set\n  dat.test$pred.rf <- predict(\n    fit.rf[[fold]], \n    data = dat.test)$predictions[,2]\n  \n  # AUC on the test set with sampling weights\n  auc.rf[fold] <- WeightedAUC(\n    WeightedROC(dat.test$pred.rf, \n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.rf[dat.test$pred.rf == 0] <- 0.00001\n  mod.cal <- glm(HICP ~ Logit(dat.test$pred.rf), \n                 data = dat.test, \n                 family = binomial, \n                 weights = wgt)\n  cal.slope.rf[fold] <- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.rf[fold] <- mean(brierscore(\n    HICP ~ dat.test$pred.rf, \n    data = dat.test,\n    wt = dat.test$wgt))\n}\n\nModel performance\nLet’s check how prediction worked.\n\n# Fitted random forest models\nfit.rf[[1]]\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  500 \n#> Sample size:                      5829 \n#> Number of independent variables:  42 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.0899798\nfit.rf[[2]]\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  400 \n#> Sample size:                      5823 \n#> Number of independent variables:  42 \n#> Mtry:                             5 \n#> Target node size:                 2 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.0899217\nfit.rf[[3]]\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  500 \n#> Sample size:                      5784 \n#> Number of independent variables:  42 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.08972587\nfit.rf[[4]]\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  400 \n#> Sample size:                      5812 \n#> Number of independent variables:  42 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.08934626\nfit.rf[[5]]\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  400 \n#> Sample size:                      5872 \n#> Number of independent variables:  42 \n#> Mtry:                             5 \n#> Target node size:                 3 \n#> Variable importance mode:         impurity \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.08929408\n\n\n# AUCs from different folds\nauc.rf\n#> [1] 0.8393204 0.7905715 0.8244523 0.7811141 0.8301950\n\n# Calibration slope from different folds\ncal.slope.rf\n#> [1] 1.2842866 0.9933553 1.1486583 0.9134864 1.2263184\n\n# Brier score from different folds\nbrier.rf\n#> [1] 0.08745016 0.08881079 0.08913696 0.09163393 0.08895764\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.rf)\n#> [1] 0.8131307\n\n# Average calibration slope\nmean(cal.slope.rf)\n#> [1] 1.113221\n\n# Average Brier score\nmean(brier.rf)\n#> [1] 0.0891979\n\nThis AUC from random forest is approximately the same as obtained from the LASSO model.\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\n# Fold 1\nggplot(\n  enframe(fit.rf[[1]]$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n# Fold 5\nggplot(\n  enframe(fit.rf[[5]]$variable.importance,\n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\nReferences"
  },
  {
    "objectID": "machinelearningQ.html#live-quiz",
    "href": "machinelearningQ.html#live-quiz",
    "title": "Quiz (L)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "machinelearningQ.html#download-quiz",
    "href": "machinelearningQ.html#download-quiz",
    "title": "Quiz (L)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R functions (L)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n fancyRpartPlot \n    rattle \n    To plot an rpart object \n  \n\n fviz_nbclust \n    factoextra \n    To visualize the optimal number of clusters \n  \n\n kmeans \n    base/stats \n    To conduct K-Means cluster analysis \n  \n\n lowess \n    base/stats \n    To perform scatter plot smoothing aka lowess smoothing \n  \n\n rpart \n    rpart \n    To fit a classification tree (CART) \n  \n\n terms \n    base/stats \n    To extarct terms objects \n  \n\n varImp \n    caret \n    To calculate the variable importance measure"
  },
  {
    "objectID": "machinelearningCausal.html#background",
    "href": "machinelearningCausal.html#background",
    "title": "ML in causal inference",
    "section": "Background",
    "text": "Background\nThis chapter provides a detailed exploration of Targeted Maximum Likelihood Estimation (TMLE) in causal inference. The first tutorial motivates the use of TMLE by highlighting its advantages over traditional methods, focusing on the limitations of these approaches and introducing the RHC dataset. The second tutorial delves into the SuperLearner method for ensemble modeling, discussing the importance of algorithm diversity, cross-validation, and adaptable libraries. The third tutorial offers a comprehensive guide to applying TMLE for binary outcomes, emphasizing diverse SuperLearner libraries, effective sample sizes, and candidate learner selection. The fourth tutorial extends TMLE to continuous outcomes, covering transformations, interpretation, and comparisons with default TMLE libraries and traditional regression. These tutorials should equip readers with a robust understanding of TMLE and its practical applications in causal inference and epidemiology.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "machinelearningCausal.html#overview-of-tutorials",
    "href": "machinelearningCausal.html#overview-of-tutorials",
    "title": "ML in causal inference",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have developed a solid understanding of predictive modeling, data splitting/cross-validation, propensity scores and various machine learning algorithms. These insights have prepared us for the advanced causal inference techniques we will encounter in this chapter. As we explore TMLE and SuperLearner, we will draw upon our knowledge of propensity score and machine learning to unlock the potential of their use in building powerful causal inference tools.\n\nMotivation for learning and using TMLE\nThis tutorial discusses the motivation for using the TMLE method in causal inference. It highlights the limitations of traditional methods such as propensity score approaches and direct application of machine learning in terms of assumptions and statistical inference. TMLE is presented as a doubly robust method that incorporates machine learning while allowing straightforward statistical inference. The tutorial reintroduces RHC data for demonstration.\n\n\nUnderstanding SuperLearner\nThis tutorial focuses on using the SuperLearner method for ensemble modeling. SuperLearner is a type 2 ensemble method that combines various predictive algorithms to create a robust predictive model. It employs cross-validation to determine the best-weighted combination of algorithms, based on specified predictive performance metrics. The tutorial provides guidelines for selecting a diverse set of algorithms, considering computational feasibility, and adapting the library of algorithms based on sample characteristics. Examples of different types of learners that can be included in the SuperLearner library are provided, ranging from parametric to highly data-adaptive and non-linear models. The tutorial also mentions the default libraries for estimating outcomes and propensity scores in the context of TMLE.\n\n\nDealing with binary outcomes within TMLE framework\nThis tutorial is a comprehensive guide to applying TMLE for binary outcomes. It covers the entire TMLE process, from constructing initial outcome and exposure models to targeted adjustment via propensity scores and treatment effect estimation. It emphasizes the importance of specifying a diverse SuperLearner library for both exposure and outcome models, determining effective sample sizes, and selecting candidate learners. The tutorial demonstrates TMLE application using the tmle package and includes a thorough comparison with default SuperLearner libraries and traditional regression, presenting estimates, confidence intervals, and a comparative table of results.\n\n\nDealing with continuous outcomes within TMLE framework\nThis tutorial offers comprehensive guidance on implementing TMLE for continuous outcomes, including transformations and result interpretation. It introduces the application of TMLE for continuous outcomes, emphasizing the process of constructing a SuperLearner and key considerations such as effective sample size and learner selection. Notably, it highlights the essential transformation of continuous outcome variables to a standardized range (0 to 1) using min-max normalization before applying TMLE with a Gaussian family. The tutorial demonstrates the post-TMLE rescaling of treatment effect estimates and confidence intervals to the original scale and offers a comparative analysis with default TMLE libraries and traditional regression.\n\n\nComparing results\nIn this comprehensive tutorial, various statistical methods were applied to investigate the association between RHC and death using the RHC dataset. These methods included logistic regression, propensity score matching and weighting with both logistic regression and Super Learner, as well as TMLE. The results consistently indicated that participants with RHC use had higher odds of death compared to those without RHC use, with odds ratios ranging similarly across different modeling approaches, but also showing a trend when machine learners are incorporated.\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough of the additional resources, feel free to watch the video below.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences"
  },
  {
    "objectID": "machinelearningCausal0.html#ml-in-causal-inference",
    "href": "machinelearningCausal0.html#ml-in-causal-inference",
    "title": "Concepts (C)",
    "section": "ML in causal inference",
    "text": "ML in causal inference\nIn comparative effectiveness studies, researchers typically use propensity score methods. However, propensity score methods have known limitations in real-world scenarios, when the true data generating mechanism is unknown. Targeted maximum likelihood estimation (TMLE) is an alternative estimation method with a number of desirable statistical properties. It is a doubly robust method, enabling the integration of machine learning approaches within the framework. Despite the fact that this method has been shown to perform better in terms of statistical properties (e.g., variance estimation) than propensity score methods in a variety of scenarios, it is not widely used in medical research as the implementation details of this approach are generally not well understood. In this section, we will explain this method in details."
  },
  {
    "objectID": "machinelearningCausal0.html#reading-list",
    "href": "machinelearningCausal0.html#reading-list",
    "title": "Concepts (C)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Karim and Frank 2021)\nOptional reading: (Frank and Karim 2023)"
  },
  {
    "objectID": "machinelearningCausal0.html#video-lessons",
    "href": "machinelearningCausal0.html#video-lessons",
    "title": "Concepts (C)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning\n\n\n\nThe workshop was first developed for R/Medicine Virtual Conference https://r-medicine.org/ 2021, August 24th. What is included in this Video Lesson/workshop:\n\nChapter 1 RHC data description 4:22\nChapter 2 G-computation 23:13\nChapter 3 G-computation using ML 45:02\nChapter 4 IPTW 1:18:50\nChapter 5 IPTW using ML 1:30:11\nChapter 6 TMLE 1:36:41\nChapter 7 Pre-packaged software 1:58:05\nChapter 8 Final Words 2:14:36\n\nThe timestamps are also included in the YouTube video description."
  },
  {
    "objectID": "machinelearningCausal0.html#links",
    "href": "machinelearningCausal0.html#links",
    "title": "Concepts (C)",
    "section": "Links",
    "text": "Links\nThe original workshop materials are available here."
  },
  {
    "objectID": "machinelearningCausal0.html#references",
    "href": "machinelearningCausal0.html#references",
    "title": "Concepts (C)",
    "section": "References",
    "text": "References\n\n\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. “Implementing TMLE in the Presence of a Continuous Outcome.” Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nKarim, Ehsan, and Hanna Frank. 2021. ehsanx/TMLEworkshop: R Guide for TMLE in Medical Research (version v1.1). Zenodo. https://doi.org/10.5281/zenodo.5246085."
  },
  {
    "objectID": "machinelearningCausal1.html",
    "href": "machinelearningCausal1.html",
    "title": "Motivation",
    "section": "",
    "text": "When using methods like propensity score approaches, we are making assumptions about the model specification. For example, we must specify any interaction terms.\nWith machine learning methods, these assumptions can be relaxed somewhat, as some machine learning methods allow automatic detection of data structures such as interactions.\nHowever, machine learning was created for prediction modeling, not with causal inference in mind. Statistical inference such as calculating standard errors and confidence intervals is not as straightforward since the estimator given by machine learning methods does not follow a known statistical distribution. By contrast, the estimators resulting from a standard regression using maximum likelihood estimation will follow an approximately normal distribution, where it is easy to calculate standard errors and confidence intervals.\n\nTargeted maximum likelihood estimation (TMLE) is a causal inference method that can incorporate machine learning in a way that still allows straightforward statistical inference based on theoretical development grounded in semi-parametric theory.\n\nTMLE is a doubly robust method. This means it uses both the exposure (AKA propensity score) model and the outcome model. As long as one of these models is correctly specified, TMLE will give a consistent estimator, meaning it gets closer and closer to the true value as the sample size increases.\nSince TMLE uses both the exposure and the outcome model, machine learning can be used in each of these intermediary modeling steps while allowing straightforward statistical inference.\n\n\nIt has been shown that TMLE outperform singly robust methods with machine learning, such as IPTW.\n\nRevisiting RHC Data\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial uses the same data as some of the previous tutorials, including working with a predictive question, machine learning with a continuous outocome, and machine learning with a binary outcome.\n\n\n\nObsData <- readRDS(file = \n                     \"Data/machinelearningCausal/rhcAnalyticTest.RDS\")\nbaselinevars <- names(dplyr::select(ObsData, \n                         !c(RHC.use,Length.of.Stay,Death)))\nhead(ObsData)\n\n\n\n  \n\n\n\nTable 1\nOnly for some demographic and comorbidity variables; matches with Table 1 in Connors et al. (1996).\n\ntab0 <- CreateTableOne(vars = c(\"age\", \"sex\", \"race\", \n                                \"Disease.category\", \"Cancer\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab0, showAllLevels = FALSE)\n#>                       Stratified by RHC.use\n#>                        0            1           \n#>   n                    3551         2184        \n#>   age (%)                                       \n#>      [-Inf,50)          884 (24.9)   540 (24.7) \n#>      [50,60)            546 (15.4)   371 (17.0) \n#>      [60,70)            812 (22.9)   577 (26.4) \n#>      [70,80)            809 (22.8)   529 (24.2) \n#>      [80, Inf)          500 (14.1)   167 ( 7.6) \n#>   sex = Female (%)     1637 (46.1)   906 (41.5) \n#>   race (%)                                      \n#>      white             2753 (77.5)  1707 (78.2) \n#>      black              585 (16.5)   335 (15.3) \n#>      other              213 ( 6.0)   142 ( 6.5) \n#>   Disease.category (%)                          \n#>      ARF               1581 (44.5)   909 (41.6) \n#>      CHF                247 ( 7.0)   209 ( 9.6) \n#>      Other              955 (26.9)   208 ( 9.5) \n#>      MOSF               768 (21.6)   858 (39.3) \n#>   Cancer (%)                                    \n#>      None              2652 (74.7)  1727 (79.1) \n#>      Localized (Yes)    638 (18.0)   334 (15.3) \n#>      Metastatic         261 ( 7.4)   123 ( 5.6)\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996."
  },
  {
    "objectID": "machinelearningCausal2.html",
    "href": "machinelearningCausal2.html",
    "title": "SuperLearner",
    "section": "",
    "text": "Choosing Learners\nSuperLearner is a type 2 ensemble method, meaning it combines many methods of different types into one predictive model. SuperLearner uses cross-validation to find the best weighted combination of algorithms based on the predictive performance measure specified (default in the SuperLearner package is non-negative least squares based on the Lawson-Hanson algorithm (Mullen and Stokkum 2023), but measures such as AUC can also be used). To run SuperLearner, the user needs to specify a library consisting of all the different methods SuperLearner should incorporate in the final model, as well as the number of cross-validation folds.\n\n\nSee previous chapter for other types of ensemble learning methods.\nSuperLearner will perform as well as possible given the library of algorithms considered. A very recent paper by Phillips et al. (2023) provides some concrete guidelines for the determination of the number of cross-validation folds necessary and the selection of algorithms to include. Overall, we want to make sure the set of algorithms provided is:\n\n\nDiverse: Having a rich library of algorithms allows the SuperLearner to adapt to a range of underlying data structures. Diverse libraries include:\n\nParametric learners such as generalized linear models (GLMs)\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\n\n\nComputationally feasible: Lots of machine learning algorithms take a long time to run. Having multiple computationally intensive algorithms in your library will cause the SuperLearner as a whole to take much too long to run.\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the more specific guidelines depend on our effective sample size. For binary outcomes, this can be calculated as:\n\\[ n_{eff}=min(n, 5*(n*min(\\bar{p},1-\\bar{p})))   \\]\nwhere \\(\\bar{p}\\): prevalence of the outcome.\nFor continuous outcomes, the effective sample size is the same as the sample size (\\(n_{eff} = n\\)).\n\n\nWe also want to consider the characteristics of our particular sample.\n\nIf there are continuous covariates: We should include learners that do not force relationships to be linear/monotonic. For example, we could include regression splines, support vector machines, and tree-based methods like regression trees.\nIf we have high-dimensional data (a large number of covariates e.g. more than \\(n_{eff}/20\\) ): We should include some learners that fall under the class of screeners. These are learners which incorporate dimension reduction such as LASSO and random forests.\nIf the sample size is very large (i.e. \\(n_{eff}>500\\) ): We should include as many learners as is computationally feasible.\nIf the sample size is small (i.e. \\(n_{eff} \\leq 500\\) ): We should include fewer learners (e.g. up to \\(n_{eff}/5\\) ), and include less flexible learners.\n\nSome examples of learners that could be included are given in the table below (Polley 2021):\n\n\n\n\n\n\nType of learner\nExamples\n\n\n\nParametric\n\nSL.mean: simple mean\nSL.glm: generalized linear models\nSL.lm: ordinary least squares\nSL.speedglm: fast version of glm\nSL.speedlm: fast version of lm\nSL.gam: generalized additive methods\nSL.step: choose model based on AIC (backwards or forwards or both)\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression using elastic net (ridge regression and Lasso)\n\nKernel-based methods\n\nSL.kernelKnn: k-nearest neighbours\nSL.ksvm: kernel-based support vector machine\n\n\nSL.xgboost: extreme gradient boosting\nSL.gbm: gradient-boosted machines\nSL.nnet: neural networks\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.earth: multivariate adaptive regression splines\n\nTree-based methods\n\nSL.randomForest: random forests\ntmle.SL.dbarts2: bayesian additive regression trees\nSL.cforest: random forests using conditional inference trees\nSL.ranger: fast implementation of random forest suited for high dimensional data\n\n\nSL.svm: support vector machines\n\n\n\nScreeners\n\nscreen.corP: retain covariates with correlation with outcome p-value <0.1\nscreen.corRank: retain top j covariates with highest correlation with outcome\nscreen.glmnet: Lasso\nscreen.randomForest: random forests\nscreen.SIS: retain covariates based on distance correlation\n\n\n\n\nThere is also a useful tool implemented in the SuperLearner library which allows us to easily see a list of all available learners.\n\nSuperLearner::listWrappers()\n#> All prediction algorithm wrappers in SuperLearner:\n#>  [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n#>  [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n#>  [7] \"SL.earth\"            \"SL.extraTrees\"       \"SL.gam\"             \n#> [10] \"SL.gbm\"              \"SL.glm\"              \"SL.glm.interaction\" \n#> [13] \"SL.glmnet\"           \"SL.ipredbagg\"        \"SL.kernelKnn\"       \n#> [16] \"SL.knn\"              \"SL.ksvm\"             \"SL.lda\"             \n#> [19] \"SL.leekasso\"         \"SL.lm\"               \"SL.loess\"           \n#> [22] \"SL.logreg\"           \"SL.mean\"             \"SL.nnet\"            \n#> [25] \"SL.nnls\"             \"SL.polymars\"         \"SL.qda\"             \n#> [28] \"SL.randomForest\"     \"SL.ranger\"           \"SL.ridge\"           \n#> [31] \"SL.rpart\"            \"SL.rpartPrune\"       \"SL.speedglm\"        \n#> [34] \"SL.speedlm\"          \"SL.step\"             \"SL.step.forward\"    \n#> [37] \"SL.step.interaction\" \"SL.stepAIC\"          \"SL.svm\"             \n#> [40] \"SL.template\"         \"SL.xgboost\"\n#> \n#> All screening algorithm wrappers in SuperLearner:\n#> [1] \"All\"\n#> [1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n#> [4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n#> [7] \"screen.ttest\"          \"write.screen.template\"\n\nSuperLearner in TMLE\n\n\nThe default SuperLearner library for estimating the outcome includes (Gruber, Van Der Laan, and Kennedy 2020)\n\n\nSL.glm: generalized linear models (GLMs)\n\nSL.glmnet: least absolute shrinkage and selection operator (LASSO)\n\ntmle.SL.dbarts2: modeling and prediction using Bayesian Additive Regression Trees (BART)\n\n\n\nThe default library for estimating the propensity scores includes\n\n\nSL.glm: generalized linear models (GLMs)\n\ntmle.SL.dbarts.k.5: SL wrappers for modeling and prediction using BART\n\nSL.gam: generalized additive models (GAMs)\n\n\n\nIt is certainly possible to use different set of learners\n\nMore methods can be added by\n\nspecifying lists of models in the Q.SL.library (for the outcome model) and g.SL.library (for the propensity score model)\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nGruber, S., M. Van Der Laan, and C. Kennedy. 2020. Package ’Tmle’. https://cran.r-project.org/web/packages/tmle/tmle.pdf.\n\n\nMullen, Katharine M., and Ivo H. M. van Stokkum. 2023. Package ’Nnls’. https://cran.r-project.org/web/packages/nnls/nnls.pdf.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023.\n\n\nPolley, Eric. 2021. Package ’SuperLearner’. https://cran.r-project.org/web/packages/SuperLearner/SuperLearner.pdf."
  },
  {
    "objectID": "machinelearningCausal3.html",
    "href": "machinelearningCausal3.html",
    "title": "Binary Outcomes",
    "section": "",
    "text": "For this example we will be looking at the binary outcome variable death.\n\n# Data\nload(file = \"Data/machinelearningCausal/cl2.RData\")\n\n# Table 1\ntab1 <- CreateTableOne(vars = c(\"Death\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#>                    Stratified by RHC.use\n#>                     0           1          \n#>   n                 3551        2184       \n#>   Death (mean (SD)) 0.63 (0.48) 0.68 (0.47)\n\nTMLE\nTMLE works by first constructing an initial outcome and extracting a crude estimate of the treatment effect. Then, TMLE aims to refine the initial estimate in the direction of the true value of the parameter of interest through use of the exposure model.\nTo label the treatment effect given by TMLE as causal, the same conditional exchangeability, positivity, and consistency assumptions must be met as for other modeling strategies (see introduction to propensity score). TMLE also assumes that at least one of the exposure or outcome model is correctly specified. If this does not hold, TMLE does not necessarily produce a consistent estimator.\n\n\nLuque-Fernandez et al. (2018) discussed the implementation of TMLE, and providing a detailed step-by-step guide, primarily focusing on a binary outcome.\nThe basic steps are:\n\nConstruct initial outcome model & get crude estimate\nConstruct exposure model and use propensity scores to update the initial outcome model through a targeted adjustment\nExtract treatment effect estimate\nEstimate confidence interval based on a closed-form formula\n\nThe tmle package implements TMLE for both binary and continuous outcomes, and uses the SuperLearner to construct the exposure and outcome models.\nThe tmle method takes a number of parameters, including:\n#> Warning: package 'knitr' was built under R version 4.2.3\n\n\n\n\n Term \n    Description \n  \n\n\n Y \n    Outcome vector \n  \n\n A \n    Exposure vector \n  \n\n W \n    Matrix that includes vectors of all covariates \n  \n\n family \n    Distribution \n  \n\n V \n    Cross-validation folds for exposure and outcome modeling \n  \n\n Q.SL.library \n    Set of machine learning methods to use for SuperLearner for outcome modeling \n  \n\n g.SL.library \n    Set of machine learning methods to use for SuperLearner for exposure modeling \n  \n\n\n\nConstructing SuperLearner\nWe will need to specify two SuperLearners, one for the exposure and one for the outcome model. We will need to consider the characteristics of our sample in order to decide the number of cross-validation folds and construct a diverse and computationally feasible library of algorithms.\nNumber of folds\nFirst, we need to define the number of cross-validation folds to use for each model. This depends on our effective sample size (Phillips et al. 2023).\nOur effective sample size for the outcome model is:\n\nn <- nrow(ObsData) \np <- nrow(ObsData[ObsData$Death == 1,])/n \nn_eff <- min(n, 5*(n*min(p, 1-p))) \nn_eff\n#> [1] 5735\n\nOur effective sample size for the exposure model is:\n\np_exp <- nrow(ObsData[ObsData$RHC.use == 1,])/n \nn_eff_exp <- min(n, 5*(n*min(p, 1-p))) \nn_eff_exp\n#> [1] 5735\n\nFor both models, the effective sample size is the same as our sample size, \\(n = 5,735\\).\nSince \\(5,000 \\leq n_{eff} \\leq 10,000\\), we should use 5 or more cross-validation folds according to Phillips et al. (2023). For the sake of computational feasibility, we will use 5 folds in this example.\nCandidate learners\nThe second step is to define the library of learners we will feed in to SuperLearner as potential options for each model (exposure and outcome). In this example, some of our covariates are continuous variables, such as temperature and blood pressure, so we need to include learners that allow non-linear/monotonic relationships.\nSince \\(n\\) is large (\\(>5000\\)), we should include as many learners as is computationally feasible in our libraries.\nFurthermore, we have 50 covariates:\n\nlength(c(baselinevars, \"Length.of.Stay\"))\n#> [1] 50\n\n\\(5735/20 = 286.75\\), and \\(50<286.75\\), so we do not have high-dimensional data and including screeners is optional (Phillips et al. 2023).\nSince the requirements for the exposure and outcome models are the same in this example, we will use the same SuperLearner library for both. Overall for this example we need to make sure to include:\n\nParametric learners\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\nLearners that allow non-linear/monotonic relationships\n\nFor this example, we will include the following learners:\n\n\nParametric\n\nSL.mean: only mean\nSL.glm: generalized linear model\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression such as lasso\nSL.xgboost: extreme gradient boosting\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.randomForest: random forest\ntmle.SL.dbarts2: bayesian additive regression tree\nSL.svm: support vector machine\n\n\n\n\n# Construct the SuperLearner library\nSL.library <- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nTMLE with SuperLearner\nTo run TMLE, we need to install the tmle package and load it on the R environment.\n\n#install.packages(c('tmle', 'xgboost'))\nrequire(tmle)\nrequire(xgboost)\n\nWe also need to create a data frame containing only the covariates:\n\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Death, RHC.use))\nObsData$Death <- as.numeric(ObsData$Death)\n\nThen we can run TMLE using the tmle method from the tmle package:\n\nset.seed(1444) \n\ntmle.fit <- tmle::tmle(Y = ObsData$Death, \n                   A = ObsData$RHC.use, \n                   W = ObsData.noYA, \n                   family = \"binomial\", \n                   V.Q = 5,\n                   V.g = 5,\n                   Q.SL.library = SL.library, \n                   g.SL.library = SL.library)\n\ntmle.est.bin <- tmle.fit$estimates$OR$psi\ntmle.ci.bin <- tmle.fit$estimates$OR$CI\n\n\n\n\n\n\n\nATE for binary outcome using user-specified library: 1.28 and 95% CI is 1.1992981, 1.3761869\nThese results show those who received RHC had odds of death that were 1.28 times as high as the odds of death in those who did not receive RHC.\nUnderstanding defaults\nWe can compare the results using our specified SuperLearner library to the results we would get when using the tmle package’s default SuperLearner libraries. To do this we simply do not specify libraries for the Q.SL.library and g.SL.library arguments.\n\n# small test library \n# with only glm just \n# for sake of making this work\nSL.library.test <- c(\"SL.glm\")\n\n\nset.seed(1444) \n\ntmle.fit.def <- tmle::tmle(Y = ObsData$Death, \n                           A = ObsData$RHC.use, \n                           W = ObsData.noYA, \n                           family = \"binomial\", \n                           V.Q = 5,\n                           V.g = 5)\n# Q.SL.library = SL.library.test,  ## removed this line\n# g.SL.library = SL.library.test)  ## removed this line\n\ntmle.est.bin.def <- tmle.fit.def$estimates$OR$psi\ntmle.ci.bin.def <- tmle.fit.def$estimates$OR$CI\n\n\n\n\n\n\n\nATE for binary outcome using default library: 1.32 with 95% CI 1.1466162, 1.5219877.\nThese ATE when using the default SuperLearner library (1.31) is very close to the ATE when using our user-specified SuperLearner library (1.29). However, the confidence interval from TMLE using the default SuperLearner library (1.17, 1.46) is slightly wider than the confidence interval from TMLE using our user-specified SuperLearner library (1.20, 1.39).\nComparison of results\nWe can also compare these results to those from a basic regression and from the literature.\n\n\n\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.Death <- c(baselinevars, \"Length.of.Stay\")\nout.formula.bin <- as.formula(\n  paste(\"Death~ RHC.use +\",\n        paste(baselineVars.Death, \n              collapse = \"+\")))\nfit1.bin <- glm(out.formula.bin, data = ObsData, family=\"binomial\")\n\n\n\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 4 showed that, after propensity score pair (1-to-1) matching, the odds of in-hospital mortality were 39% higher in those who received RHC (OR: 1.39 (1.15, 1.67)).\n\n\n\n\n\n\n\n\n method.list \n    Estimate \n    2.5 % \n    97.5 % \n  \n\n\n Adjusted Regression \n    0.07 \n    0.04 \n    0.09 \n  \n\n TMLE (user-specified SL library) \n    1.28 \n    1.20 \n    1.38 \n  \n\n TMLE (default SL library) \n    1.32 \n    1.15 \n    1.52 \n  \n\n Connors et al. (1996) paper \n    1.39 \n    1.15 \n    1.67 \n  \n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.\n\n\nLuque-Fernandez, Miguel Angel, Michael Schomaker, Bernard Rachet, and Mireille E Schnitzer. 2018. “Targeted Maximum Likelihood Estimation for a Binary Treatment: A Tutorial.” Statistics in Medicine 37 (16): 2530–46.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023."
  },
  {
    "objectID": "machinelearningCausal4.html",
    "href": "machinelearningCausal4.html",
    "title": "Continuous Outcomes",
    "section": "",
    "text": "We will now go through an example of using TMLE for a continuous outcome. The setup for SuperLearner in this case is similar to that for binary outcomes, so rather than going through the SuperLearner steps again, we will instead focus on the additional steps that are necessary for running the tmle method on continuous outcomes.\n\n\nFrank and Karim (2023) extensively discussed the implementation of TMLE for continuous outcomes, providing a detailed step-by-step guide using the openly accessible RHC dataset. In this tutorial, we will revisit the same example with additional explanations.\n\n\n\n\n\n\nNote\n\n\n\nOnly outcome variable (Length of stay); slightly different than Table 2 in Connors et al. (1996) (means were 20.5 vs. 25.7; and medians were 16 vs. 17).\n\n\n\ntab1 <- CreateTableOne(vars = c(\"Length.of.Stay\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#>                             Stratified by RHC.use\n#>                              0             1            \n#>   n                           3551          2184        \n#>   Length.of.Stay (mean (SD)) 19.53 (23.59) 24.86 (28.90)\n\n\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==0])\n#> [1] 12\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==1])\n#> [1] 16\n\nConstructing SuperLearner\nJust as we did for a binary outcome, we will need to specify two SuperLearners, one for the exposure and one for the outcome model.\nThe effective sample size for a continuous outcome is just \\(n_{eff}=n=5,735\\). We calculated the effective sample size for the exposure model earlier, which also turned out to be \\(n_{eff}=n=5,735\\). So once again we will use 5 folds because \\(5,000 \\leq n_{eff} \\leq 10,000\\) (Phillips et al. 2023).\nSimilarly to our example with the binary outcome, the key considerations for the library of learners are:\n\nWe have some continuous covariates, and should therefore include learners that allow non-linear/monotonic relationships.\nWe have a large \\(n\\), so should include as many learners as is computationally feasible.\nWe have 49 covariates and 5,735 observations, so we do not have high-dimensional data and including screeners is optional.\n\nAgain the requirements for the exposure and outcome models are the same and we can use the same library for both models. Note that even though one model will have a binary dependent variable, and one will have a continuous dependent variable, most of the available learners automatically adapt to binary and continuous dependent variables.\nFor this example, we will use the same SuperLearner library as for the binary outcome example.\n\n# Construct the SuperLearner library\nSL.library <- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nDealing with continuous outcomes\nFor this example, we will be examining the length of stay in hospital outcome.\nThe key difference between running TMLE on a continuous outcome in comparison to running it with a binary outcome, is that we must transform the outcome to fall within the range of 0 to 1, so that the modeled outcomes fall within the range of the outcome’s true distribution (Gruber and Laan 2010).\nTo transform the outcome, we can use min-max normalization:\n\\[\nY_{transformed} = \\frac{Y-Y_{min}}{Y_{max}-Y_{min}}\n\\]\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y <- min(ObsData$Length.of.Stay)\nmax.Y <- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf <- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nOnce we have transformed the outcome to fall within the range of 0 to 1, we can run TMLE as before, using the tmle method in the tmle package:\n\n# create data frame containing only covariates\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont <- tmle::tmle(Y = ObsData$Length.of.Stay_transf, \n                       A = ObsData$RHC.use, \n                       W = ObsData.noYA, \n                       family = \"gaussian\", \n                       V.Q = 5,\n                       V.g = 5,\n                       Q.SL.library = SL.library,\n                       g.SL.library = SL.library)\n\n\n\n\nOnce the tmle method has run, we still have one step to complete to get our final estimate. At this point, we must transform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome’s original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n\n\n\n# transform back the ATE estimate\ntmle.est.cont <- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$psi\ntmle.est.cont\n#> [1] 2.939622\n\n\n\n\nWe also have to transform the confidence interval back to the original scale:\n\ntmle.ci.cont <- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$CI\n\n\n\n\n\n\n\nATE for continuous outcome: 2.9396218, and 95 % CI is 1.959698, 3.9195455.\nThe results indicate that if all participants had received RHC, the average length of stay in hospital would be 2.95 (1.99, 3.91) days longer than if no participants had received RHC.\nUnderstanding defaults\nTransform outcome:\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y <- min(ObsData$Length.of.Stay)\nmax.Y <- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf <- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nRun TMLE, using the tmle package’s default SuperLearner library:\n\n# create data frame containing only covariates\nObsData.noYA <- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont.def <- tmle::tmle(\n  Y = ObsData$Length.of.Stay_transf, \n  A = ObsData$RHC.use, \n  W = ObsData.noYA,\n  family = \"gaussian\",\n  V.Q = 5,\n  V.g = 5)\n# Q.SL.library = SL.library.test,  \n## removed this line\n# g.SL.library = SL.library.test)  \n## removed this line\n\n\n\n\nTransform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome’s original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n\n\n\n# transform back the ATE estimate\ntmle.est.cont.def <- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$psi\ntmle.est.cont.def\n#> [1] 3.352891\n\n\n\n\nTransform the confidence interval back to the original scale:\n\ntmle.ci.cont.def <- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$CI\n\n\n\n\n\n\n\nATE for continuous outcome using default library: 3.0362984, and 95% CI 1.2686301, 4.8039667.\nThe estimate using the default SuperLearner library (2.18) is similar to the estimate we got when using our user-specified SuperLearner library (2.95). However, the confidence interval using the default SuperLearner library (1.25, 4.37) was much wider than that using our user-specified SuperLearner library (1.99, 3.91).\nComparison of results\nAdjusted regression:\n\n\n\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.LoS <- c(baselinevars, \"Death\")\nout.formula.cont <- as.formula(\n  paste(\"Length.of.Stay~ RHC.use +\", \n        paste(baselineVars.LoS,\n              collapse = \"+\")))\nfit1.cont <- lm(out.formula.cont, data = ObsData)\npublish(fit1.cont, digits=1)$regressionTable[2,]\n\n\n\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 5 showed that, after propensity score pair (1-to-1) matching, means of length of stay (\\(Y\\)), when stratified by RHC (\\(A\\)) were not significantly different (\\(p = 0.14\\)).\n\n\n\n\n\n\n\n\n method.list \n    Estimate \n    2.5 % \n    97.5 % \n  \n\n\n Adjusted Regression \n    3.04 \n    1.51 \n    4.58 \n  \n\n TMLE (user-specified SL library) \n    5.34 \n    3.90 \n    6.78 \n  \n\n TMLE (default SL library) \n    3.35 \n    1.43 \n    4.86 \n  \n\n Keele and Small (2021) paper \n    2.01 \n    0.60 \n    3.41 \n  \n\n\n\n\nDifferences in results can likely be attributed to the use of different SuperLearner libraries, the use of different combinations of variables used, or random sampling associated with the cross-validation used in the SuperLearner algorithm.\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. “Implementing TMLE in the Presence of a Continuous Outcome.” Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nGruber, Susan, and Mark J van der Laan. 2010. “A Targeted Maximum Likelihood Estimator of a Causal Effect on a Bounded Continuous Outcome.” The International Journal of Biostatistics 6 (1).\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023."
  },
  {
    "objectID": "machinelearningCausal5.html",
    "href": "machinelearningCausal5.html",
    "title": "Comparing results",
    "section": "",
    "text": "In this tutorial, we will use the RHC dataset in exploring the relationship between RHC (yes/no) and death (yes/no). We will use the following approaches:\n\nlogistic regression\npropensity score matching and weighting with logistic regression\npropensity score matching and weighting with Super Learner\nTMLE\n\nLoad packages\nWe load several R packages required for fitting the models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(Publish)\nlibrary(randomForest)\nlibrary(tmle)\nlibrary(xgboost)\nlibrary(kableExtra)\nlibrary(SuperLearner)\nlibrary(dbarts)\nlibrary(MatchIt)\nlibrary(cobalt)\nlibrary(survey)\nlibrary(knitr)\n\nLoad dataset\n\nload(file = \"Data/machinelearningCausal/cl2.RData\")\nls()\n#> [1] \"baselinevars\" \"ObsData\"      \"tab0\"\n\n\n# Data\ndat <- ObsData\nhead(dat)\n\n\n\n  \n\n\n\n# Data dimension\ndim(dat)\n#> [1] 5735   52\n\nConfounder list in the baselinevars vector is\n\n\n\n\nConfounders\n\n\n\nDisease.category\n\n\nCancer\n\n\nCardiovascular\n\n\nCongestive.HF\n\n\nDementia\n\n\nPsychiatric\n\n\nPulmonary\n\n\nRenal\n\n\nHepatic\n\n\nGI.Bleed\n\n\nTumor\n\n\nImmunosupperssion\n\n\nTransfer.hx\n\n\nMI\n\n\nage\n\n\nsex\n\n\nedu\n\n\nDASIndex\n\n\nAPACHE.score\n\n\nGlasgow.Coma.Score\n\n\nblood.pressure\n\n\nWBC\n\n\nHeart.rate\n\n\nRespiratory.rate\n\n\nTemperature\n\n\nPaO2vs.FIO2\n\n\nAlbumin\n\n\nHematocrit\n\n\nBilirubin\n\n\nCreatinine\n\n\nSodium\n\n\nPotassium\n\n\nPaCo2\n\n\nPH\n\n\nWeight\n\n\nDNR.status\n\n\nMedical.insurance\n\n\nRespiratory.Diag\n\n\nCardiovascular.Diag\n\n\nNeurological.Diag\n\n\nGastrointestinal.Diag\n\n\nRenal.Diag\n\n\nMetabolic.Diag\n\n\nHematologic.Diag\n\n\nSepsis.Diag\n\n\nTrauma.Diag\n\n\nOrthopedic.Diag\n\n\nrace\n\n\nincome\n\n\n\n\n\nLogistic regression\nLet us define the regression formula and fit logistic regression, adjusting for baseline confounders.\n\n# Formula\nFormula <- formula(paste(\"Death ~ RHC.use + \", \n                         paste(baselinevars, \n                               collapse = \" + \")))\n\n# Logistic regression\nfit.glm <- glm(Formula, \n               data = dat, \n               family = binomial)\n\nWe can use flextable package to view fit.glm, the regression output:\n\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.42\n1.23\n1.65\n\n\n\n\n\nAs we can see, the odds of death was 42% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with logistic regression\nNow we will use propensity score matching, where propensity score will be estimated using logistic regression. The first step is to define the propensity score formula and estimate the propensity scores.\nStep 1\n\n# Propensity score model define\nps.formula <- as.formula(paste0(\"RHC.use ~ \", \n                                paste(baselinevars, \n                                      collapse = \"+\")))\n\n# Propensity score model fitting\nfit.ps <- glm(ps.formula, \n              data = dat, \n              family = binomial)\n\n# Propensity scores\ndat$ps <- predict(fit.ps, \n                  type = \"response\")\nsummary(dat$ps)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.002478 0.161446 0.358300 0.380819 0.574319 0.968425\n\nStep 2\nThe second step is to match exposed (i.e., RHC users) to unexposed (i.e., RHC non-users) based on the propensity scores. We will use nearest neighborhood and caliper approach.\n\n\n\n\n\n\n\n# Caliper\ncaliper <- 0.2*sd(log(dat$ps/(1-dat$ps)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj <- matchit(ps.formula, \n                     data = dat,\n                     distance = dat$ps, \n                     method = \"nearest\",\n                     ratio = 1,\n                     caliper = caliper)\n\n\n# See how many matched\nmatch.obj \n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.068)\n#>  - number of obs.: 5735 (original), 3478 (matched)\n#>  - target estimand: ATT\n#>  - covariates: too many to name\n\n# Extract matched data\nmatched.data <- match.data(match.obj)\n\n# Overlap checking\nbal.plot(match.obj,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\nStep 3\nThe third step is to check the balancing on the matched data. We will compare the similarity of baseline characteristics between RHC users and non-users in the propensity score matched sample. Let’s consider SMD >0.1 as imbalanced.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1b <- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data, includeNA = T,\n                        addOverall = F, test = F)\n#print(tab1b, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1b) shows\n\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.06\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.03\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.01\n\n\nPsychiatric\n0.04\n\n\nPulmonary\n0.04\n\n\nRenal\n0.03\n\n\nHepatic\n0.01\n\n\nGI.Bleed\n0.00\n\n\nTumor\n0.01\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.03\n\n\nMI\n0.01\n\n\nage\n0.04\n\n\nsex\n0.02\n\n\nedu\n0.03\n\n\nDASIndex\n0.03\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.07\n\n\nWBC\n0.01\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.04\n\n\nTemperature\n0.02\n\n\nPaO2vs.FIO2\n0.06\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.03\n\n\nCreatinine\n0.03\n\n\nSodium\n0.02\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.04\n\n\nPH\n0.02\n\n\nWeight\n0.02\n\n\nDNR.status\n0.00\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.06\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.02\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.03\n\n\nTrauma.Diag\n0.02\n\n\nOrthopedic.Diag\n0.05\n\n\nrace\n0.02\n\n\nincome\n0.06\n\n\n\n\n\n\nAfter propensity score matching, all the confounders are balanced in terms of SMD. Now, we will fit the outcome model on the matched data.\nStep 4\n\nfit.psm <- glm(Death ~ RHC.use, \n               data = matched.data, \n               family = binomial)\n\nSummary of fit.psm:\n\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.29\n1.12\n1.48\n\n\n\n\n\nIn the propensity score matched data, the odds of death was 29% higher among those participants with RHC use than those with no RHC use.\nPropensity score weighting with logistic regression\nNow we will use the propensity score weighting approach where propensity scores are estimated using logistic regression.\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw <- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps, \n                            mean(1-RHC.use)/(1-ps)))\nsummary(dat$ipw)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.3932  0.6571  0.7781  0.9953  1.0624 24.1854\n\nStep 3\nNow, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design <- svydesign(id = ~1, weights = ~ipw, data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1e <- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design, includeNA = T, \n                           addOverall = F, test = F)\n#print(tab1e, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1e) shows\n\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.02\n\n\nCancer\n0.01\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.02\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.01\n\n\nMI\n0.00\n\n\nage\n0.05\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.00\n\n\nblood.pressure\n0.01\n\n\nWBC\n0.04\n\n\nHeart.rate\n0.02\n\n\nRespiratory.rate\n0.00\n\n\nTemperature\n0.01\n\n\nPaO2vs.FIO2\n0.00\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.01\n\n\nSodium\n0.01\n\n\nPotassium\n0.03\n\n\nPaCo2\n0.02\n\n\nPH\n0.01\n\n\nWeight\n0.02\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.01\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.01\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.00\n\n\nHematologic.Diag\n0.00\n\n\nSepsis.Diag\n0.01\n\n\nTrauma.Diag\n0.01\n\n\nOrthopedic.Diag\n0.01\n\n\nrace\n0.02\n\n\nincome\n0.02\n\n\n\n\n\n\nAll confounders are balanced (SMD < 0.1).\nStep 4\n\nfit.ipw <- svyglm(Death ~ RHC.use, \n                  design = w.design, \n                  family = binomial)\n\nSummary of fit.ipw:\n\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.30\n1.11\n1.53\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 30% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with super learner\nNow we will use the propensity score matching, where we will be using super learner to calculate the propensity scores. We use logistic, LASSO, and XGBoost as the candidate learners.\nStep 1\n\n\n\n\nset.seed(123)\nps.fit <- CV.SuperLearner(\n  Y = dat$RHC.use,\n  X = dat[,baselinevars], \n  family = \"binomial\",\n  SL.library = c(\"SL.glm\", \"SL.glmnet\", \"SL.xgboost\"), \n  verbose = FALSE,\n  V = 5, \n  method = \"method.NNLS\")\n\n\n# Propensity scores for all learners  \npredictions <- cbind(ps.fit$SL.predict, ps.fit$library.predict)\nhead(predictions)\n#>                SL.glm_All SL.glmnet_All SL.xgboost_All\n#> [1,] 0.3632635 0.39789364    0.39569431     0.32144672\n#> [2,] 0.5870492 0.64983590    0.61497078     0.54465985\n#> [3,] 0.5740246 0.66329951    0.65396650     0.46847290\n#> [4,] 0.2172501 0.33044244    0.34478278     0.02363573\n#> [5,] 0.6347749 0.45972157    0.43779434     0.86645132\n#> [6,] 0.0272532 0.03420477    0.03629319     0.01730520\n\n# Propensity scores for super learner\ndat$ps.sl <- predictions[,1]\nsummary(dat$ps.sl)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.001671 0.147762 0.337664 0.376282 0.587237 0.975584\n\nStep 2\nThe same as before, we will match exposed to unexposed based on their propensity scores.\n\n\n\n\n\n\n\n# Caliper\ncaliper <- 0.2*sd(log(dat$ps.sl/(1-dat$ps.sl)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj2 <- matchit(ps.formula, \n                      data = dat,\n                      distance = dat$ps.sl, \n                      method = \"nearest\",\n                      ratio = 1,\n                      caliper = caliper)\n\n\n# See how many matched\nmatch.obj2 \n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.075)\n#>  - number of obs.: 5735 (original), 3430 (matched)\n#>  - target estimand: ATT\n#>  - covariates: too many to name\n\n# Extract matched data\nmatched.data.sl <- match.data(match.obj2) \n\n# Overlap checking\nbal.plot(match.obj2,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\nStep 3\nNow we will check the balancing on the matched data.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj2, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1c <- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data.sl, includeNA = T, \n                        addOverall = T, test = F)\n#print(tab1c, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1c) shows\n\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.08\n\n\nCancer\n0.05\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.02\n\n\nDementia\n0.00\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.02\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.05\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.06\n\n\nMI\n0.03\n\n\nage\n0.06\n\n\nsex\n0.04\n\n\nedu\n0.03\n\n\nDASIndex\n0.01\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.00\n\n\nHeart.rate\n0.05\n\n\nRespiratory.rate\n0.03\n\n\nTemperature\n0.04\n\n\nPaO2vs.FIO2\n0.09\n\n\nAlbumin\n0.05\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.00\n\n\nCreatinine\n0.05\n\n\nSodium\n0.01\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.05\n\n\nPH\n0.06\n\n\nWeight\n0.06\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.08\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.02\n\n\nRenal.Diag\n0.00\n\n\nMetabolic.Diag\n0.02\n\n\nHematologic.Diag\n0.02\n\n\nSepsis.Diag\n0.04\n\n\nTrauma.Diag\n0.06\n\n\nOrthopedic.Diag\n0.06\n\n\nrace\n0.02\n\n\nincome\n0.04\n\n\n\n\n\n\nAgain, all confounders are balanced in terms of SMD (all SMDs < 0.1). Next, we will fit the outcome model.\nStep 4\n\nfit.psm.sl <- glm(Death ~ RHC.use, \n                  data = matched.data.sl, \n                  family = binomial)\n\nSummary of fit.psm.sl:\n\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.09\n1.45\n\n\n\n\n\nThe interpretation is the same as before. In the propensity score matched data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nPropensity score weighting with super learner\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw.sl <- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps.sl, \n                               mean(1-RHC.use)/(1-ps.sl)))\nsummary(dat$ipw.sl)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.3903  0.6496  0.7647  1.0191  1.0468 41.4490\n\nStep 3\nNext, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design.sl <- svydesign(id = ~1, weights = ~ipw.sl, \n                         data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1k <- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design.sl, includeNA = T, \n                           addOverall = T, test = F)\n#print(tab1k, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1k) shows\n\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.01\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.02\n\n\nCongestive.HF\n0.01\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.03\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.00\n\n\nMI\n0.01\n\n\nage\n0.07\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.02\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.06\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.01\n\n\nTemperature\n0.00\n\n\nPaO2vs.FIO2\n0.03\n\n\nAlbumin\n0.00\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.00\n\n\nSodium\n0.00\n\n\nPotassium\n0.02\n\n\nPaCo2\n0.05\n\n\nPH\n0.02\n\n\nWeight\n0.01\n\n\nDNR.status\n0.05\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.02\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.02\n\n\nGastrointestinal.Diag\n0.00\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.00\n\n\nTrauma.Diag\n0.05\n\n\nOrthopedic.Diag\n0.02\n\n\nrace\n0.05\n\n\nincome\n0.05\n\n\n\n\n\n\nAll confounders are balanced (SMD < 0.1).\nStep 4\n\nfit.ipw.sl <- svyglm(Death ~ RHC.use, \n                     design = w.design.sl, \n                     family = binomial)\n\nSummary of fit.ipw.sl:\n\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.04\n1.52\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nTMLE\nFrom the previous tutorial, we calculated the effective sample size for the outcome model as well as for the exposure model:\n\nn <- nrow(dat) \npY <- nrow(dat[dat$Death == 1,])/n \nn_eff <- min(n, 5*(n*min(pY, 1 - pY))) \nn_eff\n#> [1] 5735\n\n\nn <- nrow(dat) \npA <- nrow(dat[dat$RHC.use == 1,])/n \nn_eff <- min(n, 5*(n*min(pA, 1 - pA))) \nn_eff\n#> [1] 5735\n\nSince the effective sample size is \\(\\ge 5,000\\) for both model, we can consider \\(5 \\le \\text{V} \\le 10\\), where V is the number of folds. Let’s work with 10-fold cross-validation for both the exposure and the outcome model, with the default super learner library.\n\n\n\n\n\n\n\nfit.tmle <- tmle(Y = dat$Death, \n                 A = dat$RHC.use, \n                 W = dat[,baselinevars], \n                 family = \"binomial\", \n                 V.Q = 10, \n                 V.g = 10)\n\n\n# OR\nround(fit.tmle$estimates$OR$psi, 2)\n#> [1] 1.26\n\n# 95% CI\nround(fit.tmle$estimates$OR$CI, 2)\n#> [1] 1.12 1.42\n\nAs we can see that the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nResults comparison\n\n\n\n\n\n Model \n    OR \n    95% CI \n  \n\n\n Logistic regression \n    1.42 \n    1.32-1.65 \n  \n\n Propensity score matching with logistic \n    1.29 \n    1.12-1.48 \n  \n\n Propensity score weighting with logistic \n    1.30 \n    1.11-1.53 \n  \n\n Propensity score matching with super learner (logistic, LASSO, and XGBoost) \n    1.26 \n    1.09-1.45 \n  \n\n Propensity score weighting with super learner (logistic, LASSO, and XGBoost) \n    1.26 \n    1.04-1.52 \n  \n\n TMLE (default SL library) \n    1.26 \n    1.12-1.42 \n  \n\n\n\n\n\n\n\n\nForest Plot of Odds Ratios"
  },
  {
    "objectID": "machinelearningCausalQ.html#live-quiz",
    "href": "machinelearningCausalQ.html#live-quiz",
    "title": "Quiz (C)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "machinelearningCausalQ.html#download-quiz",
    "href": "machinelearningCausalQ.html#download-quiz",
    "title": "Quiz (C)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R functions (C)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\n Function.name \n    Package.name \n    Use \n  \n\n\n publish \n    Publish \n    To publish regression models \n  \n\n median \n    stats \n    To calculate the median of a numeric vector \n  \n\n SL.mean \n    SuperLearner \n    SuperLearner wrapper for the mean learner \n  \n\n SL.glm \n    SuperLearner \n    SuperLearner wrapper for the generalized linear model learner \n  \n\n SL.glmnet \n    SuperLearner \n    SuperLearner wrapper for the generalized linear model with elastic net penalty learner \n  \n\n SL.xgboost \n    SuperLearner \n    SuperLearner wrapper for the extreme gradient boosting learner \n  \n\n SL.randomForest \n    SuperLearner \n    SuperLearner wrapper for the random forest learner \n  \n\n SL.svm \n    SuperLearner \n    SuperLearner wrapper for the support vector machine (SVM) learner \n  \n\n CreateTableOne \n    tableone \n    To create a summary table for a dataset \n  \n\n tmle \n    tmle \n    To run Targeted Maximum Likelihood Estimation (TMLE) for causal inference \n  \n\n tmle.SL.dbarts2 \n    tmle \n    SuperLearner wrapper for the Bayesian Additive Regression Trees (BART) learner"
  },
  {
    "objectID": "nonbinary.html#background",
    "href": "nonbinary.html#background",
    "title": "Complex outcomes",
    "section": "Background",
    "text": "Background\nThis chapter offers tutorials on handling non-binary outcomes in data analysis. The first tutorial, on Polytomous and Ordinal outcomes, delves into multinomial and ordinal logistic regressions, teaching methods to analyze datasets with more than two categorical outcomes and how to assess the fit of ordinal logistic models. The second tutorial introduces Survival Analysis, guiding readers through the components of survival data, right censoring, the Kaplan-Meier method, the Cox regression model, and the complexities of analyzing time-dependent covariates and survey data. Lastly, the tutorial on Poisson and Negative Binomial focuses on regressions related to count data, showcasing both regular and survey-weighted methods, and highlights statistical techniques to understand factors influencing certain variables.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "nonbinary.html#overview-of-tutorials",
    "href": "nonbinary.html#overview-of-tutorials",
    "title": "Complex outcomes",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily focused on binary or continuous outcomes. In this chapter, we will discuss about categorical [outcome variable is categorical with more than two categories (i.e., multicategory)], survival [time-to-event outcomes and aims to model the time until an event happens, accounting for censored data (observations where the event has not occurred by the end of the study)] and count data [non-negative integers (i.e., 0, 1, 2, 3, …)] outcomes.\n\nPolytomous and ordinal\nThe tutorial covers methods for analyzing polytomous and ordinal outcomes in R. Initially, it introduces a function to exclude invalid responses from datasets. The data is loaded, and its structure is checked. The tutorial delves into the multinomial logistic regression, where tables are created to explore data characteristics. Two models are fitted: one for unweighted data and another for survey-weighted data. It then transitions into ordinal regression. Here, outcomes are ordered, and ordinal logistic models are fitted, again for both unweighted and survey-weighted data. The final part of the tutorial assesses the fit of the ordinal logistic model using various variables.\n\n\nSurvival analysis\nThe tutorial provides an in-depth understanding of survival analysis. It starts by introducing the lung dataset which contains various attributes related to patients’ health. The concept of censoring, specifically right censoring, is detailed, which happens when a subject leaves the study without experiencing the event of interest. The tutorial delves into the components of survival data, introducing the event indicator, survival function, and survival probability. It then transitions to practical applications, demonstrating how to create survival objects, estimate survival curves using the Kaplan-Meier method, and visualize these curves. There’s also an emphasis on understanding and estimating median survival time and comparing survival times between different groups using the log-rank test. The Cox regression model, which is pivotal in survival analysis, is elaborated, along with the idea of hazard ratios and assessing proportional hazards. Time-dependent covariates, which account for variables that may change over time, are also touched upon. Towards the end, the tutorial addresses how to perform survival analysis on complex survey data, covering design creation, Kaplan-Meier plotting, and the Cox Proportional Hazards model in the context of these surveys.\n\n\nSurvival analysis in NHANES\nThe tutorial demonstrates an analysis of NHANES data linked with mortality data, and conducts a survival analysis.\n\n\nPoisson and negative binomial\nThe tutorial provides a comprehensive guide on statistical analysis techniques related to Poisson and negative binomial regressions. Initially, it prepares the data by converting them into appropriate formats. A weighted summary of the dataset is then generated, emphasizing the distribution of a variable. The tutorial proceeds to demonstrate both regular and survey-weighted Poisson regressions, focusing on the relationship between fruit consumption and various factors. Finally, it explores negative binomial regressions and their survey-weighted counterparts.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "nonbinary0.html#complex-outcomes-beyond-binary",
    "href": "nonbinary0.html#complex-outcomes-beyond-binary",
    "title": "Concepts (N)",
    "section": "Complex outcomes beyond binary",
    "text": "Complex outcomes beyond binary\nIn this section, we explore regressions for outcomes beyond binary:\n\nPolytomous regression\nSurvival analysis\nPoisson regression"
  },
  {
    "objectID": "nonbinary0.html#reading-list",
    "href": "nonbinary0.html#reading-list",
    "title": "Concepts (N)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Hosmer, Lemeshow, and Sturdivant 2013; Kleinbaum and Klein 2011; Coxe, West, and Aiken 2009)"
  },
  {
    "objectID": "nonbinary0.html#video-lessons",
    "href": "nonbinary0.html#video-lessons",
    "title": "Concepts (N)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPolytomous regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson regression"
  },
  {
    "objectID": "nonbinary0.html#video-lesson-slides",
    "href": "nonbinary0.html#video-lesson-slides",
    "title": "Concepts (N)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "nonbinary0.html#links",
    "href": "nonbinary0.html#links",
    "title": "Concepts (N)",
    "section": "Links",
    "text": "Links\nPolytomous regression\n\nGoogle Slides\nPDF Slides\n\nSurvival analysis\n\nGoogle Slides\nPDF Slides\n\nPoisson regression\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "nonbinary0.html#references",
    "href": "nonbinary0.html#references",
    "title": "Concepts (N)",
    "section": "References",
    "text": "References\n\n\n\n\nCoxe, Stefany, Stephen G. West, and Leona S. Aiken. 2009. “The Analysis of Count Data: A Gentle Introduction to Poisson Regression and Its Alternatives.” Journal of Personality Assessment 91 (2): 121–36. https://doi.org/10.1080/00223890802634175.\n\n\nHosmer, Jr., David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression, 3rd Edition. Hoboken, NJ: John Wiley & Sons.\n\n\nKleinbaum, David G., and Mitchel Klein. 2011. Survival Analysis: A Self-Learning Text, Third Edition. New York, NY: Springer."
  },
  {
    "objectID": "nonbinary1.html",
    "href": "nonbinary1.html",
    "title": "Polytomous and ordinal",
    "section": "",
    "text": "Let us load the packages:\n\n# Load required packages\nrequire(Publish)\nrequire(survey)\nrequire(svyVGAM)\nrequire(car)\nlibrary(knitr)\n\nIn the code chunk below, we create a function called invalid.exclude. This function can be used to exclude invalid responses from the dataset, where don’t know, refusal, and not stated are considered invalid responses.\n\n# Function to exclude invalid responses\ninvalid.exclude <- function(Data, Var){\n  subset.data <- subset(Data, eval(parse(text = Var)) != \"DON'T KNOW\" & \n                          eval(parse(text = Var)) != \"REFUSAL\" & \n                          eval(parse(text = Var)) != \"NOT STATED\")\n  x1 <- dim(Data)[1]\n  x2 <- dim(subset.data)[1]\n  cat( format(x1-x2, big.mark = \",\"),\n       \"subjects deleted, and current N =\" , format(x2, big.mark = \",\") , \"\\n\")\n  return(subset.data)\n}\n\nData\n\n# Data\nanalytic <- readRDS(\"Data/nonbinary/cmh.Rds\")\nstr(analytic)\n#> 'data.frame':    2628 obs. of  9 variables:\n#>  $ MHcondition       : Factor w/ 3 levels \"Good\",\"Poor or Fair\",..: 2 2 3 3 3 2 3 1 3 3 ...\n#>  $ CommunityBelonging: Factor w/ 4 levels \"SOMEWHAT STRONG\",..: 1 1 3 3 3 2 1 1 3 1 ...\n#>  $ Age               : Factor w/ 6 levels \"15 TO 24 YEARS\",..: 4 1 1 4 3 5 2 5 3 1 ...\n#>  $ Sex               : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 2 2 2 2 2 1 1 2 ...\n#>  $ RaceEthnicity     : Factor w/ 2 levels \"NON-WHITE\",\"WHITE\": 2 1 2 2 1 2 2 2 2 1 ...\n#>  $ MainIncome        : Factor w/ 5 levels \"EI/WORKER'S COMP\",..: 2 2 2 2 2 5 2 2 2 2 ...\n#>  $ ReceivedHelp      : Factor w/ 4 levels \"DON'T KNOW\",\"NO\",..: 4 4 4 2 2 4 2 2 4 2 ...\n#>  $ Weight            : num  678 1298 196 917 2384 ...\n#>  $ Disorder          : Factor w/ 1 level \"YES\": 1 1 1 1 1 1 1 1 1 1 ...\n\nLet us drop invalid responses\n\n# Drop invalid responses\nanalytic <- invalid.exclude(analytic, Var = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \n                                              \"MainIncome\", \"MHcondition\"))\n#> 0 subjects deleted, and current N = 2,628\n\nMultinomial logistic\nUnweighted Tables\nLet us see the summary statistic of the variables, stratified by mental health condition:\n\nrequire(\"tableone\")\ntab1 <- CreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"),\n                       strata = \"MHcondition\", data = analytic, test = F)\nprint(tab1, showAllLevels = TRUE, smd = T)\n#>                         Stratified by MHcondition\n#>                          level             Good        Poor or Fair\n#>   n                                        885         1002        \n#>   CommunityBelonging (%) SOMEWHAT STRONG   362 (40.9)   288 (28.7) \n#>                          SOMEWHAT WEAK     309 (34.9)   358 (35.7) \n#>                          VERY STRONG        96 (10.8)    74 ( 7.4) \n#>                          VERY WEAK         118 (13.3)   282 (28.1) \n#>   Age (%)                15 TO 24 YEARS    264 (29.8)   191 (19.1) \n#>                          25 TO 34 YEARS    167 (18.9)   141 (14.1) \n#>                          35 TO 44 YEARS    119 (13.4)   185 (18.5) \n#>                          35 TO 54 YEARS    139 (15.7)   220 (22.0) \n#>                          55 TO 64 YEARS    113 (12.8)   198 (19.8) \n#>                          65 years or older  83 ( 9.4)    67 ( 6.7) \n#>   Sex (%)                FEMALE            487 (55.0)   616 (61.5) \n#>                          MALE              398 (45.0)   386 (38.5) \n#>   RaceEthnicity (%)      NON-WHITE         140 (15.8)   184 (18.4) \n#>                          WHITE             745 (84.2)   818 (81.6) \n#>   MainIncome (%)         EI/WORKER'S COMP   78 ( 8.8)   195 (19.5) \n#>                          EMPLOYMENT INC.   641 (72.4)   560 (55.9) \n#>                          NOT STATED         23 ( 2.6)    25 ( 2.5) \n#>                          OTHER              36 ( 4.1)    66 ( 6.6) \n#>                          SENIOR BENEFITS   107 (12.1)   156 (15.6) \n#>                         Stratified by MHcondition\n#>                          Very good/excellent SMD   \n#>   n                      741                       \n#>   CommunityBelonging (%) 355 (47.9)           0.425\n#>                          190 (25.6)                \n#>                          116 (15.7)                \n#>                           80 (10.8)                \n#>   Age (%)                285 (38.5)           0.420\n#>                          167 (22.5)                \n#>                           89 (12.0)                \n#>                           79 (10.7)                \n#>                           68 ( 9.2)                \n#>                           53 ( 7.2)                \n#>   Sex (%)                304 (41.0)           0.277\n#>                          437 (59.0)                \n#>   RaceEthnicity (%)      134 (18.1)           0.045\n#>                          607 (81.9)                \n#>   MainIncome (%)          36 ( 4.9)           0.400\n#>                          598 (80.7)                \n#>                            9 ( 1.2)                \n#>                           22 ( 3.0)                \n#>                           76 (10.3)\n\nMultinomial Model fitting\nBefore fitting the multinomial regression, let us redefine the reference categories of the variables.\n\nanalytic$MHcondition2 <- relevel(analytic$MHcondition, ref = \"Poor or Fair\")\nanalytic$CommunityBelonging2 <- relevel(analytic$CommunityBelonging, ref = \"VERY WEAK\")\nanalytic$Age2 <- relevel(analytic$Age, ref = \"65 years or older\")\nanalytic$Sex2 <- relevel(analytic$Sex, ref = \"FEMALE\")\nanalytic$RaceEthnicity2 <- relevel(analytic$RaceEthnicity, ref = \"NON-WHITE\")\n\nNow we will fit the multinomial logistic regression model using the multinom function from the nnet package:\n\nrequire(nnet)\nfit4 <- multinom(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                    data = analytic)\n#> # weights:  48 (30 variable)\n#> initial  value 2887.153095 \n#> iter  10 value 2640.386436\n#> iter  20 value 2625.127331\n#> iter  30 value 2622.465409\n#> final  value 2622.328038 \n#> converged\nkable(round(exp(cbind(coef(fit4), confint(fit4))),2))\n#> Warning in cbind(coef(fit4), confint(fit4)): number of rows of result is not a\n#> multiple of vector length (arg 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nCommunityBelonging2SOMEWHAT STRONG\nCommunityBelonging2SOMEWHAT WEAK\nCommunityBelonging2VERY STRONG\nSex2MALE\nAge215 TO 24 YEARS\nAge225 TO 34 YEARS\nAge235 TO 44 YEARS\nAge235 TO 54 YEARS\nAge255 TO 64 YEARS\nRaceEthnicity2WHITE\nMainIncomeEMPLOYMENT INC.\nMainIncomeNOT STATED\nMainIncomeOTHER\nMainIncomeSENIOR BENEFITS\n\n\n\n\nGood\n0.38\n2.72\n1.84\n3.08\n1.29\n0.70\n0.62\n0.32\n0.37\n0.36\n1.25\n2.29\n1.57\n1.14\n1.11\n0.21\n\n\nVery good/excellent\n0.09\n3.92\n1.64\n5.65\n2.19\n1.27\n1.08\n0.39\n0.37\n0.37\n1.15\n3.89\n1.25\n1.34\n2.11\n2.07\n\n\n\n\n\nMultinomial logistic for complex survey\nSurvey-weighted Tables\nNow, we will set up the survey design with the survey weights and then see design-adjusted summary statistics.\n\nrequire(survey)\n# Design\nsvy.analytic <- svydesign(ids = ~ 1, weights = ~ Weight, data = analytic)\n\n# Table 1\ntab1a <- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                  data = svy.analytic)\nprint(tab1a, showAllLevels = TRUE)\n#>                         \n#>                          level             Overall          \n#>   n                                        2734564.0        \n#>   CommunityBelonging (%) SOMEWHAT STRONG   1015381.0 (37.1) \n#>                          SOMEWHAT WEAK      948838.7 (34.7) \n#>                          VERY STRONG        297141.6 (10.9) \n#>                          VERY WEAK          473202.7 (17.3) \n#>   Age (%)                15 TO 24 YEARS     795568.6 (29.1) \n#>                          25 TO 34 YEARS     564720.6 (20.7) \n#>                          35 TO 44 YEARS     457821.1 (16.7) \n#>                          35 TO 54 YEARS     447324.8 (16.4) \n#>                          55 TO 64 YEARS     319374.3 (11.7) \n#>                          65 years or older  149754.6 ( 5.5) \n#>   Sex (%)                FEMALE            1312605.0 (48.0) \n#>                          MALE              1421958.9 (52.0) \n#>   RaceEthnicity (%)      NON-WHITE          565784.5 (20.7) \n#>                          WHITE             2168779.4 (79.3) \n#>   MainIncome (%)         EI/WORKER'S COMP   245758.5 ( 9.0) \n#>                          EMPLOYMENT INC.   2059689.1 (75.3) \n#>                          NOT STATED          60101.5 ( 2.2) \n#>                          OTHER              116260.6 ( 4.3) \n#>                          SENIOR BENEFITS    252754.4 ( 9.2)\n\n# table 1 stratified by MHcondition\ntab1b <- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                           strata = \"MHcondition\", data = svy.analytic)\nprint(tab1b, showAllLevels = TRUE)\n#>                         Stratified by MHcondition\n#>                          level             Good             Poor or Fair    \n#>   n                                        949208.1         973112.1        \n#>   CommunityBelonging (%) SOMEWHAT STRONG   346216.2 (36.5)  273163.9 (28.1) \n#>                          SOMEWHAT WEAK     378431.4 (39.9)  365731.0 (37.6) \n#>                          VERY STRONG        97528.8 (10.3)   69438.0 ( 7.1) \n#>                          VERY WEAK         127031.7 (13.4)  264779.1 (27.2) \n#>   Age (%)                15 TO 24 YEARS    286103.0 (30.1)  187610.9 (19.3) \n#>                          25 TO 34 YEARS    189084.8 (19.9)  184528.6 (19.0) \n#>                          35 TO 44 YEARS    140796.4 (14.8)  199172.7 (20.5) \n#>                          35 TO 54 YEARS    171968.1 (18.1)  194689.1 (20.0) \n#>                          55 TO 64 YEARS    109114.5 (11.5)  148432.2 (15.3) \n#>                          65 years or older  52141.3 ( 5.5)   58678.6 ( 6.0) \n#>   Sex (%)                FEMALE            436830.4 (46.0)  582311.1 (59.8) \n#>                          MALE              512377.7 (54.0)  390801.0 (40.2) \n#>   RaceEthnicity (%)      NON-WHITE         167887.9 (17.7)  220525.5 (22.7) \n#>                          WHITE             781320.2 (82.3)  752586.6 (77.3) \n#>   MainIncome (%)         EI/WORKER'S COMP   66003.9 ( 7.0)  151095.4 (15.5) \n#>                          EMPLOYMENT INC.   752208.0 (79.2)  608703.4 (62.6) \n#>                          NOT STATED         23589.0 ( 2.5)   28371.0 ( 2.9) \n#>                          OTHER              32247.9 ( 3.4)   68453.1 ( 7.0) \n#>                          SENIOR BENEFITS    75159.3 ( 7.9)  116489.2 (12.0) \n#>                         Stratified by MHcondition\n#>                          Very good/excellent p      test\n#>   n                      812243.7                       \n#>   CommunityBelonging (%) 396000.8 (48.8)     <0.001     \n#>                          204676.3 (25.2)                \n#>                          130174.7 (16.0)                \n#>                           81391.9 (10.0)                \n#>   Age (%)                321854.6 (39.6)     <0.001     \n#>                          191107.2 (23.5)                \n#>                          117852.1 (14.5)                \n#>                           80667.6 ( 9.9)                \n#>                           61827.6 ( 7.6)                \n#>                           38934.6 ( 4.8)                \n#>   Sex (%)                293463.5 (36.1)     <0.001     \n#>                          518780.2 (63.9)                \n#>   RaceEthnicity (%)      177371.1 (21.8)      0.219     \n#>                          634872.7 (78.2)                \n#>   MainIncome (%)          28659.2 ( 3.5)     <0.001     \n#>                          698777.7 (86.0)                \n#>                            8141.5 ( 1.0)                \n#>                           15559.6 ( 1.9)                \n#>                           61105.8 ( 7.5)\n\nSetting up the design\nLet us redefine the reference categories within the survey design and convert the design to use replicate weights.\n\nw.design <- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design <- update(w.design , MHcondition2 = relevel(MHcondition, ref = \"Poor or Fair\"),\n                  CommunityBelonging2 = relevel(CommunityBelonging, ref = \"VERY WEAK\"),\n                  Age2 = relevel(Age, ref = \"65 years or older\"),\n                  Sex2 = relevel(Sex, ref = \"FEMALE\"),\n                  RaceEthnicity2 = relevel(RaceEthnicity, ref = \"NON-WHITE\"))\n\n# Convert a survey design to use replicate weights\nw.design2 <- as.svrepdesign(w.design, type = \"bootstrap\" , replicates = 50)\n\nMultinomial Model fitting\nNow, we will fit the design-adjusted multinomial logistic regression:\n\nrequire(svyVGAM)\nfit5 <- svy_vglm(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome,\n                    design = w.design2, family = multinomial)\n\n\nkable(round(exp(cbind(coef(fit5), confint(fit5))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n(Intercept):1\n16.57\n7.53\n36.46\n\n\n(Intercept):2\n3.24\n1.21\n8.69\n\n\nCommunityBelonging2SOMEWHAT STRONG:1\n0.22\n0.13\n0.37\n\n\nCommunityBelonging2SOMEWHAT STRONG:2\n0.56\n0.37\n0.85\n\n\nCommunityBelonging2SOMEWHAT WEAK:1\n0.58\n0.35\n0.95\n\n\nCommunityBelonging2SOMEWHAT WEAK:2\n1.18\n0.73\n1.90\n\n\nCommunityBelonging2VERY STRONG:1\n0.15\n0.08\n0.29\n\n\nCommunityBelonging2VERY STRONG:2\n0.46\n0.25\n0.84\n\n\nSex2MALE:1\n0.39\n0.29\n0.51\n\n\nSex2MALE:2\n0.68\n0.50\n0.93\n\n\nAge215 TO 24 YEARS:1\n0.56\n0.28\n1.12\n\n\nAge215 TO 24 YEARS:2\n0.65\n0.37\n1.12\n\n\nAge225 TO 34 YEARS:1\n0.83\n0.39\n1.76\n\n\nAge225 TO 34 YEARS:2\n0.68\n0.37\n1.28\n\n\nAge235 TO 44 YEARS:1\n1.74\n0.77\n3.90\n\n\nAge235 TO 44 YEARS:2\n0.91\n0.48\n1.73\n\n\nAge235 TO 54 YEARS:1\n2.02\n0.96\n4.23\n\n\nAge235 TO 54 YEARS:2\n1.48\n0.72\n3.01\n\n\nAge255 TO 64 YEARS:1\n1.91\n0.83\n4.41\n\n\nAge255 TO 64 YEARS:2\n1.33\n0.67\n2.64\n\n\nRaceEthnicity2WHITE:1\n0.91\n0.64\n1.29\n\n\nRaceEthnicity2WHITE:2\n1.22\n0.83\n1.81\n\n\nMainIncomeEMPLOYMENT INC.:1\n0.25\n0.14\n0.44\n\n\nMainIncomeEMPLOYMENT INC.:2\n0.58\n0.30\n1.15\n\n\nMainIncomeNOT STATED:1\n0.91\n0.23\n3.59\n\n\nMainIncomeNOT STATED:2\n1.37\n0.30\n6.25\n\n\nMainIncomeOTHER:1\n1.43\n0.47\n4.39\n\n\nMainIncomeOTHER:2\n1.30\n0.49\n3.47\n\n\nMainIncomeSENIOR BENEFITS:1\n0.39\n0.18\n0.83\n\n\nMainIncomeSENIOR BENEFITS:2\n0.49\n0.21\n1.13\n\n\n\n\n\nOrdinal Regression\nOrdering outcome\nLet’s define MHcondition as an ordinal outcome. We can do it by using ordered = TRUE in the factor function.\n\n# Ordinal outcome\nanalytic$MHcondition3 <- factor(analytic$MHcondition, \n                               levels = c(\"Poor or Fair\", \"Good\", \"Very good or excellent\"), \n                      ordered = TRUE)\n\nOrdinal logistic\nLet’s fit the ordinal logistic using the polr function:\n\nrequire(MASS)\nfit5o1 <- polr(MHcondition3 ~CommunityBelonging2 + \n                  Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                data=analytic)\nkable(round(exp(cbind(coef(fit5o1), confint(fit5o1))),2))\n#> Waiting for profiling to be done...\n#> \n#> Re-fitting to get Hessian\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.69\n2.05\n3.55\n\n\nCommunityBelonging2SOMEWHAT WEAK\n1.83\n1.40\n2.41\n\n\nCommunityBelonging2VERY STRONG\n3.15\n2.15\n4.66\n\n\nSex2MALE\n1.30\n1.07\n1.58\n\n\nAge215 TO 24 YEARS\n0.69\n0.42\n1.11\n\n\nAge225 TO 34 YEARS\n0.60\n0.36\n0.98\n\n\nAge235 TO 44 YEARS\n0.32\n0.19\n0.53\n\n\nAge235 TO 54 YEARS\n0.37\n0.23\n0.59\n\n\nAge255 TO 64 YEARS\n0.36\n0.22\n0.56\n\n\nRaceEthnicity2WHITE\n1.26\n0.97\n1.63\n\n\nMainIncomeEMPLOYMENT INC.\n2.26\n1.68\n3.06\n\n\nMainIncomeNOT STATED\n1.53\n0.79\n2.94\n\n\nMainIncomeOTHER\n1.15\n0.70\n1.90\n\n\nMainIncomeSENIOR BENEFITS\n1.10\n0.70\n1.70\n\n\n\n\n\nOrdinal logistic for complex survey\nThe same as before, we can set up the design, relevel variables within the design, define MHcondition as an ordinal variable, and then fit the design-adjusted ordinal logistic regression.\n\nw.design <- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design<-update(w.design , \n                  CommunityBelonging2=relevel(CommunityBelonging, ref=\"VERY WEAK\"),\n                  Age2=relevel(Age, ref=\"65 years or older\"),\n                  Sex2=relevel(Sex, ref=\"FEMALE\"),\n                  RaceEthnicity2=relevel(RaceEthnicity, ref=\"NON-WHITE\"),\n                  MHcondition3 = factor(MHcondition, levels=c(\"Poor or Fair\", \"Good\", \n                                                              \"Very good or excellent\"), \n                                        ordered=TRUE))\n\n# Design-adjusted Ordinal logistic\nfit5o <- svyolr(MHcondition3 ~CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                design=w.design)\nkable(round(exp(cbind(coef(fit5o), confint(fit5o))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.49\n1.65\n3.75\n\n\nCommunityBelonging2SOMEWHAT WEAK\n2.04\n1.37\n3.05\n\n\nCommunityBelonging2VERY STRONG\n3.02\n1.74\n5.26\n\n\nSex2MALE\n1.76\n1.30\n2.38\n\n\nAge215 TO 24 YEARS\n1.13\n0.52\n2.44\n\n\nAge225 TO 34 YEARS\n0.77\n0.34\n1.74\n\n\nAge235 TO 44 YEARS\n0.52\n0.24\n1.13\n\n\nAge235 TO 54 YEARS\n0.70\n0.32\n1.52\n\n\nAge255 TO 64 YEARS\n0.69\n0.32\n1.46\n\n\nRaceEthnicity2WHITE\n1.31\n0.90\n1.91\n\n\nMainIncomeEMPLOYMENT INC.\n2.37\n1.53\n3.67\n\n\nMainIncomeNOT STATED\n1.42\n0.52\n3.86\n\n\nMainIncomeOTHER\n0.88\n0.39\n2.00\n\n\nMainIncomeSENIOR BENEFITS\n1.26\n0.66\n2.41\n\n\nPoor or Fair|Good\n4.79\n1.92\n11.91\n\n\nGood|Very good or excellent\n2636603.90\n996905.04\n6973262.11\n\n\n\n\n\nAssessing model fit\nWe can use the regTermTest function from the survey package to do the Wald test of a regression coefficient.\n\nregTermTest(fit5o, ~CommunityBelonging2 , df = Inf) \n#> Wald test for CommunityBelonging2\n#>  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#>     MainIncome, design = w.design)\n#> Chisq =  24.53941  on  3  df: p= 1.9272e-05\nregTermTest(fit5o, ~Age2 , df = Inf) \n#> Wald test for Age2\n#>  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#>     MainIncome, design = w.design)\n#> Chisq =  14.00491  on  5  df: p= 0.015578\nregTermTest(fit5o, ~Sex2 , df = Inf)\n#> Wald test for Sex2\n#>  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#>     MainIncome, design = w.design)\n#> Chisq =  13.46032  on  1  df: p= 0.00024366\nregTermTest(fit5o, ~RaceEthnicity2 , df = Inf)\n#> Wald test for RaceEthnicity2\n#>  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#>     MainIncome, design = w.design)\n#> Chisq =  2.002794  on  1  df: p= 0.15701\nregTermTest(fit5o, ~MainIncome , df = Inf)\n#> Wald test for MainIncome\n#>  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#>     MainIncome, design = w.design)\n#> Chisq =  22.13656  on  4  df: p= 0.00018826\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "nonbinary2.html#data-and-variables",
    "href": "nonbinary2.html#data-and-variables",
    "title": "Survival analysis",
    "section": "Data and Variables",
    "text": "Data and Variables\n\nload(\"Data/nonbinary/nh_99-06.Rdata\") # \nanalytic.miss <- as.data.frame(MainTable[c(\"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\", \"PERMTH_INT\", \"PERMTH_EXM\", \"MORTSTAT\", \"female\", \"RIDAGEYR\", \"white\")])\n\n\nMORTSTAT: Final Mortality Status\n\n0 Assumed alive\n1 Assumed deceased\nBlank Ineligible for mortality follow-up or under age 17\n\n\nPERMTH_EXM: Person Months of Follow-up from MEC/Home Exam Date\n\n0 - 217\nBlank Ineligible\n\n\nPERMTH_INT: Person Months of Follow-up from Interview Date\n\nData issues\n\nwith(analytic.miss[1:30,], \n     Surv(PERMTH_EXM, MORTSTAT))\n#>  [1] NA? 90+ NA? NA? 74+ 86+ 76+ NA? NA? 79+ NA? 82+ 16  85+ 92+ 62  NA? NA? NA?\n#> [20] 86+ 87+ NA? NA? 72+ 84+ NA? 85+ 91+ 26  NA?\nsummary(analytic.miss$WTMEC2YR)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0    6365   15572   27245   38896  261361\n# avoiding 0 weight issues\nanalytic.miss$WTMEC2YR[analytic.miss$WTMEC2YR == 0] <- 0.001\nrequire(DataExplorer)\nplot_missing(analytic.miss)\n\n\n\n\nDesign creation\n\nanalytic.miss$ID <- 1:nrow(analytic.miss)\nanalytic.miss$miss <- 0\nanalytic.cc <- as.data.frame(na.omit(analytic.miss))\ndim(analytic.cc)\n#> [1] 10557    11\nanalytic.miss$miss[analytic.miss$ID %in% \n                     analytic.cc$ID] <- 0\nw.design0 <- svydesign(id=~SDMVPSU, \n                       strata=~SDMVSTRA, \n                       weights=~WTMEC2YR, \n                       nest=TRUE,\n                       data=analytic.miss)\nw.design <- subset(w.design0, \n                   miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0    6365   15572   27245   38896  261361\n\nSurvival Analysis within Complex Survey\nKM plot\n\nfit0 <- svykm(Surv(PERMTH_EXM, MORTSTAT) ~ 1, \n              design = w.design)\nplot(fit0)\n\n\n\n\nCox PH\n\nfit <- svycoxph(Surv(PERMTH_EXM, MORTSTAT) ~ \n                  white + female + RIDAGEYR, \n                design = w.design) \npublish(fit)\n#> Stratified 1 - level Cluster Sampling design (with replacement)\n#> With (117) clusters.\n#> subset(w.design0, miss == 0)\n#>  Variable Units HazardRatio       CI.95   p-value \n#>     white              0.78 [0.66;0.93]   0.00441 \n#>    female              0.61 [0.50;0.75]   < 0.001 \n#>  RIDAGEYR              1.09 [1.08;1.10]   < 0.001\n\nPH assumption\n\ntestPh <- cox.zph(fit) \nprint(testPh)\n#>             chisq df    p\n#> white    7.39e-05  1 0.99\n#> female   2.84e-07  1 1.00\n#> RIDAGEYR 9.16e-05  1 0.99\n#> GLOBAL   1.45e-04  3 1.00\nplot(testPh) \n\n\n\n\n\n\n\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "nonbinary2a.html",
    "href": "nonbinary2a.html",
    "title": "Survival analysis: NHANES",
    "section": "",
    "text": "The tutorial demonstrates analyzing complex survey data (NHANES) with mortality as a survival outcome. See the tutorial in the previous chapter on linking public-use US mortality data with the NHANES.\nWe will explore the relationship between caffeine consumption and mortality in adults with diabetes using NHANES 1999-2010 datasets. We will follow the following article by Neves et al. (2018).\nLoad required packages\n\n# Load required packages\nlibrary(tableone)\nlibrary(survival)\nlibrary(Publish)\nlibrary(survey)\nlibrary(DataExplorer)\n\nLoad datasets\nLet us load NHANES 1999-2000, 2001-2002, 2003-2004, 2005-2006, 2007-2008, and 2009-2010 datasets and merged all cycles.\n\n# Load\nload(\"Data/nonbinary/coffee.RData\")\nls()\n#> [1] \"dat.analytic1999\" \"dat.analytic2001\" \"dat.analytic2003\" \"dat.analytic2005\"\n#> [5] \"dat.analytic2007\" \"dat.analytic2009\" \"has_annotations\"\n\n# Merge all cycles\ndat.full <- rbind(dat.analytic1999, dat.analytic2001, dat.analytic2003, \n                  dat.analytic2005, dat.analytic2007, dat.analytic2009)\nhead(dat.full)\n\n\n\n  \n\n\ndim(dat.full)\n#> [1] 62160    26\n\nThe merged dataset contains 62,160 subjects with 26 relevant variables:\n\nid: Respondent sequence/ID number\nsurvey.weight: Full sample 2 year weights\npsu: Masked pseudo-PSU\nstrata: Masked pseudo-stratum\ncaff: Caffeine (exposure variable)\nstime: Follow-up time (time from interview date to death or censoring)\nstatus: Mortality status\nsex: Sex\nage: Age in years\nrace: Race/ethnicity\nincome: Annual household income\nsmoking: Smoking status\ndiabetic.nephropathy: Diabetic nephropathy (no data)\nbmi.cat: BMI - categorical\neducation: Education level\ncarbohyd: Carbohydrate in gm\nalcohol: Alcohol consumption\ndiab.years: Years since diabetes\nhtn: Hypertension\ndaib.retino: Diabetes retinopathy\nmacrovascular: Macrovascular complications\ninsulin: Insulin\nsurvey.cycle: Survey cycle\nphysical.activity: Physical activity\ndiabetes: Diabetes status\ncal.total: Total calories in kcal\nData pre-processing\nEligibility criteria\nThe authors considered adults aged 18 years or more, only diabetic, and total calories between 500 and 3500.\n\n# Total samples in the merged dataset\nnrow(dat.full) # N = 62,160\n#> [1] 62160\n\n# Age >= 18 years\ndat2 <- subset(dat.full, age >= 18) \nnrow(dat2) # N = 35,379\n#> [1] 35379\n\n# With diabetes\ndat3 <- subset(dat2, diabetes == \"Yes\") \nnrow(dat3) # N = 4,687 - numbers don't match with the paper (N = 4,544)\n#> [1] 4687\n\n# Implausible alimentary reports\ndat4 <- subset(dat3, cal.total >= 500 & cal.total <= 3500) \nnrow(dat4) # N = 4,083 - numbers don't match with the paper (N = 3,948)\n#> [1] 4083\n\nComplete case data\nLet us drop missing values in the exposure and outcome:\n\n# Drop missing exposure and outcome\ndat <- dat4[complete.cases(dat4$id),]\ndat <- dat[complete.cases(dat$caff),]\ndat <- dat[complete.cases(dat$status),]\ndat <- dat[complete.cases(dat$stime),] # N = 4,080\ndim(dat)\n#> [1] 4080   26\n\n# Missing plot\nplot_missing(dat)\n\n\n\n\nNow, let us drop variables with high missingness for this exercise. As explained in the Missing data analysis chapter, a better approach could be imputing missing values under the missing at random assumption.\n\n# Drop variables with high missingness\ndat$diabetic.nephropathy <- dat$diab.years <- dat$daib.retino <- dat$income <- NULL\n\n# Complete case data\ndat <- na.omit(dat)\ndim(dat) # N = 3,780\n#> [1] 3780   22\n\nTable 1\nNow, let us create Table 1 stratified by coffee consumption (exposure), separately for males and females, as done in the article.\n\nvars <- c(\"age\", \"race\", \"education\", \"smoking\", \"alcohol\", \"carbohyd\", \"physical.activity\",\n          \"bmi.cat\", \"htn\", \"macrovascular\", \"insulin\", \"survey.cycle\")\n\ntab1a <- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Female\",], \n                        test = F)\ntab1b <- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Male\",], \n                        test = F)\n\ntab1 <- list(Female = tab1a, Male = tab1b)\nprint(tab1, showAllLevels = T, smd = T)\n#> $Female\n#>                        Stratified by caff\n#>                         level                     No consumption <100 mg/day   \n#>   n                                                  203            916        \n#>   age (mean (SD))                                  59.33 (15.14)  62.92 (14.02)\n#>   race (%)              Non-Hispanic White            38 (18.7)     286 (31.2) \n#>                         Non-Hispanic Black            96 (47.3)     284 (31.0) \n#>                         Mexican American              47 (23.2)     228 (24.9) \n#>                         Other Hispanic                10 ( 4.9)      80 ( 8.7) \n#>                         Other race                    12 ( 5.9)      38 ( 4.1) \n#>   education (%)         Less than 9th grade           58 (28.6)     238 (26.0) \n#>                         9-11th grade                  48 (23.6)     182 (19.9) \n#>                         High school grade             43 (21.2)     224 (24.5) \n#>                         Some college                  39 (19.2)     209 (22.8) \n#>                         College graduate or above     15 ( 7.4)      63 ( 6.9) \n#>   smoking (%)           Never smoker                 143 (70.4)     598 (65.3) \n#>                         Current smoker                24 (11.8)     127 (13.9) \n#>                         Former smoker                 36 (17.7)     191 (20.9) \n#>   alcohol (%)           No consumption               192 (94.6)     826 (90.2) \n#>                         <20 grams/day                  4 ( 2.0)      69 ( 7.5) \n#>                         20+ grams/day                  7 ( 3.4)      21 ( 2.3) \n#>   carbohyd (mean (SD))                            176.17 (73.88) 190.64 (73.34)\n#>   physical.activity (%) Low                           95 (46.8)     393 (42.9) \n#>                         Intermediate                  59 (29.1)     309 (33.7) \n#>                         High                          49 (24.1)     214 (23.4) \n#>   bmi.cat (%)           <20.0                          4 ( 2.0)       6 ( 0.7) \n#>                         20.0 to <25.0                 24 (11.8)     107 (11.7) \n#>                         25.0 to <30.0                 35 (17.2)     254 (27.7) \n#>                         30.0 to <35.0                 45 (22.2)     263 (28.7) \n#>                         35.0 to <40.0                 47 (23.2)     138 (15.1) \n#>                         40.0+                         48 (23.6)     148 (16.2) \n#>   htn (%)               No                            58 (28.6)     273 (29.8) \n#>                         Yes                          145 (71.4)     643 (70.2) \n#>   macrovascular (%)     No                           166 (81.8)     758 (82.8) \n#>                         Yes                           37 (18.2)     158 (17.2) \n#>   insulin (%)           No                           156 (76.8)     755 (82.4) \n#>                         Yes                           47 (23.2)     161 (17.6) \n#>   survey.cycle (%)      1999-00                       45 (22.2)     111 (12.1) \n#>                         2001-02                       45 (22.2)      98 (10.7) \n#>                         2003-04                       26 (12.8)     127 (13.9) \n#>                         2005-06                       20 ( 9.9)     154 (16.8) \n#>                         2007-08                       37 (18.2)     198 (21.6) \n#>                         2009-10                       30 (14.8)     228 (24.9) \n#>                        Stratified by caff\n#>                         100-200 mg/day 200+ mg/day    SMD   \n#>   n                        424            332               \n#>   age (mean (SD))        62.04 (13.75)  60.19 (13.10)  0.150\n#>   race (%)                 154 (36.3)     189 (56.9)   0.527\n#>                            108 (25.5)      40 (12.0)        \n#>                            111 (26.2)      69 (20.8)        \n#>                             30 ( 7.1)      20 ( 6.0)        \n#>                             21 ( 5.0)      14 ( 4.2)        \n#>   education (%)             79 (18.6)      56 (16.9)   0.203\n#>                             89 (21.0)      76 (22.9)        \n#>                            109 (25.7)      81 (24.4)        \n#>                            109 (25.7)      89 (26.8)        \n#>                             38 ( 9.0)      30 ( 9.0)        \n#>   smoking (%)              238 (56.1)     136 (41.0)   0.377\n#>                             81 (19.1)     119 (35.8)        \n#>                            105 (24.8)      77 (23.2)        \n#>   alcohol (%)              370 (87.3)     283 (85.2)   0.211\n#>                             38 ( 9.0)      35 (10.5)        \n#>                             16 ( 3.8)      14 ( 4.2)        \n#>   carbohyd (mean (SD))  193.79 (72.18) 203.36 (76.05)  0.190\n#>   physical.activity (%)    179 (42.2)     148 (44.6)   0.093\n#>                            151 (35.6)     117 (35.2)        \n#>                             94 (22.2)      67 (20.2)        \n#>   bmi.cat (%)                7 ( 1.7)       4 ( 1.2)   0.250\n#>                             40 ( 9.4)      44 (13.3)        \n#>                            109 (25.7)      80 (24.1)        \n#>                            133 (31.4)      82 (24.7)        \n#>                             73 (17.2)      63 (19.0)        \n#>                             62 (14.6)      59 (17.8)        \n#>   htn (%)                  123 (29.0)     110 (33.1)   0.052\n#>                            301 (71.0)     222 (66.9)        \n#>   macrovascular (%)        352 (83.0)     266 (80.1)   0.042\n#>                             72 (17.0)      66 (19.9)        \n#>   insulin (%)              340 (80.2)     252 (75.9)   0.094\n#>                             84 (19.8)      80 (24.1)        \n#>   survey.cycle (%)          42 ( 9.9)      46 (13.9)   0.305\n#>                             53 (12.5)      48 (14.5)        \n#>                             63 (14.9)      46 (13.9)        \n#>                             59 (13.9)      43 (13.0)        \n#>                            111 (26.2)      82 (24.7)        \n#>                             96 (22.6)      67 (20.2)        \n#> \n#> $Male\n#>                        Stratified by caff\n#>                         level                     No consumption <100 mg/day   \n#>   n                                                  168            707        \n#>   age (mean (SD))                                  60.95 (12.68)  62.86 (12.88)\n#>   race (%)              Non-Hispanic White            35 (20.8)     238 (33.7) \n#>                         Non-Hispanic Black            78 (46.4)     209 (29.6) \n#>                         Mexican American              37 (22.0)     190 (26.9) \n#>                         Other Hispanic                13 ( 7.7)      48 ( 6.8) \n#>                         Other race                     5 ( 3.0)      22 ( 3.1) \n#>   education (%)         Less than 9th grade           50 (29.8)     194 (27.4) \n#>                         9-11th grade                  41 (24.4)     135 (19.1) \n#>                         High school grade             31 (18.5)     145 (20.5) \n#>                         Some college                  30 (17.9)     136 (19.2) \n#>                         College graduate or above     16 ( 9.5)      97 (13.7) \n#>   smoking (%)           Never smoker                  72 (42.9)     281 (39.7) \n#>                         Current smoker                34 (20.2)     155 (21.9) \n#>                         Former smoker                 62 (36.9)     271 (38.3) \n#>   alcohol (%)           No consumption               131 (78.0)     539 (76.2) \n#>                         <20 grams/day                 16 ( 9.5)      96 (13.6) \n#>                         20+ grams/day                 21 (12.5)      72 (10.2) \n#>   carbohyd (mean (SD))                            195.01 (85.93) 213.00 (80.40)\n#>   physical.activity (%) Low                           63 (37.5)     230 (32.5) \n#>                         Intermediate                  54 (32.1)     284 (40.2) \n#>                         High                          51 (30.4)     193 (27.3) \n#>   bmi.cat (%)           <20.0                          5 ( 3.0)       4 ( 0.6) \n#>                         20.0 to <25.0                 26 (15.5)      95 (13.4) \n#>                         25.0 to <30.0                 53 (31.5)     276 (39.0) \n#>                         30.0 to <35.0                 46 (27.4)     185 (26.2) \n#>                         35.0 to <40.0                 21 (12.5)      95 (13.4) \n#>                         40.0+                         17 (10.1)      52 ( 7.4) \n#>   htn (%)               No                            50 (29.8)     279 (39.5) \n#>                         Yes                          118 (70.2)     428 (60.5) \n#>   macrovascular (%)     No                           120 (71.4)     526 (74.4) \n#>                         Yes                           48 (28.6)     181 (25.6) \n#>   insulin (%)           No                           135 (80.4)     571 (80.8) \n#>                         Yes                           33 (19.6)     136 (19.2) \n#>   survey.cycle (%)      1999-00                       36 (21.4)      72 (10.2) \n#>                         2001-02                       28 (16.7)      92 (13.0) \n#>                         2003-04                       21 (12.5)     104 (14.7) \n#>                         2005-06                       18 (10.7)     115 (16.3) \n#>                         2007-08                       36 (21.4)     168 (23.8) \n#>                         2009-10                       29 (17.3)     156 (22.1) \n#>                        Stratified by caff\n#>                         100-200 mg/day 200+ mg/day    SMD   \n#>   n                        461            569               \n#>   age (mean (SD))        61.67 (13.66)  61.70 (12.40)  0.074\n#>   race (%)                 214 (46.4)     344 (60.5)   0.575\n#>                             92 (20.0)      67 (11.8)        \n#>                             90 (19.5)     115 (20.2)        \n#>                             54 (11.7)      26 ( 4.6)        \n#>                             11 ( 2.4)      17 ( 3.0)        \n#>   education (%)             98 (21.3)      97 (17.0)   0.269\n#>                             79 (17.1)      95 (16.7)        \n#>                            104 (22.6)     125 (22.0)        \n#>                            100 (21.7)     161 (28.3)        \n#>                             80 (17.4)      91 (16.0)        \n#>   smoking (%)              162 (35.1)     153 (26.9)   0.216\n#>                            109 (23.6)     196 (34.4)        \n#>                            190 (41.2)     220 (38.7)        \n#>   alcohol (%)              349 (75.7)     427 (75.0)   0.080\n#>                             63 (13.7)      80 (14.1)        \n#>                             49 (10.6)      62 (10.9)        \n#>   carbohyd (mean (SD))  225.58 (87.51) 237.26 (85.11)  0.273\n#>   physical.activity (%)    153 (33.2)     195 (34.3)   0.099\n#>                            177 (38.4)     199 (35.0)        \n#>                            131 (28.4)     175 (30.8)        \n#>   bmi.cat (%)                4 ( 0.9)       4 ( 0.7)   0.181\n#>                             61 (13.2)      80 (14.1)        \n#>                            169 (36.7)     187 (32.9)        \n#>                            132 (28.6)     146 (25.7)        \n#>                             64 (13.9)      95 (16.7)        \n#>                             31 ( 6.7)      57 (10.0)        \n#>   htn (%)                  190 (41.2)     234 (41.1)   0.126\n#>                            271 (58.8)     335 (58.9)        \n#>   macrovascular (%)        353 (76.6)     412 (72.4)   0.066\n#>                            108 (23.4)     157 (27.6)        \n#>   insulin (%)              375 (81.3)     449 (78.9)   0.032\n#>                             86 (18.7)     120 (21.1)        \n#>   survey.cycle (%)          48 (10.4)      64 (11.2)   0.284\n#>                             59 (12.8)      69 (12.1)        \n#>                             48 (10.4)     106 (18.6)        \n#>                             70 (15.2)      65 (11.4)        \n#>                            112 (24.3)     130 (22.8)        \n#>                            124 (26.9)     135 (23.7)\n\nSurvey design\nThe paper analyzed the data separately for males and females. Let us create the survey design:\n\n# Revised weight - weight divided by 6 cycles\ndat.full$svy.weight <- dat.full$survey.weight/6 \ndat$svy.weight <- dat$survey.weight/6 \nsummary(dat$svy.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   223.2  1760.2  3349.3  4555.9  5817.8 25347.4\n\n# Create an indicator variable\ndat.full$miss <- 1\ndat.full$miss[dat.full$id %in% dat$id] <- 0\n\n# Set up the design\nw.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~svy.weight, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design\nw.design1 <- subset(w.design0, miss == 0)\n\n# Subset the design for females\nw.design.f <- subset(w.design1, sex == \"Female\")\ndim(w.design.f)\n#> [1] 1875   28\n\n# Subset the design for males\nw.design.m <- subset(w.design1, sex == \"Male\")\ndim(w.design.m)\n#> [1] 1905   28\n\nKaplan-Meier plot\nLet us create the Kaplan-Meier plot for males and females:\n\n# KM for females\nfit0.f <- svykm(Surv(stime, status) ~ caff, design = w.design.f)\nplot(fit0.f)\n\n\n\n\n# KM for males\nfit0.m <- svykm(Surv(stime, status) ~ caff, design = w.design.m)\nplot(fit0.m)\n\n\n\n\nCox PH\nNow, we will fit the Cox proportional hazards model, separately for males and females. Let us run the unadjusted model first.\n\n# Unadjusted Cox PH for females\nfit1.f <- svycoxph(Surv(stime, status) ~ caff, design = w.design.f)\npublish(fit1.f)\n#> Stratified 1 - level Cluster Sampling design (with replacement)\n#> With (180) clusters.\n#> subset(w.design1, sex == \"Female\")\n#>  Variable          Units HazardRatio       CI.95   p-value \n#>      caff No consumption         Ref                       \n#>              <100 mg/day        0.80 [0.62;1.03]   0.08543 \n#>           100-200 mg/day        0.58 [0.42;0.80]   < 0.001 \n#>              200+ mg/day        0.61 [0.43;0.86]   0.00507\n\n# Unadjusted Cox PH for males\nfit1.m <- svycoxph(Surv(stime, status) ~ caff, design = w.design.m)\npublish(fit1.m)\n#> Stratified 1 - level Cluster Sampling design (with replacement)\n#> With (180) clusters.\n#> subset(w.design1, sex == \"Male\")\n#>  Variable          Units HazardRatio       CI.95 p-value \n#>      caff No consumption         Ref                     \n#>              <100 mg/day        1.20 [0.88;1.63]   0.246 \n#>           100-200 mg/day        1.10 [0.75;1.59]   0.632 \n#>              200+ mg/day        1.10 [0.76;1.58]   0.607\n\nNow, we will fit the Cox PH model, adjusting for covariates.\n\n# Covariate adjusted Cox PH for females\nfit2.f <- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.f)\npublish(fit2.f)\n#> Stratified 1 - level Cluster Sampling design (with replacement)\n#> With (180) clusters.\n#> subset(w.design1, sex == \"Female\")\n#>           Variable                     Units HazardRatio       CI.95   p-value \n#>               caff            No consumption         Ref                       \n#>                                  <100 mg/day        0.62 [0.46;0.84]   0.00166 \n#>                               100-200 mg/day        0.44 [0.30;0.64]   < 0.001 \n#>                                  200+ mg/day        0.43 [0.28;0.66]   < 0.001 \n#>                age                                  1.08 [1.07;1.09]   < 0.001 \n#>               race        Non-Hispanic White         Ref                       \n#>                           Non-Hispanic Black        0.81 [0.65;1.01]   0.06322 \n#>                             Mexican American        0.79 [0.61;1.03]   0.07976 \n#>                               Other Hispanic        0.65 [0.41;1.01]   0.05634 \n#>                                   Other race        0.64 [0.38;1.08]   0.09394 \n#>          education       Less than 9th grade         Ref                       \n#>                                 9-11th grade        1.15 [0.84;1.57]   0.37396 \n#>                            High school grade        1.03 [0.75;1.41]   0.84759 \n#>                                 Some college        1.06 [0.79;1.44]   0.69484 \n#>                    College graduate or above        0.61 [0.40;0.93]   0.02192 \n#>            smoking              Never smoker         Ref                       \n#>                               Current smoker        1.84 [1.42;2.38]   < 0.001 \n#>                                Former smoker        1.17 [0.95;1.46]   0.14379 \n#>            alcohol            No consumption         Ref                       \n#>                                <20 grams/day        0.94 [0.65;1.36]   0.75056 \n#>                                20+ grams/day        1.05 [0.58;1.93]   0.86786 \n#>           carbohyd                                  1.00 [1.00;1.00]   0.69680 \n#>  physical.activity                       Low         Ref                       \n#>                                 Intermediate        0.73 [0.57;0.92]   0.00834 \n#>                                         High        0.66 [0.49;0.89]   0.00638 \n#>            bmi.cat                     <20.0         Ref                       \n#>                                20.0 to <25.0        0.39 [0.21;0.73]   0.00301 \n#>                                25.0 to <30.0        0.33 [0.19;0.57]   < 0.001 \n#>                                30.0 to <35.0        0.30 [0.17;0.54]   < 0.001 \n#>                                35.0 to <40.0        0.26 [0.13;0.49]   < 0.001 \n#>                                        40.0+        0.31 [0.16;0.62]   < 0.001 \n#>                htn                        No         Ref                       \n#>                                          Yes        1.00 [0.80;1.24]   0.98084 \n#>      macrovascular                        No         Ref                       \n#>                                          Yes        1.85 [1.48;2.32]   < 0.001 \n#>            insulin                        No         Ref                       \n#>                                          Yes        1.87 [1.50;2.32]   < 0.001 \n#>       survey.cycle                   1999-00         Ref                       \n#>                                      2001-02        0.58 [0.43;0.78]   < 0.001 \n#>                                      2003-04        0.75 [0.54;1.04]   0.08384 \n#>                                      2005-06        0.81 [0.57;1.15]   0.24147 \n#>                                      2007-08        0.88 [0.60;1.28]   0.49860 \n#>                                      2009-10        0.76 [0.56;1.04]   0.08292\n\n# Covariate adjusted Cox PH for males\nfit2.m <- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.m)\npublish(fit2.m)\n#> Stratified 1 - level Cluster Sampling design (with replacement)\n#> With (180) clusters.\n#> subset(w.design1, sex == \"Male\")\n#>           Variable                     Units HazardRatio       CI.95   p-value \n#>               caff            No consumption         Ref                       \n#>                                  <100 mg/day        1.06 [0.75;1.49]   0.74199 \n#>                               100-200 mg/day        1.04 [0.69;1.57]   0.84938 \n#>                                  200+ mg/day        1.05 [0.71;1.54]   0.81249 \n#>                age                                  1.08 [1.06;1.09]   < 0.001 \n#>               race        Non-Hispanic White         Ref                       \n#>                           Non-Hispanic Black        0.75 [0.61;0.91]   0.00423 \n#>                             Mexican American        0.68 [0.52;0.88]   0.00348 \n#>                               Other Hispanic        0.83 [0.51;1.36]   0.46367 \n#>                                   Other race        1.03 [0.60;1.76]   0.91073 \n#>          education       Less than 9th grade         Ref                       \n#>                                 9-11th grade        1.47 [1.10;1.96]   0.00846 \n#>                            High school grade        1.03 [0.80;1.33]   0.81768 \n#>                                 Some college        1.02 [0.76;1.36]   0.91606 \n#>                    College graduate or above        0.68 [0.47;0.98]   0.03833 \n#>            smoking              Never smoker         Ref                       \n#>                               Current smoker        1.90 [1.38;2.62]   < 0.001 \n#>                                Former smoker        1.03 [0.85;1.26]   0.74101 \n#>            alcohol            No consumption         Ref                       \n#>                                <20 grams/day        0.95 [0.74;1.21]   0.67612 \n#>                                20+ grams/day        1.14 [0.83;1.58]   0.42116 \n#>           carbohyd                                  1.00 [1.00;1.00]   0.37461 \n#>  physical.activity                       Low         Ref                       \n#>                                 Intermediate        0.64 [0.52;0.80]   < 0.001 \n#>                                         High        0.69 [0.56;0.87]   0.00137 \n#>            bmi.cat                     <20.0         Ref                       \n#>                                20.0 to <25.0        0.25 [0.11;0.58]   0.00134 \n#>                                25.0 to <30.0        0.20 [0.09;0.48]   < 0.001 \n#>                                30.0 to <35.0        0.19 [0.08;0.45]   < 0.001 \n#>                                35.0 to <40.0        0.25 [0.10;0.64]   0.00380 \n#>                                        40.0+        0.26 [0.11;0.60]   0.00160 \n#>                htn                        No         Ref                       \n#>                                          Yes        1.18 [0.98;1.42]   0.08442 \n#>      macrovascular                        No         Ref                       \n#>                                          Yes        1.40 [1.17;1.66]   < 0.001 \n#>            insulin                        No         Ref                       \n#>                                          Yes        1.48 [1.21;1.81]   < 0.001 \n#>       survey.cycle                   1999-00         Ref                       \n#>                                      2001-02        1.37 [1.04;1.80]   0.02569 \n#>                                      2003-04        0.98 [0.68;1.43]   0.93245 \n#>                                      2005-06        1.07 [0.77;1.48]   0.67918 \n#>                                      2007-08        1.05 [0.78;1.41]   0.75981 \n#>                                      2009-10        0.83 [0.58;1.18]   0.29625\n\nThe adjusted results are approximately the same as in Table 2 of the article. Caffeine consumption was associated with mortality among women but not among men.\nPH assumption\nNow, we will check the proportional hazard assumption.\n\n# PH assumption among females\ncox.zph(fit2.f)\n#>                      chisq df    p\n#> caff              3.65e-03  3 1.00\n#> age               2.15e-06  1 1.00\n#> race              2.87e-03  4 1.00\n#> education         2.42e-03  4 1.00\n#> smoking           2.44e-03  2 1.00\n#> alcohol           1.14e-03  2 1.00\n#> carbohyd          7.38e-04  1 0.98\n#> physical.activity 3.34e-03  2 1.00\n#> bmi.cat           2.21e-03  5 1.00\n#> htn               1.33e-03  1 0.97\n#> macrovascular     4.24e-04  1 0.98\n#> insulin           2.26e-04  1 0.99\n#> survey.cycle      3.49e-03  5 1.00\n#> GLOBAL            2.55e-02 32 1.00\n\n# PH assumption among males\ncox.zph(fit2.m)\n#>                      chisq df    p\n#> caff              2.70e-03  3 1.00\n#> age               2.45e-03  1 0.96\n#> race              2.40e-03  4 1.00\n#> education         1.13e-03  4 1.00\n#> smoking           1.67e-03  2 1.00\n#> alcohol           1.50e-03  2 1.00\n#> carbohyd          1.33e-03  1 0.97\n#> physical.activity 7.73e-03  2 1.00\n#> bmi.cat           3.60e-03  5 1.00\n#> htn               4.45e-06  1 1.00\n#> macrovascular     4.32e-06  1 1.00\n#> insulin           9.24e-05  1 0.99\n#> survey.cycle      2.21e-03  5 1.00\n#> GLOBAL            2.78e-02 32 1.00\n\nThe large p-values indicate that the proportional hazard assumption was met for both models."
  },
  {
    "objectID": "nonbinary3.html",
    "href": "nonbinary3.html",
    "title": "Poisson",
    "section": "",
    "text": "In this tutorial, we will see how to use Poisson and negative binomial regression. In practice, we use Poisson regression to model a count outcome. Note that we assume the mean is equal to the variance in Poisson. When the variance is greater than what’s assumed by the model, overdispersion occurs. Poisson regression of overdispersed data leads to under-estimated or deflated standard errors, which leads to inflated test statistics and p-values. We can use negative binomial regression to model overdispersed data.\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(Publish)\nrequire(survey)\n\nData\n\n# Load\nload(\"Data/nonbinary/OAcvd.RData\")\n\n# Survey weight\nanalytic2$weight <- analytic2$weight/3\n\n# Make fruit.cont as a numeric variable\nanalytic2$fruit.cont <- as.numeric(as.character(analytic2$fruit.cont))\n\n# Make fruit.cont as a integer/count variable\nanalytic2$fruit.cont <- floor(analytic2$fruit.cont) # round\n\n# Factor variables using lapply\nvar.names <- c(\"age\", \"sex\", \"income\", \"race\", \"bmicat\", \"phyact\", \"smoke\",\n               \"fruit\", \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic2[var.names] <- lapply(analytic2[var.names] , factor)\nstr(analytic2)\n#> 'data.frame':    21623 obs. of  17 variables:\n#>  $ CVD       : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ age       : Factor w/ 4 levels \"20-39 years\",..: 2 3 4 1 3 1 3 1 3 3 ...\n#>  $ sex       : Factor w/ 2 levels \"Female\",\"Male\": 2 1 1 1 2 1 2 1 1 2 ...\n#>  $ income    : Factor w/ 4 levels \"$29,999 or less\",..: 3 4 1 2 1 2 2 3 3 4 ...\n#>  $ race      : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ bmicat    : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 1 2 2 2 1 2 1 2 2 ...\n#>  $ phyact    : Factor w/ 3 levels \"Active\",\"Inactive\",..: 3 1 1 2 1 3 2 3 2 3 ...\n#>  $ smoke     : Factor w/ 3 levels \"Current smoker\",..: 2 3 3 2 2 2 2 2 3 2 ...\n#>  $ fruit     : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 3 2 1 3 1 3 2 2 ...\n#>  $ painmed   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 1 1 ...\n#>  $ ht        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ copd      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ diab      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ edu       : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 2 2 1 4 4 4 1 4 4 1 ...\n#>  $ weight    : num  4.8 13.29 4.43 11.1 9.61 ...\n#>  $ OA        : Factor w/ 2 levels \"Control\",\"OA\": 2 1 2 1 1 1 2 1 1 1 ...\n#>  $ fruit.cont: num  3 4 8 3 2 10 1 8 5 3 ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:219757] 1 2 3 4 5 6 7 8 9 10 ...\n#>   ..- attr(*, \"names\")= chr [1:219757] \"3\" \"5\" \"7\" \"9\" ...\n\nSurvey weighted summary\n\n# Survey design\nw.design <- svydesign(id=~1, weights=~weight, data=analytic2)\n\n# Cross-tabulation\nxtabs(~fruit.cont, analytic2)\n#> fruit.cont\n#>    0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n#>  407 1628 3304 4261 3759 2887 1980 1288  809  480  303  174  109   80   51   25 \n#>   16   17   18   19   20   21   22   23   24   25   26   27   29   42 \n#>   21   14    9    4    5    4    2    6    7    1    2    1    1    1\nsvytable(~fruit.cont, w.design)\n#> fruit.cont\n#>            0            1            2            3            4            5 \n#>   9085.00222  39974.10444  88216.88222 119206.45778 108665.00556  82149.23111 \n#>            6            7            8            9           10           11 \n#>  53791.70444  35463.49111  22593.73000  13023.83556   8878.24778   4520.85000 \n#>           12           13           14           15           16           17 \n#>   3476.42667   2210.76000   1911.70556    973.93444    571.37667    414.44444 \n#>           18           19           20           21           22           23 \n#>    270.82556     77.76222    253.13000     24.74778     34.17444    185.70778 \n#>           24           25           26           27           29           42 \n#>    154.67778    109.88444    149.81222     89.61444     14.91111     93.76889\nsvyhist(~fruit.cont, w.design)\n\n\n\nsvyby(~fruit.cont, ~phyact, w.design, svymean, deff = TRUE)\n\n\n\n  \n\n\n\nPoisson regression\nLet us fit the traditional (not design-adjusted) Poisson regression using the glm function:\n\nrequire(jtools)\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Poisson regression - crude\nfit1 <- glm(fruit.cont ~phyact2, data=analytic2, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    21623 \n  \n\n Dependent variable \n    fruit.cont \n  \n\n Type \n    Generalized linear model \n  \n\n Family \n    poisson \n  \n\n Link \n    log \n  \n\n\n\n χ²(2) \n    1219.347 \n  \n\n Pseudo-R² (Cragg-Uhler) \n    0.055 \n  \n\n Pseudo-R² (McFadden) \n    0.012 \n  \n\n AIC \n    98570.197 \n  \n\n BIC \n    98594.142 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    z val. \n    p \n  \n\n\n (Intercept) \n    1.337 \n    1.328 \n    1.347 \n    267.387 \n    0.000 \n  \n\n phyact2Active \n    0.274 \n    0.259 \n    0.289 \n    34.983 \n    0.000 \n  \n\n phyact2Moderate \n    0.136 \n    0.120 \n    0.152 \n    16.741 \n    0.000 \n  \n\n\n Standard errors: MLE\n\n\n\n# Poisson regression - adjusted for covariates\nfit2 <- glm(fruit.cont ~ phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, data=analytic2, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\n Observations \n    21623 \n  \n\n Dependent variable \n    fruit.cont \n  \n\n Type \n    Generalized linear model \n  \n\n Family \n    poisson \n  \n\n Link \n    log \n  \n\n\n\n χ²(17) \n    2984.005 \n  \n\n Pseudo-R² (Cragg-Uhler) \n    0.130 \n  \n\n Pseudo-R² (McFadden) \n    0.030 \n  \n\n AIC \n    96835.540 \n  \n\n BIC \n    96979.207 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    z val. \n    p \n    VIF \n  \n\n\n (Intercept) \n    1.156 \n    1.123 \n    1.189 \n    68.773 \n    0.000 \n    NA \n  \n\n phyact2Active \n    0.250 \n    0.235 \n    0.266 \n    31.432 \n    0.000 \n    1.040 \n  \n\n phyact2Moderate \n    0.112 \n    0.095 \n    0.128 \n    13.640 \n    0.000 \n    1.040 \n  \n\n age40-49 years \n    0.027 \n    0.010 \n    0.043 \n    3.196 \n    0.001 \n    1.102 \n  \n\n age50-59 years \n    0.083 \n    0.066 \n    0.100 \n    9.400 \n    0.000 \n    1.102 \n  \n\n age60-64 years \n    0.127 \n    0.103 \n    0.151 \n    10.287 \n    0.000 \n    1.102 \n  \n\n sexMale \n    -0.173 \n    -0.187 \n    -0.160 \n    -25.108 \n    0.000 \n    1.082 \n  \n\n income$30,000-$49,999 \n    0.039 \n    0.017 \n    0.060 \n    3.557 \n    0.000 \n    1.144 \n  \n\n income$50,000-$79,999 \n    0.050 \n    0.030 \n    0.070 \n    4.923 \n    0.000 \n    1.144 \n  \n\n income$80,000 or more \n    0.083 \n    0.062 \n    0.103 \n    7.964 \n    0.000 \n    1.144 \n  \n\n raceWhite \n    -0.003 \n    -0.024 \n    0.018 \n    -0.240 \n    0.811 \n    1.077 \n  \n\n bmicatOverweight \n    -0.021 \n    -0.035 \n    -0.008 \n    -3.054 \n    0.002 \n    1.096 \n  \n\n bmicatUnderweight \n    -0.001 \n    -0.034 \n    0.033 \n    -0.030 \n    0.976 \n    1.096 \n  \n\n smokeFormer smoker \n    0.131 \n    0.114 \n    0.148 \n    15.050 \n    0.000 \n    1.132 \n  \n\n smokeNever smoker \n    0.181 \n    0.163 \n    0.199 \n    19.436 \n    0.000 \n    1.132 \n  \n\n edu2nd grad. \n    0.040 \n    0.016 \n    0.064 \n    3.291 \n    0.001 \n    1.125 \n  \n\n eduOther 2nd grad. \n    0.051 \n    0.020 \n    0.083 \n    3.201 \n    0.001 \n    1.125 \n  \n\n eduPost-2nd grad. \n    0.122 \n    0.101 \n    0.144 \n    11.219 \n    0.000 \n    1.125 \n  \n\n\n Standard errors: MLE\n\n\n\nSurvey weighted Poisson regression\nNow, let’s fit the design-adjusted Poisson regression using the svyglm function:\n\nrequire(jtools)\n# Design\nw.design <- update (w.design , phyact2=relevel(phyact, ref =\"Inactive\"))\n\n# Design-adjusted Poisson - crude\nfit1 <- svyglm(fruit.cont ~phyact2, design=w.design, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    21623 \n  \n\n Dependent variable \n    fruit.cont \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    poisson \n  \n\n Link \n    log \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.064 \n  \n\n Pseudo-R² (McFadden) \n    0.035 \n  \n\n AIC \n    99087.436 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.373 \n    1.354 \n    1.391 \n    146.410 \n    0.000 \n  \n\n phyact2Active \n    0.261 \n    0.231 \n    0.292 \n    16.877 \n    0.000 \n  \n\n phyact2Moderate \n    0.124 \n    0.095 \n    0.154 \n    8.230 \n    0.000 \n  \n\n\n Standard errors: Robust\n\n\n\n# Design-adjusted Poisson - adjusted for covariates\nfit2<-svyglm(fruit.cont ~phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, design=w.design, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\n Observations \n    21623 \n  \n\n Dependent variable \n    fruit.cont \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    poisson \n  \n\n Link \n    log \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.137 \n  \n\n Pseudo-R² (McFadden) \n    0.076 \n  \n\n AIC \n    97795.011 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n    VIF \n  \n\n\n (Intercept) \n    1.175 \n    1.113 \n    1.237 \n    37.230 \n    0.000 \n    NA \n  \n\n phyact2Active \n    0.241 \n    0.210 \n    0.272 \n    15.211 \n    0.000 \n    1.346 \n  \n\n phyact2Moderate \n    0.105 \n    0.074 \n    0.135 \n    6.775 \n    0.000 \n    1.346 \n  \n\n age40-49 years \n    0.041 \n    0.010 \n    0.071 \n    2.623 \n    0.009 \n    1.447 \n  \n\n age50-59 years \n    0.102 \n    0.071 \n    0.133 \n    6.402 \n    0.000 \n    1.447 \n  \n\n age60-64 years \n    0.149 \n    0.096 \n    0.203 \n    5.461 \n    0.000 \n    1.447 \n  \n\n sexMale \n    -0.136 \n    -0.161 \n    -0.111 \n    -10.500 \n    0.000 \n    1.148 \n  \n\n income$30,000-$49,999 \n    0.025 \n    -0.015 \n    0.066 \n    1.217 \n    0.224 \n    1.294 \n  \n\n income$50,000-$79,999 \n    0.031 \n    -0.008 \n    0.070 \n    1.559 \n    0.119 \n    1.294 \n  \n\n income$80,000 or more \n    0.057 \n    0.018 \n    0.096 \n    2.874 \n    0.004 \n    1.294 \n  \n\n raceWhite \n    0.023 \n    -0.012 \n    0.059 \n    1.289 \n    0.197 \n    1.193 \n  \n\n bmicatOverweight \n    -0.009 \n    -0.035 \n    0.017 \n    -0.708 \n    0.479 \n    1.331 \n  \n\n bmicatUnderweight \n    0.029 \n    -0.033 \n    0.091 \n    0.928 \n    0.353 \n    1.331 \n  \n\n smokeFormer smoker \n    0.094 \n    0.059 \n    0.128 \n    5.352 \n    0.000 \n    1.474 \n  \n\n smokeNever smoker \n    0.140 \n    0.102 \n    0.177 \n    7.290 \n    0.000 \n    1.474 \n  \n\n edu2nd grad. \n    0.026 \n    -0.020 \n    0.072 \n    1.093 \n    0.274 \n    1.302 \n  \n\n eduOther 2nd grad. \n    0.033 \n    -0.024 \n    0.090 \n    1.136 \n    0.256 \n    1.302 \n  \n\n eduPost-2nd grad. \n    0.129 \n    0.085 \n    0.173 \n    5.807 \n    0.000 \n    1.302 \n  \n\n\n Standard errors: Robust\n\n\n\nNegative binomial regression\nLet’s fit the negative binomial regression model using the glm function. Below, we specify the dispersion parameter (theta) is equal to 1, which suggests that the ratio of mean and variance is assumed to be 1.\n\nrequire(MASS)\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Negative binomial regression - crude\nfit3<- glm(fruit.cont ~ phyact2, data=analytic2, family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#> Waiting for profiling to be done...\n#>                      2.5 % 97.5 %\n#> (Intercept)     3.81  3.76   3.85\n#> phyact2Active   1.32  1.29   1.34\n#> phyact2Moderate 1.15  1.12   1.17\n\n# Negative binomial regression - adjusted for covariates\nfit4<-glm(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, data=analytic2,\n          family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#> Waiting for profiling to be done...\n#>                            2.5 % 97.5 %\n#> (Intercept)           3.17  3.05   3.30\n#> phyact2Active         1.29  1.26   1.31\n#> phyact2Moderate       1.12  1.10   1.14\n#> age40-49 years        1.03  1.01   1.05\n#> age50-59 years        1.08  1.06   1.11\n#> age60-64 years        1.14  1.10   1.17\n#> sexMale               0.84  0.83   0.86\n#> income$30,000-$49,999 1.05  1.02   1.07\n#> income$50,000-$79,999 1.06  1.03   1.08\n#> income$80,000 or more 1.09  1.07   1.12\n#> raceWhite             0.99  0.97   1.02\n#> bmicatOverweight      0.98  0.96   1.00\n#> bmicatUnderweight     0.99  0.95   1.04\n#> smokeFormer smoker    1.14  1.12   1.16\n#> smokeNever smoker     1.20  1.17   1.23\n#> edu2nd grad.          1.04  1.01   1.07\n#> eduOther 2nd grad.    1.05  1.01   1.09\n#> eduPost-2nd grad.     1.13  1.10   1.16\n\nSurvey weighted negative binomial regression\nNow, let’s fit the design-adjusted negative binomial regression model:\n\nrequire(sjstats)\n\n# Design-adjusted negative binomial - crude\nfit3<- svyglm.nb(fruit.cont ~phyact2, design=w.design)\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#>                                2.5 %    97.5 %\n#> theta.(Intercept)   28859.41 5797.98 143647.44\n#> eta.(Intercept)         3.95    3.87      4.02\n#> eta.phyact2Active       1.30    1.26      1.34\n#> eta.phyact2Moderate     1.13    1.10      1.17\n\n# Design-adjusted negative binomial - adjusted for covariates\nfit4<-svyglm.nb(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, \n                design=w.design)\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#>                                        2.5 %     97.5 %\n#> theta.(Intercept)         132340.69 15727.49 1113594.94\n#> eta.(Intercept)                3.24     3.04       3.44\n#> eta.phyact2Active              1.27     1.23       1.31\n#> eta.phyact2Moderate            1.11     1.08       1.14\n#> eta.age40-49 years             1.04     1.01       1.07\n#> eta.age50-59 years             1.11     1.07       1.14\n#> eta.age60-64 years             1.16     1.10       1.22\n#> eta.sexMale                    0.87     0.85       0.90\n#> eta.income$30,000-$49,999      1.03     0.99       1.07\n#> eta.income$50,000-$79,999      1.03     0.99       1.07\n#> eta.income$80,000 or more      1.06     1.02       1.10\n#> eta.raceWhite                  1.02     0.99       1.06\n#> eta.bmicatOverweight           0.99     0.97       1.02\n#> eta.bmicatUnderweight          1.03     0.97       1.09\n#> eta.smokeFormer smoker         1.10     1.06       1.14\n#> eta.smokeNever smoker          1.15     1.11       1.19\n#> eta.edu2nd grad.               1.03     0.98       1.07\n#> eta.eduOther 2nd grad.         1.03     0.98       1.09\n#> eta.eduPost-2nd grad.          1.14     1.09       1.19\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "nonbinaryF.html",
    "href": "nonbinaryF.html",
    "title": "R functions (N)",
    "section": "",
    "text": "The list of new R functions introduced in this non-binary data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n cox.zph \n    survival \n    To assess the proportional hazard assumption \n  \n\n coxph \n    survival \n    To fit cox regression model \n  \n\n ggforest \n    survminer \n    To produce a forest plot \n  \n\n multinom \n    nnet \n    To fit multinomial models \n  \n\n polr \n    MASS \n    To fir ordinal logistic and probit regressions \n  \n\n Surv \n    survival \n    To create a survival object \n  \n\n survdiff \n    survival \n    To compare survival times between groups \n  \n\n survfit \n    survival \n    To create survival curves \n  \n\n svy_vglm \n    svyVGAM \n    To fit design-based generalised linear models \n  \n\n svycoxph \n    survey \n    To fit cox regression model for complex survey data \n  \n\n svyglm.nb \n    sjstats \n    Negative binomial model for complex survey data \n  \n\n svykm \n    survey \n    Estimate survival function for complex survey data \n  \n\n svyolr \n    survey \n    Ordinal logistic for complex survey"
  },
  {
    "objectID": "nonbinaryQ.html#live-quiz",
    "href": "nonbinaryQ.html#live-quiz",
    "title": "Quiz (N)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "nonbinaryQ.html#download-quiz",
    "href": "nonbinaryQ.html#download-quiz",
    "title": "Quiz (N)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "longitudinal.html#background",
    "href": "longitudinal.html#background",
    "title": "Longitudinal data",
    "section": "Background",
    "text": "Background\nThe chapter focuses on longitudinal data analysis. The first dives into mixed effects models, highlighting their importance in studying repeated measurements, especially in longitudinal or clustered data. These models comprise two essential components: fixed effects, which represent universal trends across subjects or clusters, and random effects, capturing individual attributes of each subject or cluster. The tutorial explains their application using datasets, emphasizing model selection metrics and validation methods. The second tutorial introduces the generalized estimating equation (GEE), suitable for modeling non-normally distributed longitudinal data. GEE differentiates from mixed-effects models in its distribution assumptions and its capability to handle various correlation structures. The tutorial illustrates GEE’s application, emphasizing its advantages over other regression models for repeated measurements, and ends with a comparison between GEE and mixed models’ random effects.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "longitudinal.html#overview-of-tutorials",
    "href": "longitudinal.html#overview-of-tutorials",
    "title": "Longitudinal data",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nLongitudinal data analysis is specialized for handling data collected over time with repeated measurements on the same subjects. It accounts for within-subject correlation, time trends, and subject-specific effects, making it distinct from traditional regression, propensity score analysis, and machine learning approaches we discussed earlier, which are not designed for the longitudinal aspect of the data. Choosing the appropriate analysis method depends on the specific research question and the nature of the data at hand.\n\nMixed effects models\nThis tutorial provides an in-depth look into mixed effects models, which are instrumental in analyzing repeated measurements, especially in longitudinal or clustered data scenarios. Within the context of such models, there are two core components: fixed effects and random effects. Fixed effects depict broad trends applicable universally across subjects or clusters, offering insights such as average student performance across different subjects. In contrast, random effects spotlight the distinct attributes of each subject or cluster, accounting for variables such as the quality of resources or the experience of teachers in schools. Using a dataset, the tutorial demonstrates the application and visualization of linear mixed effects models. Emphasis is also placed on model selection, using metrics like AIC and BIC, and on validating the assumptions of the model using residual plots and QQ-plots.\n\n\nGEE\nThe tutorial introduces the generalized estimating equation (GEE) as a method for modeling longitudinal or clustered data that can be non-normally distributed, such as binary, count, or skewed data. Unlike mixed-effects models that assume a normal distribution for error terms and beta coefficients, GEE is distribution-free but assumes specific correlation structures within subjects or clusters. The tutorial exemplifies the application of GEE using respiratory data with a binary response variable. While logistic regression and Poisson regression handle binary and count data, they do not account for repeated measurement structures, potentially leading to incorrect standard error estimates. GEE allows for various correlation structures, and the tutorial details several types, including independent, exchangeable, AR-1 autoregressive, and unstructured correlations. The GEE models produce consistent estimates even if the correlation structure is misspecified. The tutorial concludes by comparing GEE with random effects in mixed models using several code examples.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "longitudinal0.html#longitudinal-data-analysis",
    "href": "longitudinal0.html#longitudinal-data-analysis",
    "title": "Concepts (T)",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\nThe section offers a brief overview of longitudinal data analysis, focusing on key concepts and methods. It explains that longitudinal data involves the repeated measurement of variables of interest over time, often referred to as panel data. The example study data is initially in wide-form, with multiple variables for each time point, and then it is reshaped into long-form data for analysis.\nThe analysis of longitudinal data includes various models, such as linear models, mixed-effect models, and marginal models (GEE). Linear models are introduced initially, where each subject has a common slope and intercept. These ideas can be expanded to incorporate random intercepts, random slopes, and combinations of both in mixed-effect models. Model diagnostics, including AIC, BIC, and -loglik, are discussed for model evaluation. The section briefly discussing different correlation structures and highlights the differences in interpretation between mixed models and marginal models. This section also includes an introduction to marginal structual model, which is a GEE model under a situation when treatment-confounder feedback exists."
  },
  {
    "objectID": "longitudinal0.html#reading-list",
    "href": "longitudinal0.html#reading-list",
    "title": "Concepts (T)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Faraway 2016; Hothorn and Everitt 2014)\nOptional references: (Karim et al. 2021; Cui and Qian 2007)"
  },
  {
    "objectID": "longitudinal0.html#video-lessons",
    "href": "longitudinal0.html#video-lessons",
    "title": "Concepts (T)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nLongitudinal data formatting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: mixed effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: GEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarginal structural model"
  },
  {
    "objectID": "longitudinal0.html#video-lesson-slides",
    "href": "longitudinal0.html#video-lesson-slides",
    "title": "Concepts (T)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nLongitudinal models\n\n\nMarginal structural model"
  },
  {
    "objectID": "longitudinal0.html#links",
    "href": "longitudinal0.html#links",
    "title": "Concepts (T)",
    "section": "Links",
    "text": "Links\nLongitudinal models\n\nGoogle Slides\nPDF Slides\n\nMarginal structural model\n\nGoogle Slides\nPDF Slides\nGitHub page about marginal structural model simulation"
  },
  {
    "objectID": "longitudinal0.html#references",
    "href": "longitudinal0.html#references",
    "title": "Concepts (T)",
    "section": "References",
    "text": "References\n\n\n\n\nCui, Jianwen, and Guoqing Qian. 2007. “Selection of Working Correlation Structure and Best Model in GEE Analyses of Longitudinal Data.” Communications in Statistics—Simulation and Computation® 36 (5): 987–96.\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.\n\n\nKarim, Mohammad Ehsanul, Helen Tremlett, Feng Zhu, John Petkau, and Elaine Kingwell. 2021. “Dealing with Treatment-Confounder Feedback and Sparse Follow-up in Longitudinal Studies: Application of a Marginal Structural Model in a Multiple Sclerosis Cohort.” American Journal of Epidemiology 190 (5): 908–17."
  },
  {
    "objectID": "longitudinal1.html",
    "href": "longitudinal1.html",
    "title": "Mixed effects models",
    "section": "",
    "text": "In this section, we will learn about mixed effects models. Mixed effects models are popular choices for modeling repeated measurements, such as longitudinal or clustered data. Examples of longitudinal data include blood pressure measurements taken over time from the same individuals and CD4 count over time from the same individuals. Examples of clustered data include students within schools and patients within hospitals. There are two components in a mixed effects model: fixed effects and random effects:\n\nFixed effects refer to general trends that are applicable to all subjects/clusters. This implies that we might want to investigate how students perform (on average) in different subjects such as math and history. These effects are commonly observed, i.e., fixed, across all schools.\nRandom Effects capture the unique characteristics of each subject/cluster that differentiate them from one another. For instance, some schools may have better resources, more experienced teachers, or a more supportive learning environment. These differences are specific to each school, and we use random effects to capture them.\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(kableExtra)\nrequire(Matrix)\nrequire(jtools)\n\nRepeated measures\n\n\nA useful reference on repeated measures is Section 9.2 of Faraway (2016).\nIn repeated measurement design, the response variable are measured multiple times for each individuals.\nLinear Mixed Effects Models for Repeated Measures Data\n\n\nA useful reference on linear mixed effects models is Section 12.4 of Hothorn and Everitt (2014).\n\n\n\n\n\n\nImportant\n\n\n\n\nThe linear mixed effect models are based on the idea that the correlation of an individual’s responses depends on some unobserved individual characteristics.\nIn linear mixed effect models, we treat these unobserved characteristics as random effects in our model. If we could conditional on the random effects, then the repeated measurements can be assumed to be independent.\n\n\n\nData\nWe are going to use the BtheB dataset from the HSAUR2 package to explain the linear mixed effect model:\n\nlibrary(HSAUR2)\ndata(\"BtheB\")\nkable(head(BtheB))%>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\n drug \n    length \n    treatment \n    bdi.pre \n    bdi.2m \n    bdi.3m \n    bdi.5m \n    bdi.8m \n  \n\n\n No \n    >6m \n    TAU \n    29 \n    2 \n    2 \n    NA \n    NA \n  \n\n Yes \n    >6m \n    BtheB \n    32 \n    16 \n    24 \n    17 \n    20 \n  \n\n Yes \n    <6m \n    TAU \n    25 \n    20 \n    NA \n    NA \n    NA \n  \n\n No \n    >6m \n    BtheB \n    21 \n    17 \n    16 \n    10 \n    9 \n  \n\n Yes \n    >6m \n    BtheB \n    26 \n    23 \n    NA \n    NA \n    NA \n  \n\n Yes \n    <6m \n    BtheB \n    7 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\nA typical form of repeated measurement data from a clinical trial data.\nThe individuals are allocated to different treatments then the response Beck Depression Inventory II were taken at baseline, 2, 3, 5, and 8 months\n\n\n## Box-plot of responses at different time points in treatment and control groups\ndata(\"BtheB\", package = \"HSAUR2\")\nlayout(matrix(1:2, nrow = 1))\nylim <- range(BtheB[,grep(\"bdi\", names(BtheB))],na.rm = TRUE)\ntau <- subset(BtheB, treatment == \"TAU\")[, grep(\"bdi\", names(BtheB))]\nboxplot(tau, main = \"Treated as Usual\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\nbtheb <- subset(BtheB, treatment == \"BtheB\")[, grep(\"bdi\", names(BtheB))]\nboxplot(btheb, main = \"Beat the Blues\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\n\n\n\n\n\nThe side-by-side box plots show the distributions of BDI overtime between control (Treated as Usual) and intervention (Beat the Blues) groups.\nAs time goes, drops in BDI are more obvious in intervention which may indicate the intervention is effective.\nRegular model fixed intercept and slope\nTo compare, we start with fixed effect linear model, i.e., a regular linear model without any random effect:\n\n## To analyze the data, we first need to convert the dataset to a analysis-ready form\nBtheB$subject <- factor(rownames(BtheB))\nnobs <- nrow(BtheB)\nBtheB_long <- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \n                                  \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time <- rep(c(2, 3, 5, 8), rep(nobs, 4))\nkable(head(BtheB_long))%>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\n   \n    drug \n    length \n    treatment \n    bdi.pre \n    subject \n    time \n    bdi \n  \n\n\n 1.2m \n    No \n    >6m \n    TAU \n    29 \n    1 \n    2 \n    2 \n  \n\n 2.2m \n    Yes \n    >6m \n    BtheB \n    32 \n    2 \n    2 \n    16 \n  \n\n 3.2m \n    Yes \n    <6m \n    TAU \n    25 \n    3 \n    2 \n    20 \n  \n\n 4.2m \n    No \n    >6m \n    BtheB \n    21 \n    4 \n    2 \n    17 \n  \n\n 5.2m \n    Yes \n    >6m \n    BtheB \n    26 \n    5 \n    2 \n    23 \n  \n\n 6.2m \n    Yes \n    <6m \n    BtheB \n    7 \n    6 \n    2 \n    0 \n  \n\n\n\nunique(BtheB_long$subject)\n#>   [1] 1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18 \n#>  [19] 19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 \n#>  [37] 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 \n#>  [55] 55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 \n#>  [73] 73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 \n#>  [91] 91  92  93  94  95  96  97  98  99  100\n#> 100 Levels: 1 10 100 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 ... 99\nkable(subset(BtheB_long,  subject == 2))%>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\n   \n    drug \n    length \n    treatment \n    bdi.pre \n    subject \n    time \n    bdi \n  \n\n\n 2.2m \n    Yes \n    >6m \n    BtheB \n    32 \n    2 \n    2 \n    16 \n  \n\n 2.3m \n    Yes \n    >6m \n    BtheB \n    32 \n    2 \n    3 \n    24 \n  \n\n 2.5m \n    Yes \n    >6m \n    BtheB \n    32 \n    2 \n    5 \n    17 \n  \n\n 2.8m \n    Yes \n    >6m \n    BtheB \n    32 \n    2 \n    8 \n    20 \n  \n\n\n\nkable(subset(BtheB_long,  subject == 99))%>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\n   \n    drug \n    length \n    treatment \n    bdi.pre \n    subject \n    time \n    bdi \n  \n\n\n 99.2m \n    No \n    <6m \n    TAU \n    13 \n    99 \n    2 \n    5 \n  \n\n 99.3m \n    No \n    <6m \n    TAU \n    13 \n    99 \n    3 \n    5 \n  \n\n 99.5m \n    No \n    <6m \n    TAU \n    13 \n    99 \n    5 \n    0 \n  \n\n 99.8m \n    No \n    <6m \n    TAU \n    13 \n    99 \n    8 \n    6 \n  \n\n\n\n\n\nlmfit <- lm(bdi ~ bdi.pre + time + treatment + drug +length, \n            data = BtheB_long, na.action = na.omit)\nrequire(jtools)\nsumm(lmfit)\n\n\n\n\n Observations \n    280 (120 missing obs. deleted) \n  \n\n Dependent variable \n    bdi \n  \n\n Type \n    OLS linear regression \n  \n\n\n\n F(5,274) \n    35.78 \n  \n\n R² \n    0.40 \n  \n\n Adj. R² \n    0.38 \n  \n\n\n\n   \n    Est. \n    S.E. \n    t val. \n    p \n  \n\n\n (Intercept) \n    7.32 \n    1.73 \n    4.24 \n    0.00 \n  \n\n bdi.pre \n    0.57 \n    0.05 \n    10.44 \n    0.00 \n  \n\n time \n    -0.94 \n    0.24 \n    -3.97 \n    0.00 \n  \n\n treatmentBtheB \n    -3.32 \n    1.10 \n    -3.02 \n    0.00 \n  \n\n drugYes \n    -3.57 \n    1.15 \n    -3.11 \n    0.00 \n  \n\n length>6m \n    1.71 \n    1.11 \n    1.54 \n    0.12 \n  \n\n\n Standard errors: OLS\n\n\n\nRandom intercept but fixed slope\nLet us start with a model with a random intercept but fixed slope. In this case, the resulting regression line for each individual is parallel (for fixed slope) but have different intercepts (for random intercept).\n\n## Fit a random intercept model with lme4 package\nlibrary(\"lme4\")\nBtheB_lmer1 <- lmer(bdi ~ bdi.pre + time + treatment + drug +length \n                    + (1 | subject), data = BtheB_long, \n                    REML = FALSE, na.action = na.omit)\n\nsummary(BtheB_lmer1)\n#> Linear mixed model fit by maximum likelihood  ['lmerMod']\n#> Formula: bdi ~ bdi.pre + time + treatment + drug + length + (1 | subject)\n#>    Data: BtheB_long\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>   1887.5   1916.6   -935.7   1871.5      272 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.6975 -0.5026 -0.0638  0.4124  3.8203 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  subject  (Intercept) 48.78    6.984   \n#>  Residual             25.14    5.014   \n#> Number of obs: 280, groups:  subject, 97\n#> \n#> Fixed effects:\n#>                Estimate Std. Error t value\n#> (Intercept)     5.59239    2.24244   2.494\n#> bdi.pre         0.63968    0.07789   8.212\n#> time           -0.70476    0.14639  -4.814\n#> treatmentBtheB -2.32908    1.67036  -1.394\n#> drugYes        -2.82495    1.72684  -1.636\n#> length>6m       0.19708    1.63832   0.120\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) bdi.pr time   trtmBB drugYs\n#> bdi.pre     -0.682                            \n#> time        -0.238  0.020                     \n#> tretmntBthB -0.390  0.121  0.018              \n#> drugYes     -0.073 -0.237 -0.022 -0.323       \n#> length>6m   -0.243 -0.242 -0.036  0.002  0.157\nsumm(BtheB_lmer1)\n\n\n\n\n Observations \n    280 \n  \n\n Dependent variable \n    bdi \n  \n\n Type \n    Mixed effects linear regression \n  \n\n\n\n AIC \n    1887.49 \n  \n\n BIC \n    1916.57 \n  \n\n Pseudo-R² (fixed effects) \n    0.39 \n  \n\n Pseudo-R² (total) \n    0.79 \n  \n\n\n\nFixed Effects\n\n   \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n\n\n\n (Intercept) \n    5.59 \n    2.24 \n    2.49 \n    108.98 \n    0.01 \n  \n\n bdi.pre \n    0.64 \n    0.08 \n    8.21 \n    104.08 \n    0.00 \n  \n\n time \n    -0.70 \n    0.15 \n    -4.81 \n    199.32 \n    0.00 \n  \n\n treatmentBtheB \n    -2.33 \n    1.67 \n    -1.39 \n    97.12 \n    0.17 \n  \n\n drugYes \n    -2.82 \n    1.73 \n    -1.64 \n    98.20 \n    0.11 \n  \n\n length>6m \n    0.20 \n    1.64 \n    0.12 \n    100.26 \n    0.90 \n  \n\n\n  p values calculated using Satterthwaite d.f. \n\n\n\nRandom Effects\n\n Group \n    Parameter \n    Std. Dev. \n  \n\n\n\n subject \n    (Intercept) \n    6.98 \n  \n\n Residual \n     \n    5.01 \n  \n\n\n\n\nGrouping Variables\n\n Group \n    # groups \n    ICC \n  \n\n\n subject \n    97 \n    0.66 \n  \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nAIC, BIC, and -loglik etc are goodness-of-fit statistics, which tells you how well the model fits your data. Since there is no standard to tell what values of these statistics are good, without comparison with other models, they have little information to tell.\nFixed effects: this is the standard output we will have in any fixed-effect model. The interpretation of estimated coefficients will be similar to a regular linear model.\nYou may compare the outputs with the regular linear model, then you will find that lm tends to underestimate the SE for estimated coefficients.\n\n\n\n\nlibrary(ggplot2)\nBtheB_longna <- na.omit(BtheB_long)\ndat <- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer1),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, \n                    color=Subject)) + theme_classic() +\n    geom_line() \n\n\n\n\nAs we can see, for each individual, we have different intercepts but the slope over follow-up time is the same. Next, we will fit the model with random intercept and random slope.\nRandom intercept and random slope\nIn the codes below, we fitted a mixed effects model with both random intercept and random slope:\n\n## We can fit a random slope and intercept model using lme4 package and treat variable time as random slope. \nlibrary(\"lme4\")\nBtheB_lmer2 <- lmer(bdi ~ bdi.pre + time + treatment + drug +length + \n                   (time | subject), data = BtheB_long, REML = FALSE, \n                   na.action = na.omit)\n\nsumm(BtheB_lmer2)\n\n\n\n\n Observations \n    280 \n  \n\n Dependent variable \n    bdi \n  \n\n Type \n    Mixed effects linear regression \n  \n\n\n\n AIC \n    1891.04 \n  \n\n BIC \n    1927.39 \n  \n\n Pseudo-R² (fixed effects) \n    0.39 \n  \n\n Pseudo-R² (total) \n    0.80 \n  \n\n\n\nFixed Effects\n\n   \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n\n\n\n (Intercept) \n    5.61 \n    2.25 \n    2.50 \n    106.79 \n    0.01 \n  \n\n bdi.pre \n    0.64 \n    0.08 \n    8.25 \n    102.78 \n    0.00 \n  \n\n time \n    -0.70 \n    0.15 \n    -4.56 \n    57.69 \n    0.00 \n  \n\n treatmentBtheB \n    -2.38 \n    1.67 \n    -1.42 \n    97.12 \n    0.16 \n  \n\n drugYes \n    -2.87 \n    1.73 \n    -1.66 \n    98.18 \n    0.10 \n  \n\n length>6m \n    0.14 \n    1.64 \n    0.09 \n    100.04 \n    0.93 \n  \n\n\n  p values calculated using Satterthwaite d.f. \n\n\n\nRandom Effects\n\n Group \n    Parameter \n    Std. Dev. \n  \n\n\n\n subject \n    (Intercept) \n    7.12 \n  \n\n subject \n    time \n    0.43 \n  \n\n Residual \n     \n    4.90 \n  \n\n\n\n\nGrouping Variables\n\n Group \n    # groups \n    ICC \n  \n\n\n subject \n    97 \n    0.68 \n  \n\n\n\nThe interpretation of the model outputs will be similar to the model with only random intercepts. Let us plot the data:\n\nlibrary(ggplot2)\nBtheB_longna <- na.omit(BtheB_long)\ndat <- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer2),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, color=Subject)) + \n  theme_classic() +\n    geom_line() \n\n\n\n\nFrom the figure, we can see, we have different intercepts and different slopes over follow-up time for each individual.\nChoice among models\nA common question is to ask should I add random slope to our model or random intercept is good enough. We may want to compare the models in terms of AIC and BIC. Smaller values indicate a better model.\n\nlm usually will not be considered as a competitor of lme as they basically apply to different types of data.\nWhen choosing between random intercept and random slope, a quick solution is to fit all possible models then do likelihood ratio tests.\nFor example, I am not sure whether I should use random intercept only or random intercept + random slope. I could fit both model, then do a likelihood ratio test:\n\n\nanova(BtheB_lmer1, BtheB_lmer2)\n\n\n\n  \n\n\n## The non-significant p-value shows that the second model is not \n## statistically different from the first model. Therefore, adding a \n## random slope is not necessary\n\nThe p-value is greater than 0.05, which indicate that adding a random slope does not make the fitting significantly better. To keep the model simple, we may just use random intercept.\nPrediction of Random Effects\n\nRef: (Hothorn and Everitt 2014) Section 12.5\n\nIf you have noticed in the R output of linear mixed effect model. Random effects are not estimated in the model.\n\nWe could use the fitted model to predict random effects.\nAlso, the predicted random effects can be used to examine the assumptions we have for linear mixed effect model.\n\nThe ranef function is used to predict the random effect in R\n\nqint <- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqint\n#>  [1] -16.36173411  -2.44276827  -3.81655251   3.05333928   3.45142248\n#>  [6]   7.76598376   2.72263773  -7.22484569   6.84680617  -1.02552084\n#> [11]   2.40417048   1.16611114  -8.55346533  -6.25844483  -2.26117896\n#> [16]  -5.23031336   2.52509381  -1.64263857  -1.07523649   3.93689163\n#> [21]   7.66903473 -16.38156595   1.74883174  -1.31871410  -9.02883854\n#> [26]  -2.75150756   2.38884151   2.72740404  -3.41918542   6.27519245\n#> [31]  -4.52154811  -8.67437225  -0.34470208  -0.63972054  -0.08927194\n#> [36]   7.61914282   2.91050412  -2.58455318   4.24637616 -16.01664364\n#> [41]   5.90993779   3.21014012  -7.04631398   3.09608824   4.71327710\n#> [46]  16.70161338   2.26241522   2.26584476  15.77002952   1.75410838\n#> [51]   6.18741474   1.99791717   0.56774362   4.04107968  10.72891321\n#> [56]  -1.54551028  -5.28330207   2.04552526   2.36472056  -1.56879952\n#> [61]   6.63565636   5.45929329  -4.19695096  -6.39632488  -2.21496288\n#> [66]  -0.95956136  -3.96021851   7.17628718  -1.41644518  -4.99763144\n#> [71]  -2.83374092  -1.28015494   8.79345988  -0.33213428   1.43146353\n#> [76]  -3.13316295  -6.52989461   2.51445573   5.93415004   3.42003755\n#> [81]   4.49954804   3.75178702  -6.44832897  12.88173218  -9.36110814\n#> [86]   2.02028152 -18.09960241  -6.53428535   5.06613705  -3.31816912\n#> [91]   7.26187445   0.03103101  -8.14350181  -4.89326763   2.92437698\n#> [96]   5.24835927  -5.96778945\n## predict random effects using the fitted model\n\nCheck assumptions\nRemember, we have two assumptions in the linear mixed effect model with random intercept:\n\nerror term follows normal distribution\nbeta for subject \\(i\\) follows normal distribution\n\nWe have predict the values of random effect and we could extract residuals from the fitted model. Therefore, we can use QQ-plot to check their normality\n\n# Assumption 1\nqres <- residuals(BtheB_lmer1)\nqqnorm(qres, xlim = c(-3, 3), ylim = c(-20, 20), \n       ylab = \"Estimated residuals\",\n       main = \"Residuals\")\nqqline(qres)\n\n\n\n\n# Assumption 2\nqint <- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqqnorm(qint, ylab = \"Estimated random intercepts\", \n       xlim = c(-3, 3), ylim = c(-20, 20),\n       main = \"Random intercepts\")\nqqline(qint)\n\n\n\n\nSince points are almost on the lines, we can say that the normality assumption is met.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press."
  },
  {
    "objectID": "longitudinal2.html",
    "href": "longitudinal2.html",
    "title": "GEE",
    "section": "",
    "text": "In this section, we will learn about generalized estimating equation (GEE). GEE is another popular method for modeling longitudinal or clustered data.\n\n\n\n\n\n\nNote\n\n\n\n\nGEE is a population-averaged (e.g., marginal) model, whereas mixed effects models are subject/cluster-specific. For example, we can use GEE when we are interested in exploring the population average effect. On the other hand, when we are interested in individual/cluster-specific effects, we can use the mixed effects models.\nAlso, in contrast to the mixed effects models where we assume error term and beta coefficients for subject \\(i\\) both follow normal distribution, GEE is distribution free. However, in GEE, we assume some correlation structures for within subjects/clusters. Hence, we can use GEE for non-normal data such as skewed data, binary data, or count data.\n\n\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(HSAUR2)\nlibrary(gee)\nlibrary(MuMIn)\nlibrary(geepack)\nlibrary(\"lme4\")\nrequire(afex)\n\nData Description\nIn addition to BtheB dataset in part 1, we used respiratory data from HSAUR2 package to demonstrate the analysis with non-normal responses:\n\nThe response variable in this dataset is status (the respiratory status), which is a binary response\nOther covariates are: treatment, age, gender, the study center\nThe response has been measured at 0, 1, 2, 3, 4 mths for each subject\n\n\ndata(\"respiratory\", package = \"HSAUR2\")\nhead(respiratory)\n\n\n\n  \n\n\n\nMethods for Non-normal Distributions\n\nref: (Hothorn and Everitt 2014) Section 13.2\nIn addition to normally distributed response variables, the response variable can also follow non-normal distributions, such as binary or count responses.\nWe have learned logistic regression or Poisson regression (glm) to analyze binary and count data but they are not considering the “repeated measurement” structure.\nThe consequence of ignoring the longitudinal/repeated measurements structure is to have the wrong estimated standard errors for regression coefficients. Hence, our inference will be invalid. However, the glm still gives consistent estimated beta coefficients.\nThere are many correlation structures in modelling GEE.\nWith non-normal responses, different assumptions about these correlation structures can lead to different interpretations of the beta coefficients. Here, we will introduce two different GEE models: marginal model and conditional model.\nMarginal Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.1\n\nRepeated measurement and longitudinal data has responses taken at different time points. Therefore, we could simply review them as many series of cross-sectional data. Each cross-sectional data can be analyzed using glm introduced in previous lectures. Then a correlation structure can be assumed to connect different “cross-sections”. The common correlation structures are:\nAn independent structure\nAn independent structure which assumes the repeated measures are independent. An example of independent structure is basically is an identity matrix:\n\\[\n\\left(\\begin{array}{ccc}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{array}\\right)\n\\]\nAn exchangeable correlation\nAn exchangeable correlation assumes that for the each individual, the correlation between each pair of repeated measurements is the same, i.e., \\(Cor(Y_{ij},Y_{ik})=\\rho\\). In the correlation matrix, only \\(\\rho\\) is unknown. Therefore, it is a single parameter working correlation matrix. An example of exchangeable correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho\\\\\n\\rho & 1 & \\rho\\\\\n\\rho & \\rho & 1\n\\end{array}\\right)\n\\]\nAn AR-1 autoregressive correlation\nDefined as \\(Cor(Y_{ij},Y_{ik})=\\rho^{|k-j|}\\). If two measurements are taken at two closer time points the correlation is higher than these taken at two farther apart time points. It is also a single parameter working correlation matrix. An example of AR-1 correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho^2\\\\\n\\rho & 1 & \\rho\\\\\n\\rho^2 & \\rho & 1\n\\end{array}\\right)\n\\]\nAn unstructured correlation\nDifferent pairs of observation for each individual have different correlations \\(Cor(Y_{ij},Y_{ik})=\\rho_{jk}\\). Assume that each individual has \\(K\\) pairs of measurements, it is a \\(K(K-1)/2\\) parameters working correlation matrix. An example of unstructured correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho_{12} & \\rho_{13}\\\\\n\\rho_{12} & 1 & \\rho_{23}\\\\\n\\rho_{13} & \\rho_{23} & 1\n\\end{array}\\right)\n\\]\n\nSometimes, specifying a “right” correlation matrix is hard. However, the marginal model (usually we use GEE) gives us consistent estimated coefficients even with misspecified correlation structure.\nConditional Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.2\nIn GEE marginal models, the estimated regression coefficients are marginal (or population-averaged) effects. Therefore, the interpretation are at population-level. It is almost impossible to make inference on any specific individual or cluster.\nOne solution is to do conditional models. The random effect approach in the part 1 can be extended to non-Gaussian response.\nGEE models\nAfter the short introduction of two models, let’s take a look at real examples\n\nref: (Hothorn and Everitt 2014) Section 13.3\nref: (Faraway 2016) Section 10.2\n\nBinary response\nWe started with binary response using respiratory dataset:\n\nlibrary(gee)\ndata(\"respiratory\", package = \"HSAUR2\")\n## Data manipulation\nresp <- subset(respiratory, month > \"0\")\nresp$baseline <- rep(subset(respiratory, month == \"0\")$status, rep(4, 111))\n## Change the response to 0 and 1\nresp$nstat <- as.numeric(resp$status == \"good\")\nresp$month <- resp$month[, drop = TRUE]\n\nNow we will fit a regular glm, i.e., model without random effect or any correlation structures. For binary outcomes, the estimated coefficients are log odds.\n\n## Regular GLM \nresp_glm <- glm(status ~ centre + treatment + gender + baseline+ age, \n                data = resp, family = \"binomial\")\nsummary(resp_glm)\n#> \n#> Call:\n#> glm(formula = status ~ centre + treatment + gender + baseline + \n#>     age, family = \"binomial\", data = resp)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3146  -0.8551   0.4336   0.8953   1.9246  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)        -0.900171   0.337653  -2.666  0.00768 ** \n#> centre2             0.671601   0.239567   2.803  0.00506 ** \n#> treatmenttreatment  1.299216   0.236841   5.486 4.12e-08 ***\n#> gendermale          0.119244   0.294671   0.405  0.68572    \n#> baselinegood        1.882029   0.241290   7.800 6.20e-15 ***\n#> age                -0.018166   0.008864  -2.049  0.04043 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 608.93  on 443  degrees of freedom\n#> Residual deviance: 483.22  on 438  degrees of freedom\n#> AIC: 495.22\n#> \n#> Number of Fisher Scoring iterations: 4\n\nNow we will fit GEE with independent correlation structure:\n\n## GEE with identity matrix\nresp_gee.in <- gee(nstat ~ centre + treatment + gender + baseline + age, \n                 data = resp, family = \"binomial\", \n                 id = subject,corstr = \"independence\", \n                 scale.fix = TRUE, scale.value = 1)\n#> Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#> running glm to get initial regression estimate\n#>        (Intercept)            centre2 treatmenttreatment         gendermale \n#>        -0.90017133         0.67160098         1.29921589         0.11924365 \n#>       baselinegood                age \n#>         1.88202860        -0.01816588\nsummary(resp_gee.in)\n#> \n#>  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#>  gee S-function, version 4.13 modified 98/01/27 (1998) \n#> \n#> Model:\n#>  Link:                      Logit \n#>  Variance to Mean Relation: Binomial \n#>  Correlation Structure:     Independent \n#> \n#> Call:\n#> gee(formula = nstat ~ centre + treatment + gender + baseline + \n#>     age, id = subject, data = resp, family = \"binomial\", corstr = \"independence\", \n#>     scale.fix = TRUE, scale.value = 1)\n#> \n#> Summary of Residuals:\n#>         Min          1Q      Median          3Q         Max \n#> -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#> \n#> \n#> Coefficients:\n#>                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#> (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#> centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#> treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#> gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#> baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#> age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\n#> \n#> Estimated Scale Parameter:  1\n#> Number of Iterations:  1\n#> \n#> Working Correlation\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    0    0    0\n#> [2,]    0    1    0    0\n#> [3,]    0    0    1    0\n#> [4,]    0    0    0    1\n\n\nThis model assumes an independent correlation structure, the output will be equal to glm.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same as GLM. For binary outcome, you may still interpret them as log odds. Naive SE and z value are estimated directly from this model. Robust SE and z are sandwich estimates.\nThe difference between naive and robust indicates that the correlation structure may not be good.\nWorking Correlation is the correlation structure estimated from the data (identity matrix for independence).\n\nLet fit the GEE model with exchangeable correlation structure:\n\n## GEE with exchangeable matrix\nresp_gee.ex <- gee(nstat ~ centre + treatment + gender + baseline+ age, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#> Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#> running glm to get initial regression estimate\n#>        (Intercept)            centre2 treatmenttreatment         gendermale \n#>        -0.90017133         0.67160098         1.29921589         0.11924365 \n#>       baselinegood                age \n#>         1.88202860        -0.01816588\nsummary(resp_gee.ex)\n#> \n#>  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#>  gee S-function, version 4.13 modified 98/01/27 (1998) \n#> \n#> Model:\n#>  Link:                      Logit \n#>  Variance to Mean Relation: Binomial \n#>  Correlation Structure:     Exchangeable \n#> \n#> Call:\n#> gee(formula = nstat ~ centre + treatment + gender + baseline + \n#>     age, id = subject, data = resp, family = \"binomial\", corstr = \"exchangeable\", \n#>     scale.fix = TRUE, scale.value = 1)\n#> \n#> Summary of Residuals:\n#>         Min          1Q      Median          3Q         Max \n#> -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#> \n#> \n#> Coefficients:\n#>                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#> (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#> centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#> treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#> gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#> baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#> age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n#> \n#> Estimated Scale Parameter:  1\n#> Number of Iterations:  1\n#> \n#> Working Correlation\n#>           [,1]      [,2]      [,3]      [,4]\n#> [1,] 1.0000000 0.3359883 0.3359883 0.3359883\n#> [2,] 0.3359883 1.0000000 0.3359883 0.3359883\n#> [3,] 0.3359883 0.3359883 1.0000000 0.3359883\n#> [4,] 0.3359883 0.3359883 0.3359883 1.0000000\n\n\nThis model assumes an exchangeable correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same GLM. For binary outcome, you may still interpret them as log odds. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nThe difference between naive and robust is smaller, which indicates that the correlation structure is better specified.\nWorking Correlation is the correlation structure estimated from the data\n\nLet’s check the estimated coefficients from all three models.\n\nsummary(resp_glm)$coefficients\n#>                       Estimate  Std. Error    z value     Pr(>|z|)\n#> (Intercept)        -0.90017133 0.337652992 -2.6659658 7.676750e-03\n#> centre2             0.67160098 0.239566555  2.8034004 5.056684e-03\n#> treatmenttreatment  1.29921589 0.236840962  5.4856047 4.120574e-08\n#> gendermale          0.11924365 0.294671000  0.4046671 6.857223e-01\n#> baselinegood        1.88202860 0.241290163  7.7998563 6.197770e-15\n#> age                -0.01816588 0.008864401 -2.0493065 4.043215e-02\nsummary(resp_gee.in)$coefficients\n#>                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#> (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#> centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#> treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#> gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#> baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#> age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\nsummary(resp_gee.ex)$coefficients\n#>                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#> (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#> centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#> treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#> gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#> baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#> age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n# Same estimated coefficients but different SEs\n\nGEE with identity matrix is the same as GLM model. If we change the correlation structure to exchangeable does not change the beta estimates, but the naive SEs are closer to Robust SE, which indicates that the exchangeable correlation structure is a better reflection of the correlation structures.\nGaussian response\nGEE can also be applied to Gaussian response\n\nlibrary(gee)\nlibrary(HSAUR2)\nBtheB$subject <- factor(rownames(BtheB))\nnobs <- nrow(BtheB)\nBtheB_long <- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time <- rep(c(2, 3, 5, 8), rep(nobs, 4))\nosub <- order(as.integer(BtheB_long$subject))\nBtheB_long <- BtheB_long[osub,]\nbtb_gee.ind <- gee(bdi ~ bdi.pre + treatment + length + drug, \n               data = BtheB_long, id = subject, \n               family = gaussian, corstr = \"independence\")\n#> Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#> running glm to get initial regression estimate\n#>    (Intercept)        bdi.pre treatmentBtheB      length>6m        drugYes \n#>      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ind)\n#> \n#>  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#>  gee S-function, version 4.13 modified 98/01/27 (1998) \n#> \n#> Model:\n#>  Link:                      Identity \n#>  Variance to Mean Relation: Gaussian \n#>  Correlation Structure:     Independent \n#> \n#> Call:\n#> gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#>     data = BtheB_long, family = gaussian, corstr = \"independence\")\n#> \n#> Summary of Residuals:\n#>         Min          1Q      Median          3Q         Max \n#> -21.6497810  -5.8485100   0.1131663   5.5838383  28.1871039 \n#> \n#> \n#> Coefficients:\n#>                  Estimate Naive S.E.   Naive z Robust S.E.   Robust z\n#> (Intercept)     3.5686314  1.4833349  2.405816  2.26947617  1.5724472\n#> bdi.pre         0.5818494  0.0563904 10.318235  0.09156455  6.3545274\n#> treatmentBtheB -3.2372285  1.1295569 -2.865928  1.77459534 -1.8242066\n#> length>6m       1.4577182  1.1380277  1.280916  1.48255866  0.9832449\n#> drugYes        -3.7412982  1.1766321 -3.179667  1.78271179 -2.0986557\n#> \n#> Estimated Scale Parameter:  79.25813\n#> Number of Iterations:  1\n#> \n#> Working Correlation\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    0    0    0\n#> [2,]    0    1    0    0\n#> [3,]    0    0    1    0\n#> [4,]    0    0    0    1\n# require(Publish)\n# publish(btb_gee.ind)\n\n\nThis model assumes an independent correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficient will be interpreted the same way as linear model. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation struture estimated from the data (identity matrix for indepedence)\n\nWith exchangeable correlation matrix:\n\nbtb_gee.ex <- gee(bdi ~ bdi.pre + treatment + length + drug,\n                data = BtheB_long, id = subject, \n                family = gaussian, corstr = \"exchangeable\")\n#> Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#> running glm to get initial regression estimate\n#>    (Intercept)        bdi.pre treatmentBtheB      length>6m        drugYes \n#>      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ex)\n#> \n#>  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#>  gee S-function, version 4.13 modified 98/01/27 (1998) \n#> \n#> Model:\n#>  Link:                      Identity \n#>  Variance to Mean Relation: Gaussian \n#>  Correlation Structure:     Exchangeable \n#> \n#> Call:\n#> gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#>     data = BtheB_long, family = gaussian, corstr = \"exchangeable\")\n#> \n#> Summary of Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -23.955980  -6.643864  -1.109741   4.257688  25.452310 \n#> \n#> \n#> Coefficients:\n#>                  Estimate Naive S.E.     Naive z Robust S.E.   Robust z\n#> (Intercept)     3.0231602 2.30390185  1.31219140  2.23204410  1.3544357\n#> bdi.pre         0.6479276 0.08228567  7.87412417  0.08351405  7.7583066\n#> treatmentBtheB -2.1692863 1.76642861 -1.22806339  1.73614385 -1.2494854\n#> length>6m      -0.1112910 1.73091679 -0.06429596  1.55092705 -0.0717577\n#> drugYes        -2.9995608 1.82569913 -1.64296559  1.73155411 -1.7322940\n#> \n#> Estimated Scale Parameter:  81.7349\n#> Number of Iterations:  5\n#> \n#> Working Correlation\n#>           [,1]      [,2]      [,3]      [,4]\n#> [1,] 1.0000000 0.6757951 0.6757951 0.6757951\n#> [2,] 0.6757951 1.0000000 0.6757951 0.6757951\n#> [3,] 0.6757951 0.6757951 1.0000000 0.6757951\n#> [4,] 0.6757951 0.6757951 0.6757951 1.0000000\n#publish(btb_gee.ex)\n\n\nThe interpretation of estimated coefficients are similar to LM. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation structure estimated from the data\nWhen we change the structure of correlation, the estimates and naive SE and z changed. A closer naive SE to robust SE indicates that the correlation structure is better specified.\nCompare with Random Effects\n\nref: (Hothorn and Everitt 2014) Section 13.4 We then use the conditional models (i.e., adding random effect) with non-normal response\n\nLet us compare the GEE models with the mixed effect models.\n\n## Generalized mixed effect model model\nlibrary(\"lme4\")\nresp_lmer <- glmer(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp)\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.197453 (tol = 0.002, component 1)\n\n\nrequire(afex)\nresp_afex <- mixed(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp, method = \"LRT\")\n#> Contrasts set to contr.sum for the following variables: baseline, month, treatment, gender, centre, subject\n#> Numerical variables NOT centered on 0: age\n#> If in interactions, interpretation of lower order (e.g., main) effects difficult.\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.00495318 (tol = 0.002, component 1)\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.0047889 (tol = 0.002, component 1)\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.0466122 (tol = 0.002, component 1)\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.0949357 (tol = 0.002, component 1)\n#> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#> Model failed to converge with max|grad| = 0.041661 (tol = 0.002, component 1)\n\n\n## GEE model\nresp_gee3 <- gee(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#> Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#> running glm to get initial regression estimate\n#>        (Intercept)       baselinegood            month.L            month.Q \n#>        -0.90363465         1.88945124        -0.14372490        -0.02455122 \n#>            month.C treatmenttreatment         gendermale                age \n#>        -0.23255161         1.30410916         0.11969528        -0.01823703 \n#>            centre2 \n#>         0.67417628\nlibrary(MuMIn)\nlibrary(geepack)\nresp_gee4 <- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nresp_gee5 <- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"independence\", \n                 scale.fix = TRUE)\nresp_gee6 <- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"ar1\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#>        QIC       QICu  Quasi Lik        CIC     params       QICC \n#>  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee5)\n#>        QIC       QICu  Quasi Lik        CIC     params       QICC \n#>  511.14981  499.69791 -240.84895   14.72595    9.00000  512.93199\nQIC(resp_gee6)\n#>        QIC       QICu  Quasi Lik        CIC     params       QICC \n#>  511.81242  500.27657 -241.13829   14.76792    9.00000  514.01242\n# Smaller QIC values for correlation structure represents better models\n# i.e., \"exchangeable\" in our case\n\nresp_gee4a <- geeglm(nstat ~ month + treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4b <- geeglm(nstat ~ treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4c <- geeglm(nstat ~ gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4d <- geeglm(nstat ~ age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4e <- geeglm(nstat ~ centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#>        QIC       QICu  Quasi Lik        CIC     params       QICC \n#>  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee4a)[2]\n#>   QICu \n#> 567.35\nQIC(resp_gee4b)[2]\n#>     QICu \n#> 562.6247\nQIC(resp_gee4c)[2]\n#>     QICu \n#> 585.3099\nQIC(resp_gee4d)[2]\n#>     QICu \n#> 586.1581\nQIC(resp_gee4e)[2]\n#>     QICu \n#> 592.3437\nQIC(resp_gee4d)[2]\n#>     QICu \n#> 586.1581\n# Covariates are selected based on the QICu criteria\n\n\n## compare estimates (conditional vs. marginal)\nsummary(resp_lmer)$coefficients # Model failed to converge\n#>                       Estimate Std. Error    z value     Pr(>|z|)\n#> (Intercept)        -1.65464822 0.77621551 -2.1316866 3.303262e-02\n#> baselinegood        3.08898642 0.59858787  5.1604561 2.463489e-07\n#> month.L            -0.20348428 0.27957512 -0.7278340 4.667152e-01\n#> month.Q            -0.02821472 0.27907476 -0.1011009 9.194703e-01\n#> month.C            -0.35569321 0.28084908 -1.2664923 2.053369e-01\n#> treatmenttreatment  2.16621998 0.55157850  3.9273104 8.590108e-05\n#> gendermale          0.23836172 0.66606490  0.3578656 7.204439e-01\n#> age                -0.02557199 0.01993989 -1.2824544 1.996833e-01\n#> centre2             1.03849559 0.54182606  1.9166586 5.528132e-02\nsummary(resp_afex)$coefficients # Model failed to converge\n#>                Estimate Std. Error    z value     Pr(>|z|)\n#> (Intercept)  1.61366355 0.79597814  2.0272712 4.263469e-02\n#> baseline1   -1.55321951 0.30467425 -5.0979678 3.433192e-07\n#> month1       0.21660822 0.24441639  0.8862263 3.754956e-01\n#> month2      -0.17701748 0.24250556 -0.7299523 4.654194e-01\n#> month3       0.21559489 0.24440392  0.8821253 3.777090e-01\n#> treatment1  -1.09162957 0.28098692 -3.8849836 1.023368e-04\n#> gender1     -0.10313396 0.33932808 -0.3039358 7.611768e-01\n#> age         -0.02576276 0.02031564 -1.2681244 2.047535e-01\n#> centre1     -0.52829986 0.27639119 -1.9114207 5.595053e-02\nsummary(resp_gee3)$coefficients # Marginalized model\n#>                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#> (Intercept)        -0.90565507 0.47940039 -1.8891413  0.46042640 -1.9669920\n#> baselinegood        1.86665412 0.34220231  5.4548261  0.35023043  5.3297885\n#> month.L            -0.14330949 0.18044542 -0.7941986  0.18210027 -0.7869812\n#> month.Q            -0.02448267 0.18034926 -0.1357514  0.18329974 -0.1335663\n#> month.C            -0.23187407 0.18073662 -1.2829391  0.15929035 -1.4556693\n#> treatmenttreatment  1.28311415 0.33592099  3.8196903  0.35055323  3.6602548\n#> gendermale          0.11513183 0.41851611  0.2750953  0.44135628  0.2608592\n#> age                -0.01785399 0.01258158 -1.4190585  0.01299686 -1.3737157\n#> centre2             0.68481139 0.34010911  2.0135050  0.35681635  1.9192265\nsummary(resp_gee4)$coefficients # Marginalized model\n\n\n\n  \n\n\n\nThe significance of variables are similar in both variables, but the estimated coefficients are larger in generalized mixed effect model. However, it does not mean the estimated coefficients are inconsistent. Instead, two models are estimating different parameters. Remember, the mixed effect model is conditional on random effects, while the GEE is a marginalized model.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press."
  },
  {
    "objectID": "longitudinalF.html",
    "href": "longitudinalF.html",
    "title": "R functions (T)",
    "section": "",
    "text": "The list of new R functions introduced in this longitudinal data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n gee \n    gee \n    To fit a generalized estimation equation model \n  \n\n geeglm \n    geepack \n    To fit a generalized estimation equation model \n  \n\n lmer \n    lme4 \n    To fit linear mixed effects models \n  \n\n glmer \n    lme4 \n    To fit generalized linear mixed effects models \n  \n\n mixed \n    afex \n    To fit generalized linear mixed effects models \n  \n\n qqnorm \n    base/stats \n    To fit a QQ plot \n  \n\n ranef \n    lme5 \n    To extract the random effects from a model \n  \n\n reshape \n    base/stats \n    Reshape data, e.g., into wide to long or long to wide format \n  \n\n residuals \n    base/stats \n    To extract residuals of a model"
  },
  {
    "objectID": "longitudinalQ.html#live-quiz",
    "href": "longitudinalQ.html#live-quiz",
    "title": "Quiz (T)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "longitudinalQ.html#download-quiz",
    "href": "longitudinalQ.html#download-quiz",
    "title": "Quiz (T)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "mediation.html#background",
    "href": "mediation.html#background",
    "title": "Mediation analysis",
    "section": "Background",
    "text": "Background\nThis chapter provides comprehensive tutorials on mediation analysis. The Baron and Kenny approach explores non-binary outcomes through directed acyclic graphs (DAGs) and regression in big data scenarios, with a focus on both continuous and binary mediators and outcomes. The justification of mediation analysis evaluates the connection between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD), considering various covariates like BMI, smoking status, and associations with diseases like diabetes. The final mediation example centers on decomposing the total effect of OA on CVD through direct and indirect pathways via pain medication, including data preparation, weight computation, and outcome evaluation, accompanied by considerations of non-linearity and potential interactions.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "mediation.html#overview-of-tutorials",
    "href": "mediation.html#overview-of-tutorials",
    "title": "Mediation analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily discussed about total effect of an exposure to the outcome. In this chapter, we will discuss about decomposition of the effect in the presence of a mediator.\n\nBaron and Kenny Approach\nThe chapter, referencing the Baron and Kenny approach, delves into the analysis of non-binary outcomes using directed acyclic graphs (DAGs) and regression techniques for big data scenarios with a million observations. Initially, the chapter focuses on a continuous outcome and continuous mediator, where the true treatment effect is known. Through the data generating process, a DAG is formulated and data simulated, followed by an estimation of effects using generalized linear models. Subsequently, the Baron and Kenny approaches are applied to determine direct, total, and indirect effects. The chapter progresses to explore binary outcomes with both continuous and binary mediators, each time employing a similar approach: creating a DAG, generating data, estimating effects using regression models, and then using the Baron and Kenny methodology to elucidate the relationships.\n\n\nJustification of Mediation Analysis\nIn this chapter, the data analysis process centers on understanding the relationship between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD) using a mediation analysis. Specifically, the analysis seeks to determine if OA, the exposure, is associated with an increased risk of CVD, the outcome. Additionally, it investigates whether pain medication acts as a mediator in this causal pathway. The total effect of OA on CVD risk was found to be significant. Furthermore, OA was observed to significantly influence the use of pain medication, which is the proposed mediator. To facilitate the analysis, the study considers various adjustment covariates, including demographic variables, confounders such as BMI and smoking status, and associations with other diseases like diabetes. The data used in this study is pre-processed, analyzed, and subsequently saved for further use.\n\n\nMediation Example\nIn the chapter, the focus is on decomposing the “total effect” of a given exposure, OA (\\(A\\)), on the outcome CVD (\\(Y\\)) into its natural direct effect (NDE; \\(A \\rightarrow Y\\)) and a natural indirect effect (NIE) that routes through a mediator, in this case, pain medication (\\(M\\)). Initially, the required data is loaded and preprocessed. The mediation analysis involves several steps: (1) Preparing the data and ensuring it has the necessary variables; (2) Modifying data for different exposures and duplicating it; (3) Computing weights for the mediation based on logistic regressions, where the weights are applied to factor in the mediator’s effect; (4) Building a weighted outcome model, which is a logistic regression to evaluate the outcome. To quantify these effects, the chapter derives point estimates for the total effect, direct effect, and indirect effect. Furthermore, confidence intervals for these effects are determined using bootstrap methods. The results, including the proportion mediated by pain medication, are visualized using graphs. The chapter also delves into considerations of non-linearity and potential interactions between variables.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "mediation0.html#mediation-analysis",
    "href": "mediation0.html#mediation-analysis",
    "title": "Concepts (I)",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\nThe section provides an overview of the concept and methods of mediation analysis in the context of epidemiology and statistical modeling. The section discusses total effects, indirect and direct paths, and highlights the limitations of the Baron and Kenny approach (Baron and Kenny 1986). It introduces the counterfactual definition, emphasizing the importance of adjusting for confounders and providing insights into the mechanics of mediation analysis through imputation and weighting methods. The section also covers sensitivity analysis, proportion mediated, and methodological extensions, including multi-category mediators and software references for conducting mediation analysis."
  },
  {
    "objectID": "mediation0.html#reading-list",
    "href": "mediation0.html#reading-list",
    "title": "Concepts (I)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Rochon, Bois, and Lange 2014; Lange et al. 2017)"
  },
  {
    "objectID": "mediation0.html#video-lessons",
    "href": "mediation0.html#video-lessons",
    "title": "Concepts (I)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMediation Analysis - Baron and Kenny (1986)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Mediation analysis with counterfactual definitions and why confounding adjustment helps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis mechanism under counterfactual definition (i) Outcome imputation & (ii) Weighting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis using survey data, assumptions, extensions, software and references"
  },
  {
    "objectID": "mediation0.html#video-lesson-slides",
    "href": "mediation0.html#video-lesson-slides",
    "title": "Concepts (I)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides"
  },
  {
    "objectID": "mediation0.html#links",
    "href": "mediation0.html#links",
    "title": "Concepts (I)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides"
  },
  {
    "objectID": "mediation0.html#references",
    "href": "mediation0.html#references",
    "title": "Concepts (I)",
    "section": "References",
    "text": "References\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. “The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173.\n\n\nLange, Theis, Kristoffer W Hansen, Rune Sørensen, and Søren Galatius. 2017. “Applied Mediation Analyses: A Review and Tutorial.” Epidemiology and Health 39.\n\n\nRochon, Justine, Andreas du Bois, and Theis Lange. 2014. “Mediation Analysis of the Relationship Between Institutional Research Activity and Patient Survival.” BMC Medical Research Methodology 14 (1): 9."
  },
  {
    "objectID": "mediation1.html",
    "href": "mediation1.html",
    "title": "Baron and Kenny",
    "section": "",
    "text": "Baron and Kenny (1986) approach 1\nIn Baron and Kenny approach 1 (Baron and Kenny 1986), two regression models are fitted to estimate the total effect, direct effect, and indirect effect.\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] where \\(Y\\) is the outcome, \\(A\\) is the exposure, \\(M\\) is the mediator, and \\(\\alpha, \\beta, \\gamma\\) are regression coefficients. The effects are then calculated as:\n\nTotal effect of A on Y: \\(\\hat{\\beta}_1\\)\n\nDirect effect of A on Y: \\(\\hat{\\beta}_0\\)\n\nIndirect effect of A on Y through M: \\(\\hat{\\beta}_1 - \\hat{\\beta}_0\\),\n\nwhere \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\nBaron and Kenny (1986) approach 2\nIn the second approach, three models are fitted:\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] \\[ M = \\alpha_{2} + \\beta_2 A, \\]\nThe indirect effect of A on Y through M can be calculated as: \\(\\hat{\\beta}_2 \\times \\hat{\\beta}_0\\), where \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\n\n# Load required packages\nrequire(simcausal)\n\nBig data: What if we had 1,000,000 (1 million) observations?\nLet us explore the mediation analysis using Baron and Kenny (1986) approaches. We first simulate a big dataset and then show the results from the mediation analysis.\nContinuous outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n    node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n    node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node M, order:2\n#> node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#> simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.55\nfit.am <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.am),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\nfit.m <- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#> (Intercept)           A \n#>         0.0         0.5\n\n\n# from 1st model\na.coef <- round(coef(fit),2)[2]\na.coef\n#>    A \n#> 1.55\n# from 2nd (adjusted) model\nam.coef <- round(coef(fit.am),2)[2]\nam.coef\n#>   A \n#> 1.3\nm.coef <- round(coef(fit.am),2)[3]\nm.coef\n#>   M \n#> 0.5\n# from 3rd (mediator) model\nma.coef <- round(coef(fit.m),2)[2]\nma.coef\n#>   A \n#> 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#>   A \n#> 1.3\n# Total effect\na.coef\n#>    A \n#> 1.55\n# Indirect effect\na.coef - am.coef\n#>    A \n#> 0.25\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#>    M \n#> 0.25\n\nBinary outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node M, order:2\n#> node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#> simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.48\nfit.am <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\nfit.m <- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#> (Intercept)           A \n#>         0.0         0.5\n\n\n# from 1st model\na.coef <- round(coef(fit),2)[2]\na.coef\n#>    A \n#> 1.48\n# from 2nd (adjusted) model\nam.coef <- round(coef(fit.am),2)[2]\nam.coef\n#>   A \n#> 1.3\nm.coef <- round(coef(fit.am),2)[3]\nm.coef\n#>   M \n#> 0.5\n# from 3rd (mediator) model\nma.coef <- round(coef(fit.m),2)[2]\nma.coef\n#>   A \n#> 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#>   A \n#> 1.3\n# Total effect\na.coef\n#>    A \n#> 1.48\n# Indirect effect\na.coef - am.coef\n#>    A \n#> 0.18\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#>    M \n#> 0.25\n\nBinary outcome, binary mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rbern\", prob = plogis(0.5 * A)) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node M, order:2\n#> node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#> simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.25        1.34\nfit.am <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\nfit.m <- glm(M ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.m),2)\n#> (Intercept)           A \n#>         0.0         0.5\n\n\n# from 1st model\na.coef <- round(coef(fit),2)[2]\na.coef\n#>    A \n#> 1.34\n# from 2nd (adjusted) model\nam.coef <- round(coef(fit.am),2)[2]\nam.coef\n#>   A \n#> 1.3\nm.coef <- round(coef(fit.am),2)[3]\nm.coef\n#>   M \n#> 0.5\n# from 3rd (mediator) model\nma.coef <- round(coef(fit.m),2)[2]\nma.coef\n#>   A \n#> 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#>   A \n#> 1.3\n# Total effect\na.coef\n#>    A \n#> 1.34\n# Indirect effect\na.coef - am.coef\n#>    A \n#> 0.04\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#>    M \n#> 0.25\n\nAs we can see, the estimated indirect effects are different from the two approaches. That means Baron and Kenny’s (1986) approach doesn’t work for a non-collapsible effect measure such as the odds ratio.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. “The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173."
  },
  {
    "objectID": "mediation2.html",
    "href": "mediation2.html",
    "title": "Justification",
    "section": "",
    "text": "We must show enough justification to do a mediation analysis. A causal diagram would be the first step to conceptualize the mediation analysis problem hypothetically. Then, we can empirically verify where doing the mediation analysis makes sense in that particular context.\n\n# Load required packages\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\n\nPre-processing\nLoad saved data\n\nload(\"Data/mediation/cchs123pain.RData\")\n\nPrepared the data\n\nanalytic.miss$mediator <- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure <- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome <- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nNotation\n\nOutcome (\\(Y\\)): Cardiovascular disease (CVD)\nExposure (\\(A\\)): Osteoarthritis (OA)\nMediator (\\(M\\)): Pain medication\nAdjustment covariates (\\(C\\))\nHypothesis\n\nFor total effect (TE): Is OA (\\(A\\)) associated with CVD (\\(Y\\))?\n\n\n\n\n\n\n\n\n\n\nFor mediation analysis: Does pain-medication (\\(M\\)) play a mediating role in the causal pathway between OA (\\(A\\)) and CVD (\\(Y\\))? Here, we will decompose total effect (TE) to a natural direct effect (NDE) and a natural indirect effect (NIE).\n\n\n\n\n\n\n\n\n\nAdjustment variables (\\(C\\)):\n\nDemographics\n\nage\nsex\nincome\nrace\neducation status\n\n\nImportant confounders\n\nBMI\nphysical activity\nsmoking status\nfruit and vegetable consumption\n\n\nRelation with other diseases\n\nhypertension\nCOPD\ndiabetes\n\n\nAnalysis/empirical exploration\nTotal effect\nOutcome model (weighted) is\n\\(logit [P(Y_{a}=1 | C = c] = \\theta_0 + \\theta_1 a + \\theta_3 c\\)\nSetting Design\n\nrequire(survey)\nsummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Survey design\nw.design0 <- svydesign(id=~1, weights=~weight, data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#> [1] 80.34263\n\n# Subset the design\nw.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#>    2.053   28.480   53.218   82.472  102.860 1295.970\nsd(weights(w.design))\n#> [1] 91.98946\n\n\n# Model\nTE <- svyglm(outcome ~ exposure + age + sex + income + race + bmi + edu + phyact + \n               smoke + fruit + diab, design = w.design, family = quasibinomial(\"logit\"))\n# painmed is mediator; not included here.\nTE.save <- exp(c(summary(TE)$coef[\"exposure\",1], \n                confint(TE)[\"exposure\",]))\nTE.save\n#>             2.5 %   97.5 % \n#> 1.537005 1.230735 1.919489\n\nExposure to OA has a detrimental effect on CVD risk (significant!).\nEffect on the mediators\n\nfit.m = svyglm(mediator ~ exposure + age + sex + income + race + bmi + edu +\n                 phyact + smoke + fruit + diab, design = w.design,\n               family = binomial(\"logit\"))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nmed.save <- exp(c(summary(fit.m)$coef[\"exposure\",1], confint(fit.m)[\"exposure\",]))\nmed.save\n#>             2.5 %   97.5 % \n#> 2.428463 2.059265 2.863853\n\nExposure to OA has a substantial effect on the mediator (Pain medication) as well (significant!). Hence, it would be interesting to explore a mediation analysis to assess the mediating role.\nSave data\n\nsave(w.design, file = \"Data/mediation/cchs123painW.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "mediation3.html",
    "href": "mediation3.html",
    "title": "Mediation Example",
    "section": "",
    "text": "# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(Publish)\n\nWe want to decompose of the “total effect” of a given exposure OA (\\(A\\)) on the outcome CVD (\\(Y\\)) into\n\na natural direct effect (NDE; \\(A \\rightarrow Y\\)) and\na natural indirect effect (NIE) through a mediator pain medication (\\(M\\)) through (\\(A \\rightarrow M \\rightarrow Y\\)).\n\nStep 0: Build data first\n\nload(\"Data/mediation/cchs123pain.RData\")\nsource(\"Data/mediation/medFunc.R\")\nls()\n#> [1] \"analytic.cc\"        \"analytic.miss\"      \"doEffectDecomp\"    \n#> [4] \"doEffectDecomp.int\" \"has_annotations\"\n\nvarlist <- c(\"age\", \"sex\", \"income\", \"race\", \"bmi\", \"edu\", \"phyact\", \"smoke\", \"fruit\", \"diab\")\nanalytic.miss$mediator <- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure <- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome <- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nPre-run step 3 model\nWe will utilize this fit in step 3\n\n# A = actual exposure (without any change)\nanalytic.miss$exposureTemp <- analytic.miss$exposure\n\n# Design\nw.design0 <- svydesign(id=~1, weights=~weight, data=analytic.miss)\nw.design <- subset(w.design0, miss == 0)\n\n# Replace exposure with exposureTemp. This will be necessary in step 3\nfit.m <- svyglm(mediator ~ exposureTemp + \n                 age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab,\n                design = w.design, family = binomial(\"logit\"))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.m)\n#>      Variable             Units OddsRatio       CI.95     p-value \n#>  exposureTemp                        2.43 [2.06;2.86]     < 1e-04 \n#>           age       20-29 years       Ref                         \n#>                     30-39 years      1.00 [0.88;1.13]   0.9442989 \n#>                     40-49 years      0.93 [0.82;1.06]   0.2651302 \n#>                     50-59 years      0.66 [0.58;0.76]     < 1e-04 \n#>                     60-64 years      0.61 [0.51;0.72]     < 1e-04 \n#>               65 years and over      0.61 [0.52;0.71]     < 1e-04 \n#>           sex            Female       Ref                         \n#>                            Male      0.50 [0.46;0.55]     < 1e-04 \n#>        income   $29,999 or less       Ref                         \n#>                 $30,000-$49,999      1.20 [1.06;1.35]   0.0043533 \n#>                 $50,000-$79,999      1.21 [1.08;1.37]   0.0014914 \n#>                 $80,000 or more      1.28 [1.14;1.45]     < 1e-04 \n#>          race         Non-white       Ref                         \n#>                           White      1.81 [1.62;2.02]     < 1e-04 \n#>           bmi       Underweight       Ref                         \n#>                  healthy weight      1.09 [0.82;1.44]   0.5631582 \n#>                      Overweight      1.33 [1.01;1.77]   0.0449616 \n#>           edu          < 2ndary       Ref                         \n#>                       2nd grad.      1.13 [0.98;1.30]   0.1014986 \n#>                 Other 2nd grad.      1.30 [1.08;1.55]   0.0050596 \n#>                  Post-2nd grad.      1.25 [1.10;1.42]   0.0008252 \n#>        phyact            Active       Ref                         \n#>                        Inactive      1.12 [1.02;1.23]   0.0184447 \n#>                        Moderate      1.12 [1.01;1.25]   0.0364592 \n#>         smoke      Never smoker       Ref                         \n#>                  Current smoker      1.29 [1.16;1.44]     < 1e-04 \n#>                   Former smoker      1.28 [1.17;1.40]     < 1e-04 \n#>         fruit 0-3 daily serving       Ref                         \n#>               4-6 daily serving      0.92 [0.83;1.02]   0.0976967 \n#>                6+ daily serving      0.80 [0.71;0.90]   0.0001979 \n#>          diab                No       Ref                         \n#>                             Yes      1.23 [0.99;1.52]   0.0626501\n\nStep 1 and 2: Replicate data with different exposures\nWe manipulate and duplicate data here\n\ndim(analytic.miss)\n#> [1] 397173     28\ndim(analytic.cc)\n#> [1] 28734    23\nnrow(analytic.miss) - nrow(analytic.cc)\n#> [1] 368439\n\n# Create counterfactual data. This will be necessary in step 3\nd1 <- d2 <- analytic.miss\n\n# Exposed = Exposed\nd1$exposure.counterfactual <- d1$exposure\n\n# Exposed = Not exposed\nd2$exposure.counterfactual <- !d2$exposure \n\n# duplicated data (double the amount)\nnewd <- rbind(d1, d2)\nnewd <- newd[order(newd$ID), ]\ndim(newd)\n#> [1] 794346     29\n\nStep 3: Compute weights for the mediation\nWeight is computed by\n\\(W^{M|C} = \\frac{P(M|A^*, C)}{P(M|A, C)}\\)\nin all data newd (fact d1 + alternative fact d2).\n\n\n\\(P(M|A, C)\\) is computed from a logistic regression of \\(M\\) on \\(A\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta_1 a + \\beta_3 c\\)\n\n\n\n\\(P(M|A^{*}, C)\\) is computed from a logistic regression of \\(M\\) on \\(A^*\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta'_1 a^* + \\beta'_3 c\\)\n\n\n\n\n# First, use original exposure (all A + all A):\n# A = actual exposure (without any change)\n# A = exposure\nnewd$exposureTemp <- newd$exposure\n\n# Probability of M given A + C\nw <- predict(fit.m, newdata=newd, type='response') \ndirect <- ifelse(newd$mediator, w, 1-w)\n\n# Second, use counterfactual exposures (all A + all !A):\n# A* = Opposite (counterfactual) values of the exposure\n# A* = exposure.counterfactual\nnewd$exposureTemp <- newd$exposure.counterfactual\n\n# Probability of M given A* + C\nw <- predict(fit.m, newdata=newd, type='response') \nindirect <- ifelse(newd$mediator, w, 1-w)\n\n# Mediator weights\nnewd$W.mediator <- indirect/direct\nsummary(newd$W.mediator)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>     0.4     1.0     1.0     1.0     1.2     2.2  670758\nhist(newd$W.mediator)\n\n\n\n\nIncorporating the survey weights:\nNote: scaling can often be helpful if there exists extreme weights.\n\n# scale survey weights\n#newd$S.w <- with(newd,(weight)/mean(weight))\nnewd$S.w <- with(newd,weight)\nnewd$S.w[is.na(newd$S.w)]\n#> numeric(0)\nsummary(newd$S.w)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Multiply mediator weights with scaled survey weights\nnewd$SM.w <- with(newd,(W.mediator * S.w))\n\nsummary(newd$SM.w)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>     0.4    24.7    46.4    73.9    88.8  3531.6  670758\ntable(newd$miss[is.na(newd$SM.w)])\n#> \n#>      1 \n#> 670758\nnewd$SM.w[is.na(newd$SM.w)] <- 0\nsummary(newd$SM.w)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     0.0     0.0     0.0    11.5     0.0  3531.6\n\nhist(newd$SM.w, main = \"\", xlab = \"Combined weights\", \n     ylab = \"Frequency\", freq = TRUE)\n\n\n\n\nHere all missing weights are associated with incomplete cases (miss==1)! Hence, doesn’t matter if they are missing or other value (0) in them.\nStep 4: Weighted outcome Model\nOutcome model is\n\\(logit [P(Y_{a,M(a^*)}=1 | C = c)] = \\theta_0 + \\theta_1 a + \\theta_2 a^* + \\theta_3 c\\)\nafter weighting (combination of mediator weight + sampling weight).\n\n# Outcome analysis\nw.design0 <- svydesign(id=~1, weights=~SM.w, data=newd)\nw.design <- subset(w.design0, miss == 0)\n\n# Fit Y on (A + A* + C)\nfit <- svyglm(outcome ~ exposure + exposure.counterfactual + \n                age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab, \n             design = w.design, family = binomial(\"logit\"))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPoint estimates\nFollowing are the conditional ORs:\n\n\\(OR_{TE}(C=c) = \\exp(\\theta_1 + \\theta_2)\\)\n\\(OR_{NDE}(A=1,M=0,C=c) = \\exp(\\theta_1)\\)\n\\(OR_{NIE}(A^{*}=1,M=0,C=c) = \\exp(\\theta_2)\\)\n\n\n# total effect of A-> Y + A -> M -> Y\nTE <- exp(sum(coef(fit)[c('exposure', 'exposure.counterfactual')])) \nTE \n#> [1] 1.544694\n\n# direct effect of A-> Y (not through M)\nDE <- exp(unname(coef(fit)['exposure']))\nDE \n#> [1] 1.488554\n\n# indirect effect of A-> Y (A -> M -> Y)\nIE <- exp(unname(coef(fit)[c('exposure.counterfactual')])) \nIE \n#> [1] 1.037714\n\n# Product of ORs; same as TE \nDE * IE \n#> [1] 1.544694\n\n# Proportion mediated\nPM <- log(IE) / log(TE)\nPM \n#> [1] 0.08513902\n\nObtaining results fast\nUser-written function doEffectDecomp() (specific to OA-CVD problem):\n\ndoEffectDecomp(analytic.miss, ind = NULL, varlist = varlist)\n#>         TE         DE         IE         PM \n#> 1.54469393 1.48855395 1.03771444 0.08513902\n# function is provided in the appendix\n\nConfidence intervals\nStandard errors and confidence intervals are determined by bootstrap methods.\n\nrequire(boot)\n# I ran the computation on a 24 core computer,\n# hence set ncpus = 5 (keep some free). \n# If you have more / less cores, adjust accordingly. \n# Try parallel package to find how many cores you have.\n# library(parallel)\n# detectCores()\n# doEffectDecomp is a user-written function\n# See appendix for the function\nset.seed(504)\nbootresBin <- boot(data=analytic.miss, statistic=doEffectDecomp, \n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1b <- boot.ci(bootresBin,type = \"perc\",index=1)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2b <- boot.ci(bootresBin,type = \"perc\",index=2)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3b <- boot.ci(bootresBin,type = \"perc\",index=3)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4b <- boot.ci(bootresBin,type = \"perc\",index=4)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\n# Number of bootstraps\nbootresBin$R\n#> [1] 5\n\n# Total Effect\nc(bootresBin$t0[1], bootci1b$percent[4:5])\n#>       TE                   \n#> 1.544694 1.293208 1.894417\n\n# Direct Effect\nc(bootresBin$t0[2], bootci2b$percent[4:5])\n#>       DE                   \n#> 1.488554 1.303554 1.876916\n\n# Indirect Effect\nc(bootresBin$t0[3], bootci3b$percent[4:5])\n#>        IE                     \n#> 1.0377144 0.9738072 1.0093246\n\n# Proportion Mediated\nc(bootresBin$t0[4], bootci4b$percent[4:5])\n#>          PM                         \n#>  0.08513902 -0.08360848  0.01655013\n\nThe proportion mediated through pain medication was about 8.51% on the log odds ratio scale.\nVisualization for main effects\n\nrequire(plotrix)\nTEc <- c(bootresBin$t0[1], bootci1b$percent[4:5])\nDEc <- c(bootresBin$t0[2], bootci2b$percent[4:5])\nIEc <- c(bootresBin$t0[3], bootci3b$percent[4:5])\nmat <- rbind(TEc,DEc,IEc)\ncolnames(mat) <- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#>        Point      2.5%    97.5%\n#> TEc 1.544694 1.2932078 1.894417\n#> DEc 1.488554 1.3035540 1.876916\n#> IEc 1.037714 0.9738072 1.009325\n\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2], xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\nNon-linearity\nConsider\n\nnon-linear relationships (polynomials) and interactions between exposure, demographic/baseline covariates and mediators,\nIs misclassification of the mediators possible?\n\nHere we are again using a user-written function doEffectDecomp.int() (including interaction phyact*diab in the mediation model as well as the outcome model):\n\n# doEffectDecomp.int is a user-written function\n# See appendix for the function\ndoEffectDecomp.int(analytic.miss, ind = NULL, varlist = varlist)\n#>         TE         DE         IE         PM \n#> 1.54472021 1.48868864 1.03763821 0.08496674\n# try bootstrap on it?\n\nVisualization for main + interactions\n\nset.seed(504)\nbootresInt <- boot(data=analytic.miss, statistic=doEffectDecomp.int,\n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1i <- boot.ci(bootresInt,type = \"perc\",index=1)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2i <- boot.ci(bootresInt,type = \"perc\",index=2)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3i <- boot.ci(bootresInt,type = \"perc\",index=3)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4i <- boot.ci(bootresInt,type = \"perc\",index=4)\n#> Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\nbootresInt$R\n#> [1] 5\n# from saved boostrap results: bootresInt \n# (similar as before)\nTEc <- c(bootresInt$t0[1], bootci1i$percent[4:5])\nDEc <- c(bootresInt$t0[2], bootci2i$percent[4:5])\nIEc <- c(bootresInt$t0[3], bootci3i$percent[4:5])\nmat<- rbind(TEc,DEc,IEc)\ncolnames(mat) <- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#>        Point      2.5%    97.5%\n#> TEc 1.544720 1.2931358 1.893575\n#> DEc 1.488689 1.3040953 1.876521\n#> IEc 1.037638 0.9742042 1.009088\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2],\n       xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\nAppendix: OA-CVD Functions for bootstrap\nThese functions are written basically for performing bootstrap for the OA-CVD analysis. However, changing the covariates names/model-specifications should not be too hard, once you understand the basic steps.\n\n# without interactions (binary mediator)\ndoEffectDecomp\n#> function (dat, ind = NULL, varlist) \n#> {\n#>     if (is.null(ind)) \n#>         ind <- 1:nrow(dat)\n#>     d <- dat[ind, ]\n#>     d$mediator <- ifelse(as.character(d$painmed) == \"Yes\", 1, \n#>         0)\n#>     d$exposure <- ifelse(as.character(d$OA) == \"OA\", 1, 0)\n#>     d$outcome <- ifelse(as.character(d$CVD) == \"event\", 1, 0)\n#>     d$exposureTemp <- d$exposure\n#>     w.design0 <- svydesign(id = ~1, weights = ~weight, data = d)\n#>     w.design <- subset(w.design0, miss == 0)\n#>     fit.m <- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp  + \"), \n#>         paste0(varlist, collapse = \"+\"))), design = w.design, \n#>         family = quasibinomial(\"logit\"))\n#>     d1 <- d2 <- d\n#>     d1$exposure.counterfactual <- d1$exposure\n#>     d2$exposure.counterfactual <- !d2$exposure\n#>     newd <- rbind(d1, d2)\n#>     newd <- newd[order(newd$ID), ]\n#>     newd$exposureTemp <- newd$exposure\n#>     w <- predict(fit.m, newdata = newd, type = \"response\")\n#>     direct <- ifelse(newd$mediator, w, 1 - w)\n#>     newd$exposureTemp <- newd$exposure.counterfactual\n#>     w <- predict(fit.m, newdata = newd, type = \"response\")\n#>     indirect <- ifelse(newd$mediator, w, 1 - w)\n#>     newd$W.mediator <- indirect/direct\n#>     summary(newd$W.mediator)\n#>     newd$S.w <- with(newd, weight)\n#>     summary(newd$S.w)\n#>     newd$SM.w <- with(newd, (W.mediator * S.w))\n#>     newd$SM.w[is.na(newd$SM.w)] <- 0\n#>     summary(newd$SM.w)\n#>     w.design0 <- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#>     w.design <- subset(w.design0, miss == 0)\n#>     fit <- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#>         paste0(varlist, collapse = \"+\"))), design = w.design, \n#>         family = quasibinomial(\"logit\"))\n#>     TE <- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#>     DE <- exp(unname(coef(fit)[\"exposure\"]))\n#>     IE <- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#>     PM <- log(IE)/log(TE)\n#>     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#> }\n#> <bytecode: 0x000001992a60f740>\n\n# with interactions (binary mediator)\ndoEffectDecomp.int\n#> function (dat, ind = NULL, varlist) \n#> {\n#>     if (is.null(ind)) \n#>         ind <- 1:nrow(dat)\n#>     d <- dat[ind, ]\n#>     d$exposureTemp <- d$exposure\n#>     w.design0 <- svydesign(id = ~1, weights = ~weight, data = d)\n#>     w.design <- subset(w.design0, miss == 0)\n#>     fit.m <- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp + phyact*diab +\"), \n#>         paste0(varlist, collapse = \"+\"))), design = w.design, \n#>         family = quasibinomial(\"logit\"))\n#>     d1 <- d2 <- d\n#>     d1$exposure.counterfactual <- d1$exposure\n#>     d2$exposure.counterfactual <- !d2$exposure\n#>     newd <- rbind(d1, d2)\n#>     newd <- newd[order(newd$ID), ]\n#>     newd$exposureTemp <- newd$exposure\n#>     w <- predict(fit.m, newdata = newd, type = \"response\")\n#>     direct <- ifelse(newd$mediator, w, 1 - w)\n#>     newd$exposureTemp <- newd$exposure.counterfactual\n#>     w <- predict(fit.m, newdata = newd, type = \"response\")\n#>     indirect <- ifelse(newd$mediator, w, 1 - w)\n#>     newd$W.mediator <- indirect/direct\n#>     summary(newd$W.mediator)\n#>     newd$S.w <- with(newd, weight)\n#>     summary(newd$S.w)\n#>     newd$SM.w <- with(newd, (W.mediator * S.w))\n#>     newd$SM.w[is.na(newd$SM.w)] <- 0\n#>     summary(newd$SM.w)\n#>     w.design0 <- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#>     w.design <- subset(w.design0, miss == 0)\n#>     fit <- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#>         paste0(varlist, collapse = \"+\"))), design = w.design, \n#>         family = quasibinomial(\"logit\"))\n#>     TE <- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#>     DE <- exp(unname(coef(fit)[\"exposure\"]))\n#>     IE <- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#>     PM <- log(IE)/log(TE)\n#>     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#> }\n#> <bytecode: 0x0000019937f6f0a8>\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "mediationF.html",
    "href": "mediationF.html",
    "title": "R functions (I)",
    "section": "",
    "text": "The list of new R functions introduced in this mediation analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n boot \n    boot \n    To conduct bootstrap resampling \n  \n\n boot.ci \n    boot \n    To calculate confidence intervals from bootstrap samples"
  },
  {
    "objectID": "mediationQ.html#live-quiz",
    "href": "mediationQ.html#live-quiz",
    "title": "Quiz (I)",
    "section": "Live Quiz",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):\n\n    <p>Your browser does not support iframes.</p>"
  },
  {
    "objectID": "mediationQ.html#download-quiz",
    "href": "mediationQ.html#download-quiz",
    "title": "Quiz (I)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "reporting.html#background",
    "href": "reporting.html#background",
    "title": "Writing tools",
    "section": "Background",
    "text": "Background\nThe tutorial offers a detailed guide on using R, RStudio, and GitHub for collaborative manuscript writing and management, covering aspects from software installation, through manuscript creation and editing, to publishing and sharing via GitHub Pages. Additionally, it introduces various supplementary resources, including LaTex, TablesGenerator, and Zotero, to further assist and streamline the research and writing processes.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "reporting.html#overview-of-tutorials",
    "href": "reporting.html#overview-of-tutorials",
    "title": "Writing tools",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nScientific Writing Collaboratively\nIn this tutorial, we navigated through a comprehensive guide on utilizing R, RStudio, and GitHub for collaborative manuscript writing. Beginning with the installation of R and RStudio, we explored creating a GitHub repository and managing it using GitHub Desktop. Subsequently, we delved into using RStudio to create a manuscript with a Bookdown template, compiling it, and updating the manuscript content. The tutorial also covered updating the GitHub repository with manuscript changes and publishing the manuscript using GitHub Pages, enabling a seamless, collaborative, and organized manuscript writing and sharing process, ensuring efficiency and ease in academic writing collaborations.\n\n\nFormatting Resources\nThis section provides a wealth of tools and platforms beneficial for researchers and writers in managing and creating scientific documents. It encompasses LaTex and ShareLaTeX for document preparation, TablesGenerator for converting tables from MS Word to various formats, and R packages like “officer” and “flextable” for generating tables and charts. It also introduces draw.io for crafting flow charts, platforms like jane for identifying suitable journals, officetimeline for creating Gantt charts, and Google Docs for real-time collaborative writing. Moreover, it highlights Zotero and ZoteroBib as comprehensive tools for reference management and bibliography creation, facilitating organized, collaborative, and streamlined research and writing processes.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "reporting1.html",
    "href": "reporting1.html",
    "title": "Collaborative Writing",
    "section": "",
    "text": "This tutorial provides a step-by-step guide to using R, RStudio, GitHub, and GitHub Desktop for collaborative manuscript writing. These tools facilitate collaboration and help keep track of the manuscript writing progress.\nAll necessary files for this tutorial are available here. If you’re new to Rmarkdown, consider revisiting the introductory tutorial.\n\nGitHub Account\n\nVisit GitHub and click “Sign up”, and follow the instructions to create your account.\nOn GitHub, click the “+” or “new” icon on the top right and select “New repository”.\nName your repository, add a description, initialize it with a README, and click “Create repository”.\n\n\n\nNecessary Software\n\nR and RStudio (revisit previous chapter)\nGitHub Desktop. Install and sign in with your GitHub credentials.\n\n\n\nCloning Repository\n\nOn GitHub Desktop, go to “File” > “Clone repository”,\nchoose the repository you want to clone and select the local path,\nand click “Clone”.\nNavigate to the local path and ensure all files are cloned.\n\n\n\nBookdown Template in RStudio\n\nIn RStudio, go to “File” > “New Project” > “New Directory” > “Book Project using bookdown”.\nName your project (e.g., “test1”) and create it.\nNavigate to the project directory, copy all files, and paste them into your original repository folder.\nRename the project file to a suitable name (e.g., “template”).\n\n\n\nCompiling\n\nOpen the index file in your RStudio project.\nUpdate the YAML header and chapter/section names as per your manuscript.\nCompile the document into HTML and/or PDF using the “Build” tab in RStudio.\n\n\n\nUpdating GitHub Repository\n\nMake changes to your manuscript files in RStudio.\nOpen GitHub Desktop, review changes, commit them with a descriptive message, and push to the origin.\n\n\n\nPublishing Using GitHub Pages\n\nIn your GitHub repository, create a new folder named “docs” and copy your compiled HTML files into it.\nGo to the settings of your GitHub repository, navigate to “GitHub Pages”, and set the source to the “docs” folder.\nYour manuscript will be available at “username.github.io/repository_name”.\n\n\n\nInvite collaborators\nTo invite collaborators to a GitHub repository, begin by navigating to your desired repository and clicking on the “Settings” tab. In the left sidebar, select “Manage Access” and click “Invite a collaborator.” Enter the username, full name, or email of the person you wish to invite and select them from the suggestions. Assign a role to define their access level and send the invitation. The invitee can accept via the email link sent or directly through their GitHub account. You can review and manage all collaborators and pending invitations in the “Manage Access” section.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\nUseful resources\n\nManuscript template. The output website can be accessed here"
  },
  {
    "objectID": "reporting2.html",
    "href": "reporting2.html",
    "title": "Formatting Tools",
    "section": "",
    "text": "The tutorial elucidates a variety of tools and methodologies aimed at streamlining and enhancing academic writing and presentation creation, discussing topics such as utilizing a typesetting system, converting tables across different formats, employing various R packages for enhanced data visualization and presentation, drawing flow and Gantt charts, crafting HTML5 presentations, enabling collaborative writing and document sharing, managing references efficiently, formatting articles, and employing specific platforms for identifying appropriate journals for publishing academic articles.\nLaTex\nLaTeX is a high-quality software system for typesetting document. LaTeX is designed for the production of technical and scientific documentation, such as book, articles.\nGet an account in ShareLaTeX/Overleaf.\nTable conversion\nFrom MS word to latex / markdown / HTML in TablesGenerator.\nYou can use tableone package to generate csv file, and then import them in the TablesGenerator to convert to HTML to paste to doc file!\n\ntab1x <- print(tab1, quote = FALSE, noSpaces = TRUE, printToggle = FALSE)\nwrite.csv(tab1x, file = \"tab1x.csv\")\n\nFancy table and chart generators:\nR Packages\n\nofficer\nofficedown\nflextable\nmschart\nDrawing flow chart\ndraw.io\nPresentation\nThe “xaringan” package, derived from a love for the Japanese manga and anime “Naruto”, serves as an R Markdown extension, and it facilitates the creation of distinctively styled HTML5 presentations by leveraging the JavaScript library remark.js. Originating from an intent to produce a unique, though not widely adopted style, due to its potentially challenging pronunciation unless familiar with the anime, “xaringan” offers significant customizability in presentation design and has garnered additional theme contributions from its user community. Despite only supporting Markdown, “xaringan” enhances remark.js by introducing support for R Markdown and additional utilities, simplifying the slide-building and previewing processes. Further insights into “xaringan”, its background, and its utility can be explored through its documentation.\nGantt charts\nofficetimeline\nSimultaneous collaborative writing\nGoogle Docs offers a platform for real-time collaborative writing. Multiple users can edit documents simultaneously, and changes are saved automatically. Note that Google Docs requires sign-in for a Google account.\n\nCommenting and Suggesting: Use the comment and suggest features to provide feedback without altering the original text.\nRevision History: Navigate through the revision history to view changes and revert to previous versions if needed.\nSharing and Permissions: Manage who can view, comment, or edit the document with varied permission levels.\nReference manager\nZotero stands out as a free, open-source reference management software that assists researchers, academics, and students in organizing, managing, and formatting their citations and bibliographies. It’s not just a reference manager but also a powerful tool for collaborative work on research projects. Try the Zotero desktop manager as well for assisting with reference inserting.\nZotero syncs data across devices, ensuring that users can access their libraries from any location. Users can work offline with Zotero, and any changes made will be synchronized when the internet connection is restored.\nZoteroBib: Use ZoteroBib to generate bibliographies instantly without creating an account or installing software.\nArticle formatting\nThe rticles package in R provides a diverse selection of templates for creating academic articles and is easily accessible directly within the RStudio environment by navigating through File -> New File -> R Markdown, where users can select their desired template. For users not utilizing RStudio, the installation of Pandoc is requisite, with articles being creatable using the rmarkdown::draft() function, and specifying the template and package parameters as needed. Additionally, the package enables viewing a list of available journal names using rticles::journals(). To employ enhanced features, such as automatic figure numbering and cross-referencing of tables, users can utilize functionalities from the bookdown package. This involves adjusting the YAML to use bookdown::pdf_book as the output format, and designating the chosen rticles template as the base_format. Comprehensive details and tutorials regarding the use of the rticles package can be found in its online documentation. The complete array of options can be explored within the R Markdown templates window in RStudio, via the packages’s GitHub readme or accessed programmatically via the following function:\n\nrticles::journals()\n#>  [1] \"acm\"            \"acs\"            \"aea\"            \"agu\"           \n#>  [5] \"ajs\"            \"amq\"            \"ams\"            \"arxiv\"         \n#>  [9] \"asa\"            \"bioinformatics\" \"biometrics\"     \"copernicus\"    \n#> [13] \"ctex\"           \"elsevier\"       \"frontiers\"      \"glossa\"        \n#> [17] \"ieee\"           \"ims\"            \"informs\"        \"iop\"           \n#> [21] \"isba\"           \"jasa\"           \"jedm\"           \"joss\"          \n#> [25] \"jss\"            \"lipics\"         \"lncs\"           \"mdpi\"          \n#> [29] \"mnras\"          \"oup_v0\"         \"oup_v1\"         \"peerj\"         \n#> [33] \"pihph\"          \"plos\"           \"pnas\"           \"rjournal\"      \n#> [37] \"rsos\"           \"rss\"            \"sage\"           \"sim\"           \n#> [41] \"springer\"       \"tf\"             \"trb\"            \"wellcomeor\"\n\nFind appropriate journals\n\njane\nSee more extensive list here\n\nSpecific Epidemiology-focus journals\n\nSummary\n\n\nCategory\nTool\nDescription\n\n\n\nAcademic Search Engine\nGoogle Scholar\nFreely accessible search engine indexing scholarly literature.\n\n\nArticle Formatting\nrticles (R Package)\nR package providing templates for various academic journals.\n\n\nBrainstorming & Mapping\nMindMeister\nOnline mind mapping tool for brainstorming and collaborative visualization.\n\n\nDocument Creation & Editing\nOverleaf\nCollaborative LaTeX editor online.\n\n\n\nGoogle Docs\nPlatform for real-time collaborative writing.\n\n\n\nofficer (R Package)\nR package for generating Word and PowerPoint files.\n\n\n\nofficedown (R Package)\nR package to produce Word documents with officer.\n\n\nGantt Chart Generators\nOffice Timeline\nOnline tool for creating Gantt charts.\n\n\nJournal Finding\njane\nTool to assist in finding the right journal for publishing.\n\n\n\nCustom List\nExtensive list and guide on finding suitable journals.\n\n\n\nEpidemiology Journals\nSpecific list of epidemiology-focus journals.\n\n\nNote & Research Management\nEvernote\nNote-taking and organization tool for managing research notes and drafts.\n\n\nPresentation & Sharing\nPrezi\nDynamic and visually engaging presentation creation tool.\n\n\n\nSlideShare\nPlatform for sharing presentations and professional documents.\n\n\nPresentation\nxaringan (R Package)\nR Markdown extension for creating presentations using remark.js.\n\n\nProject Management\nAsana\nProject management tool for workflow organization and collaboration.\n\n\n\nTrello\nProject management tool for task tracking and collaboration.\n\n\nReference Management\nEndNote\nReference management software for organizing and integrating references.\n\n\n\nJabRef\nOpen-source bibliography reference manager using BibTeX.\n\n\n\nMendeley\nReference management and academic social network.\n\n\n\nPaperpile\nReference management and academic research library.\n\n\n\nZotero\nReference management and collaborative tool.\n\n\n\nZoteroBib\nQuick bibliography generation tool.\n\n\nResearch Identity Management\nORCID\nProvides a persistent digital identifier to distinguish researchers.\n\n\nTable & Chart Generators\ndraw.io\nOnline diagram software for flow charts and various diagrams.\n\n\n\nTablesGenerator\nConverts tables to LaTeX, markdown, HTML formats.\n\n\n\nflextable (R Package)\nR package for tabular reporting in various formats (Word, HTML, etc.).\n\n\n\nmschart (R Package)\nR package to create PowerPoint charts.\n\n\nWriting & Editing\nAuthorea\nCollaborative platform for writing, citing, and publishing.\n\n\n\nGrammarly\nWriting assistant for grammar and style enhancement."
  }
]