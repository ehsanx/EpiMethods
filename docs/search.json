[
  {
    "objectID": "missingdata12.html",
    "href": "missingdata12.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Setup\nThis tutorial demonstrates how to perform a survey-weighted survival analysis using NHANES data when there are missing values in the predictors. Instead of relying on a complete-case analysis, which can lose statistical power and introduce bias, we will use multiple imputation with the mice package in R.\nThe workflow follows the structure of a standard epidemiological analysis:\nFirst, we load the necessary R packages. We then load the dat.full.with.mortality.RDS file (from here), which contains the merged NHANES and mortality data from 1999-2018.\n# Load all necessary packages for the analysis\nlibrary(dplyr)\nlibrary(car)\nlibrary(survival)\nlibrary(mice)         # For imputation\nlibrary(survey)       # For survey analysis (svydesign, svyglm, etc.)\nlibrary(mitools)      # FOR imputationList() and other MI tools\nlibrary(Publish)\nlibrary(DataExplorer)\nlibrary(knitr)\nlibrary(kableExtra)\n# devtools::install_github(\"ehsanx/svyTable1\", build_vignettes = TRUE, dependencies = TRUE)\nlibrary(svyTable1) # for svypooled\n# Set survey option for compatibility\noptions(survey.want.obsolete = TRUE)\n\n# Load the full, merged dataset with mortality information\ndat.full.with.mortality &lt;- readRDS(\"Data/missingdata/dat.full.with.mortality.RDS\")",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#data-preparation-and-cleaning",
    "href": "missingdata12.html#data-preparation-and-cleaning",
    "title": "Survival Analysis",
    "section": "3. Data Preparation and Cleaning",
    "text": "3. Data Preparation and Cleaning\nBefore imputation, we must create the final analytic variables and clean the dataset. This includes creating the exposure, survival time, and status variables, and then dropping all unnecessary raw or intermediate columns.\n\n# --- Create Analytic Variables ---\n\n# 1. Exposure Variable ('exposure.cat')\ndat.full.with.mortality$exposure.cat &lt;- car::recode(\n  dat.full.with.mortality$smoking.age,\n  \"0 = 'Never smoked'; 1:9 = 'Started before 10';\n   10:14 = 'Started at 10-14'; 15:17 = 'Started at 15-17';\n   18:20 = 'Started at 18-20'; 21:80 = 'Started after 20';\n   else = NA\",\n  as.factor = TRUE\n)\ndat.full.with.mortality$exposure.cat &lt;- factor(\n  dat.full.with.mortality$exposure.cat,\n  levels = c(\"Never smoked\", \"Started before 10\", \"Started at 10-14\",\n             \"Started at 15-17\", \"Started at 18-20\", \"Started after 20\")\n)\n\n# 2. Survival Time ('stime.since.birth') and Status ('status_all')\ndat.full.with.mortality$stime.since.birth &lt;-\n  ((dat.full.with.mortality$age * 12) + dat.full.with.mortality$mort_permth_int) / 12\n# 'status_all' is our event indicator, derived from 'mort_stat'. It's essential for the Surv() object.\ndat.full.with.mortality$status_all &lt;- dat.full.with.mortality$mort_stat\n\n# 3. Categorical Year ('year.cat')\ndat.full.with.mortality$year.cat &lt;- dat.full.with.mortality$year\nlevels(dat.full.with.mortality$year.cat) &lt;- c(\n  \"1999-2000\", \"2001-2002\", \"2003-2004\", \"2005-2006\", \"2007-2008\",\n  \"2009-2010\", \"2011-2012\", \"2013-2014\", \"2015-2016\", \"2017-2018\"\n)\n\n# --- Define the Analytic Cohort & Drop Unnecessary Variables ---\n\n# 4. Apply age restriction (20-79 years)\ndat.analytic &lt;- subset(dat.full.with.mortality, age &gt;= 20 & age &lt; 80)\n\n# 5. Drop all raw, intermediate, or unused columns\nvars_to_drop &lt;- c(\n  \"age\", \"born\", \"smoking.age\", \"smoked.while.child\", \"smoking\", \"year\",\n  \"mort_eligstat\", \"mort_stat\", \"mort_ucod_leading\", \"mort_diabetes\",\n  \"mort_hyperten\", \"mort_permth_int\", \"mort_permth_exm\"\n)\ndat.analytic[vars_to_drop] &lt;- NULL\n\n# Verify the cleaned data\ncat(\"Remaining columns for analysis:\\n\")\n#&gt; Remaining columns for analysis:\nnames(dat.analytic)\n#&gt;  [1] \"id\"                \"sex\"               \"race\"             \n#&gt;  [4] \"psu\"               \"strata\"            \"survey.weight.new\"\n#&gt;  [7] \"exposure.cat\"      \"stime.since.birth\" \"status_all\"       \n#&gt; [10] \"year.cat\"\nplot_missing(dat.analytic)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ‚Ñπ Please use tidy evaluation idioms with `aes()`.\n#&gt; ‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ‚Ñπ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\nprofile_missing(dat.analytic)",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#multiple-imputation-with-mice",
    "href": "missingdata12.html#multiple-imputation-with-mice",
    "title": "Survival Analysis",
    "section": "Multiple Imputation with mice ü™Ñ",
    "text": "Multiple Imputation with mice ü™Ñ\nWe will impute the missing values in dat.analytic before running the final models.\nPreparing for Imputation\nWe must include the survival outcome information in the imputation model. The best way to do this is by creating and including the Nelson-Aalen cumulative hazard estimate (White and Royston 2009).\n\n# Create the Nelson-Aalen cumulative hazard estimate\n# It's a more informative summary of survival than time alone\ndat.analytic$nelson_aalen &lt;- nelsonaalen(\n  dat.analytic,\n  time = stime.since.birth,\n  status = status_all\n)\nsummary(dat.analytic$nelson_aalen)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n#&gt; 0.000000 0.006434 0.026145 0.122797 0.112218 2.763365      134\nsummary(dat.analytic$stime.since.birth)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   21.08   43.67   57.08   57.22   70.58   99.42     134\ntable(dat.analytic$status_all)\n#&gt; \n#&gt;     0     1 \n#&gt; 44475  6215\nhist(dat.analytic$nelson_aalen)\n\n\n\n\n\n\n\n4.2 Configuring the Predictor Matrix\nThe predictorMatrix tells mice what to do. Here‚Äôs the logic for our setup:\n\n\nWhat variable are we imputing?\n\nexposure.cat\n\n\n\nSince exposure.cat is the only predictor with missing data, it‚Äôs the only variable we will actively impute in this tutorial.\n\n\nWhat about the missing outcome data?\n\nData shows that stime.since.birth and status_all also have missing values.\nIt is standard practice not to impute the outcome variables in a survival analysis.\n\n\n\nWhat variables will help the imputation (i.e., act as predictors)?\n\n\nOutcome Information: status_all and nelson_aalen. These are crucial for making the imputation model compatible with the survival analysis.\n\nConfounders: sex, race, year.cat.\n\nAuxiliary Variables: psu, strata, and survey.weight.new. Including the survey design variables makes the imputation ‚Äúsurvey-aware‚Äù and more accurate.\n\n\n\nWhat variables will we ignore as predictors?\n\n\nid (it‚Äôs just an identifier).\n\nstime.since.birth (its information is better and more simply captured by nelson_aalen).\n\n\n\n\n# Initialize the predictor matrix\npred_matrix &lt;- make.predictorMatrix(dat.analytic)\n\n# --- DO NOT use these variables AS PREDICTORS ---\n# We exclude the raw survival time and identifier variables from being predictors.\npred_matrix[, c(\"id\", \"stime.since.birth\")] &lt;- 0\n\n# --- DO NOT IMPUTE these variables ---\n# These variables are complete, identifiers, or part of the outcome.\npred_matrix[c(\"id\", \"sex\", \"race\", \"psu\", \"strata\", \"survey.weight.new\",\n              \"stime.since.birth\", \"status_all\", \"year.cat\",\n              \"nelson_aalen\"), ] &lt;- 0\n\n# Run the imputation. m and maxit are low for demonstration\nimputed_data &lt;- mice(\n  dat.analytic,\n  m = 2,              # Number of imputed datasets\n  maxit = 2,         # Number of iterations per imputation\n  predictorMatrix = pred_matrix,\n  method = 'pmm',     # Predictive Mean Matching is a good default\n  seed = 123          # For reproducibility\n)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   1   2  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   2   1  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   2   2  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt; Warning: Number of logged events: 8",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#survival-analysis-on-imputed-data",
    "href": "missingdata12.html#survival-analysis-on-imputed-data",
    "title": "Survival Analysis",
    "section": "5. Survival Analysis on Imputed Data üìä",
    "text": "5. Survival Analysis on Imputed Data üìä\nWith our m=2 complete datasets, we follow the ‚Äúanalyze then pool‚Äù procedure:\n\n\nAnalyze: Run the svycoxph model on each of the 2 datasets.\n\nPool: Combine the 2 sets of results into a single, final estimate using pool().\n\n\n# --- 5. Survival Analysis on Imputed Data  ---\n\n# --- Step 5.1: Re-integrate Ineligible Subjects for Correct Survey Variance ---\n\n# First, extract the 'm' imputed datasets into a single long-format data frame.\n# Add a flag to identify this group as our analytic/eligible sample.\nimputed_analytic_data &lt;- complete(imputed_data, \"long\", include = FALSE)\nimputed_analytic_data$eligible &lt;- 1\n\n# Next, identify the subjects from the original full dataset who were NOT in our analytic sample.\n# The analytic sample was defined as age &gt;= 20 & age &lt; 80.\ndat_ineligible &lt;- subset(dat.full.with.mortality, !(age &gt;= 20 & age &lt; 80))\n\n# Replicate this ineligible dataset 'm' times, once for each imputation.\nineligible_list &lt;- lapply(1:imputed_data$m, function(i) {\n  df &lt;- dat_ineligible\n  df$.imp &lt;- i # Add the imputation number\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# Now, align the columns. Add columns that exist in the imputed data (like 'nelson_aalen')\n# to the ineligible data, filling them with NA.\ncols_to_add &lt;- setdiff(names(imputed_analytic_data), names(ineligible_stacked))\nineligible_stacked[, cols_to_add] &lt;- NA\n\n# Set the eligibility flag for this group to 0.\nineligible_stacked$eligible &lt;- 0\n\n# CRITICAL: Ensure the column order is identical before row-binding.\nineligible_final &lt;- ineligible_stacked[, names(imputed_analytic_data)]\n\n# Finally, combine the imputed analytic data with the prepared ineligible data.\nimputed_full_data &lt;- rbind(imputed_analytic_data, ineligible_final)\n\n\n# --- Step 5.2: Create Survey Design and Run Pooled Analysis ---\n\n# Create the complex survey design object using an `imputationList`.\n# This tells the survey package how to handle the 'm' imputed datasets.\n# The design is specified on the *full* data to capture the total population structure.\ndesign_full &lt;- svydesign(ids = ~psu,\n                         strata = ~strata,\n                         weights = ~survey.weight.new,\n                         nest = TRUE,\n                         data = imputationList(split(imputed_full_data, imputed_full_data$.imp)))\n\n# Subset the design object to include only the eligible participants for the analysis.\n# This ensures variance is calculated correctly based on the full sample design.\ndesign_analytic &lt;- subset(design_full, eligible == 1)\n\n# Fit the Cox model across all 'm' imputed datasets using the `with()` function.\n# This is more efficient than a for-loop.\nfit_pooled &lt;- with(design_analytic,\n                   svycoxph(Surv(stime.since.birth, status_all) ~ exposure.cat + sex + race + year.cat))\n\n# Pool the results from the list of model fits using Rubin's Rules.\npooled_results &lt;- pool(fit_pooled)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n\n# Display the final, pooled results.\nprint(\"--- Final Adjusted Cox Model Results (from Pooled Imputed Data) ---\")\n#&gt; [1] \"--- Final Adjusted Cox Model Results (from Pooled Imputed Data) ---\"\nsummary(pooled_results, conf.int = TRUE, exponentiate = TRUE)\n\n\n  \n\n\n\n\n# Option A: Fallacy-safe table showing only the main exposure\nsvypooled(\n  pooled_model = pooled_results,\n  main_exposure = \"exposure.cat\",\n  adj_var_names = c(\"sex\", \"race\", \"year.cat\"),\n  measure = \"HR\",\n  title = \"Adjusted Hazard Ratios for All-Cause Mortality\"\n)\n\n\nAdjusted Hazard Ratios for All-Cause Mortality\n\nCharacteristic\nHR (95% CI)\np-value\n\n\n\nexposure.cat\n\n\nStarted before 10\n2.71 (2.05, 3.59)\n&lt;0.001\n\n\nStarted at 10-14\n2.37 (2.10, 2.68)\n&lt;0.001\n\n\nStarted at 15-17\n1.95 (1.77, 2.16)\n&lt;0.001\n\n\nStarted at 18-20\n1.48 (1.34, 1.65)\n&lt;0.001\n\n\nStarted after 20\n1.54 (1.39, 1.70)\n&lt;0.001\n\n\n\n\n   Adjusted for: sex, race, year.cat\n\n\n\n\n\n\n# Option B: Full table for an appendix\nsvypooled(\n  pooled_model = pooled_results,\n  main_exposure = \"exposure.cat\",\n  adj_var_names = c(\"sex\", \"race\", \"year.cat\"),\n  measure = \"HR\",\n  title = \"Full Adjusted Model Results (for Appendix)\",\n  fallacy_safe = FALSE\n)\n\n\nFull Adjusted Model Results (for Appendix)\n\nCharacteristic\nHR (95% CI)\np-value\n\n\n\nexposure.cat\n\n\nStarted before 10\n2.71 (2.05, 3.59)\n&lt;0.001\n\n\nStarted at 10-14\n2.37 (2.10, 2.68)\n&lt;0.001\n\n\nStarted at 15-17\n1.95 (1.77, 2.16)\n&lt;0.001\n\n\nStarted at 18-20\n1.48 (1.34, 1.65)\n&lt;0.001\n\n\nStarted after 20\n1.54 (1.39, 1.70)\n&lt;0.001\n\n\nsex\n\n\nFemale\n0.74 (0.69, 0.79)\n&lt;0.001\n\n\nrace\n\n\nBlack\n1.60 (1.47, 1.74)\n&lt;0.001\n\n\nHispanic\n1.04 (0.93, 1.16)\n0.493\n\n\nOthers\n1.13 (0.96, 1.34)\n0.148\n\n\nyear.cat\n\n\n2001-2002\n0.97 (0.86, 1.08)\n0.538\n\n\n2003-2004\n0.84 (0.75, 0.95)\n0.004\n\n\n2005-2006\n0.76 (0.68, 0.85)\n&lt;0.001\n\n\n2007-2008\n0.78 (0.67, 0.90)\n0.001\n\n\n2009-2010\n0.67 (0.59, 0.78)\n&lt;0.001\n\n\n2011-2012\n0.63 (0.52, 0.77)\n&lt;0.001\n\n\n2013-2014\n0.58 (0.49, 0.70)\n&lt;0.001\n\n\n2015-2016\n0.35 (0.27, 0.46)\n&lt;0.001\n\n\n2017-2018\n0.20 (0.13, 0.29)\n&lt;0.001",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#conclusion",
    "href": "missingdata12.html#conclusion",
    "title": "Survival Analysis",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nThis tutorial demonstrated how to replace a complete-case analysis with a multiple imputation workflow for a survey-weighted survival analysis. By correctly preparing the data, configuring mice with survival-specific information, and pooling the final results, we can generate valid estimates that properly account for missing data.",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#references",
    "href": "missingdata12.html#references",
    "title": "Survival Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, and Chuyi Zheng. 2025. ‚ÄúExamining the Role of Race/Ethnicity and Sex in Modifying the Association Between Early Smoking Initiation and Mortality: A 20-Year NHANES Analysis.‚Äù AJPM Focus 4 (2): 100282.\n\n\nWhite, Ian R, and Patrick Royston. 2009. ‚ÄúImputing Missing Covariate Values for the Cox Model.‚Äù Statistics in Medicine 28 (15): 1982‚Äì98.",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R functions (M)",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\naggr\nVIM\nTo calculate/plot the missing values in the variables\n\n\nboxplot\nbase/graphics\nTo produce a box plot\n\n\nbwplot\nmice\nTo produce box plot to compare the imputed and observed values\n\n\ncolMeans\nbase\nTo compute the column-wise mean, i.e., mean for each variable/column\n\n\ncomplete\nmice\nTo extract the imputed dataset\n\n\ncomplete.cases\nbase/stats\nTo select the complete cases, i.e., observations without missing values\n\n\nD1\nmice\nTo conduct the multivariate Wald test with D1-statistic\n\n\ndensityplot\nmice\nTo produce desnsity plots\n\n\nexpression\nbase\nTo set/create an expression\n\n\nimputationList\nmice\nTo combine multiple imputed datasets\n\n\nmarginplot\nVIM\nTo draw a scatterplot with additional information when there are missing values\n\n\nmcar_test\nnaniar\nTo conduct Little's MCAR test\n\n\nmd.pattern\nmice\nTo see the pattern of the missing data\n\n\nmice\nmice\nTo impute missing data where the argument m represents the number of multiple imputation\n\n\nMIcombine\nmitools\nTo combine/pool the results using Rubin's rule\n\n\nMIextract\nmitools\nTo extract parameters from a list of outputs\n\n\nna.test\nmisty\nTo conduct Little's MCAR test\n\n\nparlmice\nmice\nTo run `mice` function in parallel, i.e., parallel computing of mice\n\n\nplot_missing\nDataExplorer\nTo plot the profile of missing values, e.g., the percentage of missing per variable\n\n\npool\nmice\nTo pool the results using Rubin's rule\n\n\npool.compare\nmice\nTo compare two nested models\n\n\npool_mi\nmiceadds\nTo combine/pool the results using Rubin's rule\n\n\nquickpred\nmice\nTo set imputation model based on the correlation\n\n\nsim_slopes\ninteractions\nTo perform simple slope analyses\n\n\nTestMCARNormality\nMissMech\nTo test multivariate normality and homoscedasticity in the context of missing data\n\n\nunlist\nbase\nTo convert a list to a vector",
    "crumbs": [
      "Missing data analysis",
      "R functions (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html",
    "href": "missingdataQ.html",
    "title": "Quiz (M)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html#live-quiz",
    "href": "missingdataQ.html#live-quiz",
    "title": "Quiz (M)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html#download-quiz",
    "href": "missingdataQ.html#download-quiz",
    "title": "Quiz (M)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataS.html",
    "href": "missingdataS.html",
    "title": "App (M)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from various imputations as well as pooled results from multiple imputation.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveM\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, and ggplot2 packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Missing data analysis",
      "App (M)"
    ]
  },
  {
    "objectID": "missingdataE.html",
    "href": "missingdataE.html",
    "title": "Exercise 1 (M)",
    "section": "",
    "text": "Problem Statement\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n4 Outcome variables\n4 predictors (i.e., exposure variables)\nConfounders and other variables",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#problem-statement",
    "href": "missingdataE.html#problem-statement",
    "title": "Exercise 1 (M)",
    "section": "",
    "text": "id: Respondent sequence number\nsurvey.weight: Full sample 4 year interview weight\npsu: Masked pseudo PSU\nstrata: Masked pseudo strata (strata is nested within PSU)\n\n\n\nweight.loss.behavior: doing lifestyle behavior changes - controlling or losing weight\nexercise.behavior: doing lifestyle behavior changes - increasing exercise\nsalt.behavior: doing lifestyle behavior changes - reducing salt in diet\nfat.behavior: doing lifestyle behavior changes - reducing fat in diet\n\n\n\nweight.loss.advice: told by a doctor or health professional - to control/lose weight\nexercise.advice: told by a doctor or health professional - to exercise\nsalt.advice: told by a doctor or health professional - to reduce salt in diet\nfat.advice: told by a doctor or health professional - to reduce fat/calories\n\n\n\ngender: Gender\nage: Age in years at screening\nincome: The ratio of family income to federal poverty level\nrace: Race/Ethnicity\nbmi: Body Mass Index in kg/m\\(^2\\)\n\ncomorbidity: Comorbidity index\nDIQ010: Self-report to have been informed by a provider to have diabetes\nBPQ020: Self-report to have been informed by a provider to have hypertension",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-1-analytic-dataset",
    "href": "missingdataE.html#question-1-analytic-dataset",
    "title": "Exercise 1 (M)",
    "section": "Question 1: Analytic dataset",
    "text": "Question 1: Analytic dataset\n1(a) Importing dataset\n\n# download the data in the same folder\nload(\"Data/missingdata/Williams2021.RData\")\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\n# Drop &lt; 18 years\ndat &lt;- dat.full\ndat &lt;- dat[dat$age &gt;= 18,] \n\n# Eligibility\ndat &lt;- dat[dat$DIQ010==\"Yes\" | dat$BPQ020==\"Yes\",] \n\n# Dataset with missing values in outcomes, predictors, and confounders\ndat.with.miss &lt;- dat\nnrow(dat.with.miss) # N = 4,746\n#&gt; [1] 4746\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the ‚ÄúSelf-reported behavior change and receipt of advice‚Äù paragraph.\n\n\ndat &lt;- dat.with.miss\n\n# Drop missing or don't know outcomes \ndat &lt;- dat[complete.cases(dat$weight.loss.behavior),]\ndat &lt;- dat[complete.cases(dat$exercise.behavior),]\ndat &lt;- dat[complete.cases(dat$salt.behavior),]\ndat &lt;- dat[complete.cases(dat$fat.behavior),]\n\n# Drop missing or don't know predictors\ndat &lt;- dat[complete.cases(dat$weight.loss.advice),]\ndat &lt;- dat[complete.cases(dat$exercise.advice),]\ndat &lt;- dat[complete.cases(dat$salt.advice),]\ndat &lt;- dat[complete.cases(dat$fat.advice),] \n\n# Dataset without missing in outcomes and predictors but missing in confounders \ndat.with.miss2 &lt;- dat\nnrow(dat.with.miss2) # N = 4,716\n#&gt; [1] 4716\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nHint 2: You may need to generate the Condition variable.\nHint 3: age and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\ndat &lt;- dat.with.miss2\n\n# Create the condition variable\ndat$condition &lt;- NA\ndat$condition[dat$BPQ020 == \"Yes\"] &lt;- \"Hypertension Only\"\ndat$condition[dat$DIQ010 == \"Yes\"] &lt;- \"Diabetes Only\"\ndat$condition[dat$BPQ020 == \"Yes\" & dat$DIQ010 == \"Yes\"] &lt;- \"Both\"\ndat$condition &lt;- factor(dat$condition, levels=c(\"Hypertension Only\", \"Diabetes Only\",\n                                                \"Both\"))\ntable(dat$condition, useNA = \"always\")\n#&gt; \n#&gt; Hypertension Only     Diabetes Only              Both              &lt;NA&gt; \n#&gt;              3004               533              1179                 0\n\n\n# First column of Table 1\nvars &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 &lt;- CreateTableOne(vars = vars, data = dat, includeNA = F)\nprint(tab1, format = \"f\")\n#&gt;                          \n#&gt;                           Overall      \n#&gt;   n                        4716        \n#&gt;   gender = Male            2332        \n#&gt;   age (mean (SD))         59.94 (14.96)\n#&gt;   income                               \n#&gt;      &lt;100%                  881        \n#&gt;      100-199%              1193        \n#&gt;      200-299%               672        \n#&gt;      300-399%               424        \n#&gt;      400+%                  930        \n#&gt;   race                                 \n#&gt;      Hispanic              1161        \n#&gt;      Non-Hispanic white    1630        \n#&gt;      Non-Hispanic black    1239        \n#&gt;      Others                 686        \n#&gt;   bmi                                  \n#&gt;      Reference              753        \n#&gt;      Overweight            1372        \n#&gt;      Obese                 2287        \n#&gt;   condition                            \n#&gt;      Hypertension Only     3004        \n#&gt;      Diabetes Only          533        \n#&gt;      Both                  1179        \n#&gt;   comorbidity (mean (SD))  1.29 (1.45)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "href": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "title": "Exercise 1 (M)",
    "section": "Question 2: Dealing with missing values in confoudners [100% grade]",
    "text": "Question 2: Dealing with missing values in confoudners [100% grade]\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nHint 1: There are four outcome variables and four predictor variables used in the study.\nHint 2: The authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\n# Create the condition variable in the analytic dataset\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\"] &lt;- \"Hypertension Only\"\ndat.with.miss2$condition[dat.with.miss2$DIQ010 == \"Yes\"] &lt;- \"Diabetes Only\"\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\" & \n                          dat.with.miss2$DIQ010 == \"Yes\"] &lt;- \"Both\"\ndat.with.miss2$condition &lt;- factor(dat.with.miss2$condition, \n                                  levels=c(\"Hypertension Only\", \"Diabetes Only\", \"Both\"))\n\n# Variables of interest\nvars &lt;- c(\n  # Outcome\n  \"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\",\n  \n  #Predictors\n  \"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\",\n   \n  # Confounders       \n  \"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\n\n# Plot missing values using DataExplorer\nplot_missing(dat.with.miss2[,vars])\n\n\n\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nPerform multiple imputations to deal with missing values only in confounders. Use the dataset created in Dataset with missing values only in confounders (dat.with.miss2). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to lose weights, i.e., create only the first column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types. lapply function could be helpful.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 5: Set your seed to 123.\nHint 6: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 7: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 8: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Setup the data such that the variables are of appropriate types\nfactor.names &lt;- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \n                  \"fat.behavior\", \"weight.loss.advice\", \"exercise.advice\", \n                  \"salt.advice\", \"fat.advice\", \"gender\", \"income\", \"race\", \"bmi\", \n                  \"condition\")\n# your codes \n\n\n## Change the reference categories\n# your codes\n\n\n## Imputation model set up\n# your codes\n\n\n## Regression analysis\n# your codes\n\n\n## Pooled estimates\n# your codes",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "href": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "title": "Exercise 1 (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Include all 4 outcomes and 4 predictors in your imputation model.\nHint 5: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 6: Set your seed to 123.\nHint 7: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 8: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 9: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Create a missing indicator so that MID can be applied\n# your codes here\n\n## MID\n# your codes here",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html",
    "href": "missingdataEsolution.html",
    "title": "Exercise 1 Solution (M)",
    "section": "",
    "text": "Question 1: Analytic dataset\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n4 Outcome variables\n4 predictors (i.e., exposure variables)\nConfounders and other variables",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-1-analytic-dataset",
    "href": "missingdataEsolution.html#question-1-analytic-dataset",
    "title": "Exercise 1 Solution (M)",
    "section": "",
    "text": "1(a) Importing dataset\n\n# 1(a) Importing dataset\nload(\"Data/missingdata/Williams2021.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\n\ndim(dat.full)\n#&gt; [1] 19225    20\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\n# 1(b) Subsetting according to eligibility (N = 4,746)\ndat.analytic &lt;- dat.full %&gt;%\n  filter(age &gt;= 18, DIQ010 == \"Yes\" | BPQ020 == \"Yes\")\n\n# Dataset with missing values in outcomes, predictors, and confounders\nnrow(dat.analytic) # N = 4,746\n#&gt; [1] 4746\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the ‚ÄúSelf-reported behavior change and receipt of advice‚Äù paragraph.\n\n\n# 1(c) Dataset with no missingness in outcomes/predictors (N = 4,716)\noutcome_vars &lt;- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\")\npredictor_vars &lt;- c(\"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\")\n\ndat.with.miss &lt;- dat.analytic %&gt;%\n  filter(complete.cases(.[, c(outcome_vars, predictor_vars)]))\n\n# Dataset without missing in outcomes and predictors but missing in confounders \nnrow(dat.with.miss) # N = 4,716\n#&gt; [1] 4716\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nThe authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nYou may need to generate the Condition variable.\nage and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\n# 1(d) Reproduce Table 1\ndat.with.miss &lt;- dat.with.miss %&gt;%\n  mutate(condition = case_when(\n    BPQ020 == \"Yes\" & DIQ010 == \"Yes\" ~ \"Both\",\n    DIQ010 == \"Yes\"                   ~ \"Diabetes Only\",\n    BPQ020 == \"Yes\"                   ~ \"Hypertension Only\"\n  )) %&gt;%\n  mutate(condition = factor(condition, levels = c(\"Hypertension Only\", \"Diabetes Only\", \"Both\")))\n\n\nvars_for_table1 &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 &lt;- CreateTableOne(vars = vars_for_table1, data = dat.with.miss, includeNA = FALSE)\nprint(tab1, format = \"f\")\n#&gt;                          \n#&gt;                           Overall      \n#&gt;   n                        4716        \n#&gt;   gender = Male            2332        \n#&gt;   age (mean (SD))         59.94 (14.96)\n#&gt;   income                               \n#&gt;      &lt;100%                  881        \n#&gt;      100-199%              1193        \n#&gt;      200-299%               672        \n#&gt;      300-399%               424        \n#&gt;      400+%                  930        \n#&gt;   race                                 \n#&gt;      Hispanic              1161        \n#&gt;      Non-Hispanic white    1630        \n#&gt;      Non-Hispanic black    1239        \n#&gt;      Others                 686        \n#&gt;   bmi                                  \n#&gt;      Reference              753        \n#&gt;      Overweight            1372        \n#&gt;      Obese                 2287        \n#&gt;   condition                            \n#&gt;      Hypertension Only     3004        \n#&gt;      Diabetes Only          533        \n#&gt;      Both                  1179        \n#&gt;   comorbidity (mean (SD))  1.29 (1.45)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-2-dealing-with-missing-values-in-confoudners",
    "href": "missingdataEsolution.html#question-2-dealing-with-missing-values-in-confoudners",
    "title": "Exercise 1 Solution (M)",
    "section": "Question 2: Dealing with missing values in confoudners",
    "text": "Question 2: Dealing with missing values in confoudners\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nThere are four outcome variables and four predictor variables used in the study.\nThe authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\n# 2(a) Check missingness using a plot\nconfounder_vars &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\nplot_missing(dat.with.miss[, c(outcome_vars, predictor_vars, confounder_vars)])\n\n\n\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nLet‚Äôs we are interested in exploring the relationship between weight loss advice (exposure) and weight loss behavior (outcome). Perform multiple imputations to deal with missing values only in confounders. Use the dataset dat.with.miss.\nConsider:\n\n5 imputed datasets\n10 iterations\nFit the design-adjusted logistic regression in all of the 5 imputed datasets\nObtain the pooled adjusted odds ratio with the 95% confidence intervals, i.e., create only the first column of Table 3.\n\nYou must:\n\nSetup the data such that the variables are of appropriate types (e.g., factors, numeric). lapply function could be helpful.\nRelevel the confounders as shown in Table 3.\nUse the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nThere are four exposure and four outcome variables in the dataset. Include all these variables in the imputation model.\nConsider predictive mean matching (pmm) method for bmi and comorbidity variable in the imputation model.\nSet your seed to 123.\nRemove any subject ID variable from the imputation model, if created in an intermediate step.\n\nHints:\n\nThe point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nRemember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n# Helper Function to Reduce Duplication\n# This function automates the post-analysis process for imputed data.\n# It takes a 'mira' object (the result of running a model on multiple imputations)\n# and returns a clean, formatted table of odds ratios and confidence intervals.\npool_and_format_results &lt;- function(fit_mira) {\n  # 'MIcombine' applies Rubin's Rules to pool the estimates from each imputed dataset.\n  pooled &lt;- MIcombine(fit_mira)\n  \n  # Exponentiate the log-odds coefficients and confidence intervals to get Odds Ratios (ORs).\n  or_estimates &lt;- exp(coef(pooled))\n  ci_estimates &lt;- exp(confint(pooled))\n  \n  # Create and return a final data frame, rounding the results for clarity.\n  results_df &lt;- data.frame(\n    OR = round(or_estimates, 2),\n    `2.5 %` = round(ci_estimates[, 1], 2),\n    `97.5 %` = round(ci_estimates[, 2], 2)\n  )\n  return(results_df)\n}\n\n#----------------------------------------------------------------\n# Question 2(b) Reproduce Table 3 (First Column)\n#----------------------------------------------------------------\n\n# Convert character variables to factors for regression modeling.\nfactor_vars &lt;- c(outcome_vars, predictor_vars, \"gender\", \"income\", \"race\", \"bmi\", \"condition\")\ndat.with.miss[, factor_vars] &lt;- lapply(dat.with.miss[, factor_vars], factor)\n\n# Set the reference levels for categorical variables to match the study's analysis.\n# This ensures the odds ratios are interpreted correctly (e.g., comparing 'Female' to 'Male').\ndat.with.miss &lt;- dat.with.miss %&gt;%\n  mutate(\n    gender = relevel(gender, ref = \"Male\"),\n    income = relevel(income, ref = \"400+%\"),\n    race = relevel(race, ref = \"Non-Hispanic white\"),\n    condition = relevel(condition, ref = \"Both\")\n  )\n\n# Remove the original variables used to create 'condition' to prevent\n# perfect multicollinearity during the imputation step.\ndat_for_imputation &lt;- dat.with.miss %&gt;%\n  select(-DIQ010, -BPQ020)\n\n# Run the multiple imputation using the 'mice' package.\n# m = 5: Creates 5 imputed datasets.\n# maxit = 10: Runs 10 iterations for the Gibbs sampler to converge.\n# meth = c(bmi = \"pmm\"): Specifies predictive mean matching for imputing BMI.\nmeth &lt;- make.method(dat_for_imputation)\nmeth[\"bmi\"] &lt;- \"pmm\"\nmeth[\"comorbidity\"] &lt;- \"pmm\"\nimputation_q2 &lt;- mice(dat_for_imputation, m = 5, maxit = 10, seed = 123, print = FALSE,\n                      meth = meth)\n\n# Extract the 5 imputed datasets into a single \"long\" format data frame.\nimpdata_q2 &lt;- complete(imputation_q2, \"long\", include = FALSE)\n\n\n# Re-integrate Ineligible Subjects for Correct Survey Variance\n# This entire block is necessary to correctly estimate variance with survey data.\n# The 'svydesign' object needs the full sample structure, even those outside the analysis.\n\n# Add a flag to identify the analytic (eligible) group in the imputed data.\nimpdata_q2$eligible &lt;- 1\n\n# 1. Isolate all subjects from the original dataset who were not in our analytic sample.\ndat_ineligible &lt;- filter(dat.full, !(id %in% dat.with.miss$id))\n\n# 2. Replicate the ineligible dataset 5 times, once for each imputation (.imp = 1 to 5).\nineligible_list &lt;- lapply(1:5, function(i) {\n  df &lt;- dat_ineligible\n  df$.imp &lt;- i\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# 3. Find which columns exist in the imputed data but not in the ineligible data.\ncols_to_add &lt;- setdiff(names(impdata_q2), names(ineligible_stacked))\n\n# 4. Add these missing columns (e.g., 'condition', imputed values) to the ineligible data, filling with NA.\nineligible_stacked[, cols_to_add] &lt;- NA\n\n# 5. Set the eligibility flag for this group to 0.\nineligible_stacked$eligible &lt;- 0\n\n# 6. CRITICAL: Force the ineligible data to have the exact same column names and order\n#    as the imputed data. This prevents errors when row-binding.\nineligible_final &lt;- ineligible_stacked[, names(impdata_q2)]\n\n# 7. Combine the imputed analytic data with the prepared ineligible data.\nimpdata2_full &lt;- rbind(impdata_q2, ineligible_final)\n\n\n# --- Survey Analysis --- #\n\n# Create the complex survey design object.\n# 'imputationList' tells the survey package how to handle the 5 imputed datasets.\n# The design is specified on the *full* data to capture the total population structure.\ndesign_full &lt;- svydesign(ids = ~psu, weights = ~survey.weight, strata = ~strata,\n                         data = imputationList(split(impdata2_full, impdata2_full$.imp)), nest = TRUE)\n\n# Subset the design object to include only the eligible participants for analysis.\ndesign_analytic &lt;- subset(design_full, eligible == 1)\n\n# Fit the logistic regression model across each of the 5 imputed datasets.\n# 'with()' applies the 'svyglm' function to each dataset in the 'design_analytic' object.\nfit_q2 &lt;- with(design_analytic,\n               svyglm(I(weight.loss.behavior == \"Yes\") ~ weight.loss.advice + gender + age +\n                        income + race + bmi + condition + comorbidity, family = binomial(\"logit\")))\n\n# Use our helper function to pool the 5 models and display the final formatted results.\npool_and_format_results(fit_q2)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners",
    "href": "missingdataEsolution.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners",
    "title": "Exercise 1 Solution (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nSetup the data such that the variables are of appropriate types.\nRelevel the confounders as shown in Table 3.\nUse the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nInclude all 4 outcomes and 4 predictors in your imputation model.\nConsider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nSet your seed to 123.\nRemove any subject ID variable from the imputation model, if created in an intermediate step.\nThe point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nRemember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n#----------------------------------------------------------------\n# Question 3: Dealing with Missing Values in All Variables (MID)\n#----------------------------------------------------------------\n\n# For this question, we start with 'dat.analytic' (N=4,746), which includes\n# subjects who have missing values in the outcome and predictor variables.\n\n# STEP 1: Prepare the data for imputation.\n# This involves creating the 'condition' variable and setting correct factor levels.\ndat_q3_prepped &lt;- dat.analytic %&gt;%\n  mutate(\n    # Create the 'condition' variable based on diabetes and hypertension status.\n    condition = case_when(\n      BPQ020 == \"Yes\" & DIQ010 == \"Yes\" ~ \"Both\",\n      DIQ010 == \"Yes\"                   ~ \"Diabetes Only\",\n      BPQ020 == \"Yes\"                   ~ \"Hypertension Only\"\n    ),\n    # Set the reference levels for categorical variables to ensure correct interpretation of model results.\n    condition = factor(condition, levels = c(\"Hypertension Only\", \"Diabetes Only\", \"Both\")),\n    gender = relevel(as.factor(gender), ref = \"Male\"),\n    income = relevel(as.factor(income), ref = \"400+%\"),\n    race = relevel(as.factor(race), ref = \"Non-Hispanic white\"),\n    condition = relevel(condition, ref = \"Both\")\n  )\n\n# STEP 2: Convert all relevant columns to the factor data type *after* creating 'condition'.\n# This avoids the \"undefined columns selected\" error.\nfactor_vars &lt;- c(outcome_vars, predictor_vars, \"gender\", \"income\", \"race\", \"bmi\", \"condition\")\ndat_q3_prepped[, factor_vars] &lt;- lapply(dat_q3_prepped[, factor_vars], factor)\n\n# STEP 3: Remove the original source variables to prevent perfect multicollinearity during imputation.\ndat_for_imputation_q3 &lt;- dat_q3_prepped %&gt;%\n  select(-DIQ010, -BPQ020)\n\n# STEP 4: Perform multiple imputation on the prepared dataset.\n# This will impute missing values in outcomes, predictors, and confounders.\nimputation_q3 &lt;- mice(dat_for_imputation_q3, m = 5, maxit = 5, seed = 123, print = FALSE,\n                     meth = meth) # Use predictive mean matching for BMI.\n\n# Extract the 5 complete datasets into a single \"long\" data frame.\nimpdata_q3 &lt;- complete(imputation_q3, \"long\", include = FALSE)\n\n\n# Re-integrate Ineligible Subjects for Correct Survey Variance \n# The survey design requires the full sample structure to correctly estimate variance.\n\n# Add an 'eligible' flag to the imputed analytic data.\nimpdata_q3$eligible &lt;- 1\n\n# Isolate subjects from the original full dataset who were not part of this analysis.\ndat_ineligible_q3 &lt;- filter(dat.full, !(id %in% dat.analytic$id))\n\n# Replicate the ineligible dataset 5 times, one for each imputation set.\nineligible_list &lt;- lapply(1:5, function(i) {\n  df &lt;- dat_ineligible_q3\n  df$.imp &lt;- i      # Add imputation number\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# Align the structure of the ineligible data to perfectly match the imputed data.\ncols_to_add &lt;- setdiff(names(impdata_q3), names(ineligible_stacked))\nineligible_stacked[, cols_to_add] &lt;- NA # Add missing columns (e.g., 'condition', '.id') as NA.\nineligible_stacked$eligible &lt;- 0         # Set eligibility flag for this group.\n\n# CRITICAL: Reorder columns to prevent 'rbind' errors.\nineligible_final &lt;- ineligible_stacked[, names(impdata_q3)]\n\n# Combine the imputed analytic data and the prepared ineligible data.\nimpdata3_full &lt;- rbind(impdata_q3, ineligible_final)\n\n\n# --- Survey Analysis using Multiple Imputation then Deletion (MID) --- #\n\n# Create the complex survey design object on the full combined data.\ndesign_full_q3 &lt;- svydesign(ids = ~psu, weights = ~survey.weight, strata = ~strata,\n                         data = imputationList(split(impdata3_full, impdata3_full$.imp)), nest = TRUE)\n\n# Apply the \"Deletion\" step of MID: subset the design to the analytic group\n# AND only those with complete data for the specific variables in this model.\ndesign_analytic_q3 &lt;- subset(design_full_q3, eligible == 1 & complete.cases(fat.behavior, fat.advice))\n\n# Fit the logistic regression model across each of the 5 imputed datasets.\nfit_q3 &lt;- with(design_analytic_q3,\n               svyglm(I(fat.behavior == \"Yes\") ~ fat.advice + gender + age + income +\n                        race + bmi + condition + comorbidity, family = binomial(\"logit\")))\n\n# Use our helper function to pool the results and display the final formatted table.\npool_and_format_results(fit_q3)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "propensityscore.html",
    "href": "propensityscore.html",
    "title": "Propensity score",
    "section": "",
    "text": "Background\nThis chapter provides a comprehensive set of tutorials that guide readers through various methodologies of Propensity Score Matching (PSM) and Multiple Imputation (MI) using R, with practical applications using datasets like the Canadian Community Health Survey (CCHS) and the National Health and Nutrition Examination Survey (NHANES). The tutorials explore different scenarios and methodologies in handling and analyzing data, particularly focusing on estimating treatment effects and managing missing data. They delve into specific examples, such as exploring the relationship between Osteoarthritis (OA) and Cardiovascular Disease (CVD), and between Body Mass Index (BMI) and diabetes, while emphasizing the importance of accurate data handling, variable management, and robust analysis through PSM and MI. The tutorials are meticulously structured, providing step-by-step guides, code snippets, and thorough explanations, ensuring that readers can comprehend and replicate the processes in their research, thereby enhancing the reliability and robustness of their analyses, especially in the presence of missing data.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#background",
    "href": "propensityscore.html#background",
    "title": "Propensity score",
    "section": "",
    "text": "Stepping into this chapter, we are diving deeper into the world of survey data analysis, exploring how to combine propensity score matching (PSM) and strategies for handling missing data. PSM helps us balance our data, making sure our study groups are comparable, while managing missing data ensures our results are as accurate as possible. In the upcoming tutorials, we will weave through the steps of using PSM while also dealing with the gaps in our data, ensuring our analyses are solid and dependable. So, this chapter is not just a next step, but a leap into a more advanced exploration, blending matching methods with careful data handling strategies.\n\n\n\n\n\n\nNote\n\n\n\nShould you find yourself seeking a refresher on PSM, we invite you to revisit our dedicated/external tutorial, which elucidates PSM within a non-survey data analysis context. This resource not only provides a foundational understanding but also serves as a comprehensive guide through the nuanced steps of PSM. Additionally, our external discussion page offers a succinct summary of the tutorial and thoughtfully extends the conversation into more intricate directions, exploring the complexities and advanced applications of PSM (propensity score weighting, categorical and continuous exposure). Both resources are crafted to enhance your understanding and application of PSM, ensuring a robust and informed approach to your data analysis journey\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#overview-of-tutorials",
    "href": "propensityscore.html#overview-of-tutorials",
    "title": "Propensity score",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCovariate matching using CCHS: example of OA-CVD\nThe tutorial illustrate a comprehensive data analysis workflow using R, focusing on matching methods to estimate treatment effects with the CCHS data. Initially, we conduct data pre-processing steps to handle categorical variables and missing data. Subsequent sections delve into setting up design objects for survey-weighted analyses and conducting preliminary analyses to explore variable distributions and treatment effects. The core of the analysis involves implementing matching techniques, starting with a single variable and progressively including more variables to refine the matching. Various matching scenarios are explored, each followed by logistic regression models to estimate treatment effects.\n\n\nPropensity score matching using CCHS: revisiting example of OA-CVD\nThe tutorial provides a thorough walkthrough of implementing Propensity Score Matching (PSM) in R, specifically in the context of an OA - CVD health study from the CCHS. PSM is utilized to mitigate bias from confounding variables in observational studies by pairing treated and control units with analogous propensity scores. The guide underscores that PSM is iterative, often requiring refinement of the matching strategy to achieve satisfactory covariate balance in the matched sample. Various strategies for estimating treatment effects in the matched sample are explored, each with distinct assumptions and implications. The tutorial also delves into different matching strategies, such as nearest-neighbor matching with and without calipers, matching with different ratios, and matching with replacement, all while emphasizing the importance of assessing and re-assessing covariate balance at each step using both graphical and numerical methods.\n\n\nPropensity score matching using NHANES: example of OA - CVD\nThe provided text outlines methodologies for conducting PSM using the NHANES dataset, with a particular emphasis on handling survey design and weights in the analysis. Three distinct approaches, attributed to Zanutto (2006), DuGoff et al.¬†(2014), and Austin et al.¬†(2018), are delineated, each with a structured four-step process: 1) specifying the propensity score model, 2) matching treated and untreated subjects based on estimated propensity scores, 3) comparing baseline characteristics between matched groups, and 4) estimating treatment effects using the matched sample. The procedures utilize various R packages and functions to manipulate data, visualize missing data patterns, format variables, and perform analyses, ensuring that survey weights and design are appropriately considered to avoid bias in population-level effect estimates. The text underscores the importance of incorporating survey design into at least propensity score outcome analysis (e.g., during step 4: treatment effect estimation), as neglecting survey weights can significantly impact the estimates of population-level effects.\n\n\nPropensity score matching using NHANES: example of BMI - diabetes\nThe tutorial provides a comprehensive guide on implementing PSM in R, utilizing the NHANES dataset, with a specific focus on diabetes as an outcome and body mass index (BMI) as an exposure variable. The methodology encompasses ensuring accurate and reproducible results in PSM. The tutorial, again, meticulously follows three distinct approaches for PSM, as recommended by Zanutto (2006), DuGoff et al.¬†(2014), and Austin et al.¬†(2018), each providing a unique perspective on handling and analyzing variables within the propensity score model. Notably, the tutorial introduces a nuanced approach to variable handling, model specifications, and matching steps, ensuring a thorough understanding of implementing PSM with varied methodologies. Furthermore, the tutorial introduces a ‚Äúdouble adjustment‚Äù step in each approach, providing a robust estimate of the treatment effect while adjusting for covariates, thereby offering readers a holistic view on conducting PSM with a different set of variables and methodologies in the analysis steps.\n\n\nPropensity score matching using NHANES when some variables have missing observations\nThis tutorial offers a clear and straightforward guide on how to use Propensity Score Matching (PSM) and Multiple Imputation (MI) in R, using the NHANES dataset for practical illustration. The main goal is to explore the relationship between ‚Äúdiabetes‚Äù (outcome) and being ‚Äúborn in the US‚Äù (exposure), while effectively managing missing data through MI. The first part of the tutorial, focusing on logistic regression, explains how to perform multiple imputations, fit a logistic regression model to all imputed datasets, and then obtain pooled Odds Ratios (OR) and 95% confidence intervals. Following this, the PSM analysis section carefully applies the PSM method, following Zanutto E. L. (2006), to all imputed datasets, and presents the pooled OR estimates and 95% confidence intervals. The tutorial emphasizes the crucial role of managing missing data through multiple imputation and provides a detailed, step-by-step guide, including code and thorough explanations, to ensure a deep understanding and ability to replicate the PSM with MI process in epidemiological research. This resource is invaluable for researchers and data analysts looking to strengthen their analyses when dealing with missing data.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting",
    "href": "propensityscore.html#propensity-score-weighting",
    "title": "Propensity score",
    "section": "Propensity score weighting",
    "text": "Propensity score weighting\nIn this section, propensity score weighting with different methods is employed to estimate treatment effects in a modified NHANES dataset. Three different propensity score weighting approaches are applied: Zanutto‚Äôs (2006), DuGoff et al.‚Äôs (2014), and Austin et al.‚Äôs (2018). Each approach involves multiple steps, including the estimation of propensity scores, the calculation of weights (both unstabilized and stabilized), balance checking to ensure covariate balancing, and fitting outcome models using the weighted data. The double adjustment technique is also considered in each approach.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score matching with multiple imputation in subpopulations",
    "text": "Propensity score matching with multiple imputation in subpopulations\nIn this section, the goal is to use propensity score matching (PSM) with multiple imputation (MI) to analyze a modified dataset from NHANES 2017-2018. The analysis focuses on specific subpopulations defined by eligibility criteria. This analysis provides insights into how to handle complex survey data with missing values and perform PSM with MI for subpopulations defined by eligibility criteria.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score weighting with multiple imputation in subpopulations",
    "text": "Propensity score weighting with multiple imputation in subpopulations\nIn this chapter, we employ propensity score (PS) weighting with MI to analyze a modified dataset from NHANES 2017-2018, focusing on specific subpopulations defined by eligibility criteria. Here we demonstrate how to perform PS weighting with MI for subpopulations defined by eligibility criteria.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "href": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "title": "Propensity score",
    "section": "Propensity score weighting for multiple treatment categories",
    "text": "Propensity score weighting for multiple treatment categories\nIn this chapter, we use propensity score weighting for multiple treatment categories using CCHS data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore0.html",
    "href": "propensityscore0.html",
    "title": "Concepts (S)",
    "section": "",
    "text": "Propensity Score Analysis\nThis section provides a comprehensive exploration into various facets of propensity score (PS) methods and their application in observational studies and surveys. Beginning with an in-depth look into key concepts and calculations related to ATE and ATT, the content navigates through the practical application and diagnostic checks of covariate balance using the SMD. It further elucidates the methodology and application of PS, particularly focusing on matching and weighting to mitigate bias and create comparable groups for causal inference. The intricacies of employing PS methods within surveys are explored, highlighting different approaches and the incorporation of design variables in PS and outcome models. Fundamental assumptions for causal inference, namely Conditional Exchangeability, Positivity, and Causal Consistency, are dissected to form a foundational understanding for conducting robust causal analyses. Additionally, the content optionally delves into the nuances of implementing IPW in surveys. Lastly, additional optional content features an insightful workshop, offering more explanations of PS method implementations in research contexts.",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#reading-list",
    "href": "propensityscore0.html#reading-list",
    "title": "Concepts (S)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Peter C. Austin 2011)\nOptional reading:\n\nPropensity score introduction (Karim 2021) External link\nExtensions of Propensity score approaches External link: prepared for Guest Lecture in SPPH 500/007 (Analytical Methods in Epidemiological Research)\nPropensity score for complex surveys External link: Uses the same lectures here, with some added text descriptions. This also includes a a structured framework for reporting analyses using PS methods in research manuscripts.\nReporting guideline (Stuart 2018; Simoneau et al. 2022)\nAssumptions (Hern√°n and Robins 2020)\n\nTheoretical references for propensity score analyses in complex surveys:\n(Peter C. Austin, Jembere, and Chiu 2018; DuGoff, Schuler, and Stuart 2014; Zanutto 2006; Leite, Stapleton, and Bettini 2018; Lenis, Ackerman, and Stuart 2018; Lenis et al. 2017; Ridgeway et al. 2015)",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#video-lessons",
    "href": "propensityscore0.html#video-lessons",
    "title": "Concepts (S)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nTarget parameters\n\n\n\nAverage Treatment Effect (ATE) vs.¬†Average Treatment effect on the Treated (ATT)\n\n\n\n\n\n\n\n\n\n\n\n\nBalance\n\n\n\nBalance and standardized mean difference (SMD) in observational studies\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score weighting in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConference Workshop (Optional)\n\n\n\nPost Conference Workshop for 2021 Conference - Canadian Society for Epidemiology and Biostatistics (CSEB)",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#video-lesson-slides",
    "href": "propensityscore0.html#video-lesson-slides",
    "title": "Concepts (S)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nTarget parameters\n\n\nBalance\n\n\nPropensity score matching\n\n\nPropensity score matching in complex survey\n\n\nPropensity score weighting in complex survey\n\n\nCausal Assumptions\n\n\nFAQ",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#links",
    "href": "propensityscore0.html#links",
    "title": "Concepts (S)",
    "section": "Links",
    "text": "Links\nTarget parameters\n\nGoogle Slides\nPDF Slides\n\nBalance\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching in complex survey\n\nGoogle Slides\nPDF Slides\n\nPropensity score weighting in complex survey\n\nGoogle Slides\nPDF Slides\n\nCausal Assumptions\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#references",
    "href": "propensityscore0.html#references",
    "title": "Concepts (S)",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C. 2011. ‚ÄúA Tutorial and Case Study in Propensity Score Analysis: An Application to Estimating the Effect of in-Hospital Smoking Cessation Counseling on Mortality.‚Äù Multivariate Behavioral Research 46 (1): 119‚Äì51.\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. ‚ÄúPropensity Score Matching and Complex Surveys.‚Äù Statistical Methods in Medical Research 27 (4): 1240‚Äì57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. ‚ÄúGeneralizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.‚Äù Health Services Research 49 (1): 284‚Äì303.\n\n\nHern√°n, Miguel A., and James M. Robins. 2020. ‚ÄúChapter 3.‚Äù In Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\n\n\nKarim, ME. 2021. ‚ÄúUnderstanding Propensity Score Matching.‚Äù 2021. https://ehsanx.github.io/psw/.\n\n\nLeite, Walter L., Laura M. Stapleton, and Eduardo F. Bettini. 2018. ‚ÄúPropensity Score Analysis of Complex Survey Data with Structural Equation Modeling: A Tutorial with Mplus.‚Äù Structural Equation Modeling: A Multidisciplinary Journal, 1‚Äì22.\n\n\nLenis, Diego, Benjamin Ackerman, and Elizabeth A. Stuart. 2018. ‚ÄúMeasuring Model Misspecification: Application to Propensity Score Methods with Complex Survey Data.‚Äù Computational Statistics & Data Analysis.\n\n\nLenis, Diego, Thuan Quoc Nguyen, Dong, and Elizabeth A. Stuart. 2017. ‚ÄúIt‚Äôs All about Balance: Propensity Score Matching in the Context of Complex Survey Data.‚Äù Biostatistics.\n\n\nRidgeway, Greg, Stephanie A. Kovalchik, Beth Ann Griffin, and Mohammed U. Kabeto. 2015. ‚ÄúPropensity Score Analysis with Survey Weighted Data.‚Äù Journal of Causal Inference 3 (2): 237‚Äì49.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette, Johanna Mu√±oz, Robert W Platt, John Petkau, et al. 2022. ‚ÄúRecommendations for the Use of Propensity Score Methods in Multiple Sclerosis Research.‚Äù Multiple Sclerosis Journal 28 (9): 1467‚Äì80.\n\n\nStuart, Elizabeth A. 2018. ‚ÄúChapter 28. Propensity Scores and Matching Methods.‚Äù In The Reviewer‚Äôs Guide to Quantitative Methods in the Social Sciences, Second Edition, edited by Gregory R. Hancock, Ralph O. Mueller, and Laura M. Stapleton. Routledge.\n\n\nZanutto, Elaine L. 2006. ‚ÄúA Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.‚Äù Journal of Data Science 4 (1): 67‚Äì91.",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore1.html",
    "href": "propensityscore1.html",
    "title": "Exact Matching (CCHS)",
    "section": "",
    "text": "In the following code chunk, we load the necessary R libraries for our analysis. MatchIt is used for matching methods to find comparable control units, tableone for creating Table 1 to describe baseline characteristics, Publish for generating readable output of regression analysis, and survey for analyzing complex survey samples.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(Publish)\nrequire(survey)\n\nLoad data\nIn the following code chunk, we load the CCHS dataset which is related to the Canadian Community Health Survey (CCHS). We then use ls() to list all objects in the workspace and str to display the structure of the data frame, providing a quick overview of the data and checking for any character variables.\n\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"\nstr(analytic.miss) # is there any character variable?\n#&gt; 'data.frame':    397173 obs. of  22 variables:\n#&gt;  $ CVD      : chr  \"event\" \"no event\" \"no event\" \"no event\" ...\n#&gt;  $ age      : chr  \"65 years and over\" \"65 years and over\" \"30-39 years\" \"65 years and over\" ...\n#&gt;  $ sex      : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n#&gt;  $ married  : chr  \"single\" \"single\" \"not single\" \"single\" ...\n#&gt;  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#&gt;  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" ...\n#&gt;  $ income   : chr  \"$29,999 or less\" \"$29,999 or less\" \"$80,000 or more\" \"$29,999 or less\" ...\n#&gt;  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#&gt;  $ phyact   : chr  \"Inactive\" \"Inactive\" \"Inactive\" \"Inactive\" ...\n#&gt;  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n#&gt;  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"stressed\" \"Not too stressed\" ...\n#&gt;  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#&gt;  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#&gt;  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#&gt;  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ weight   : num  142.8 71.4 168.3 71.4 196.1 ...\n#&gt;  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ OA       : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n#&gt;  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\nData pre-pocessing\nIn the following code chunk, we define a vector containing the names of variables of interest that needs to be converted to factor variables. We then convert these variables to factors, ensuring they are treated as categorical in subsequent analyses. We also recode the Osteoarthritis (OA) variable into a numeric binary format and display the frequency table of OA before and after the transformation.\n\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"doctor\", \"drink\", \"bp\", \"province\",\n               \"immigrate\", \"fruit\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic.miss[var.names] &lt;- lapply(analytic.miss[var.names] , factor)\ntable(analytic.miss$OA)\n#&gt; \n#&gt; Control      OA \n#&gt;  314542   40943\nanalytic.miss$OA &lt;- as.numeric(analytic.miss$OA==\"OA\") \ntable(analytic.miss$OA)\n#&gt; \n#&gt;      0      1 \n#&gt; 314542  40943\n\nIdentify subjects with missing\nIn the following code chunk, we create a new variable miss and initially assign all its values to 1 in the full dataset (that contains some missing observations). We then adjust this assignment by setting miss to 0 for observations that are also present in another complete case dataset. That means any row with miss equal to 0 means that row has no missing observations. Finally, we display the frequency table of the miss variable to check the number of missing and non-missing observations.\n\nanalytic.miss$miss &lt;- 1\nhead(analytic.miss$ID) # full data\n#&gt; [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#&gt; [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#&gt; [1]  3  5  7 10 11 13\n# if associated with complete case, assign miss &lt;- 0\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] &lt;- 0\ntable(analytic.miss$miss)\n#&gt; \n#&gt;      0      1 \n#&gt; 185613 211560\n\nSetting Design\nUnconditional design\nIn the following code chunk, we explore the summary of the weight variable and establish an unconditional survey design object w.design0 using the svydesign function, which will be used for subsequent survey-weighted analyses. We then explore the summary, standard deviation, and sum of the weights within our design object.\n\nsummary(analytic.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   65.28  126.63  200.09  243.21 7154.95\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   65.28  126.63  200.09  243.21 7154.95\nsd(weights(w.design0))\n#&gt; [1] 241.0279\nsum(weights(w.design0))\n#&gt; [1] 79468929\n\nConditioning the design\nIn the following code chunk, we create a new survey design object w.design by subsetting w.design0 to only include observations without missing data (miss == 0). We then explore the summary, standard deviation, and sum of the weights within this new design object.\n\nw.design &lt;- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   71.56  137.95  214.61  261.91 7154.95\nsd(weights(w.design))\n#&gt; [1] 254.9346\nsum(weights(w.design))\n#&gt; [1] 39835061\n\nSubset data (more!)\nWe subset the data for fast results (less computation). We will only work with cycle 1.1, and the people from Northern provinces in Canada.\n\nw.design1 &lt;- subset(w.design, cycle == 11 & province == \"North\")\nsum(weights(w.design1))\n#&gt; [1] 42786.28\n\nPreliminary analysis\nTable 1\nIn the following code chunk, we define a new variable vector var.names and create a categorical table using svyCreateCatTable to explore the distribution of age and sex across strata of OA within our subsetted design object w.design1. We then print the table with standardized mean differences (SMD) to assess the balance of these variables across strata.\n\nvar.names &lt;- c(\"age\", \"sex\")\ntab0 &lt;- svyCreateCatTable(var = var.names, strata= \"OA\", data=w.design1,test=FALSE)\nprint(tab0, smd = TRUE)\n#&gt;                       Stratified by OA\n#&gt;                        0               1              SMD   \n#&gt;   n                    40691.2         2095.1               \n#&gt;   age (%)                                              1.084\n#&gt;      20-29 years       10889.4 (26.8)   120.9 ( 5.8)        \n#&gt;      30-39 years       12251.7 (30.1)   237.8 (11.3)        \n#&gt;      40-49 years       11094.0 (27.3)   572.7 (27.3)        \n#&gt;      50-59 years        5346.6 (13.1)  1092.4 (52.1)        \n#&gt;      60-64 years        1109.4 ( 2.7)    71.4 ( 3.4)        \n#&gt;      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   sex = Male (%)       20824.6 (51.2)  1050.8 (50.2)   0.020\n\nTreatment effect\nIn the following code chunk, we fit a logistic regression model using svyglm to estimate the effect of OA and other covariates on the binary outcome CVD (cardiovascular disease). We then use publish to display the results in a readable format.\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design1,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#&gt;   Variable             Units OddsRatio         CI.95    p-value \n#&gt;         OA                        0.89   [0.17;4.59]   0.887411 \n#&gt;        age       20-29 years       Ref                          \n#&gt;                  30-39 years      2.62  [0.29;23.43]   0.389521 \n#&gt;                  40-49 years      4.89  [0.59;40.73]   0.142280 \n#&gt;                  50-59 years     17.95 [2.59;124.68]   0.003550 \n#&gt;                  60-64 years     23.95 [3.41;168.27]   0.001439 \n#&gt;        sex            Female       Ref                          \n#&gt;                         Male      1.32   [0.64;2.71]   0.456222 \n#&gt;     stress  Not too stressed       Ref                          \n#&gt;                     stressed      0.54   [0.21;1.39]   0.198815 \n#&gt;    married        not single       Ref                          \n#&gt;                       single      0.75   [0.31;1.80]   0.513807 \n#&gt;     income   $29,999 or less       Ref                          \n#&gt;              $30,000-$49,999      0.72   [0.24;2.16]   0.556703 \n#&gt;              $50,000-$79,999      0.95   [0.27;3.40]   0.939104 \n#&gt;              $80,000 or more      0.47   [0.10;2.15]   0.332557 \n#&gt;       race         Non-white       Ref                          \n#&gt;                        White      0.33   [0.11;0.94]   0.038131 \n#&gt;        bmi       Underweight       Ref                          \n#&gt;               healthy weight      0.29   [0.03;3.20]   0.310237 \n#&gt;                   Overweight      0.44   [0.04;4.77]   0.503130 \n#&gt;     phyact            Active       Ref                          \n#&gt;                     Inactive      0.84   [0.30;2.40]   0.751345 \n#&gt;                     Moderate      1.02   [0.32;3.27]   0.979528 \n#&gt;      smoke      Never smoker       Ref                          \n#&gt;               Current smoker      0.98   [0.26;3.76]   0.981454 \n#&gt;                Former smoker      0.71   [0.18;2.71]   0.612518 \n#&gt;  immigrate     not immigrant       Ref                          \n#&gt;                   &gt; 10 years      0.14   [0.03;0.78]   0.025010 \n#&gt;                       recent      0.00   [0.00;0.00]    &lt; 1e-04 \n#&gt;      fruit 0-3 daily serving       Ref                          \n#&gt;            4-6 daily serving      1.15   [0.52;2.56]   0.725722 \n#&gt;             6+ daily serving      0.68   [0.17;2.71]   0.583752 \n#&gt;       diab                No       Ref                          \n#&gt;                          Yes      3.08  [0.93;10.23]   0.066677 \n#&gt;        edu          &lt; 2ndary       Ref                          \n#&gt;                    2nd grad.      4.12  [0.87;19.43]   0.074178 \n#&gt;              Other 2nd grad.      3.04  [0.63;14.67]   0.167135 \n#&gt;               Post-2nd grad.      3.00  [0.82;10.98]   0.096939\n\nMatching: Estimating treatment effect\nGoing back to the data (not working on design here while matching)\nIn the following code chunk, we create a new dataset by omitting NA values from analytic.miss and converting it to a data frame. We then create a subset analytic11n which includes only observations from cycle 1.1 and the Northern provinces. We display the dimensions of this subset, as well as frequency tables of OA and a cross-tabulation of OA and age to understand the distribution of our target variable and a key covariate.\n\n# Create the dataset without design features\nanalytic2 &lt;- as.data.frame(na.omit(analytic.miss))\nanalytic11n &lt;- subset(analytic2, cycle == 11 & province == \"North\")\ndim(analytic11n)\n#&gt; [1] 1424   23\ntable(analytic11n$OA)\n#&gt; \n#&gt;    0    1 \n#&gt; 1357   67\ntable(analytic11n$OA,analytic11n$age)\n#&gt;    \n#&gt;     20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#&gt;   0         345         432         358         177          45\n#&gt;   1           4          11          18          31           3\n#&gt;    \n#&gt;     65 years and over teen\n#&gt;   0                 0    0\n#&gt;   1                 0    0\n\nMatching by 1 matching variable\nIn the following code chunk, we perform exact matching using a single variable, age. We define the matching formula and apply the matchit function to create matched sets of treated and control units. The resulting matching.obj object is displayed to summarize the matching results.\n\nmatch.formula &lt;- as.formula(\"OA ~ age\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1424 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age\n\nMatching by 2 matching variables\nIn the following code chunk, we extend the matching to include two variables, age and sex. We create a new variable var.comb that concatenates these two variables and display its frequency table and the number of unique combinations. We then perform exact matching using both variables and display the resulting object.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex')])\ntable(var.comb)\n#&gt; var.comb\n#&gt; 20-29 yearsFemale   20-29 yearsMale 30-39 yearsFemale   30-39 yearsMale \n#&gt;               184               165               220               223 \n#&gt; 40-49 yearsFemale   40-49 yearsMale 50-59 yearsFemale   50-59 yearsMale \n#&gt;               187               189               101               107 \n#&gt; 60-64 yearsFemale   60-64 yearsMale \n#&gt;                24                24\nlength(table(var.comb))\n#&gt; [1] 10\nmatch.formula &lt;- as.formula(\"OA ~ age + sex\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1424 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex\n\nMatching by 3 matching variables\nIn the following code chunk, we further extend the matching to include three variables: age, sex, and stress. We explore the unique combinations of these variables and their distribution across levels of OA. We then perform exact matching using these three variables and display the resulting object.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex', 'stress')])\ntable(var.comb)\n#&gt; var.comb\n#&gt; 20-29 yearsFemaleNot too stressed         20-29 yearsFemalestressed \n#&gt;                               157                                27 \n#&gt;   20-29 yearsMaleNot too stressed           20-29 yearsMalestressed \n#&gt;                               147                                18 \n#&gt; 30-39 yearsFemaleNot too stressed         30-39 yearsFemalestressed \n#&gt;                               170                                50 \n#&gt;   30-39 yearsMaleNot too stressed           30-39 yearsMalestressed \n#&gt;                               183                                40 \n#&gt; 40-49 yearsFemaleNot too stressed         40-49 yearsFemalestressed \n#&gt;                               142                                45 \n#&gt;   40-49 yearsMaleNot too stressed           40-49 yearsMalestressed \n#&gt;                               141                                48 \n#&gt; 50-59 yearsFemaleNot too stressed         50-59 yearsFemalestressed \n#&gt;                                72                                29 \n#&gt;   50-59 yearsMaleNot too stressed           50-59 yearsMalestressed \n#&gt;                                78                                29 \n#&gt; 60-64 yearsFemaleNot too stressed         60-64 yearsFemalestressed \n#&gt;                                18                                 6 \n#&gt;   60-64 yearsMaleNot too stressed           60-64 yearsMalestressed \n#&gt;                                20                                 4\nlength(table(var.comb))\n#&gt; [1] 20\ntable(var.comb,analytic11n$OA)\n#&gt;                                    \n#&gt; var.comb                              0   1\n#&gt;   20-29 yearsFemaleNot too stressed 156   1\n#&gt;   20-29 yearsFemalestressed          27   0\n#&gt;   20-29 yearsMaleNot too stressed   144   3\n#&gt;   20-29 yearsMalestressed            18   0\n#&gt;   30-39 yearsFemaleNot too stressed 168   2\n#&gt;   30-39 yearsFemalestressed          49   1\n#&gt;   30-39 yearsMaleNot too stressed   178   5\n#&gt;   30-39 yearsMalestressed            37   3\n#&gt;   40-49 yearsFemaleNot too stressed 130  12\n#&gt;   40-49 yearsFemalestressed          42   3\n#&gt;   40-49 yearsMaleNot too stressed   138   3\n#&gt;   40-49 yearsMalestressed            48   0\n#&gt;   50-59 yearsFemaleNot too stressed  65   7\n#&gt;   50-59 yearsFemalestressed          22   7\n#&gt;   50-59 yearsMaleNot too stressed    67  11\n#&gt;   50-59 yearsMalestressed            23   6\n#&gt;   60-64 yearsFemaleNot too stressed  17   1\n#&gt;   60-64 yearsFemalestressed           5   1\n#&gt;   60-64 yearsMaleNot too stressed    19   1\n#&gt;   60-64 yearsMalestressed             4   0\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1327 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress\n\nMatching by 4 matching variables\nThe process of matching by 4 variables involves creating combinations of the 4 variables, exploring their distributions, and performing exact matching.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income')])\n#table(var.comb)\nlength(table(var.comb))\n#&gt; [1] 76\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 900 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income\n\nMatching by 5 matching variables\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race')])\nlength(table(var.comb))\n#&gt; [1] 146\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income + race\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 616 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income, race\n\nMatching by 6 matching variables\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race','edu')])\nlength(table(var.comb))\n#&gt; [1] 354\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income + race + edu\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 399 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income, race, edu\nOACVD.match.11n &lt;- match.data(matching.obj)\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"income\", \"race\", \"edu\")\ntab1 &lt;- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n,test=FALSE)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     337         62               \n#&gt;   age (%)                                       0.565\n#&gt;      20-29 years         63 (18.7)   4 ( 6.5)        \n#&gt;      30-39 years         61 (18.1)  11 (17.7)        \n#&gt;      40-49 years        127 (37.7)  17 (27.4)        \n#&gt;      50-59 years         82 (24.3)  28 (45.2)        \n#&gt;      60-64 years          4 ( 1.2)   2 ( 3.2)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        211 (62.6)  29 (46.8)   0.322\n#&gt;   stress = stressed (%)  42 (12.5)  17 (27.4)   0.381\n#&gt;   income (%)                                    0.115\n#&gt;      $29,999 or less     69 (20.5)  11 (17.7)        \n#&gt;      $30,000-$49,999     57 (16.9)  13 (21.0)        \n#&gt;      $50,000-$79,999     69 (20.5)  12 (19.4)        \n#&gt;      $80,000 or more    142 (42.1)  26 (41.9)        \n#&gt;   race = White (%)      242 (71.8)  43 (69.4)   0.054\n#&gt;   edu (%)                                       0.146\n#&gt;      &lt; 2ndary            73 (21.7)  11 (17.7)        \n#&gt;      2nd grad.            5 ( 1.5)   2 ( 3.2)        \n#&gt;      Other 2nd grad.      0 ( 0.0)   0 ( 0.0)        \n#&gt;      Post-2nd grad.     259 (76.9)  49 (79.0)\n\nTreatment effect\nConvert data to design\nIn the following code chunk, we create a new variable matched in the analytic.miss dataset to indicate whether an observation was included in the matched dataset OACVD.match.11n. We then create a new survey design object w.design.m that includes only the matched observations for subsequent analyses.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match.11n$ID) # matched data\n#&gt; [1] 399\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n$ID])\n#&gt; [1] 399\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match.11n$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396774    399\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\nOutcome analysis\nThe subsequent code chunks involve fitting logistic regression models to estimate the treatment effect, both in a crude and adjusted manner, respectively. The models are fitted using the matched survey design object and the results are displayed in a readable format.\nCrude\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA,\n                   design = w.design.m,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#&gt;  Variable Units OddsRatio        CI.95  p-value \n#&gt;        OA            3.14 [0.80;12.40]   0.1025\n\nAdjusted\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + \n                        age + sex + stress + income + race + edu,\n                   design = w.design.m,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\npublish(fit.outcome)\n#&gt;  Variable            Units    OddsRatio                        CI.95   p-value \n#&gt;        OA                          2.04                 [0.34;12.16]   0.43593 \n#&gt;       age      20-29 years          Ref                                        \n#&gt;                30-39 years         0.54                  [0.08;3.51]   0.51962 \n#&gt;                40-49 years  30148597.85    [7758796.44;117149349.12]   &lt; 1e-04 \n#&gt;                50-59 years  63290825.96   [12589873.89;318170673.23]   &lt; 1e-04 \n#&gt;                60-64 years         0.31                  [0.01;9.33]   0.49735 \n#&gt;       sex           Female          Ref                                        \n#&gt;                       Male         1.58                  [0.29;8.62]   0.59729 \n#&gt;    stress Not too stressed          Ref                                        \n#&gt;                   stressed         0.15                  [0.01;1.80]   0.13666 \n#&gt;    income  $29,999 or less          Ref                                        \n#&gt;            $30,000-$49,999         0.20                  [0.01;3.84]   0.28640 \n#&gt;            $50,000-$79,999         0.20                  [0.02;1.95]   0.16543 \n#&gt;            $80,000 or more         0.08                  [0.01;0.68]   0.02122 \n#&gt;      race        Non-white          Ref                                        \n#&gt;                      White         1.02                  [0.11;9.45]   0.98723 \n#&gt;       edu         &lt; 2ndary          Ref                                        \n#&gt;                  2nd grad. 845233466.89 [44642865.50;16002996347.89]   &lt; 1e-04 \n#&gt;             Post-2nd grad.  69867459.42    [9660579.88;505296985.12]   &lt; 1e-04\n\nQuestions for the students\n\nLook at all the ORs. Some of them are VERY high. Why?\nLook at the CI in the above table. Some of them are Inf. Why?\nShould we match matching variables in the regression?\nMatching by a lot of variables\nThe code chunks involve performing matching using a large number of variables and estimating the treatment effect using the matched data. The process involves creating matched datasets, converting them to survey design objects, and fitting logistic regression models.\nMatching part in data\n\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu\")\nmatching.obj2 &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj2\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 22 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, immigrate, fruit, diab, edu\nOACVD.match.11n2 &lt;- match.data(matching.obj2)\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"immigrate\", \"fruit\", \"diab\", \"edu\")\ntab2 &lt;- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n2,test=FALSE)\nprint(tab2, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     11         11               \n#&gt;   age (%)                                     &lt;0.001\n#&gt;      20-29 years         3 (27.3)   3 (27.3)        \n#&gt;      30-39 years         1 ( 9.1)   1 ( 9.1)        \n#&gt;      40-49 years         4 (36.4)   4 (36.4)        \n#&gt;      50-59 years         3 (27.3)   3 (27.3)        \n#&gt;      60-64 years         0 ( 0.0)   0 ( 0.0)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)         6 (54.5)   6 (54.5)  &lt;0.001\n#&gt;   stress = stressed (%)  1 ( 9.1)   1 ( 9.1)  &lt;0.001\n#&gt;   married = single (%)   3 (27.3)   3 (27.3)  &lt;0.001\n#&gt;   income (%)                                  &lt;0.001\n#&gt;      $29,999 or less     1 ( 9.1)   1 ( 9.1)        \n#&gt;      $30,000-$49,999     2 (18.2)   2 (18.2)        \n#&gt;      $50,000-$79,999     2 (18.2)   2 (18.2)        \n#&gt;      $80,000 or more     6 (54.5)   6 (54.5)        \n#&gt;   race = White (%)      10 (90.9)  10 (90.9)  &lt;0.001\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      4 (36.4)   4 (36.4)        \n#&gt;      Overweight          7 (63.6)   7 (63.6)        \n#&gt;   phyact (%)                                  &lt;0.001\n#&gt;      Active              3 (27.3)   3 (27.3)        \n#&gt;      Inactive            5 (45.5)   5 (45.5)        \n#&gt;      Moderate            3 (27.3)   3 (27.3)        \n#&gt;   smoke (%)                                   &lt;0.001\n#&gt;      Never smoker        3 (27.3)   3 (27.3)        \n#&gt;      Current smoker      2 (18.2)   2 (18.2)        \n#&gt;      Former smoker       6 (54.5)   6 (54.5)        \n#&gt;   immigrate (%)                               &lt;0.001\n#&gt;      not immigrant      10 (90.9)  10 (90.9)        \n#&gt;      &gt; 10 years          1 ( 9.1)   1 ( 9.1)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                   &lt;0.001\n#&gt;      0-3 daily serving   3 (27.3)   3 (27.3)        \n#&gt;      4-6 daily serving   6 (54.5)   6 (54.5)        \n#&gt;      6+ daily serving    2 (18.2)   2 (18.2)        \n#&gt;   diab = Yes (%)         0 ( 0.0)   0 ( 0.0)  &lt;0.001\n#&gt;   edu (%)                                     &lt;0.001\n#&gt;      &lt; 2ndary            1 ( 9.1)   1 ( 9.1)        \n#&gt;      2nd grad.           0 ( 0.0)   0 ( 0.0)        \n#&gt;      Other 2nd grad.     0 ( 0.0)   0 ( 0.0)        \n#&gt;      Post-2nd grad.     10 (90.9)  10 (90.9)\n\nTreatment effect estimation in design\nCreate design\n\nanalytic.miss$matched2 &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match.11n2$ID) # matched data\n#&gt; [1] 22\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n2$ID])\n#&gt; [1] 22\nanalytic.miss$matched2[analytic.miss$ID %in% OACVD.match.11n2$ID] &lt;- 1\ntable(analytic.miss$matched2)\n#&gt; \n#&gt;      0      1 \n#&gt; 397151     22\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m2 &lt;- subset(w.design0, matched2 == 1)\n\noutcome analysis\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married +\n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design.m2,\n                   family = binomial(logit))\npublish(fit.outcome)\n# Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : \n#   contrasts can be applied only to factors with 2 or more levels\n\nQuestions for the students\n\nWhy the above model not fitting?\nSave data for later use\n\nsave(analytic11n, analytic2, analytic.miss, file=\"Data/propensityscore/cchs123c.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Propensity score",
      "Exact Matching (CCHS)"
    ]
  },
  {
    "objectID": "propensityscore2.html",
    "href": "propensityscore2.html",
    "title": "PSM in OA-CVD (CCHS)",
    "section": "",
    "text": "This tutorial is a comprehensive guide on implementing Propensity Score Matching (PSM) using R, particularly focusing on a OA - CVD health study from the Canadian Community Health Survey (CCHS). This PSM method is used to reduce bias due to confounding variables in observational studies by matching treated and control units with similar propensity scores. The tutorial illustrates that PSM is an iterative process, where researchers may need to refine their matching strategy to achieve satisfactory balance in the matched sample. Different strategies for estimating the treatment effect in the matched sample are explored, each with its own assumptions and implications.\nLoad packages\nAt first, various R packages are loaded to utilize their functions for data manipulation, statistical analysis, and visualization.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\n\nLoad data\nThe dataset is loaded into the R environment. Variables are renamed to avoid conflicts in subsequent analyses.\n\nload(file=\"Data/propensityscore/cchs123c.RData\")\nhead(analytic11n)\n\n\n  \n\n\n\n# later we will create another variable called weights\n# hence to avoid any conflict/ambiguity,\n# renaming weight variable to survey.weight\nanalytic.miss$survey.weight &lt;- analytic.miss$weight\nanalytic11n$survey.weight &lt;- analytic11n$weight\nanalytic.miss$weight &lt;- analytic11n$weight &lt;- NULL\n\nAnalysis\nWe are going to apply propensity score analysis (Matching) in our OA - CVD problem from CCHS. For computation considerations, we will only work with cycle 1.1, and the people from Northern provinces in Canada (analytic11n data).\nStep 1\nSpecify PS\nA logistic regression model formula is specified to calculate the propensity scores (PS), which is the probability of receiving the treatment given the observed covariates.\n\nps.formula &lt;- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                        doctor + drink + bp + \n                         immigrate + fruit + diab + edu\")\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \n               \"income\", \"race\", \"bmi\", \"phyact\", \"smoke\", \n               \"doctor\", \"drink\", \"bp\", \n               \"immigrate\", \"fruit\", \"diab\", \"edu\")\n\nFit model\nThe software fits the PS model using a logistic regression by default. This package actually performs step 1 and 2 with one command matchit.\nLook at the website for arguments of matchit (RDocumentation 2023)]. It looks like this\n\nmatchit(formula, data, model=\"logit\", discard=0, reestimate=FALSE, nearest=TRUE,\n                 replace=FALSE, m.order=2, ratio=1, caliper=0, calclosest=FALSE,\n                 subclass=0, sub.by=\"treat\", mahvars=NULL, exact=FALSE, counter=TRUE, full=FALSE, full.options=list(),...)\n\n\n\n\n\n\n\nTip\n\n\n\nNearest-Neighbor Matching:\nNearest-neighbor matching is a widely used technique in PSM to pair treated and control units based on the proximity of their propensity scores. It is straightforward and computationally efficient, making it a popular choice in many applications of PSM. Nearest-neighbor matching is often termed a ‚Äúgreedy‚Äù algorithm because it matches units in order, without considering the global distribution of propensity scores. Once a match is made, it is not revisited, even if a later unit would have been a better match. The method seeks to minimize bias by creating closely matched pairs but can increase variance if the pool of potential matches is reduced too much (e.g., using a very narrow caliper). It is essential to ensure that there is a common support region where the distributions of propensity scores for treated and control units overlap, ensuring comparability.\n\n\nStep 2\nMatch subjects by PS\nWe are going to match using a Nearest neighbor algorithm. This is a greedy matching algorithm. Note that we are not even defining any caliper.\n\n\n\n\n\n\nTip\n\n\n\nCaliper:\nIn the context of PSM, a caliper is a predefined maximum allowable difference between the propensity scores of matched units. Essentially, it sets a threshold for how dissimilar matched units can be in terms of their propensity scores. When a caliper is used, a treated unit is only matched with a control unit if the absolute difference in their propensity scores is less than or equal to the specified caliper width. The caliper is used to avoid bad matches and thereby minimize bias in the estimated treatment effect. The size of the caliper is crucial. Too wide a caliper may allow poor matches, while too narrow a caliper may result in many units going unmatched. Implementing a caliper involves a trade-off between bias and efficiency. Using a caliper reduces bias by avoiding poor matches but may increase variance by reducing the number of matched pairs available for analysis. Therefore, the use of a caliper in PSM is a strategic decision to enhance the quality of matches and thereby improve the validity of causal inferences drawn from observational data. It is a practical tool to ensure that matched units are sufficiently similar in terms of their propensity scores, reducing the likelihood of bias due to poor matches.\n\n\n\n# set seed\nset.seed(123)\n# match\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score\n#&gt;              - estimated with logistic regression\n#&gt;  - number of obs.: 1424 (original), 134 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\n# create the \"matched\"\" data\nOACVD.match &lt;- match.data(matching.obj)\n# see the dimension\ndim(analytic11n)\n#&gt; [1] 1424   23\ndim(OACVD.match)\n#&gt; [1] 134  26\n\nLet‚Äôs try to understand how this is working.\nExtract matched IDs\n\nm.mat&lt;-matching.obj$match.matrix\nhead(m.mat)\n#&gt;       [,1]    \n#&gt; 17864 \"96719\" \n#&gt; 17921 \"17846\" \n#&gt; 18191 \"97168\" \n#&gt; 18256 \"111999\"\n#&gt; 18264 \"17989\" \n#&gt; 18383 \"108197\"\n\nExtract the matched treated IDs\n\ntreated.id&lt;-as.numeric(row.names(m.mat))\ntreated.id # basically row names\n#&gt;  [1]  17864  17921  18191  18256  18264  18383  18389  18475  39105  96344\n#&gt; [11]  96364  96407  96424  96460  96484  96571  96582  96625  96632  96641\n#&gt; [21]  96657  96686  96693  96696  96705  96734  96795  96840  96913  97027\n#&gt; [31]  97065  97125 108178 108183 108185 108192 111809 111813 111856 111859\n#&gt; [41] 111895 111896 111920 111942 112014 112026 112046 112083 112086 112114\n#&gt; [51] 112122 112151 112167 112189 112197 112215 112232 112245 112275 112284\n#&gt; [61] 112289 112290 112300 112325 112375 126477 126522\n\nExtract the matched untreated IDs\n\nuntreated.id &lt;- as.numeric(m.mat)\nuntreated.id # basically row names\n#&gt;  [1]  96719  17846  97168 111999  17989 108197 112384  17909 126561 111880\n#&gt; [11] 112184  18117  96865  18120  97023 112379  97017 126562  96356 126470\n#&gt; [21] 126385  96374  18203  18262  96972 111924  96354  96983  18235  96882\n#&gt; [31] 112054  18321 112349  18426  38996 126516 111814 112087  96569 111932\n#&gt; [41] 126539  18315  96665  18225 112052 112324 112165  18329  96609 126376\n#&gt; [51]  96474 126570 126547 126343  96680  96558 111931  96718  96533 111823\n#&gt; [61] 112177  17953  17904 111908 111962  96644  96576\n\nExtract the matched treated data\n\ntx &lt;- analytic11n[rownames(analytic11n) %in% treated.id,]\nhead(tx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n  \n\n\n\nExtract the matched untreated data\n\nutx &lt;- analytic11n[rownames(analytic11n) %in% untreated.id,]\nhead(utx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n  \n\n\n\nExtract the matched data altogether\nSimply using match.data is enough (as done earlier).\n\nOACVD.match &lt;- match.data(matching.obj)\n\nAssign match ID\n\nOACVD.match$match.ID &lt;- NA\nOACVD.match$match.ID[rownames(OACVD.match) %in% treated.id] &lt;- 1:length(treated.id)\nOACVD.match$match.ID[rownames(OACVD.match) %in% untreated.id] &lt;- 1:length(untreated.id)\ntable(OACVD.match$match.ID)\n#&gt; \n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#&gt; 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n\nTake a look at individual matches for the first match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 1,])\n\n\n  \n\n\n\nTake a look at individual matches for the second match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 2,])\n\n\n  \n\n\n\nStep 3\nBoth graphical and numerical methods are used to assess the quality of the matches and the balance of covariates in the matched sample.\nExamining PS graphically\nVisually inspect the PS and assess the balance of covariates in the matched sample. Various plots are generated to visualize the distribution of PS across treatment groups and to check the balance of covariates before and after matching.\nmatchit package\n\n# plot(matching.obj) # covariate balance\nplot(matching.obj, type = \"jitter\") # propensity score locations\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(matching.obj, type = \"hist\") #check matched treated vs matched control\n\n\n\n\n\n\nsummrize.output &lt;- summary(matching.obj, standardize = TRUE)\nplot(summrize.output)\n\n\n\n\n\n\n\nOveralp check\n\n# plot propensity scores by exposure group\nplot(density(OACVD.match$distance[OACVD.match$OA==1]), \n     col = \"red\", main = \"\")\nlines(density(OACVD.match$distance[OACVD.match$OA==0]), \n      col = \"blue\", lty = 2)\nlegend(\"topright\", c(\"Non-arthritis\",\"OA\"), \n       col = c(\"red\", \"blue\"), lty=1:2)\n\n\n\n\n\n\n\ncobalt package\nOverlap check in a more convenient way\n\n# different badwidth\nbal.plot(matching.obj, var.name = \"distance\")\n#&gt; Ignoring unknown labels:\n#&gt; ‚Ä¢ colour : \"Treatment\"\n\n\n\n\n\n\n\nLook at the data\n\n# what is distance variable here?\nhead(OACVD.match)\n\n\n  \n\n\n\nNumerical values of PS\n\nsummary(OACVD.match$distance)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.044834 0.099094 0.138969 0.200576 0.611166\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.047344 0.099215 0.135042 0.199279 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.047346 0.098973 0.142897 0.198741 0.611166\n\nQuestion for the students\n\nAre you happy with the matching after reviewing the plots?\nCovariate balance in matched sample\nCovariate balance is assessed numerically using standardized mean differences (SMD).\n\n\n\n\n\n\nTip\n\n\n\nStandardized mean differences: SMD is a versatile and widely used statistical measure that facilitates the comparison of groups in research by providing a scale-free metric of difference and balance. In the context of propensity score matching, achieving low SMD values for covariates after matching is crucial to ensuring the validity of causal inferences drawn from the matched sample.\nBenifits:\n\nSMD is not influenced by the scale of the measured variable, making it suitable for comparing the balance of different variables measured on different scales.\nUnlike hypothesis testing, SMD is not affected by sample size, making it a reliable measure for assessing balance in matched samples.\n\n\n\n\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     67         67               \n#&gt;   age (%)                                      0.190\n#&gt;      20-29 years         4 ( 6.0)   4 ( 6.0)        \n#&gt;      30-39 years        16 (23.9)  11 (16.4)        \n#&gt;      40-49 years        16 (23.9)  18 (26.9)        \n#&gt;      50-59 years        28 (41.8)  31 (46.3)        \n#&gt;      60-64 years         3 ( 4.5)   3 ( 4.5)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        28 (41.8)  32 (47.8)   0.120\n#&gt;   stress = stressed (%) 20 (29.9)  21 (31.3)   0.032\n#&gt;   married = single (%)  22 (32.8)  23 (34.3)   0.032\n#&gt;   income (%)                                   0.183\n#&gt;      $29,999 or less    11 (16.4)  13 (19.4)        \n#&gt;      $30,000-$49,999    19 (28.4)  15 (22.4)        \n#&gt;      $50,000-$79,999    14 (20.9)  12 (17.9)        \n#&gt;      $80,000 or more    23 (34.3)  27 (40.3)        \n#&gt;   race = White (%)      43 (64.2)  44 (65.7)   0.031\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight     18 (26.9)  18 (26.9)        \n#&gt;      Overweight         49 (73.1)  49 (73.1)        \n#&gt;   phyact (%)                                   0.041\n#&gt;      Active             16 (23.9)  16 (23.9)        \n#&gt;      Inactive           40 (59.7)  39 (58.2)        \n#&gt;      Moderate           11 (16.4)  12 (17.9)        \n#&gt;   smoke (%)                                    0.258\n#&gt;      Never smoker       14 (20.9)   9 (13.4)        \n#&gt;      Current smoker     20 (29.9)  27 (40.3)        \n#&gt;      Former smoker      33 (49.3)  31 (46.3)        \n#&gt;   doctor = Yes (%)      51 (76.1)  47 (70.1)   0.135\n#&gt;   drink (%)                                    0.116\n#&gt;      Never drank         2 ( 3.0)   2 ( 3.0)        \n#&gt;      Current drinker    54 (80.6)  51 (76.1)        \n#&gt;      Former driker      11 (16.4)  14 (20.9)        \n#&gt;   bp = Yes (%)           7 (10.4)   5 ( 7.5)   0.105\n#&gt;   immigrate (%)                                0.180\n#&gt;      not immigrant      64 (95.5)  61 (91.0)        \n#&gt;      &gt; 10 years          3 ( 4.5)   6 ( 9.0)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                    0.146\n#&gt;      0-3 daily serving  19 (28.4)  19 (28.4)        \n#&gt;      4-6 daily serving  28 (41.8)  32 (47.8)        \n#&gt;      6+ daily serving   20 (29.9)  16 (23.9)        \n#&gt;   diab = Yes (%)         1 ( 1.5)   4 ( 6.0)   0.238\n#&gt;   edu (%)                                      0.105\n#&gt;      &lt; 2ndary           14 (20.9)  13 (19.4)        \n#&gt;      2nd grad.           1 ( 1.5)   2 ( 3.0)        \n#&gt;      Other 2nd grad.     1 ( 1.5)   1 ( 1.5)        \n#&gt;      Post-2nd grad.     51 (76.1)  51 (76.1)\n\nQuestion for the students\n\nAll SMD &lt; 0.20?\nOther balance measures\nIndividual categories\nIf you want to check balance at each category (not very useful in general situations). We are generally interested if the variables are balanced or not (not categories).\n\nbaltab &lt;- bal.tab(matching.obj)\nbaltab\n#&gt; Balance Measures\n#&gt;                             Type Diff.Adj\n#&gt; distance                Distance   0.0597\n#&gt; age_20-29 years           Binary   0.0000\n#&gt; age_30-39 years           Binary  -0.0746\n#&gt; age_40-49 years           Binary   0.0299\n#&gt; age_50-59 years           Binary   0.0448\n#&gt; age_60-64 years           Binary   0.0000\n#&gt; sex_Male                  Binary   0.0597\n#&gt; stress_stressed           Binary   0.0149\n#&gt; married_single            Binary   0.0149\n#&gt; income_$29,999 or less    Binary   0.0299\n#&gt; income_$30,000-$49,999    Binary  -0.0597\n#&gt; income_$50,000-$79,999    Binary  -0.0299\n#&gt; income_$80,000 or more    Binary   0.0597\n#&gt; race_White                Binary   0.0149\n#&gt; bmi_Underweight           Binary   0.0000\n#&gt; bmi_healthy weight        Binary   0.0000\n#&gt; bmi_Overweight            Binary   0.0000\n#&gt; phyact_Active             Binary   0.0000\n#&gt; phyact_Inactive           Binary  -0.0149\n#&gt; phyact_Moderate           Binary   0.0149\n#&gt; smoke_Never smoker        Binary  -0.0746\n#&gt; smoke_Current smoker      Binary   0.1045\n#&gt; smoke_Former smoker       Binary  -0.0299\n#&gt; doctor_Yes                Binary  -0.0597\n#&gt; drink_Never drank         Binary   0.0000\n#&gt; drink_Current drinker     Binary  -0.0448\n#&gt; drink_Former driker       Binary   0.0448\n#&gt; bp_Yes                    Binary  -0.0299\n#&gt; immigrate_not immigrant   Binary  -0.0448\n#&gt; immigrate_&gt; 10 years      Binary   0.0448\n#&gt; immigrate_recent          Binary   0.0000\n#&gt; fruit_0-3 daily serving   Binary   0.0000\n#&gt; fruit_4-6 daily serving   Binary   0.0597\n#&gt; fruit_6+ daily serving    Binary  -0.0597\n#&gt; diab_Yes                  Binary   0.0448\n#&gt; edu_&lt; 2ndary              Binary  -0.0149\n#&gt; edu_2nd grad.             Binary   0.0149\n#&gt; edu_Other 2nd grad.       Binary   0.0000\n#&gt; edu_Post-2nd grad.        Binary   0.0000\n#&gt; \n#&gt; Sample sizes\n#&gt;           Control Treated\n#&gt; All          1357      67\n#&gt; Matched        67      67\n#&gt; Unmatched    1290       0\n\nIndividual plots\nYou could plot each variables individually\n\nbal.plot(matching.obj, var.name = \"income\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"age\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"race\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"diab\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"immigrate\")\n\n\n\n\n\n\n\nLove plot\n\n# Individual categories again\nlove.plot(baltab, threshold = .2)\n#&gt; Warning: Unadjusted values are missing. This can occur when `un = FALSE` and\n#&gt; `quick = TRUE` in the original call to `bal.tab()`.\n#&gt; Warning: Standardized mean differences and raw mean differences are present in\n#&gt; the same plot. Use the `stars` argument to distinguish between them and\n#&gt; appropriately label the x-axis. See `?love.plot` for details.\n\n\n\n\n\n\n\nRepeat of Step 1-3 again\nCovariate balance is reassessed in each step to ensure the quality of the match.\nAdd caliper\nThe matching process is repeated, this time introducing a caliper to ensure that matches are only made within a specified range of PS.\n\nlogitPS &lt;-  -log(1/OACVD.match$distance - 1) \n# logit of the propensity score\n.2*sd(logitPS) # suggested in the literature\n#&gt; [1] 0.2334615\n\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1,\n                        caliper = .2*sd(logitPS))\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.015)\n#&gt;  - number of obs.: 1424 (original), 128 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n\n\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041740 0.094963 0.124665 0.184103 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     64         64               \n#&gt;   age (%)                                      0.196\n#&gt;      20-29 years         4 ( 6.2)   4 ( 6.2)        \n#&gt;      30-39 years        16 (25.0)  11 (17.2)        \n#&gt;      40-49 years        16 (25.0)  18 (28.1)        \n#&gt;      50-59 years        25 (39.1)  28 (43.8)        \n#&gt;      60-64 years         3 ( 4.7)   3 ( 4.7)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        27 (42.2)  30 (46.9)   0.094\n#&gt;   stress = stressed (%) 18 (28.1)  18 (28.1)  &lt;0.001\n#&gt;   married = single (%)  21 (32.8)  23 (35.9)   0.066\n#&gt;   income (%)                                   0.204\n#&gt;      $29,999 or less    11 (17.2)  13 (20.3)        \n#&gt;      $30,000-$49,999    19 (29.7)  14 (21.9)        \n#&gt;      $50,000-$79,999    13 (20.3)  12 (18.8)        \n#&gt;      $80,000 or more    21 (32.8)  25 (39.1)        \n#&gt;   race = White (%)      40 (62.5)  42 (65.6)   0.065\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight     18 (28.1)  18 (28.1)        \n#&gt;      Overweight         46 (71.9)  46 (71.9)        \n#&gt;   phyact (%)                                   0.096\n#&gt;      Active             14 (21.9)  16 (25.0)        \n#&gt;      Inactive           39 (60.9)  36 (56.2)        \n#&gt;      Moderate           11 (17.2)  12 (18.8)        \n#&gt;   smoke (%)                                    0.267\n#&gt;      Never smoker       14 (21.9)   9 (14.1)        \n#&gt;      Current smoker     19 (29.7)  26 (40.6)        \n#&gt;      Former smoker      31 (48.4)  29 (45.3)        \n#&gt;   doctor = Yes (%)      48 (75.0)  44 (68.8)   0.139\n#&gt;   drink (%)                                    0.123\n#&gt;      Never drank         2 ( 3.1)   2 ( 3.1)        \n#&gt;      Current drinker    52 (81.2)  49 (76.6)        \n#&gt;      Former driker      10 (15.6)  13 (20.3)        \n#&gt;   bp = Yes (%)           7 (10.9)   5 ( 7.8)   0.107\n#&gt;   immigrate (%)                                0.260\n#&gt;      not immigrant      62 (96.9)  58 (90.6)        \n#&gt;      &gt; 10 years          2 ( 3.1)   6 ( 9.4)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                    0.116\n#&gt;      0-3 daily serving  19 (29.7)  19 (29.7)        \n#&gt;      4-6 daily serving  27 (42.2)  30 (46.9)        \n#&gt;      6+ daily serving   18 (28.1)  15 (23.4)        \n#&gt;   diab = Yes (%)         0 ( 0.0)   3 ( 4.7)   0.314\n#&gt;   edu (%)                                      0.108\n#&gt;      &lt; 2ndary           14 (21.9)  13 (20.3)        \n#&gt;      2nd grad.           1 ( 1.6)   2 ( 3.1)        \n#&gt;      Other 2nd grad.     1 ( 1.6)   1 ( 1.6)        \n#&gt;      Post-2nd grad.     48 (75.0)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable for pair matching?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       1       1       1       1       1       1\n\nStep 4\nEstimate treatment effect for matched data\nDifferent models (e.g., unconditional logistic regression, survey design) are fitted to estimate the treatment effect in the matched sample.\nUnconditional logistic\n\n# Wrong model for population!!\noutcome.model &lt;- glm(CVD ~ OA, data = OACVD.match, family = binomial())\npublish(outcome.model)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.48 [0.09;2.74]   0.4119\n\nSurvey design\nConvert data to design\nThe matched data is converted to a survey design object to account for the matched pairs in the analysis.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 128\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 128\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 397045    128\nw.design0 &lt;- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\nBalance in matched population?\n\ntab1 &lt;- svyCreateTableOne(strata = \"OA\", data = w.design.m, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0              1              SMD   \n#&gt;   n                     1783.6         2002.1               \n#&gt;   age (%)                                              0.307\n#&gt;      20-29 years         131.0 ( 7.3)   120.9 ( 6.0)        \n#&gt;      30-39 years         388.0 (21.8)   237.8 (11.9)        \n#&gt;      40-49 years         518.3 (29.1)   572.7 (28.6)        \n#&gt;      50-59 years         680.1 (38.1)   999.3 (49.9)        \n#&gt;      60-64 years          66.1 ( 3.7)    71.4 ( 3.6)        \n#&gt;      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   sex = Male (%)         852.4 (47.8)   985.1 (49.2)   0.028\n#&gt;   stress = stressed (%)  544.6 (30.5)   531.8 (26.6)   0.088\n#&gt;   married = single (%)   419.8 (23.5)   427.5 (21.4)   0.052\n#&gt;   income (%)                                           0.222\n#&gt;      $29,999 or less     266.6 (14.9)   352.4 (17.6)        \n#&gt;      $30,000-$49,999     462.8 (25.9)   348.8 (17.4)        \n#&gt;      $50,000-$79,999     298.6 (16.7)   315.7 (15.8)        \n#&gt;      $80,000 or more     755.5 (42.4)   985.2 (49.2)        \n#&gt;   race = White (%)      1129.2 (63.3)  1364.9 (68.2)   0.103\n#&gt;   bmi (%)                                              0.045\n#&gt;      Underweight           0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      healthy weight      483.3 (27.1)   583.3 (29.1)        \n#&gt;      Overweight         1300.2 (72.9)  1418.8 (70.9)        \n#&gt;   phyact (%)                                           0.054\n#&gt;      Active              448.0 (25.1)   493.9 (24.7)        \n#&gt;      Inactive           1075.2 (60.3)  1176.6 (58.8)        \n#&gt;      Moderate            260.3 (14.6)   331.5 (16.6)        \n#&gt;   smoke (%)                                            0.288\n#&gt;      Never smoker        400.2 (22.4)   265.8 (13.3)        \n#&gt;      Current smoker      548.8 (30.8)   836.6 (41.8)        \n#&gt;      Former smoker       834.6 (46.8)   899.6 (44.9)        \n#&gt;   doctor = Yes (%)      1376.0 (77.1)  1430.1 (71.4)   0.131\n#&gt;   drink (%)                                            0.194\n#&gt;      Never drank          44.0 ( 2.5)   112.6 ( 5.6)        \n#&gt;      Current drinker    1464.1 (82.1)  1510.6 (75.5)        \n#&gt;      Former driker       275.4 (15.4)   378.9 (18.9)        \n#&gt;   bp = Yes (%)           166.6 ( 9.3)   153.5 ( 7.7)   0.060\n#&gt;   immigrate (%)                                        0.235\n#&gt;      not immigrant      1694.8 (95.0)  1774.1 (88.6)        \n#&gt;      &gt; 10 years           88.8 ( 5.0)   228.0 (11.4)        \n#&gt;      recent                0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   fruit (%)                                            0.293\n#&gt;      0-3 daily serving   426.1 (23.9)   480.8 (24.0)        \n#&gt;      4-6 daily serving   748.1 (41.9)  1082.3 (54.1)        \n#&gt;      6+ daily serving    609.4 (34.2)   439.0 (21.9)        \n#&gt;   diab = Yes (%)           0.0 ( 0.0)    83.6 ( 4.2)   0.295\n#&gt;   edu (%)                                              0.172\n#&gt;      &lt; 2ndary            324.9 (18.2)   342.8 (17.1)        \n#&gt;      2nd grad.            18.8 ( 1.1)    47.6 ( 2.4)        \n#&gt;      Other 2nd grad.      15.2 ( 0.9)    52.2 ( 2.6)        \n#&gt;      Post-2nd grad.     1424.7 (79.9)  1559.4 (77.9)\n\nOutcome analysis\n\nfit.design &lt;- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.50 [0.08;3.09]   0.4535\n\nMatched data with increase ratio\nThe matching process is repeated with a different ratio (e.g., 1:5) to explore how changing the ratio affects the covariate balance and treatment effect estimation.\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 5:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.013)\n#&gt;  - number of obs.: 1424 (original), 349 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004643 0.039181 0.084421 0.101576 0.146403 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     285         64               \n#&gt;   age (%)                                       0.217\n#&gt;      20-29 years         24 ( 8.4)   4 ( 6.2)        \n#&gt;      30-39 years         59 (20.7)  11 (17.2)        \n#&gt;      40-49 years         94 (33.0)  18 (28.1)        \n#&gt;      50-59 years         98 (34.4)  28 (43.8)        \n#&gt;      60-64 years         10 ( 3.5)   3 ( 4.7)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        132 (46.3)  30 (46.9)   0.011\n#&gt;   stress = stressed (%)  81 (28.4)  18 (28.1)   0.007\n#&gt;   married = single (%)   91 (31.9)  23 (35.9)   0.085\n#&gt;   income (%)                                    0.065\n#&gt;      $29,999 or less     64 (22.5)  13 (20.3)        \n#&gt;      $30,000-$49,999     65 (22.8)  14 (21.9)        \n#&gt;      $50,000-$79,999     51 (17.9)  12 (18.8)        \n#&gt;      $80,000 or more    105 (36.8)  25 (39.1)        \n#&gt;   race = White (%)      169 (59.3)  42 (65.6)   0.131\n#&gt;   bmi (%)                                       0.097\n#&gt;      Underweight          0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      68 (23.9)  18 (28.1)        \n#&gt;      Overweight         217 (76.1)  46 (71.9)        \n#&gt;   phyact (%)                                    0.136\n#&gt;      Active              57 (20.0)  16 (25.0)        \n#&gt;      Inactive           178 (62.5)  36 (56.2)        \n#&gt;      Moderate            50 (17.5)  12 (18.8)        \n#&gt;   smoke (%)                                     0.097\n#&gt;      Never smoker        46 (16.1)   9 (14.1)        \n#&gt;      Current smoker     123 (43.2)  26 (40.6)        \n#&gt;      Former smoker      116 (40.7)  29 (45.3)        \n#&gt;   doctor = Yes (%)      183 (64.2)  44 (68.8)   0.096\n#&gt;   drink (%)                                     0.051\n#&gt;      Never drank         10 ( 3.5)   2 ( 3.1)        \n#&gt;      Current drinker    212 (74.4)  49 (76.6)        \n#&gt;      Former driker       63 (22.1)  13 (20.3)        \n#&gt;   bp = Yes (%)           22 ( 7.7)   5 ( 7.8)   0.003\n#&gt;   immigrate (%)                                 0.100\n#&gt;      not immigrant      266 (93.3)  58 (90.6)        \n#&gt;      &gt; 10 years          19 ( 6.7)   6 ( 9.4)        \n#&gt;      recent               0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                     0.149\n#&gt;      0-3 daily serving  104 (36.5)  19 (29.7)        \n#&gt;      4-6 daily serving  124 (43.5)  30 (46.9)        \n#&gt;      6+ daily serving    57 (20.0)  15 (23.4)        \n#&gt;   diab = Yes (%)         12 ( 4.2)   3 ( 4.7)   0.023\n#&gt;   edu (%)                                       0.137\n#&gt;      &lt; 2ndary            67 (23.5)  13 (20.3)        \n#&gt;      2nd grad.            9 ( 3.2)   2 ( 3.1)        \n#&gt;      Other 2nd grad.      9 ( 3.2)   1 ( 1.6)        \n#&gt;      Post-2nd grad.     200 (70.2)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.8906  0.8906  0.8906  1.0000  0.8906  4.4531\n\nCombining matching weights\nDifferent approaches to incorporating weights (e.g., matching weights, survey weights) are explored.\nNot incorporating matching weights\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396824    349\nw.design0 &lt;- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95 p-value \n#&gt;        OA            0.80 [0.23;2.80]   0.722\n\nIncorporating matching weights\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396824    349\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight &lt;- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] &lt;-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 &lt;- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            1.14 [0.32;4.07]   0.8419\n\nMatched with replacement\nMatching is performed with replacement, allowing control units to be used in more than one match.\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2,\n                        replace = TRUE)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 5:1 nearest neighbor matching with replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.013)\n#&gt;  - number of obs.: 1424 (original), 308 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004643 0.034244 0.067224 0.100844 0.148958 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.042322 0.097877 0.129743 0.189256 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     243         65               \n#&gt;   age (%)                                       0.266\n#&gt;      20-29 years         22 ( 9.1)   4 ( 6.2)        \n#&gt;      30-39 years         57 (23.5)  11 (16.9)        \n#&gt;      40-49 years         74 (30.5)  18 (27.7)        \n#&gt;      50-59 years         82 (33.7)  29 (44.6)        \n#&gt;      60-64 years          8 ( 3.3)   3 ( 4.6)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        113 (46.5)  31 (47.7)   0.024\n#&gt;   stress = stressed (%)  67 (27.6)  19 (29.2)   0.037\n#&gt;   married = single (%)   75 (30.9)  23 (35.4)   0.096\n#&gt;   income (%)                                    0.079\n#&gt;      $29,999 or less     53 (21.8)  13 (20.0)        \n#&gt;      $30,000-$49,999     57 (23.5)  14 (21.5)        \n#&gt;      $50,000-$79,999     44 (18.1)  12 (18.5)        \n#&gt;      $80,000 or more     89 (36.6)  26 (40.0)        \n#&gt;   race = White (%)      146 (60.1)  42 (64.6)   0.094\n#&gt;   bmi (%)                                       0.088\n#&gt;      Underweight          0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      58 (23.9)  18 (27.7)        \n#&gt;      Overweight         185 (76.1)  47 (72.3)        \n#&gt;   phyact (%)                                    0.160\n#&gt;      Active              45 (18.5)  16 (24.6)        \n#&gt;      Inactive           155 (63.8)  37 (56.9)        \n#&gt;      Moderate            43 (17.7)  12 (18.5)        \n#&gt;   smoke (%)                                     0.095\n#&gt;      Never smoker        40 (16.5)   9 (13.8)        \n#&gt;      Current smoker     101 (41.6)  26 (40.0)        \n#&gt;      Former smoker      102 (42.0)  30 (46.2)        \n#&gt;   doctor = Yes (%)      152 (62.6)  45 (69.2)   0.141\n#&gt;   drink (%)                                     0.059\n#&gt;      Never drank          9 ( 3.7)   2 ( 3.1)        \n#&gt;      Current drinker    181 (74.5)  50 (76.9)        \n#&gt;      Former driker       53 (21.8)  13 (20.0)        \n#&gt;   bp = Yes (%)           19 ( 7.8)   5 ( 7.7)   0.005\n#&gt;   immigrate (%)                                 0.115\n#&gt;      not immigrant      228 (93.8)  59 (90.8)        \n#&gt;      &gt; 10 years          15 ( 6.2)   6 ( 9.2)        \n#&gt;      recent               0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                     0.147\n#&gt;      0-3 daily serving   87 (35.8)  19 (29.2)        \n#&gt;      4-6 daily serving  109 (44.9)  31 (47.7)        \n#&gt;      6+ daily serving    47 (19.3)  15 (23.1)        \n#&gt;   diab = Yes (%)         11 ( 4.5)   3 ( 4.6)   0.004\n#&gt;   edu (%)                                       0.175\n#&gt;      &lt; 2ndary            58 (23.9)  13 (20.0)        \n#&gt;      2nd grad.            8 ( 3.3)   2 ( 3.1)        \n#&gt;      Other 2nd grad.      9 ( 3.7)   1 ( 1.5)        \n#&gt;      Post-2nd grad.     168 (69.1)  49 (75.4)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.7477  0.7477  0.7477  1.0000  1.0000  7.4769\n\nSurvey design\nThe matched data is converted into a survey design object, and the treatment effect is estimated while accounting for the complex survey design.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 308\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 308\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396865    308\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight &lt;- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] &lt;-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 &lt;- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.99 [0.26;3.72]   0.9909\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nRDocumentation. 2023. ‚ÄúMatchit: Matchit: Matching Software for Causal Inference.‚Äù https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit.",
    "crumbs": [
      "Propensity score",
      "PSM in OA-CVD (CCHS)"
    ]
  },
  {
    "objectID": "propensityscore3.html",
    "href": "propensityscore3.html",
    "title": "PSM in OA-CVD (US)",
    "section": "",
    "text": "Pre-processing\nLoad data\nLoad the dataset and inspect its structure and variables.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") \nls()\n#&gt; [1] \"analytic\"           \"analytic.with.miss\"\n\nVisualize missing data patterns.\n\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:data.table':\n#&gt; \n#&gt;     between, first, last\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nanalytic.with.miss &lt;- dplyr::select(analytic.with.miss, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, \n                  married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\ndim(analytic.with.miss)\n#&gt; [1] 9254   13\nstr(analytic.with.miss)\n#&gt; 'data.frame':    9254 obs. of  13 variables:\n#&gt;  $ cholesterol: 'labelled' int  NA NA 157 148 189 209 176 NA 238 182 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Total Cholesterol (mg/dL)\"\n#&gt;  $ gender     : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;  $ age        : 'labelled' int  NA NA 66 NA NA 66 75 NA 56 NA ...\n#&gt;   ..- attr(*, \"label\")= chr \"Age in years at screening\"\n#&gt;  $ born       : chr  \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;  $ race       : chr  \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;  $ education  : chr  NA NA \"High.School\" NA ...\n#&gt;  $ married    : chr  NA NA \"Previously.married\" NA ...\n#&gt;  $ income     : chr  \"Over100k\" \"Over100k\" \"&lt;25k\" NA ...\n#&gt;  $ bmi        : 'labelled' num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Body Mass Index (kg/m**2)\"\n#&gt;  $ diabetes   : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ weight     : 'labelled' num  8540 42567 8338 8723 7065 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Full sample 2 year MEC exam weight\"\n#&gt;  $ psu        : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;  $ strata     : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\nnames(analytic.with.miss)\n#&gt;  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#&gt;  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#&gt; [11] \"weight\"      \"psu\"         \"strata\"\n\nlibrary(DataExplorer)\nplot_missing(analytic.with.miss)\n\n\n\n\n\n\n\nFormatting variables\nRename variables to avoid conflicts. Recode variables into binary or categorical as needed. Ensure variable types (factor, numeric) are appropriate.\n\n# to avaoid any confusion later\n# rename weight variable as weights \n# is reserved for matching weights\nanalytic.with.miss$survey.weight &lt;- analytic.with.miss$weight\nanalytic.with.miss$weight &lt;- NULL\n\n#Creating binary variable for cholesterol\nanalytic.with.miss$cholesterol.bin &lt;- ifelse(analytic.with.miss$cholesterol &lt;200, \n                                             1, #\"healthy\",\n                                             0) #\"unhealthy\")\n# exposure recoding\nanalytic.with.miss$diabetes &lt;- ifelse(analytic.with.miss$diabetes == \"Yes\", 1, 0)\n\n# ID\nanalytic.with.miss$ID &lt;- 1:nrow(analytic.with.miss)\n\n# covariates\nanalytic.with.miss$born &lt;- ifelse(analytic.with.miss$born == \"Other\", \n                                             0,\n                                             1)\n\nvars = c(\"gender\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\")\n\nnumeric.names &lt;- c(\"cholesterol\", \"bmi\")\nfactor.names &lt;- vars[!vars %in% numeric.names] \n\nanalytic.with.miss[factor.names] &lt;- apply(X = analytic.with.miss[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanalytic.with.miss[numeric.names] &lt;- apply(X = analytic.with.miss[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\nanalytic.with.miss$income &lt;- factor(analytic.with.miss$income, \n                                    ordered = TRUE, \n                                levels = c(\"&lt;25k\", \"Between.25kto54k\", \n                                           \"Between.55kto99k\", \n                                           \"Over100k\"))\n\n# features\ntable(analytic.with.miss$strata)\n#&gt; \n#&gt; 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#&gt; 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\ntable(analytic.with.miss$psu)\n#&gt; \n#&gt;    1    2 \n#&gt; 4464 4790\ntable(analytic.with.miss$strata,analytic.with.miss$psu)\n#&gt;      \n#&gt;         1   2\n#&gt;   134 215 295\n#&gt;   135 316 322\n#&gt;   136 320 375\n#&gt;   137 306 248\n#&gt;   138 308 297\n#&gt;   139 278 375\n#&gt;   140 315 297\n#&gt;   141 282 411\n#&gt;   142 349 386\n#&gt;   143 232 319\n#&gt;   144 351 338\n#&gt;   145 339 270\n#&gt;   146 277 327\n#&gt;   147 335 261\n#&gt;   148 241 269\n\n# impute\n# require(mice)\n# imputation1 &lt;- mice(analytic.with.miss, seed = 123,\n#                    m = 1, # Number of multiple imputations. \n#                    maxit = 10 # Number of iteration; mostly useful for convergence\n#                    )\n# analytic.with.miss &lt;- complete(imputation1)\n# plot_missing(analytic.with.miss)\n\nComplete case data\nCreate a dataset (analytic.data) without NA values for analysis. This is done for simplified analysis, but this approach has it‚Äôs own challenges. In a next tutorial, we will appropriately deal with missing observations in a propensity score modelling.\n\ndim(analytic.with.miss)\n#&gt; [1] 9254   15\nanalytic.data &lt;- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic.data) # complete case\n#&gt; [1] 4167   15\n\nZanutto (2006)\n\nRef: Zanutto (2006)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSet seed\n\nset.seed(123)\n\n\n‚Äúit is not necessary to use survey-weighted estimation for the propensity score model‚Äù\n\nPropensity score analysis in 4 steps:\n\nStep 1: PS model specification\nStep 2: Matching based on the estimated propensity scores\nStep 3: Balance checking\nStep 4: Outcome modelling\nStep 1\nSpecify the propensity score model to estimate propensity scores\n\nps.formula &lt;- as.formula(diabetes ~ gender + born +\n                         race + education + married + income + bmi)\n\nStep 2\nMatch treated and untreated subjects based on the estimated propensity scores. Perform nearest-neighbor matching using the propensity scores. Visualize the distribution of propensity scores before and after matching.\n\nrequire(MatchIt)\nset.seed(123)\n# This function fits propensity score model (using logistic \n# regression as above) when specified distance = 'logit'\n# performs nearest-neighbor (NN) matching, \n# without replacement \n# with caliper = .2*SD of propensity score  \n# within which to draw control units \n# with 1:1 ratio (pair-matching)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\n# see matchit function options here\n# https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02901 0.12255 0.17658 0.18982 0.23876 0.82164\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02901 0.11509 0.16687 0.17949 0.22816 0.75968 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.04793 0.16489 0.21300 0.23395 0.27768 0.82164\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.019)\n#&gt;  - number of obs.: 4167 (original), 1564 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data &lt;- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nrequire(tableone)\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1 &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars,\n                       data = analytic.data, test = FALSE)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                      3376           791               \n#&gt;   gender = Male (%)      1578 (46.7)    434 (54.9)   0.163\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.060\n#&gt;      Black                728 (21.6)    183 (23.1)        \n#&gt;      Hispanic             727 (21.5)    170 (21.5)        \n#&gt;      Other                642 (19.0)    159 (20.1)        \n#&gt;      White               1279 (37.9)    279 (35.3)        \n#&gt;   education (%)                                      0.185\n#&gt;      College             1992 (59.0)    415 (52.5)        \n#&gt;      High.School         1174 (34.8)    290 (36.7)        \n#&gt;      School               210 ( 6.2)     86 (10.9)        \n#&gt;   married (%)                                        0.316\n#&gt;      Married             2027 (60.0)    488 (61.7)        \n#&gt;      Never.married        631 (18.7)     70 ( 8.8)        \n#&gt;      Previously.married   718 (21.3)    233 (29.5)        \n#&gt;   income (%)                                         0.092\n#&gt;      &lt;25k                 830 (24.6)    225 (28.4)        \n#&gt;      Between.25kto54k    1064 (31.5)    244 (30.8)        \n#&gt;      Between.55kto99k     778 (23.0)    173 (21.9)        \n#&gt;      Over100k             704 (20.9)    149 (18.8)        \n#&gt;   bmi (mean (SD))       29.29 (7.11)  32.31 (8.03)   0.399\n\n\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                       782           782               \n#&gt;   gender = Male (%)       422 (54.0)    430 (55.0)   0.021\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.068\n#&gt;      Black                180 (23.0)    179 (22.9)        \n#&gt;      Hispanic             151 (19.3)    170 (21.7)        \n#&gt;      Other                171 (21.9)    156 (19.9)        \n#&gt;      White                280 (35.8)    277 (35.4)        \n#&gt;   education (%)                                      0.077\n#&gt;      College              441 (56.4)    411 (52.6)        \n#&gt;      High.School          262 (33.5)    286 (36.6)        \n#&gt;      School                79 (10.1)     85 (10.9)        \n#&gt;   married (%)                                        0.044\n#&gt;      Married              502 (64.2)    486 (62.1)        \n#&gt;      Never.married         63 ( 8.1)     69 ( 8.8)        \n#&gt;      Previously.married   217 (27.7)    227 (29.0)        \n#&gt;   income (%)                                         0.065\n#&gt;      &lt;25k                 202 (25.8)    218 (27.9)        \n#&gt;      Between.25kto54k     236 (30.2)    244 (31.2)        \n#&gt;      Between.55kto99k     187 (23.9)    171 (21.9)        \n#&gt;      Over100k             157 (20.1)    149 (19.1)        \n#&gt;   bmi (mean (SD))       32.12 (8.29)  32.05 (7.58)   0.009\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample. Use the matched sample to estimate the treatment effect, considering survey design.\nIncorporating the survey design into both linear regression and propensity score analysis is crucial. Neglecting the survey weights can significantly impact the estimates, altering the representation of population-level effects.\n\nrequire(survey)\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data$ID) # matched data\n#&gt; [1] 1564\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#&gt; [1] 1564\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7690 1564\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial(logit), design = w.design.m)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1564\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.02\n\n\nPseudo-R¬≤ (McFadden)\n0.01\n\n\nAIC\n1925.24\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.33\n0.97\n1.80\n1.79\n0.09\n\n\ndiabetes\n1.68\n1.17\n2.41\n2.84\n0.01\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDuGoff et al.¬†(2014)\n\nRef: DuGoff, Schuler, and Stuart (2014)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Similar to Zanutto but includes additional covariates in the model.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula &lt;- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi+\n                           psu+strata+survey.weight)\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00363 0.11341 0.17509 0.18982 0.24766 0.80853\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00363 0.10394 0.16243 0.17690 0.23461 0.73143 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01182 0.17245 0.22748 0.24500 0.29948 0.80853\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4167 (original), 1570 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi, psu, strata, survey.weight\n# extract matched data\nmatched.data &lt;- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nrequire(tableone)\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\", \n                  \"psu\", \"strata\", \"survey.weight\")\nmatched.data$survey.weight &lt;- as.numeric(as.character(matched.data$survey.weight))\nmatched.data$strata &lt;- as.numeric(as.character(matched.data$strata))\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                            Stratified by diabetes\n#&gt;                             0                   1                   SMD   \n#&gt;   n                              785                 785                  \n#&gt;   gender = Male (%)              433 (55.2)          431 (54.9)      0.005\n#&gt;   born (mean (SD))              1.00 (0.00)         1.00 (0.00)     &lt;0.001\n#&gt;   race (%)                                                           0.048\n#&gt;      Black                       193 (24.6)          180 (22.9)           \n#&gt;      Hispanic                    163 (20.8)          170 (21.7)           \n#&gt;      Other                       163 (20.8)          158 (20.1)           \n#&gt;      White                       266 (33.9)          277 (35.3)           \n#&gt;   education (%)                                                      0.032\n#&gt;      College                     403 (51.3)          412 (52.5)           \n#&gt;      High.School                 300 (38.2)          288 (36.7)           \n#&gt;      School                       82 (10.4)           85 (10.8)           \n#&gt;   married (%)                                                        0.030\n#&gt;      Married                     473 (60.3)          484 (61.7)           \n#&gt;      Never.married                71 ( 9.0)           70 ( 8.9)           \n#&gt;      Previously.married          241 (30.7)          231 (29.4)           \n#&gt;   income (%)                                                         0.035\n#&gt;      &lt;25k                        232 (29.6)          222 (28.3)           \n#&gt;      Between.25kto54k            236 (30.1)          242 (30.8)           \n#&gt;      Between.55kto99k            176 (22.4)          173 (22.0)           \n#&gt;      Over100k                    141 (18.0)          148 (18.9)           \n#&gt;   bmi (mean (SD))              31.92 (8.33)        32.09 (7.52)      0.020\n#&gt;   psu = 2 (%)                    382 (48.7)          394 (50.2)      0.031\n#&gt;   strata (mean (SD))          140.84 (4.24)       140.97 (4.23)      0.031\n#&gt;   survey.weight (mean (SD)) 35647.01 (37699.98) 35596.81 (45212.82)  0.001\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample\n\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data$ID) # matched data\n#&gt; [1] 1570\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#&gt; [1] 1570\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7684 1570\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial, design = w.design.m)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1570\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.01\n\n\nPseudo-R¬≤ (McFadden)\n0.00\n\n\nAIC\n1918.09\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.69\n1.33\n2.15\n4.24\n0.00\n\n\ndiabetes\n1.32\n0.99\n1.77\n1.86\n0.08\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nAustin et al.¬†(2018)\n\nRef: Austin, Jembere, and Chiu (2018)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Use survey logistic regression to account for survey design in propensity score estimation.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula &lt;- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi)\nrequire(survey)\nanalytic.design &lt;- svydesign(id=~psu,weights=~survey.weight, \n                             strata=~strata,\n                             data=analytic.data, nest=TRUE)\nps.fit &lt;- svyglm(ps.formula, design=analytic.design, family=quasibinomial)\nanalytic.data$PS &lt;- fitted(ps.fit)\nsummary(analytic.data$PS)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores. Two methods are explored: using the Matching package and the MatchIt package.\n\nrequire(Matching)\n#&gt; Loading required package: Matching\n#&gt; Loading required package: MASS\n#&gt; \n#&gt; Attaching package: 'MASS'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     select\n#&gt; ## \n#&gt; ##  Matching (Version 4.10-15, Build Date: 2024-10-14)\n#&gt; ##  See https://www.jsekhon.com for additional documentation.\n#&gt; ##  Please cite software as:\n#&gt; ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n#&gt; ##   Software with Automated Balance Optimization: The Matching package for R.''\n#&gt; ##   Journal of Statistical Software, 42(7): 1-52. \n#&gt; ##\nmatch.obj2 &lt;- Match(Y=analytic.data$cholesterol, \n                    Tr=analytic.data$diabetes, \n                    X=analytic.data$PS, \n                    M=1, \n                    estimand = \"ATT\",\n                    replace=FALSE, \n                    caliper = 0.2)\nsummary(match.obj2)\n#&gt; \n#&gt; Estimate...  -15.287 \n#&gt; SE.........  2.118 \n#&gt; T-stat.....  -7.2175 \n#&gt; p.val......  5.2958e-13 \n#&gt; \n#&gt; Original number of observations..............  4167 \n#&gt; Original number of treated obs...............  791 \n#&gt; Matched number of observations...............  781 \n#&gt; Matched number of observations  (unweighted).  781 \n#&gt; \n#&gt; Caliper (SDs)........................................   0.2 \n#&gt; Number of obs dropped by 'exact' or 'caliper'  10\nmatched.data2 &lt;- analytic.data[c(match.obj2$index.treated, \n                                 match.obj2$index.control),]\ndim(matched.data2)\n#&gt; [1] 1562   16\n\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = analytic.data$PS, \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08182 0.12644 0.14119 0.18267 0.75047 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02352 0.12362 0.16998 0.19389 0.23171 0.86375\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.019)\n#&gt;  - number of obs.: 4167 (original), 1568 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data2 &lt;- match.data(match.obj)\ndim(matched.data2)\n#&gt; [1] 1568   19\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", \n                           vars = baselinevars,\n                           data = matched.data2, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                       784           784               \n#&gt;   gender = Male (%)       405 (51.7)    431 (55.0)   0.067\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.134\n#&gt;      Black                163 (20.8)    182 (23.2)        \n#&gt;      Hispanic             139 (17.7)    170 (21.7)        \n#&gt;      Other                171 (21.8)    157 (20.0)        \n#&gt;      White                311 (39.7)    275 (35.1)        \n#&gt;   education (%)                                      0.040\n#&gt;      College              428 (54.6)    413 (52.7)        \n#&gt;      High.School          274 (34.9)    288 (36.7)        \n#&gt;      School                82 (10.5)     83 (10.6)        \n#&gt;   married (%)                                        0.070\n#&gt;      Married              509 (64.9)    485 (61.9)        \n#&gt;      Never.married         59 ( 7.5)     70 ( 8.9)        \n#&gt;      Previously.married   216 (27.6)    229 (29.2)        \n#&gt;   income (%)                                         0.063\n#&gt;      &lt;25k                 220 (28.1)    220 (28.1)        \n#&gt;      Between.25kto54k     226 (28.8)    242 (30.9)        \n#&gt;      Between.55kto99k     192 (24.5)    173 (22.1)        \n#&gt;      Over100k             146 (18.6)    149 (19.0)        \n#&gt;   bmi (mean (SD))       31.93 (8.16)  32.11 (7.61)   0.023\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample.\n\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data2$ID) # matched data\n#&gt; [1] 1568\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data2$ID])\n#&gt; [1] 1568\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data2$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7686 1568\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m2 &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial, design = w.design.m2)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1568\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.01\n\n\nPseudo-R¬≤ (McFadden)\n0.01\n\n\nAIC\n1919.53\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.46\n1.15\n1.86\n3.11\n0.01\n\n\ndiabetes\n1.52\n1.21\n1.90\n3.65\n0.00\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. ‚ÄúPropensity Score Matching and Complex Surveys.‚Äù Statistical Methods in Medical Research 27 (4): 1240‚Äì57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. ‚ÄúGeneralizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.‚Äù Health Services Research 49 (1): 284‚Äì303.\n\n\nZanutto, Elaine L. 2006. ‚ÄúA Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.‚Äù Journal of Data Science 4 (1): 67‚Äì91.",
    "crumbs": [
      "Propensity score",
      "PSM in OA-CVD (US)"
    ]
  },
  {
    "objectID": "propensityscore4.html",
    "href": "propensityscore4.html",
    "title": "PSM in BMI-diabetes",
    "section": "",
    "text": "Propensity analysis problem\nSee explained in the previous chapter on PSM in OA-CVD (US), there are four steps of the propensity score matching:\nOnly Step 1 is different for Zanutto (2006), DuGoff et al.¬†(2014), and Austin et al.¬†(2018) approaches.",
    "crumbs": [
      "Propensity score",
      "PSM in BMI-diabetes"
    ]
  },
  {
    "objectID": "propensityscore4.html#references",
    "href": "propensityscore4.html#references",
    "title": "PSM in BMI-diabetes",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. ‚ÄúPropensity Score Matching and Complex Surveys.‚Äù Statistical Methods in Medical Research 27 (4): 1240‚Äì57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. ‚ÄúGeneralizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.‚Äù Health Services Research 49 (1): 284‚Äì303.\n\n\nZanutto, Elaine L. 2006. ‚ÄúA Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.‚Äù Journal of Data Science 4 (1): 67‚Äì91.",
    "crumbs": [
      "Propensity score",
      "PSM in BMI-diabetes"
    ]
  },
  {
    "objectID": "propensityscore5.html",
    "href": "propensityscore5.html",
    "title": "PSM with MI",
    "section": "",
    "text": "The tutorial provides a detailed walkthrough of implementing Propensity Score Matching (PSM) combined with Multiple Imputation (MI) in a statistical analysis, focusing on handling missing data and mitigating bias in observational studies.\nThe initial chunk is dedicated to loading various R packages that will be utilized throughout the tutorial. These libraries provide functions and tools that facilitate data manipulation, statistical modeling, visualization, and more.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\nrequire(data.table)\nrequire(jtools)\nrequire(ggstance)\nrequire(DataExplorer)\nrequire(mitools)\nlibrary(kableExtra)\nlibrary(mice)\n\nProblem Statement\nLogistic regression\n\nPerform multiple imputation to deal with missing values; with 3 imputed datasets, 5 iterations,\nfit survey featured logistic regression in all of the 3 imputed datasets, and\nobtain the pooled OR (adjusted) and the corresponding 95% confidence intervals.\n\nHints\n\nUse the covariates (listed below) in the imputation model.\n\nImputation model covariates can be different than the original analysis covariates. You are encouraged to use variables in the imputation model that can be predictive of the variables with missing observations. In this example, we use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nAlso the imputation model specification can be modified. For example, we use pmm method for bmi in the imputation model.\nRemove any subject ID variable from the imputation model, if created in an intermediate step. Indeed ID variables should not be in the imputation model, if they are not predictive of the variables with missing observations.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPredictive Mean Matching:\nThe ‚ÄúPredictive Mean Matching‚Äù (PMM) method in Multiple Imputation (MI) is a widely used technique to handle missing data, particularly well-suited for continuous variables. PMM operates by first creating a predictive model for the variable with missing data, using observed values from other variables in the dataset. For each missing value, PMM identifies a set of observed values with predicted scores that are close to the predicted score for the missing value, derived from the predictive model. Then, instead of imputing a predicted score directly, PMM randomly selects one of the observed values from this set and assigns it as the imputed value. This method retains the original distribution of the imputed variable since it only uses observed values for imputation, and it also tends to preserve relationships between variables. PMM is particularly advantageous when the normality assumption of the imputed variable is questionable, providing a robust and practical approach to managing missing data in various research contexts.\n\n\nPropensity score matching (Zanutto, 2006)\n\nUse the propensity score matching as per Zanutto E. L. (2006)‚Äôs recommendation in all of the imputed datasets.\nReport the pooled OR estimates (adjusted) and corresponding 95% confidence intervals (adjusted OR).\nData and variables\nAnalytic data\nThe analytic dataset is saved as NHANES17.RData.\nVariables\nWe are primarily interested in outcome diabetes and exposure whether born in the US (born).\nVariables under consideration:\n\nsurvey features\n\nPSU\nstrata\nsurvey weight\n\n\nCovariates\n\nrace\nage\nmarriage\neducation\ngender\nBMI\nsystolic blood pressure\n\n\nPre-processing\nThe data is loaded and variables of interest are identified.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") # read data\nls()\n#&gt; [1] \"analytic\"           \"analytic.with.miss\"\ndim(analytic.with.miss)\n#&gt; [1] 9254   34\nvars &lt;- c(\"ID\", # ID\n          \"psu\", \"strata\", \"weight\", # Survey features \n          \"race\", \"age\", \"married\",\"education\",\"gender\",\"bmi\",\"systolicBP\", # Covariates\n          \"born\", # Exposure\n          \"diabetes\") # Outcome\n\nSubset the dataset\nThe dataset is then subsetted to retain only the relevant variables, ensuring that subsequent analyses are focused and computationally efficient.\n\ndat.with.miss &lt;- analytic.with.miss[,vars]\ndim(analytic.with.miss)\n#&gt; [1] 9254   34\n\nInspect weights\nThe weights of the observations are inspected and adjusted to avoid issues in subsequent analyses.\n\nsummary(dat.with.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12347   21060   34671   37562  419763\n# weight = 0 would create problem in the analysis\n# ad-hoc solution to 0 weight problem\ndat.with.miss$weight[dat.with.miss$weight == 0] &lt;- 0.00000001\n\nRecode the exposure variable\nThe exposure variable is recoded for clarity and ease of interpretation in results.\n\ndat.with.miss$born &lt;- car::recode(dat.with.miss$born, \nrecodes = \" 'Born in 50 US states or Washingt' = \n'Born in US'; 'Others' = 'Others'; else = NA \" )\ndat.with.miss$born &lt;- factor(dat.with.miss$born, levels = c(\"Born in US\", \"Others\"))\n\nvariable types\nVariable types are set, ensuring that each variable is treated appropriately in the analyses.\n\nfactor.names &lt;- c(\"race\", \"married\", \"education\", \"gender\", \"diabetes\")\ndat.with.miss[,factor.names] &lt;- lapply(dat.with.miss[,factor.names], factor)\n\nInspect extent of missing data problem\nA visualization is generated to explore the extent and pattern of missing data in the dataset, which informs the strategy for handling them.\n\nrequire(DataExplorer)\nplot_missing(dat.with.miss)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ‚Ñπ Please use tidy evaluation idioms with `aes()`.\n#&gt; ‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ‚Ñπ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nNote that, multiple imputation then delete (MID) approach can be applied if the outcome had some missing values. Due to the small number of missingness, MICE may not impute the outcomes BTW.\n\n\n\n\n\n\nTip\n\n\n\nMultiple imputation then delete (MID):\nMID is a specific approach used in the context of multiple imputation (MI) when dealing with missing outcome data. All missing values, including those in the outcome variable, are imputed to create several complete datasets. In subsequent analyses, the imputed values for the outcome variable are deleted, so that only observed outcome values are analyzed. Each dataset (with observed outcome values and imputed predictor values) is analyzed separately, and results are pooled to provide a single estimate.\n\n\nLogistic regression\nInitialization\nThe MI process is initialized, setting up the framework for subsequent imputations.\n\nimputation &lt;- mice(data = dat.with.miss, maxit = 0, print = FALSE)\n\nSetting imputation model covariates\nThe predictor matrix is adjusted to specify which variables will be used to predict missing values in the imputation model. Setting strata as auxiliary variable:\n\npred &lt;- imputation$pred\npred\n#&gt;            ID psu strata weight race age married education gender bmi\n#&gt; ID          0   1      1      1    1   1       1         1      1   1\n#&gt; psu         1   0      1      1    1   1       1         1      1   1\n#&gt; strata      1   1      0      1    1   1       1         1      1   1\n#&gt; weight      1   1      1      0    1   1       1         1      1   1\n#&gt; race        1   1      1      1    0   1       1         1      1   1\n#&gt; age         1   1      1      1    1   0       1         1      1   1\n#&gt; married     1   1      1      1    1   1       0         1      1   1\n#&gt; education   1   1      1      1    1   1       1         0      1   1\n#&gt; gender      1   1      1      1    1   1       1         1      0   1\n#&gt; bmi         1   1      1      1    1   1       1         1      1   0\n#&gt; systolicBP  1   1      1      1    1   1       1         1      1   1\n#&gt; born        1   1      1      1    1   1       1         1      1   1\n#&gt; diabetes    1   1      1      1    1   1       1         1      1   1\n#&gt;            systolicBP born diabetes\n#&gt; ID                  1    1        1\n#&gt; psu                 1    1        1\n#&gt; strata              1    1        1\n#&gt; weight              1    1        1\n#&gt; race                1    1        1\n#&gt; age                 1    1        1\n#&gt; married             1    1        1\n#&gt; education           1    1        1\n#&gt; gender              1    1        1\n#&gt; bmi                 1    1        1\n#&gt; systolicBP          0    1        1\n#&gt; born                1    0        1\n#&gt; diabetes            1    1        0\npred[,\"ID\"] &lt;- pred[\"ID\",] &lt;- 0\npred[,\"psu\"] &lt;- pred[\"psu\",] &lt;- 0\npred[,\"weight\"] &lt;- pred[\"weight\",] &lt;- 0\npred[\"strata\",] &lt;- 0\npred\n#&gt;            ID psu strata weight race age married education gender bmi\n#&gt; ID          0   0      0      0    0   0       0         0      0   0\n#&gt; psu         0   0      0      0    0   0       0         0      0   0\n#&gt; strata      0   0      0      0    0   0       0         0      0   0\n#&gt; weight      0   0      0      0    0   0       0         0      0   0\n#&gt; race        0   0      1      0    0   1       1         1      1   1\n#&gt; age         0   0      1      0    1   0       1         1      1   1\n#&gt; married     0   0      1      0    1   1       0         1      1   1\n#&gt; education   0   0      1      0    1   1       1         0      1   1\n#&gt; gender      0   0      1      0    1   1       1         1      0   1\n#&gt; bmi         0   0      1      0    1   1       1         1      1   0\n#&gt; systolicBP  0   0      1      0    1   1       1         1      1   1\n#&gt; born        0   0      1      0    1   1       1         1      1   1\n#&gt; diabetes    0   0      1      0    1   1       1         1      1   1\n#&gt;            systolicBP born diabetes\n#&gt; ID                  0    0        0\n#&gt; psu                 0    0        0\n#&gt; strata              0    0        0\n#&gt; weight              0    0        0\n#&gt; race                1    1        1\n#&gt; age                 1    1        1\n#&gt; married             1    1        1\n#&gt; education           1    1        1\n#&gt; gender              1    1        1\n#&gt; bmi                 1    1        1\n#&gt; systolicBP          0    1        1\n#&gt; born                1    0        1\n#&gt; diabetes            1    1        0\n\nSetting imputation model specification\nThe method for imputing a particular variable is specified (e.g., using Predictive Mean Matching). Here, we add pmm for bmi:\n\nmeth &lt;- imputation$meth\nmeth[\"bmi\"] &lt;- \"pmm\"\n\nImpute incomplete data\nMultiple datasets are imputed, each providing a different ‚Äúguess‚Äù at the missing values, based on observed data. We are imputing m = 3 times.\n\nimputation &lt;- mice(data = dat.with.miss, \n                   seed = 123, \n                   predictorMatrix = pred,\n                   method = meth, \n                   m = 3, \n                   maxit = 5, \n                   print = FALSE)\nimpdata &lt;- mice::complete(imputation, action=\"long\")\nimpdata$.id &lt;- NULL\nm &lt;- 3\nset.seed(123)\nallImputations &lt;-  imputationList(lapply(1:m, \n                                         function(n)\n                                           subset(impdata, \n                                                  subset=.imp==n)))\nstr(allImputations)\n#&gt; List of 2\n#&gt;  $ imputations:List of 3\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 57 46 66 50 23 66 75 49 56 36 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 2 1 3 3 1 1 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 2 2 1 1 3 1 2 1 3 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 31.2 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 108 96 200 112 128 124 120 122 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 24 49 66 32 34 66 75 80 56 28 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 2 1 3 2 1 1 3 3 1 2 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 2 3 1 1 1 1 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 24.9 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 102 104 136 112 128 120 120 120 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 47 71 66 71 45 66 75 37 56 47 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 1 1 3 1 1 1 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 1 3 1 1 1 2 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 15.9 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 100 114 162 112 128 166 120 116 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#&gt;  - attr(*, \"class\")= chr \"imputationList\"\n\nDesign\nA survey design object is created, ensuring that subsequent analyses appropriately account for the survey design.\n\nw.design &lt;- svydesign(ids = ~psu, weights = ~weight, strata = ~strata,\n                      data = allImputations, nest = TRUE)\n\nSurvey data analysis\nA logistic regression model is fitted to each imputed dataset.\n\nmodel.formula &lt;- as.formula(I(diabetes == 'Yes') ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\nfit.from.logistic &lt;- with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPooled estimates\nResults from models across all imputed datasets are pooled to provide a single estimate, accounting for the uncertainty due to missing data.\n\npooled.estimates &lt;- MIcombine(fit.from.logistic)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#&gt; Multiple imputation results:\n#&gt;       with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#&gt;       MIcombine.default(fit.from.logistic)\n#&gt;                           results      se  (lower  upper) missInfo\n#&gt; (Intercept)               0.00013 7.5e-05 3.9e-05 0.00041     22 %\n#&gt; bornOthers                1.44729 2.7e-01 1.0e+00 2.07384      0 %\n#&gt; raceHispanic              0.81619 1.1e-01 6.3e-01 1.05882      0 %\n#&gt; raceOther                 1.43817 2.6e-01 1.0e+00 2.04954      3 %\n#&gt; raceWhite                 0.86411 1.3e-01 6.5e-01 1.14994      3 %\n#&gt; age                       1.06157 3.6e-03 1.1e+00 1.06874      6 %\n#&gt; marriedNever.married      0.83242 1.6e-01 5.7e-01 1.20809     10 %\n#&gt; marriedPreviously.married 0.88401 1.1e-01 6.8e-01 1.14163     11 %\n#&gt; educationHigh.School      1.16331 1.9e-01 8.4e-01 1.60803      0 %\n#&gt; educationSchool           1.41943 2.4e-01 1.0e+00 1.98397      7 %\n#&gt; genderMale                1.53458 1.8e-01 1.2e+00 1.94217      3 %\n#&gt; bmi                       1.10597 1.2e-02 1.1e+00 1.12956      1 %\n#&gt; systolicBP                1.00325 3.2e-03 1.0e+00 1.01001     39 %\nOR &lt;- round(exp(pooled.estimates$coefficients), 2) \nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR[2,]\n\n\n  \n\n\n\nPropensity score matching analysis\nInitialization\nThe MI process is re-initialized to facilitate PSM in the context of MI.\n\nimputation &lt;- mice(data = dat.with.miss, maxit = 0, print = FALSE)\nimpdata &lt;- mice::complete(imputation, action=\"long\")\nm &lt;- 3\nallImputations &lt;- imputationList(lapply(1:m, \n                                        function(n) \n                                          subset(impdata, \n                                                 subset=.imp==n)))\n\nZanutto E. L. (2006) under multiple imputation\n\n\n\n\n\n\nTip\n\n\n\nAn iterative process is performed within each imputed dataset, which involves:\n\nEstimating propensity scores.\nMatching treated and untreated subjects based on these scores.\nExtracting matched data and checking the balance of covariates across matched groups.\nFitting outcome models to the survey weighted matched data and estimating treatment effects.\n\n\n\nNotice that we are performing multi-step process within MI\n\nmatch.statm &lt;- SMDm &lt;- tab1m &lt;- vector(\"list\", m) \nfit.from.PS &lt;- vector(\"list\", m)\n\nfor (i in 1:m) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  # Rename the weight variable into survey.weight\n  names(analytic.i)[names(analytic.i) == \"weight\"] &lt;- \"survey.weight\"\n  \n  # Specify the PS model to estimate propensity scores\n  ps.formula &lt;- as.formula(I(born==\"Others\") ~ \n                             race + age + married + education + \n                             gender + bmi + systolicBP)\n\n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = analytic.i, family = binomial(\"logit\"))\n  analytic.i$PS &lt;- fitted(ps.fit)\n  \n  # Match exposed and unexposed subjects \n  set.seed(123)\n  match.obj &lt;- matchit(ps.formula, data = analytic.i, \n                       distance = analytic.i$PS, \n                       method = \"nearest\", \n                       replace = FALSE,\n                       caliper = 0.2, \n                       ratio = 1)\n  match.statm[[i]] &lt;- match.obj\n  analytic.i$PS &lt;- match.obj$distance\n  \n  # Extract matched data\n  matched.data &lt;- match.data(match.obj) \n  \n  # Balance checking\n  cov &lt;- c(\"race\", \"age\", \"married\", \"education\", \"gender\", \"bmi\", \"systolicBP\")\n  \n  tab1m[[i]] &lt;- CreateTableOne(strata = \"born\", \n                               vars = cov, data = matched.data, \n                               test = FALSE, smd = TRUE)\n  SMDm[[i]] &lt;- ExtractSmd(tab1m[[i]])\n  \n  # Setup the design with survey features\n  analytic.i$matched &lt;- 0\n  analytic.i$matched[analytic.i$ID %in% matched.data$ID] &lt;- 1\n  \n  # Survey setup for full data\n  w.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                         data = analytic.i, nest = TRUE)\n  \n  # Subset matched data\n  w.design.m &lt;- subset(w.design0, matched == 1)\n  \n  # Outcome model (double adjustment)\n  out.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\n  fit.from.PS[[i]] &lt;- svyglm(out.formula, design = w.design.m, \n                     family = quasibinomial(\"logit\"))\n}\n\nCheck matched data\nThe matched data is inspected to ensure that matching was successful and appropriate.\n\nmatch.statm\n#&gt; [[1]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3590 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n#&gt; \n#&gt; [[2]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3598 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n#&gt; \n#&gt; [[3]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3594 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n\nCheck balance in matched data\nThe balance of covariates across matched groups is assessed to ensure that matching has successfully reduced bias.\n\nSMDm\n#&gt; [[1]]\n#&gt;                 1 vs 2\n#&gt; race       0.028883793\n#&gt; age        0.033614763\n#&gt; married    0.007318561\n#&gt; education  0.117536503\n#&gt; gender     0.040145831\n#&gt; bmi        0.043350560\n#&gt; systolicBP 0.054549772\n#&gt; \n#&gt; [[2]]\n#&gt;                 1 vs 2\n#&gt; race       0.019901420\n#&gt; age        0.016267050\n#&gt; married    0.017196043\n#&gt; education  0.128811588\n#&gt; gender     0.003338016\n#&gt; bmi        0.057014434\n#&gt; systolicBP 0.071553721\n#&gt; \n#&gt; [[3]]\n#&gt;                1 vs 2\n#&gt; race       0.04490482\n#&gt; age        0.01959377\n#&gt; married    0.03687394\n#&gt; education  0.13301810\n#&gt; gender     0.01225625\n#&gt; bmi        0.03697878\n#&gt; systolicBP 0.10025529\n\nPooled estimate\nFinally, the treatment effect estimates from the matched analyses across all imputed datasets are pooled to provide a single, overall estimate, ensuring that the final result appropriately accounts for the uncertainty due to both the matching process and the imputation of missing data.\n\npooled.estimates &lt;- MIcombine(fit.from.PS)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fit.from.PS)\n#&gt;                           results      se  (lower  upper) missInfo\n#&gt; (Intercept)               8.9e-05 4.9e-05 0.00003 0.00026      8 %\n#&gt; bornOthers                2.0e+00 3.1e-01 1.47719 2.73325     16 %\n#&gt; raceHispanic              7.0e-01 1.7e-01 0.42593 1.15504     27 %\n#&gt; raceOther                 1.4e+00 4.1e-01 0.77209 2.53278     26 %\n#&gt; raceWhite                 4.9e-01 2.7e-01 0.15853 1.52308     28 %\n#&gt; age                       1.1e+00 4.6e-03 1.04472 1.06298      8 %\n#&gt; marriedNever.married      5.9e-01 2.0e-01 0.28824 1.18926     38 %\n#&gt; marriedPreviously.married 1.0e+00 2.5e-01 0.62417 1.64438     11 %\n#&gt; educationHigh.School      1.4e+00 3.0e-01 0.89403 2.10852      2 %\n#&gt; educationSchool           1.3e+00 3.4e-01 0.81042 2.21438      7 %\n#&gt; genderMale                1.3e+00 2.3e-01 0.86695 1.83880     31 %\n#&gt; bmi                       1.1e+00 1.1e-02 1.08062 1.12285      5 %\n#&gt; systolicBP                1.0e+00 3.0e-03 1.00314 1.01494      3 %\nOR &lt;- round(exp(pooled.estimates$coefficients), 2) \nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR[2,]",
    "crumbs": [
      "Propensity score",
      "PSM with MI"
    ]
  },
  {
    "objectID": "propensityscore6.html",
    "href": "propensityscore6.html",
    "title": "PS Weighting (US)",
    "section": "",
    "text": "Propensity analysis problem\nIn this chapter, we will use propensity score weighting (using SMD cut-point 0.2, may adjust for imbalanced and/or all covariates in the outcome model, if any) analysis as per the following recommendations - (Zanutto 2006) - (DuGoff, Schuler, and Stuart 2014) - (Austin, Jembere, and Chiu 2018)\nDataset\n\nThe following modified NHANES dataset\n\nNHANES15lab5.RData\n\n\nuse the data set ‚Äúanalytic.with.miss‚Äù within this file.\n\nfor obtaining the final treatment effect estimates, you can omit missing values, but only after creating the design (e.g., subset the design, not the data itself directly).\n\n\nThe same dataset was used for propensity score weighting\n\nVariables\n\nOutcome: diabetes\n\n‚ÄòNo‚Äô as the reference category\n\n\nExposure: bmi\n\nconvert to binary with &gt;25 vs.¬†&lt;= 25,\nwith &gt; 25 as the reference category\n\n\nConfounder list:\n\ngender\nage\n\nassume continuous\n\n\nrace\nincome\neducation\nmarried\ncholesterol\ndiastolicBP\nsystolicBP\n\n\nMediator:\n\nphysical.work\n\n‚ÄòNo‚Äô as the reference category\n\n\n\n\nSurvey features\n\npsu\nstrata\nweight\n\n\nPre-processing\nLoad data\n\nload(file=\"Data/propensityscore/NHANES15lab5.RData\")\n\nVariable summary\n\n# Full data\ndat.full &lt;- analytic.with.miss\n\n# Exposure\ndat.full$bmi &lt;- with(dat.full, ifelse(bmi&gt;25, \"Overweight\", \n                                      ifelse(bmi&lt;=25, \"Not overweight\", NA)))\ndat.full$bmi &lt;- as.factor(dat.full$bmi)\ndat.full$bmi &lt;- relevel(dat.full$bmi, ref = \"Overweight\")\n\n# Drop unnecessary variables \ndat.full$born &lt;- NULL\ndat.full$physical.work &lt;- NULL\n\n# Rename the weight variable into interview.weight\nnames(dat.full)[names(dat.full) == \"weight\"] &lt;- \"interview.weight\"\n\nComplete case data\nWe will use the complete case data to perform the analysis.\n\n# Complete case data \nanalytic.data &lt;- dat.full[complete.cases(dat.full),]\ndim(analytic.data)\n#&gt; [1] 6316   15\n\nReproducibility\n\nset.seed(504)\n\nApproach by Zanutto (2006)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(bmi==\"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\nStep 2\nFor the second step, we will calculate the both unstabilized and stabilized weights. However, stabilized inverse probability weight is often recommended to prevent from extreme weights (Hern√°n and Robins 2006).\n\n# Propensity scores\nps.fit &lt;- glm(ps.formula, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps &lt;- predict(ps.fit, type = \"response\", newdata = analytic.data)\n\n# Unstabilized weight\nanalytic.data$usweight &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                     1/ps, 1/(1-ps)))\n\n# Unstabilized weight summary\nround(summary(analytic.data$usweight), 2)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.01    1.33    1.63    2.10    2.14   62.31\n\n# Stabilized weight\nanalytic.data$sweight &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps)))\n\n# Stabilized weight summary\nround(summary(analytic.data$sweight), 2)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.44    0.70    0.84    1.04    1.08   25.40\n\nWe can see that the mean of stabilized weights is 1, while it is approximately 2.1 for unstabilized weights. For both unstabilized and stabilized weights, it seems there are extreme weights, particularly for the unstabilized weights. Extreme weights could be dealt with weight truncation, typically truncated at the 1st and 99th percentiles (Cole and Hern√°n 2008).\nLet us truncate the weights at the 1st and 99th percentiles\n\n# Truncating unstabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(usweight_t = pmin(pmax(usweight, quantile(usweight, 0.01)), \n                           quantile(usweight, 0.99)))\nsummary(analytic.data$usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.055   1.335   1.629   2.020   2.140  10.912\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight_t = pmin(pmax(sweight, quantile(sweight, 0.01)), \n                          quantile(sweight, 0.99)))\nsummary(analytic.data$sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4861  0.7035  0.8400  1.0044  1.0833  4.4901\n\nStep 3\nNow we will check the distribution of the covariates by the exposure status on the pseudo population in terms of pre-specified SMD.\n\n# Covariates\nvars &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated unstabilized weight\ndesign.unstab &lt;- svydesign(ids = ~ID, weights = ~usweight_t, data = analytic.data)\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight_t, data = analytic.data)\n\n# Balance checking with truncated unstabilized weight\ntab.unstab &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.unstab, test = F)\nprint(tab.unstab, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        6199.3          6559.7               \n#&gt;   gender = Male (%)        2999.3 (48.4)   3126.6 (47.7)   0.014\n#&gt;   age (mean (SD))           48.38 (17.38)   49.83 (18.11)  0.082\n#&gt;   race (%)                                                 0.047\n#&gt;      Black                 1304.7 (21.0)   1407.8 (21.5)        \n#&gt;      Hispanic              1901.9 (30.7)   1873.7 (28.6)        \n#&gt;      Other                  956.4 (15.4)   1029.3 (15.7)        \n#&gt;      White                 2036.2 (32.8)   2249.0 (34.3)        \n#&gt;   income (%)                                               0.033\n#&gt;      &lt;25k                  1664.3 (26.8)   1829.5 (27.9)        \n#&gt;      Between.25kto54k      1986.5 (32.0)   2133.5 (32.5)        \n#&gt;      Between.55kto99k      1449.1 (23.4)   1466.6 (22.4)        \n#&gt;      Over100k              1099.4 (17.7)   1130.0 (17.2)        \n#&gt;   education (%)                                            0.019\n#&gt;      College               3469.5 (56.0)   3610.4 (55.0)        \n#&gt;      High.School           2047.7 (33.0)   2217.9 (33.8)        \n#&gt;      School                 682.1 (11.0)    731.4 (11.1)        \n#&gt;   married (%)                                              0.040\n#&gt;      Married               3728.2 (60.1)   3853.6 (58.7)        \n#&gt;      Never.married         1101.9 (17.8)   1147.0 (17.5)        \n#&gt;      Previously.married    1369.2 (22.1)   1559.1 (23.8)        \n#&gt;   cholesterol (mean (SD))  181.58 (40.93)  183.13 (43.59)  0.037\n#&gt;   diastolicBP (mean (SD))   66.31 (14.64)   66.87 (14.78)  0.038\n#&gt;   systolicBP (mean (SD))   121.04 (16.61)  123.60 (23.62)  0.125\n\n# Balance checking with truncated stabilized weight\ntab.stab &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3665.9          2677.9               \n#&gt;   gender = Male (%)        1771.7 (48.3)   1276.5 (47.7)   0.013\n#&gt;   age (mean (SD))           48.40 (17.38)   49.84 (18.12)  0.081\n#&gt;   race (%)                                                 0.048\n#&gt;      Black                  772.6 (21.1)    575.0 (21.5)        \n#&gt;      Hispanic              1126.3 (30.7)    764.6 (28.6)        \n#&gt;      Other                  561.1 (15.3)    420.5 (15.7)        \n#&gt;      White                 1206.0 (32.9)    917.9 (34.3)        \n#&gt;   income (%)                                               0.033\n#&gt;      &lt;25k                   982.8 (26.8)    747.3 (27.9)        \n#&gt;      Between.25kto54k      1176.4 (32.1)    870.8 (32.5)        \n#&gt;      Between.55kto99k       857.6 (23.4)    598.5 (22.3)        \n#&gt;      Over100k               649.2 (17.7)    461.3 (17.2)        \n#&gt;   education (%)                                            0.019\n#&gt;      College               2051.4 (56.0)   1473.3 (55.0)        \n#&gt;      High.School           1210.5 (33.0)    905.9 (33.8)        \n#&gt;      School                 404.0 (11.0)    298.7 (11.2)        \n#&gt;   married (%)                                              0.040\n#&gt;      Married               2205.9 (60.2)   1572.9 (58.7)        \n#&gt;      Never.married          650.0 (17.7)    468.0 (17.5)        \n#&gt;      Previously.married     810.1 (22.1)    637.1 (23.8)        \n#&gt;   cholesterol (mean (SD))  181.62 (40.91)  183.15 (43.61)  0.036\n#&gt;   diastolicBP (mean (SD))   66.37 (14.53)   66.87 (14.81)  0.034\n#&gt;   systolicBP (mean (SD))   121.06 (16.58)  123.63 (23.65)  0.126\n\nAs we can see, all SMDs are less than our specified cut-point of 0.2, indicating that there is good covariate balancing. next, we will fit the outcome model on the pseudo population (i.e., weighted data). Note that we must utilize the survey feature as the design for the population-level estimate. For this step, we will multiply propensity score weight and survey weight and create a new weight variable.\nStep 4 - with unstabilized weight\n\nrequire(survey)\nrequire(jtools)\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * unstabilized weight \nanalytic.data$new.usweight_t &lt;- with(analytic.data, interview.weight * usweight_t)\nsummary(analytic.data$new.usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    6437   26567   42862   77768   88106 1831548\n\n#  New weight variable in the full dataset\ndat.full$new.usweight_t &lt;- 0\ndat.full$new.usweight_t[dat.full$ID %in% analytic.data$ID] &lt;- \n  analytic.data$new.usweight_t\nsummary(dat.full$new.usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   24068   49261   55161 1831548\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.usweight_t, \n                      data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit &lt;- svyglm(out.formula, design = w.design.s, family = binomial(\"logit\"))\nsumm(fit, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.062\n\n\nPseudo-R¬≤ (McFadden)\n0.047\n\n\nAIC\n3371.907\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.164\n0.141\n0.190\n-24.213\n0.000\n\n\nbmiNot overweight\n0.280\n0.217\n0.361\n-9.814\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nStep 4 - with stabilized weight\nSimilarly, we can fit the outcome model with stabilized weights.\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight_t &lt;- with(analytic.data, interview.weight * sweight_t)\nsummary(analytic.data$new.sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2985   13311   22494   39174   43896  753678\n\n#  New weight variable in the full dataset\ndat.full$new.sweight_t &lt;- 0\ndat.full$new.sweight_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight_t\nsummary(dat.full$new.sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12115   24814   28232  753678\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.055\n\n\nPseudo-R¬≤ (McFadden)\n0.041\n\n\nAIC\n3712.451\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.164\n0.141\n0.190\n-24.235\n0.000\n\n\nbmiNot overweight\n0.280\n0.217\n0.361\n-9.815\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\nlibrary(survey)\n# Outcome model with covariates adjustment\nfit.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.194\n\n\nPseudo-R¬≤ (McFadden)\n0.149\n\n\nAIC\n3331.238\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.045\n0.015\n0.133\n-5.608\nNA\n\n\nbmiNot overweight\n0.234\n0.175\n0.312\n-9.826\nNA\n\n\ngenderMale\n1.402\n1.190\n1.652\n4.038\nNA\n\n\nage\n1.050\n1.041\n1.058\n11.793\nNA\n\n\nraceHispanic\n0.790\n0.600\n1.040\n-1.677\nNA\n\n\nraceOther\n0.849\n0.428\n1.683\n-0.470\nNA\n\n\nraceWhite\n0.541\n0.371\n0.789\n-3.192\nNA\n\n\nincomeBetween.25kto54k\n0.723\n0.478\n1.093\n-1.540\nNA\n\n\nincomeBetween.55kto99k\n0.647\n0.411\n1.018\n-1.883\nNA\n\n\nincomeOver100k\n0.476\n0.309\n0.733\n-3.371\nNA\n\n\neducationHigh.School\n0.937\n0.725\n1.211\n-0.498\nNA\n\n\neducationSchool\n0.930\n0.568\n1.524\n-0.286\nNA\n\n\nmarriedNever.married\n0.895\n0.628\n1.275\n-0.615\nNA\n\n\nmarriedPreviously.married\n0.791\n0.529\n1.184\n-1.137\nNA\n\n\ncholesterol\n0.993\n0.990\n0.997\n-3.760\nNA\n\n\ndiastolicBP\n1.007\n0.999\n1.014\n1.744\nNA\n\n\nsystolicBP\n1.002\n0.993\n1.012\n0.529\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.095813   0.552073  -5.608 4.99e-05 ***\n#&gt; bmiNot overweight         -1.454514   0.148024  -9.826 6.29e-08 ***\n#&gt; genderMale                 0.338112   0.083733   4.038  0.00107 ** \n#&gt; age                        0.048468   0.004110  11.793 5.48e-09 ***\n#&gt; raceHispanic              -0.235285   0.140287  -1.677  0.11422    \n#&gt; raceOther                 -0.164132   0.349277  -0.470  0.64517    \n#&gt; raceWhite                 -0.613643   0.192233  -3.192  0.00606 ** \n#&gt; incomeBetween.25kto54k    -0.324911   0.211048  -1.540  0.14451    \n#&gt; incomeBetween.55kto99k    -0.435472   0.231288  -1.883  0.07927 .  \n#&gt; incomeOver100k            -0.742995   0.220399  -3.371  0.00420 ** \n#&gt; educationHigh.School      -0.065157   0.130721  -0.498  0.62540    \n#&gt; educationSchool           -0.072034   0.251806  -0.286  0.77874    \n#&gt; marriedNever.married      -0.110932   0.180496  -0.615  0.54803    \n#&gt; marriedPreviously.married -0.233875   0.205646  -1.137  0.27327    \n#&gt; cholesterol               -0.006873   0.001828  -3.760  0.00189 ** \n#&gt; diastolicBP                0.006728   0.003859   1.744  0.10169    \n#&gt; systolicBP                 0.002438   0.004605   0.529  0.60430    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9452878)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\nTip\n\n\n\nDouble adjustment:\nAs explained in the previous chapter, double adjustment should be applied thoughtfully, with careful consideration of model specification, covariate selection, and underlying assumptions to ensure valid and reliable results. Always consider the specific context of the study and consult statistical guidelines or experts when applying advanced methods like double adjustment in propensity score analysis.\n\n\nApproach by DuGoff et al.¬†(2014)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula2 &lt;- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP + \n                           psu + strata + interview.weight)\n\nStep 2\n\n# Propensity scores\nps.fit2 &lt;- glm(ps.formula2, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps2 &lt;- fitted(ps.fit2)\n\n# Stabilized weight\nanalytic.data$sweight.dug &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps2, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps2)))\nsummary(analytic.data$sweight.dug)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4349  0.6942  0.8296  1.0307  1.0859 23.0226\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight.dug_t = pmin(pmax(sweight.dug, quantile(sweight.dug, 0.01)), \n                          quantile(sweight.dug, 0.99)))\nsummary(analytic.data$sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4802  0.6942  0.8296  1.0019  1.0859  4.4041\n\nStep 3\n\n# Balance checking\ncov2 &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight.dug_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab2 &lt;- svyCreateTableOne(vars = cov2, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab2, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3652.9          2675.0               \n#&gt;   gender = Male (%)        1766.5 (48.4)   1276.8 (47.7)   0.013\n#&gt;   age (mean (SD))           48.47 (17.42)   49.80 (17.99)  0.075\n#&gt;   race (%)                                                 0.056\n#&gt;      Black                  769.1 (21.1)    568.7 (21.3)        \n#&gt;      Hispanic              1121.5 (30.7)    755.4 (28.2)        \n#&gt;      Other                  553.0 (15.1)    417.0 (15.6)        \n#&gt;      White                 1209.4 (33.1)    933.9 (34.9)        \n#&gt;   income (%)                                               0.023\n#&gt;      &lt;25k                   982.1 (26.9)    734.0 (27.4)        \n#&gt;      Between.25kto54k      1172.1 (32.1)    871.1 (32.6)        \n#&gt;      Between.55kto99k       846.1 (23.2)    597.0 (22.3)        \n#&gt;      Over100k               652.6 (17.9)    472.9 (17.7)        \n#&gt;   education (%)                                            0.012\n#&gt;      College               2049.1 (56.1)   1484.2 (55.5)        \n#&gt;      High.School           1202.5 (32.9)    893.7 (33.4)        \n#&gt;      School                 401.3 (11.0)    297.1 (11.1)        \n#&gt;   married (%)                                              0.035\n#&gt;      Married               2201.3 (60.3)   1581.2 (59.1)        \n#&gt;      Never.married          647.4 (17.7)    465.6 (17.4)        \n#&gt;      Previously.married     804.2 (22.0)    628.2 (23.5)        \n#&gt;   cholesterol (mean (SD))  181.74 (40.94)  182.97 (43.25)  0.029\n#&gt;   diastolicBP (mean (SD))   66.46 (14.42)   66.88 (14.79)  0.029\n#&gt;   systolicBP (mean (SD))   121.14 (16.59)  123.39 (23.38)  0.111\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.dug_t &lt;- with(analytic.data, interview.weight * sweight.dug_t)\nsummary(analytic.data$new.sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2996   13343   22260   39436   43914  855144\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.dug_t &lt;- 0\ndat.full$new.sweight.dug_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight.dug_t\nsummary(dat.full$new.sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12058   24980   28225  855144\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.dug_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.dug &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.dug, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.060\n\n\nPseudo-R¬≤ (McFadden)\n0.045\n\n\nAIC\n3518.691\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.161\n0.140\n0.185\n-26.027\n0.000\n\n\nbmiNot overweight\n0.272\n0.203\n0.364\n-8.727\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit2.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.196\n\n\nPseudo-R¬≤ (McFadden)\n0.152\n\n\nAIC\n3154.781\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.043\n0.014\n0.131\n-5.537\nNA\n\n\nbmiNot overweight\n0.235\n0.169\n0.325\n-8.730\nNA\n\n\ngenderMale\n1.393\n1.150\n1.688\n3.381\nNA\n\n\nage\n1.049\n1.042\n1.056\n13.564\nNA\n\n\nraceHispanic\n0.813\n0.624\n1.061\n-1.524\nNA\n\n\nraceOther\n0.841\n0.454\n1.558\n-0.549\nNA\n\n\nraceWhite\n0.535\n0.369\n0.774\n-3.316\nNA\n\n\nincomeBetween.25kto54k\n0.720\n0.492\n1.053\n-1.693\nNA\n\n\nincomeBetween.55kto99k\n0.659\n0.440\n0.985\n-2.035\nNA\n\n\nincomeOver100k\n0.491\n0.338\n0.713\n-3.735\nNA\n\n\neducationHigh.School\n0.930\n0.753\n1.150\n-0.667\nNA\n\n\neducationSchool\n0.994\n0.625\n1.580\n-0.026\nNA\n\n\nmarriedNever.married\n0.982\n0.671\n1.439\n-0.092\nNA\n\n\nmarriedPreviously.married\n0.783\n0.534\n1.150\n-1.245\nNA\n\n\ncholesterol\n0.993\n0.989\n0.996\n-3.913\nNA\n\n\ndiastolicBP\n1.006\n0.998\n1.013\n1.426\nNA\n\n\nsystolicBP\n1.004\n0.995\n1.012\n0.853\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit2.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.143943   0.567807  -5.537 5.70e-05 ***\n#&gt; bmiNot overweight         -1.450047   0.166099  -8.730 2.89e-07 ***\n#&gt; genderMale                 0.331561   0.098056   3.381  0.00411 ** \n#&gt; age                        0.047790   0.003523  13.564 7.97e-10 ***\n#&gt; raceHispanic              -0.206657   0.135568  -1.524  0.14822    \n#&gt; raceOther                 -0.172590   0.314341  -0.549  0.59105    \n#&gt; raceWhite                 -0.625821   0.188746  -3.316  0.00471 ** \n#&gt; incomeBetween.25kto54k    -0.328863   0.194223  -1.693  0.11107    \n#&gt; incomeBetween.55kto99k    -0.417687   0.205223  -2.035  0.05989 .  \n#&gt; incomeOver100k            -0.711500   0.190509  -3.735  0.00199 ** \n#&gt; educationHigh.School      -0.072053   0.108040  -0.667  0.51496    \n#&gt; educationSchool           -0.006136   0.236588  -0.026  0.97965    \n#&gt; marriedNever.married      -0.017832   0.194660  -0.092  0.92822    \n#&gt; marriedPreviously.married -0.244058   0.196041  -1.245  0.23226    \n#&gt; cholesterol               -0.007067   0.001806  -3.913  0.00138 ** \n#&gt; diastolicBP                0.005516   0.003869   1.426  0.17440    \n#&gt; systolicBP                 0.003739   0.004381   0.853  0.40682    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9373219)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\nApproach by Austin et al.¬†(2018)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula3 &lt;- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\n# Survey design\nrequire(survey)\nanalytic.design &lt;- svydesign(id = ~psu, weights = ~interview.weight, strata = ~strata,\n                             data = analytic.data, nest = TRUE)\n\nStep 2\n\n# Propensity scores\nps.fit3 &lt;- svyglm(ps.formula3, design = analytic.design, family = binomial(\"logit\"))\nanalytic.data$ps3 &lt;- fitted(ps.fit3)\n\n# Stabilized weight\nanalytic.data$sweight.aus &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps3, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps3)))\nsummary(analytic.data$sweight.aus)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4436  0.7138  0.8410  1.0392  1.0645 26.2776\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight.aus_t = pmin(pmax(sweight.aus, quantile(sweight.aus, 0.01)), \n                          quantile(sweight.aus, 0.99)))\nsummary(analytic.data$sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.5198  0.7138  0.8410  1.0071  1.0645  4.8830\n\nStep 3\n\n# Balance checking\nvars &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n          \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight.aus_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab.aus &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab.aus, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3434.3          2926.8               \n#&gt;   gender = Male (%)        1609.8 (46.9)   1458.6 (49.8)   0.059\n#&gt;   age (mean (SD))           48.51 (17.32)   49.95 (18.14)  0.081\n#&gt;   race (%)                                                 0.099\n#&gt;      Black                  737.1 (21.5)    623.8 (21.3)        \n#&gt;      Hispanic              1085.0 (31.6)    826.0 (28.2)        \n#&gt;      Other                  471.2 (13.7)    489.7 (16.7)        \n#&gt;      White                 1141.0 (33.2)    987.3 (33.7)        \n#&gt;   income (%)                                               0.041\n#&gt;      &lt;25k                   933.7 (27.2)    813.0 (27.8)        \n#&gt;      Between.25kto54k      1108.2 (32.3)    946.8 (32.3)        \n#&gt;      Between.55kto99k       776.7 (22.6)    685.4 (23.4)        \n#&gt;      Over100k               615.7 (17.9)    481.6 (16.5)        \n#&gt;   education (%)                                            0.039\n#&gt;      College               1908.2 (55.6)   1601.1 (54.7)        \n#&gt;      High.School           1129.9 (32.9)   1011.4 (34.6)        \n#&gt;      School                 396.2 (11.5)    314.3 (10.7)        \n#&gt;   married (%)                                              0.030\n#&gt;      Married               2063.9 (60.1)   1737.6 (59.4)        \n#&gt;      Never.married          605.8 (17.6)    501.4 (17.1)        \n#&gt;      Previously.married     764.6 (22.3)    687.8 (23.5)        \n#&gt;   cholesterol (mean (SD))  182.67 (41.11)  182.70 (43.38)  0.001\n#&gt;   diastolicBP (mean (SD))   66.76 (14.28)   66.85 (14.85)  0.006\n#&gt;   systolicBP (mean (SD))   121.39 (16.74)  123.98 (23.72)  0.126\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.aus_t &lt;- with(analytic.data, interview.weight * sweight.aus_t)\nsummary(analytic.data$new.sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    3255   13604   22527   38953   43663  819622\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.aus_t &lt;- 0\ndat.full$new.sweight.aus_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight.aus_t\nsummary(dat.full$new.sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12356   24675   28169  819622\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.aus_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.aus &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.aus, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.055\n\n\nPseudo-R¬≤ (McFadden)\n0.041\n\n\nAIC\n3622.951\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.162\n0.140\n0.187\n-24.912\n0.000\n\n\nbmiNot overweight\n0.291\n0.225\n0.377\n-9.397\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit3.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit3.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.193\n\n\nPseudo-R¬≤ (McFadden)\n0.149\n\n\nAIC\n3247.730\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.045\n0.015\n0.129\n-5.735\nNA\n\n\nbmiNot overweight\n0.233\n0.175\n0.311\n-9.886\nNA\n\n\ngenderMale\n1.416\n1.186\n1.690\n3.850\nNA\n\n\nage\n1.049\n1.041\n1.057\n12.066\nNA\n\n\nraceHispanic\n0.773\n0.585\n1.022\n-1.808\nNA\n\n\nraceOther\n0.867\n0.451\n1.666\n-0.428\nNA\n\n\nraceWhite\n0.520\n0.353\n0.765\n-3.316\nNA\n\n\nincomeBetween.25kto54k\n0.700\n0.461\n1.062\n-1.677\nNA\n\n\nincomeBetween.55kto99k\n0.650\n0.407\n1.039\n-1.801\nNA\n\n\nincomeOver100k\n0.467\n0.303\n0.721\n-3.437\nNA\n\n\neducationHigh.School\n0.948\n0.744\n1.209\n-0.428\nNA\n\n\neducationSchool\n0.976\n0.598\n1.592\n-0.099\nNA\n\n\nmarriedNever.married\n0.896\n0.629\n1.276\n-0.609\nNA\n\n\nmarriedPreviously.married\n0.784\n0.525\n1.169\n-1.195\nNA\n\n\ncholesterol\n0.993\n0.990\n0.997\n-3.653\nNA\n\n\ndiastolicBP\n1.005\n0.998\n1.013\n1.412\nNA\n\n\nsystolicBP\n1.004\n0.995\n1.012\n0.826\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit3.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.109181   0.542121  -5.735 3.94e-05 ***\n#&gt; bmiNot overweight         -1.456869   0.147369  -9.886 5.81e-08 ***\n#&gt; genderMale                 0.347655   0.090292   3.850  0.00157 ** \n#&gt; age                        0.047457   0.003933  12.066 4.01e-09 ***\n#&gt; raceHispanic              -0.257364   0.142343  -1.808  0.09069 .  \n#&gt; raceOther                 -0.142755   0.333175  -0.428  0.67440    \n#&gt; raceWhite                 -0.653902   0.197190  -3.316  0.00470 ** \n#&gt; incomeBetween.25kto54k    -0.357122   0.212996  -1.677  0.11432    \n#&gt; incomeBetween.55kto99k    -0.430289   0.238964  -1.801  0.09190 .  \n#&gt; incomeOver100k            -0.761311   0.221498  -3.437  0.00367 ** \n#&gt; educationHigh.School      -0.052990   0.123816  -0.428  0.67475    \n#&gt; educationSchool           -0.024734   0.249884  -0.099  0.92246    \n#&gt; marriedNever.married      -0.109989   0.180532  -0.609  0.55148    \n#&gt; marriedPreviously.married -0.243725   0.203990  -1.195  0.25072    \n#&gt; cholesterol               -0.006605   0.001808  -3.653  0.00235 ** \n#&gt; diastolicBP                0.005253   0.003721   1.412  0.17842    \n#&gt; systolicBP                 0.003618   0.004380   0.826  0.42172    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9416613)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\nReferences\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. ‚ÄúPropensity Score Matching and Complex Surveys.‚Äù Statistical Methods in Medical Research 27 (4): 1240‚Äì57.\n\n\nCole, Stephen R, and Miguel A Hern√°n. 2008. ‚ÄúConstructing Inverse Probability Weights for Marginal Structural Models.‚Äù American Journal of Epidemiology 168 (6): 656‚Äì64.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. ‚ÄúGeneralizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.‚Äù Health Services Research 49 (1): 284‚Äì303.\n\n\nHern√°n, Miguel A, and James M Robins. 2006. ‚ÄúEstimating Causal Effects from Epidemiological Data.‚Äù Journal of Epidemiology and Community Health 60 (7): 578.\n\n\nZanutto, Elaine L. 2006. ‚ÄúA Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.‚Äù Journal of Data Science 4 (1): 67‚Äì91.",
    "crumbs": [
      "Propensity score",
      "PS Weighting (US)"
    ]
  },
  {
    "objectID": "propensityscore7.html",
    "href": "propensityscore7.html",
    "title": "PSM with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score matching (PSM) with multiple imputation, focusing on specific subpopulations defined by the study‚Äôs eligibility criteria. We will use PSM as per DuGoff, Schuler, and Stuart (2014) recommendation, with SMD cut-point 0.2 and adjust for imbalanced and/or all covariates in the outcome model, if any.\nThe modified dataset from NHANES 2017- 2018, which was also used in missing data subpopulations chapter, will be used. This example aims to demonstrate how to do the missing data analysis using multiple imputation with PSM in the context of complex surveys.\nPre-processing\nLoad data\nLet us import the dataset:\n\nload(\"Data/missingdata/MIexample.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\nVariables\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nData pre-processng\n\n# Categorical age\ndat.full$age.cat &lt;- with(dat.full, ifelse(age &gt;= 20 & age &lt; 50, \"20-49\", \n                                  ifelse(age &gt;= 50 & age &lt; 65, \"50-64\", \"65+\")))\ndat.full$age.cat &lt;- factor(dat.full$age.cat, levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.full$age.cat, useNA = \"always\")\n#&gt; \n#&gt; 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  2500  1569  5185     0\n\n# Recode rheumatoid to arthritis\ndat.full$arthritis &lt;- car::recode(dat.full$rheumatoid, \" 'No' = 'No arthritis';\n                                      'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.full$arthritis, useNA = \"always\")\n#&gt; \n#&gt;         No arthritis Rheumatoid arthritis                 &lt;NA&gt; \n#&gt;                 3857                  337                 5060\n\nSubsetting according to eligibility\nWe will create the analytic dataset with\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\n# Drop &lt; 20 years\ndat.with.miss &lt;- subset(dat.full, age &gt;= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3857  337 1375\n\n# Drop missing in outcome and exposure - dataset with missing values only in covariates\ndat.analytic &lt;- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#&gt; [1] 4191\n\nWe have 4,191 participants in our analytic dataset. The general strategy of solution to implement PSM with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nApply PSM on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin‚Äôs rule\nvariable summary\nLet us see the summary statistics as we did in the missing data analysis:\n\n# Keep only relevant variables\nvars &lt;-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\", \"arthritis\", \"age.cat\", \n           \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 &lt;- dat.analytic[, vars]\n\n# Create Table 1\nvars &lt;- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"cvd\", data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#&gt;                  Stratified by cvd\n#&gt;                   level                     Overall      No          \n#&gt;   n                                          4191         3823       \n#&gt;   arthritis       No arthritis               3854         3580       \n#&gt;                   Rheumatoid arthritis        337          243       \n#&gt;   age.cat         20-49                      2280         2240       \n#&gt;                   50-64                      1097          979       \n#&gt;                   65+                         814          604       \n#&gt;   sex             Male                       2126         1884       \n#&gt;                   Female                     2065         1939       \n#&gt;   education       Less than high school       828          728       \n#&gt;                   High school                2292         2094       \n#&gt;                   College graduate or above  1063          993       \n#&gt;   race            White                      1275         1113       \n#&gt;                   Black                       998          898       \n#&gt;                   Hispanic                   1015          958       \n#&gt;                   Others                      903          854       \n#&gt;   income          less than $20,000           659          557       \n#&gt;                   $20,000 to $74,999         1967         1796       \n#&gt;                   $75,000 and Over           1143         1079       \n#&gt;   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#&gt;   smoking         Never smoker               2570         2427       \n#&gt;                   Previous smoker             882          726       \n#&gt;                   Current smoker              739          670       \n#&gt;   htn             No                         1424         1380       \n#&gt;                   Yes                        2415         2107       \n#&gt;   diabetes        No                         3622         3396       \n#&gt;                   Yes                         566          424       \n#&gt;                  Stratified by cvd\n#&gt;                   Yes         \n#&gt;   n                 368       \n#&gt;   arthritis         274       \n#&gt;                      94       \n#&gt;   age.cat            40       \n#&gt;                     118       \n#&gt;                     210       \n#&gt;   sex               242       \n#&gt;                     126       \n#&gt;   education         100       \n#&gt;                     198       \n#&gt;                      70       \n#&gt;   race              162       \n#&gt;                     100       \n#&gt;                      57       \n#&gt;                      49       \n#&gt;   income            102       \n#&gt;                     171       \n#&gt;                      64       \n#&gt;   bmi (mean (SD)) 30.09 (7.29)\n#&gt;   smoking           143       \n#&gt;                     156       \n#&gt;                      69       \n#&gt;   htn                44       \n#&gt;                     308       \n#&gt;   diabetes          226       \n#&gt;                     142\n\n\n# missingness\nDataExplorer::plot_missing(dat.analytic2)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ‚Ñπ Please use tidy evaluation idioms with `aes()`.\n#&gt; ‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ‚Ñπ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nDealing with missing values in covariates\nSimilar to the previous exercise, we will create 5 imputed datasets with 3 iterations, and the predictive mean matching method for bmi and income. We will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nStep 0: Set up the imputation model\n\n# Step 0: Set imputation model\nini &lt;- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred &lt;- ini$pred\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",] &lt;- 0\n\n# Do not use survey weight or PSU variable as auxiliary variables\npred[,\"studyid\"] &lt;- pred[\"studyid\",] &lt;- 0\npred[,\"psu\"] &lt;- pred[\"psu\",] &lt;- 0\npred[,\"survey.weight\"] &lt;- pred[\"survey.weight\",] &lt;- 0\n\n# Set imputation method\nmeth &lt;- ini$meth\nmeth[\"bmi\"] &lt;- \"pmm\"\nmeth[\"income\"] &lt;- \"pmm\"\nmeth\n#&gt;       studyid survey.weight           psu        strata           cvd \n#&gt;            \"\"            \"\"            \"\"            \"\"            \"\" \n#&gt;     arthritis       age.cat           sex     education          race \n#&gt;            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#&gt;        income           bmi       smoking           htn      diabetes \n#&gt;         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\nStep 1: Imputing missing values using mice for eligible subjects\n\n# Step 1: impute the incomplete data\nimputation &lt;- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\nLet us save the datasets.\n\nsave(dat.full, dat.analytic, dat.analytic2, imputation, \n     file = \"Data/propensityscore/analytic_imputed.RData\")\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nimpdata &lt;- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#&gt; [1] 20955    17\n\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id &lt;- NULL\n\n# Number of subjects\nnrow(impdata)\n#&gt; [1] 20955\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n\n\n\n\n\n\n\nThere is no missing value after imputation. There is an additional variable (.imp) in the imputed dataset, which goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\nStep 2: PSM steps 1-3 by DuGoff et al.¬†(2014)\nOur next step is to use steps 1-3 of the PSM analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Match an exposed subject without replacement within the caliper of 0.2 times the standard deviation of the logit of PS.\nStep 2.3: Balance checking using SMD. Consider SMD &lt;0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and matching for each imputed dataset\n\n# Null vector or list to store values\ncaliper &lt;- NULL\ndat.matched &lt;- match.obj &lt;- list(NULL)\n\nm &lt;- 5 # 5 imputed dataset\n\n# PSM on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed &lt;- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps &lt;- fitted(ps.fit)\n  \n  # Caliper fixing to 0.2*sd(logit of PS)\n  caliper[ii] &lt;- 0.2*sd(log(dat.imputed$ps/(1-dat.imputed$ps)))\n  \n  # 1:1 PS matching  \n  set.seed(504)\n  match.obj[[ii]] &lt;- matchit(ps.formula, data = dat.imputed,\n                        distance = dat.imputed$ps, \n                        method = \"nearest\", \n                        replace = FALSE,\n                        caliper = caliper[ii], \n                        ratio = 1)\n  dat.imputed$ps &lt;- match.obj[[ii]]$distance\n  \n  # Extract matched data\n  dat.matched[[ii]] &lt;- match.data(match.obj[[ii]]) \n}\nmatch.obj\n#&gt; [[1]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 668 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[2]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.02)\n#&gt;  - number of obs.: 4191 (original), 666 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[3]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 672 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[4]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 664 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[5]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 662 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n\n\n# Dimension of each of the matched dataset\nlapply(dat.matched, dim)\n#&gt; [[1]]\n#&gt; [1] 668  20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 666  20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 672  20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 664  20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 662  20\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m &lt;- list(NULL)\nfor (ii in 1:m) {\n  # Matched data\n  dat &lt;- dat.matched[[ii]]\n  \n  # Covariates\n  vars &lt;- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Balance checking \n  tab1m[[ii]] &lt;- CreateTableOne(strata = \"arthritis\", vars = vars, data = dat, test = F)\n}\nprint(tab1m, smd = TRUE)\n#&gt; [[1]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              334           334                      \n#&gt;   age.cat (%)                                                      0.018\n#&gt;      20-49                        51 (15.3)     52 (15.6)               \n#&gt;      50-64                       131 (39.2)    133 (39.8)               \n#&gt;      65+                         152 (45.5)    149 (44.6)               \n#&gt;   sex = Female (%)               172 (51.5)    178 (53.3)          0.036\n#&gt;   education (%)                                                    0.034\n#&gt;      Less than high school        80 (24.0)     84 (25.1)               \n#&gt;      High school                 204 (61.1)    203 (60.8)               \n#&gt;      College graduate or above    50 (15.0)     47 (14.1)               \n#&gt;   race (%)                                                         0.049\n#&gt;      White                       105 (31.4)    103 (30.8)               \n#&gt;      Black                       107 (32.0)    112 (33.5)               \n#&gt;      Hispanic                     67 (20.1)     69 (20.7)               \n#&gt;      Others                       55 (16.5)     50 (15.0)               \n#&gt;   income (%)                                                       0.073\n#&gt;      less than $20,000            95 (28.4)    100 (29.9)               \n#&gt;      $20,000 to $74,999          162 (48.5)    167 (50.0)               \n#&gt;      $75,000 and Over             77 (23.1)     67 (20.1)               \n#&gt;   bmi (mean (SD))              30.62 (8.16)  30.54 (7.34)          0.011\n#&gt;   smoking (%)                                                      0.052\n#&gt;      Never smoker                158 (47.3)    161 (48.2)               \n#&gt;      Previous smoker             102 (30.5)    106 (31.7)               \n#&gt;      Current smoker               74 (22.2)     67 (20.1)               \n#&gt;   htn = Yes (%)                  283 (84.7)    279 (83.5)          0.033\n#&gt;   diabetes = Yes (%)             106 (31.7)    100 (29.9)          0.039\n#&gt; \n#&gt; [[2]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              333           333                      \n#&gt;   age.cat (%)                                                      0.055\n#&gt;      20-49                        50 (15.0)     52 (15.6)               \n#&gt;      50-64                       142 (42.6)    133 (39.9)               \n#&gt;      65+                         141 (42.3)    148 (44.4)               \n#&gt;   sex = Female (%)               172 (51.7)    176 (52.9)          0.024\n#&gt;   education (%)                                                    0.068\n#&gt;      Less than high school        92 (27.6)     84 (25.2)               \n#&gt;      High school                 191 (57.4)    202 (60.7)               \n#&gt;      College graduate or above    50 (15.0)     47 (14.1)               \n#&gt;   race (%)                                                         0.146\n#&gt;      White                        91 (27.3)    104 (31.2)               \n#&gt;      Black                       100 (30.0)    110 (33.0)               \n#&gt;      Hispanic                     79 (23.7)     69 (20.7)               \n#&gt;      Others                       63 (18.9)     50 (15.0)               \n#&gt;   income (%)                                                       0.131\n#&gt;      less than $20,000           115 (34.5)     96 (28.8)               \n#&gt;      $20,000 to $74,999          166 (49.8)    175 (52.6)               \n#&gt;      $75,000 and Over             52 (15.6)     62 (18.6)               \n#&gt;   bmi (mean (SD))              30.50 (7.88)  30.51 (7.46)          0.001\n#&gt;   smoking (%)                                                      0.015\n#&gt;      Never smoker                160 (48.0)    161 (48.3)               \n#&gt;      Previous smoker             104 (31.2)    105 (31.5)               \n#&gt;      Current smoker               69 (20.7)     67 (20.1)               \n#&gt;   htn = Yes (%)                  288 (86.5)    278 (83.5)          0.084\n#&gt;   diabetes = Yes (%)              97 (29.1)     99 (29.7)          0.013\n#&gt; \n#&gt; [[3]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              336           336                      \n#&gt;   age.cat (%)                                                      0.077\n#&gt;      20-49                        46 (13.7)     52 (15.5)               \n#&gt;      50-64                       145 (43.2)    133 (39.6)               \n#&gt;      65+                         145 (43.2)    151 (44.9)               \n#&gt;   sex = Female (%)               176 (52.4)    179 (53.3)          0.018\n#&gt;   education (%)                                                    0.076\n#&gt;      Less than high school        80 (23.8)     86 (25.6)               \n#&gt;      High school                 215 (64.0)    203 (60.4)               \n#&gt;      College graduate or above    41 (12.2)     47 (14.0)               \n#&gt;   race (%)                                                         0.084\n#&gt;      White                       114 (33.9)    105 (31.2)               \n#&gt;      Black                       114 (33.9)    112 (33.3)               \n#&gt;      Hispanic                     59 (17.6)     69 (20.5)               \n#&gt;      Others                       49 (14.6)     50 (14.9)               \n#&gt;   income (%)                                                       0.118\n#&gt;      less than $20,000            87 (25.9)    104 (31.0)               \n#&gt;      $20,000 to $74,999          183 (54.5)    166 (49.4)               \n#&gt;      $75,000 and Over             66 (19.6)     66 (19.6)               \n#&gt;   bmi (mean (SD))              30.56 (7.71)  30.54 (7.53)          0.003\n#&gt;   smoking (%)                                                      0.127\n#&gt;      Never smoker                180 (53.6)    161 (47.9)               \n#&gt;      Previous smoker              89 (26.5)    107 (31.8)               \n#&gt;      Current smoker               67 (19.9)     68 (20.2)               \n#&gt;   htn = Yes (%)                  280 (83.3)    281 (83.6)          0.008\n#&gt;   diabetes = Yes (%)             104 (31.0)    102 (30.4)          0.013\n#&gt; \n#&gt; [[4]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              332           332                      \n#&gt;   age.cat (%)                                                      0.062\n#&gt;      20-49                        48 (14.5)     52 (15.7)               \n#&gt;      50-64                       127 (38.3)    133 (40.1)               \n#&gt;      65+                         157 (47.3)    147 (44.3)               \n#&gt;   sex = Female (%)               174 (52.4)    175 (52.7)          0.006\n#&gt;   education (%)                                                    0.059\n#&gt;      Less than high school        79 (23.8)     85 (25.6)               \n#&gt;      High school                 200 (60.2)    200 (60.2)               \n#&gt;      College graduate or above    53 (16.0)     47 (14.2)               \n#&gt;   race (%)                                                         0.034\n#&gt;      White                       100 (30.1)    105 (31.6)               \n#&gt;      Black                       111 (33.4)    108 (32.5)               \n#&gt;      Hispanic                     71 (21.4)     69 (20.8)               \n#&gt;      Others                       50 (15.1)     50 (15.1)               \n#&gt;   income (%)                                                       0.067\n#&gt;      less than $20,000            93 (28.0)     95 (28.6)               \n#&gt;      $20,000 to $74,999          165 (49.7)    172 (51.8)               \n#&gt;      $75,000 and Over             74 (22.3)     65 (19.6)               \n#&gt;   bmi (mean (SD))              30.06 (7.85)  30.46 (7.49)          0.052\n#&gt;   smoking (%)                                                      0.053\n#&gt;      Never smoker                166 (50.0)    161 (48.5)               \n#&gt;      Previous smoker              97 (29.2)    105 (31.6)               \n#&gt;      Current smoker               69 (20.8)     66 (19.9)               \n#&gt;   htn = Yes (%)                  281 (84.6)    278 (83.7)          0.025\n#&gt;   diabetes = Yes (%)              93 (28.0)     98 (29.5)          0.033\n#&gt; \n#&gt; [[5]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              331           331                      \n#&gt;   age.cat (%)                                                      0.026\n#&gt;      20-49                        50 (15.1)     52 (15.7)               \n#&gt;      50-64                       137 (41.4)    133 (40.2)               \n#&gt;      65+                         144 (43.5)    146 (44.1)               \n#&gt;   sex = Female (%)               172 (52.0)    175 (52.9)          0.018\n#&gt;   education (%)                                                    0.025\n#&gt;      Less than high school        86 (26.0)     84 (25.4)               \n#&gt;      High school                 196 (59.2)    200 (60.4)               \n#&gt;      College graduate or above    49 (14.8)     47 (14.2)               \n#&gt;   race (%)                                                         0.026\n#&gt;      White                        99 (29.9)    103 (31.1)               \n#&gt;      Black                       111 (33.5)    109 (32.9)               \n#&gt;      Hispanic                     70 (21.1)     69 (20.8)               \n#&gt;      Others                       51 (15.4)     50 (15.1)               \n#&gt;   income (%)                                                       0.074\n#&gt;      less than $20,000            84 (25.4)     94 (28.4)               \n#&gt;      $20,000 to $74,999          181 (54.7)    170 (51.4)               \n#&gt;      $75,000 and Over             66 (19.9)     67 (20.2)               \n#&gt;   bmi (mean (SD))              30.17 (7.58)  30.49 (7.86)          0.042\n#&gt;   smoking (%)                                                      0.027\n#&gt;      Never smoker                165 (49.8)    161 (48.6)               \n#&gt;      Previous smoker             104 (31.4)    105 (31.7)               \n#&gt;      Current smoker               62 (18.7)     65 (19.6)               \n#&gt;   htn = Yes (%)                  276 (83.4)    277 (83.7)          0.008\n#&gt;   diabetes = Yes (%)             102 (30.8)     97 (29.3)          0.033\n\nFor each of the datasets, all SMDs are less than our specified cut-point of 0.2.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Remember, we must utilize survey features to correctly estimate the standard error.\n3.1 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction and unmatched) with the matched datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible &lt;- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat &lt;- dat.matched[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible &lt;- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] &lt;- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] &lt;- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp &lt;- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#&gt; [[1]]\n#&gt; [1] 8586   19\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 8588   19\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 8582   19\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 8590   19\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 8592   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.matched[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"arthritis\"     \"age.cat\"       \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \".imp\"         \n#&gt; [17] \"ps\"            \"distance\"      \"weights\"       \"subclass\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#&gt; [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\nFour variables (ps, distance, weights, and subclass) are unavailable in our full, analytic, or ineligible datasets but in the matched datasets. We need to create these 4 variables in the ineligible datasets.\n\ndat.ineligible2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  dat &lt;- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible &lt;- NULL\n  \n  # Create ps, distance, weights, and subclass\n  dat$ps &lt;- NA\n  dat$distance &lt;- NA\n  dat$weights &lt;- NA\n  dat$subclass &lt;- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars &lt;- names(dat.matched[[1]])\n  dat &lt;- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] &lt;- dat\n}\n\nWe created ps, distance, weights, and subclass with missing values for the ineligible participants. Note that it doesn‚Äôt matter whether there are missing covariate values for ineligible. Since we will create the design on the full dataset and subset the design for only eligible (i.e., matched participants), missing covariate values for ineligible will not impact our analysis.\n3.2 Combining eligible (matched) and ineligible (unimputed + unmatched) subjects\nNow, we will merge matched eligible and unimputed and unmatched ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 &lt;- data.frame(dat.matched[[ii]])\n  d1$eligible &lt;- 1\n  \n  # Ineligible\n  d2 &lt;- data.frame(dat.ineligible2[[ii]])\n  d2$eligible &lt;- 0\n  \n  # Full data\n  d3 &lt;- rbind(d1, d2)\n  dat.full2[[ii]] &lt;- d3\n}\nlapply(dat.full2, dim)\n#&gt; [[1]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 9254   21\n\n# Stacked dataset\ndat.stacked &lt;- rbindlist(dat.full2)\ndim(dat.stacked)\n#&gt; [1] 46270    21\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset.\n\nallImputations &lt;- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 &lt;- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design &lt;- subset(w.design0, eligible == 1) \n#&gt; Warning in subset.svyimputationList(w.design0, eligible == 1): subset differed\n#&gt; between imputations\n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#&gt; [[1]]\n#&gt; [1] 668  21\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 666  21\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 672  21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 664  21\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 662  21\n\nNow we will run the design-adjusted logistic regression on and pool the estimate using Rubin‚Äôs rule:\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis, family = quasibinomial))\nres &lt;- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) &lt;- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#&gt;               (Intercept) arthritisRheumatoid arthritis\n#&gt; OR from m = 1        0.18                          1.53\n#&gt; OR from m = 2        0.24                          1.09\n#&gt; OR from m = 3        0.21                          1.28\n#&gt; OR from m = 4        0.17                          1.61\n#&gt; OR from m = 5        0.12                          2.32\n\nStep 3.5: Pooling estimates\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit2)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nReferences\n\n\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. ‚ÄúGeneralizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.‚Äù Health Services Research 49 (1): 284‚Äì303.",
    "crumbs": [
      "Propensity score",
      "PSM with MI in subset"
    ]
  },
  {
    "objectID": "propensityscore8.html",
    "href": "propensityscore8.html",
    "title": "PSW with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score (PS) weighting with multiple imputation (MI), focusing on specific subpopulations defined by the study‚Äôs eligibility criteria. Similar to the previous chapter on PSM with MI for subpopulation, the modified dataset from NHANES 2017- 2018, will be used.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/analytic_imputed.RData\")\nls()\n#&gt; [1] \"dat.analytic\"  \"dat.analytic2\" \"dat.full\"      \"imputation\"\n\n\n\ndat.full: Full dataset of 9,254 subjects\n\ndat.analytic and dat.analytic2: Analytic dataset of 4,191 participants with only missing in the covariates. There are no missing values for the exposure or outcomes.\n\nimputation: m = 5 imputed datasets from dat.analytic2 using MI.\n\nThe general strategy of solution to implement PS weighting with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects.\nApply PS weghting on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin‚Äôs rule\nDealing with missing values in covariates\nStep 1: Imputing missing values using mice for eligible subjects\nWe already completed this step in the previous chapter, where we imputed m = 5 datasets using MI.\nNow we will combine 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\n# Stacked imputed dataset\nimpdata &lt;- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#&gt; [1] 20955    17\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id &lt;- NULL\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ‚Ñπ Please use tidy evaluation idioms with `aes()`.\n#&gt; ‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ‚Ñπ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nStep 2: PS weighting steps 1-3 by DuGoff et al.¬†(2014)\nOur next step is to use steps 1-3 of the PS weighting analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Calculate PS weights\nStep 2.3: Balance checking using SMD. Consider SMD &lt;0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and calculating weights\n\ndat.ps &lt;- list(NULL)\n\nm &lt;- 5 # 5 imputed dataset\n\n# PS weighting on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed &lt;- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps &lt;- fitted(ps.fit)\n  \n  # Stabilized weight\n  dat.imputed$sweight &lt;- with(dat.imputed, \n                              ifelse(I(arthritis == \"Rheumatoid arthritis\"), \n                                     mean(I(arthritis == \"Rheumatoid arthritis\"))/ps, \n                                     (1-mean(I(arthritis == \"Rheumatoid arthritis\")))/(1-ps)))\n\n  # Dataset\n  dat.ps[[ii]] &lt;- dat.imputed\n}\n\n# Weight summary\npurrr::map_df(dat.ps, function(df){summary(df$sweight)})\n\n\n  \n\n\n\n\n# Dimension of each of the imputed dataset\nlapply(dat.ps, dim)\n#&gt; [[1]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 4191   18\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m &lt;- list(NULL)\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat &lt;- dat.ps[[ii]]\n  \n  # Covariates\n  vars &lt;- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Design with truncated stabilized weight\n  wdesign &lt;- svydesign(ids = ~studyid, weights = ~sweight, data = dat)\n  \n  # Balance checking \n  tab1m[[ii]] &lt;- svyCreateTableOne(vars = vars, strata = \"arthritis\", data = wdesign,\n                                   test = F)\n}\nprint(tab1m, smd = TRUE)\n#&gt; [[1]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.0          316.3                     \n#&gt;   age.cat (%)                                                        0.077\n#&gt;      20-49                      2096.4 (54.4)   159.8 (50.5)              \n#&gt;      50-64                      1010.5 (26.2)    89.2 (28.2)              \n#&gt;      65+                         749.1 (19.4)    67.3 (21.3)              \n#&gt;   sex = Female (%)              1900.0 (49.3)   144.1 (45.6)         0.075\n#&gt;   education (%)                                                      0.053\n#&gt;      Less than high school       764.6 (19.8)    60.9 (19.3)              \n#&gt;      High school                2109.7 (54.7)   180.9 (57.2)              \n#&gt;      College graduate or above   981.8 (25.5)    74.4 (23.5)              \n#&gt;   race (%)                                                           0.074\n#&gt;      White                      1173.6 (30.4)   107.0 (33.8)              \n#&gt;      Black                       917.9 (23.8)    70.1 (22.2)              \n#&gt;      Hispanic                    933.5 (24.2)    73.4 (23.2)              \n#&gt;      Others                      831.0 (21.6)    65.8 (20.8)              \n#&gt;   income (%)                                                         0.045\n#&gt;      less than $20,000           685.9 (17.8)    59.3 (18.7)              \n#&gt;      $20,000 to $74,999         2015.6 (52.3)   158.2 (50.0)              \n#&gt;      $75,000 and Over           1154.5 (29.9)    98.8 (31.3)              \n#&gt;   bmi (mean (SD))                29.34 (7.29)   29.93 (6.90)         0.082\n#&gt;   smoking (%)                                                        0.211\n#&gt;      Never smoker               2362.5 (61.3)   162.7 (51.4)              \n#&gt;      Previous smoker             814.5 (21.1)    91.9 (29.1)              \n#&gt;      Current smoker              679.1 (17.6)    61.6 (19.5)              \n#&gt;   htn = Yes (%)                 2403.5 (62.3)   211.4 (66.8)         0.094\n#&gt;   diabetes = Yes (%)             523.3 (13.6)    50.9 (16.1)         0.071\n#&gt; \n#&gt; [[2]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3855.7          317.6                     \n#&gt;   age.cat (%)                                                        0.079\n#&gt;      20-49                      2096.4 (54.4)   160.2 (50.4)              \n#&gt;      50-64                      1010.3 (26.2)    89.8 (28.3)              \n#&gt;      65+                         749.0 (19.4)    67.6 (21.3)              \n#&gt;   sex = Female (%)              1899.4 (49.3)   144.0 (45.3)         0.079\n#&gt;   education (%)                                                      0.050\n#&gt;      Less than high school       765.1 (19.8)    61.3 (19.3)              \n#&gt;      High school                2113.3 (54.8)   181.6 (57.2)              \n#&gt;      College graduate or above   977.2 (25.3)    74.7 (23.5)              \n#&gt;   race (%)                                                           0.076\n#&gt;      White                      1173.6 (30.4)   107.8 (33.9)              \n#&gt;      Black                       917.9 (23.8)    71.0 (22.4)              \n#&gt;      Hispanic                    933.2 (24.2)    72.0 (22.7)              \n#&gt;      Others                      831.0 (21.6)    66.8 (21.0)              \n#&gt;   income (%)                                                         0.039\n#&gt;      less than $20,000           688.0 (17.8)    57.8 (18.2)              \n#&gt;      $20,000 to $74,999         2015.9 (52.3)   160.1 (50.4)              \n#&gt;      $75,000 and Over           1151.8 (29.9)    99.7 (31.4)              \n#&gt;   bmi (mean (SD))                29.33 (7.22)   29.90 (6.95)         0.081\n#&gt;   smoking (%)                                                        0.213\n#&gt;      Never smoker               2362.4 (61.3)   162.6 (51.2)              \n#&gt;      Previous smoker             813.9 (21.1)    91.7 (28.9)              \n#&gt;      Current smoker              679.3 (17.6)    63.3 (19.9)              \n#&gt;   htn = Yes (%)                 2394.0 (62.1)   213.6 (67.3)         0.108\n#&gt;   diabetes = Yes (%)             522.8 (13.6)    50.3 (15.8)         0.065\n#&gt; \n#&gt; [[3]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.6          313.7                     \n#&gt;   age.cat (%)                                                        0.088\n#&gt;      20-49                      2096.4 (54.4)   156.9 (50.0)              \n#&gt;      50-64                      1010.1 (26.2)    88.9 (28.3)              \n#&gt;      65+                         750.1 (19.4)    67.9 (21.6)              \n#&gt;   sex = Female (%)              1900.1 (49.3)   142.2 (45.3)         0.079\n#&gt;   education (%)                                                      0.069\n#&gt;      Less than high school       765.0 (19.8)    57.8 (18.4)              \n#&gt;      High school                2112.6 (54.8)   182.5 (58.2)              \n#&gt;      College graduate or above   979.0 (25.4)    73.4 (23.4)              \n#&gt;   race (%)                                                           0.082\n#&gt;      White                      1174.1 (30.4)   106.8 (34.1)              \n#&gt;      Black                       918.2 (23.8)    69.9 (22.3)              \n#&gt;      Hispanic                    933.4 (24.2)    69.8 (22.2)              \n#&gt;      Others                      830.9 (21.5)    67.2 (21.4)              \n#&gt;   income (%)                                                         0.095\n#&gt;      less than $20,000           698.8 (18.1)    63.2 (20.1)              \n#&gt;      $20,000 to $74,999         2003.2 (51.9)   148.1 (47.2)              \n#&gt;      $75,000 and Over           1154.6 (29.9)   102.4 (32.7)              \n#&gt;   bmi (mean (SD))                29.29 (7.21)   29.80 (6.97)         0.072\n#&gt;   smoking (%)                                                        0.216\n#&gt;      Never smoker               2362.7 (61.3)   160.3 (51.1)              \n#&gt;      Previous smoker             814.6 (21.1)    91.0 (29.0)              \n#&gt;      Current smoker              679.2 (17.6)    62.5 (19.9)              \n#&gt;   htn = Yes (%)                 2398.6 (62.2)   206.3 (65.8)         0.074\n#&gt;   diabetes = Yes (%)             523.7 (13.6)    50.9 (16.2)         0.075\n#&gt; \n#&gt; [[4]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.2          312.9                     \n#&gt;   age.cat (%)                                                        0.098\n#&gt;      20-49                      2096.3 (54.4)   154.7 (49.5)              \n#&gt;      50-64                      1010.8 (26.2)    91.2 (29.1)              \n#&gt;      65+                         749.1 (19.4)    67.0 (21.4)              \n#&gt;   sex = Female (%)              1900.1 (49.3)   144.4 (46.2)         0.063\n#&gt;   education (%)                                                      0.064\n#&gt;      Less than high school       763.7 (19.8)    58.4 (18.7)              \n#&gt;      High school                2115.2 (54.9)   181.5 (58.0)              \n#&gt;      College graduate or above   977.2 (25.3)    72.9 (23.3)              \n#&gt;   race (%)                                                           0.078\n#&gt;      White                      1173.7 (30.4)   106.5 (34.0)              \n#&gt;      Black                       918.0 (23.8)    70.1 (22.4)              \n#&gt;      Hispanic                    933.5 (24.2)    71.2 (22.8)              \n#&gt;      Others                      831.0 (21.5)    65.0 (20.8)              \n#&gt;   income (%)                                                         0.096\n#&gt;      less than $20,000           694.1 (18.0)    63.9 (20.4)              \n#&gt;      $20,000 to $74,999         2009.3 (52.1)   148.3 (47.4)              \n#&gt;      $75,000 and Over           1152.9 (29.9)   100.7 (32.2)              \n#&gt;   bmi (mean (SD))                29.28 (7.21)   29.86 (7.13)         0.081\n#&gt;   smoking (%)                                                        0.209\n#&gt;      Never smoker               2362.7 (61.3)   161.3 (51.6)              \n#&gt;      Previous smoker             813.8 (21.1)    90.7 (29.0)              \n#&gt;      Current smoker              679.7 (17.6)    60.9 (19.5)              \n#&gt;   htn = Yes (%)                 2394.5 (62.1)   206.6 (66.0)         0.082\n#&gt;   diabetes = Yes (%)             523.0 (13.6)    50.2 (16.0)         0.069\n#&gt; \n#&gt; [[5]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3855.7          316.8                     \n#&gt;   age.cat (%)                                                        0.080\n#&gt;      20-49                      2096.4 (54.4)   159.6 (50.4)              \n#&gt;      50-64                      1010.3 (26.2)    89.8 (28.4)              \n#&gt;      65+                         749.1 (19.4)    67.4 (21.3)              \n#&gt;   sex = Female (%)              1899.6 (49.3)   143.9 (45.4)         0.077\n#&gt;   education (%)                                                      0.060\n#&gt;      Less than high school       767.4 (19.9)    62.1 (19.6)              \n#&gt;      High school                2110.3 (54.7)   181.8 (57.4)              \n#&gt;      College graduate or above   978.0 (25.4)    72.9 (23.0)              \n#&gt;   race (%)                                                           0.074\n#&gt;      White                      1173.5 (30.4)   107.1 (33.8)              \n#&gt;      Black                       917.9 (23.8)    70.1 (22.1)              \n#&gt;      Hispanic                    933.4 (24.2)    73.1 (23.1)              \n#&gt;      Others                      830.9 (21.5)    66.5 (21.0)              \n#&gt;   income (%)                                                         0.047\n#&gt;      less than $20,000           685.0 (17.8)    60.6 (19.1)              \n#&gt;      $20,000 to $74,999         2022.6 (52.5)   159.2 (50.3)              \n#&gt;      $75,000 and Over           1148.1 (29.8)    97.0 (30.6)              \n#&gt;   bmi (mean (SD))                29.31 (7.20)   29.77 (7.04)         0.064\n#&gt;   smoking (%)                                                        0.206\n#&gt;      Never smoker               2362.7 (61.3)   163.1 (51.5)              \n#&gt;      Previous smoker             813.3 (21.1)    90.0 (28.4)              \n#&gt;      Current smoker              679.7 (17.6)    63.6 (20.1)              \n#&gt;   htn = Yes (%)                 2409.4 (62.5)   210.2 (66.3)         0.080\n#&gt;   diabetes = Yes (%)             522.5 (13.6)    50.3 (15.9)         0.066\n\nFor each of the datasets, all SMDs except for smoking are less than our specified cut-point of 0.2. We will adjust our outcome model for smoking.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Note that, we must utilize survey features to correctly estimate the standard error. For this step, we will multiply PS weight and survey weight and create a new weight variable.\n3.1 Calculating new weights\n\ndat.ps2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat &lt;- dat.ps[[ii]]\n  \n  # New weight = survey weight * PS weight \n  dat$new_weight &lt;- with(dat, survey.weight * sweight)\n  \n  dat.ps2[[ii]] &lt;- dat\n}\n\n3.2 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction) with the PS weighted datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible &lt;- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat &lt;- dat.ps[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible &lt;- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] &lt;- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] &lt;- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp &lt;- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#&gt; [[1]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 5063   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.ps2[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"arthritis\"     \"age.cat\"       \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \".imp\"         \n#&gt; [17] \"ps\"            \"sweight\"       \"new_weight\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#&gt; [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\n\ndat.ineligible2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  dat &lt;- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible &lt;- NULL\n  \n  # Create ps and sweight\n  dat$ps &lt;- NA\n  dat$sweight &lt;- NA\n  dat$new_weight &lt;- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars &lt;- names(dat.ps2[[1]])\n  dat &lt;- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] &lt;- dat\n}\n\n3.2 Combining eligible (imputed and PS weighted) and ineligible (unimputed) subjects\nNow, we will merge imputed eligible and unimputed ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 &lt;- data.frame(dat.ps2[[ii]])\n  d1$eligible &lt;- 1\n  \n  # Ineligible\n  d2 &lt;- data.frame(dat.ineligible2[[ii]])\n  d2$eligible &lt;- 0\n  \n  # Full data\n  d3 &lt;- rbind(d1, d2)\n  \n  #  New weight variable in the full dataset\n  d3$new_weight &lt;- 0\n  d3$new_weight[d3$studyid %in% d1$studyid] &lt;- d1$new_weight\n  \n  # Full data in list format\n  dat.full2[[ii]] &lt;- d3\n}\nlapply(dat.full2, dim)\n#&gt; [[1]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 9254   20\n\n# Stacked dataset\ndat.stacked &lt;- rbindlist(dat.full2)\ndim(dat.stacked)\n#&gt; [1] 46270    20\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset. Make sure to use the new weight variable that combines survey weights and PS weights.\n\nallImputations &lt;- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 &lt;- svydesign(ids = ~psu, \n                       weights = ~new_weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design &lt;- subset(w.design0, eligible == 1) \n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#&gt; [[1]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 4191   20\n\nNow we will run the design-adjusted logistic regression, adjusting for smoking since smoking was not balanced in terms of SMD.\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + smoking, family = quasibinomial))\nres &lt;- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) &lt;- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#&gt;               (Intercept) arthritisRheumatoid arthritis smokingPrevious smoker\n#&gt; OR from m = 1        0.04                          1.21                   2.93\n#&gt; OR from m = 2        0.04                          1.24                   2.93\n#&gt; OR from m = 3        0.04                          1.23                   2.93\n#&gt; OR from m = 4        0.04                          1.22                   2.92\n#&gt; OR from m = 5        0.04                          1.22                   2.92\n#&gt;               smokingCurrent smoker\n#&gt; OR from m = 1                  1.55\n#&gt; OR from m = 2                  1.56\n#&gt; OR from m = 3                  1.54\n#&gt; OR from m = 4                  1.55\n#&gt; OR from m = 5                  1.55\n\nStep 3.5: Pooling estimates\nNow, we will pool the estimate using Rubin‚Äôs rule:\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit2)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nReferences",
    "crumbs": [
      "Propensity score",
      "PSW with MI in subset"
    ]
  },
  {
    "objectID": "propensityscore9.html",
    "href": "propensityscore9.html",
    "title": "PSW with multiple tx",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score weighting (PSW) for multiple treatment categories. We will use CCHS data that was used in the previous chapter on exact matching with CCHS.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"\n\n\n\nanalytic.miss: Full dataset of 397,173 participants from CCHS cycles 1.1, 2.1, and 3.1 with missing values in some covariates\n\nanalytic2: Analytic dataset of 185,613 participants without missing in the covariates.\nPre-processing\nLet us create the full and analytic datasets for only CCHS 3.1.\n\n# Full dataset with missing\ndat.full &lt;- subset(analytic.miss, cycle == \"31\")\ndim(dat.full)\n#&gt; [1] 132221     22\n\n# Analytic dataset without missing\ndat.analytic &lt;- subset(analytic2, cycle == \"31\")\ndim(dat.analytic)\n#&gt; [1] 39634    22\n\nWe will use the analytic dataset (dat.analytic) to run our PSW analysis with the following variables: - Outcome: CVD - Exposure: phyact (3-level physical activity) - Covariates: age, sex, married (marital status), race, edu (education), income, bmi (body mass index), doctor (whether visited to a doctor), stress, smoke, drink (drink alcohol or not), fruit (fruit consumption), bp (blood pressure), diab (diabetes), OA (osteoarthritis), immigrate (immigrant or not)\n- Sampling weight: weight\n\n# Is there any character variable?\nstr(dat.analytic) \n#&gt; 'data.frame':    39634 obs. of  22 variables:\n#&gt;  $ CVD      : chr  \"no event\" \"no event\" \"no event\" \"no event\" ...\n#&gt;  $ age      : chr  \"20-29 years\" \"65 years and over\" \"20-29 years\" \"20-29 years\" ...\n#&gt;  $ sex      : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n#&gt;  $ married  : chr  \"not single\" \"single\" \"single\" \"single\" ...\n#&gt;  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#&gt;  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"2nd grad.\" \"Other 2nd grad.\" ...\n#&gt;  $ income   : chr  \"$50,000-$79,999\" \"$29,999 or less\" \"$29,999 or less\" \"$50,000-$79,999\" ...\n#&gt;  $ bmi      : Factor w/ 3 levels \"Underweight\",..: 3 2 2 2 2 3 2 3 3 2 ...\n#&gt;  $ phyact   : chr  \"Inactive\" \"Moderate\" \"Active\" \"Moderate\" ...\n#&gt;  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n#&gt;  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" ...\n#&gt;  $ smoke    : chr  \"Former smoker\" \"Never smoker\" \"Never smoker\" \"Never smoker\" ...\n#&gt;  $ drink    : chr  \"Current drinker\" \"Former driker\" \"Never drank\" \"Current drinker\" ...\n#&gt;  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 1 1 3 2 2 2 2 3 ...\n#&gt;  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ province : chr  \"South\" \"South\" \"South\" \"South\" ...\n#&gt;  $ weight   : num  93.5 111.4 120.4 328.2 810.6 ...\n#&gt;  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ ID       : int  264953 264954 264961 264962 264963 264964 264969 264971 264975 264976 ...\n#&gt;  $ OA       : chr  \"Control\" \"OA\" \"Control\" \"Control\" ...\n#&gt;  $ immigrate: chr  \"not immigrant\" \"not immigrant\" \"not immigrant\" \"not immigrant\" ...\n\n# Make all variables (except for ID and weight) as factor\nvar.names &lt;- c(\"CVD\", \"phyact\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \n               \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"province\", \"OA\", \"immigrate\")\ndat.full[var.names] &lt;- lapply(dat.full[var.names] , factor)\ndat.analytic[var.names] &lt;- lapply(dat.analytic[var.names], factor)\n\n# Outcome - CVD\ntable(dat.analytic$CVD, useNA = \"always\")\n#&gt; \n#&gt;    event no event     &lt;NA&gt; \n#&gt;     1931    37703        0\ndat.full$CVD &lt;- car::recode(dat.full$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.full$CVD &lt;- factor(dat.full$CVD, levels = c(\"No\", \"Yes\"))\n\ndat.analytic$CVD &lt;- car::recode(dat.analytic$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.analytic$CVD &lt;- factor(dat.analytic$CVD, levels = c(\"No\", \"Yes\"))\ntable(dat.analytic$CVD, useNA = \"always\")\n#&gt; \n#&gt;    No   Yes  &lt;NA&gt; \n#&gt; 37703  1931     0\n\n# Exposure - physical activity\ntable(dat.analytic$phyact, useNA = \"always\")\n#&gt; \n#&gt;   Active Inactive Moderate     &lt;NA&gt; \n#&gt;    11508    17569    10557        0\ndat.full$phyact &lt;- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\ndat.analytic$phyact &lt;- factor(dat.analytic$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\n\n# Table 1\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"phyact\", data = dat.analytic, test = F)\nprint(tab1, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive      Moderate      Active       \n#&gt;   n                               17569         10557         11508        \n#&gt;   age (%)       20-29 years        2537 (14.4)   1709 (16.2)   2316 (20.1) \n#&gt;                 30-39 years        3526 (20.1)   2265 (21.5)   2276 (19.8) \n#&gt;                 40-49 years        3270 (18.6)   1939 (18.4)   2037 (17.7) \n#&gt;                 50-59 years        2901 (16.5)   1659 (15.7)   1480 (12.9) \n#&gt;                 60-64 years        1130 ( 6.4)    665 ( 6.3)    570 ( 5.0) \n#&gt;                 65 years and over  3310 (18.8)   1568 (14.9)   1183 (10.3) \n#&gt;                 teen                895 ( 5.1)    752 ( 7.1)   1646 (14.3) \n#&gt;   sex (%)       Female             9403 (53.5)   5709 (54.1)   5526 (48.0) \n#&gt;                 Male               8166 (46.5)   4848 (45.9)   5982 (52.0) \n#&gt;   married (%)   not single         9600 (54.6)   5920 (56.1)   5637 (49.0) \n#&gt;                 single             7969 (45.4)   4637 (43.9)   5871 (51.0) \n#&gt;   race (%)      Non-white          1757 (10.0)    886 ( 8.4)   1066 ( 9.3) \n#&gt;                 White             15812 (90.0)   9671 (91.6)  10442 (90.7) \n#&gt;   edu (%)       &lt; 2ndary           3690 (21.0)   1635 (15.5)   2039 (17.7) \n#&gt;                 2nd grad.          3246 (18.5)   1749 (16.6)   1845 (16.0) \n#&gt;                 Other 2nd grad.    1566 ( 8.9)    969 ( 9.2)   1150 (10.0) \n#&gt;                 Post-2nd grad.     9067 (51.6)   6204 (58.8)   6474 (56.3) \n#&gt;   income (%)    $29,999 or less    4480 (25.5)   1929 (18.3)   1906 (16.6) \n#&gt;                 $30,000-$49,999    4018 (22.9)   2059 (19.5)   2097 (18.2) \n#&gt;                 $50,000-$79,999    4512 (25.7)   2974 (28.2)   3095 (26.9) \n#&gt;                 $80,000 or more    4559 (25.9)   3595 (34.1)   4410 (38.3) \n#&gt;   bmi (%)       Underweight         532 ( 3.0)    243 ( 2.3)    340 ( 3.0) \n#&gt;                 healthy weight     7349 (41.8)   5023 (47.6)   6233 (54.2) \n#&gt;                 Overweight         9688 (55.1)   5291 (50.1)   4935 (42.9) \n#&gt;   doctor (%)    No                 2322 (13.2)   1272 (12.0)   1520 (13.2) \n#&gt;                 Yes               15247 (86.8)   9285 (88.0)   9988 (86.8) \n#&gt;   stress (%)    Not too stressed  13544 (77.1)   8371 (79.3)   9314 (80.9) \n#&gt;                 stressed           4025 (22.9)   2186 (20.7)   2194 (19.1) \n#&gt;   smoke (%)     Current smoker     5032 (28.6)   2386 (22.6)   2488 (21.6) \n#&gt;                 Former smoker      6900 (39.3)   4562 (43.2)   4672 (40.6) \n#&gt;                 Never smoker       5637 (32.1)   3609 (34.2)   4348 (37.8) \n#&gt;   drink (%)     Current drinker   13913 (79.2)   9016 (85.4)   9863 (85.7) \n#&gt;                 Former driker      2582 (14.7)   1102 (10.4)   1063 ( 9.2) \n#&gt;                 Never drank        1074 ( 6.1)    439 ( 4.2)    582 ( 5.1) \n#&gt;   fruit (%)     0-3 daily serving  5610 (31.9)   2270 (21.5)   1902 (16.5) \n#&gt;                 4-6 daily serving  8827 (50.2)   5445 (51.6)   5481 (47.6) \n#&gt;                 6+ daily serving   3132 (17.8)   2842 (26.9)   4125 (35.8) \n#&gt;   bp (%)        No                14188 (80.8)   8976 (85.0)  10349 (89.9) \n#&gt;                 Yes                3381 (19.2)   1581 (15.0)   1159 (10.1) \n#&gt;   diab (%)      No                16393 (93.3)  10114 (95.8)  11165 (97.0) \n#&gt;                 Yes                1176 ( 6.7)    443 ( 4.2)    343 ( 3.0) \n#&gt;   OA (%)        Control           14864 (84.6)   9310 (88.2)  10565 (91.8) \n#&gt;                 OA                 2705 (15.4)   1247 (11.8)    943 ( 8.2) \n#&gt;   immigrate (%) not immigrant     16557 (94.2)  10150 (96.1)  11098 (96.4) \n#&gt;                 recent             1012 ( 5.8)    407 ( 3.9)    410 ( 3.6) \n#&gt;                Stratified by phyact\n#&gt;                 SMD   \n#&gt;   n                   \n#&gt;   age (%)        0.285\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   sex (%)        0.081\n#&gt;                       \n#&gt;   married (%)    0.095\n#&gt;                       \n#&gt;   race (%)       0.037\n#&gt;                       \n#&gt;   edu (%)        0.119\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   income (%)     0.213\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   bmi (%)        0.173\n#&gt;                       \n#&gt;                       \n#&gt;   doctor (%)     0.023\n#&gt;                       \n#&gt;   stress (%)     0.063\n#&gt;                       \n#&gt;   smoke (%)      0.129\n#&gt;                       \n#&gt;                       \n#&gt;   drink (%)      0.133\n#&gt;                       \n#&gt;                       \n#&gt;   fruit (%)      0.323\n#&gt;                       \n#&gt;                       \n#&gt;   bp (%)         0.175\n#&gt;                       \n#&gt;   diab (%)       0.116\n#&gt;                       \n#&gt;   OA (%)         0.150\n#&gt;                       \n#&gt;   immigrate (%)  0.070\n#&gt; \n\nPSW for multiple tx\nNominal categories (option 1)\nFor this part (option 1), we consider physical activity as a nominal variable.\nEstimating Propensity score\nLet us fit the PS model by considering physical activity as a nominal variable and estimate the propensity scores:\n\n# Formula\nps.formula &lt;- formula(phyact ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\n#&gt; Loading required package: stats4\n#&gt; Loading required package: splines\n#&gt; \n#&gt; Attaching package: 'VGAM'\n#&gt; The following object is masked from 'package:survey':\n#&gt; \n#&gt;     calibrate\nps.fit &lt;- vglm(ps.formula, weights = weight, data = dat.analytic, \n               family = multinomial(parallel = FALSE))\n\n# Propensity scores\nps &lt;- data.frame(fitted(ps.fit))\nhead(ps)\n\n\n  \n\n\n\n# Summary\napply(ps, 2, summary)\n#&gt;           Inactive   Moderate    Active\n#&gt; Min.    0.06879258 0.07957314 0.0285961\n#&gt; 1st Qu. 0.34233334 0.23371928 0.1867680\n#&gt; Median  0.44756388 0.27020295 0.2634537\n#&gt; Mean    0.44778510 0.26768926 0.2845256\n#&gt; 3rd Qu. 0.55452938 0.30446807 0.3616572\n#&gt; Max.    0.89058237 0.39864744 0.7524817\n\nCreating weights\nLet us create PS weights. For subject \\(i\\), PS weight is calculated as\n\\[w_i = \\frac{1}{P(A_i = a|L)}, \\] where \\(A\\) is the exposure with levels \\(a\\) (Inactive, Moderate, and Active in our case), and \\(L\\) is the list of covariates.\n\n# IPW\ndat.analytic$ipw &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps$Moderate, \n                                  1/ps$Inactive))\nwith(dat.analytic, by(ipw, phyact, summary))\n#&gt; phyact: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.123   1.671   1.994   2.233   2.505  14.536 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   2.508   3.206   3.576   3.743   4.087  11.695 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.329   2.310   3.086   3.534   4.225  25.049\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw, data = dat.analytic)\n\n# Table 1\ntabw &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               39231.2         39519.8        \n#&gt;   age (%)       20-29 years        6477.4 (16.5)   6856.1 (17.3) \n#&gt;                 30-39 years        7706.8 (19.6)   8174.8 (20.7) \n#&gt;                 40-49 years        7098.2 (18.1)   7163.0 (18.1) \n#&gt;                 50-59 years        5922.4 (15.1)   5901.2 (14.9) \n#&gt;                 60-64 years        2414.8 ( 6.2)   2265.4 ( 5.7) \n#&gt;                 65 years and over  6187.1 (15.8)   5843.8 (14.8) \n#&gt;                 teen               3424.6 ( 8.7)   3315.4 ( 8.4) \n#&gt;   sex (%)       Female            20188.3 (51.5)  20693.4 (52.4) \n#&gt;                 Male              19042.9 (48.5)  18826.3 (47.6) \n#&gt;   married (%)   not single        20665.9 (52.7)  20907.0 (52.9) \n#&gt;                 single            18565.3 (47.3)  18612.8 (47.1) \n#&gt;   race (%)      Non-white          3471.9 ( 8.8)   4095.5 (10.4) \n#&gt;                 White             35759.3 (91.2)  35424.2 (89.6) \n#&gt;   edu (%)       &lt; 2ndary           7267.3 (18.5)   7288.2 (18.4) \n#&gt;                 2nd grad.          6902.6 (17.6)   6901.3 (17.5) \n#&gt;                 Other 2nd grad.    3783.0 ( 9.6)   3671.6 ( 9.3) \n#&gt;                 Post-2nd grad.    21278.3 (54.2)  21658.7 (54.8) \n#&gt;   income (%)    $29,999 or less    8432.3 (21.5)   8425.7 (21.3) \n#&gt;                 $30,000-$49,999    8115.6 (20.7)   8109.8 (20.5) \n#&gt;                 $50,000-$79,999   10169.1 (25.9)  10602.5 (26.8) \n#&gt;                 $80,000 or more   12514.2 (31.9)  12381.9 (31.3) \n#&gt;   bmi (%)       Underweight        1092.7 ( 2.8)   1110.6 ( 2.8) \n#&gt;                 healthy weight    18191.1 (46.4)  18613.7 (47.1) \n#&gt;                 Overweight        19947.4 (50.8)  19795.5 (50.1) \n#&gt;   doctor (%)    No                 5073.1 (12.9)   5127.6 (13.0) \n#&gt;                 Yes               34158.1 (87.1)  34392.2 (87.0) \n#&gt;   stress (%)    Not too stressed  30921.0 (78.8)  31245.6 (79.1) \n#&gt;                 stressed           8310.3 (21.2)   8274.2 (20.9) \n#&gt;   smoke (%)     Current smoker    10018.8 (25.5)  10121.1 (25.6) \n#&gt;                 Former smoker     15860.1 (40.4)  15737.2 (39.8) \n#&gt;                 Never smoker      13352.4 (34.0)  13661.5 (34.6) \n#&gt;   drink (%)     Current drinker   32257.4 (82.2)  32836.4 (83.1) \n#&gt;                 Former driker      4857.5 (12.4)   4621.5 (11.7) \n#&gt;                 Never drank        2116.4 ( 5.4)   2061.9 ( 5.2) \n#&gt;   fruit (%)     0-3 daily serving  9738.2 (24.8)   9786.2 (24.8) \n#&gt;                 4-6 daily serving 19523.6 (49.8)  19803.8 (50.1) \n#&gt;                 6+ daily serving   9969.4 (25.4)   9929.8 (25.1) \n#&gt;   bp (%)        No                33045.9 (84.2)  33662.1 (85.2) \n#&gt;                 Yes                6185.3 (15.8)   5857.7 (14.8) \n#&gt;   diab (%)      No                37227.0 (94.9)  37572.5 (95.1) \n#&gt;                 Yes                2004.2 ( 5.1)   1947.3 ( 4.9) \n#&gt;   OA (%)        Control           34297.7 (87.4)  34835.2 (88.1) \n#&gt;                 OA                 4933.5 (12.6)   4684.6 (11.9) \n#&gt;   immigrate (%) not immigrant     37503.8 (95.6)  37532.8 (95.0) \n#&gt;                 recent             1727.4 ( 4.4)   1987.0 ( 5.0) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             40667.9               \n#&gt;   age (%)        6467.5 (15.9)   0.046\n#&gt;                  8588.7 (21.1)        \n#&gt;                  7538.2 (18.5)        \n#&gt;                  6327.6 (15.6)        \n#&gt;                  2420.1 ( 6.0)        \n#&gt;                  5989.1 (14.7)        \n#&gt;                  3336.8 ( 8.2)        \n#&gt;   sex (%)       21026.2 (51.7)   0.012\n#&gt;                 19641.7 (48.3)        \n#&gt;   married (%)   22651.2 (55.7)   0.040\n#&gt;                 18016.8 (44.3)        \n#&gt;   race (%)       3834.0 ( 9.4)   0.034\n#&gt;                 36834.0 (90.6)        \n#&gt;   edu (%)        7395.4 (18.2)   0.023\n#&gt;                  6825.8 (16.8)        \n#&gt;                  3765.2 ( 9.3)        \n#&gt;                 22681.6 (55.8)        \n#&gt;   income (%)     8029.8 (19.7)   0.041\n#&gt;                  8544.1 (21.0)        \n#&gt;                 11407.3 (28.0)        \n#&gt;                 12686.7 (31.2)        \n#&gt;   bmi (%)        1241.6 ( 3.1)   0.017\n#&gt;                 19120.5 (47.0)        \n#&gt;                 20305.8 (49.9)        \n#&gt;   doctor (%)     5272.7 (13.0)   0.001\n#&gt;                 35395.3 (87.0)        \n#&gt;   stress (%)    32100.6 (78.9)   0.004\n#&gt;                  8567.4 (21.1)        \n#&gt;   smoke (%)      9760.6 (24.0)   0.032\n#&gt;                 17024.0 (41.9)        \n#&gt;                 13883.4 (34.1)        \n#&gt;   drink (%)     33927.5 (83.4)   0.022\n#&gt;                  4634.1 (11.4)        \n#&gt;                  2106.3 ( 5.2)        \n#&gt;   fruit (%)     10170.0 (25.0)   0.007\n#&gt;                 20200.6 (49.7)        \n#&gt;                 10297.3 (25.3)        \n#&gt;   bp (%)        34372.4 (84.5)   0.017\n#&gt;                  6295.6 (15.5)        \n#&gt;   diab (%)      38850.3 (95.5)   0.020\n#&gt;                  1817.7 ( 4.5)        \n#&gt;   OA (%)        35666.9 (87.7)   0.015\n#&gt;                  5001.1 (12.3)        \n#&gt;   immigrate (%) 38662.6 (95.1)   0.020\n#&gt;                  2005.4 ( 4.9)\n\nAll covariates are balanced in terms of SMD (SMD \\(\\le\\) 0.20).\nOutcome analysis\nNow, we will fit the outcome model. To get the correct estimate of the standard error, we will set up the design with full data and subset the design.\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight &lt;- with(dat.analytic, ipw * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight &lt;- 0\ndat.full$ATEweight[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n  \n\n\n\n# Outcome model\nfit &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95  p-value \n#&gt;    phyact Inactive       Ref                      \n#&gt;           Moderate      0.94 [0.64;1.38]   0.7693 \n#&gt;             Active      0.83 [0.53;1.29]   0.4133\n\nOrdinal categories (for comparison)\nFor comparison, let us consider physical activity as a ordinal variable (option 2).\nDefine ordinal variable\n\n# Exposure - ordinal physical activity\ndat.full$phyact.ord &lt;- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"), \n                              ordered = T)\ndat.analytic$phyact.ord &lt;- factor(dat.analytic$phyact, \n                                  levels = c(\"Inactive\", \"Moderate\", \"Active\"), ordered = T)\nhead(dat.analytic$phyact.ord)\n#&gt; [1] Inactive Moderate Active   Moderate Active   Active  \n#&gt; Levels: Inactive &lt; Moderate &lt; Active\n\nEstimating Propensity score\n\n# Formula\nps.formula2 &lt;- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\nps.fit2 &lt;- vglm(ps.formula2, weights = weight, data = dat.analytic, family = propodds)\n\n# Propensity scores\nps2 &lt;- data.frame(fitted(ps.fit2))\nhead(ps2)\n\n\n  \n\n\n\n# Summary\napply(ps2, 2, summary)\n#&gt;           Inactive   Moderate     Active\n#&gt; Min.    0.07517679 0.07505615 0.03456101\n#&gt; 1st Qu. 0.34108922 0.25002048 0.18907940\n#&gt; Median  0.44711595 0.28026693 0.26446898\n#&gt; Mean    0.44780744 0.26682382 0.28536874\n#&gt; 3rd Qu. 0.55497781 0.29473347 0.35967960\n#&gt; Max.    0.89038285 0.29934482 0.78152250\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw2 &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps2$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps2$Moderate, \n                                  1/ps2$Inactive))\nwith(dat.analytic, by(ipw2, phyact.ord, summary))\n#&gt; phyact.ord: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.123   1.668   2.001   2.237   2.516  13.302 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact.ord: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   3.341   3.382   3.525   3.748   3.875  11.248 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact.ord: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.280   2.319   3.073   3.504   4.211  19.768\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw2, data = dat.analytic)\n\n# Table 1\ntabw2 &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw2, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               39305.5         39566.3        \n#&gt;   age (%)       20-29 years        6679.4 (17.0)   6125.6 (15.5) \n#&gt;                 30-39 years        7635.7 (19.4)   8357.8 (21.1) \n#&gt;                 40-49 years        7085.9 (18.0)   7124.1 (18.0) \n#&gt;                 50-59 years        5820.8 (14.8)   6307.9 (15.9) \n#&gt;                 60-64 years        2339.1 ( 6.0)   2494.0 ( 6.3) \n#&gt;                 65 years and over  6108.9 (15.5)   6264.9 (15.8) \n#&gt;                 teen               3635.8 ( 9.3)   2892.0 ( 7.3) \n#&gt;   sex (%)       Female            19987.1 (50.9)  21622.5 (54.6) \n#&gt;                 Male              19318.4 (49.1)  17943.8 (45.4) \n#&gt;   married (%)   not single        20342.7 (51.8)  22151.4 (56.0) \n#&gt;                 single            18962.9 (48.2)  17414.9 (44.0) \n#&gt;   race (%)      Non-white          3595.2 ( 9.1)   3507.2 ( 8.9) \n#&gt;                 White             35710.4 (90.9)  36059.1 (91.1) \n#&gt;   edu (%)       &lt; 2ndary           7467.4 (19.0)   6739.9 (17.0) \n#&gt;                 2nd grad.          7027.3 (17.9)   6644.2 (16.8) \n#&gt;                 Other 2nd grad.    3815.2 ( 9.7)   3628.7 ( 9.2) \n#&gt;                 Post-2nd grad.    20995.6 (53.4)  22553.5 (57.0) \n#&gt;   income (%)    $29,999 or less    8596.0 (21.9)   7724.2 (19.5) \n#&gt;                 $30,000-$49,999    8182.3 (20.8)   7888.9 (19.9) \n#&gt;                 $50,000-$79,999   10126.1 (25.8)  11034.9 (27.9) \n#&gt;                 $80,000 or more   12401.1 (31.6)  12918.3 (32.6) \n#&gt;   bmi (%)       Underweight        1134.1 ( 2.9)    960.4 ( 2.4) \n#&gt;                 healthy weight    18284.0 (46.5)  18426.4 (46.6) \n#&gt;                 Overweight        19887.4 (50.6)  20179.5 (51.0) \n#&gt;   doctor (%)    No                 5183.0 (13.2)   4741.2 (12.0) \n#&gt;                 Yes               34122.5 (86.8)  34825.0 (88.0) \n#&gt;   stress (%)    Not too stressed  31032.0 (79.0)  31233.5 (78.9) \n#&gt;                 stressed           8273.6 (21.0)   8332.8 (21.1) \n#&gt;   smoke (%)     Current smoker    10263.8 (26.1)   9253.9 (23.4) \n#&gt;                 Former smoker     15572.0 (39.6)  16822.9 (42.5) \n#&gt;                 Never smoker      13469.8 (34.3)  13489.4 (34.1) \n#&gt;   drink (%)     Current drinker   32206.8 (81.9)  33304.9 (84.2) \n#&gt;                 Former driker      4898.9 (12.5)   4438.8 (11.2) \n#&gt;                 Never drank        2199.8 ( 5.6)   1822.5 ( 4.6) \n#&gt;   fruit (%)     0-3 daily serving  9841.8 (25.0)   9509.3 (24.0) \n#&gt;                 4-6 daily serving 19586.3 (49.8)  19922.3 (50.4) \n#&gt;                 6+ daily serving   9877.5 (25.1)  10134.7 (25.6) \n#&gt;   bp (%)        No                33233.3 (84.6)  33115.0 (83.7) \n#&gt;                 Yes                6072.2 (15.4)   6451.2 (16.3) \n#&gt;   diab (%)      No                37292.2 (94.9)  37648.3 (95.2) \n#&gt;                 Yes                2013.4 ( 5.1)   1918.0 ( 4.8) \n#&gt;   OA (%)        Control           34458.3 (87.7)  34493.7 (87.2) \n#&gt;                 OA                 4847.3 (12.3)   5072.6 (12.8) \n#&gt;   immigrate (%) not immigrant     37534.6 (95.5)  37816.7 (95.6) \n#&gt;                 recent             1770.9 ( 4.5)   1749.5 ( 4.4) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             40328.5               \n#&gt;   age (%)        6789.1 (16.8)   0.080\n#&gt;                  8504.7 (21.1)        \n#&gt;                  7562.1 (18.8)        \n#&gt;                  6079.6 (15.1)        \n#&gt;                  2283.6 ( 5.7)        \n#&gt;                  5614.4 (13.9)        \n#&gt;                  3495.0 ( 8.7)        \n#&gt;   sex (%)       20379.8 (50.5)   0.055\n#&gt;                 19948.8 (49.5)        \n#&gt;   married (%)   21840.5 (54.2)   0.057\n#&gt;                 18488.0 (45.8)        \n#&gt;   race (%)       4125.6 (10.2)   0.031\n#&gt;                 36202.9 (89.8)        \n#&gt;   edu (%)        7564.8 (18.8)   0.052\n#&gt;                  6886.8 (17.1)        \n#&gt;                  3777.5 ( 9.4)        \n#&gt;                 22099.4 (54.8)        \n#&gt;   income (%)     8371.7 (20.8)   0.056\n#&gt;                  8581.4 (21.3)        \n#&gt;                 11013.7 (27.3)        \n#&gt;                 12361.7 (30.7)        \n#&gt;   bmi (%)        1294.7 ( 3.2)   0.037\n#&gt;                 19134.4 (47.4)        \n#&gt;                 19899.5 (49.3)        \n#&gt;   doctor (%)     5467.9 (13.6)   0.031\n#&gt;                 34860.6 (86.4)        \n#&gt;   stress (%)    31816.9 (78.9)   0.001\n#&gt;                  8511.6 (21.1)        \n#&gt;   smoke (%)     10153.3 (25.2)   0.048\n#&gt;                 16300.6 (40.4)        \n#&gt;                 13874.6 (34.4)        \n#&gt;   drink (%)     33415.6 (82.9)   0.043\n#&gt;                  4714.1 (11.7)        \n#&gt;                  2198.9 ( 5.5)        \n#&gt;   fruit (%)     10190.6 (25.3)   0.020\n#&gt;                 19953.3 (49.5)        \n#&gt;                 10184.6 (25.3)        \n#&gt;   bp (%)        34557.1 (85.7)   0.037\n#&gt;                  5771.5 (14.3)        \n#&gt;   diab (%)      38519.4 (95.5)   0.020\n#&gt;                  1809.1 ( 4.5)        \n#&gt;   OA (%)        35641.0 (88.4)   0.024\n#&gt;                  4687.5 (11.6)        \n#&gt;   immigrate (%) 38153.4 (94.6)   0.030\n#&gt;                  2175.2 ( 5.4)\n\nAgain, all covariates are balanced in terms of SMD.\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight2 &lt;- with(dat.analytic, ipw2 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight2 &lt;- 0\ndat.full$ATEweight2[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight2\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight2, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop2 &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop2\n\n\n  \n\n\n\n# Outcome model\nfit2 &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit2, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95  p-value \n#&gt;    phyact Inactive       Ref                      \n#&gt;           Moderate      1.01 [0.71;1.43]   0.9775 \n#&gt;             Active      0.81 [0.52;1.27]   0.3645\n\nMachine learning / GBM (option 3)\nIn this part, we will use Gradient Boosting as one of the machine learning techniques to estimate the propensity scores.\nEstimating Propensity score\n\n# Formula\nps.formula3 &lt;- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\npacman::p_load(twang)\n#&gt; package 'deldir' successfully unpacked and MD5 sums checked\n#&gt; package 'interp' successfully unpacked and MD5 sums checked\n#&gt; package 'gbm' successfully unpacked and MD5 sums checked\n#&gt; package 'latticeExtra' successfully unpacked and MD5 sums checked\n#&gt; package 'twang' successfully unpacked and MD5 sums checked\n#&gt; \n#&gt; The downloaded binary packages are in\n#&gt;  C:\\Users\\wilds\\AppData\\Local\\Temp\\RtmpCkwiCl\\downloaded_packages\nset.seed(123)\nps.fit3 &lt;- mnps(ps.formula3, data = dat.analytic, estimand = \"ATE\", verbose = FALSE,\n                stop.method = c(\"es.max\"), n.trees = 200, sampw = dat.analytic$weight)\nsummary(ps.fit3)\n#&gt; Summary of pairwise comparisons:\n#&gt;   max.std.eff.sz min.p max.ks min.ks.pval stop.method\n#&gt; 1      0.4134866     0      1           0         unw\n#&gt; 2      0.1880231     0      1           0      es.max\n#&gt; \n#&gt; Sample sizes and effective sample sizes:\n#&gt;   treatment     n ESS:es.max\n#&gt; 1  Inactive 17569   7361.137\n#&gt; 2  Moderate 10557   4875.807\n#&gt; 3    Active 11508   5376.500\n\n\n# Propensity scores\nps3 &lt;- data.frame(Active = ps.fit3$psList$Active$ps,\n                 Moderate = ps.fit3$psList$Moderate$ps,\n                 Inactive = ps.fit3$psList$Inactive$ps)\nnames(ps3) &lt;- c(\"Active\",\"Moderate\",\"Inactive\")\nhead(ps3)\n\n\n  \n\n\n\n# Summary\napply(ps3, 2, summary)\n#&gt;            Active  Moderate  Inactive\n#&gt; Min.    0.1827539 0.1884745 0.2759573\n#&gt; 1st Qu. 0.2316300 0.2478217 0.3726945\n#&gt; Median  0.2664345 0.2666218 0.4485734\n#&gt; Mean    0.2894693 0.2679683 0.4420585\n#&gt; 3rd Qu. 0.3343953 0.2859762 0.5015505\n#&gt; Max.    0.5330670 0.3255325 0.6389846\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw3 &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps3$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps3$Moderate, \n                                  1/ps3$Inactive))\nwith(dat.analytic, by(ipw3, phyact, summary))\n#&gt; phyact: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.565   1.877   2.086   2.206   2.484   3.624 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   3.072   3.415   3.652   3.709   3.947   5.306 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.876   2.716   3.229   3.320   3.952   5.465\n\nWeights are not large compared to options 1 and 2.\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw3, data = dat.analytic)\n\n# Table 1\ntabw &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               38763.2         39159.6        \n#&gt;   age (%)       20-29 years        6086.5 (15.7)   6641.5 (17.0) \n#&gt;                 30-39 years        7624.6 (19.7)   8171.6 (20.9) \n#&gt;                 40-49 years        7057.4 (18.2)   7073.0 (18.1) \n#&gt;                 50-59 years        6267.5 (16.2)   5894.9 (15.1) \n#&gt;                 60-64 years        2399.6 ( 6.2)   2390.7 ( 6.1) \n#&gt;                 65 years and over  6883.8 (17.8)   5859.8 (15.0) \n#&gt;                 teen               2443.7 ( 6.3)   3128.1 ( 8.0) \n#&gt;   sex (%)       Female            20928.4 (54.0)  21097.6 (53.9) \n#&gt;                 Male              17834.8 (46.0)  18062.0 (46.1) \n#&gt;   married (%)   not single        21244.6 (54.8)  21279.5 (54.3) \n#&gt;                 single            17518.5 (45.2)  17880.1 (45.7) \n#&gt;   race (%)      Non-white          3683.4 ( 9.5)   3666.8 ( 9.4) \n#&gt;                 White             35079.7 (90.5)  35492.8 (90.6) \n#&gt;   edu (%)       &lt; 2ndary           7610.9 (19.6)   6692.3 (17.1) \n#&gt;                 2nd grad.          7088.9 (18.3)   6693.3 (17.1) \n#&gt;                 Other 2nd grad.    3558.1 ( 9.2)   3619.6 ( 9.2) \n#&gt;                 Post-2nd grad.    20505.3 (52.9)  22154.2 (56.6) \n#&gt;   income (%)    $29,999 or less    9138.3 (23.6)   7750.6 (19.8) \n#&gt;                 $30,000-$49,999    8400.8 (21.7)   7956.3 (20.3) \n#&gt;                 $50,000-$79,999    9946.0 (25.7)  10696.8 (27.3) \n#&gt;                 $80,000 or more   11278.0 (29.1)  12755.9 (32.6) \n#&gt;   bmi (%)       Underweight        1195.7 ( 3.1)    967.7 ( 2.5) \n#&gt;                 healthy weight    16766.4 (43.3)  18791.6 (48.0) \n#&gt;                 Overweight        20801.0 (53.7)  19400.2 (49.5) \n#&gt;   doctor (%)    No                 5070.5 (13.1)   4827.3 (12.3) \n#&gt;                 Yes               33692.7 (86.9)  34332.3 (87.7) \n#&gt;   stress (%)    Not too stressed  29885.7 (77.1)  31099.5 (79.4) \n#&gt;                 stressed           8877.5 (22.9)   8060.0 (20.6) \n#&gt;   smoke (%)     Current smoker    10737.3 (27.7)   9401.4 (24.0) \n#&gt;                 Former smoker     15204.3 (39.2)  16211.3 (41.4) \n#&gt;                 Never smoker      12821.5 (33.1)  13546.9 (34.6) \n#&gt;   drink (%)     Current drinker   31123.7 (80.3)  33043.2 (84.4) \n#&gt;                 Former driker      5322.0 (13.7)   4301.7 (11.0) \n#&gt;                 Never drank        2317.5 ( 6.0)   1814.6 ( 4.6) \n#&gt;   fruit (%)     0-3 daily serving 10529.0 (27.2)   8996.8 (23.0) \n#&gt;                 4-6 daily serving 19466.5 (50.2)  19851.5 (50.7) \n#&gt;                 6+ daily serving   8767.6 (22.6)  10311.3 (26.3) \n#&gt;   bp (%)        No                31712.5 (81.8)  33335.5 (85.1) \n#&gt;                 Yes                7050.7 (18.2)   5824.1 (14.9) \n#&gt;   diab (%)      No                36326.4 (93.7)  37499.3 (95.8) \n#&gt;                 Yes                2436.8 ( 6.3)   1660.2 ( 4.2) \n#&gt;   OA (%)        Control           33060.8 (85.3)  34579.5 (88.3) \n#&gt;                 OA                 5702.3 (14.7)   4580.0 (11.7) \n#&gt;   immigrate (%) not immigrant     36812.8 (95.0)  37425.9 (95.6) \n#&gt;                 recent             1950.3 ( 5.0)   1733.7 ( 4.4) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             38211.2               \n#&gt;   age (%)        6567.9 (17.2)   0.145\n#&gt;                  8208.6 (21.5)        \n#&gt;                  7313.3 (19.1)        \n#&gt;                  5555.3 (14.5)        \n#&gt;                  2186.9 ( 5.7)        \n#&gt;                  4539.2 (11.9)        \n#&gt;                  3840.0 (10.0)        \n#&gt;   sex (%)       18441.4 (48.3)   0.077\n#&gt;                 19769.8 (51.7)        \n#&gt;   married (%)   20012.7 (52.4)   0.033\n#&gt;                 18198.5 (47.6)        \n#&gt;   race (%)       3397.2 ( 8.9)   0.014\n#&gt;                 34814.0 (91.1)        \n#&gt;   edu (%)        6186.6 (16.2)   0.081\n#&gt;                  6159.6 (16.1)        \n#&gt;                  3630.7 ( 9.5)        \n#&gt;                 22234.2 (58.2)        \n#&gt;   income (%)     6758.9 (17.7)   0.120\n#&gt;                  7420.8 (19.4)        \n#&gt;                 10612.9 (27.8)        \n#&gt;                 13418.7 (35.1)        \n#&gt;   bmi (%)        1041.5 ( 2.7)   0.112\n#&gt;                 19648.4 (51.4)        \n#&gt;                 17521.2 (45.9)        \n#&gt;   doctor (%)     5001.8 (13.1)   0.015\n#&gt;                 33209.4 (86.9)        \n#&gt;   stress (%)    30845.7 (80.7)   0.059\n#&gt;                  7365.5 (19.3)        \n#&gt;   smoke (%)      8531.3 (22.3)   0.083\n#&gt;                 16200.7 (42.4)        \n#&gt;                 13479.2 (35.3)        \n#&gt;   drink (%)     32817.1 (85.9)   0.101\n#&gt;                  3703.9 ( 9.7)        \n#&gt;                  1690.1 ( 4.4)        \n#&gt;   fruit (%)      7924.2 (20.7)   0.120\n#&gt;                 19284.4 (50.5)        \n#&gt;                 11002.5 (28.8)        \n#&gt;   bp (%)        33698.1 (88.2)   0.120\n#&gt;                  4513.1 (11.8)        \n#&gt;   diab (%)      36930.8 (96.6)   0.092\n#&gt;                  1280.4 ( 3.4)        \n#&gt;   OA (%)        34640.5 (90.7)   0.110\n#&gt;                  3570.7 ( 9.3)        \n#&gt;   immigrate (%) 36764.3 (96.2)   0.040\n#&gt;                  1446.8 ( 3.8)\n\nAll covariates are balanced in terms of SMD.\n\nplot(ps.fit3, color = TRUE, plots = 2, figureRows = 1)\n\n\n\n\n\n\n#&gt; [[1]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\n\nplot(ps.fit3, plots = 3, color=TRUE, pairwiseMax = FALSE)\n\n\n\n\n\n\n#&gt; [[1]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight3 &lt;- with(dat.analytic, ipw3 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight3 &lt;- 0\ndat.full$ATEweight3[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight3\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight3, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n  \n\n\n\n# Outcome model\nfit3 &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit3, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95   p-value \n#&gt;    phyact Inactive       Ref                       \n#&gt;           Moderate      0.82 [0.56;1.21]   0.32241 \n#&gt;             Active      0.65 [0.44;0.96]   0.03203\n\nOther approches for multiple treatments\nNot covered here, but possible to do in a multiple treatments context:\n\nPropensity score matching\nPropensity score stratification\nMarginal mean weighting",
    "crumbs": [
      "Propensity score",
      "PSW with multiple tx"
    ]
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R functions (S)",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nbal.plot\ncobalt\nTo produce a overalp/balance plot for propensity scoes\n\n\nbal.tab\ncobalt\nTo check the balance at each category of covariates\n\n\nCreateCatTable\ntableone\nTo create a frequency table with categorical variables only\n\n\ndo.call\nbase\nTo execute a function call\n\n\nlove.plot\ncobalt\nTo plot the standardized mean differences at each category of covariates\n\n\nmatch.data\nMatchIt\nTo extract the matched dataste from a matchit object\n\n\nmatchit\nMatchIt\nTo match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m.\n\n\nrownames\nbase\nNames of the rows",
    "crumbs": [
      "Propensity score",
      "R functions (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html",
    "href": "propensityscoreQ.html",
    "title": "Quiz (S)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html#live-quiz",
    "href": "propensityscoreQ.html#live-quiz",
    "title": "Quiz (S)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html#download-quiz",
    "href": "propensityscoreQ.html#download-quiz",
    "title": "Quiz (S)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreS.html",
    "href": "propensityscoreS.html",
    "title": "App (S)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from a propensity score analysis using complex survey data, and implications of choosing different list of covariates for the propensity score model fit in each step of the analysis.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveS\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, ggplot2, survey, tableone, tibble, dplyr, tidyr, broom.mixed and jtools packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Propensity score",
      "App (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html",
    "href": "propensityscoreE.html",
    "title": "Exercise 1 (S)",
    "section": "",
    "text": "Problem Statement\nWe will use the article by Moon et al.¬†(2021):\nWe will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\nOutcome variable\nExposure\nConfounders and other variables\nTwo important warnings before we start:",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#problem-statement",
    "href": "propensityscoreE.html#problem-statement",
    "title": "Exercise 1 (S)",
    "section": "",
    "text": "SEQN: Respondent sequence number\nstrata: Masked pseudo strata (strata is nested within PSU)\npsu: Masked pseudo PSU\nsurvey.weight: Full sample 8 year interview weight divided by 4\nsurvey.cycle: NHANES cycle\n\n\n\ncvd: Cardiovascular disease\n\n\n\nnocturia: Binary nocturia\n\n\n\nage: Age in years at screening\ngender: Gender\nrace: Race/Ethnicity\nsmoking: 100+ cigarettes in life\nalcohol: Alcohol consumption (12+ drinks in 1 year)\nsleep: Sleep duration, h\nbmi: Body Mass Index in kg/m\\(^2\\)\n\nsystolic: Systolic blood pressure, mmHg\ndiastolic: Diastolic blood pressure, mmHg\ntcholesterol: Total cholesterol, mg/dl\ntriglycerides: Triglycerides, mg/dl\nhdl: HDL‚Äêcholesterol, mg/dl\ndiabetes: Diabetes mellitus\nhypertension: Hypertension\n\n\n\nIn this paper, there is insufficient information to create the analytic dataset. This is mainly because of not sufficiently defining the covariates and not explicitly explaining the inclusion/exclusion criteria.\nThe authors did incorrect analyses. For example, they didn‚Äôt consider survey features. Since we will utilize survey features in our analysis, our results will likely be different than the results shown by the authors in Table 2.",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-1-0-grade",
    "href": "propensityscoreE.html#question-1-0-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 1: [0% grade]",
    "text": "Question 1: [0% grade]\n1(a) Importing dataset\n\nload(file = \"Data/propensityscore/Moon2021.RData\")\n\n1(b) Subsetting according to eligibility\n\n# Age 20+\ndat.analytic &lt;- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars &lt;- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic &lt;- dat.analytic[,vars]\n\n# Complete case\ndat.analytic &lt;- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#&gt; [1] 15404    21\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, hypertension, diabetes mellitus, and survey cycles.\n\nHint 1: the authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nHint 2: Adjust the model for age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, and survey cycle.\nHint 3: Use Publish package to report the odds ratio with the 95% CI and p-value.\n\n\n# Create an indicator variable in the full data\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\n\n# Design setup\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Design-adjusted logistic\nfit.logit &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#&gt;       Variable              Units OddsRatio         CI.95     p-value \n#&gt;       nocturia                 &lt;2       Ref                           \n#&gt;                                2+      1.44   [1.21;1.71]   0.0001496 \n#&gt;            age            [20,40)       Ref                           \n#&gt;                           [40,60)      4.21   [3.05;5.82]     &lt; 1e-04 \n#&gt;                           [60,80)     11.46  [7.89;16.64]     &lt; 1e-04 \n#&gt;                          [80,Inf)     25.28 [17.51;36.50]     &lt; 1e-04 \n#&gt;         gender               Male       Ref                           \n#&gt;                            Female      0.68   [0.58;0.79]     &lt; 1e-04 \n#&gt;           race          Hispanics       Ref                           \n#&gt;                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#&gt;                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#&gt;                       Other races      1.55   [1.05;2.30]   0.0319116 \n#&gt;            bmi                         1.02   [1.01;1.03]   0.0003273 \n#&gt;        smoking                 No       Ref                           \n#&gt;                               Yes      1.74   [1.46;2.07]     &lt; 1e-04 \n#&gt;        alcohol                 No       Ref                           \n#&gt;                               Yes      0.92   [0.59;1.45]   0.7273627 \n#&gt;          sleep                         0.96   [0.90;1.01]   0.1146287 \n#&gt;   tcholesterol                         0.99   [0.99;0.99]     &lt; 1e-04 \n#&gt;  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#&gt;            hdl                         0.99   [0.98;1.00]   0.0416900 \n#&gt;   hypertension                 No       Ref                           \n#&gt;                               Yes      2.73   [2.27;3.29]     &lt; 1e-04 \n#&gt;       diabetes                 No       Ref                           \n#&gt;                               Yes      1.83   [1.51;2.22]     &lt; 1e-04 \n#&gt;   survey.cycle            2005-06       Ref                           \n#&gt;                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#&gt;                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#&gt;                           2011-11      0.82   [0.68;0.99]   0.0398975",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "href": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 2: Propensity score matching by DuGoff et al.¬†(2014) [50% grade]",
    "text": "Question 2: Propensity score matching by DuGoff et al.¬†(2014) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Question 1) using the propensity score 1:1 matching analysis as per DuGoff et al.¬†(2014) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia &lt;2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare your results with the results reported by the authors. [Expected answer: 2-3 sentences]\n\n\n# your codes here",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "href": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 3: Propensity score matching by Austin et al.¬†(2018) [50% grade]",
    "text": "Question 3: Propensity score matching by Austin et al.¬†(2018) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Questions 1 and 2) using the propensity score 1:4 matching analysis as per Austin et al.¬†(2018) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia &lt;2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing. Remember, you need to multiply matching weights and survey weights to get survey-based estimates.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare the results with Question 2. What‚Äôs the overall conclusion? [Expected answer: 2-3 sentences]\n\n\n# your codes here",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html",
    "href": "propensityscoreEsolution.html",
    "title": "Exercise 1 solution (S)",
    "section": "",
    "text": "Problem 1:\nWe will use the article by Moon et al.¬†(2021). We will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\nOutcome variable\nExposure\nConfounders and other variables\nTwo important warnings before we start:",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-1",
    "href": "propensityscoreEsolution.html#problem-1",
    "title": "Exercise 1 solution (S)",
    "section": "",
    "text": "1(a) Importing dataset\n\nload(file = \"Data/propensityscore/Moon2021.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\n1(b) Subsetting according to eligibility\n\n# Age 20+\ndat.analytic &lt;- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars &lt;- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic &lt;- dat.analytic[,vars]\n\n# Complete case\ndat.analytic &lt;- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#&gt; [1] 15404    21\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for the following covariates: age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, total cholesterol, triglycerides, HDL-cholesterol, hypertension, diabetes mellitus, and survey cycles.\nNote:\n\nThe authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nYou must create your design on the full data and then subset the design.\nReport the odds ratio with the 95% CI.\n\n\n# Create an indicator variable in the full data\ndat.full$indicator &lt;- 1\ndat.full$indicator[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\n\n# Design setup\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design &lt;- subset(svy.design0, indicator == 0)\n\n# Design-adjusted logistic\nfit.logit &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#&gt;       Variable              Units OddsRatio         CI.95     p-value \n#&gt;       nocturia                 &lt;2       Ref                           \n#&gt;                                2+      1.44   [1.21;1.71]   0.0001496 \n#&gt;            age            [20,40)       Ref                           \n#&gt;                           [40,60)      4.21   [3.05;5.82]     &lt; 1e-04 \n#&gt;                           [60,80)     11.46  [7.89;16.64]     &lt; 1e-04 \n#&gt;                          [80,Inf)     25.28 [17.51;36.50]     &lt; 1e-04 \n#&gt;         gender               Male       Ref                           \n#&gt;                            Female      0.68   [0.58;0.79]     &lt; 1e-04 \n#&gt;           race          Hispanics       Ref                           \n#&gt;                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#&gt;                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#&gt;                       Other races      1.55   [1.05;2.30]   0.0319116 \n#&gt;            bmi                         1.02   [1.01;1.03]   0.0003273 \n#&gt;        smoking                 No       Ref                           \n#&gt;                               Yes      1.74   [1.46;2.07]     &lt; 1e-04 \n#&gt;        alcohol                 No       Ref                           \n#&gt;                               Yes      0.92   [0.59;1.45]   0.7273627 \n#&gt;          sleep                         0.96   [0.90;1.01]   0.1146287 \n#&gt;   tcholesterol                         0.99   [0.99;0.99]     &lt; 1e-04 \n#&gt;  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#&gt;            hdl                         0.99   [0.98;1.00]   0.0416900 \n#&gt;   hypertension                 No       Ref                           \n#&gt;                               Yes      2.73   [2.27;3.29]     &lt; 1e-04 \n#&gt;       diabetes                 No       Ref                           \n#&gt;                               Yes      1.83   [1.51;2.22]     &lt; 1e-04 \n#&gt;   survey.cycle            2005-06       Ref                           \n#&gt;                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#&gt;                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#&gt;                           2011-11      0.82   [0.68;0.99]   0.0398975",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-2-propensity-score-matching-by-dugoff-et-al.-2014",
    "href": "propensityscoreEsolution.html#problem-2-propensity-score-matching-by-dugoff-et-al.-2014",
    "title": "Exercise 1 solution (S)",
    "section": "Problem 2: Propensity score matching by DuGoff et al.¬†(2014)",
    "text": "Problem 2: Propensity score matching by DuGoff et al.¬†(2014)\n2(a): 1:1 matching\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Problem 1) using the propensity score 1:1 matching analysis as per DuGoff et al.¬†(2014) recommendations.\nYou should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are the covariates used in 1(c).\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia &lt;2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with the 95% CI. You should utilize the survey feature as the design (NOT covariates).\n\nStep 1\n\n## Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(nocturia==\"2+\") ~ age + gender + race + bmi + smoking + \n                           alcohol + sleep + tcholesterol + triglycerides + hdl + \n                           hypertension + diabetes + survey.cycle + \n                           psu + strata + survey.weight)\n\n## Fit the PS model\nps.fit &lt;- glm(ps.formula, data = dat.analytic, family = binomial(\"logit\"))\ndat.analytic$PS &lt;- predict(ps.fit, type = \"response\")\n#summary(dat.analytic$PS)\n\n## Caliper: 0.2*sd(logit of PS)\ncaliper &lt;- 0.2*sd(log(dat.analytic$PS/(1-dat.analytic$PS)))\n#caliper\n\nStep 2\n\n# Match exposed and unexposed subjects \nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, \n                     data = dat.analytic,\n                     distance = dat.analytic$PS, \n                     method = \"nearest\", \n                     replace = FALSE,\n                     caliper = caliper, \n                     ratio = 1)\ndat.analytic$PS &lt;- match.obj$distance\n#summary(match.obj$distance)\n\n## See how many matched\n#match.obj\n\n## Extract matched data\nmatched.data &lt;- match.data(match.obj) \n\nStep 3\n\n## Summary of PS by the exposure\n#tapply(matched.data$distance, matched.data$nocturia, summary)\n\n## Balance checking\nvars &lt;- c(\"age\", \"gender\", \"race\", \"bmi\", \"smoking\", \"alcohol\", \"sleep\", \"tcholesterol\",\n         \"triglycerides\", \"hdl\", \"hypertension\", \"diabetes\", \"survey.cycle\")\n\n\ntab1m &lt;- CreateTableOne(vars = vars, strata = \"nocturia\", data = matched.data, \n                           test = F, includeNA = F)\nprint(tab1m, smd = TRUE, format = \"p\") # All SMDs &lt;0.1\n#&gt;                            Stratified by nocturia\n#&gt;                             &lt;2              2+              SMD   \n#&gt;   n                           4502            4502                \n#&gt;   age (%)                                                    0.047\n#&gt;      [20,40)                  19.5            20.2                \n#&gt;      [40,60)                  32.9            31.7                \n#&gt;      [60,80)                  39.2            38.7                \n#&gt;      [80,Inf)                  8.4             9.5                \n#&gt;   gender = Female (%)         50.7            49.8           0.017\n#&gt;   race (%)                                                   0.020\n#&gt;      Hispanics                24.4            24.3                \n#&gt;      Non-Hispanic White       46.0            45.3                \n#&gt;      Non-Hispanic Black       25.4            26.0                \n#&gt;      Other races               4.2             4.4                \n#&gt;   bmi (mean (SD))            29.96 (7.07)    30.18 (7.06)    0.031\n#&gt;   smoking = Yes (%)           57.5            57.0           0.009\n#&gt;   alcohol = Yes (%)            4.0             3.5           0.022\n#&gt;   sleep (mean (SD))           6.77 (1.39)     6.73 (1.60)    0.024\n#&gt;   tcholesterol (mean (SD))  199.01 (42.48)  197.51 (44.94)   0.034\n#&gt;   triglycerides (mean (SD)) 163.55 (140.06) 164.00 (144.94)  0.003\n#&gt;   hdl (mean (SD))            53.44 (16.85)   53.20 (16.81)   0.014\n#&gt;   hypertension = Yes (%)      47.3            48.7           0.029\n#&gt;   diabetes = Yes (%)          15.3            17.4           0.058\n#&gt;   survey.cycle (%)                                           0.030\n#&gt;      2005-06                  22.8            22.0                \n#&gt;      2007-08                  27.5            27.3                \n#&gt;      2009-10                  28.2            28.0                \n#&gt;      2011-11                  21.6            22.8\n\nStep 4\n\n## Setup the design with survey features\ndat.full$matched &lt;- 0\ndat.full$matched[dat.full$SEQN %in% matched.data$SEQN] &lt;- 1\n\n## Survey setup for full data \nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n## Outcome model\nfit.psm &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia, design = w.design.m, family = binomial)\npublish(fit.psm)\n#&gt;  Variable Units OddsRatio       CI.95     p-value \n#&gt;  nocturia    &lt;2       Ref                         \n#&gt;              2+      1.33 [1.14;1.57]   0.0007554\n\n2(b): Interpretation\nCompare your results with the results reported by the authors. [Expected answer: 1-2 sentences]\nIn the US population aged 20 years or more, the odds of CVD was 33% higher among adults with \\(\\ge 2\\) times nocturia compared to PS matched controls with &lt;2 nocturia. Unlike the results shown by the authors in Table 2, this odds ratio of 1.33 is a population-level estimate (i.e., the estimate is PATT).",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-3-propensity-score-matching-by-austin-et-al.-2018",
    "href": "propensityscoreEsolution.html#problem-3-propensity-score-matching-by-austin-et-al.-2018",
    "title": "Exercise 1 solution (S)",
    "section": "Problem 3: Propensity score matching by Austin et al.¬†(2018)",
    "text": "Problem 3: Propensity score matching by Austin et al.¬†(2018)\n3(a): 1:4 matching\nRepeat Problem 2(a), i.e., create the second column of Table 2 (exploring the relationship between binary nocturia and CVD), but using the propensity score 1:4 matching analysis as per Austin et al.¬†(2018) recommendations.\nYou should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are the covariates used in 1(c).\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia &lt;2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing. Remember, you need to consider matching weights in checking the covariate balance.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with the 95% CI. You should utilize the survey feature as the design (NOT covariates).\n\nNote:\n\nFor step 4, you need to multiply matching weights and survey weights when creating your design. After creating the design with the new weight, subset the design for the matched sample. This step is required to get survey-based estimates.\n\nStep 1\n\n## Specify the PS model to estimate propensity scores\nps.formula3 &lt;- as.formula(I(nocturia==\"2+\") ~ age + gender + race + bmi + smoking + \n                           alcohol + sleep + tcholesterol + triglycerides + hdl + \n                           hypertension + diabetes + survey.cycle)\n\nps.design &lt;- svydesign(id = ~psu, weights = ~survey.weight, strata = ~strata,\n                       data = dat.analytic, nest = TRUE)\n\n## Fit the PS model\nps.fit3 &lt;- svyglm(ps.formula3, design = ps.design, family = binomial)\ndat.analytic$PS3 &lt;- predict(ps.fit3, type = \"response\")\n#summary(dat.analytic$PS3)\n\n## Caliper: 0.2*sd(logit of PS)\ncaliper3 &lt;- 0.2*sd(log(dat.analytic$PS3/(1-dat.analytic$PS3)))\n#caliper3\n\nStep 2\n\n# Match exposed and unexposed subjects \nset.seed(123)\nmatch.obj3 &lt;- matchit(ps.formula3, \n                      data = dat.analytic,\n                      distance = dat.analytic$PS3, \n                      method = \"nearest\", \n                      replace = TRUE,\n                      caliper = caliper3, \n                      ratio = 4)\ndat.analytic$PS3 &lt;- match.obj$distance\n#summary(match.obj$distance)\n\n## See how many matched\n#match.obj3\n\n## Extract matched data\nmatched.data3 &lt;- match.data(match.obj3) \n\nStep 3\n\n## Summary of PS by the exposure\n#tapply(matched.data3$distance, matched.data3$nocturia, summary)\n\n## Balance checking\nvars &lt;- c(\"age\", \"gender\", \"race\", \"bmi\", \"smoking\", \"alcohol\", \"sleep\", \"tcholesterol\",\n         \"triglycerides\", \"hdl\", \"hypertension\", \"diabetes\", \"survey.cycle\")\n\n## Setup the design with survey features\ndat.full$matched &lt;- 0\ndat.full$matched[dat.full$SEQN %in% matched.data3$SEQN] &lt;- 1\n\n## Merging PS weights in full data\ndat.full$psweight &lt;- 0\ndat.full$psweight[dat.full$SEQN %in% matched.data3$SEQN] &lt;- matched.data3$weights\n\n## Survey setup for full data with ps weights - for balance checking\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~psweight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\ntab13m &lt;- svyCreateTableOne(vars = vars, strata = \"nocturia\", data = w.design.m, \n                           test = F, includeNA = F)\nprint(tab13m, smd = TRUE, format = \"p\") # All SMDs &lt;0.1\n#&gt;                            Stratified by nocturia\n#&gt;                             &lt;2               2+               SMD   \n#&gt;   n                          7185.0           4544.0                \n#&gt;   age (%)                                                      0.053\n#&gt;      [20,40)                   18.4             20.0                \n#&gt;      [40,60)                   33.3             31.4                \n#&gt;      [60,80)                   39.3             38.9                \n#&gt;      [80,Inf)                   9.1              9.7                \n#&gt;   gender = Female (%)          51.8             49.9           0.038\n#&gt;   race (%)                                                     0.015\n#&gt;      Hispanics                 24.3             24.1                \n#&gt;      Non-Hispanic White        44.6             45.1                \n#&gt;      Non-Hispanic Black        26.9             26.5                \n#&gt;      Other races                4.2              4.4                \n#&gt;   bmi (mean (SD))             29.97 (7.09)     30.21 (7.08)    0.035\n#&gt;   smoking = Yes (%)            58.3             57.2           0.022\n#&gt;   alcohol = Yes (%)             4.0              3.6           0.023\n#&gt;   sleep (mean (SD))            6.73 (1.38)      6.73 (1.60)    0.004\n#&gt;   tcholesterol (mean (SD))   197.49 (42.18)   197.44 (45.02)   0.001\n#&gt;   triglycerides (mean (SD))  159.71 (129.75)  163.66 (144.57)  0.029\n#&gt;   hdl (mean (SD))             53.27 (16.71)    53.25 (16.82)   0.001\n#&gt;   hypertension = Yes (%)       47.6             49.2           0.031\n#&gt;   diabetes = Yes (%)           16.9             18.0           0.028\n#&gt;   survey.cycle (%)                                             0.016\n#&gt;      2005-06                   21.4             22.0                \n#&gt;      2007-08                   27.7             27.2                \n#&gt;      2009-10                   28.0             27.8                \n#&gt;      2011-11                   23.0             23.0\n\nStep 4\n\n## Survey setup for full data with new weight = survey weights * ps weights\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight*psweight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n## Outcome model\nfit.psm &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia, design = w.design.m, family = binomial)\npublish(fit.psm)\n#&gt;  Variable Units OddsRatio       CI.95     p-value \n#&gt;  nocturia    &lt;2       Ref                         \n#&gt;              2+      1.33 [1.14;1.54]   0.0005248\n\n3(b): Interpretation\nCompare the results with Problem 2. What‚Äôs the overall conclusion? [Expected answer: 2-3 sentences]\nIn this exercise, we observed that the association between nocturia and CVD is approximately the same using DuGoff et al.‚Äôs 1:1 matching and Austin et al.‚Äôs 1:4 matching. At the population-level, we observed 33% higher odds of CVD among adults with \\(\\ge 2\\) times nocturia than PS matched controls with &lt;2 nocturia.",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "machinelearning.html",
    "href": "machinelearning.html",
    "title": "Machine learning (ML)",
    "section": "",
    "text": "Background\nThe chapter encompasses a series of instructional content that sequentially explores various facets of predictive modeling and machine learning, connecting them with a previous chapter. Beginning with a tutorial that revisits the application of regression for predicting continuous outcomes, it underscores the importance of understanding prediction error and overfitting, and introduces foundational machine learning concepts. The subsequent tutorial emphasizes the pivotal role of data splitting in predictive modeling, illustrating how to partition data into training and test sets and evaluate model performance across different data scenarios. Moving forward, the concept of cross-validation is explored, detailing the k-fold cross-validation method and demonstrating its implementation both manually and using the caret package. Another tutorial navigates through predicting binary outcomes using logistic regression, evaluating model performance using various metrics, and employing k-fold cross-validation.\nThe series then delves into supervised learning, exploring regularization techniques, decision trees, and ensemble methods, while employing various model evaluation metrics and cross-validation techniques. Lastly, unsupervised learning is introduced with a focus on the k-means clustering algorithm, discussing its implementation, determining the optimal number of clusters, and addressing associated challenges. Throughout, the tutorials provide practical examples, code snippets, and visual aids, offering a comprehensive and applied exploration of predictive modeling and machine learning concepts.",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning.html#background",
    "href": "machinelearning.html#background",
    "title": "Machine learning (ML)",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning.html#overview-of-tutorials",
    "href": "machinelearning.html#overview-of-tutorials",
    "title": "Machine learning (ML)",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nBuilding upon the foundational concepts introduced in a previous chapter about prediction research, this chapter takes our understanding to the next level. We‚Äôve already explored the fundamentals of propensity score methods, and now it is time to harness the power of machine learning and prediction techniques within a causal inference context, a journey that will ultimately lead us to the concept of double robust estimation methods, such as TMLE.\nBefore we embark on this exciting journey, we will bridge the gap by dedicating this chapter to a deeper exploration of machine learning. By discussing various types of machine learning algorithms and diving into the intricacies of predictive modeling, we aim to provide you with a robust foundation.\n\nRevisiting: Explore Relationships for Continuous Outcomes\nIn this tutorial, the focus is on utilizing regression to predict continuous outcomes, specifically employing multiple linear regression to construct an initial prediction model. The tutorial revisits fundamental concepts related to prediction and introduces foundational ideas pertinent to machine learning, all while using a distinct dataset compared to previous tutorials. The process involves loading a dataset, defining variables, and fitting a model using linear regression. Subsequent sections delve into the creation of a design matrix, obtaining predictions, and measuring prediction error through various metrics like R^2 and RMSE. The tutorial also addresses the critical concept of overfitting, discussing its causes, consequences, and potential solutions, such as internal and external validation methods.\n\n\nRevisiting: Data Spliting\nThis tutorial emphasizes the crucial concept of data splitting in the context of predictive modeling and machine learning, utilizing a different dataset than previous tutorials. The process begins with loading a dataset and then strategically splitting it into training and test subsets, ensuring a robust approach to model validation. A model is trained using the training data, and its performance is evaluated using various metrics, such as R^2 and RMSE, through a custom function that extracts these performance measures. This function facilitates the evaluation of the model‚Äôs predictive accuracy and fit by applying it to different datasets (training, test, and the entire dataset), thereby enabling a comprehensive understanding of the model‚Äôs performance across different data scenarios.\n\n\nRevisiting: Cross-vaildation\nThis tutorial delves into the concept of cross-validation, a pivotal technique in predictive modeling and machine learning, using a distinct dataset for illustrative purposes. The process of k-fold cross-validation is explored, wherein the data is partitioned into ‚Äòk‚Äô subsets, and the model is trained ‚Äòk‚Äô times, each time using a different subset as the test set and the remaining data as the training set. This method is employed to assess the model‚Äôs predictive performance and to mitigate the risk of results being dependent on the initial data split. The tutorial demonstrates both manual calculations for individual folds and the utilization of the caret package to automate the cross-validation process, thereby providing a comprehensive overview of the method.\n\n\nRevisiting: Explore Relationships for Binary Outcomes\nThe tutorial navigates through the concept of predicting binary outcomes using logistic regression, emphasizing the application of various model evaluation metrics and methodologies in a machine learning context. It begins by ensuring that the outcome variable is treated as a factor and then proceeds to model fitting, where logistic regression is applied to predict a binary outcome. The model‚Äôs predictive performance is evaluated using metrics like the Area Under the Curve (AUC) and the Brier Score, which respectively assess the model‚Äôs classification accuracy and the mean squared difference between predicted probabilities and the actual outcomes. Furthermore, the tutorial explores k-fold cross-validation using the caret package, providing a robust method to assess the model‚Äôs predictive performance while avoiding overfitting. It also touches upon variable selection using stepwise regression with the Akaike Information Criterion (AIC) as a selection criterion.\n\n\nSupervised Learning\nThis tutorial delves into the realm of supervised learning, exploring beyond statistical regression and introducing various machine learning methods tailored for both continuous and binary outcomes. The tutorial explores different regularization techniques, such as LASSO, Ridge, and Elastic Net, which are used to prevent overfitting by penalizing large coefficients in regression models. It also introduces decision trees (CART), which provide a flexible, hierarchical approach to modeling data, and can automatically incorporate non-linear effects and interactions. The tutorial further explores ensemble methods, which combine predictions from multiple models to improve predictive accuracy. Two types of ensemble methods are discussed: Type I, which trains the same model on different samples of the data (e.g., bagging and boosting), and Type II, which trains different models on the same data (e.g., Super Learner). Various model evaluation metrics and cross-validation techniques are utilized throughout to assess and enhance the predictive performance of the models.\n\n\nUnsupervised Learning\nThe tutorial introduces unsupervised learning, with a focus on clustering, a technique that categorizes data into distinct groups based on similarity without using predefined labels. The k-means clustering algorithm is highlighted, which partitions data into k groups by minimizing within-cluster variation, typically using the sum of squares of Euclidean distances. The algorithm iteratively assigns data points to clusters based on the mean of the data points in each cluster and recalculates the cluster means until the cluster assignments no longer change. Various examples illustrate how to apply k-means clustering to different datasets and variable combinations. The tutorial also discusses determining the optimal number of clusters, k, and addresses challenges such as the influence of outliers and the sensitivity to the initial assignment of cluster means.\n\n\nMachine Learning for Health Survey Data using NHIS data\nThis tutorial provides a step-by-step guide to fitting machine learning models with health survey data, specifically using the National Health Interview Survey (NHIS) 2016 dataset to predict high impact chronic pain (HICP) among adults aged 65 years or older. The tutorial covers the use of (1) LASSO and (2) random forest models with sampling weights for population-level predictions. It also discusses the split-sample approach for internal validation, though it acknowledges that cross-validation and bootstrapping may be better alternatives. The tutorial includes the exploration of the analytic dataset, weight normalization, split-sample creation, defining regression formulas, fitting LASSO models with optimal lambda, and evaluating model performance with metrics such as AUC, calibration slope, and Brier score. The same process is then repeated for fitting random forest models, and a variable importance plot is generated to identify influential predictors. Finally, a performance comparison table is provided.\n\n\nReplicate Results from a Published Article\nThe tutorial guides users on implementing machine learning techniques using health survey data, specifically for predicting high impact chronic pain (HICP). It seeks to replicate results from a 2023 article, which utilized the National Health Interview Survey (NHIS) 2016 dataset. For simplicity, complete case data was used in this tutorial. In the original research, the authors developed prediction models for HICP and evaluated their performance within specific sociodemographic groups, such as gender, age groups, and race/ethnicity. They adopted LASSO and random forest models, applying 5-fold cross-validation. They also factored in survey weights in both models to achieve population-level predictions.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning0.html",
    "href": "machinelearning0.html",
    "title": "Concepts (L)",
    "section": "",
    "text": "Machine learning\nMachine learning focuses on developing algorithms and models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. In epidemiology, machine learning has several important uses and applications. This section is a very basic introduction to machine learning for Epidemiology.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#reading-list",
    "href": "machinelearning0.html#reading-list",
    "title": "Concepts (L)",
    "section": "Reading list",
    "text": "Reading list\nKey reference\n\n(Bi et al. 2019)\n\nFollowing ate optional but useful references\n\n(Karim 2021)\n(Liu et al. 2019)\n(Kuhn et al. 2013)\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#video-lessons",
    "href": "machinelearning0.html#video-lessons",
    "title": "Concepts (L)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning Reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning Terminologies\n\n\n\nWhat is included in this Video Lesson:\n\nReference 0:08\nTypes of Epidemiological models 0:32\nAnalyzing Epidemiological Study data 2:44\nMachine learning 5:42\nTerminologies 9:15\nClassification of Machine learning 12:55\nClassification of Supervised learning 17:30\nOther classifications 18:42\nPopular algorithms 20:06\nDecision tree 20:50\nShrinkage Methods 26:37\nEnsemble methods 37:04\nVariable Importance measure 40:20\nEpidemiologic applications 44:29\nFuture Reading 53:17\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#video-lesson-slides",
    "href": "machinelearning0.html#video-lesson-slides",
    "title": "Concepts (L)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#links",
    "href": "machinelearning0.html#links",
    "title": "Concepts (L)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#references",
    "href": "machinelearning0.html#references",
    "title": "Concepts (L)",
    "section": "References",
    "text": "References\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. ‚ÄúWhat Is Machine Learning? A Primer for the Epidemiologist.‚Äù American Journal of Epidemiology 188 (12): 2222‚Äì39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKarim, Ehsan. 2021. ‚ÄúUnderstanding Basics and Usage of Machine Learning in Medical Literature.‚Äù 2021. https://ehsanx.github.io/into2ML/.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. ‚ÄúOver-Fitting and Model Tuning.‚Äù Applied Predictive Modeling, 61‚Äì92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. ‚ÄúHow to Read Articles That Use Machine Learning: Users‚Äô Guides to the Medical Literature.‚Äù Jama 322 (18): 1806‚Äì16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. ‚ÄúPredictor Selection.‚Äù Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395‚Äì429.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning1.html",
    "href": "machinelearning1.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction model.\nLoad dataset\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n  \n\n\n\nPrediction for length of stay\nNow, we show the regression fitting when outcome is continuous (length of stay).\nVariables\n\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#&gt;  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#&gt;  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#&gt;  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#&gt; [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#&gt; [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#&gt; [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#&gt; [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#&gt; [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#&gt; [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#&gt; [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#&gt; [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#&gt; [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#&gt; [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#&gt; [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#&gt; [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#&gt; [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#&gt; [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula1 &lt;- as.formula(paste(\"Length.of.Stay~ \", \n                               paste(baselinevars, \n                                     collapse = \"+\")))\nsaveRDS(out.formula1, file = \"Data/machinelearning/form1.RDS\")\nfit1 &lt;- lm(out.formula1, data = ObsData)\nrequire(Publish)\nadj.fit1 &lt;- publish(fit1, digits=1)$regressionTable\n\n\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nadj.fit1\n\n\n  \n\n\n\nDesign Matrix\n\nNotations\n\nn is number of observations\np is number of covariates\n\n\n\nExpands factors to a set of dummy variables.\n\ndim(ObsData)\n#&gt; [1] 5735   52\nlength(attr(terms(out.formula1), \"term.labels\"))\n#&gt; [1] 50\n\n\nhead(model.matrix(fit1))\n#&gt;   (Intercept) Disease.categoryCHF Disease.categoryOther Disease.categoryMOSF\n#&gt; 1           1                   0                     1                    0\n#&gt; 2           1                   0                     0                    1\n#&gt; 3           1                   0                     0                    1\n#&gt; 4           1                   0                     0                    0\n#&gt; 5           1                   0                     0                    1\n#&gt; 6           1                   0                     1                    0\n#&gt;   CancerLocalized (Yes) CancerMetastatic Cardiovascular1 Congestive.HF1\n#&gt; 1                     1                0               0              0\n#&gt; 2                     0                0               1              1\n#&gt; 3                     1                0               0              0\n#&gt; 4                     0                0               0              0\n#&gt; 5                     0                0               0              0\n#&gt; 6                     0                0               0              1\n#&gt;   Dementia1 Psychiatric1 Pulmonary1 Renal1 Hepatic1 GI.Bleed1 Tumor1\n#&gt; 1         0            0          1      0        0         0      1\n#&gt; 2         0            0          0      0        0         0      0\n#&gt; 3         0            0          0      0        0         0      1\n#&gt; 4         0            0          0      0        0         0      0\n#&gt; 5         0            0          0      0        0         0      0\n#&gt; 6         0            0          1      0        0         0      0\n#&gt;   Immunosupperssion1 Transfer.hx1 MI1 age[50,60) age[60,70) age[70,80)\n#&gt; 1                  0            0   0          0          0          1\n#&gt; 2                  1            1   0          0          0          1\n#&gt; 3                  1            0   0          0          0          0\n#&gt; 4                  1            0   0          0          0          1\n#&gt; 5                  0            0   0          0          1          0\n#&gt; 6                  0            0   0          0          0          0\n#&gt;   age[80, Inf) sexFemale       edu DASIndex APACHE.score Glasgow.Coma.Score\n#&gt; 1            0         0 12.000000 23.50000           46                  0\n#&gt; 2            0         1 12.000000 14.75195           50                  0\n#&gt; 3            0         1 14.069916 18.13672           82                  0\n#&gt; 4            0         1  9.000000 22.92969           48                  0\n#&gt; 5            0         0  9.945259 21.05078           72                 41\n#&gt; 6            1         1  8.000000 17.50000           38                  0\n#&gt;   blood.pressure         WBC Heart.rate Respiratory.rate Temperature\n#&gt; 1             41 22.09765620        124               10    38.69531\n#&gt; 2             63 28.89843750        137               38    38.89844\n#&gt; 3             57  0.04999542        130               40    36.39844\n#&gt; 4             55 23.29687500         58               26    35.79688\n#&gt; 5             65 29.69921880        125               27    34.79688\n#&gt; 6            115 18.00000000        134               36    39.19531\n#&gt;   PaO2vs.FIO2  Albumin Hematocrit Bilirubin Creatinine Sodium Potassium PaCo2\n#&gt; 1     68.0000 3.500000   58.00000 1.0097656  1.1999512    145  4.000000    40\n#&gt; 2    218.3125 2.599609   32.50000 0.6999512  0.5999756    137  3.299805    34\n#&gt; 3    275.5000 3.500000   21.09766 1.0097656  2.5996094    146  2.899902    16\n#&gt; 4    156.6562 3.500000   26.29688 0.3999634  1.6999512    117  5.799805    30\n#&gt; 5    478.0000 3.500000   24.00000 1.0097656  3.5996094    126  5.799805    17\n#&gt; 6    184.1875 3.099609   30.50000 1.0097656  1.3999023    138  5.399414    68\n#&gt;         PH   Weight DNR.statusYes Medical.insuranceMedicare\n#&gt; 1 7.359375 64.69995             0                         1\n#&gt; 2 7.329102 45.69998             0                         0\n#&gt; 3 7.359375  0.00000             0                         0\n#&gt; 4 7.459961 54.59998             0                         0\n#&gt; 5 7.229492 78.39996             1                         1\n#&gt; 6 7.299805 54.89999             0                         1\n#&gt;   Medical.insuranceMedicare & Medicaid Medical.insuranceNo insurance\n#&gt; 1                                    0                             0\n#&gt; 2                                    0                             0\n#&gt; 3                                    0                             0\n#&gt; 4                                    0                             0\n#&gt; 5                                    0                             0\n#&gt; 6                                    0                             0\n#&gt;   Medical.insurancePrivate Medical.insurancePrivate & Medicare\n#&gt; 1                        0                                   0\n#&gt; 2                        0                                   1\n#&gt; 3                        1                                   0\n#&gt; 4                        0                                   1\n#&gt; 5                        0                                   0\n#&gt; 6                        0                                   0\n#&gt;   Respiratory.DiagYes Cardiovascular.DiagYes Neurological.DiagYes\n#&gt; 1                   1                      1                    0\n#&gt; 2                   0                      0                    0\n#&gt; 3                   0                      1                    0\n#&gt; 4                   1                      0                    0\n#&gt; 5                   0                      1                    0\n#&gt; 6                   1                      0                    0\n#&gt;   Gastrointestinal.DiagYes Renal.DiagYes Metabolic.DiagYes Hematologic.DiagYes\n#&gt; 1                        0             0                 0                   0\n#&gt; 2                        0             0                 0                   0\n#&gt; 3                        0             0                 0                   0\n#&gt; 4                        0             0                 0                   0\n#&gt; 5                        0             0                 0                   0\n#&gt; 6                        0             0                 0                   0\n#&gt;   Sepsis.DiagYes Trauma.DiagYes Orthopedic.DiagYes raceblack raceother\n#&gt; 1              0              0                  0         0         0\n#&gt; 2              1              0                  0         0         0\n#&gt; 3              0              0                  0         0         0\n#&gt; 4              0              0                  0         0         0\n#&gt; 5              0              0                  0         0         0\n#&gt; 6              0              0                  0         0         0\n#&gt;   income$25-$50k income&gt; $50k incomeUnder $11k RHC.use\n#&gt; 1              0            0                1       0\n#&gt; 2              0            0                1       1\n#&gt; 3              1            0                0       1\n#&gt; 4              0            0                0       0\n#&gt; 5              0            0                1       1\n#&gt; 6              0            0                1       0\ndim(model.matrix(fit1))\n#&gt; [1] 5735   64\np &lt;- dim(model.matrix(fit1))[2] # intercept + slopes\np\n#&gt; [1] 64\n\nObtain prediction\n\nobs.y &lt;- ObsData$Length.of.Stay\nsummary(obs.y)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2.00    7.00   14.00   21.56   25.00  394.00\n# Predict the above fit on ObsData data\npred.y1 &lt;- predict(fit1, ObsData)\nsummary(pred.y1)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  -32.76   16.62   21.96   21.56   26.73   42.67\nn &lt;- length(pred.y1)\nn\n#&gt; [1] 5735\nplot(obs.y,pred.y1)\nlines(lowess(obs.y,pred.y1), col = \"red\")\n\n\n\n\n\n\n\nMeasuring prediction error\nPrediction error measures how well the model can predict the outcome for new data that were not used in developing the prediction model.\n\nBias reduced for models with more variables\nUnimportant variables lead to noise / variability\nBias variance trade-off / need penalization\n\nR2\nThe provided information describes a statistical context involving a dataset of n values, \\(y_1, ..., y_n\\) (referred to as \\(y_i\\) or as a vector \\(y = [y_1,...,y_n]^T\\)), each paired with a fitted value \\(f_1,...,f_n\\) (denoted as \\(f_i\\) or sometimes \\(\\hat{y_i}\\), and as a vector \\(f\\)). The residuals, represented as \\(e_i\\), are defined as the differences between the observed and the fitted values: $ e_i = y_i ‚àí f_i$\nThe mean of the observed data is denoted by \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i \\]\nThe variability of the dataset can be quantified using two sums of squares formulas: 1. Residual Sum of Squares (SSres) or SSE: It quantifies the variance remaining in the data after fitting a model, calculated as: \\[ SS_{res} = \\sum_{i}(y_i - f_i)^2 = \\sum_{i}e_i^2 \\] 2. Total Sum of Squares (SStot) or SST: It represents the total variance in the observed data, calculated as: \\[ SS_{tot} = \\sum_{i}(y_i - \\bar{y})^2 \\]\nThe Coefficient of Determination (R¬≤) or R.2, which provides a measure of how well the model‚Äôs predictions match the observed data, is defined as: \\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nIn the ideal scenario where the model fits the data perfectly, we have \\(SS_{res} = 0\\) and thus \\(R^2 = 1\\). Conversely, a baseline model, which always predicts the mean \\(\\bar{y}\\) of the observed data, would yield \\(R^2 = 0\\). Models performing worse than this baseline model would result in a negative R¬≤ value. This metric is widely utilized in regression analysis to evaluate model performance, where a higher R¬≤ indicates a better fit of the model to the data.\n\n# Find SSE\nSSE &lt;- sum( (obs.y - pred.y1)^2 )\nSSE\n#&gt; [1] 3536398\n# Find SST\nmean.obs.y &lt;- mean(obs.y)\nSST &lt;- sum( (obs.y - mean.obs.y)^2 )\nSST\n#&gt; [1] 3836690\n# Find R2\nR.2 &lt;- 1- SSE/SST\nR.2\n#&gt; [1] 0.07826832\nrequire(caret)\ncaret::R2(pred.y1, obs.y)\n#&gt; [1] 0.07826832\n\nref\nRMSE\n\n# Find RMSE\nRmse &lt;- sqrt(SSE/(n-p)) \nRmse\n#&gt; [1] 24.97185\ncaret::RMSE(pred.y1, obs.y)\n#&gt; [1] 24.83212\n\nSee (Wikipedia 2023b)\nAdj R2\nThe Adjusted R¬≤ statistic modifies the \\(R^2\\) value to counteract the automatic increase of \\(R^2\\) when extra explanatory variables are added to a model, even if they do not improve the model fit. This adjustment is crucial for ensuring that the metric offers a reliable indication of the explanatory power of the model, especially in multiple regression where several predictors are involved.\nThe commonly used formula is defined as:\n\\[\n\\bar{R}^{2} = 1 - \\frac{SS_{\\text{res}} / df_{\\text{res}}}{SS_{\\text{tot}} / df_{\\text{tot}}}\n\\]\nWhere:\n\n\n\\(SS_{\\text{res}}\\) and \\(SS_{\\text{tot}}\\) represent the residual and total sums of squares respectively.\n\n\\(df_{\\text{res}}\\) and \\(df_{\\text{tot}}\\) refer to the degrees of freedom of the residual and total sums of squares. Usually, \\(df_{\\text{res}} = n - p\\) and \\(df_{\\text{tot}} = n - 1\\), where:\n\n\n\\(n\\) signifies the sample size.\n\n\\(p\\) denotes the number of variables in the model.\n\n\n\nThis metric plays a vital role in model selection and safeguards against overfitting by penalizing the inclusion of non-informative variables\nThe alternate formula is:\n\\[\n\\bar{R}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n\\]\nThis formula modifies the \\(R^2\\) value, accounting for the number of predictors and offering a more parsimonious model fit measure.\n\n# Find adj R2\nadjR2 &lt;- 1-(1-R.2)*((n-1)/(n-p-1))\nadjR2\n#&gt; [1] 0.06786429\n\nSee (Wikipedia 2023a)\nOverfitting and Optimism\n\nModel usually performs very well in the empirical data where the model was fitted in the same data (optimistic)\nModel performs poorly in the new data (generalization is not as good)\n\nCauses\n\nModel determined by data at hand without expert opinion\nToo many model parameters (\\(age\\), \\(age^2\\), \\(age^3\\)) / predictors\nToo small dataset (training) / data too noisy\nConsequences\n\nOverestimation of effects of predictors\nReduction in model performance in new observations\nProposed solutions\nWe generally use procedures such as\n\nInternal validation\n\nsample splitting\ncross-validation\nbootstrap\n\n\nExternal validation\n\nTemporal\nGeographical\nDifferent data source to calculate same variable\nDifferent disease\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. ‚ÄúCoefficient of Determination.‚Äù https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n‚Äî‚Äî‚Äî. 2023b. ‚ÄúOne-Way Analysis of Variance.‚Äù https://en.wikipedia.org/wiki/One-way_analysis_of_variance.",
    "crumbs": [
      "Machine learning (ML)",
      "Continuous outcome"
    ]
  },
  {
    "objectID": "machinelearning2.html",
    "href": "machinelearning2.html",
    "title": "Data spliting",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nLoad dataset\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n  \n\n\n\nSee (KDnuggets 2023; Kuhn 2023)\n\n# Using a seed to randomize in a reproducible way \nset.seed(123)\nrequire(caret)\nsplit&lt;-createDataPartition(y = ObsData$Length.of.Stay, \n                           p = 0.7, list = FALSE)\nstr(split)\n#&gt;  int [1:4017, 1] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  - attr(*, \"dimnames\")=List of 2\n#&gt;   ..$ : NULL\n#&gt;   ..$ : chr \"Resample1\"\ndim(split)\n#&gt; [1] 4017    1\ndim(ObsData)*.7 # approximate train data\n#&gt; [1] 4014.5   36.4\ndim(ObsData)*(1-.7) # approximate train data\n#&gt; [1] 1720.5   15.6\n\nSplit the data\n\n# create train data\ntrain.data&lt;-ObsData[split,]\ndim(train.data)\n#&gt; [1] 4017   52\n# create test data\ntest.data&lt;-ObsData[-split,]\ndim(test.data)\n#&gt; [1] 1718   52\n\nTrain the model\n\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nfit.train1&lt;-lm(out.formula1, data = train.data)\n# summary(fit.train1)\n\nFunction that gives performance measures\n\nperform &lt;- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p &lt;- dim(model.matrix(model.fit))[2]\n  # predicted value\n  pred.y &lt;- predict(model.fit, new.data)\n  # sample size\n  n &lt;- length(pred.y)\n  # outcome\n  new.data.y &lt;- as.numeric(new.data[,y.name])\n  # R2\n  R2 &lt;- caret:::R2(pred.y, new.data.y)\n  # adj R2 using alternate formula\n  df.residual &lt;- n-p\n  adjR2 &lt;- 1-(1-R2)*((n-1)/df.residual)\n  # RMSE\n  RMSE &lt;-  caret:::RMSE(pred.y, new.data.y)\n  # combine all of the results\n  res &lt;- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  # returning object\n  return(res)\n}\n\nExtract performance measures\n\nperform(new.data=train.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 4017 64 0.081 0.067 24.647\nperform(new.data=test.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 1718 64 0.056  0.02 25.488\nperform(new.data=ObsData,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 5735 64 0.073 0.063 24.902\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKDnuggets. 2023. ‚ÄúDataset Splitting Best Practices in Python.‚Äù https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023. ‚ÄúData Splitting.‚Äù https://topepo.github.io/caret/data-splitting.html.",
    "crumbs": [
      "Machine learning (ML)",
      "Data spliting"
    ]
  },
  {
    "objectID": "machinelearning3.html",
    "href": "machinelearning3.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nNow, we will describe the ideas of cross-validation.\nLoad previously saved data\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\n\nk-fold cross-vaildation\nSee (Wikipedia 2023)\n\n\n\n\n\n\n\n\n\nk = 5\ndim(ObsData)\n#&gt; [1] 5735   52\nset.seed(567)\n# create folds (based on outcome)\nfolds &lt;- createFolds(ObsData$Length.of.Stay, k = k, \n                     list = TRUE, returnTrain = TRUE)\nmode(folds)\n#&gt; [1] \"list\"\ndim(ObsData)*4/5 # approximate training data size\n#&gt; [1] 4588.0   41.6\ndim(ObsData)/5  # approximate test data size\n#&gt; [1] 1147.0   10.4\nlength(folds[[1]])\n#&gt; [1] 4588\nlength(folds[[5]])\n#&gt; [1] 4587\nstr(folds[[1]])\n#&gt;  int [1:4588] 1 2 4 6 7 8 9 10 11 13 ...\nstr(folds[[5]])\n#&gt;  int [1:4587] 1 3 5 6 7 8 10 11 12 13 ...\n\nCalculation for Fold 1\n\nfold.index &lt;- 1\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1] 1 2 4 6 7 8\nfold1.train &lt;- ObsData[fold1.train.ids,]\nfold1.test &lt;- ObsData[-fold1.train.ids,]\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nmodel.fit &lt;- lm(out.formula1, data = fold1.train)\npredictions &lt;- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#&gt;         n  p    R2  adjR2  RMSE\n#&gt; [1,] 1147 64 0.051 -0.004 24.86\n\nCalculation for Fold 2\n\nfold.index &lt;- 2\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1] 2 3 4 5 6 7\nfold1.train &lt;- ObsData[fold1.train.ids,]\nfold1.test &lt;- ObsData[-fold1.train.ids,]\nmodel.fit &lt;- lm(out.formula1, data = fold1.train)\npredictions &lt;- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 1147 64 0.066 0.011 24.714\n\nUsing caret package to automate\nSee (Kuhn 2023)\n\n# Using Caret package\nset.seed(504)\n# make a 5-fold CV\nctrl&lt;-trainControl(method = \"cv\",number = 5)\n# fit the model with formula = out.formula1\n# use training method lm\nfit.cv&lt;-train(out.formula1, trControl = ctrl,\n               data = ObsData, method = \"lm\")\nfit.cv\n#&gt; Linear Regression \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4588, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.05478  0.05980578  15.19515\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n# extract results from each test data \nsummary.res &lt;- fit.cv$resample\nsummary.res\n\n\n  \n\n\nmean(fit.cv$resample$Rsquared)\n#&gt; [1] 0.05980578\nsd(fit.cv$resample$Rsquared)\n#&gt; [1] 0.01204451\nmean(fit.cv$resample$RMSE)\n#&gt; [1] 25.05478\nsd(fit.cv$resample$RMSE)\n#&gt; [1] 2.240366\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKuhn, Max. 2023. ‚ÄúModel Training and Tuning.‚Äù https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. ‚ÄúCross-Validation (Statistics).‚Äù https://en.wikipedia.org/wiki/Cross-validation_(statistics).",
    "crumbs": [
      "Machine learning (ML)",
      "Cross-validation"
    ]
  },
  {
    "objectID": "machinelearning4.html",
    "href": "machinelearning4.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of binary outcomes. We will use logistic regression to build the first prediction model.\nRead previously saved data\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nOutcome levels (factor)\n\nLabel\n\nPossible values of outcome\n\n\n\n\nlevels(ObsData$Death)=c(\"No\",\"Yes\") # this is useful for caret\n# ref: https://tinyurl.com/caretbin\nclass(ObsData$Death)\n#&gt; [1] \"factor\"\ntable(ObsData$Death)\n#&gt; \n#&gt;   No  Yes \n#&gt; 2013 3722\n\nMeasuring prediction error\n\n\nBrier score\n\nBrier score 0 means perfect prediction, and\nclose to zero means better prediction,\n1 being the worst prediction.\nLess accurate forecasts get higher score in Brier score.\n\n\n\nAUC\n\nThe area under a ROC curve is called as a c statistics.\nc being 0.5 means random prediction and\n1 indicates perfect prediction\n\n\nPrediction for death\nIn this section, we show the regression fitting when outcome is binary (death).\nVariables\n\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#&gt;  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#&gt;  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#&gt;  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#&gt; [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#&gt; [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#&gt; [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#&gt; [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#&gt; [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#&gt; [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#&gt; [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#&gt; [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#&gt; [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#&gt; [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#&gt; [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#&gt; [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#&gt; [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#&gt; [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula2 &lt;- as.formula(paste(\"Death~ \", paste(baselinevars, collapse = \"+\")))\nsaveRDS(out.formula2, file = \"Data/machinelearning/form2.RDS\")\nfit2 &lt;- glm(out.formula2, data = ObsData, \n            family = binomial(link = \"logit\"))\nrequire(Publish)\nadj.fit2 &lt;- publish(fit2, digits=1)$regressionTable\n\n\nout.formula2\n#&gt; Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#&gt;     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#&gt;     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#&gt;     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#&gt;     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#&gt;     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#&gt;     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nadj.fit2\n\n\n  \n\n\n\nMeasuring prediction error\nAUC\n\nrequire(pROC)\n#&gt; Loading required package: pROC\n#&gt; Type 'citation(\"pROC\")' for a citation.\n#&gt; \n#&gt; Attaching package: 'pROC'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cov, smooth, var\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- predict(fit2, type = \"response\")\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.7682\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.7682\n\nBrier Score\n\nrequire(DescTools)\n#&gt; Loading required package: DescTools\nBrierScore(fit2)\n#&gt; [1] 0.1812502\n\nCross-validation using caret\nBasic setup\n\n# Using Caret package\nset.seed(504)\n\n# make a 5-fold CV\nrequire(caret)\n#&gt; Loading required package: caret\n#&gt; Loading required package: ggplot2\n#&gt; Loading required package: lattice\n#&gt; \n#&gt; Attaching package: 'caret'\n#&gt; The following objects are masked from 'package:DescTools':\n#&gt; \n#&gt;     MAE, RMSE\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n\n# fit the model with formula = out.formula2\n# use training method glm (have to specify family)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\")\nfit.cv.bin\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7545115  0.4659618  0.8535653\n\nExtract results from each test data\n\nsummary.res &lt;- fit.cv.bin$resample\nsummary.res\n\n\n  \n\n\nmean(fit.cv.bin$resample$ROC)\n#&gt; [1] 0.7545115\nsd(fit.cv.bin$resample$ROC)\n#&gt; [1] 0.01651437\n\nMore options\n\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\",\n              preProc = c(\"center\", \"scale\"))\nfit.cv.bin\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; Pre-processing: centered (63), scaled (63) \n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7548047  0.4629717  0.8530367\n\nVariable selection\nWe can also use stepwise regression that uses AIC as a criterion.\n\nset.seed(504)\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin.aic&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmStepAIC\",\n               direction =\"backward\",\n              family = binomial(),\n              metric=\"ROC\")\n\n\nfit.cv.bin.aic\n#&gt; Generalized Linear Model with Stepwise Feature Selection \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens      Spec     \n#&gt;   0.7540424  0.464468  0.8562535\nsummary(fit.cv.bin.aic)\n#&gt; \n#&gt; Call:\n#&gt; NULL\n#&gt; \n#&gt; Coefficients:\n#&gt;                                          Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)                             1.0783624  0.7822168   1.379 0.168019\n#&gt; Disease.categoryOther                   0.4495099  0.0919860   4.887 1.03e-06\n#&gt; `CancerLocalized (Yes)`                 1.8942512  0.5501880   3.443 0.000575\n#&gt; CancerMetastatic                        3.2703316  0.5858715   5.582 2.38e-08\n#&gt; Cardiovascular1                         0.2386749  0.0939617   2.540 0.011081\n#&gt; Congestive.HF1                          0.4539010  0.0971624   4.672 2.99e-06\n#&gt; Dementia1                               0.2380213  0.1162903   2.047 0.040679\n#&gt; Hepatic1                                0.3593093  0.1541762   2.331 0.019779\n#&gt; Tumor1                                 -1.2455123  0.5542624  -2.247 0.024630\n#&gt; Immunosupperssion1                      0.2174294  0.0730803   2.975 0.002928\n#&gt; Transfer.hx1                           -0.1849029  0.0945679  -1.955 0.050555\n#&gt; `age[50,60)`                            0.3621248  0.0984288   3.679 0.000234\n#&gt; `age[60,70)`                            0.6941924  0.0968434   7.168 7.60e-13\n#&gt; `age[70,80)`                            0.6804939  0.1126637   6.040 1.54e-09\n#&gt; `age[80, Inf)`                          0.9833851  0.1410563   6.972 3.13e-12\n#&gt; sexFemale                              -0.2805950  0.0653527  -4.294 1.76e-05\n#&gt; DASIndex                               -0.0429272  0.0062191  -6.902 5.11e-12\n#&gt; APACHE.score                            0.0174907  0.0020017   8.738  &lt; 2e-16\n#&gt; Glasgow.Coma.Score                      0.0093657  0.0012563   7.455 9.00e-14\n#&gt; WBC                                     0.0044518  0.0030090   1.479 0.139009\n#&gt; Temperature                            -0.0524703  0.0192757  -2.722 0.006487\n#&gt; PaO2vs.FIO2                             0.0004741  0.0003054   1.552 0.120548\n#&gt; Hematocrit                             -0.0154796  0.0041593  -3.722 0.000198\n#&gt; Bilirubin                               0.0313087  0.0094004   3.331 0.000867\n#&gt; Weight                                 -0.0031548  0.0011213  -2.813 0.004902\n#&gt; DNR.statusYes                           0.9347360  0.1326924   7.044 1.86e-12\n#&gt; Medical.insuranceMedicare               0.4764895  0.1257582   3.789 0.000151\n#&gt; `Medical.insuranceMedicare & Medicaid`  0.3364916  0.1584757   2.123 0.033729\n#&gt; `Medical.insuranceNo insurance`         0.3711345  0.1568820   2.366 0.017996\n#&gt; Medical.insurancePrivate                0.2632637  0.1139805   2.310 0.020903\n#&gt; `Medical.insurancePrivate & Medicare`   0.2819715  0.1313101   2.147 0.031764\n#&gt; Respiratory.DiagYes                     0.1393974  0.0769026   1.813 0.069886\n#&gt; Cardiovascular.DiagYes                  0.1804967  0.0836679   2.157 0.030982\n#&gt; Neurological.DiagYes                    0.4320266  0.1189357   3.632 0.000281\n#&gt; Gastrointestinal.DiagYes                0.2819563  0.1092206   2.582 0.009836\n#&gt; Hematologic.DiagYes                     0.9734424  0.1651363   5.895 3.75e-09\n#&gt; Sepsis.DiagYes                          0.1539651  0.0943235   1.632 0.102614\n#&gt; `incomeUnder $11k`                      0.2151437  0.0689392   3.121 0.001804\n#&gt; RHC.use                                 0.3552053  0.0713632   4.977 6.44e-07\n#&gt;                                           \n#&gt; (Intercept)                               \n#&gt; Disease.categoryOther                  ***\n#&gt; `CancerLocalized (Yes)`                ***\n#&gt; CancerMetastatic                       ***\n#&gt; Cardiovascular1                        *  \n#&gt; Congestive.HF1                         ***\n#&gt; Dementia1                              *  \n#&gt; Hepatic1                               *  \n#&gt; Tumor1                                 *  \n#&gt; Immunosupperssion1                     ** \n#&gt; Transfer.hx1                           .  \n#&gt; `age[50,60)`                           ***\n#&gt; `age[60,70)`                           ***\n#&gt; `age[70,80)`                           ***\n#&gt; `age[80, Inf)`                         ***\n#&gt; sexFemale                              ***\n#&gt; DASIndex                               ***\n#&gt; APACHE.score                           ***\n#&gt; Glasgow.Coma.Score                     ***\n#&gt; WBC                                       \n#&gt; Temperature                            ** \n#&gt; PaO2vs.FIO2                               \n#&gt; Hematocrit                             ***\n#&gt; Bilirubin                              ***\n#&gt; Weight                                 ** \n#&gt; DNR.statusYes                          ***\n#&gt; Medical.insuranceMedicare              ***\n#&gt; `Medical.insuranceMedicare & Medicaid` *  \n#&gt; `Medical.insuranceNo insurance`        *  \n#&gt; Medical.insurancePrivate               *  \n#&gt; `Medical.insurancePrivate & Medicare`  *  \n#&gt; Respiratory.DiagYes                    .  \n#&gt; Cardiovascular.DiagYes                 *  \n#&gt; Neurological.DiagYes                   ***\n#&gt; Gastrointestinal.DiagYes               ** \n#&gt; Hematologic.DiagYes                    ***\n#&gt; Sepsis.DiagYes                            \n#&gt; `incomeUnder $11k`                     ** \n#&gt; RHC.use                                ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 7433.3  on 5734  degrees of freedom\n#&gt; Residual deviance: 6198.0  on 5696  degrees of freedom\n#&gt; AIC: 6276\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Machine learning (ML)",
      "Binary outcome"
    ]
  },
  {
    "objectID": "machinelearning5.html",
    "href": "machinelearning5.html",
    "title": "Supervised learning",
    "section": "",
    "text": "In this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods.\nIn the first code chunk, we load necessary R libraries that will be utilized throughout the chapter for various machine learning methods and data visualization.\nRead previously saved data\nThe second chunk is dedicated to reading previously saved data and formulas from specified file paths, ensuring that the dataset and predefined formulas are available for subsequent analyses.\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nlevels(ObsData$Death)=c(\"No\",\"Yes\")\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula2 &lt;- readRDS(file = \"Data/machinelearning/form2.RDS\")\n\nRidge, LASSO, and Elastic net\nThe traditional regression models (e.g., linear regression, logistic regression) show poor model performance when\n\npredictors are highly correlated or\nthere are many predictors\n\nIn both cases, the variance of the estimated regression coefficients could be highly variable. Hence, the model often results in poor predictions. The general solution to this problem is to reduce the variance at the cost of introducing some bias in the coefficients. This approach is called regularization or shrinking. Since we are interested in overall prediction rather than individual regression coefficients in a prediction context, this shrinkage approach is almost always beneficial for the model‚Äôs predictive performance. Ridge, LASSO, and Elastic Net are shrinkage machine learning techniques.\n\nRidge: Penalizes the sum of squared regression coefficients (the so-called \\(L_2\\) penalty). This approach does not remove irrelevant predictors, but minimizes the impact of the irrelevant predictors. There is a hyperparameter called \\(\\lambda\\) (lambda) that determines the amount of shrinkage of the coefficients. The larger \\(\\lambda\\) indicates more penalization of the coefficients.\nLASSO: The Least Absolute Shrinkage and Selection Operator (LASSO) is quite similar conceptually to the ridge regression. However, lasso penalizes the sum of the absolute values of regression coefficients (so-called \\(L_1\\) penalty). As a result, a high \\(\\lambda\\) value forces many coefficients to be exactly zero in lasso regression, suggesting a reduced model with fewer predictors, which is never the case in ridge regression.\nElastic Net: The elastic net combines the penalties of ridge regression and lasso to get the best of both methods. Two hyperparameters in the elastic net are \\(\\alpha\\) (alpha) and \\(\\lambda\\).\n\nWe can use the glmnet function in R to fit these there models.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nContinuous outcome\nCross-validation LASSO\n\n\n\n\n\n\n\n\nIn this code chunk, we implement a machine learning model training process with a focus on utilizing cross-validation and tuning parameters to optimize the model. Cross-validation is a technique used to assess how well the model will generalize to an independent dataset by partitioning the original dataset into a training set to train the model, and a test set to evaluate it. Here, we specify that we are using a particular type of cross-validation, denoted as ‚Äúcv‚Äù, and that we will be creating 5 folds (or partitions) of the data, as indicated by number = 5.\nThe model being trained is specified to use a method known as ‚Äúglmnet‚Äù, which is capable of performing lasso, ridge, and elastic net regularization regressions. Tuning parameters are crucial in controlling the behavior of our learning algorithm. In this instance, we specify lambda and alpha as our tuning parameters, which control the amount of regularization applied to the model and the mixing percentage between lasso and ridge regression, respectively. The tuneGrid argument is used to specify the exact values of alpha and lambda that the model should consider during training. The verbose = FALSE argument ensures that additional model training details are not printed during the training process. Finally, the trained model is stored in an object for further examination and use.\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nfit.cv.con &lt;- train(out.formula1, \n                    trControl = ctrl,\n                    data = ObsData, method = \"glmnet\",\n                    lambda= 0,\n                    tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                    verbose = FALSE)\nfit.cv.con\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.11407  0.05861443  15.21187\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 1\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Ridge\nSubsequent code chunks explore Ridge regression and Elastic Net, employing similar methodologies but adjusting tuning parameters accordingly.\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nfit.cv.con &lt;-train(out.formula1, \n                   trControl = ctrl,\n                   data = ObsData, method = \"glmnet\",\n                   lambda= 0,\n                   tuneGrid = expand.grid(alpha = 0, lambda = 0),\n                   verbose = FALSE)\nfit.cv.con\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4588, 4588, 4587, 4589 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.05711  0.06233138  15.15647\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 0\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nBinary outcome\nCross-validation LASSO\nWe then shift to binary outcomes, exploring LASSO and Ridge regression with similar implementations but adjusting for the binary nature of the outcome variable.\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, \n                  trControl = ctrl,\n                  data = ObsData, \n                  method = \"glmnet\",\n                  lambda= 0,\n                  tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                  verbose = FALSE,\n                  metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7554574  0.4669704  0.8533019\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 1\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\n\nNot okay to select variables from a shrinkage model, and then use them in a regular regression\nCross-validation Ridge\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               lambda= 0,\n               tuneGrid = expand.grid(alpha = 0,  \n                                      lambda = 0),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4587, 4587, 4588, 4589 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens      Spec     \n#&gt;   0.7557931  0.468932  0.8519607\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 0\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Elastic net\n\nAlpha = mixing parameter\nLambda = regularization or tuning parameter\nWe can use expand.grid for model tuning\n\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  \n                                      lambda = seq(0.05,0.3,by = 0.05)),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   alpha  lambda  ROC        Sens          Spec     \n#&gt;   0.10   0.05    0.7501508  0.3661247114  0.8981789\n#&gt;   0.10   0.10    0.7468010  0.2747132822  0.9374017\n#&gt;   0.10   0.15    0.7417450  0.1758589188  0.9674937\n#&gt;   0.10   0.20    0.7363601  0.0874263916  0.9862983\n#&gt;   0.10   0.25    0.7300909  0.0258249694  0.9965079\n#&gt;   0.10   0.30    0.7233685  0.0009925558  0.9997312\n#&gt;   0.15   0.05    0.7492697  0.3437650457  0.9070455\n#&gt;   0.15   0.10    0.7422014  0.2205696085  0.9511056\n#&gt;   0.15   0.15    0.7335331  0.1023332469  0.9830739\n#&gt;   0.15   0.20    0.7226744  0.0188733751  0.9967764\n#&gt;   0.15   0.25    0.7152975  0.0000000000  1.0000000\n#&gt;   0.15   0.30    0.7098185  0.0000000000  1.0000000\n#&gt;   0.20   0.05    0.7476579  0.3229053245  0.9148355\n#&gt;   0.20   0.10    0.7367101  0.1679061269  0.9669564\n#&gt;   0.20   0.15    0.7223293  0.0382454971  0.9914029\n#&gt;   0.20   0.20    0.7125355  0.0004962779  0.9994627\n#&gt;   0.20   0.25    0.7062552  0.0000000000  1.0000000\n#&gt;   0.20   0.30    0.6962746  0.0000000000  1.0000000\n#&gt; \n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final values used for the model were alpha = 0.1 and lambda = 0.05.\nplot(fit.cv.bin)\n\n\n\n\n\n\n\nDecision tree\nDecision trees are then introduced and implemented, with visualizations and evaluation metrics provided to assess their performance.\n\nDecision tree\n\nReferred to as Classification and regression trees or CART\nCovers\n\nClassification (categorical outcome)\nRegression (continuous outcome)\n\n\nFlexible to incorporate non-linear effects automatically\n\nNo need to specify higher order terms / interactions\n\n\nUnstable, prone to overfitting, suffers from high variance\n\n\n\nSimple CART\n\nrequire(rpart)\nsummary(ObsData$DASIndex) # Duke Activity Status Index\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   11.00   16.06   19.75   20.50   23.43   33.00\ncart.fit &lt;- rpart(Death~DASIndex, data = ObsData)\npar(mfrow = c(1,1), xpd = NA)\nplot(cart.fit)\ntext(cart.fit, use.n = TRUE)\n\n\n\n\n\n\nprint(cart.fit)\n#&gt; n= 5735 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 5735 2013 Yes (0.3510026 0.6489974)  \n#&gt;   2) DASIndex&gt;=24.92383 1143  514 No (0.5503062 0.4496938)  \n#&gt;     4) DASIndex&gt;=29.14648 561  199 No (0.6452763 0.3547237) *\n#&gt;     5) DASIndex&lt; 29.14648 582  267 Yes (0.4587629 0.5412371) *\n#&gt;   3) DASIndex&lt; 24.92383 4592 1384 Yes (0.3013937 0.6986063) *\nrequire(rattle)\nrequire(rpart.plot)\nrequire(RColorBrewer)\nfancyRpartPlot(cart.fit, caption = NULL)\n\n\n\n\n\n\n\nAUC\n\nrequire(pROC)\n#&gt; Loading required package: pROC\n#&gt; Type 'citation(\"pROC\")' for a citation.\n#&gt; \n#&gt; Attaching package: 'pROC'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cov, smooth, var\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.5912\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.5912\n\nComplex CART\nMore variables\n\nout.formula2\n#&gt; Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#&gt;     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#&gt;     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#&gt;     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#&gt;     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#&gt;     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#&gt;     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nrequire(rpart)\ncart.fit &lt;- rpart(out.formula2, data = ObsData)\n\nCART Variable importance\n\ncart.fit$variable.importance\n#&gt;            DASIndex              Cancer               Tumor                 age \n#&gt;         123.2102455          33.4559400          32.5418433          24.0804860 \n#&gt;   Medical.insurance                 WBC                 edu Cardiovascular.Diag \n#&gt;          14.5199953           5.6673997           3.7441554           3.6449371 \n#&gt;          Heart.rate      Cardiovascular         Trauma.Diag               PaCo2 \n#&gt;           3.4059248           3.1669125           0.5953098           0.2420672 \n#&gt;           Potassium              Sodium             Albumin \n#&gt;           0.2420672           0.2420672           0.1984366\n\nAUC\n\nrequire(pROC)\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.5981\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.5981\n\nCross-validation CART\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"rpart\",\n              metric=\"ROC\")\nfit.cv.bin\n#&gt; CART \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   cp           ROC        Sens       Spec     \n#&gt;   0.007203179  0.6304911  0.2816488  0.9086574\n#&gt;   0.039741679  0.5725283  0.2488649  0.8981807\n#&gt;   0.057128664  0.5380544  0.1287804  0.9473284\n#&gt; \n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final value used for the model was cp = 0.007203179.\n# extract results from each test data \nsummary.res &lt;- fit.cv.bin$resample\nsummary.res\n\n\n  \n\n\n\nEnsemble methods (Type I)\nWe explore ensemble methods, specifically bagging and boosting, through implementation and evaluation in the context of binary outcomes.\nTraining same model to different samples (of the same data)\nCross-validation bagging\n\nBagging or bootstrap aggregation\n\nindependent bootstrap samples (sampling with replacement, B times),\napplies CART on each i (no prunning)\nAverage the resulting predictions\nReduces variance as a result of using bootstrap\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"bag\",\n               bagControl = bagControl(fit = ldaBag$fit, \n                                       predict = ldaBag$pred, \n                                       aggregate = ldaBag$aggregate),\n               metric=\"ROC\")\n#&gt; Warning: executing %dopar% sequentially: no parallel backend registered\nfit.cv.bin\n#&gt; Bagged Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7506666  0.4485809  0.8602811\n#&gt; \n#&gt; Tuning parameter 'vars' was held constant at a value of 63\n\n\nBagging improves prediction accuracy\n\nover prediction using a single tree\n\n\nLooses interpretability\n\nas this is an average of many diagrams now\n\n\nBut we can get a summary of the importance of each variable\n\nBagging Variable importance\n\ncaret::varImp(fit.cv.bin, scale = FALSE)\n#&gt; ROC curve variable importance\n#&gt; \n#&gt;   only 20 most important variables shown (out of 50)\n#&gt; \n#&gt;                    Importance\n#&gt; age                    0.6159\n#&gt; APACHE.score           0.6140\n#&gt; DASIndex               0.5962\n#&gt; Cancer                 0.5878\n#&gt; Creatinine             0.5835\n#&gt; Tumor                  0.5807\n#&gt; blood.pressure         0.5697\n#&gt; Glasgow.Coma.Score     0.5656\n#&gt; Disease.category       0.5641\n#&gt; Temperature            0.5584\n#&gt; DNR.status             0.5572\n#&gt; Hematocrit             0.5525\n#&gt; Weight                 0.5424\n#&gt; Bilirubin              0.5397\n#&gt; income                 0.5319\n#&gt; Immunosupperssion      0.5278\n#&gt; RHC.use                0.5263\n#&gt; Dementia               0.5252\n#&gt; Congestive.HF          0.5250\n#&gt; Hematologic.Diag       0.5250\n\nCross-validation boosting\n\nBoosting\n\nsequentially updated/weighted bootstrap based on previous learning\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"gbm\",\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; Stochastic Gradient Boosting \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   interaction.depth  n.trees  ROC        Sens       Spec     \n#&gt;   1                   50      0.7218938  0.2145970  0.9505647\n#&gt;   1                  100      0.7410292  0.2980581  0.9234228\n#&gt;   1                  150      0.7483014  0.3487142  0.9030028\n#&gt;   2                   50      0.7414513  0.2960631  0.9263816\n#&gt;   2                  100      0.7534264  0.3869684  0.8917212\n#&gt;   2                  150      0.7575826  0.4187512  0.8777477\n#&gt;   3                   50      0.7496078  0.3626125  0.9070358\n#&gt;   3                  100      0.7579645  0.4078244  0.8764076\n#&gt;   3                  150      0.7637074  0.4445909  0.8702298\n#&gt; \n#&gt; Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#&gt; \n#&gt; Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final values used for the model were n.trees = 150, interaction.depth =\n#&gt;  3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nplot(fit.cv.bin)\n\n\n\n\n\n\n\nEnsemble methods (Type II)\nWe introduce the concept of Super Learner, providing external resources for further exploration.\nTraining different models on the same data\nSuper Learner\n\nLarge number of candidate learners (CL) with different strengths\n\nParametric (logistic)\nNon-parametric (CART)\n\n\nCross-validation: CL applied on training data, prediction made on test data\nFinal prediction uses a weighted version of all predictions\n\nWeights = coef of Observed outcome ~ prediction from each CL\n\n\nSteps\nRefer to this tutorial for steps and examples! Refer to the next chapter for more details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following is a brief exercise of super learners in the propensity score context, but we will explore more about this topic in the next chapter.",
    "crumbs": [
      "Machine learning (ML)",
      "Supervised learning"
    ]
  },
  {
    "objectID": "machinelearning6.html",
    "href": "machinelearning6.html",
    "title": "Unsupervised learning",
    "section": "",
    "text": "In this chapter, we will talk about unsupervised learning.\nIn the initial code chunk, we load a specific library that will be utilized for publishing-related functionality throughout the chapter.\nClustering\nClustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity.\nGroup characteristics include (to the extent that is possible)\n\nlow inter-class similarity: observation from different clusters would be dissimilar\nhigh intra-class similarity: observation from the same cluster would be similar\n\nWithin-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances (Wikipedia 2023a)\n\n\n\n\n\n\n\n\nK-means\nK-means is a very popular clustering algorithm, that partitions the data into \\(k\\) groups.\nAlgorithm:\n\nDetermine a number \\(k\\) (e.g., could be 3)\nrandomly select \\(k\\) subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster.\nBy Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong.\ncompute new mean value for each cluster.\nbased on this new mean, try to determine again in which cluster the data points belong.\nprocess continues until the data points do not change cluster membership.\nRead previously saved data\nWe read a previously saved dataset from a specified file path.\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nIn the next few code chunks, we implement k-means clustering on various subsets of the data, visualizing the results and displaying the cluster centers. The first example uses two variables, the second example uses three, and in the third example, a larger subset of variables is selected but not immediately utilized in the clustering. In the subsequent code chunk, we apply k-means clustering to the larger subset of variables, displaying various results and aggregating data by cluster to display mean and standard deviation values for each variable within each cluster.\nExample 1\n\ndatax0 &lt;- ObsData[c(\"Heart.rate\", \"edu\")]\nkres0 &lt;- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#&gt;   Heart.rate      edu\n#&gt; 1   54.55138 11.44494\n#&gt; 2  134.96277 11.75466\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\n\n\n\nExample 2\n\ndatax0 &lt;- ObsData[c(\"blood.pressure\", \"Heart.rate\", \"Respiratory.rate\")]\nkres0 &lt;- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#&gt;   blood.pressure Heart.rate Respiratory.rate\n#&gt; 1       73.71684   54.95789         22.76723\n#&gt; 2       80.10812  135.08956         29.85267\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\n\n\n\nExample with many variables\n\ndatax &lt;- ObsData[c(\"edu\", \"blood.pressure\", \"Heart.rate\", \n                   \"Respiratory.rate\" , \"Temperature\",\n                   \"PH\", \"Weight\", \"Length.of.Stay\")]\n\n\nkres &lt;- kmeans(datax, centers = 3)\n#kres\nhead(kres$cluster)\n#&gt; [1] 2 2 2 1 2 3\nkres$size\n#&gt; [1] 1252 2795 1688\nkres$centers\n#&gt;        edu blood.pressure Heart.rate Respiratory.rate Temperature       PH\n#&gt; 1 11.46447       65.46446   53.17332         22.66717    37.01512 7.378432\n#&gt; 2 11.85665       54.28086  136.34597         29.75277    37.85056 7.385267\n#&gt; 3 11.54214      128.33886  126.12026         29.36611    37.68129 7.401027\n#&gt;     Weight Length.of.Stay\n#&gt; 1 67.48365       18.59425\n#&gt; 2 68.67307       23.41789\n#&gt; 3 66.68351       20.68128\naggregate(datax, by = list(cluster = kres$cluster), mean)\n\n\n  \n\n\naggregate(datax, by = list(cluster = kres$cluster), sd)\n\n\n  \n\n\n\nOptimal number of clusters\nNext, we explore determining the optimal number of clusters, visualizing the total within-cluster sum of squares for different values of k and indicating a chosen value of k with a vertical line on the plot.\n\nrequire(factoextra)\n#&gt; Loading required package: factoextra\n#&gt; Loading required package: ggplot2\n#&gt; Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_nbclust(datax, kmeans, method = \"wss\")+\n  geom_vline(xintercept=3,linetype=3)\n\n\n\n\n\n\n\nHere the vertical line is chosen based on elbow method (Wikipedia 2023b).\nDiscussion\n\nWe need to supply a number, \\(k\\): but we can test different \\(k\\)s to identify optimal value\nClustering can be influenced by outliners, so median based clustering is possible\nmere ordering can influence clustering, hence we should choose different initial means (e.g., nstart should be greater than 1).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. ‚ÄúCross-Validation (Statistics).‚Äù https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n\n\n‚Äî‚Äî‚Äî. 2023b. ‚ÄúElbow Method (Clustering).‚Äù https://en.wikipedia.org/wiki/Elbow_method_(clustering).",
    "crumbs": [
      "Machine learning (ML)",
      "Unsupervised learning"
    ]
  },
  {
    "objectID": "machinelearning6a.html",
    "href": "machinelearning6a.html",
    "title": "NHIS Example",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning (ML) techniques with health survey data. We will use the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP) among adults aged 65 years or older. We will use LASSO and random forest models with sampling weights to obtain population-level predictions. In this tutorial, the split-sample approach as an internal validation technique will be used. You can review the earlier tutorial on data splitting technique. Note that this split-sample approach is flagged as a problematic approach in the literature (Steyerberg et al. 2001). The better approach could be cross-validation and bootstrapping Steyerberg and Steyerberg (2019). In the next tutorial, we will apply the ML techniques for survey data with cross-validation.\n\n\nSteyerberg EW, Harrell Jr FE, Borsboom GJ, Eijkemans MJ, Vergouwe Y, Habbema JD. Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of Clinical Epidemiology. 2001; 54(8):774-81. DOI: 10.1016/S0895-4356(01)00341-9\nSteyerberg EW, Steyerberg EW. Overfitting and optimism in prediction models. Clinical prediction models: A practical approach to development, validation, and updating. 2019:95-112. DOI: 10.1007/978-3-030-16399-0_5\n\n\n\n\n\n\nNote\n\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/nhis2016.RData\")\nls()\n#&gt; [1] \"dat.analytic\"\n\n\ndim(dat.analytic)\n#&gt; [1] 7828   14\n\nhead(dat.analytic)\n\n\n  \n\n\n\nThe dataset contains 7,828 complete case participants (i.e., no missing) with 14 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (high impact chronic pain, the binary outcome variable)\n\nsex: Sex\n\nmarital: Marital status\n\nrace: Race/ethnicity\n\npoverty.status: Poverty status\n\ndiabetes: Diabetes\n\nhigh.cholesterol: High cholesterol\n\nstroke: Stroke\n\narthritis: Arthritis and rheumatism\n\ncurrent.smoker: Current smoker\n\nLet‚Äôs see the descriptive statistics of the predictors stratified by the outcome variable (HICP).\nDescriptive statistics\n\n# Predictors\npredictors &lt;- c(\"sex\", \"marital\", \"race\", \"poverty.status\", \n                \"diabetes\", \"high.cholesterol\", \"stroke\",\n                \"arthritis\", \"current.smoker\")\n\n# Table 1 - Unweighted \n#tab1 &lt;- CreateTableOne(vars = predictors, strata = \"HICP\", \n#                       data = dat.analytic, test = F)\n#print(tab1, showAllLevels = T)\n\ntbl_summary(data = dat.analytic, include = predictors, \n            by = HICP, missing = \"no\") %&gt;% \n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nHICP\n\n\n\n\n0\nN = 6,8471\n\n\n1\nN = 9811\n\n\n\n\n\nsex\n\n\n\n\n¬†¬†¬†¬†Female\n3,841 (56%)\n623 (64%)\n\n\n¬†¬†¬†¬†Male\n3,006 (44%)\n358 (36%)\n\n\nmarital\n\n\n\n\n¬†¬†¬†¬†Never married\n444 (6.5%)\n60 (6.1%)\n\n\n¬†¬†¬†¬†Married/with partner\n3,183 (46%)\n379 (39%)\n\n\n¬†¬†¬†¬†Divorced/separated\n1,252 (18%)\n198 (20%)\n\n\n¬†¬†¬†¬†Widowed\n1,968 (29%)\n344 (35%)\n\n\nrace\n\n\n\n\n¬†¬†¬†¬†White\n5,455 (80%)\n756 (77%)\n\n\n¬†¬†¬†¬†Black\n606 (8.9%)\n99 (10%)\n\n\n¬†¬†¬†¬†Hispanic\n431 (6.3%)\n79 (8.1%)\n\n\n¬†¬†¬†¬†Others\n355 (5.2%)\n47 (4.8%)\n\n\npoverty.status\n\n\n\n\n¬†¬†¬†¬†&lt;100% FPL\n543 (7.9%)\n176 (18%)\n\n\n¬†¬†¬†¬†100-200% FPL\n1,520 (22%)\n307 (31%)\n\n\n¬†¬†¬†¬†200-400% FPL\n2,309 (34%)\n309 (31%)\n\n\n¬†¬†¬†¬†400%+ FPL\n2,475 (36%)\n189 (19%)\n\n\ndiabetes\n1,275 (19%)\n315 (32%)\n\n\nhigh.cholesterol\n3,602 (53%)\n633 (65%)\n\n\nstroke\n527 (7.7%)\n146 (15%)\n\n\narthritis\n3,226 (47%)\n769 (78%)\n\n\ncurrent.smoker\n619 (9.0%)\n123 (13%)\n\n\n\n\n1 n (%)\n\n\n\n\n\nWeight normalization\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Note that we are not interested in the statistical significance of the \\(\\beta\\) coefficients. Hence, not utilizing PSU and strata should not be an issue in this prediction problem. However, we still need to use sampling weights to get population-level predictions. Large weights are usually problematic, particularly with model evaluation. One way to solve the problem is weight normalization (Bruin 2023).\n\n# Normalize weight\ndat.analytic$wgt &lt;- dat.analytic$weight * \n  nrow(dat.analytic)/sum(dat.analytic$weight)\n\n# Weight summary\nsummary(dat.analytic$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     243    1521    2604    2914    3791   14662\nsummary(dat.analytic$wgt)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.08339 0.52198 0.89365 1.00000 1.30109 5.03175\n\n# The weighted and unweighted n are equal\nnrow(dat.analytic)\n#&gt; [1] 7828\nsum(dat.analytic$wgt)\n#&gt; [1] 7828\n\nSplit-sample\nLet us create our training and test data using the split-sample approach. We created 70% training and 30% test data for our example.\n\nset.seed(604001)\ndat.analytic$datasplit &lt;- rbinom(nrow(dat.analytic), \n                                 size = 1, prob = 0.7) \ntable(dat.analytic$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2343 5485\n\n# Training data\ndat.train &lt;- dat.analytic[dat.analytic$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 5485   16\n\n# Test data\ndat.test &lt;- dat.analytic[dat.analytic$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2343   16\n\nRegression formula\nLet‚Äôs us define the regression formula:\n\nFormula &lt;- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#&gt; HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#&gt;     stroke + arthritis + current.smoker\n\nLASSO for Surveys\nNow, we will fit the LASSO model for our survey data. Here are the steps:\n\nWe will fit 5-fold cross-validation on the training data to find the value of lambda that gives minimum prediction error. We will incorporate sampling weights in the model to account for survey data.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nData in matrix\nTo perform LASSO with the glmnet package, we need to set the predictors in the data.matrix format and outcome variable as a vector.\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$HICP) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$HICP) \n\nLet us see the few rows of the data:\n\nhead(X.train)\n#&gt;    sexMale maritalMarried/with partner maritalDivorced/separated maritalWidowed\n#&gt; 12       1                           1                         0              0\n#&gt; 13       1                           0                         1              0\n#&gt; 16       0                           0                         1              0\n#&gt; 42       0                           0                         0              1\n#&gt; 63       0                           0                         0              1\n#&gt; 65       0                           0                         0              1\n#&gt;    raceBlack raceHispanic raceOthers poverty.status100-200% FPL\n#&gt; 12         0            0          0                          0\n#&gt; 13         0            0          0                          0\n#&gt; 16         1            0          0                          0\n#&gt; 42         0            0          0                          0\n#&gt; 63         0            0          0                          0\n#&gt; 65         0            1          0                          1\n#&gt;    poverty.status200-400% FPL poverty.status400%+ FPL diabetesYes\n#&gt; 12                          1                       0           0\n#&gt; 13                          0                       1           0\n#&gt; 16                          1                       0           0\n#&gt; 42                          0                       1           0\n#&gt; 63                          1                       0           0\n#&gt; 65                          0                       0           1\n#&gt;    high.cholesterolYes strokeYes arthritisYes current.smokerYes\n#&gt; 12                   0         0            1                 0\n#&gt; 13                   0         0            1                 0\n#&gt; 16                   1         0            1                 0\n#&gt; 42                   1         0            0                 0\n#&gt; 63                   0         0            0                 0\n#&gt; 65                   1         0            1                 0\n\nAs we can see, factor predictors are coded into dummy variables. It is important to note that the continuous predictors should be standardized. glmnet does this by default. Next, we will use the glmnet function to fit the LASSO model.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nFind best lambda\nNow, we will use k-fold cross-validation with the cv.glmnet function to find the best lambda value. In this example, we choose k = 5. Note that we must incorporate sampling weight to account for survey data.\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, \n                          nfolds = 5, alpha = 1, \n                          family = \"binomial\", \n                          weights = dat.train$wgt)\nfit.cv.lasso\n#&gt; \n#&gt; Call:  cv.glmnet(x = X.train, y = y.train, weights = dat.train$wgt,      nfolds = 5, alpha = 1, family = \"binomial\") \n#&gt; \n#&gt; Measure: Binomial Deviance \n#&gt; \n#&gt;       Lambda Index Measure      SE Nonzero\n#&gt; min 0.001355    43  0.6856 0.01905      12\n#&gt; 1se 0.020128    14  0.7036 0.01446       5\n\nWe can also plot all the lambda values against the deviance (i.e., prediction error).\n\nplot(fit.cv.lasso)\n\n\n\n\n\n\n\n\n# Best lambda\nfit.cv.lasso$lambda.min\n#&gt; [1] 0.001355482\n\nThe lambda value that has the lowest deviance is 0.0013555. Our next step is to fit the LASSO model with the best lambda. Again, we must incorporate sampling weight to account for survey data.\nLASSO with best lambda\n\n# Fit the model on the training set with optimum lambda\nfit.lasso &lt;- glmnet(x = X.train, y = y.train, \n                    alpha = 1, family = \"binomial\",\n                    lambda = fit.cv.lasso$lambda.min, \n                    weights = dat.train$wgt)\nfit.lasso\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df %Dev   Lambda\n#&gt; 1 12 9.68 0.001355\n\nLet‚Äôs check the coefficients from the model:\n\n# Intercept \nfit.lasso$a0\n#&gt;        s0 \n#&gt; -2.491484\n\n# Beta coefficients\nfit.lasso$beta\n#&gt; 15 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                       s0\n#&gt; sexMale                     -0.009832818\n#&gt; maritalMarried/with partner  .          \n#&gt; maritalDivorced/separated    .          \n#&gt; maritalWidowed               0.041111308\n#&gt; raceBlack                   -0.064853067\n#&gt; raceHispanic                -0.037985160\n#&gt; raceOthers                   .          \n#&gt; poverty.status100-200% FPL  -0.268494032\n#&gt; poverty.status200-400% FPL  -0.602097419\n#&gt; poverty.status400%+ FPL     -1.052635980\n#&gt; diabetesYes                  0.369667918\n#&gt; high.cholesterolYes          0.299475863\n#&gt; strokeYes                    0.449050679\n#&gt; arthritisYes                 1.241288036\n#&gt; current.smokerYes            0.253439388\n\nAs we can see, the coefficient is not shown for some predictors. This is because the LASSO model shrunk the coefficient to zero. In other words, these predictors were dropped entirely from the model because they were not contributing enough to predict the outcome. next, we will use the final model to make predictions on new observations or our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.lasso &lt;- predict(fit.lasso, \n                               newx = X.test, \n                               type = \"response\")\nhead(dat.test$pred.lasso)\n#&gt;            s0\n#&gt; 3  0.05711170\n#&gt; 11 0.03716633\n#&gt; 28 0.14049527\n#&gt; 30 0.05764351\n#&gt; 31 0.11408034\n#&gt; 59 0.32540267\n\nModel performance\nNow, we will calculate the model performance measures such as AUC, calibration slope, and Brier score Christodoulou et al. (2019). We will incorporate sampling weights to get population-level estimates.\n\n\nSteyerberg EW, Vickers AJ, Cook NR, Gerds T, Gonen M, Obuchowski N, Pencina MJ, Kattan MW. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.). 2010;21(1):128. DOI: 10.1097/EDE.0b013e3181c30fb2\nSteyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European heart journal. 2014;35(29):1925-31. DOI: 10.1093/eurheartj/ehu207\nChristodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology. 2019;110:12-22. DOI: 10.1016/j.jclinepi.2019.02.004\n\n\n\n\n\n\nNote\n\n\n\n\nArea under the curve (AUC) is a measure of discrimination or accuracy of a model. A higher AUC is better. An AUC value of 1 is considered a perfect prediction, while an AUC value of 0.50 is no better than a coin toss. In practice, AUC values of 0.70 to 0.80 are considered good, and those \\(&gt;0.80\\) are considered very good.\nCalibration is defined as the agreement between observed and predicted probability of the outcome. In this exercise, we will estimate the calibration slope as a measure of calibration. A calibration slope of 1 reflects a well-calibrated model, a calibration slope less than 1 indicates overfitting and greater than 1 indicates underfitting of the model.\nThe Brier score is a measure of overall performance. The Brier score can range from 0 to 1 and is similar to the mean squared error. A lower Brier score value (closer to 0) indicates a better model.\n\n\n\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, \n                                     dat.test$HICP, \n                                     weight = dat.test$wgt))\nauc.lasso\n#&gt; [1] 0.7662941\n\n\n# Logit of the predicted probability\ndat.test$pred.lasso.logit &lt;- Logit(dat.test$pred.lasso)\n\n# Weighted calibration slope\nmod.cal &lt;- glm(HICP ~ pred.lasso.logit, data = dat.test, \n               family = binomial, weights = wgt)\ncal.slope.lasso &lt;- summary(mod.cal)$coef[2,1]\ncal.slope.lasso\n#&gt; [1] 1.244645\n\n\n# Weighted Brier Score\nbrier.lasso &lt;- mean(brierscore(HICP ~ dat.test$pred.lasso, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier.lasso\n#&gt; [1] 0.09978551\n\nRandom Forest for Surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model:\n\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey data.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nFormula\nWe will use the same formula defined above.\n\nFormula\n#&gt; HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#&gt;     stroke + arthritis + current.smoker\n\nHyperparameter tuning\nFor tuning the hyperparameters, let‚Äôs use the grid search approach.\n\n# Grid with 1000 models - huge time consuming\n#grid.search &lt;- expand.grid(mtry = 1:10, node.size = 1:10, \n#                           num.trees = seq(50,500,50), \n#                           oob_error = 0)\n  \n# Grid with 36 models as an exercise\ngrid.search &lt;- expand.grid(mtry = 5:7, node.size = 1:3, \n                           num.trees = seq(200,500,100), \n                           oob_error = 0)\nhead(grid.search)\n\n\n  \n\n\n\nNow, we will fit the random forest model with the selected grids. We will incorporate sampling weight as the case weight in the ranger function.\n\n## Calculate prediction error for each grid \nfor(ii in 1:nrow(grid.search)) {\n  # Model on training set with grid\n  fit.rf.tune &lt;- ranger(formula = Formula, \n                        data = dat.train, \n                        num.trees = grid.search$num.trees[ii],\n                        mtry = grid.search$mtry[ii], \n                        min.node.size = grid.search$node.size[ii],\n                        importance = 'impurity', \n                        case.weights = dat.train$wgt)\n  \n  # Add Out-of-bag (OOB) error to each grid\n  grid.search$oob_error[ii] &lt;- sqrt(fit.rf.tune$prediction.error)\n}\nhead(grid.search)\n\n\n  \n\n\n\nLet‚Äôs check which combination of hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) gives minimum prediction error.\n\nposition &lt;- which.min(grid.search$oob_error)\ngrid.search[position,]\n\n\n  \n\n\n\nModel after tuning\nNow, we will fit the random forest model with the tuned hyperparameters.\n\n# Fit the model on the training set \nfit.rf &lt;- ranger(formula = Formula, \n                 data = dat.train, \n                 case.weights = dat.train$wgt, \n                 probability = T,\n                 num.trees = grid.search$num.trees[position], \n                 min.node.size = grid.search$node.size[position], \n                 mtry = grid.search$mtry[position], \n                 importance = 'impurity')\n\n# Fitted random forest model\nfit.rf\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  300 \n#&gt; Sample size:                      5485 \n#&gt; Number of independent variables:  9 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.1110941\n\nNow, we can use the model to make predictions on our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.rf &lt;- predict(fit.rf, \n                            data = dat.test)$predictions[,2]\nhead(dat.test$pred.rf)\n#&gt; [1] 0.031093101 0.003712312 0.129992168 0.044802288 0.025764960 0.307125058\n\nModel performance\nThe same as the LASSO model, we can calculate the AUC, calibration slope, and Brier score.\n\n# AUC on the test set with sampling weights\nauc.rf &lt;- WeightedAUC(WeightedROC(dat.test$pred.rf, \n                                  dat.test$HICP, \n                                  weight = dat.test$wgt))\nauc.rf\n#&gt; [1] 0.6941022\n\n\n# Logit of the predicted probability\ndat.test$pred.rf[dat.test$pred.rf == 0] &lt;- 0.00001\ndat.test$pred.rf.logit &lt;- Logit(dat.test$pred.rf)\n\n# Weighted calibration slope\nmod.cal &lt;- glm(HICP ~ pred.rf.logit, \n               data = dat.test, \n               family = binomial, \n               weights = wgt)\ncal.slope.rf &lt;- summary(mod.cal)$coef[2,1]\ncal.slope.rf\n#&gt; [1] 0.4977901\n\n\n# Weighted Brier Score\nbrier.rf &lt;- mean(brierscore(HICP ~ dat.test$pred.rf, \n                            data = dat.test,\n                            wt = dat.test$wgt))\nbrier.rf\n#&gt; [1] 0.1095384\n\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\nggplot(\n  enframe(fit.rf$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") + \n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\nAs per the figure, marital status, poverty status, sex, and arthritis are the most influential predictors in predicting HICP, while stroke is the least important predictor.\nPerformance comparison\n\n\n\n\n\n\n\n\nModel\nAUC\nCalibration slope\nBrier score\n\n\n\nLASSO\n0.7662941\n1.2446448\n0.0997855\n\n\nRandom forest\n0.6941022\n0.4977901\n0.1095384\n\n\nReferences\n\n\n\n\nBruin, J. 2023. ‚ÄúAdvanced Topics in Survey Data Analysis.‚Äù 2023. https://stats.oarc.ucla.edu/other/mult-pkg/seminars/advanced-topics-in-survey-data-analysis/.\n\n\nChristodoulou, Evangelia, Jie Ma, Gary S Collins, Ewout W Steyerberg, Jan Y Verbakel, and Ben Van Calster. 2019. ‚ÄúA Systematic Review Shows No Performance Benefit of Machine Learning over Logistic Regression for Clinical Prediction Models.‚Äù Journal of Clinical Epidemiology 110: 12‚Äì22.\n\n\nSteyerberg, Ewout W, Frank E Harrell Jr, Gerard JJM Borsboom, MJC Eijkemans, Yvonne Vergouwe, and J Dik F Habbema. 2001. ‚ÄúInternal Validation of Predictive Models: Efficiency of Some Procedures for Logistic Regression Analysis.‚Äù Journal of Clinical Epidemiology 54 (8): 774‚Äì81.\n\n\nSteyerberg, Ewout W, and Ewout W Steyerberg. 2019. ‚ÄúOverfitting and Optimism in Prediction Models.‚Äù Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating, 95‚Äì112.\n\n\nSteyerberg, Ewout W, and Yvonne Vergouwe. 2014. ‚ÄúTowards Better Clinical Prediction Models: Seven Steps for Development and an ABCD for Validation.‚Äù European Heart Journal 35 (29): 1925‚Äì31.\n\n\nSteyerberg, Ewout W, Andrew J Vickers, Nancy R Cook, Thomas Gerds, Mithat Gonen, Nancy Obuchowski, Michael J Pencina, and Michael W Kattan. 2010. ‚ÄúAssessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures.‚Äù Epidemiology (Cambridge, Mass.) 21 (1): 128.",
    "crumbs": [
      "Machine learning (ML)",
      "NHIS Example"
    ]
  },
  {
    "objectID": "machinelearning6b.html",
    "href": "machinelearning6b.html",
    "title": "Replicate Results",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning techniques with health survey data. We will replicate some of the results of this article by Falasinnu et al.¬†(2023).\n\n\nFalasinnu T, Hossain MB, Weber II KA, Helmick CG, Karim ME, Mackey S. The Problem of Pain in the United States: A Population-Based Characterization of Biopsychosocial Correlates of High Impact Chronic Pain Using the National Health Interview Survey. The Journal of Pain. 2023;24(6):1094-103. DOI: 10.1016/j.jpain.2023.03.008\nThe authors used the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP). They also evaluated the predictive performances of the models within sociodemographic subgroups, such as sex (male, female), age (\\(&lt;65\\), \\(\\ge 65\\)), and race/ethnicity (White, Black, Hispanic). They used LASSO and random forest models with 5-fold cross-validation as an internal validation. To obtain population-level predictions, they account for survey weights in both models.\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\n\n\n\n\nNote\n\n\n\nTo handle missing data in the predictors, they used multiple imputation technique. However, for simplicity, this tutorial focuses on a complete case dataset. We will also only focus on predicting HICP for people aged 65 years or older (a dataset of ~8,800 participants compared to the dataset of 33,000 participants aged 18 years or older).\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/Falasinnu2023.RData\")\nls()\n#&gt; [1] \"dat\"\n\n\ndim(dat)\n#&gt; [1] 8881   49\n\nThe dataset contains 8,881 participants aged 65 years or older with 49 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (binary outcome variable)\n\nage: Age\n\nsex: Sex\n\nhhsize: Number of people in household\n\nborn: Citizenship\n\nmarital: Marital status\n\nregion: Region\n\nrace: Race/ethnicity\n\neducation: Education\n\nemployment.status: Employment status\n\npoverty.status: Poverty status\n\nveteran: Veteran\n\ninsurance: Health insurance coverage\n\nsex.orientation: Sexual orientation\n\nworried.money: Worried about money\n\ngood.neighborhood: Good neighborhood\n\npsy.symptom: Psychological symptoms\n\nvisit.ED: Number of times in ER/ED\n\nsurgery: Number of surgeries in past 12 months\n\ndr.visit: Time since doctor visits\n\ncancer: Cancer\n\nasthma: Asthma\n\nhtn: Hypertension\n\nliver.disease: Liver disease\n\ndiabetes: Diabetes\n\nulcer: Ulcer\n\nstroke: Stroke\n\nemphysema: Emphysema\n\ncopd: COPD\n\nhigh.cholesterol: High cholesterol\n\ncoronary.heart.disease: Coronary heart disease\n\nangina: Angina pectoris\n\nheart.attack: Heart attack\n\nheart.disease: Heart condition/disease\n\narthritis: Arthritis and rheumatism\n\ncrohns.disease: Crohn‚Äôs disease\n\nplace.routine.care: Usual place for routine care\n\ntrouble.asleep: Trouble falling asleep\n\nobese: Obesity\n\ncurrent.smoker: Current smoker\n\nheavy.drinker: Heavy drinker\n\nhospitalization: Hospital stay days\n\nbetter.health.status: Better health status\n\nphysical.activity: Physical activity\n\nSee the NHIS 2016 dataset and the article for better understanding of the variables.\nComplete case data\n\n# Age\ntable(dat$age, useNA = \"always\")\n#&gt; \n#&gt;  &lt;65  65+ &lt;NA&gt; \n#&gt;    0 8881    0\n\nLet us consider a complete case dataset\n\ndat.complete &lt;- na.omit(dat)\ndim(dat.complete)\n#&gt; [1] 7280   49\n\nAs we can see, there are 7,280 participants with complete case information. Let‚Äôs see the descriptive statistics of the predictors stratified by HICP.\nDescriptive statistics\n\n# Predictors\npredictors &lt;- c(\"sex\", \"hhsize\", \"born\", \"marital\", \n                \"region\", \"race\", \"education\", \n                \"employment.status\", \"poverty.status\",\n                \"veteran\", \"insurance\", \n                \"sex.orientation\", \"worried.money\", \n                \"good.neighborhood\", \n                \"psy.symptom\", \"visit.ED\", \"surgery\", \n                \"dr.visit\", \"cancer\", \n                \"asthma\", \"htn\", \"liver.disease\", \n                \"diabetes\", \"ulcer\", \"stroke\",\n                \"emphysema\", \"copd\", \"high.cholesterol\",\n                \"coronary.heart.disease\", \n                \"angina\", \"heart.attack\", \n                \"heart.disease\", \"arthritis\", \n                \"crohns.disease\", \"place.routine.care\", \n                \"trouble.asleep\", \"obese\", \n                \"current.smoker\", \"heavy.drinker\",\n                \"hospitalization\", \n                \"better.health.status\", \n                \"physical.activity\")\n\n# Table 1 - Unweighted \ntbl_summary(data = dat.complete, \n            include = predictors, \n            by = HICP, missing = \"no\") %&gt;% \n  modify_spanning_header(c(\"stat_1\", \n                           \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nHICP\n\n\n\n\n0\nN = 6,3891\n\n\n1\nN = 8911\n\n\n\n\n\nsex\n\n\n\n\n¬†¬†¬†¬†Female\n3,587 (56%)\n569 (64%)\n\n\n¬†¬†¬†¬†Male\n2,802 (44%)\n322 (36%)\n\n\nhhsize\n2 (1, 2)\n2 (1, 2)\n\n\nborn\n\n\n\n\n¬†¬†¬†¬†Born in US\n5,775 (90%)\n802 (90%)\n\n\n¬†¬†¬†¬†Other place\n614 (9.6%)\n89 (10.0%)\n\n\nmarital\n\n\n\n\n¬†¬†¬†¬†Never married\n411 (6.4%)\n57 (6.4%)\n\n\n¬†¬†¬†¬†Married/with partner\n2,990 (47%)\n349 (39%)\n\n\n¬†¬†¬†¬†Divorced/separated\n1,161 (18%)\n179 (20%)\n\n\n¬†¬†¬†¬†Widowed\n1,827 (29%)\n306 (34%)\n\n\nregion\n\n\n\n\n¬†¬†¬†¬†Northeast\n1,172 (18%)\n143 (16%)\n\n\n¬†¬†¬†¬†Midwest\n1,451 (23%)\n189 (21%)\n\n\n¬†¬†¬†¬†South\n2,203 (34%)\n331 (37%)\n\n\n¬†¬†¬†¬†West\n1,563 (24%)\n228 (26%)\n\n\nrace\n\n\n\n\n¬†¬†¬†¬†White\n5,090 (80%)\n694 (78%)\n\n\n¬†¬†¬†¬†Black\n564 (8.8%)\n88 (9.9%)\n\n\n¬†¬†¬†¬†Hispanic\n406 (6.4%)\n70 (7.9%)\n\n\n¬†¬†¬†¬†Others\n329 (5.1%)\n39 (4.4%)\n\n\neducation\n\n\n\n\n¬†¬†¬†¬†Less than high school\n954 (15%)\n220 (25%)\n\n\n¬†¬†¬†¬†High school/GED\n1,863 (29%)\n269 (30%)\n\n\n¬†¬†¬†¬†Some college\n1,716 (27%)\n248 (28%)\n\n\n¬†¬†¬†¬†Bachelors degree or higher\n1,856 (29%)\n154 (17%)\n\n\nemployment.status\n\n\n\n\n¬†¬†¬†¬†Employed hourly\n578 (9.0%)\n22 (2.5%)\n\n\n¬†¬†¬†¬†Employed non-hourly\n608 (9.5%)\n27 (3.0%)\n\n\n¬†¬†¬†¬†Worked previously\n4,923 (77%)\n777 (87%)\n\n\n¬†¬†¬†¬†Never worked\n280 (4.4%)\n65 (7.3%)\n\n\npoverty.status\n\n\n\n\n¬†¬†¬†¬†&lt;100% FPL\n496 (7.8%)\n154 (17%)\n\n\n¬†¬†¬†¬†100-200% FPL\n1,401 (22%)\n276 (31%)\n\n\n¬†¬†¬†¬†200-400% FPL\n2,157 (34%)\n283 (32%)\n\n\n¬†¬†¬†¬†400%+ FPL\n2,335 (37%)\n178 (20%)\n\n\nveteran\n1,482 (23%)\n163 (18%)\n\n\ninsurance\n\n\n\n\n¬†¬†¬†¬†Uninsured\n32 (0.5%)\n6 (0.7%)\n\n\n¬†¬†¬†¬†Medicaid/Medicare\n2,995 (47%)\n478 (54%)\n\n\n¬†¬†¬†¬†Privately Insured\n2,847 (45%)\n311 (35%)\n\n\n¬†¬†¬†¬†Other\n515 (8.1%)\n96 (11%)\n\n\nsex.orientation\n\n\n\n\n¬†¬†¬†¬†Heterosexual\n6,224 (97%)\n861 (97%)\n\n\n¬†¬†¬†¬†Other\n165 (2.6%)\n30 (3.4%)\n\n\nworried.money\n2,351 (37%)\n491 (55%)\n\n\ngood.neighborhood\n5,988 (94%)\n781 (88%)\n\n\npsy.symptom\n694 (11%)\n353 (40%)\n\n\nvisit.ED\n\n\n\n\n¬†¬†¬†¬†None\n5,050 (79%)\n531 (60%)\n\n\n¬†¬†¬†¬†One\n949 (15%)\n187 (21%)\n\n\n¬†¬†¬†¬†2-3\n313 (4.9%)\n123 (14%)\n\n\n¬†¬†¬†¬†4+\n77 (1.2%)\n50 (5.6%)\n\n\nsurgery\n\n\n\n\n¬†¬†¬†¬†None\n5,218 (82%)\n650 (73%)\n\n\n¬†¬†¬†¬†One\n898 (14%)\n176 (20%)\n\n\n¬†¬†¬†¬†Two\n206 (3.2%)\n44 (4.9%)\n\n\n¬†¬†¬†¬†3+\n67 (1.0%)\n21 (2.4%)\n\n\ndr.visit\n\n\n\n\n¬†¬†¬†¬†&lt;6 months\n5,390 (84%)\n837 (94%)\n\n\n¬†¬†¬†¬†6-12 months\n591 (9.3%)\n41 (4.6%)\n\n\n¬†¬†¬†¬†1-5 years\n281 (4.4%)\n10 (1.1%)\n\n\n¬†¬†¬†¬†&gt;5 years/never\n127 (2.0%)\n3 (0.3%)\n\n\ncancer\n1,566 (25%)\n271 (30%)\n\n\nasthma\n645 (10%)\n176 (20%)\n\n\nhtn\n3,953 (62%)\n689 (77%)\n\n\nliver.disease\n122 (1.9%)\n50 (5.6%)\n\n\ndiabetes\n1,179 (18%)\n278 (31%)\n\n\nulcer\n545 (8.5%)\n161 (18%)\n\n\nstroke\n485 (7.6%)\n130 (15%)\n\n\nemphysema\n235 (3.7%)\n76 (8.5%)\n\n\ncopd\n496 (7.8%)\n145 (16%)\n\n\nhigh.cholesterol\n3,373 (53%)\n577 (65%)\n\n\ncoronary.heart.disease\n814 (13%)\n211 (24%)\n\n\nangina\n298 (4.7%)\n100 (11%)\n\n\nheart.attack\n552 (8.6%)\n154 (17%)\n\n\nheart.disease\n1,083 (17%)\n210 (24%)\n\n\narthritis\n3,017 (47%)\n700 (79%)\n\n\ncrohns.disease\n102 (1.6%)\n29 (3.3%)\n\n\nplace.routine.care\n\n\n\n\n¬†¬†¬†¬†No place\n235 (3.7%)\n26 (2.9%)\n\n\n¬†¬†¬†¬†Doctor's office\n4,634 (73%)\n621 (70%)\n\n\n¬†¬†¬†¬†Hospital/Clinic\n1,403 (22%)\n221 (25%)\n\n\n¬†¬†¬†¬†Other place\n117 (1.8%)\n23 (2.6%)\n\n\ntrouble.asleep\n1,970 (31%)\n424 (48%)\n\n\nobese\n1,757 (28%)\n397 (45%)\n\n\ncurrent.smoker\n565 (8.8%)\n111 (12%)\n\n\nheavy.drinker\n308 (4.8%)\n25 (2.8%)\n\n\nhospitalization\n\n\n\n\n¬†¬†¬†¬†None\n5,009 (78%)\n503 (56%)\n\n\n¬†¬†¬†¬†1-2 days\n592 (9.3%)\n57 (6.4%)\n\n\n¬†¬†¬†¬†3-5 days\n360 (5.6%)\n63 (7.1%)\n\n\n¬†¬†¬†¬†6+ days\n428 (6.7%)\n268 (30%)\n\n\nbetter.health.status\n890 (14%)\n119 (13%)\n\n\nphysical.activity\n\n\n\n\n¬†¬†¬†¬†Less\n3,734 (58%)\n743 (83%)\n\n\n¬†¬†¬†¬†Moderate\n1,794 (28%)\n108 (12%)\n\n\n¬†¬†¬†¬†High\n861 (13%)\n40 (4.5%)\n\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n\n\n\nLASSO for surveys\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Similar to the previous chapter, we will normalize the weight.\nWeight normalization\n\n# Normalize weight\ndat.complete$wgt &lt;- dat.complete$weight * \n  nrow(dat.complete)/sum(dat.complete$weight)\n\n# Weight summary\nsummary(dat.complete$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     243    1503    2583    2886    3747   14662\nsummary(dat.complete$wgt)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.0842  0.5208  0.8950  1.0000  1.2983  5.0804\n\n# The weighted and unweighted n are equal\nnrow(dat.complete)\n#&gt; [1] 7280\nsum(dat.complete$wgt)\n#&gt; [1] 7280\n\nFolds\nLet‚Äôs create five random folds and specify the regression formula.\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, \n                 size = nrow(dat.complete), \n                 replace = T)\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1451 1457 1496 1468 1408\n\nFormula\n\nFormula &lt;- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#&gt; HICP ~ sex + hhsize + born + marital + region + race + education + \n#&gt;     employment.status + poverty.status + veteran + insurance + \n#&gt;     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#&gt;     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#&gt;     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#&gt;     coronary.heart.disease + angina + heart.attack + heart.disease + \n#&gt;     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#&gt;     obese + current.smoker + heavy.drinker + hospitalization + \n#&gt;     better.health.status + physical.activity\n\n5-fold CV LASSO\nNow, we will fit the LASSO model with 5-fold cross-validation (CV). Here are the steps:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit 5-fold cross-validation on the training set to find the value of lambda that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\n\nfit.lasso &lt;- list(NULL)\nauc.lasso &lt;- NULL\ncal.slope.lasso &lt;- NULL\nbrier.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$HICP)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$HICP)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, \n                            y = y.train, \n                            nfolds = 5, \n                            alpha = 1, \n                            family = \"binomial\", \n                            weights = dat.train$wgt)\n  \n  # Fit the model on the training set with optimum lambda\n  fit.lasso[[fold]] &lt;- glmnet(\n    x = X.train, \n    y = y.train, \n    alpha = 1, \n    family = \"binomial\",\n    lambda = fit.cv.lasso$lambda.min,\n    weights = dat.train$wgt)\n  \n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.lasso[[fold]], \n                                 newx = X.test, \n                                 type = \"response\")\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(\n    WeightedROC(dat.test$pred.lasso,\n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  mod.cal &lt;- glm(\n    HICP ~ Logit(dat.test$pred.lasso), \n    data = dat.test, \n    family = binomial, \n    weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.lasso[fold] &lt;- mean(\n    brierscore(HICP ~ dat.test$pred.lasso,\n               data = dat.test, \n               wt = dat.test$wgt))\n}\n\nModel performance\nLet‚Äôs check how prediction worked.\n\n# Fitted LASSO models\nfit.lasso[[1]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 46 23.91 0.002972\nfit.lasso[[2]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 47 25.32 0.002559\nfit.lasso[[3]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df %Dev   Lambda\n#&gt; 1 47 24.3 0.002724\nfit.lasso[[4]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 34 25.27 0.004726\nfit.lasso[[5]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 39 24.32 0.003525\n\n\n# Intercept from the LASSO models in different folds\nfit.lasso[[1]]$a0\n#&gt;        s0 \n#&gt; -3.733405\nfit.lasso[[2]]$a0\n#&gt;        s0 \n#&gt; -3.534898\nfit.lasso[[3]]$a0\n#&gt;        s0 \n#&gt; -3.486223\nfit.lasso[[4]]$a0\n#&gt;        s0 \n#&gt; -3.800206\nfit.lasso[[5]]$a0\n#&gt;       s0 \n#&gt; -3.68776\n\n\n# Beta coefficients from the LASSO models in different folds\nfit.lasso[[1]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                 s0\n#&gt; sexMale                               .           \n#&gt; hhsize                                0.0092060605\n#&gt; bornOther place                       .           \n#&gt; maritalMarried/with partner           .           \n#&gt; maritalDivorced/separated             .           \n#&gt; maritalWidowed                        0.0926365970\n#&gt; regionMidwest                         .           \n#&gt; regionSouth                           .           \n#&gt; regionWest                            0.1470219542\n#&gt; raceBlack                            -0.0436983700\n#&gt; raceHispanic                          .           \n#&gt; raceOthers                            .           \n#&gt; educationHigh school/GED              .           \n#&gt; educationSome college                 .           \n#&gt; educationBachelors degree or higher   .           \n#&gt; employment.statusEmployed non-hourly  .           \n#&gt; employment.statusWorked previously    0.5412332930\n#&gt; employment.statusNever worked         0.8300536966\n#&gt; poverty.status100-200% FPL            0.0636289868\n#&gt; poverty.status200-400% FPL           -0.0697612513\n#&gt; poverty.status400%+ FPL              -0.2887583235\n#&gt; veteranYes                           -0.0300714331\n#&gt; insuranceMedicaid/Medicare            .           \n#&gt; insurancePrivately Insured           -0.0924641963\n#&gt; insuranceOther                        0.1503323815\n#&gt; sex.orientationOther                  0.0765093892\n#&gt; worried.moneyYes                      0.2812674935\n#&gt; good.neighborhoodYes                 -0.2491796538\n#&gt; psy.symptomYes                        0.9726200151\n#&gt; visit.EDOne                           0.0674459188\n#&gt; visit.ED2-3                           0.1375781535\n#&gt; visit.ED4+                            0.4225671575\n#&gt; surgeryOne                            .           \n#&gt; surgeryTwo                            .           \n#&gt; surgery3+                            -0.0839622285\n#&gt; dr.visit6-12 months                  -0.2120063637\n#&gt; dr.visit1-5 years                    -0.4068474570\n#&gt; dr.visit&gt;5 years/never               -0.1453128227\n#&gt; cancerYes                             0.0993613856\n#&gt; asthmaYes                             0.2997325771\n#&gt; htnYes                                0.2082877598\n#&gt; liver.diseaseYes                      0.8058966864\n#&gt; diabetesYes                           0.0007357323\n#&gt; ulcerYes                              0.4180133887\n#&gt; strokeYes                             .           \n#&gt; emphysemaYes                         -0.0509549802\n#&gt; copdYes                               0.1002374105\n#&gt; high.cholesterolYes                   0.1021030868\n#&gt; coronary.heart.diseaseYes             0.0220558291\n#&gt; anginaYes                             0.0849595261\n#&gt; heart.attackYes                       0.2476656673\n#&gt; heart.diseaseYes                      .           \n#&gt; arthritisYes                          0.9746692568\n#&gt; crohns.diseaseYes                     .           \n#&gt; place.routine.careDoctor's office    -0.0583977619\n#&gt; place.routine.careHospital/Clinic     .           \n#&gt; place.routine.careOther place         .           \n#&gt; trouble.asleepYes                     0.2030296869\n#&gt; obeseYes                              0.3879895531\n#&gt; current.smokerYes                     0.3374178965\n#&gt; heavy.drinkerYes                     -0.1268971235\n#&gt; hospitalization1-2 days               0.1192556336\n#&gt; hospitalization3-5 days               .           \n#&gt; hospitalization6+ days                1.0866087297\n#&gt; better.health.statusYes              -0.1013785074\n#&gt; physical.activityModerate            -0.7523741651\n#&gt; physical.activityHigh                -0.6865233686\nfit.lasso[[2]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                s0\n#&gt; sexMale                               .          \n#&gt; hhsize                                0.018493189\n#&gt; bornOther place                       .          \n#&gt; maritalMarried/with partner           .          \n#&gt; maritalDivorced/separated            -0.001903136\n#&gt; maritalWidowed                        .          \n#&gt; regionMidwest                        -0.077656662\n#&gt; regionSouth                           0.066061919\n#&gt; regionWest                            0.198988965\n#&gt; raceBlack                            -0.313705357\n#&gt; raceHispanic                         -0.160881871\n#&gt; raceOthers                            .          \n#&gt; educationHigh school/GED             -0.073948767\n#&gt; educationSome college                 .          \n#&gt; educationBachelors degree or higher  -0.092601336\n#&gt; employment.statusEmployed non-hourly  .          \n#&gt; employment.statusWorked previously    0.662301617\n#&gt; employment.statusNever worked         0.990458783\n#&gt; poverty.status100-200% FPL            .          \n#&gt; poverty.status200-400% FPL           -0.253983648\n#&gt; poverty.status400%+ FPL              -0.485846823\n#&gt; veteranYes                           -0.095649252\n#&gt; insuranceMedicaid/Medicare            .          \n#&gt; insurancePrivately Insured           -0.005530756\n#&gt; insuranceOther                        0.249278523\n#&gt; sex.orientationOther                  .          \n#&gt; worried.moneyYes                      0.301627606\n#&gt; good.neighborhoodYes                 -0.554793692\n#&gt; psy.symptomYes                        0.975043141\n#&gt; visit.EDOne                           0.058872693\n#&gt; visit.ED2-3                           0.255785667\n#&gt; visit.ED4+                            0.422172434\n#&gt; surgeryOne                            0.002831717\n#&gt; surgeryTwo                            0.099101540\n#&gt; surgery3+                             .          \n#&gt; dr.visit6-12 months                  -0.105087667\n#&gt; dr.visit1-5 years                    -0.381233762\n#&gt; dr.visit&gt;5 years/never               -0.459656177\n#&gt; cancerYes                             0.281041121\n#&gt; asthmaYes                             0.427654297\n#&gt; htnYes                                0.219625784\n#&gt; liver.diseaseYes                      0.580991873\n#&gt; diabetesYes                           .          \n#&gt; ulcerYes                              0.348542693\n#&gt; strokeYes                             0.070369016\n#&gt; emphysemaYes                          .          \n#&gt; copdYes                               .          \n#&gt; high.cholesterolYes                   0.150862244\n#&gt; coronary.heart.diseaseYes             0.009536678\n#&gt; anginaYes                             .          \n#&gt; heart.attackYes                       0.405053759\n#&gt; heart.diseaseYes                      .          \n#&gt; arthritisYes                          0.954063592\n#&gt; crohns.diseaseYes                     .          \n#&gt; place.routine.careDoctor's office    -0.045764606\n#&gt; place.routine.careHospital/Clinic     .          \n#&gt; place.routine.careOther place         0.207253975\n#&gt; trouble.asleepYes                     0.212898902\n#&gt; obeseYes                              0.389340100\n#&gt; current.smokerYes                     0.332982403\n#&gt; heavy.drinkerYes                     -0.135194457\n#&gt; hospitalization1-2 days               .          \n#&gt; hospitalization3-5 days               .          \n#&gt; hospitalization6+ days                1.018420971\n#&gt; better.health.statusYes              -0.001173805\n#&gt; physical.activityModerate            -0.703703292\n#&gt; physical.activityHigh                -0.677811398\nfit.lasso[[3]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                               s0\n#&gt; sexMale                              -0.03555390\n#&gt; hhsize                                0.01276707\n#&gt; bornOther place                       .         \n#&gt; maritalMarried/with partner          -0.00226132\n#&gt; maritalDivorced/separated             .         \n#&gt; maritalWidowed                        .         \n#&gt; regionMidwest                         .         \n#&gt; regionSouth                           .         \n#&gt; regionWest                            0.25300309\n#&gt; raceBlack                            -0.21629021\n#&gt; raceHispanic                          .         \n#&gt; raceOthers                            0.07458935\n#&gt; educationHigh school/GED             -0.07527970\n#&gt; educationSome college                 .         \n#&gt; educationBachelors degree or higher  -0.08524865\n#&gt; employment.statusEmployed non-hourly  .         \n#&gt; employment.statusWorked previously    0.66693373\n#&gt; employment.statusNever worked         0.96274685\n#&gt; poverty.status100-200% FPL            .         \n#&gt; poverty.status200-400% FPL           -0.11543598\n#&gt; poverty.status400%+ FPL              -0.31422327\n#&gt; veteranYes                           -0.08849244\n#&gt; insuranceMedicaid/Medicare            .         \n#&gt; insurancePrivately Insured           -0.14856615\n#&gt; insuranceOther                        0.14368311\n#&gt; sex.orientationOther                  .         \n#&gt; worried.moneyYes                      0.28107756\n#&gt; good.neighborhoodYes                 -0.37214169\n#&gt; psy.symptomYes                        1.02129791\n#&gt; visit.EDOne                           0.11427725\n#&gt; visit.ED2-3                           0.23654896\n#&gt; visit.ED4+                            0.53011900\n#&gt; surgeryOne                            0.06030144\n#&gt; surgeryTwo                            .         \n#&gt; surgery3+                            -0.28355643\n#&gt; dr.visit6-12 months                  -0.04219568\n#&gt; dr.visit1-5 years                    -0.83131719\n#&gt; dr.visit&gt;5 years/never               -0.48832578\n#&gt; cancerYes                             0.18366379\n#&gt; asthmaYes                             0.13556093\n#&gt; htnYes                                0.12622357\n#&gt; liver.diseaseYes                      0.62123990\n#&gt; diabetesYes                           .         \n#&gt; ulcerYes                              0.39650846\n#&gt; strokeYes                             .         \n#&gt; emphysemaYes                          .         \n#&gt; copdYes                               0.16563326\n#&gt; high.cholesterolYes                   0.10541633\n#&gt; coronary.heart.diseaseYes             0.08229669\n#&gt; anginaYes                             .         \n#&gt; heart.attackYes                       0.37154172\n#&gt; heart.diseaseYes                      .         \n#&gt; arthritisYes                          0.91902311\n#&gt; crohns.diseaseYes                    -0.01478240\n#&gt; place.routine.careDoctor's office    -0.04968533\n#&gt; place.routine.careHospital/Clinic     .         \n#&gt; place.routine.careOther place         0.20094580\n#&gt; trouble.asleepYes                     0.07725267\n#&gt; obeseYes                              0.40542559\n#&gt; current.smokerYes                     0.27207489\n#&gt; heavy.drinkerYes                     -0.06932537\n#&gt; hospitalization1-2 days              -0.13525647\n#&gt; hospitalization3-5 days               .         \n#&gt; hospitalization6+ days                1.02580763\n#&gt; better.health.statusYes               .         \n#&gt; physical.activityModerate            -0.74686507\n#&gt; physical.activityHigh                -0.90554222\nfit.lasso[[4]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                s0\n#&gt; sexMale                               .          \n#&gt; hhsize                                .          \n#&gt; bornOther place                       .          \n#&gt; maritalMarried/with partner           .          \n#&gt; maritalDivorced/separated             .          \n#&gt; maritalWidowed                        .          \n#&gt; regionMidwest                         .          \n#&gt; regionSouth                           .          \n#&gt; regionWest                            0.151732262\n#&gt; raceBlack                            -0.063747960\n#&gt; raceHispanic                          .          \n#&gt; raceOthers                            .          \n#&gt; educationHigh school/GED              .          \n#&gt; educationSome college                 .          \n#&gt; educationBachelors degree or higher   .          \n#&gt; employment.statusEmployed non-hourly  .          \n#&gt; employment.statusWorked previously    0.521879242\n#&gt; employment.statusNever worked         0.671265401\n#&gt; poverty.status100-200% FPL            .          \n#&gt; poverty.status200-400% FPL           -0.014851029\n#&gt; poverty.status400%+ FPL              -0.229122884\n#&gt; veteranYes                            .          \n#&gt; insuranceMedicaid/Medicare            .          \n#&gt; insurancePrivately Insured           -0.092128877\n#&gt; insuranceOther                        .          \n#&gt; sex.orientationOther                  0.150490732\n#&gt; worried.moneyYes                      0.282333603\n#&gt; good.neighborhoodYes                 -0.104236603\n#&gt; psy.symptomYes                        1.132555465\n#&gt; visit.EDOne                           0.009230339\n#&gt; visit.ED2-3                           0.288663967\n#&gt; visit.ED4+                            0.673934923\n#&gt; surgeryOne                            .          \n#&gt; surgeryTwo                            .          \n#&gt; surgery3+                             .          \n#&gt; dr.visit6-12 months                  -0.018831608\n#&gt; dr.visit1-5 years                    -0.496368171\n#&gt; dr.visit&gt;5 years/never               -0.415165313\n#&gt; cancerYes                             .          \n#&gt; asthmaYes                             0.240957776\n#&gt; htnYes                                0.125845727\n#&gt; liver.diseaseYes                      0.672282863\n#&gt; diabetesYes                           .          \n#&gt; ulcerYes                              0.352645788\n#&gt; strokeYes                             .          \n#&gt; emphysemaYes                          .          \n#&gt; copdYes                               0.054275559\n#&gt; high.cholesterolYes                   0.057735457\n#&gt; coronary.heart.diseaseYes             0.070550439\n#&gt; anginaYes                             .          \n#&gt; heart.attackYes                       0.234807175\n#&gt; heart.diseaseYes                      .          \n#&gt; arthritisYes                          1.043934915\n#&gt; crohns.diseaseYes                     0.086038829\n#&gt; place.routine.careDoctor's office     .          \n#&gt; place.routine.careHospital/Clinic     .          \n#&gt; place.routine.careOther place         .          \n#&gt; trouble.asleepYes                     0.177916442\n#&gt; obeseYes                              0.363088024\n#&gt; current.smokerYes                     0.339985811\n#&gt; heavy.drinkerYes                      .          \n#&gt; hospitalization1-2 days               .          \n#&gt; hospitalization3-5 days               0.073321609\n#&gt; hospitalization6+ days                1.066617646\n#&gt; better.health.statusYes               .          \n#&gt; physical.activityModerate            -0.699175264\n#&gt; physical.activityHigh                -0.642594150\nfit.lasso[[5]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                               s0\n#&gt; sexMale                               .         \n#&gt; hhsize                                0.01585559\n#&gt; bornOther place                       .         \n#&gt; maritalMarried/with partner           .         \n#&gt; maritalDivorced/separated            -0.04982466\n#&gt; maritalWidowed                        0.03837620\n#&gt; regionMidwest                        -0.25040909\n#&gt; regionSouth                           .         \n#&gt; regionWest                            0.08288425\n#&gt; raceBlack                            -0.07020324\n#&gt; raceHispanic                          .         \n#&gt; raceOthers                            .         \n#&gt; educationHigh school/GED              .         \n#&gt; educationSome college                 .         \n#&gt; educationBachelors degree or higher   .         \n#&gt; employment.statusEmployed non-hourly  .         \n#&gt; employment.statusWorked previously    0.64497666\n#&gt; employment.statusNever worked         0.83560645\n#&gt; poverty.status100-200% FPL            .         \n#&gt; poverty.status200-400% FPL           -0.03113577\n#&gt; poverty.status400%+ FPL              -0.22313059\n#&gt; veteranYes                           -0.20269803\n#&gt; insuranceMedicaid/Medicare            .         \n#&gt; insurancePrivately Insured           -0.14170986\n#&gt; insuranceOther                        0.19604683\n#&gt; sex.orientationOther                  .         \n#&gt; worried.moneyYes                      0.31572735\n#&gt; good.neighborhoodYes                 -0.31850186\n#&gt; psy.symptomYes                        1.15194136\n#&gt; visit.EDOne                           0.05961452\n#&gt; visit.ED2-3                           0.34349175\n#&gt; visit.ED4+                            0.25637410\n#&gt; surgeryOne                            .         \n#&gt; surgeryTwo                            .         \n#&gt; surgery3+                             .         \n#&gt; dr.visit6-12 months                   .         \n#&gt; dr.visit1-5 years                    -0.32790023\n#&gt; dr.visit&gt;5 years/never                .         \n#&gt; cancerYes                             0.21372688\n#&gt; asthmaYes                             0.21206722\n#&gt; htnYes                                0.13025900\n#&gt; liver.diseaseYes                      0.44892643\n#&gt; diabetesYes                           0.05119678\n#&gt; ulcerYes                              0.27325347\n#&gt; strokeYes                             .         \n#&gt; emphysemaYes                          .         \n#&gt; copdYes                               0.04954730\n#&gt; high.cholesterolYes                   0.14824572\n#&gt; coronary.heart.diseaseYes             .         \n#&gt; anginaYes                             .         \n#&gt; heart.attackYes                       0.27924438\n#&gt; heart.diseaseYes                      .         \n#&gt; arthritisYes                          1.00382986\n#&gt; crohns.diseaseYes                     .         \n#&gt; place.routine.careDoctor's office    -0.01943640\n#&gt; place.routine.careHospital/Clinic     .         \n#&gt; place.routine.careOther place         0.07273335\n#&gt; trouble.asleepYes                     0.14684831\n#&gt; obeseYes                              0.38029871\n#&gt; current.smokerYes                     0.14111139\n#&gt; heavy.drinkerYes                      .         \n#&gt; hospitalization1-2 days              -0.02310396\n#&gt; hospitalization3-5 days               .         \n#&gt; hospitalization6+ days                1.08731840\n#&gt; better.health.statusYes               .         \n#&gt; physical.activityModerate            -0.76639891\n#&gt; physical.activityHigh                -0.38546251\n\n\n# AUCs from different folds\nauc.lasso\n#&gt; [1] 0.8396619 0.8129805 0.8465035 0.7896878 0.8342721\n\n# Calibration slope from different folds\ncal.slope.lasso\n#&gt; [1] 1.1287166 0.9985467 1.1224240 0.8934633 1.0131154\n\n# Brier score from different folds\nbrier.lasso\n#&gt; [1] 0.08524781 0.08476661 0.08666820 0.09071329 0.08911735\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.lasso)\n#&gt; [1] 0.8246212\n\n# Average calibration slope\nmean(cal.slope.lasso)\n#&gt; [1] 1.031253\n\n# Average Brier score\nmean(brier.lasso)\n#&gt; [1] 0.08730265\n\nAlthough the authors used multiple imputation, our AUC from the LASSO model with complete case data analysis is not that different. Note: the authors reported the AUC values in Table 2.\nRandom forest for surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model with 5-fold CV:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\nFolds\n\nk &lt;- 5\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1451 1457 1496 1468 1408\n\nFormula\n\nFormula\n#&gt; HICP ~ sex + hhsize + born + marital + region + race + education + \n#&gt;     employment.status + poverty.status + veteran + insurance + \n#&gt;     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#&gt;     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#&gt;     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#&gt;     coronary.heart.disease + angina + heart.attack + heart.disease + \n#&gt;     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#&gt;     obese + current.smoker + heavy.drinker + hospitalization + \n#&gt;     better.health.status + physical.activity\n\n5-fold CV random forest\n\nfit.rf &lt;- list(NULL)\nauc.rf &lt;- NULL\ncal.slope.rf &lt;- brier.rf &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  \n  # Tuning the hyperparameters \n  ## Grid with 1000 models - huge time consuming\n  #grid.search &lt;- expand.grid(mtry = 1:10, node.size = 1:10, \n  #                          num.trees = seq(50,500,50), \n  #                           OOB_RMSE = 0)\n  \n  ## Grid with 36 models as an exercise\n  grid.search &lt;- expand.grid(\n    mtry = 5:7, \n    node.size = 1:3, \n    num.trees = seq(200,500,100),\n    OOB_RMSE = 0) \n  \n  ## Model with grids \n  for(ii in 1:nrow(grid.search)) {\n    # Model on training set with grid\n    fit.rf.tune &lt;- ranger(\n      formula = Formula,\n      data = dat.train, \n      num.trees = grid.search$num.trees[ii],\n      mtry = grid.search$mtry[ii], \n      min.node.size = grid.search$node.size[ii],\n      importance = 'impurity', \n      case.weights = dat.train$wgt)\n    \n    # Add Out-of-bag (OOB) error to grid\n    grid.search$OOB_RMSE[ii] &lt;- \n      sqrt(fit.rf.tune$prediction.error)\n  }\n  # Position of the tuned hyperparameters\n  position &lt;- which.min(grid.search$OOB_RMSE)\n  \n  # Fit the model on the training set with tuned hyperparameters\n  fit.rf[[fold]] &lt;- ranger(\n    formula = Formula,\n    data = dat.train, \n    case.weights = dat.train$wgt, \n    probability = T,\n    num.trees = grid.search$num.trees[position],\n    min.node.size = grid.search$node.size[position], \n    mtry = grid.search$mtry[position], \n    importance = 'impurity')\n  \n  # Prediction on the test set\n  dat.test$pred.rf &lt;- predict(\n    fit.rf[[fold]], \n    data = dat.test)$predictions[,2]\n  \n  # AUC on the test set with sampling weights\n  auc.rf[fold] &lt;- WeightedAUC(\n    WeightedROC(dat.test$pred.rf, \n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.rf[dat.test$pred.rf == 0] &lt;- 0.00001\n  mod.cal &lt;- glm(HICP ~ Logit(dat.test$pred.rf), \n                 data = dat.test, \n                 family = binomial, \n                 weights = wgt)\n  cal.slope.rf[fold] &lt;- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.rf[fold] &lt;- mean(brierscore(\n    HICP ~ dat.test$pred.rf, \n    data = dat.test,\n    wt = dat.test$wgt))\n}\n\nModel performance\nLet‚Äôs check how prediction worked.\n\n# Fitted random forest models\nfit.rf[[1]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  500 \n#&gt; Sample size:                      5829 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.0899798\nfit.rf[[2]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5823 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 2 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.0899217\nfit.rf[[3]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  500 \n#&gt; Sample size:                      5784 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08972587\nfit.rf[[4]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5812 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08934626\nfit.rf[[5]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5872 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08929408\n\n\n# AUCs from different folds\nauc.rf\n#&gt; [1] 0.8393204 0.7905715 0.8244523 0.7811141 0.8301950\n\n# Calibration slope from different folds\ncal.slope.rf\n#&gt; [1] 1.2842866 0.9933553 1.1486583 0.9134864 1.2263184\n\n# Brier score from different folds\nbrier.rf\n#&gt; [1] 0.08745016 0.08881079 0.08913696 0.09163393 0.08895764\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.rf)\n#&gt; [1] 0.8131307\n\n# Average calibration slope\nmean(cal.slope.rf)\n#&gt; [1] 1.113221\n\n# Average Brier score\nmean(brier.rf)\n#&gt; [1] 0.0891979\n\nThis AUC from random forest is approximately the same as obtained from the LASSO model.\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\n# Fold 1\nggplot(\n  enframe(fit.rf[[1]]$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\n# Fold 5\nggplot(\n  enframe(fit.rf[[5]]$variable.importance,\n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\nReferences",
    "crumbs": [
      "Machine learning (ML)",
      "Replicate Results"
    ]
  },
  {
    "objectID": "machinelearningQ.html",
    "href": "machinelearningQ.html",
    "title": "Quiz (L)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningQ.html#live-quiz",
    "href": "machinelearningQ.html#live-quiz",
    "title": "Quiz (L)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningQ.html#download-quiz",
    "href": "machinelearningQ.html#download-quiz",
    "title": "Quiz (L)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R functions (L)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nfancyRpartPlot\nrattle\nTo plot an rpart object\n\n\nfviz_nbclust\nfactoextra\nTo visualize the optimal number of clusters\n\n\nkmeans\nbase/stats\nTo conduct K-Means cluster analysis\n\n\nlowess\nbase/stats\nTo perform scatter plot smoothing aka lowess smoothing\n\n\nrpart\nrpart\nTo fit a classification tree (CART)\n\n\nterms\nbase/stats\nTo extarct terms objects\n\n\nvarImp\ncaret\nTo calculate the variable importance measure",
    "crumbs": [
      "Machine learning (ML)",
      "R functions (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html",
    "href": "machinelearningE.html",
    "title": "Exercise 1 (L)",
    "section": "",
    "text": "Problem Statement\nWe will revisit the article by Flegal et al.¬†(2016). We will use the same dataset as in the previous lab exercise on survey data analysis, with some additional predictors in predicting obesity.\nOur primary aim is to predict grade 3 obesity with the following predictors:",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#problem-statement",
    "href": "machinelearningE.html#problem-statement",
    "title": "Exercise 1 (L)",
    "section": "",
    "text": "Age: Age in years at screening\nGender\nRace: Race/ethnicity\nEducation: Education level\nSmoking: Smoking status\nPhysical activity: Level of vigorous work activity\nSleep: Hours of sleep\nHigh blood pressure: Ever doctor told a high blood pressure\nGeneral health: General health condition",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-1-creating-data",
    "href": "machinelearningE.html#question-1-creating-data",
    "title": "Exercise 1 (L)",
    "section": "Question 1: Creating data",
    "text": "Question 1: Creating data\n1(a) Downloading the datasets\nYou can see how datasets are downloaded and merged:\n\nlibrary(nhanesA)\n# library(SASxport)\nlibrary(plyr)\n\n# Demographic data\ndemo &lt;- nhanes('DEMO_H') # Both males and females: 0 - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\", # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                \"RIDEXPRG\", # Pregnancy status at exam\n                \"WTINT2YR\", #  Full sample 2 year weights\n                \"SDMVPSU\", # Masked variance pseudo-PSU\n                \"SDMVSTRA\")] # Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1)\ndemo2 &lt;- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n\n# BMI\nbmx &lt;- nhanes('BMX_H')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n\n# Smoking\nsmq &lt;- nhanes('SMQ_H')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n\n# Physical activity\npaq &lt;- nhanes('PAQ_H')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n\n# Sleep\nslq &lt;- nhanes('SLQ_H')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Hours of sleep\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n\n# High blood pressure\nbpq &lt;- nhanes('BPQ_H')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ020\")] # Ever told you had high blood pressure\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n\n# General health condition\nhuq &lt;- nhanes('HUQ_H')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ010\")] # General health condition\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n\n# Combined data\ndat.full &lt;- join_all(list(demo2, bmx2, smq2, paq2, slq2, bpq2, huq2), by = \"SEQN\",\n                     type='full') \ndim(dat.full) # N = 10,175\n\n1(b) Recoding\nLet us recode the outcome and predictors to make them suitable for analysis:\n\n# Survey design\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Class 3 obesity - BMI &gt;= 40 kg/m^2\nsummary(dat.full$BMXBMI)\ndat.full$obesity &lt;- ifelse(dat.full$BMXBMI &gt;= 40, 1, 0)\ntable(dat.full$obesity, useNA = \"always\")\n\n# Age\ndat.full$age.cat &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\ntable(dat.full$age.cat, useNA = \"always\")\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\ntable(dat.full$gender, useNA = \"always\")\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ntable(dat.full$age.cat, dat.full$race, useNA = \"always\")\ndat.full$race &lt;- car::recode(dat.full$race, \n                             \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black' = 'Black'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             c('Non-Hispanic Asian', \n                             'Other Race - Including Multi-Rac')= 'Other';\n                             else=NA\", \n                             levels = c(\"White\", \"Black\", \"Hispanic\", \"Other\"))\ntable(dat.full$race, useNA = \"always\")\n\n# Education\ndat.full$education &lt;- dat.full$DMDEDUC2\ndat.full$education &lt;- car::recode(dat.full$education, \n                                  \" c('Some college or AA degree', \n                                  'College graduate or above') = '&gt;High school'; \n                                  'High school graduate/GED or equi' = 'High school';\n                                  c('Less than 9th grade',\n                                  '9-11th grade (Includes 12th grad') = \n                                  '&lt;High school'; \n                                  else = NA\", \n                                  levels = c(\"&lt;High school\", \"High school\", \n                                                    \"&gt;High school\"))\ntable(dat.full$education, useNA = \"always\")\n\n# Smoking status\ndat.full$smoking &lt;- dat.full$SMQ020\ntable(dat.full$smoking, useNA = \"always\")\ndat.full$smoking &lt;- car::recode(dat.full$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA  \",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat.full$smoking[dat.full$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\ntable(dat.full$smoking, useNA = \"always\")\n\n# Physical activity\ndat.full$physical.activity &lt;- dat.full$PAQ605\ntable(dat.full$physical.activity, useNA = \"always\")\ndat.full$physical.activity &lt;- car::recode(dat.full$physical.activity, \n                                          \" 'Yes'='Yes'; 'No'='No'; else=NA \", \n                                          levels = c(\"No\", \"Yes\"))\ntable(dat.full$physical.activity, useNA = \"always\")\n\n# Sleep\ndat.full$sleep &lt;- dat.full$SLD010H\ndat.full$sleep &lt;- car::recode(dat.full$sleep, \" 1:6 = 'Less than 7'; 7:9 = '7-9'; \n                              10:24 = 'More than 9';  else=NA \",\n                              levels = c(\"Less than 7\", \"7-9\", \"More than 9\"))\ntable(dat.full$sleep, useNA = \"always\")\n\n# High blood pressure\ndat.full$high.blood.pressure &lt;- dat.full$BPQ020\ntable(dat.full$high.blood.pressure, useNA = \"always\")\ndat.full$high.blood.pressure &lt;- car::recode(dat.full$high.blood.pressure, \n                                            \" 'Yes'='Yes'; 'No'='No'; else=NA \",\n                                            levels = c(\"No\", \"Yes\"))\ntable(dat.full$high.blood.pressure, useNA = \"always\")\n\n# General health condition\ndat.full$general.health &lt;- dat.full$HUQ010\ntable(dat.full$general.health, useNA = \"always\")\ndat.full$general.health &lt;- car::recode(dat.full$general.health, \n                                       \"c('Excellent,', 'Very good,')=\n                                       'Very good or Excellent'; \n                                       'Good,'='Good';\n                                       c('Fair, or', 'Poor?') ='Poor or Fair'; \n                                       else=NA  \",\n                                       levels = c(\"Poor or Fair\", \"Good\", \n                                                  \"Very good or Excellent\"))\ntable(dat.full$general.health, useNA = \"always\")\n\n1(c) Keep relevant variables\nLet‚Äôs keep only the relevant variables for this exercise:\n\n# Keep relevant variables\nvars &lt;- c(\n  # Unique identifier\n  \"SEQN\", \n  \n  # Survey features\n  \"survey.weight\", \"psu\", \"strata\", \n  \n  # Eligibility\n  \"RIDAGEYR\", \"BMXBMI\", \"RIDEXPRG\",\n  \n  # Outcome\n  \"obesity\", \n  \n  # Predictors\n  \"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \"physical.activity\", \n  \"sleep\", \"high.blood.pressure\", \"general.health\")\n\ndat.full2 &lt;- dat.full[,vars]\n\n1(d) Weight normalization\nLarge weights can cause issues when evaluating model performance. Let‚Äôs normalize the survey weights to address this problem:\n\ndat.full2$wgt &lt;- dat.full2$survey.weight * nrow(dat.full2)/sum(dat.full2$survey.weight)\nsummary(dat.full2$wgt)\n\n1(e) Analytic dataset\nThe authors restricted their study to - adults aged 20 years and more, - non-missing body mass index, and - non-pregnant\n\n# Aged 20 years or more\ndat.analytic &lt;- subset(dat.full2, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ntable(dat.analytic$RIDEXPRG)\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\nnrow(dat.analytic)\n\n# Drop irrelevant variables\ndat.analytic$RIDAGEYR &lt;- dat.analytic$BMXBMI &lt;- dat.analytic$RIDEXPRG &lt;- NULL\n\n1(f) Complete case data\nBelow is the code for creating the complete case dataset (no missing for the outcome or predictors):\n\n# Drop missing values\ndat.complete &lt;- dat.analytic[complete.cases(dat.analytic),] # N = 5,433\n\n1(g) Save daatsets\n\nsave(dat.full, dat.full2, dat.analytic, dat.complete, file = \"Data/machinelearning/Flegal2016_v2.RData\")",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-2-importing-data-and-creating-table-1",
    "href": "machinelearningE.html#question-2-importing-data-and-creating-table-1",
    "title": "Exercise 1 (L)",
    "section": "Question 2: Importing data and creating Table 1",
    "text": "Question 2: Importing data and creating Table 1\n2(a) Importing dataset\nLet‚Äôs load the dataset:\n\nload(\"Data/machinelearning/Flegal2016_v2.RData\")\nls()\n#&gt; [1] \"dat.analytic\" \"dat.complete\" \"dat.full\"     \"dat.full2\"\n\nHere,\n\ndat.full: the full dataset with all variables\ndat.full2: the full dataset with only relevant variables for this exercise\ndat.analytic: the analytic dataset with only adults aged 20 years and more, non-missing BMI, and non-pregnant\ndat.complete: the complete case dataset without missing values in the outcome and predictors\n\n\nnames(dat.full2)\n#&gt;  [1] \"SEQN\"                \"survey.weight\"       \"psu\"                \n#&gt;  [4] \"strata\"              \"RIDAGEYR\"            \"BMXBMI\"             \n#&gt;  [7] \"RIDEXPRG\"            \"obesity\"             \"age.cat\"            \n#&gt; [10] \"gender\"              \"race\"                \"education\"          \n#&gt; [13] \"smoking\"             \"physical.activity\"   \"sleep\"              \n#&gt; [16] \"high.blood.pressure\" \"general.health\"      \"wgt\"\n\n2(b) Creating Table 1\nLet‚Äôs create Table 1 for the complete case dataset with unweighted frequencies:\n\nlibrary(tableone)\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \n                \"physical.activity\", \"sleep\", \"high.blood.pressure\", \n                \"general.health\")\ntab1 &lt;- CreateTableOne(vars = predictors, strata = \"obesity\", \n                       data = dat.complete, test = F, addOverall = T)\nprint(tab1, format=\"f\") # Showing only frequencies \n#&gt;                            Stratified by obesity\n#&gt;                             Overall 0    1  \n#&gt;   n                         5433    5023 410\n#&gt;   age.cat                                   \n#&gt;      [20,40)                1806    1659 147\n#&gt;      [40,60)                1892    1728 164\n#&gt;      [60,Inf)               1735    1636  99\n#&gt;   gender = Female           2807    2531 276\n#&gt;   race                                      \n#&gt;      White                  2333    2157 176\n#&gt;      Black                  1109     976 133\n#&gt;      Hispanic               1208    1125  83\n#&gt;      Other                   783     765  18\n#&gt;   education                                 \n#&gt;      &lt;High school           1171    1092  79\n#&gt;      High school            1216    1116 100\n#&gt;      &gt;High school           3046    2815 231\n#&gt;   smoking                                   \n#&gt;      Never smoker           3054    2828 226\n#&gt;      Former smoker          1261    1159 102\n#&gt;      Current smoker         1118    1036  82\n#&gt;   physical.activity = Yes    984     917  67\n#&gt;   sleep                                     \n#&gt;      7-9                    3174    2971 203\n#&gt;      Less than 7            2084    1896 188\n#&gt;      More than 9             175     156  19\n#&gt;   high.blood.pressure = Yes 2037    1810 227\n#&gt;   general.health                            \n#&gt;      Poor or Fair           1260    1084 176\n#&gt;      Good                   2065    1911 154\n#&gt;      Very good or Excellent 2108    2028  80",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-3-prediction-using-split-sample-approach",
    "href": "machinelearningE.html#question-3-prediction-using-split-sample-approach",
    "title": "Exercise 1 (L)",
    "section": "Question 3: Prediction using split sample approach",
    "text": "Question 3: Prediction using split sample approach\nIn this exercise, we will use the split-sample approach to predict obesity. We will create our training and test data using a 60-40 split for the training and test data. We will use the following two methods to predict obesity:\n\nDesign-adjusted logistic with all survey features (psu, strata, and survey weights)\nLASSO with survey weights\n\n3(a) Split the data into training and test\nLet us create our training and test data using the split-sample approach:\n\nset.seed(900)\ndat.complete$datasplit &lt;- rbinom(nrow(dat.complete), size = 1, prob = 0.6) \ntable(dat.complete$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2130 3303\n\n# Training data\ndat.train &lt;- dat.complete[dat.complete$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 3303   16\n\n# Test data\ndat.test &lt;- dat.complete[dat.complete$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2130   16\n\n3(b) Prediction with design-adjusted logistic\nWe will use the design-adjusted logistic regression to predict obesity with the following predictors:\n\nage.cat, gender, race, high.blood.pressure, general.health\n\nInstructions:\n\n1: Create the survey design on the full data and subset the design for those individuals in the training data.\n2: Use the training data design created in step 1 to fit the model\n3: Use the test data to predict the probability of obesity.\n4: Calculate AUC on the test data.\n5: Calculate calibration slope with 95% confidence interval on the test data.\n\nHints:\n\n\nWeightedAUC and WeightedROC are helpful functions in calculating AUC.\nThe Logit function from the DescTools package is helpful in calculating the logit of predicted probabilities for calculating calibration slope.\nUse the normalized weight variable to calculate the AUC and calibration slope.\n\nSimpler Model\n\nlibrary(survey)\nlibrary(DescTools)\nlibrary(WeightedROC)\nlibrary(Publish)\nlibrary(boot)\nlibrary(scoring)\n\n# Design\ndat.full2$miss &lt;- 1\ndat.full2$miss[dat.full2$SEQN %in% dat.train$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full2, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Formula\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \n                \"high.blood.pressure\", \"general.health\")\nFormula &lt;- formula(paste(\"obesity ~ \", paste(predictors, collapse=\" + \")))\n\n# Model\nfit.glm &lt;- svyglm(Formula, design = svy.design, family = binomial)\npublish(fit.glm)\n#&gt;             Variable                  Units OddsRatio       CI.95    p-value \n#&gt;              age.cat                [20,40)       Ref                        \n#&gt;                                     [40,60)      0.63 [0.42;0.95]   0.091799 \n#&gt;                                    [60,Inf)      0.31 [0.17;0.57]   0.018946 \n#&gt;               gender                   Male       Ref                        \n#&gt;                                      Female      1.90 [1.32;2.73]   0.026347 \n#&gt;                 race                  White       Ref                        \n#&gt;                                       Black      1.56 [1.12;2.16]   0.056561 \n#&gt;                                    Hispanic      0.91 [0.53;1.57]   0.758727 \n#&gt;                                       Other      0.37 [0.18;0.75]   0.050646 \n#&gt;            education           &lt;High school       Ref                        \n#&gt;                                 High school      1.34 [0.87;2.07]   0.252853 \n#&gt;                                &gt;High school      1.97 [1.13;3.46]   0.076212 \n#&gt;  high.blood.pressure                     No       Ref                        \n#&gt;                                         Yes      2.20 [1.61;3.02]   0.007835 \n#&gt;       general.health           Poor or Fair       Ref                        \n#&gt;                                        Good      0.52 [0.36;0.74]   0.022344 \n#&gt;                      Very good or Excellent      0.22 [0.14;0.36]   0.003885\n\n# Prediction on the test set\ndat.test$pred.glm &lt;- predict(fit.glm, newdata = dat.test, type = \"response\")\nsummary(dat.test$pred.glm)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.003889 0.027737 0.052118 0.073342 0.094836 0.413143\n\n# AUC on the test set with sampling weights\n# unfortunately strata and cluster omitted\nauc.glm &lt;- WeightedAUC(WeightedROC(dat.test$pred.glm, dat.test$obesity, \n                                   weight = dat.test$wgt))\nauc.glm\n#&gt; [1] 0.6903502\n\n# Function to calculate AUC for bootstrap samples\n# unfortunately strata and cluster omitted\ncalc_auc &lt;- function(data, indices) {\n  d &lt;- data[indices, ]\n  roc_obj &lt;- WeightedROC(d$pred.glm, d$obesity, weight = d$wgt)\n  return(WeightedAUC(roc_obj))\n}\n\n# Perform bootstrapping\nset.seed(123)\nboot_obj &lt;- boot(data = dat.test, statistic = calc_auc, R = 150)\n\n# Get 95% confidence intervals\nci_auc &lt;- boot.ci(boot_obj, type = \"perc\")\nci_auc\n#&gt; BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#&gt; Based on 150 bootstrap replicates\n#&gt; \n#&gt; CALL : \n#&gt; boot.ci(boot.out = boot_obj, type = \"perc\")\n#&gt; \n#&gt; Intervals : \n#&gt; Level     Percentile     \n#&gt; 95%   ( 0.6357,  0.7582 )  \n#&gt; Calculations and Intervals on Original Scale\n#&gt; Some percentile intervals may be unstable\n\n# Weighted calibration slope\n# unfortunately strata and cluster omitted\ndat.test$pred.glm.logit &lt;- DescTools::Logit(dat.test$pred.glm)\nslope.glm &lt;- glm(obesity ~ pred.glm.logit, data = dat.test, family = quasibinomial,\n                 weights = wgt)\npublish(slope.glm)\n#&gt;        Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.glm.logit              0.79 [0.61;0.98] &lt; 1e-04\n\n# Calculate the weighted Brier score\nbrier_score &lt;- mean(brierscore(dat.test$obesity ~ dat.test$pred.glm, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier_score\n#&gt; [1] 0.06878706\n\n3(c) Prediction with design-adjusted logistic with added covariates [90% grade]\nAdd the following variables in the existing model, and assess the model performance in terms of the same 3 performance measures:\n\neducation, smoking, physical.activity, sleep\n\nModel with more variables\n\n# Formula\n# predictors2 &lt;- ...\n# Formula2 &lt;- ...\n\n# Model\n# fit.glm2 &lt;- ...\n\n# Prediction on the test set\n\n# AUC on the test set with sampling weights\n# unfortunately strata and cluster omitted\n# auc.glm2 &lt;-...\n\n# Function to calculate AUC for bootstrap samples\n# unfortunately strata and cluster omitted\n\n# Get 95% confidence intervals from bootstrapping\n# ci_auc2 &lt;- ...\n\n# Weighted calibration slope\n# unfortunately strata and cluster omitted\n# slope.glm2 ...\n\n# Calculate the weighted Brier score\n# brier_score2 &lt;- ...\n\n3(d) Interpretation [10%]\nInterpret the AUC, calibration slope and Brier Score based on the following criteria, and suggest which model you would choose:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting\n\n\n\n\n\nBrier Score\nInterpretation\n\n\n\n0\nPerfect prediction\n\n\n0.01-0.1\nVery good model performance\n\n\n0.11-0.2\nGood model performance\n\n\n0.21-0.3\nFair model performance\n\n\n0.31-0.5\nPoor model performance\n\n\n&gt; 0.5\nVery poor model performance (no skill)\n\n\n\n\nAUC: Higher is better (closer to 1).\nCalibration Slope: Closer to 1 is better.\nBrier Score: Lower is better (closer to 0).\n\nResponse:\n3(e) Prediction with LASSO [Optional]\nNow we will use the LASSO method to predict obesity. We will incorporate sampling weights in the model to account for survey data (no psu or strata). Note that we are not interested in the statistical significance of the beta coefficients. Hence, not utilizing psu and strata should not be an issue in this prediction problem.\nInstructions:\n\n1: Use the training data with normalized weight to fit the model.\n2: Find the optimum lambda using 5-fold cross-validation. Consider the lambda value that gives the minimum prediction error.\n3: Predict the probability of obesity on the test set\n3: Calculate AUC on the test data.\n4: Calculate calibration slope with 95% confidence interval on the test data.\n\nModel\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula2, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$obesity) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula2, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$obesity)\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                          family = \"binomial\", weights = dat.train$wgt)\n\n# Prediction on the test set\ndat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                               s = fit.cv.lasso$lambda.min)\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, dat.test$obesity, \n                                     weight = dat.test$wgt))\nauc.lasso\n\n# Weighted calibration slope\ndat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\nslope.lasso &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, \n                   family = quasibinomial, weights = wgt)\npublish(slope.lasso)\n\nInterpretation [optional]\nInterpret the AUC and calibration slope.",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-4-prediction-using-croos-validation-approach-optional",
    "href": "machinelearningE.html#question-4-prediction-using-croos-validation-approach-optional",
    "title": "Exercise 1 (L)",
    "section": "Question 4: Prediction using croos-validation approach [optional]",
    "text": "Question 4: Prediction using croos-validation approach [optional]\nUse LASSO with 5-fold cross-validation to predict obesity with the same set of predictors (from larger model) used in Question 2. Report the average AUC and average calibration slope with 95% confidence interval over 5 folds.\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, size = nrow(dat.complete), replace = T)\ntable(nfolds)\n\nauc.lasso &lt;- cal.slope.lasso &lt;- cal.slope.se.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula2, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$obesity)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula2, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$obesity)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                            family = \"binomial\", weights = dat.train$wgt)\n\n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                                 s = fit.cv.lasso$lambda.min)\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso,dat.test$obesity, \n                                             weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\n  mod.cal &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, family = binomial, \n                 weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  cal.slope.se.lasso[fold] &lt;- summary(mod.cal)$coef[2,2]\n}\n\n# Average AUC\nmean(auc.lasso)\n\n# Average calibration slope\nmean(cal.slope.lasso)\n\n# 95% CI for calibration slope\ncbind(mean(cal.slope.lasso) - 1.96 * mean(cal.slope.se.lasso), \n      mean(cal.slope.lasso) + 1.96 * mean(cal.slope.se.lasso))",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#knit-your-file",
    "href": "machinelearningE.html#knit-your-file",
    "title": "Exercise 1 (L)",
    "section": "Knit your file",
    "text": "Knit your file\nPlease knit your file once you finished and submit the knitted PDF or doc file. Please also fill-up the following table:\nGroup name: Put the group name here\n\n\nStudent initial\n% contribution\n\n\n\nPut Student 1 initial here\nPut % contribution for Student 1 here\n\n\nPut Student 2 initial here\nPut % contribution for Student 2 here\n\n\nPut Student 3 initial here\nPut % contribution for Student 3 here",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html",
    "href": "machinelearningEsolution.html",
    "title": "Exercise 1 Solution (L)",
    "section": "",
    "text": "Question 1: Creating data\nWe will revisit the article by Flegal et al.¬†(2016). We will use the same dataset as in the previous lab exercise on survey data analysis, with some additional predictors in predicting obesity.\nOur primary aim is to predict grade 3 obesity with the following predictors:",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-1-creating-data",
    "href": "machinelearningEsolution.html#question-1-creating-data",
    "title": "Exercise 1 Solution (L)",
    "section": "",
    "text": "1(a) Downloading the datasets\nYou can see how datasets are downloaded and merged:\n\nlibrary(nhanesA)\n# library(SASxport)\nlibrary(plyr)\n\n# Demographic data\ndemo &lt;- nhanes('DEMO_H') # Both males and females: 0 - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\", # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                \"RIDEXPRG\", # Pregnancy status at exam\n                \"WTINT2YR\", #  Full sample 2 year weights\n                \"SDMVPSU\", # Masked variance pseudo-PSU\n                \"SDMVSTRA\")] # Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1)\ndemo2 &lt;- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n\n# BMI\nbmx &lt;- nhanes('BMX_H')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n\n# Smoking\nsmq &lt;- nhanes('SMQ_H')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n\n# Physical activity\npaq &lt;- nhanes('PAQ_H')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n\n# Sleep\nslq &lt;- nhanes('SLQ_H')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Hours of sleep\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n\n# High blood pressure\nbpq &lt;- nhanes('BPQ_H')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ020\")] # Ever told you had high blood pressure\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n\n# General health condition\nhuq &lt;- nhanes('HUQ_H')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ010\")] # General health condition\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n\n# Combined data\ndat.full &lt;- join_all(list(demo2, bmx2, smq2, paq2, slq2, bpq2, huq2), by = \"SEQN\",\n                     type='full') \ndim(dat.full) # N = 10,175\n\n1(b) Recoding\nLet us recode the outcome and predictors to make them suitable for analysis:\n\n# Survey design\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Class 3 obesity - BMI &gt;= 40 kg/m^2\nsummary(dat.full$BMXBMI)\ndat.full$obesity &lt;- ifelse(dat.full$BMXBMI &gt;= 40, 1, 0)\ntable(dat.full$obesity, useNA = \"always\")\n\n# Age\ndat.full$age.cat &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\ntable(dat.full$age.cat, useNA = \"always\")\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\ntable(dat.full$gender, useNA = \"always\")\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ntable(dat.full$age.cat, dat.full$race, useNA = \"always\")\ndat.full$race &lt;- car::recode(dat.full$race, \n                             \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black' = 'Black'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             c('Non-Hispanic Asian', \n                             'Other Race - Including Multi-Rac')= 'Other';\n                             else=NA\", \n                             levels = c(\"White\", \"Black\", \"Hispanic\", \"Other\"))\ntable(dat.full$race, useNA = \"always\")\n\n# Education\ndat.full$education &lt;- dat.full$DMDEDUC2\ndat.full$education &lt;- car::recode(dat.full$education, \n                                  \" c('Some college or AA degree', \n                                  'College graduate or above') = '&gt;High school'; \n                                  'High school graduate/GED or equi' = 'High school';\n                                  c('Less than 9th grade',\n                                  '9-11th grade (Includes 12th grad') = \n                                  '&lt;High school'; \n                                  else = NA\", \n                                  levels = c(\"&lt;High school\", \"High school\", \n                                                    \"&gt;High school\"))\ntable(dat.full$education, useNA = \"always\")\n\n# Smoking status\ndat.full$smoking &lt;- dat.full$SMQ020\ntable(dat.full$smoking, useNA = \"always\")\ndat.full$smoking &lt;- car::recode(dat.full$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA  \",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat.full$smoking[dat.full$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\ntable(dat.full$smoking, useNA = \"always\")\n\n# Physical activity\ndat.full$physical.activity &lt;- dat.full$PAQ605\ntable(dat.full$physical.activity, useNA = \"always\")\ndat.full$physical.activity &lt;- car::recode(dat.full$physical.activity, \n                                          \" 'Yes'='Yes'; 'No'='No'; else=NA \", \n                                          levels = c(\"No\", \"Yes\"))\ntable(dat.full$physical.activity, useNA = \"always\")\n\n# Sleep\ndat.full$sleep &lt;- dat.full$SLD010H\ndat.full$sleep &lt;- car::recode(dat.full$sleep, \" 1:6 = 'Less than 7'; 7:9 = '7-9'; \n                              10:24 = 'More than 9';  else=NA \",\n                              levels = c(\"Less than 7\", \"7-9\", \"More than 9\"))\ntable(dat.full$sleep, useNA = \"always\")\n\n# High blood pressure\ndat.full$high.blood.pressure &lt;- dat.full$BPQ020\ntable(dat.full$high.blood.pressure, useNA = \"always\")\ndat.full$high.blood.pressure &lt;- car::recode(dat.full$high.blood.pressure, \n                                            \" 'Yes'='Yes'; 'No'='No'; else=NA \",\n                                            levels = c(\"No\", \"Yes\"))\ntable(dat.full$high.blood.pressure, useNA = \"always\")\n\n# General health condition\ndat.full$general.health &lt;- dat.full$HUQ010\ntable(dat.full$general.health, useNA = \"always\")\ndat.full$general.health &lt;- car::recode(dat.full$general.health, \n                                       \"c('Excellent,', 'Very good,')=\n                                       'Very good or Excellent'; \n                                       'Good,'='Good';\n                                       c('Fair, or', 'Poor?') ='Poor or Fair'; \n                                       else=NA  \",\n                                       levels = c(\"Poor or Fair\", \"Good\", \n                                                  \"Very good or Excellent\"))\ntable(dat.full$general.health, useNA = \"always\")\n\n1(c) Keep relevant variables\nLet‚Äôs keep only the relevant variables for this exercise:\n\n# Keep relevant variables\nvars &lt;- c(\n  # Unique identifier\n  \"SEQN\", \n  \n  # Survey features\n  \"survey.weight\", \"psu\", \"strata\", \n  \n  # Eligibility\n  \"RIDAGEYR\", \"BMXBMI\", \"RIDEXPRG\",\n  \n  # Outcome\n  \"obesity\", \n  \n  # Predictors\n  \"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \"physical.activity\", \n  \"sleep\", \"high.blood.pressure\", \"general.health\")\n\ndat.full2 &lt;- dat.full[,vars]\n\n1(d) Weight normalization\nLarge weights can cause issues when evaluating model performance. Let‚Äôs normalize the survey weights to address this problem:\n\ndat.full2$wgt &lt;- dat.full2$survey.weight * nrow(dat.full2)/sum(dat.full2$survey.weight)\nsummary(dat.full2$wgt)\n\n1(e) Analytic dataset\nThe authors restricted their study to - adults aged 20 years and more, - non-missing body mass index, and - non-pregnant\n\n# Aged 20 years or more\ndat.analytic &lt;- subset(dat.full2, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ntable(dat.analytic$RIDEXPRG)\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\nnrow(dat.analytic)\n\n# Drop irrelevant variables\ndat.analytic$RIDAGEYR &lt;- dat.analytic$BMXBMI &lt;- dat.analytic$RIDEXPRG &lt;- NULL\n\n1(f) Complete case data\nBelow is the code for creating the complete case dataset (no missing for the outcome or predictors):\n\n# Drop missing values\ndat.complete &lt;- dat.analytic[complete.cases(dat.analytic),] # N = 5,433\n\n1(g) Save daatsets\n\nsave(dat.full, dat.full2, dat.analytic, dat.complete, file = \"Data/machinelearning/Flegal2016_v2.RData\")",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-2-importing-data-and-creating-table-1",
    "href": "machinelearningEsolution.html#question-2-importing-data-and-creating-table-1",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 2: Importing data and creating Table 1",
    "text": "Question 2: Importing data and creating Table 1\n2(a) Importing dataset\nLet‚Äôs load the dataset:\n\nload(\"Data/machinelearning/Flegal2016_v2.RData\")\nls()\n#&gt; [1] \"dat.analytic\" \"dat.complete\" \"dat.full\"     \"dat.full2\"\n\nHere,\n\ndat.full: the full dataset with all variables\ndat.full2: the full dataset with only relevant variables for this exercise\ndat.analytic: the analytic dataset with only adults aged 20 years and more, non-missing BMI, and non-pregnant\ndat.complete: the complete case dataset without missing values in the outcome and predictors\n\n\nnames(dat.full2)\n#&gt;  [1] \"SEQN\"                \"survey.weight\"       \"psu\"                \n#&gt;  [4] \"strata\"              \"RIDAGEYR\"            \"BMXBMI\"             \n#&gt;  [7] \"RIDEXPRG\"            \"obesity\"             \"age.cat\"            \n#&gt; [10] \"gender\"              \"race\"                \"education\"          \n#&gt; [13] \"smoking\"             \"physical.activity\"   \"sleep\"              \n#&gt; [16] \"high.blood.pressure\" \"general.health\"      \"wgt\"\n\n2(b) Creating Table 1\nLet‚Äôs create Table 1 for the complete case dataset with unweighted frequencies:\n\nlibrary(tableone)\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \n                \"physical.activity\", \"sleep\", \"high.blood.pressure\", \n                \"general.health\")\ntab1 &lt;- CreateTableOne(vars = predictors, strata = \"obesity\", \n                       data = dat.complete, test = F, addOverall = T)\nprint(tab1, format=\"f\") # Showing only frequencies \n#&gt;                            Stratified by obesity\n#&gt;                             Overall 0    1  \n#&gt;   n                         5433    5023 410\n#&gt;   age.cat                                   \n#&gt;      [20,40)                1806    1659 147\n#&gt;      [40,60)                1892    1728 164\n#&gt;      [60,Inf)               1735    1636  99\n#&gt;   gender = Female           2807    2531 276\n#&gt;   race                                      \n#&gt;      White                  2333    2157 176\n#&gt;      Black                  1109     976 133\n#&gt;      Hispanic               1208    1125  83\n#&gt;      Other                   783     765  18\n#&gt;   education                                 \n#&gt;      &lt;High school           1171    1092  79\n#&gt;      High school            1216    1116 100\n#&gt;      &gt;High school           3046    2815 231\n#&gt;   smoking                                   \n#&gt;      Never smoker           3054    2828 226\n#&gt;      Former smoker          1261    1159 102\n#&gt;      Current smoker         1118    1036  82\n#&gt;   physical.activity = Yes    984     917  67\n#&gt;   sleep                                     \n#&gt;      7-9                    3174    2971 203\n#&gt;      Less than 7            2084    1896 188\n#&gt;      More than 9             175     156  19\n#&gt;   high.blood.pressure = Yes 2037    1810 227\n#&gt;   general.health                            \n#&gt;      Poor or Fair           1260    1084 176\n#&gt;      Good                   2065    1911 154\n#&gt;      Very good or Excellent 2108    2028  80",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-3-prediction-using-split-sample-approach",
    "href": "machinelearningEsolution.html#question-3-prediction-using-split-sample-approach",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 3: Prediction using split sample approach",
    "text": "Question 3: Prediction using split sample approach\nIn this exercise, we will use the split-sample approach to predict obesity. We will create our training and test data using a 60-40 split for the training and test data. We will use the following two methods to predict obesity:\n\nDesign-adjusted logistic with all survey features (psu, strata, and survey weights)\nLASSO with survey weights\n\n3(a) Split the data into training and test\nLet us create our training and test data using the split-sample approach:\n\nset.seed(900)\ndat.complete$datasplit &lt;- rbinom(nrow(dat.complete), size = 1, prob = 0.6) \ntable(dat.complete$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2130 3303\n\n# Training data\ndat.train &lt;- dat.complete[dat.complete$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 3303   16\n\n# Test data\ndat.test &lt;- dat.complete[dat.complete$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2130   16\n\n3(b) Prediction with design-adjusted logistic\nWe will use the design-adjusted logistic regression to predict obesity with the following predictors:\n\nage.cat, gender, race, education, smoking, physical.activity, sleep, high.blood.pressure, general.health\n\nInstructions:\n\n1: Create the survey design on the full data and subset the design for those individuals in the training data.\n2: Use the training data design created in step 1 to fit the model\n3: Use the test data to predict the probability of obesity.\n4: Calculate AUC on the test data.\n5: Calculate calibration slope with 95% confidence interval on the test data.\n\nHints:\n\n\nWeightedAUC and WeightedROC are helpful functions in calculating AUC.\nThe Logit function from the DescTools package is helpful in calculating the logit of predicted probabilities for calculating calibration slope.\nUse the normalized weight variable to calculate the AUC and calibration slope.\n\nModel\n\nlibrary(survey)\nlibrary(DescTools)\nlibrary(WeightedROC)\nlibrary(Publish)\n\n# Design\ndat.full2$miss &lt;- 1\ndat.full2$miss[dat.full2$SEQN %in% dat.train$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full2, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Formula\nFormula &lt;- formula(paste(\"obesity ~ \", paste(predictors, collapse=\" + \")))\n\n# Model\nfit.glm &lt;- svyglm(Formula, design = svy.design, family = binomial)\n\n# Prediction on the test set\ndat.test$pred.glm &lt;- predict(fit.glm, newdata = dat.test, type = \"response\")\n\n# AUC on the test set with sampling weights\nauc.glm &lt;- WeightedAUC(WeightedROC(dat.test$pred.glm, dat.test$obesity, \n                                   weight = dat.test$wgt))\nauc.glm\n#&gt; [1] 0.6813118\n\n# Weighted calibration slope\ndat.test$pred.glm.logit &lt;- DescTools::Logit(dat.test$pred.glm)\nslope.glm &lt;- glm(obesity ~ pred.glm.logit, data = dat.test, family = quasibinomial,\n                 weights = wgt)\npublish(slope.glm)\n#&gt;        Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.glm.logit              0.74 [0.56;0.92] &lt; 1e-04\n\nInterpretation [optional]\nInterpret the AUC and calibration slope based on the following criteria:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting\n\n\n3(c) Prediction with LASSO\nNow we will use the LASSO method to predict obesity. We will incorporate sampling weights in the model to account for survey data (no psu or strata). Note that we are not interested in the statistical significance of the beta coefficients. Hence, not utilizing psu and strata should not be an issue in this prediction problem.\nInstructions:\n\n1: Use the training data with normalized weight to fit the model.\n2: Find the optimum lambda using 5-fold cross-validation. Consider the lambda value that gives the minimum prediction error.\n3: Predict the probability of obesity on the test set\n3: Calculate AUC on the test data.\n4: Calculate calibration slope with 95% confidence interval on the test data.\n\nModel\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$obesity) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$obesity)\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                          family = \"binomial\", weights = dat.train$wgt)\n\n# Prediction on the test set\ndat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                               s = fit.cv.lasso$lambda.min)\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, dat.test$obesity, \n                                     weight = dat.test$wgt))\nauc.lasso\n#&gt; [1] 0.6875668\n\n# Weighted calibration slope\ndat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\nslope.lasso &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, \n                   family = quasibinomial, weights = wgt)\npublish(slope.lasso)\n#&gt;          Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.lasso.logit              0.88 [0.67;1.08] &lt; 1e-04\n\nInterpretation [optional]\nInterpret the AUC and calibration slope based on the following criteria:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-4-prediction-using-croos-validation-approach-optional",
    "href": "machinelearningEsolution.html#question-4-prediction-using-croos-validation-approach-optional",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 4: Prediction using croos-validation approach [optional]",
    "text": "Question 4: Prediction using croos-validation approach [optional]\nUse LASSO with 5-fold cross-validation to predict obesity with the same set of predictors used in Question 2. Report the average AUC and average calibration slope with 95% confidence interval over 5 folds.\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, size = nrow(dat.complete), replace = T)\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1090 1074 1130 1083 1056\n\nauc.lasso &lt;- cal.slope.lasso &lt;- cal.slope.se.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$obesity)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$obesity)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                            family = \"binomial\", weights = dat.train$wgt)\n\n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                                 s = fit.cv.lasso$lambda.min)\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso,dat.test$obesity, \n                                             weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\n  mod.cal &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, family = binomial, \n                 weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  cal.slope.se.lasso[fold] &lt;- summary(mod.cal)$coef[2,2]\n}\n\n# Average AUC\nmean(auc.lasso)\n#&gt; [1] 0.7055226\n\n# Average calibration slope\nmean(cal.slope.lasso)\n#&gt; [1] 0.9994\n\n# 95% CI for calibration slope\ncbind(mean(cal.slope.lasso) - 1.96 * mean(cal.slope.se.lasso), \n      mean(cal.slope.lasso) + 1.96 * mean(cal.slope.se.lasso))\n#&gt;           [,1]     [,2]\n#&gt; [1,] 0.7299176 1.268882",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningCausal.html",
    "href": "machinelearningCausal.html",
    "title": "ML in causal inference",
    "section": "",
    "text": "Background\nThis chapter provides a detailed exploration of Targeted Maximum Likelihood Estimation (TMLE) in causal inference. The first tutorial motivates the use of TMLE by highlighting its advantages over traditional methods, focusing on the limitations of these approaches and introducing the RHC dataset. The second tutorial delves into the SuperLearner method for ensemble modeling, discussing the importance of algorithm diversity, cross-validation, and adaptable libraries. The third tutorial offers a comprehensive guide to applying TMLE for binary outcomes, emphasizing diverse SuperLearner libraries, effective sample sizes, and candidate learner selection. The fourth tutorial extends TMLE to continuous outcomes, covering transformations, interpretation, and comparisons with default TMLE libraries and traditional regression. These tutorials should equip readers with a robust understanding of TMLE and its practical applications in causal inference and epidemiology.",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal.html#background",
    "href": "machinelearningCausal.html#background",
    "title": "ML in causal inference",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal.html#overview-of-tutorials",
    "href": "machinelearningCausal.html#overview-of-tutorials",
    "title": "ML in causal inference",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have developed a solid understanding of predictive modeling, data splitting/cross-validation, propensity scores and various machine learning algorithms. These insights have prepared us for the advanced causal inference techniques we will encounter in this chapter. As we explore TMLE and SuperLearner, we will draw upon our knowledge of propensity score and machine learning to unlock the potential of their use in building powerful causal inference tools.\n\nMotivation for learning and using TMLE\nThis tutorial discusses the motivation for using the TMLE method in causal inference. It highlights the limitations of traditional methods such as propensity score approaches and direct application of machine learning in terms of assumptions and statistical inference. TMLE is presented as a doubly robust method that incorporates machine learning while allowing straightforward statistical inference. The tutorial reintroduces RHC data for demonstration.\n\n\nUnderstanding SuperLearner\nThis tutorial focuses on using the SuperLearner method for ensemble modeling. SuperLearner is a type 2 ensemble method that combines various predictive algorithms to create a robust predictive model. It employs cross-validation to determine the best-weighted combination of algorithms, based on specified predictive performance metrics. The tutorial provides guidelines for selecting a diverse set of algorithms, considering computational feasibility, and adapting the library of algorithms based on sample characteristics. Examples of different types of learners that can be included in the SuperLearner library are provided, ranging from parametric to highly data-adaptive and non-linear models. The tutorial also mentions the default libraries for estimating outcomes and propensity scores in the context of TMLE.\n\n\nDealing with binary outcomes within TMLE framework\nThis tutorial is a comprehensive guide to applying TMLE for binary outcomes. It covers the entire TMLE process, from constructing initial outcome and exposure models to targeted adjustment via propensity scores and treatment effect estimation. It emphasizes the importance of specifying a diverse SuperLearner library for both exposure and outcome models, determining effective sample sizes, and selecting candidate learners. The tutorial demonstrates TMLE application using the tmle package and includes a thorough comparison with default SuperLearner libraries and traditional regression, presenting estimates, confidence intervals, and a comparative table of results.\n\n\nDealing with continuous outcomes within TMLE framework\nThis tutorial offers comprehensive guidance on implementing TMLE for continuous outcomes, including transformations and result interpretation. It introduces the application of TMLE for continuous outcomes, emphasizing the process of constructing a SuperLearner and key considerations such as effective sample size and learner selection. Notably, it highlights the essential transformation of continuous outcome variables to a standardized range (0 to 1) using min-max normalization before applying TMLE with a Gaussian family. The tutorial demonstrates the post-TMLE rescaling of treatment effect estimates and confidence intervals to the original scale and offers a comparative analysis with default TMLE libraries and traditional regression.\n\n\nComparing results\nIn this comprehensive tutorial, various statistical methods were applied to investigate the association between RHC and death using the RHC dataset. These methods included logistic regression, propensity score matching and weighting with both logistic regression and Super Learner, as well as TMLE. The results consistently indicated that participants with RHC use had higher odds of death compared to those without RHC use, with odds ratios ranging similarly across different modeling approaches, but also showing a trend when machine learners are incorporated.\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough of the additional resources, feel free to watch the video below.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html",
    "href": "machinelearningCausal0.html",
    "title": "Concepts (C)",
    "section": "",
    "text": "ML in causal inference\nIn comparative effectiveness studies, researchers typically use propensity score methods. However, propensity score methods have known limitations in real-world scenarios, when the true data generating mechanism is unknown. Targeted maximum likelihood estimation (TMLE) is an alternative estimation method with a number of desirable statistical properties. It is a doubly robust method, enabling the integration of machine learning approaches within the framework. Despite the fact that this method has been shown to perform better in terms of statistical properties (e.g., variance estimation) than propensity score methods in a variety of scenarios, it is not widely used in medical research as the implementation details of this approach are generally not well understood. In this section, we will explain this method in details.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#reading-list",
    "href": "machinelearningCausal0.html#reading-list",
    "title": "Concepts (C)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Karim and Frank 2021)\nOptional reading: (Frank and Karim 2023)",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#video-lessons",
    "href": "machinelearningCausal0.html#video-lessons",
    "title": "Concepts (C)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning\n\n\n\nThe workshop was first developed for R/Medicine Virtual Conference https://r-medicine.org/ 2021, August 24th. What is included in this Video Lesson/workshop:\n\nChapter 1 RHC data description 4:22\nChapter 2 G-computation 23:13\nChapter 3 G-computation using ML 45:02\nChapter 4 IPTW 1:18:50\nChapter 5 IPTW using ML 1:30:11\nChapter 6 TMLE 1:36:41\nChapter 7 Pre-packaged software 1:58:05\nChapter 8 Final Words 2:14:36\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#links",
    "href": "machinelearningCausal0.html#links",
    "title": "Concepts (C)",
    "section": "Links",
    "text": "Links\nThe original workshop materials are available here.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#references",
    "href": "machinelearningCausal0.html#references",
    "title": "Concepts (C)",
    "section": "References",
    "text": "References\n\n\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. ‚ÄúImplementing TMLE in the Presence of a Continuous Outcome.‚Äù Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nKarim, Ehsan, and Hanna Frank. 2021. ‚Äúehsanx/TMLEworkshop: R Guide for TMLE in Medical Research.‚Äù Zenodo. https://doi.org/10.5281/zenodo.5246085.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal1.html",
    "href": "machinelearningCausal1.html",
    "title": "Motivation",
    "section": "",
    "text": "When using methods like propensity score approaches, we are making assumptions about the model specification. For example, we must specify any interaction terms.\nWith machine learning methods, these assumptions can be relaxed somewhat, as some machine learning methods allow automatic detection of data structures such as interactions.\nHowever, machine learning was created for prediction modeling, not with causal inference in mind. Statistical inference such as calculating standard errors and confidence intervals is not as straightforward since the estimator given by machine learning methods does not follow a known statistical distribution. By contrast, the estimators resulting from a standard regression using maximum likelihood estimation will follow an approximately normal distribution, where it is easy to calculate standard errors and confidence intervals.\n\nTargeted maximum likelihood estimation (TMLE) is a causal inference method that can incorporate machine learning in a way that still allows straightforward statistical inference based on theoretical development grounded in semi-parametric theory.\n\nTMLE is a doubly robust method. This means it uses both the exposure (AKA propensity score) model and the outcome model. As long as one of these models is correctly specified, TMLE will give a consistent estimator, meaning it gets closer and closer to the true value as the sample size increases.\nSince TMLE uses both the exposure and the outcome model, machine learning can be used in each of these intermediary modeling steps while allowing straightforward statistical inference.\n\n\nIt has been shown that TMLE outperform singly robust methods with machine learning, such as IPTW.\n\nRevisiting RHC Data\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial uses the same data as some of the previous tutorials, including working with a predictive question, machine learning with a continuous outocome, and machine learning with a binary outcome.\n\n\n\nObsData &lt;- readRDS(file = \n                     \"Data/machinelearningCausal/rhcAnalyticTest.RDS\")\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(RHC.use,Length.of.Stay,Death)))\nhead(ObsData)\n\n\n  \n\n\n\nTable 1\nOnly for some demographic and comorbidity variables; matches with Table 1 in Connors et al. (1996).\n\ntab0 &lt;- CreateTableOne(vars = c(\"age\", \"sex\", \"race\", \n                                \"Disease.category\", \"Cancer\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab0, showAllLevels = FALSE)\n#&gt;                       Stratified by RHC.use\n#&gt;                        0            1           \n#&gt;   n                    3551         2184        \n#&gt;   age (%)                                       \n#&gt;      [-Inf,50)          884 (24.9)   540 (24.7) \n#&gt;      [50,60)            546 (15.4)   371 (17.0) \n#&gt;      [60,70)            812 (22.9)   577 (26.4) \n#&gt;      [70,80)            809 (22.8)   529 (24.2) \n#&gt;      [80, Inf)          500 (14.1)   167 ( 7.6) \n#&gt;   sex = Female (%)     1637 (46.1)   906 (41.5) \n#&gt;   race (%)                                      \n#&gt;      white             2753 (77.5)  1707 (78.2) \n#&gt;      black              585 (16.5)   335 (15.3) \n#&gt;      other              213 ( 6.0)   142 ( 6.5) \n#&gt;   Disease.category (%)                          \n#&gt;      ARF               1581 (44.5)   909 (41.6) \n#&gt;      CHF                247 ( 7.0)   209 ( 9.6) \n#&gt;      Other              955 (26.9)   208 ( 9.5) \n#&gt;      MOSF               768 (21.6)   858 (39.3) \n#&gt;   Cancer (%)                                    \n#&gt;      None              2652 (74.7)  1727 (79.1) \n#&gt;      Localized (Yes)    638 (18.0)   334 (15.3) \n#&gt;      Metastatic         261 ( 7.4)   123 ( 5.6)\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. ‚ÄúThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.‚Äù Jama 276 (11): 889‚Äì97. https://tinyurl.com/Connors1996.",
    "crumbs": [
      "ML in causal inference",
      "Motivation"
    ]
  },
  {
    "objectID": "machinelearningCausal2.html",
    "href": "machinelearningCausal2.html",
    "title": "SuperLearner",
    "section": "",
    "text": "Choosing Learners\nSuperLearner is a type 2 ensemble method, meaning it combines many methods of different types into one predictive model. SuperLearner uses cross-validation to find the best weighted combination of algorithms based on the predictive performance measure specified (default in the SuperLearner package is non-negative least squares based on the Lawson-Hanson algorithm (Mullen and Stokkum 2023), but measures such as AUC can also be used). To run SuperLearner, the user needs to specify a library consisting of all the different methods SuperLearner should incorporate in the final model, as well as the number of cross-validation folds.\n\n\nSee previous chapter for other types of ensemble learning methods.\nSuperLearner will perform as well as possible given the library of algorithms considered. A very recent paper by Phillips et al. (2023) provides some concrete guidelines for the determination of the number of cross-validation folds necessary and the selection of algorithms to include. Overall, we want to make sure the set of algorithms provided is:\n\n\nDiverse: Having a rich library of algorithms allows the SuperLearner to adapt to a range of underlying data structures. Diverse libraries include:\n\nParametric learners such as generalized linear models (GLMs)\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\n\n\nComputationally feasible: Lots of machine learning algorithms take a long time to run. Having multiple computationally intensive algorithms in your library will cause the SuperLearner as a whole to take much too long to run.\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the more specific guidelines depend on our effective sample size. For binary outcomes, this can be calculated as:\n\\[ n_{eff}=min(n, 5*(n*min(\\bar{p},1-\\bar{p})))   \\]\nwhere \\(\\bar{p}\\): prevalence of the outcome.\nFor continuous outcomes, the effective sample size is the same as the sample size (\\(n_{eff} = n\\)).\n\n\nWe also want to consider the characteristics of our particular sample.\n\nIf there are continuous covariates: We should include learners that do not force relationships to be linear/monotonic. For example, we could include regression splines, support vector machines, and tree-based methods like regression trees.\nIf we have high-dimensional data (a large number of covariates e.g.¬†more than \\(n_{eff}/20\\) ): We should include some learners that fall under the class of screeners. These are learners which incorporate dimension reduction such as LASSO and random forests.\nIf the sample size is very large (i.e.¬†\\(n_{eff}&gt;500\\) ): We should include as many learners as is computationally feasible.\nIf the sample size is small (i.e.¬†\\(n_{eff} \\leq 500\\) ): We should include fewer learners (e.g.¬†up to \\(n_{eff}/5\\) ), and include less flexible learners.\n\nSome examples of learners that could be included are given in the table below (Polley 2021):\n\n\n\n\n\n\nType of learner\nExamples\n\n\n\nParametric\n\nSL.mean: simple mean\nSL.glm: generalized linear models\nSL.lm: ordinary least squares\nSL.speedglm: fast version of glm\nSL.speedlm: fast version of lm\nSL.gam: generalized additive methods\nSL.step: choose model based on AIC (backwards or forwards or both)\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression using elastic net (ridge regression and Lasso)\n\nKernel-based methods\n\nSL.kernelKnn: k-nearest neighbours\nSL.ksvm: kernel-based support vector machine\n\n\nSL.xgboost: extreme gradient boosting\nSL.gbm: gradient-boosted machines\nSL.nnet: neural networks\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.earth: multivariate adaptive regression splines\n\nTree-based methods\n\nSL.randomForest: random forests\ntmle.SL.dbarts2: bayesian additive regression trees\nSL.cforest: random forests using conditional inference trees\nSL.ranger: fast implementation of random forest suited for high dimensional data\n\n\nSL.svm: support vector machines\n\n\n\nScreeners\n\nscreen.corP: retain covariates with correlation with outcome p-value &lt;0.1\nscreen.corRank: retain top j covariates with highest correlation with outcome\nscreen.glmnet: Lasso\nscreen.randomForest: random forests\nscreen.SIS: retain covariates based on distance correlation\n\n\n\n\nThere is also a useful tool implemented in the SuperLearner library which allows us to easily see a list of all available learners.\n\nSuperLearner::listWrappers()\n#&gt; All prediction algorithm wrappers in SuperLearner:\n#&gt;  [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n#&gt;  [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n#&gt;  [7] \"SL.earth\"            \"SL.gam\"              \"SL.gbm\"             \n#&gt; [10] \"SL.glm\"              \"SL.glm.interaction\"  \"SL.glmnet\"          \n#&gt; [13] \"SL.ipredbagg\"        \"SL.kernelKnn\"        \"SL.knn\"             \n#&gt; [16] \"SL.ksvm\"             \"SL.lda\"              \"SL.leekasso\"        \n#&gt; [19] \"SL.lm\"               \"SL.loess\"            \"SL.logreg\"          \n#&gt; [22] \"SL.mean\"             \"SL.nnet\"             \"SL.nnls\"            \n#&gt; [25] \"SL.polymars\"         \"SL.qda\"              \"SL.randomForest\"    \n#&gt; [28] \"SL.ranger\"           \"SL.ridge\"            \"SL.rpart\"           \n#&gt; [31] \"SL.rpartPrune\"       \"SL.speedglm\"         \"SL.speedlm\"         \n#&gt; [34] \"SL.step\"             \"SL.step.forward\"     \"SL.step.interaction\"\n#&gt; [37] \"SL.stepAIC\"          \"SL.svm\"              \"SL.template\"        \n#&gt; [40] \"SL.xgboost\"\n#&gt; \n#&gt; All screening algorithm wrappers in SuperLearner:\n#&gt; [1] \"All\"\n#&gt; [1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n#&gt; [4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n#&gt; [7] \"screen.ttest\"          \"write.screen.template\"\n\nSuperLearner in TMLE\n\n\nThe default SuperLearner library for estimating the outcome includes (Gruber, Van Der Laan, and Kennedy 2020)\n\n\nSL.glm: generalized linear models (GLMs)\n\nSL.glmnet: least absolute shrinkage and selection operator (LASSO)\n\ntmle.SL.dbarts2: modeling and prediction using Bayesian Additive Regression Trees (BART)\n\n\n\nThe default library for estimating the propensity scores includes\n\n\nSL.glm: generalized linear models (GLMs)\n\ntmle.SL.dbarts.k.5: SL wrappers for modeling and prediction using BART\n\nSL.gam: generalized additive models (GAMs)\n\n\n\nIt is certainly possible to use different set of learners\n\nMore methods can be added by\n\nspecifying lists of models in the Q.SL.library (for the outcome model) and g.SL.library (for the propensity score model)\n\n\n\n\nReferences\n\n\n\n\nGruber, S., M. Van Der Laan, and C. Kennedy. 2020. Package ‚ÄôTmle‚Äô. https://cran.r-project.org/web/packages/tmle/tmle.pdf.\n\n\nMullen, Katharine M., and Ivo H. M. van Stokkum. 2023. Package ‚ÄôNnls‚Äô. https://cran.r-project.org/web/packages/nnls/nnls.pdf.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. ‚ÄúPractical Considerations for Specifying a Super Learner.‚Äù International Journal of Epidemiology 52: 1276‚Äì85. https://doi.org/10.1093/ije/dyad023.\n\n\nPolley, Eric. 2021. Package ‚ÄôSuperLearner‚Äô. https://cran.r-project.org/web/packages/SuperLearner/SuperLearner.pdf.",
    "crumbs": [
      "ML in causal inference",
      "SuperLearner"
    ]
  },
  {
    "objectID": "machinelearningCausal3.html",
    "href": "machinelearningCausal3.html",
    "title": "Binary Outcomes",
    "section": "",
    "text": "For this example we will be looking at the binary outcome variable death.\n\n# Data\nload(file = \"Data/machinelearningCausal/cl2.RData\")\n\n# Table 1\ntab1 &lt;- CreateTableOne(vars = c(\"Death\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#&gt;                    Stratified by RHC.use\n#&gt;                     0           1          \n#&gt;   n                 3551        2184       \n#&gt;   Death (mean (SD)) 0.63 (0.48) 0.68 (0.47)\n\nTMLE\nTMLE works by first constructing an initial outcome and extracting a crude estimate of the treatment effect. Then, TMLE aims to refine the initial estimate in the direction of the true value of the parameter of interest through use of the exposure model.\nTo label the treatment effect given by TMLE as causal, the same conditional exchangeability, positivity, and consistency assumptions must be met as for other modeling strategies (see introduction to propensity score). TMLE also assumes that at least one of the exposure or outcome model is correctly specified. If this does not hold, TMLE does not necessarily produce a consistent estimator.\n\n\nLuque-Fernandez et al. (2018) discussed the implementation of TMLE, and providing a detailed step-by-step guide, primarily focusing on a binary outcome.\nThe basic steps are:\n\nConstruct initial outcome model & get crude estimate\nConstruct exposure model and use propensity scores to update the initial outcome model through a targeted adjustment\nExtract treatment effect estimate\nEstimate confidence interval based on a closed-form formula\n\nThe tmle package implements TMLE for both binary and continuous outcomes, and uses the SuperLearner to construct the exposure and outcome models.\nThe tmle method takes a number of parameters, including:\n\n\nTerm\nDescription\n\n\n\nY\nOutcome vector\n\n\nA\nExposure vector\n\n\nW\nMatrix that includes vectors of all covariates\n\n\nfamily\nDistribution\n\n\nV\nCross-validation folds for exposure and outcome modeling\n\n\nQ.SL.library\nSet of machine learning methods to use for SuperLearner for outcome modeling\n\n\ng.SL.library\nSet of machine learning methods to use for SuperLearner for exposure modeling\n\n\nConstructing SuperLearner\nWe will need to specify two SuperLearners, one for the exposure and one for the outcome model. We will need to consider the characteristics of our sample in order to decide the number of cross-validation folds and construct a diverse and computationally feasible library of algorithms.\nNumber of folds\nFirst, we need to define the number of cross-validation folds to use for each model. This depends on our effective sample size (Phillips et al. 2023).\nOur effective sample size for the outcome model is:\n\nn &lt;- nrow(ObsData) \np &lt;- nrow(ObsData[ObsData$Death == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(p, 1-p))) \nn_eff\n#&gt; [1] 5735\n\nOur effective sample size for the exposure model is:\n\np_exp &lt;- nrow(ObsData[ObsData$RHC.use == 1,])/n \nn_eff_exp &lt;- min(n, 5*(n*min(p, 1-p))) \nn_eff_exp\n#&gt; [1] 5735\n\nFor both models, the effective sample size is the same as our sample size, \\(n = 5,735\\).\nSince \\(5,000 \\leq n_{eff} \\leq 10,000\\), we should use 5 or more cross-validation folds according to Phillips et al. (2023). For the sake of computational feasibility, we will use 5 folds in this example.\nCandidate learners\nThe second step is to define the library of learners we will feed in to SuperLearner as potential options for each model (exposure and outcome). In this example, some of our covariates are continuous variables, such as temperature and blood pressure, so we need to include learners that allow non-linear/monotonic relationships.\nSince \\(n\\) is large (\\(&gt;5000\\)), we should include as many learners as is computationally feasible in our libraries.\nFurthermore, we have 50 covariates:\n\nlength(c(baselinevars, \"Length.of.Stay\"))\n#&gt; [1] 50\n\n\\(5735/20 = 286.75\\), and \\(50&lt;286.75\\), so we do not have high-dimensional data and including screeners is optional (Phillips et al. 2023).\nSince the requirements for the exposure and outcome models are the same in this example, we will use the same SuperLearner library for both. Overall for this example we need to make sure to include:\n\nParametric learners\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\nLearners that allow non-linear/monotonic relationships\n\nFor this example, we will include the following learners:\n\n\nParametric\n\nSL.mean: only mean\nSL.glm: generalized linear model\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression such as lasso\nSL.xgboost: extreme gradient boosting\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.randomForest: random forest\ntmle.SL.dbarts2: bayesian additive regression tree\nSL.svm: support vector machine\n\n\n\n\n# Construct the SuperLearner library\nSL.library &lt;- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nTMLE with SuperLearner\nTo run TMLE, we need to install the tmle package and load it on the R environment.\n\n#install.packages(c('tmle', 'xgboost'))\nrequire(tmle)\nrequire(xgboost)\n\nWe also need to create a data frame containing only the covariates:\n\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Death, RHC.use))\nObsData$Death &lt;- as.numeric(ObsData$Death)\n\nThen we can run TMLE using the tmle method from the tmle package:\n\nset.seed(1444) \n\ntmle.fit &lt;- tmle::tmle(Y = ObsData$Death, \n                   A = ObsData$RHC.use, \n                   W = ObsData.noYA, \n                   family = \"binomial\", \n                   V.Q = 5,\n                   V.g = 5,\n                   Q.SL.library = SL.library, \n                   g.SL.library = SL.library)\n\ntmle.est.bin &lt;- tmle.fit$estimates$OR$psi\ntmle.ci.bin &lt;- tmle.fit$estimates$OR$CI\n\nATE for binary outcome using user-specified library: 1.29 and 95% CI is 1.2011697, 1.3878896\nThese results show those who received RHC had odds of death that were 1.29 times as high as the odds of death in those who did not receive RHC.\nUnderstanding defaults\nWe can compare the results using our specified SuperLearner library to the results we would get when using the tmle package‚Äôs default SuperLearner libraries. To do this we simply do not specify libraries for the Q.SL.library and g.SL.library arguments.\n\n# small test library \n# with only glm just \n# for sake of making this work\nSL.library.test &lt;- c(\"SL.glm\")\n\n\nset.seed(1444) \n\ntmle.fit.def &lt;- tmle::tmle(Y = ObsData$Death, \n                           A = ObsData$RHC.use, \n                           W = ObsData.noYA, \n                           family = \"binomial\", \n                           V.Q = 5,\n                           V.g = 5)\n# Q.SL.library = SL.library.test,  ## removed this line\n# g.SL.library = SL.library.test)  ## removed this line\n\ntmle.est.bin.def &lt;- tmle.fit.def$estimates$OR$psi\ntmle.ci.bin.def &lt;- tmle.fit.def$estimates$OR$CI\n\nATE for binary outcome using default library: 1.32 with 95% CI 1.1808107, 1.4657624.\nThese ATE when using the default SuperLearner library (1.31) is very close to the ATE when using our user-specified SuperLearner library (1.29). However, the confidence interval from TMLE using the default SuperLearner library (1.17, 1.46) is slightly wider than the confidence interval from TMLE using our user-specified SuperLearner library (1.20, 1.39).\nComparison of results\nWe can also compare these results to those from a basic regression and from the literature.\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.Death &lt;- c(baselinevars, \"Length.of.Stay\")\nout.formula.bin &lt;- as.formula(\n  paste(\"Death~ RHC.use +\",\n        paste(baselineVars.Death, \n              collapse = \"+\")))\nfit1.bin &lt;- glm(out.formula.bin, data = ObsData, family=\"binomial\")\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 4 showed that, after propensity score pair (1-to-1) matching, the odds of in-hospital mortality were 39% higher in those who received RHC (OR: 1.39 (1.15, 1.67)).\n\n\n\n\nmethod.list\nEstimate\n2.5 %\n97.5 %\n\n\n\nAdjusted Regression\n0.36\n0.22\n0.51\n\n\nTMLE (user-specified SL library)\n1.29\n1.20\n1.39\n\n\nTMLE (default SL library)\n1.32\n1.18\n1.47\n\n\nConnors et al. (1996) paper\n1.39\n1.15\n1.67\n\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. ‚ÄúThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.‚Äù Jama 276 (11): 889‚Äì97. https://tinyurl.com/Connors1996.\n\n\nLuque-Fernandez, Miguel Angel, Michael Schomaker, Bernard Rachet, and Mireille E Schnitzer. 2018. ‚ÄúTargeted Maximum Likelihood Estimation for a Binary Treatment: A Tutorial.‚Äù Statistics in Medicine 37 (16): 2530‚Äì46.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. ‚ÄúPractical Considerations for Specifying a Super Learner.‚Äù International Journal of Epidemiology 52: 1276‚Äì85. https://doi.org/10.1093/ije/dyad023.",
    "crumbs": [
      "ML in causal inference",
      "Binary Outcomes"
    ]
  },
  {
    "objectID": "machinelearningCausal4.html",
    "href": "machinelearningCausal4.html",
    "title": "Continuous Outcomes",
    "section": "",
    "text": "We will now go through an example of using TMLE for a continuous outcome. The setup for SuperLearner in this case is similar to that for binary outcomes, so rather than going through the SuperLearner steps again, we will instead focus on the additional steps that are necessary for running the tmle method on continuous outcomes.\n\n\nFrank and Karim (2023) extensively discussed the implementation of TMLE for continuous outcomes, providing a detailed step-by-step guide using the openly accessible RHC dataset. In this tutorial, we will revisit the same example with additional explanations.\n\n\n\n\n\n\nNote\n\n\n\nOnly outcome variable (Length of stay); slightly different than Table 2 in Connors et al. (1996) (means were 20.5 vs.¬†25.7; and medians were 16 vs.¬†17).\n\n\n\ntab1 &lt;- CreateTableOne(vars = c(\"Length.of.Stay\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#&gt;                             Stratified by RHC.use\n#&gt;                              0             1            \n#&gt;   n                           3551          2184        \n#&gt;   Length.of.Stay (mean (SD)) 19.53 (23.59) 24.86 (28.90)\n\n\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==0])\n#&gt; [1] 12\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==1])\n#&gt; [1] 16\n\nConstructing SuperLearner\nJust as we did for a binary outcome, we will need to specify two SuperLearners, one for the exposure and one for the outcome model.\nThe effective sample size for a continuous outcome is just \\(n_{eff}=n=5,735\\). We calculated the effective sample size for the exposure model earlier, which also turned out to be \\(n_{eff}=n=5,735\\). So once again we will use 5 folds because \\(5,000 \\leq n_{eff} \\leq 10,000\\) (Phillips et al. 2023).\nSimilarly to our example with the binary outcome, the key considerations for the library of learners are:\n\nWe have some continuous covariates, and should therefore include learners that allow non-linear/monotonic relationships.\nWe have a large \\(n\\), so should include as many learners as is computationally feasible.\nWe have 49 covariates and 5,735 observations, so we do not have high-dimensional data and including screeners is optional.\n\nAgain the requirements for the exposure and outcome models are the same and we can use the same library for both models. Note that even though one model will have a binary dependent variable, and one will have a continuous dependent variable, most of the available learners automatically adapt to binary and continuous dependent variables.\nFor this example, we will use the same SuperLearner library as for the binary outcome example.\n\n# Construct the SuperLearner library\nSL.library &lt;- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nDealing with continuous outcomes\nFor this example, we will be examining the length of stay in hospital outcome.\nThe key difference between running TMLE on a continuous outcome in comparison to running it with a binary outcome, is that we must transform the outcome to fall within the range of 0 to 1, so that the modeled outcomes fall within the range of the outcome‚Äôs true distribution (Gruber and Laan 2010).\nTo transform the outcome, we can use min-max normalization:\n\\[\nY_{transformed} = \\frac{Y-Y_{min}}{Y_{max}-Y_{min}}\n\\]\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y &lt;- min(ObsData$Length.of.Stay)\nmax.Y &lt;- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf &lt;- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nOnce we have transformed the outcome to fall within the range of 0 to 1, we can run TMLE as before, using the tmle method in the tmle package:\n\n# create data frame containing only covariates\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont &lt;- tmle::tmle(Y = ObsData$Length.of.Stay_transf, \n                       A = ObsData$RHC.use, \n                       W = ObsData.noYA, \n                       family = \"gaussian\", \n                       V.Q = 5,\n                       V.g = 5,\n                       Q.SL.library = SL.library,\n                       g.SL.library = SL.library)\n\nOnce the tmle method has run, we still have one step to complete to get our final estimate. At this point, we must transform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome‚Äôs original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n# transform back the ATE estimate\ntmle.est.cont &lt;- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$psi\ntmle.est.cont\n#&gt; [1] 2.939622\n\nWe also have to transform the confidence interval back to the original scale:\n\ntmle.ci.cont &lt;- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$CI\n\nATE for continuous outcome: 2.9396218, and 95 % CI is 1.959698, 3.9195455.\nThe results indicate that if all participants had received RHC, the average length of stay in hospital would be 2.95 (1.99, 3.91) days longer than if no participants had received RHC.\nUnderstanding defaults\nTransform outcome:\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y &lt;- min(ObsData$Length.of.Stay)\nmax.Y &lt;- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf &lt;- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nRun TMLE, using the tmle package‚Äôs default SuperLearner library:\n\n# create data frame containing only covariates\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont.def &lt;- tmle::tmle(\n  Y = ObsData$Length.of.Stay_transf, \n  A = ObsData$RHC.use, \n  W = ObsData.noYA,\n  family = \"gaussian\",\n  V.Q = 5,\n  V.g = 5)\n# Q.SL.library = SL.library.test,  \n## removed this line\n# g.SL.library = SL.library.test)  \n## removed this line\n\nTransform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome‚Äôs original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n# transform back the ATE estimate\ntmle.est.cont.def &lt;- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$psi\ntmle.est.cont.def\n#&gt; [1] 3.036298\n\nTransform the confidence interval back to the original scale:\n\ntmle.ci.cont.def &lt;- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$CI\n\nATE for continuous outcome using default library: 3.0362984, and 95% CI 1.2686301, 4.8039667.\nThe estimate using the default SuperLearner library (2.18) is similar to the estimate we got when using our user-specified SuperLearner library (2.95). However, the confidence interval using the default SuperLearner library (1.25, 4.37) was much wider than that using our user-specified SuperLearner library (1.99, 3.91).\nComparison of results\nAdjusted regression:\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.LoS &lt;- c(baselinevars, \"Death\")\nout.formula.cont &lt;- as.formula(\n  paste(\"Length.of.Stay~ RHC.use +\", \n        paste(baselineVars.LoS,\n              collapse = \"+\")))\nfit1.cont &lt;- lm(out.formula.cont, data = ObsData)\npublish(fit1.cont, digits=1)$regressionTable[2,]\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 5 showed that, after propensity score pair (1-to-1) matching, means of length of stay (\\(Y\\)), when stratified by RHC (\\(A\\)) were not significantly different (\\(p = 0.14\\)).\n\n\n\n\nmethod.list\nEstimate\n2.5 %\n97.5 %\n\n\n\nAdjusted Regression\n3.04\n1.51\n4.58\n\n\nTMLE (user-specified SL library)\n2.94\n1.96\n3.92\n\n\nTMLE (default SL library)\n3.04\n1.27\n4.80\n\n\nKeele and Small (2021) paper\n2.01\n0.60\n3.41\n\n\n\n\n\nDifferences in results can likely be attributed to the use of different SuperLearner libraries, the use of different combinations of variables used, or random sampling associated with the cross-validation used in the SuperLearner algorithm.\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. ‚ÄúThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.‚Äù Jama 276 (11): 889‚Äì97. https://tinyurl.com/Connors1996.\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. ‚ÄúImplementing TMLE in the Presence of a Continuous Outcome.‚Äù Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nGruber, Susan, and Mark J van der Laan. 2010. ‚ÄúA Targeted Maximum Likelihood Estimator of a Causal Effect on a Bounded Continuous Outcome.‚Äù The International Journal of Biostatistics 6 (1).\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. ‚ÄúPractical Considerations for Specifying a Super Learner.‚Äù International Journal of Epidemiology 52: 1276‚Äì85. https://doi.org/10.1093/ije/dyad023.",
    "crumbs": [
      "ML in causal inference",
      "Continuous Outcomes"
    ]
  },
  {
    "objectID": "machinelearningCausal5.html",
    "href": "machinelearningCausal5.html",
    "title": "Comparing results",
    "section": "",
    "text": "In this tutorial, we will use the RHC dataset in exploring the relationship between RHC (yes/no) and death (yes/no). We will use the following approaches:\n\nlogistic regression\npropensity score matching and weighting with logistic regression\npropensity score matching and weighting with Super Learner\nTMLE\n\nLoad packages\nWe load several R packages required for fitting the models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(Publish)\nlibrary(randomForest)\nlibrary(tmle)\nlibrary(xgboost)\nlibrary(kableExtra)\nlibrary(SuperLearner)\nlibrary(dbarts)\nlibrary(MatchIt)\nlibrary(cobalt)\nlibrary(survey)\nlibrary(knitr)\n\nLoad dataset\n\nload(file = \"Data/machinelearningCausal/cl2.RData\")\nls()\n#&gt; [1] \"baselinevars\" \"ObsData\"      \"tab0\"\n\n\n# Data\ndat &lt;- ObsData\nhead(dat)\n\n\n  \n\n\n\n# Data dimension\ndim(dat)\n#&gt; [1] 5735   52\n\nConfounder list in the baselinevars vector is\n\n\n\n\nConfounders\n\n\n\nDisease.category\n\n\nCancer\n\n\nCardiovascular\n\n\nCongestive.HF\n\n\nDementia\n\n\nPsychiatric\n\n\nPulmonary\n\n\nRenal\n\n\nHepatic\n\n\nGI.Bleed\n\n\nTumor\n\n\nImmunosupperssion\n\n\nTransfer.hx\n\n\nMI\n\n\nage\n\n\nsex\n\n\nedu\n\n\nDASIndex\n\n\nAPACHE.score\n\n\nGlasgow.Coma.Score\n\n\nblood.pressure\n\n\nWBC\n\n\nHeart.rate\n\n\nRespiratory.rate\n\n\nTemperature\n\n\nPaO2vs.FIO2\n\n\nAlbumin\n\n\nHematocrit\n\n\nBilirubin\n\n\nCreatinine\n\n\nSodium\n\n\nPotassium\n\n\nPaCo2\n\n\nPH\n\n\nWeight\n\n\nDNR.status\n\n\nMedical.insurance\n\n\nRespiratory.Diag\n\n\nCardiovascular.Diag\n\n\nNeurological.Diag\n\n\nGastrointestinal.Diag\n\n\nRenal.Diag\n\n\nMetabolic.Diag\n\n\nHematologic.Diag\n\n\nSepsis.Diag\n\n\nTrauma.Diag\n\n\nOrthopedic.Diag\n\n\nrace\n\n\nincome\n\n\n\n\n\nLogistic regression\nLet us define the regression formula and fit logistic regression, adjusting for baseline confounders.\n\n# Formula\nFormula &lt;- formula(paste(\"Death ~ RHC.use + \", \n                         paste(baselinevars, \n                               collapse = \" + \")))\n\n# Logistic regression\nfit.glm &lt;- glm(Formula, \n               data = dat, \n               family = binomial)\n\nWe can use flextable package to view fit.glm, the regression output:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.42\n1.23\n1.65\n\n\n\n\n\nAs we can see, the odds of death was 42% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with logistic regression\nNow we will use propensity score matching, where propensity score will be estimated using logistic regression. The first step is to define the propensity score formula and estimate the propensity scores.\nStep 1\n\n# Propensity score model define\nps.formula &lt;- as.formula(paste0(\"RHC.use ~ \", \n                                paste(baselinevars, \n                                      collapse = \"+\")))\n\n# Propensity score model fitting\nfit.ps &lt;- glm(ps.formula, \n              data = dat, \n              family = binomial)\n\n# Propensity scores\ndat$ps &lt;- predict(fit.ps, \n                  type = \"response\")\nsummary(dat$ps)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.002478 0.161446 0.358300 0.380820 0.574319 0.968425\n\nStep 2\nThe second step is to match exposed (i.e., RHC users) to unexposed (i.e., RHC non-users) based on the propensity scores. We will use nearest neighborhood and caliper approach.\n\n# Caliper\ncaliper &lt;- 0.2*sd(log(dat$ps/(1-dat$ps)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, \n                     data = dat,\n                     distance = dat$ps, \n                     method = \"nearest\",\n                     ratio = 1,\n                     caliper = caliper)\n\n\n# See how many matched\nmatch.obj \n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.068)\n#&gt;  - number of obs.: 5735 (original), 3478 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: too many to name\n\n# Extract matched data\nmatched.data &lt;- match.data(match.obj)\n\n# Overlap checking\nbal.plot(match.obj,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nStep 3\nThe third step is to check the balancing on the matched data. We will compare the similarity of baseline characteristics between RHC users and non-users in the propensity score matched sample. Let‚Äôs consider SMD &gt;0.1 as imbalanced.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1b &lt;- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data, includeNA = T,\n                        addOverall = F, test = F)\n#print(tab1b, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1b) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.06\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.03\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.01\n\n\nPsychiatric\n0.04\n\n\nPulmonary\n0.04\n\n\nRenal\n0.03\n\n\nHepatic\n0.01\n\n\nGI.Bleed\n0.00\n\n\nTumor\n0.01\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.03\n\n\nMI\n0.01\n\n\nage\n0.04\n\n\nsex\n0.02\n\n\nedu\n0.03\n\n\nDASIndex\n0.03\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.07\n\n\nWBC\n0.01\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.04\n\n\nTemperature\n0.02\n\n\nPaO2vs.FIO2\n0.06\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.03\n\n\nCreatinine\n0.03\n\n\nSodium\n0.02\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.04\n\n\nPH\n0.02\n\n\nWeight\n0.02\n\n\nDNR.status\n0.00\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.06\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.02\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.03\n\n\nTrauma.Diag\n0.02\n\n\nOrthopedic.Diag\n0.05\n\n\nrace\n0.02\n\n\nincome\n0.06\n\n\n\n\n\n\nAfter propensity score matching, all the confounders are balanced in terms of SMD. Now, we will fit the outcome model on the matched data.\nStep 4\n\nfit.psm &lt;- glm(Death ~ RHC.use, \n               data = matched.data, \n               family = binomial)\n\nSummary of fit.psm:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.29\n1.12\n1.48\n\n\n\n\n\nIn the propensity score matched data, the odds of death was 29% higher among those participants with RHC use than those with no RHC use.\nPropensity score weighting with logistic regression\nNow we will use the propensity score weighting approach where propensity scores are estimated using logistic regression.\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw &lt;- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps, \n                            mean(1-RHC.use)/(1-ps)))\nsummary(dat$ipw)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.3932  0.6571  0.7781  0.9953  1.0624 24.1854\n\nStep 3\nNow, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design &lt;- svydesign(id = ~1, weights = ~ipw, data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1e &lt;- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design, includeNA = T, \n                           addOverall = F, test = F)\n#print(tab1e, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1e) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.02\n\n\nCancer\n0.01\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.02\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.01\n\n\nMI\n0.00\n\n\nage\n0.05\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.00\n\n\nblood.pressure\n0.01\n\n\nWBC\n0.04\n\n\nHeart.rate\n0.02\n\n\nRespiratory.rate\n0.00\n\n\nTemperature\n0.01\n\n\nPaO2vs.FIO2\n0.00\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.01\n\n\nSodium\n0.01\n\n\nPotassium\n0.03\n\n\nPaCo2\n0.02\n\n\nPH\n0.01\n\n\nWeight\n0.02\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.01\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.01\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.00\n\n\nHematologic.Diag\n0.00\n\n\nSepsis.Diag\n0.01\n\n\nTrauma.Diag\n0.01\n\n\nOrthopedic.Diag\n0.01\n\n\nrace\n0.02\n\n\nincome\n0.02\n\n\n\n\n\n\nAll confounders are balanced (SMD &lt; 0.1).\nStep 4\n\nfit.ipw &lt;- svyglm(Death ~ RHC.use, \n                  design = w.design, \n                  family = binomial)\n\nSummary of fit.ipw:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.30\n1.11\n1.53\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 30% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with super learner\nNow we will use the propensity score matching, where we will be using super learner to calculate the propensity scores. We use logistic, LASSO, and XGBoost as the candidate learners.\nStep 1\n\nset.seed(123)\nps.fit &lt;- CV.SuperLearner(\n  Y = dat$RHC.use,\n  X = dat[,baselinevars], \n  family = \"binomial\",\n  SL.library = c(\"SL.glm\", \"SL.glmnet\", \"SL.xgboost\"), \n  verbose = FALSE,\n  V = 5, \n  method = \"method.NNLS\")\n\n\n# Propensity scores for all learners  \npredictions &lt;- cbind(ps.fit$SL.predict, ps.fit$library.predict)\nhead(predictions)\n#&gt;                SL.glm_All SL.glmnet_All SL.xgboost_All\n#&gt; [1,] 0.3632635 0.39789364    0.39569431     0.32144672\n#&gt; [2,] 0.5870492 0.64983590    0.61497078     0.54465985\n#&gt; [3,] 0.5740246 0.66329951    0.65396650     0.46847290\n#&gt; [4,] 0.2172501 0.33044244    0.34478278     0.02363573\n#&gt; [5,] 0.6347749 0.45972157    0.43779434     0.86645132\n#&gt; [6,] 0.0272532 0.03420477    0.03629319     0.01730520\n\n# Propensity scores for super learner\ndat$ps.sl &lt;- predictions[,1]\nsummary(dat$ps.sl)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.001671 0.147762 0.337664 0.376282 0.587237 0.975584\n\nStep 2\nThe same as before, we will match exposed to unexposed based on their propensity scores.\n\n# Caliper\ncaliper &lt;- 0.2*sd(log(dat$ps.sl/(1-dat$ps.sl)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj2 &lt;- matchit(ps.formula, \n                      data = dat,\n                      distance = dat$ps.sl, \n                      method = \"nearest\",\n                      ratio = 1,\n                      caliper = caliper)\n\n\n# See how many matched\nmatch.obj2 \n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.075)\n#&gt;  - number of obs.: 5735 (original), 3430 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: too many to name\n\n# Extract matched data\nmatched.data.sl &lt;- match.data(match.obj2) \n\n# Overlap checking\nbal.plot(match.obj2,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nStep 3\nNow we will check the balancing on the matched data.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj2, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1c &lt;- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data.sl, includeNA = T, \n                        addOverall = T, test = F)\n#print(tab1c, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1c) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.08\n\n\nCancer\n0.05\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.02\n\n\nDementia\n0.00\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.02\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.05\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.06\n\n\nMI\n0.03\n\n\nage\n0.06\n\n\nsex\n0.04\n\n\nedu\n0.03\n\n\nDASIndex\n0.01\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.00\n\n\nHeart.rate\n0.05\n\n\nRespiratory.rate\n0.03\n\n\nTemperature\n0.04\n\n\nPaO2vs.FIO2\n0.09\n\n\nAlbumin\n0.05\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.00\n\n\nCreatinine\n0.05\n\n\nSodium\n0.01\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.05\n\n\nPH\n0.06\n\n\nWeight\n0.06\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.08\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.02\n\n\nRenal.Diag\n0.00\n\n\nMetabolic.Diag\n0.02\n\n\nHematologic.Diag\n0.02\n\n\nSepsis.Diag\n0.04\n\n\nTrauma.Diag\n0.06\n\n\nOrthopedic.Diag\n0.06\n\n\nrace\n0.02\n\n\nincome\n0.04\n\n\n\n\n\n\nAgain, all confounders are balanced in terms of SMD (all SMDs &lt; 0.1). Next, we will fit the outcome model.\nStep 4\n\nfit.psm.sl &lt;- glm(Death ~ RHC.use, \n                  data = matched.data.sl, \n                  family = binomial)\n\nSummary of fit.psm.sl:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.09\n1.45\n\n\n\n\n\nThe interpretation is the same as before. In the propensity score matched data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nPropensity score weighting with super learner\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw.sl &lt;- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps.sl, \n                               mean(1-RHC.use)/(1-ps.sl)))\nsummary(dat$ipw.sl)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.3903  0.6496  0.7647  1.0191  1.0468 41.4490\n\nStep 3\nNext, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design.sl &lt;- svydesign(id = ~1, weights = ~ipw.sl, \n                         data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1k &lt;- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design.sl, includeNA = T, \n                           addOverall = T, test = F)\n#print(tab1k, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1k) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.01\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.02\n\n\nCongestive.HF\n0.01\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.03\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.00\n\n\nMI\n0.01\n\n\nage\n0.07\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.02\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.06\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.01\n\n\nTemperature\n0.00\n\n\nPaO2vs.FIO2\n0.03\n\n\nAlbumin\n0.00\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.00\n\n\nSodium\n0.00\n\n\nPotassium\n0.02\n\n\nPaCo2\n0.05\n\n\nPH\n0.02\n\n\nWeight\n0.01\n\n\nDNR.status\n0.05\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.02\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.02\n\n\nGastrointestinal.Diag\n0.00\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.00\n\n\nTrauma.Diag\n0.05\n\n\nOrthopedic.Diag\n0.02\n\n\nrace\n0.05\n\n\nincome\n0.05\n\n\n\n\n\n\nAll confounders are balanced (SMD &lt; 0.1).\nStep 4\n\nfit.ipw.sl &lt;- svyglm(Death ~ RHC.use, \n                     design = w.design.sl, \n                     family = binomial)\n\nSummary of fit.ipw.sl:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.04\n1.52\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nTMLE\nFrom the previous tutorial, we calculated the effective sample size for the outcome model as well as for the exposure model:\n\nn &lt;- nrow(dat) \npY &lt;- nrow(dat[dat$Death == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(pY, 1 - pY))) \nn_eff\n#&gt; [1] 5735\n\n\nn &lt;- nrow(dat) \npA &lt;- nrow(dat[dat$RHC.use == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(pA, 1 - pA))) \nn_eff\n#&gt; [1] 5735\n\nSince the effective sample size is \\(\\ge 5,000\\) for both model, we can consider \\(5 \\le \\text{V} \\le 10\\), where V is the number of folds. Let‚Äôs work with 10-fold cross-validation for both the exposure and the outcome model, with the default super learner library.\n\nfit.tmle &lt;- tmle(Y = dat$Death, \n                 A = dat$RHC.use, \n                 W = dat[,baselinevars], \n                 family = \"binomial\", \n                 V.Q = 10, \n                 V.g = 10)\n\n\n# OR\nround(fit.tmle$estimates$OR$psi, 2)\n#&gt; [1] 1.26\n\n# 95% CI\nround(fit.tmle$estimates$OR$CI, 2)\n#&gt; [1] 1.12 1.42\n\nAs we can see that the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nResults comparison\n\n\n\n\nModel\nOR\n95% CI\n\n\n\nLogistic regression\n1.42\n1.32-1.65\n\n\nPropensity score matching with logistic\n1.29\n1.12-1.48\n\n\nPropensity score weighting with logistic\n1.30\n1.11-1.53\n\n\nPropensity score matching with super learner (logistic, LASSO, and XGBoost)\n1.26\n1.09-1.45\n\n\nPropensity score weighting with super learner (logistic, LASSO, and XGBoost)\n1.26\n1.04-1.52\n\n\nTMLE (default SL library)\n1.26\n1.12-1.42\n\n\n\n\n\n\n\n\n\nForest Plot of Odds Ratios",
    "crumbs": [
      "ML in causal inference",
      "Comparing results"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html",
    "href": "machinelearningCausalQ.html",
    "title": "Quiz (C)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html#live-quiz",
    "href": "machinelearningCausalQ.html#live-quiz",
    "title": "Quiz (C)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html#download-quiz",
    "href": "machinelearningCausalQ.html#download-quiz",
    "title": "Quiz (C)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R functions (C)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\nFunction.name\nPackage.name\nUse\n\n\n\npublish\nPublish\nTo publish regression models\n\n\nmedian\nstats\nTo calculate the median of a numeric vector\n\n\nSL.mean\nSuperLearner\nSuperLearner wrapper for the mean learner\n\n\nSL.glm\nSuperLearner\nSuperLearner wrapper for the generalized linear model learner\n\n\nSL.glmnet\nSuperLearner\nSuperLearner wrapper for the generalized linear model with elastic net penalty learner\n\n\nSL.xgboost\nSuperLearner\nSuperLearner wrapper for the extreme gradient boosting learner\n\n\nSL.randomForest\nSuperLearner\nSuperLearner wrapper for the random forest learner\n\n\nSL.svm\nSuperLearner\nSuperLearner wrapper for the support vector machine (SVM) learner\n\n\nCreateTableOne\ntableone\nTo create a summary table for a dataset\n\n\ntmle\ntmle\nTo run Targeted Maximum Likelihood Estimation (TMLE) for causal inference\n\n\ntmle.SL.dbarts2\ntmle\nSuperLearner wrapper for the Bayesian Additive Regression Trees (BART) learner",
    "crumbs": [
      "ML in causal inference",
      "R functions (C)"
    ]
  },
  {
    "objectID": "nonbinary.html",
    "href": "nonbinary.html",
    "title": "Complex outcomes",
    "section": "",
    "text": "Background\nThis chapter offers tutorials on handling non-binary outcomes in data analysis. The first tutorial, on Polytomous and Ordinal outcomes, delves into multinomial and ordinal logistic regressions, teaching methods to analyze datasets with more than two categorical outcomes and how to assess the fit of ordinal logistic models. The second tutorial introduces Survival Analysis, guiding readers through the components of survival data, right censoring, the Kaplan-Meier method, the Cox regression model, and the complexities of analyzing time-dependent covariates and survey data. Lastly, the tutorial on Poisson and Negative Binomial focuses on regressions related to count data, showcasing both regular and survey-weighted methods, and highlights statistical techniques to understand factors influencing certain variables.",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary.html#background",
    "href": "nonbinary.html#background",
    "title": "Complex outcomes",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary.html#overview-of-tutorials",
    "href": "nonbinary.html#overview-of-tutorials",
    "title": "Complex outcomes",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily focused on binary or continuous outcomes. In this chapter, we will discuss about categorical [outcome variable is categorical with more than two categories (i.e., multicategory)], survival [time-to-event outcomes and aims to model the time until an event happens, accounting for censored data (observations where the event has not occurred by the end of the study)] and count data [non-negative integers (i.e., 0, 1, 2, 3, ‚Ä¶)] outcomes.\n\nPolytomous and ordinal\nThe tutorial covers methods for analyzing polytomous and ordinal outcomes in R. Initially, it introduces a function to exclude invalid responses from datasets. The data is loaded, and its structure is checked. The tutorial delves into the multinomial logistic regression, where tables are created to explore data characteristics. Two models are fitted: one for unweighted data and another for survey-weighted data. It then transitions into ordinal regression. Here, outcomes are ordered, and ordinal logistic models are fitted, again for both unweighted and survey-weighted data. The final part of the tutorial assesses the fit of the ordinal logistic model using various variables.\n\n\nSurvival analysis\nThe tutorial provides an in-depth understanding of survival analysis. It starts by introducing the lung dataset which contains various attributes related to patients‚Äô health. The concept of censoring, specifically right censoring, is detailed, which happens when a subject leaves the study without experiencing the event of interest. The tutorial delves into the components of survival data, introducing the event indicator, survival function, and survival probability. It then transitions to practical applications, demonstrating how to create survival objects, estimate survival curves using the Kaplan-Meier method, and visualize these curves. There‚Äôs also an emphasis on understanding and estimating median survival time and comparing survival times between different groups using the log-rank test. The Cox regression model, which is pivotal in survival analysis, is elaborated, along with the idea of hazard ratios and assessing proportional hazards. Time-dependent covariates, which account for variables that may change over time, are also touched upon. Towards the end, the tutorial addresses how to perform survival analysis on complex survey data, covering design creation, Kaplan-Meier plotting, and the Cox Proportional Hazards model in the context of these surveys.\n\n\nSurvival analysis in NHANES\nThe tutorial demonstrates an analysis of NHANES data linked with mortality data, and conducts a survival analysis.\n\n\nPoisson and negative binomial\nThe tutorial provides a comprehensive guide on statistical analysis techniques related to Poisson and negative binomial regressions. Initially, it prepares the data by converting them into appropriate formats. A weighted summary of the dataset is then generated, emphasizing the distribution of a variable. The tutorial proceeds to demonstrate both regular and survey-weighted Poisson regressions, focusing on the relationship between fruit consumption and various factors. Finally, it explores negative binomial regressions and their survey-weighted counterparts.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary0.html",
    "href": "nonbinary0.html",
    "title": "Concepts (N)",
    "section": "",
    "text": "Complex outcomes beyond binary\nIn this section, we explore regressions for outcomes beyond binary:",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#complex-outcomes-beyond-binary",
    "href": "nonbinary0.html#complex-outcomes-beyond-binary",
    "title": "Concepts (N)",
    "section": "",
    "text": "Polytomous regression\nSurvival analysis\nPoisson regression",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#reading-list",
    "href": "nonbinary0.html#reading-list",
    "title": "Concepts (N)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Hosmer, Lemeshow, and Sturdivant 2013; Kleinbaum and Klein 2011; Coxe, West, and Aiken 2009)",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#video-lessons",
    "href": "nonbinary0.html#video-lessons",
    "title": "Concepts (N)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPolytomous regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson regression",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#video-lesson-slides",
    "href": "nonbinary0.html#video-lesson-slides",
    "title": "Concepts (N)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#links",
    "href": "nonbinary0.html#links",
    "title": "Concepts (N)",
    "section": "Links",
    "text": "Links\nPolytomous regression\n\nGoogle Slides\nPDF Slides\n\nSurvival analysis\n\nGoogle Slides\nPDF Slides\n\nPoisson regression\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#references",
    "href": "nonbinary0.html#references",
    "title": "Concepts (N)",
    "section": "References",
    "text": "References\n\n\n\n\nCoxe, Stefany, Stephen G. West, and Leona S. Aiken. 2009. ‚ÄúThe Analysis of Count Data: A Gentle Introduction to Poisson Regression and Its Alternatives.‚Äù Journal of Personality Assessment 91 (2): 121‚Äì36. https://doi.org/10.1080/00223890802634175.\n\n\nHosmer, Jr., David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression, 3rd Edition. Hoboken, NJ: John Wiley & Sons.\n\n\nKleinbaum, David G., and Mitchel Klein. 2011. Survival Analysis: A Self-Learning Text, Third Edition. New York, NY: Springer.",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary1.html",
    "href": "nonbinary1.html",
    "title": "Polytomous and ordinal",
    "section": "",
    "text": "Let us load the packages:\n\n# Load required packages\nrequire(Publish)\nrequire(survey)\nrequire(svyVGAM)\nrequire(car)\nlibrary(knitr)\n\nIn the code chunk below, we create a function called invalid.exclude. This function can be used to exclude invalid responses from the dataset, where don‚Äôt know, refusal, and not stated are considered invalid responses.\n\n# Function to exclude invalid responses\ninvalid.exclude &lt;- function(Data, Var){\n  subset.data &lt;- subset(Data, eval(parse(text = Var)) != \"DON'T KNOW\" & \n                          eval(parse(text = Var)) != \"REFUSAL\" & \n                          eval(parse(text = Var)) != \"NOT STATED\")\n  x1 &lt;- dim(Data)[1]\n  x2 &lt;- dim(subset.data)[1]\n  cat( format(x1-x2, big.mark = \",\"),\n       \"subjects deleted, and current N =\" , format(x2, big.mark = \",\") , \"\\n\")\n  return(subset.data)\n}\n\nData\n\n# Data\nanalytic &lt;- readRDS(\"Data/nonbinary/cmh.Rds\")\nstr(analytic)\n#&gt; 'data.frame':    2628 obs. of  9 variables:\n#&gt;  $ MHcondition       : Factor w/ 3 levels \"Good\",\"Poor or Fair\",..: 2 2 3 3 3 2 3 1 3 3 ...\n#&gt;  $ CommunityBelonging: Factor w/ 4 levels \"SOMEWHAT STRONG\",..: 1 1 3 3 3 2 1 1 3 1 ...\n#&gt;  $ Age               : Factor w/ 6 levels \"15 TO 24 YEARS\",..: 4 1 1 4 3 5 2 5 3 1 ...\n#&gt;  $ Sex               : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 2 2 2 2 2 1 1 2 ...\n#&gt;  $ RaceEthnicity     : Factor w/ 2 levels \"NON-WHITE\",\"WHITE\": 2 1 2 2 1 2 2 2 2 1 ...\n#&gt;  $ MainIncome        : Factor w/ 5 levels \"EI/WORKER'S COMP\",..: 2 2 2 2 2 5 2 2 2 2 ...\n#&gt;  $ ReceivedHelp      : Factor w/ 4 levels \"DON'T KNOW\",\"NO\",..: 4 4 4 2 2 4 2 2 4 2 ...\n#&gt;  $ Weight            : num  678 1298 196 917 2384 ...\n#&gt;  $ Disorder          : Factor w/ 1 level \"YES\": 1 1 1 1 1 1 1 1 1 1 ...\n\nLet us drop invalid responses\n\n# Drop invalid responses\nanalytic &lt;- invalid.exclude(analytic, Var = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \n                                              \"MainIncome\", \"MHcondition\"))\n#&gt; 0 subjects deleted, and current N = 2,628\n\nMultinomial logistic\nUnweighted Tables\nLet us see the summary statistic of the variables, stratified by mental health condition:\n\nrequire(\"tableone\")\n#&gt; Loading required package: tableone\ntab1 &lt;- CreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"),\n                       strata = \"MHcondition\", data = analytic, test = F)\nprint(tab1, showAllLevels = TRUE, smd = T)\n#&gt;                         Stratified by MHcondition\n#&gt;                          level             Good        Poor or Fair\n#&gt;   n                                        885         1002        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   362 (40.9)   288 (28.7) \n#&gt;                          SOMEWHAT WEAK     309 (34.9)   358 (35.7) \n#&gt;                          VERY STRONG        96 (10.8)    74 ( 7.4) \n#&gt;                          VERY WEAK         118 (13.3)   282 (28.1) \n#&gt;   Age (%)                15 TO 24 YEARS    264 (29.8)   191 (19.1) \n#&gt;                          25 TO 34 YEARS    167 (18.9)   141 (14.1) \n#&gt;                          35 TO 44 YEARS    119 (13.4)   185 (18.5) \n#&gt;                          35 TO 54 YEARS    139 (15.7)   220 (22.0) \n#&gt;                          55 TO 64 YEARS    113 (12.8)   198 (19.8) \n#&gt;                          65 years or older  83 ( 9.4)    67 ( 6.7) \n#&gt;   Sex (%)                FEMALE            487 (55.0)   616 (61.5) \n#&gt;                          MALE              398 (45.0)   386 (38.5) \n#&gt;   RaceEthnicity (%)      NON-WHITE         140 (15.8)   184 (18.4) \n#&gt;                          WHITE             745 (84.2)   818 (81.6) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   78 ( 8.8)   195 (19.5) \n#&gt;                          EMPLOYMENT INC.   641 (72.4)   560 (55.9) \n#&gt;                          NOT STATED         23 ( 2.6)    25 ( 2.5) \n#&gt;                          OTHER              36 ( 4.1)    66 ( 6.6) \n#&gt;                          SENIOR BENEFITS   107 (12.1)   156 (15.6) \n#&gt;                         Stratified by MHcondition\n#&gt;                          Very good/excellent SMD   \n#&gt;   n                      741                       \n#&gt;   CommunityBelonging (%) 355 (47.9)           0.425\n#&gt;                          190 (25.6)                \n#&gt;                          116 (15.7)                \n#&gt;                           80 (10.8)                \n#&gt;   Age (%)                285 (38.5)           0.420\n#&gt;                          167 (22.5)                \n#&gt;                           89 (12.0)                \n#&gt;                           79 (10.7)                \n#&gt;                           68 ( 9.2)                \n#&gt;                           53 ( 7.2)                \n#&gt;   Sex (%)                304 (41.0)           0.277\n#&gt;                          437 (59.0)                \n#&gt;   RaceEthnicity (%)      134 (18.1)           0.045\n#&gt;                          607 (81.9)                \n#&gt;   MainIncome (%)          36 ( 4.9)           0.400\n#&gt;                          598 (80.7)                \n#&gt;                            9 ( 1.2)                \n#&gt;                           22 ( 3.0)                \n#&gt;                           76 (10.3)\n\nMultinomial Model fitting\nBefore fitting the multinomial regression, let us redefine the reference categories of the variables.\n\nanalytic$MHcondition2 &lt;- relevel(analytic$MHcondition, ref = \"Poor or Fair\")\nanalytic$CommunityBelonging2 &lt;- relevel(analytic$CommunityBelonging, ref = \"VERY WEAK\")\nanalytic$Age2 &lt;- relevel(analytic$Age, ref = \"65 years or older\")\nanalytic$Sex2 &lt;- relevel(analytic$Sex, ref = \"FEMALE\")\nanalytic$RaceEthnicity2 &lt;- relevel(analytic$RaceEthnicity, ref = \"NON-WHITE\")\n\nNow we will fit the multinomial logistic regression model using the multinom function from the nnet package:\n\nrequire(nnet)\n#&gt; Loading required package: nnet\nfit4 &lt;- multinom(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                    data = analytic)\n#&gt; # weights:  48 (30 variable)\n#&gt; initial  value 2887.153095 \n#&gt; iter  10 value 2640.386436\n#&gt; iter  20 value 2625.127331\n#&gt; iter  30 value 2622.465409\n#&gt; final  value 2622.328038 \n#&gt; converged\nkable(round(exp(cbind(coef(fit4), confint(fit4))),2))\n#&gt; Warning in cbind(coef(fit4), confint(fit4)): number of rows of result is not a\n#&gt; multiple of vector length (arg 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nCommunityBelonging2SOMEWHAT STRONG\nCommunityBelonging2SOMEWHAT WEAK\nCommunityBelonging2VERY STRONG\nSex2MALE\nAge215 TO 24 YEARS\nAge225 TO 34 YEARS\nAge235 TO 44 YEARS\nAge235 TO 54 YEARS\nAge255 TO 64 YEARS\nRaceEthnicity2WHITE\nMainIncomeEMPLOYMENT INC.\nMainIncomeNOT STATED\nMainIncomeOTHER\nMainIncomeSENIOR BENEFITS\n\n\n\n\nGood\n0.38\n2.72\n1.84\n3.08\n1.29\n0.70\n0.62\n0.32\n0.37\n0.36\n1.25\n2.29\n1.57\n1.14\n1.11\n0.21\n\n\nVery good/excellent\n0.09\n3.92\n1.64\n5.65\n2.19\n1.27\n1.08\n0.39\n0.37\n0.37\n1.15\n3.89\n1.25\n1.34\n2.11\n2.07\n\n\n\n\n\nMultinomial logistic for complex survey\nSurvey-weighted Tables\nNow, we will set up the survey design with the survey weights and then see design-adjusted summary statistics.\n\nrequire(survey)\n# Design\nsvy.analytic &lt;- svydesign(ids = ~ 1, weights = ~ Weight, data = analytic)\n\n# Table 1\ntab1a &lt;- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                  data = svy.analytic)\nprint(tab1a, showAllLevels = TRUE)\n#&gt;                         \n#&gt;                          level             Overall          \n#&gt;   n                                        2734564.0        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   1015381.0 (37.1) \n#&gt;                          SOMEWHAT WEAK      948838.7 (34.7) \n#&gt;                          VERY STRONG        297141.6 (10.9) \n#&gt;                          VERY WEAK          473202.7 (17.3) \n#&gt;   Age (%)                15 TO 24 YEARS     795568.6 (29.1) \n#&gt;                          25 TO 34 YEARS     564720.6 (20.7) \n#&gt;                          35 TO 44 YEARS     457821.1 (16.7) \n#&gt;                          35 TO 54 YEARS     447324.8 (16.4) \n#&gt;                          55 TO 64 YEARS     319374.3 (11.7) \n#&gt;                          65 years or older  149754.6 ( 5.5) \n#&gt;   Sex (%)                FEMALE            1312605.0 (48.0) \n#&gt;                          MALE              1421958.9 (52.0) \n#&gt;   RaceEthnicity (%)      NON-WHITE          565784.5 (20.7) \n#&gt;                          WHITE             2168779.4 (79.3) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   245758.5 ( 9.0) \n#&gt;                          EMPLOYMENT INC.   2059689.1 (75.3) \n#&gt;                          NOT STATED          60101.5 ( 2.2) \n#&gt;                          OTHER              116260.6 ( 4.3) \n#&gt;                          SENIOR BENEFITS    252754.4 ( 9.2)\n\n# table 1 stratified by MHcondition\ntab1b &lt;- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                           strata = \"MHcondition\", data = svy.analytic)\nprint(tab1b, showAllLevels = TRUE)\n#&gt;                         Stratified by MHcondition\n#&gt;                          level             Good             Poor or Fair    \n#&gt;   n                                        949208.1         973112.1        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   346216.2 (36.5)  273163.9 (28.1) \n#&gt;                          SOMEWHAT WEAK     378431.4 (39.9)  365731.0 (37.6) \n#&gt;                          VERY STRONG        97528.8 (10.3)   69438.0 ( 7.1) \n#&gt;                          VERY WEAK         127031.7 (13.4)  264779.1 (27.2) \n#&gt;   Age (%)                15 TO 24 YEARS    286103.0 (30.1)  187610.9 (19.3) \n#&gt;                          25 TO 34 YEARS    189084.8 (19.9)  184528.6 (19.0) \n#&gt;                          35 TO 44 YEARS    140796.4 (14.8)  199172.7 (20.5) \n#&gt;                          35 TO 54 YEARS    171968.1 (18.1)  194689.1 (20.0) \n#&gt;                          55 TO 64 YEARS    109114.5 (11.5)  148432.2 (15.3) \n#&gt;                          65 years or older  52141.3 ( 5.5)   58678.6 ( 6.0) \n#&gt;   Sex (%)                FEMALE            436830.4 (46.0)  582311.1 (59.8) \n#&gt;                          MALE              512377.7 (54.0)  390801.0 (40.2) \n#&gt;   RaceEthnicity (%)      NON-WHITE         167887.9 (17.7)  220525.5 (22.7) \n#&gt;                          WHITE             781320.2 (82.3)  752586.6 (77.3) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   66003.9 ( 7.0)  151095.4 (15.5) \n#&gt;                          EMPLOYMENT INC.   752208.0 (79.2)  608703.4 (62.6) \n#&gt;                          NOT STATED         23589.0 ( 2.5)   28371.0 ( 2.9) \n#&gt;                          OTHER              32247.9 ( 3.4)   68453.1 ( 7.0) \n#&gt;                          SENIOR BENEFITS    75159.3 ( 7.9)  116489.2 (12.0) \n#&gt;                         Stratified by MHcondition\n#&gt;                          Very good/excellent p      test\n#&gt;   n                      812243.7                       \n#&gt;   CommunityBelonging (%) 396000.8 (48.8)     &lt;0.001     \n#&gt;                          204676.3 (25.2)                \n#&gt;                          130174.7 (16.0)                \n#&gt;                           81391.9 (10.0)                \n#&gt;   Age (%)                321854.6 (39.6)     &lt;0.001     \n#&gt;                          191107.2 (23.5)                \n#&gt;                          117852.1 (14.5)                \n#&gt;                           80667.6 ( 9.9)                \n#&gt;                           61827.6 ( 7.6)                \n#&gt;                           38934.6 ( 4.8)                \n#&gt;   Sex (%)                293463.5 (36.1)     &lt;0.001     \n#&gt;                          518780.2 (63.9)                \n#&gt;   RaceEthnicity (%)      177371.1 (21.8)      0.219     \n#&gt;                          634872.7 (78.2)                \n#&gt;   MainIncome (%)          28659.2 ( 3.5)     &lt;0.001     \n#&gt;                          698777.7 (86.0)                \n#&gt;                            8141.5 ( 1.0)                \n#&gt;                           15559.6 ( 1.9)                \n#&gt;                           61105.8 ( 7.5)\n\nSetting up the design\nLet us redefine the reference categories within the survey design and convert the design to use replicate weights.\n\nw.design &lt;- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design &lt;- update(w.design , MHcondition2 = relevel(MHcondition, ref = \"Poor or Fair\"),\n                  CommunityBelonging2 = relevel(CommunityBelonging, ref = \"VERY WEAK\"),\n                  Age2 = relevel(Age, ref = \"65 years or older\"),\n                  Sex2 = relevel(Sex, ref = \"FEMALE\"),\n                  RaceEthnicity2 = relevel(RaceEthnicity, ref = \"NON-WHITE\"))\n\n# Convert a survey design to use replicate weights\nw.design2 &lt;- as.svrepdesign(w.design, type = \"bootstrap\" , replicates = 50)\n\nMultinomial Model fitting\nNow, we will fit the design-adjusted multinomial logistic regression:\n\nrequire(svyVGAM)\nfit5 &lt;- svy_vglm(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome,\n                    design = w.design2, family = multinomial)\n#&gt; Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :\n#&gt; iterations terminated because half-step sizes are very small\n#&gt; Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\n#&gt; quantities such as z, residuals, SEs may be inaccurate due to convergence at a\n#&gt; half-step\n\n\nkable(round(exp(cbind(coef(fit5), confint(fit5))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n(Intercept):1\n16.57\n6.54\n41.97\n\n\n(Intercept):2\n3.24\n1.32\n7.95\n\n\nCommunityBelonging2SOMEWHAT STRONG:1\n0.22\n0.14\n0.35\n\n\nCommunityBelonging2SOMEWHAT STRONG:2\n0.56\n0.34\n0.92\n\n\nCommunityBelonging2SOMEWHAT WEAK:1\n0.58\n0.36\n0.91\n\n\nCommunityBelonging2SOMEWHAT WEAK:2\n1.18\n0.69\n1.99\n\n\nCommunityBelonging2VERY STRONG:1\n0.15\n0.09\n0.27\n\n\nCommunityBelonging2VERY STRONG:2\n0.46\n0.26\n0.83\n\n\nSex2MALE:1\n0.39\n0.27\n0.55\n\n\nSex2MALE:2\n0.68\n0.51\n0.91\n\n\nAge215 TO 24 YEARS:1\n0.56\n0.26\n1.18\n\n\nAge215 TO 24 YEARS:2\n0.65\n0.35\n1.19\n\n\nAge225 TO 34 YEARS:1\n0.83\n0.40\n1.72\n\n\nAge225 TO 34 YEARS:2\n0.68\n0.34\n1.38\n\n\nAge235 TO 44 YEARS:1\n1.74\n0.80\n3.75\n\n\nAge235 TO 44 YEARS:2\n0.91\n0.44\n1.87\n\n\nAge235 TO 54 YEARS:1\n2.02\n0.93\n4.36\n\n\nAge235 TO 54 YEARS:2\n1.48\n0.71\n3.05\n\n\nAge255 TO 64 YEARS:1\n1.91\n0.89\n4.10\n\n\nAge255 TO 64 YEARS:2\n1.33\n0.63\n2.79\n\n\nRaceEthnicity2WHITE:1\n0.91\n0.64\n1.28\n\n\nRaceEthnicity2WHITE:2\n1.22\n0.90\n1.67\n\n\nMainIncomeEMPLOYMENT INC.:1\n0.25\n0.14\n0.44\n\n\nMainIncomeEMPLOYMENT INC.:2\n0.58\n0.32\n1.07\n\n\nMainIncomeNOT STATED:1\n0.91\n0.21\n4.04\n\n\nMainIncomeNOT STATED:2\n1.37\n0.31\n6.09\n\n\nMainIncomeOTHER:1\n1.43\n0.56\n3.70\n\n\nMainIncomeOTHER:2\n1.30\n0.57\n3.00\n\n\nMainIncomeSENIOR BENEFITS:1\n0.39\n0.19\n0.79\n\n\nMainIncomeSENIOR BENEFITS:2\n0.49\n0.23\n1.04\n\n\n\n\n\nOrdinal Regression\nOrdering outcome\nLet‚Äôs define MHcondition as an ordinal outcome. We can do it by using ordered = TRUE in the factor function.\n\n# Ordinal outcome\nanalytic$MHcondition3 &lt;- factor(analytic$MHcondition, \n                               levels = c(\"Poor or Fair\", \"Good\", \"Very good or excellent\"), \n                      ordered = TRUE)\n\nOrdinal logistic\nLet‚Äôs fit the ordinal logistic using the polr function:\n\nrequire(MASS)\n#&gt; Loading required package: MASS\nfit5o1 &lt;- polr(MHcondition3 ~CommunityBelonging2 + \n                  Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                data=analytic)\nkable(round(exp(cbind(coef(fit5o1), confint(fit5o1))),2))\n#&gt; Waiting for profiling to be done...\n#&gt; \n#&gt; Re-fitting to get Hessian\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.69\n2.05\n3.55\n\n\nCommunityBelonging2SOMEWHAT WEAK\n1.83\n1.40\n2.41\n\n\nCommunityBelonging2VERY STRONG\n3.15\n2.15\n4.66\n\n\nSex2MALE\n1.30\n1.07\n1.58\n\n\nAge215 TO 24 YEARS\n0.69\n0.42\n1.11\n\n\nAge225 TO 34 YEARS\n0.60\n0.36\n0.98\n\n\nAge235 TO 44 YEARS\n0.32\n0.19\n0.53\n\n\nAge235 TO 54 YEARS\n0.37\n0.23\n0.59\n\n\nAge255 TO 64 YEARS\n0.36\n0.22\n0.56\n\n\nRaceEthnicity2WHITE\n1.26\n0.97\n1.63\n\n\nMainIncomeEMPLOYMENT INC.\n2.26\n1.68\n3.06\n\n\nMainIncomeNOT STATED\n1.53\n0.79\n2.94\n\n\nMainIncomeOTHER\n1.15\n0.70\n1.90\n\n\nMainIncomeSENIOR BENEFITS\n1.10\n0.70\n1.70\n\n\n\n\n\nOrdinal logistic for complex survey\nThe same as before, we can set up the design, relevel variables within the design, define MHcondition as an ordinal variable, and then fit the design-adjusted ordinal logistic regression.\n\nw.design &lt;- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design&lt;-update(w.design , \n                  CommunityBelonging2=relevel(CommunityBelonging, ref=\"VERY WEAK\"),\n                  Age2=relevel(Age, ref=\"65 years or older\"),\n                  Sex2=relevel(Sex, ref=\"FEMALE\"),\n                  RaceEthnicity2=relevel(RaceEthnicity, ref=\"NON-WHITE\"),\n                  MHcondition3 = factor(MHcondition, levels=c(\"Poor or Fair\", \"Good\", \n                                                              \"Very good or excellent\"), \n                                        ordered=TRUE))\n\n# Design-adjusted Ordinal logistic\nfit5o &lt;- svyolr(MHcondition3 ~CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                design=w.design)\nkable(round(exp(cbind(coef(fit5o), confint(fit5o))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.49\n1.65\n3.75\n\n\nCommunityBelonging2SOMEWHAT WEAK\n2.04\n1.37\n3.05\n\n\nCommunityBelonging2VERY STRONG\n3.02\n1.74\n5.26\n\n\nSex2MALE\n1.76\n1.30\n2.38\n\n\nAge215 TO 24 YEARS\n1.13\n0.52\n2.44\n\n\nAge225 TO 34 YEARS\n0.77\n0.34\n1.74\n\n\nAge235 TO 44 YEARS\n0.52\n0.24\n1.13\n\n\nAge235 TO 54 YEARS\n0.70\n0.32\n1.52\n\n\nAge255 TO 64 YEARS\n0.69\n0.32\n1.46\n\n\nRaceEthnicity2WHITE\n1.31\n0.90\n1.91\n\n\nMainIncomeEMPLOYMENT INC.\n2.37\n1.53\n3.67\n\n\nMainIncomeNOT STATED\n1.42\n0.52\n3.86\n\n\nMainIncomeOTHER\n0.88\n0.39\n2.00\n\n\nMainIncomeSENIOR BENEFITS\n1.26\n0.66\n2.41\n\n\nPoor or Fair|Good\n4.79\n1.92\n11.91\n\n\nGood|Very good or excellent\n2636603.90\n996905.04\n6973262.11\n\n\n\n\n\nAssessing model fit\nWe can use the regTermTest function from the survey package to do the Wald test of a regression coefficient.\n\nregTermTest(fit5o, ~CommunityBelonging2 , df = Inf) \n#&gt; Wald test for CommunityBelonging2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  24.53941  on  3  df: p= 1.9272e-05\nregTermTest(fit5o, ~Age2 , df = Inf) \n#&gt; Wald test for Age2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  14.00491  on  5  df: p= 0.015578\nregTermTest(fit5o, ~Sex2 , df = Inf)\n#&gt; Wald test for Sex2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  13.46032  on  1  df: p= 0.00024366\nregTermTest(fit5o, ~RaceEthnicity2 , df = Inf)\n#&gt; Wald test for RaceEthnicity2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  2.002794  on  1  df: p= 0.15701\nregTermTest(fit5o, ~MainIncome , df = Inf)\n#&gt; Wald test for MainIncome\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  22.13656  on  4  df: p= 0.00018826\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Polytomous and ordinal"
    ]
  },
  {
    "objectID": "nonbinary2.html",
    "href": "nonbinary2.html",
    "title": "Survival analysis",
    "section": "",
    "text": "The lung dataset\nlibrary(survival)\nhead(lung)\n\n\n  \n\n\nkable(head(lung))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninst\ntime\nstatus\nage\nsex\nph.ecog\nph.karno\npat.karno\nmeal.cal\nwt.loss\n\n\n\n3\n306\n2\n74\n1\n1\n90\n100\n1175\nNA\n\n\n3\n455\n2\n68\n1\n0\n90\n90\n1225\n15\n\n\n3\n1010\n1\n56\n1\n0\n90\n90\nNA\n15\n\n\n5\n210\n2\n57\n1\n1\n90\n60\n1150\n11\n\n\n1\n883\n2\n60\n1\n0\n100\n90\nNA\n0\n\n\n12\n1022\n1\n74\n1\n1\n50\n80\n513\n0\n\n\n\n\n# kable(lung)",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis"
    ]
  },
  {
    "objectID": "nonbinary2.html#data-and-variables",
    "href": "nonbinary2.html#data-and-variables",
    "title": "Survival analysis",
    "section": "Data and Variables",
    "text": "Data and Variables\n\nload(\"Data/nonbinary/nh_99-06.Rdata\") # \nanalytic.miss &lt;- as.data.frame(MainTable[c(\"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\", \"PERMTH_INT\", \"PERMTH_EXM\", \"MORTSTAT\", \"female\", \"RIDAGEYR\", \"white\")])\n\n\nMORTSTAT: Final Mortality Status\n\n0 Assumed alive\n1 Assumed deceased\nBlank Ineligible for mortality follow-up or under age 17\n\n\nPERMTH_EXM: Person Months of Follow-up from MEC/Home Exam Date\n\n0 - 217\nBlank Ineligible\n\n\nPERMTH_INT: Person Months of Follow-up from Interview Date\n\nData issues\n\nwith(analytic.miss[1:30,], \n     Surv(PERMTH_EXM, MORTSTAT))\n#&gt;  [1] NA? 90+ NA? NA? 74+ 86+ 76+ NA? NA? 79+ NA? 82+ 16  85+ 92+ 62  NA? NA? NA?\n#&gt; [20] 86+ 87+ NA? NA? 72+ 84+ NA? 85+ 91+ 26  NA?\nsummary(analytic.miss$WTMEC2YR)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0    6365   15572   27245   38896  261361\n# avoiding 0 weight issues\nanalytic.miss$WTMEC2YR[analytic.miss$WTMEC2YR == 0] &lt;- 0.001\nrequire(DataExplorer)\n#&gt; Loading required package: DataExplorer\nplot_missing(analytic.miss)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ‚Ñπ Please use tidy evaluation idioms with `aes()`.\n#&gt; ‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ‚Ñπ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nDesign creation\n\nanalytic.miss$ID &lt;- 1:nrow(analytic.miss)\nanalytic.miss$miss &lt;- 0\nanalytic.cc &lt;- as.data.frame(na.omit(analytic.miss))\ndim(analytic.cc)\n#&gt; [1] 10557    11\nanalytic.miss$miss[analytic.miss$ID %in% \n                     analytic.cc$ID] &lt;- 0\nw.design0 &lt;- svydesign(id=~SDMVPSU, \n                       strata=~SDMVSTRA, \n                       weights=~WTMEC2YR, \n                       nest=TRUE,\n                       data=analytic.miss)\nw.design &lt;- subset(w.design0, \n                   miss == 0)\nsummary(weights(w.design))\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; 1.000e-03 6.365e+03 1.557e+04 2.725e+04 3.890e+04 2.614e+05\n\nSurvival Analysis within Complex Survey\nKM plot\n\nfit0 &lt;- svykm(Surv(PERMTH_EXM, MORTSTAT) ~ 1, \n              design = w.design)\nplot(fit0)\n\n\n\n\n\n\n\nCox PH\n\nfit &lt;- svycoxph(Surv(PERMTH_EXM, MORTSTAT) ~ \n                  white + female + RIDAGEYR, \n                design = w.design) \npublish(fit)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (117) clusters.\n#&gt; subset(w.design0, miss == 0)\n#&gt;  Variable Units HazardRatio       CI.95   p-value \n#&gt;     white              0.78 [0.66;0.93]   0.00441 \n#&gt;    female              0.61 [0.50;0.75]   &lt; 0.001 \n#&gt;  RIDAGEYR              1.09 [1.08;1.10]   &lt; 0.001\n\nPH assumption\n\ntestPh &lt;- cox.zph(fit) \nprint(testPh)\n#&gt;             chisq df    p\n#&gt; white    7.39e-05  1 0.99\n#&gt; female   2.84e-07  1 1.00\n#&gt; RIDAGEYR 9.16e-05  1 0.99\n#&gt; GLOBAL   1.45e-04  3 1.00\nplot(testPh) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis"
    ]
  },
  {
    "objectID": "nonbinary2a.html",
    "href": "nonbinary2a.html",
    "title": "Survival analysis: NHANES",
    "section": "",
    "text": "The tutorial demonstrates analyzing complex survey data (NHANES) with mortality as a survival outcome. See the tutorial in the previous chapter on linking public-use US mortality data with the NHANES.\nWe will explore the relationship between caffeine consumption and mortality in adults with diabetes using NHANES 1999-2010 datasets. We will follow the following article by Neves et al.¬†(2018).\nLoad required packages\n\n# Load required packages\nlibrary(tableone)\nlibrary(survival)\nlibrary(Publish)\nlibrary(survey)\nlibrary(DataExplorer)\n\nLoad datasets\nLet us load NHANES 1999-2000, 2001-2002, 2003-2004, 2005-2006, 2007-2008, and 2009-2010 datasets and merged all cycles.\n\n# Load\nload(\"Data/nonbinary/coffee.RData\")\nls()\n#&gt; [1] \"dat.analytic1999\" \"dat.analytic2001\" \"dat.analytic2003\" \"dat.analytic2005\"\n#&gt; [5] \"dat.analytic2007\" \"dat.analytic2009\"\n\n# Merge all cycles\ndat.full &lt;- rbind(dat.analytic1999, dat.analytic2001, dat.analytic2003, \n                  dat.analytic2005, dat.analytic2007, dat.analytic2009)\nhead(dat.full)\n\n\n  \n\n\ndim(dat.full)\n#&gt; [1] 62160    26\n\nThe merged dataset contains 62,160 subjects with 26 relevant variables:\n\nid: Respondent sequence/ID number\nsurvey.weight: Full sample 2 year weights\npsu: Masked pseudo-PSU\nstrata: Masked pseudo-stratum\ncaff: Caffeine (exposure variable)\nstime: Follow-up time (time from interview date to death or censoring)\nstatus: Mortality status\nsex: Sex\nage: Age in years\nrace: Race/ethnicity\nincome: Annual household income\nsmoking: Smoking status\ndiabetic.nephropathy: Diabetic nephropathy (no data)\nbmi.cat: BMI - categorical\neducation: Education level\ncarbohyd: Carbohydrate in gm\nalcohol: Alcohol consumption\ndiab.years: Years since diabetes\nhtn: Hypertension\ndaib.retino: Diabetes retinopathy\nmacrovascular: Macrovascular complications\ninsulin: Insulin\nsurvey.cycle: Survey cycle\nphysical.activity: Physical activity\ndiabetes: Diabetes status\ncal.total: Total calories in kcal\nData pre-processing\nEligibility criteria\nThe authors considered adults aged 18 years or more, only diabetic, and total calories between 500 and 3500.\n\n# Total samples in the merged dataset\nnrow(dat.full) # N = 62,160\n#&gt; [1] 62160\n\n# Age &gt;= 18 years\ndat2 &lt;- subset(dat.full, age &gt;= 18) \nnrow(dat2) # N = 35,379\n#&gt; [1] 35379\n\n# With diabetes\ndat3 &lt;- subset(dat2, diabetes == \"Yes\") \nnrow(dat3) # N = 4,687 - numbers don't match with the paper (N = 4,544)\n#&gt; [1] 4687\n\n# Implausible alimentary reports\ndat4 &lt;- subset(dat3, cal.total &gt;= 500 & cal.total &lt;= 3500) \nnrow(dat4) # N = 4,083 - numbers don't match with the paper (N = 3,948)\n#&gt; [1] 4083\n\nComplete case data\nLet us drop missing values in the exposure and outcome:\n\n# Drop missing exposure and outcome\ndat &lt;- dat4[complete.cases(dat4$id),]\ndat &lt;- dat[complete.cases(dat$caff),]\ndat &lt;- dat[complete.cases(dat$status),]\ndat &lt;- dat[complete.cases(dat$stime),] # N = 4,080\ndim(dat)\n#&gt; [1] 4080   26\n\n# Missing plot\nplot_missing(dat)\n\n\n\n\n\n\n\nNow, let us drop variables with high missingness for this exercise. As explained in the Missing data analysis chapter, a better approach could be imputing missing values under the missing at random assumption.\n\n# Drop variables with high missingness\ndat$diabetic.nephropathy &lt;- dat$diab.years &lt;- dat$daib.retino &lt;- dat$income &lt;- NULL\n\n# Complete case data\ndat &lt;- na.omit(dat)\ndim(dat) # N = 3,780\n#&gt; [1] 3780   22\n\nTable 1\nNow, let us create Table 1 stratified by coffee consumption (exposure), separately for males and females, as done in the article.\n\nvars &lt;- c(\"age\", \"race\", \"education\", \"smoking\", \"alcohol\", \"carbohyd\", \"physical.activity\",\n          \"bmi.cat\", \"htn\", \"macrovascular\", \"insulin\", \"survey.cycle\")\n\ntab1a &lt;- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Female\",], \n                        test = F)\ntab1b &lt;- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Male\",], \n                        test = F)\n\ntab1 &lt;- list(Female = tab1a, Male = tab1b)\nprint(tab1, showAllLevels = T, smd = T)\n#&gt; $Female\n#&gt;                        Stratified by caff\n#&gt;                         level                     No consumption &lt;100 mg/day   \n#&gt;   n                                                  203            916        \n#&gt;   age (mean (SD))                                  59.33 (15.14)  62.92 (14.02)\n#&gt;   race (%)              Non-Hispanic White            38 (18.7)     286 (31.2) \n#&gt;                         Non-Hispanic Black            96 (47.3)     284 (31.0) \n#&gt;                         Mexican American              47 (23.2)     228 (24.9) \n#&gt;                         Other Hispanic                10 ( 4.9)      80 ( 8.7) \n#&gt;                         Other race                    12 ( 5.9)      38 ( 4.1) \n#&gt;   education (%)         Less than 9th grade           58 (28.6)     238 (26.0) \n#&gt;                         9-11th grade                  48 (23.6)     182 (19.9) \n#&gt;                         High school grade             43 (21.2)     224 (24.5) \n#&gt;                         Some college                  39 (19.2)     209 (22.8) \n#&gt;                         College graduate or above     15 ( 7.4)      63 ( 6.9) \n#&gt;   smoking (%)           Never smoker                 143 (70.4)     598 (65.3) \n#&gt;                         Current smoker                24 (11.8)     127 (13.9) \n#&gt;                         Former smoker                 36 (17.7)     191 (20.9) \n#&gt;   alcohol (%)           No consumption               192 (94.6)     826 (90.2) \n#&gt;                         &lt;20 grams/day                  4 ( 2.0)      69 ( 7.5) \n#&gt;                         20+ grams/day                  7 ( 3.4)      21 ( 2.3) \n#&gt;   carbohyd (mean (SD))                            176.17 (73.88) 190.64 (73.34)\n#&gt;   physical.activity (%) Low                           95 (46.8)     393 (42.9) \n#&gt;                         Intermediate                  59 (29.1)     309 (33.7) \n#&gt;                         High                          49 (24.1)     214 (23.4) \n#&gt;   bmi.cat (%)           &lt;20.0                          4 ( 2.0)       6 ( 0.7) \n#&gt;                         20.0 to &lt;25.0                 24 (11.8)     107 (11.7) \n#&gt;                         25.0 to &lt;30.0                 35 (17.2)     254 (27.7) \n#&gt;                         30.0 to &lt;35.0                 45 (22.2)     263 (28.7) \n#&gt;                         35.0 to &lt;40.0                 47 (23.2)     138 (15.1) \n#&gt;                         40.0+                         48 (23.6)     148 (16.2) \n#&gt;   htn (%)               No                            58 (28.6)     273 (29.8) \n#&gt;                         Yes                          145 (71.4)     643 (70.2) \n#&gt;   macrovascular (%)     No                           166 (81.8)     758 (82.8) \n#&gt;                         Yes                           37 (18.2)     158 (17.2) \n#&gt;   insulin (%)           No                           156 (76.8)     755 (82.4) \n#&gt;                         Yes                           47 (23.2)     161 (17.6) \n#&gt;   survey.cycle (%)      1999-00                       45 (22.2)     111 (12.1) \n#&gt;                         2001-02                       45 (22.2)      98 (10.7) \n#&gt;                         2003-04                       26 (12.8)     127 (13.9) \n#&gt;                         2005-06                       20 ( 9.9)     154 (16.8) \n#&gt;                         2007-08                       37 (18.2)     198 (21.6) \n#&gt;                         2009-10                       30 (14.8)     228 (24.9) \n#&gt;                        Stratified by caff\n#&gt;                         100-200 mg/day 200+ mg/day    SMD   \n#&gt;   n                        424            332               \n#&gt;   age (mean (SD))        62.04 (13.75)  60.19 (13.10)  0.150\n#&gt;   race (%)                 154 (36.3)     189 (56.9)   0.527\n#&gt;                            108 (25.5)      40 (12.0)        \n#&gt;                            111 (26.2)      69 (20.8)        \n#&gt;                             30 ( 7.1)      20 ( 6.0)        \n#&gt;                             21 ( 5.0)      14 ( 4.2)        \n#&gt;   education (%)             79 (18.6)      56 (16.9)   0.203\n#&gt;                             89 (21.0)      76 (22.9)        \n#&gt;                            109 (25.7)      81 (24.4)        \n#&gt;                            109 (25.7)      89 (26.8)        \n#&gt;                             38 ( 9.0)      30 ( 9.0)        \n#&gt;   smoking (%)              238 (56.1)     136 (41.0)   0.377\n#&gt;                             81 (19.1)     119 (35.8)        \n#&gt;                            105 (24.8)      77 (23.2)        \n#&gt;   alcohol (%)              370 (87.3)     283 (85.2)   0.211\n#&gt;                             38 ( 9.0)      35 (10.5)        \n#&gt;                             16 ( 3.8)      14 ( 4.2)        \n#&gt;   carbohyd (mean (SD))  193.79 (72.18) 203.36 (76.05)  0.190\n#&gt;   physical.activity (%)    179 (42.2)     148 (44.6)   0.093\n#&gt;                            151 (35.6)     117 (35.2)        \n#&gt;                             94 (22.2)      67 (20.2)        \n#&gt;   bmi.cat (%)                7 ( 1.7)       4 ( 1.2)   0.250\n#&gt;                             40 ( 9.4)      44 (13.3)        \n#&gt;                            109 (25.7)      80 (24.1)        \n#&gt;                            133 (31.4)      82 (24.7)        \n#&gt;                             73 (17.2)      63 (19.0)        \n#&gt;                             62 (14.6)      59 (17.8)        \n#&gt;   htn (%)                  123 (29.0)     110 (33.1)   0.052\n#&gt;                            301 (71.0)     222 (66.9)        \n#&gt;   macrovascular (%)        352 (83.0)     266 (80.1)   0.042\n#&gt;                             72 (17.0)      66 (19.9)        \n#&gt;   insulin (%)              340 (80.2)     252 (75.9)   0.094\n#&gt;                             84 (19.8)      80 (24.1)        \n#&gt;   survey.cycle (%)          42 ( 9.9)      46 (13.9)   0.305\n#&gt;                             53 (12.5)      48 (14.5)        \n#&gt;                             63 (14.9)      46 (13.9)        \n#&gt;                             59 (13.9)      43 (13.0)        \n#&gt;                            111 (26.2)      82 (24.7)        \n#&gt;                             96 (22.6)      67 (20.2)        \n#&gt; \n#&gt; $Male\n#&gt;                        Stratified by caff\n#&gt;                         level                     No consumption &lt;100 mg/day   \n#&gt;   n                                                  168            707        \n#&gt;   age (mean (SD))                                  60.95 (12.68)  62.86 (12.88)\n#&gt;   race (%)              Non-Hispanic White            35 (20.8)     238 (33.7) \n#&gt;                         Non-Hispanic Black            78 (46.4)     209 (29.6) \n#&gt;                         Mexican American              37 (22.0)     190 (26.9) \n#&gt;                         Other Hispanic                13 ( 7.7)      48 ( 6.8) \n#&gt;                         Other race                     5 ( 3.0)      22 ( 3.1) \n#&gt;   education (%)         Less than 9th grade           50 (29.8)     194 (27.4) \n#&gt;                         9-11th grade                  41 (24.4)     135 (19.1) \n#&gt;                         High school grade             31 (18.5)     145 (20.5) \n#&gt;                         Some college                  30 (17.9)     136 (19.2) \n#&gt;                         College graduate or above     16 ( 9.5)      97 (13.7) \n#&gt;   smoking (%)           Never smoker                  72 (42.9)     281 (39.7) \n#&gt;                         Current smoker                34 (20.2)     155 (21.9) \n#&gt;                         Former smoker                 62 (36.9)     271 (38.3) \n#&gt;   alcohol (%)           No consumption               131 (78.0)     539 (76.2) \n#&gt;                         &lt;20 grams/day                 16 ( 9.5)      96 (13.6) \n#&gt;                         20+ grams/day                 21 (12.5)      72 (10.2) \n#&gt;   carbohyd (mean (SD))                            195.01 (85.93) 213.00 (80.40)\n#&gt;   physical.activity (%) Low                           63 (37.5)     230 (32.5) \n#&gt;                         Intermediate                  54 (32.1)     284 (40.2) \n#&gt;                         High                          51 (30.4)     193 (27.3) \n#&gt;   bmi.cat (%)           &lt;20.0                          5 ( 3.0)       4 ( 0.6) \n#&gt;                         20.0 to &lt;25.0                 26 (15.5)      95 (13.4) \n#&gt;                         25.0 to &lt;30.0                 53 (31.5)     276 (39.0) \n#&gt;                         30.0 to &lt;35.0                 46 (27.4)     185 (26.2) \n#&gt;                         35.0 to &lt;40.0                 21 (12.5)      95 (13.4) \n#&gt;                         40.0+                         17 (10.1)      52 ( 7.4) \n#&gt;   htn (%)               No                            50 (29.8)     279 (39.5) \n#&gt;                         Yes                          118 (70.2)     428 (60.5) \n#&gt;   macrovascular (%)     No                           120 (71.4)     526 (74.4) \n#&gt;                         Yes                           48 (28.6)     181 (25.6) \n#&gt;   insulin (%)           No                           135 (80.4)     571 (80.8) \n#&gt;                         Yes                           33 (19.6)     136 (19.2) \n#&gt;   survey.cycle (%)      1999-00                       36 (21.4)      72 (10.2) \n#&gt;                         2001-02                       28 (16.7)      92 (13.0) \n#&gt;                         2003-04                       21 (12.5)     104 (14.7) \n#&gt;                         2005-06                       18 (10.7)     115 (16.3) \n#&gt;                         2007-08                       36 (21.4)     168 (23.8) \n#&gt;                         2009-10                       29 (17.3)     156 (22.1) \n#&gt;                        Stratified by caff\n#&gt;                         100-200 mg/day 200+ mg/day    SMD   \n#&gt;   n                        461            569               \n#&gt;   age (mean (SD))        61.67 (13.66)  61.70 (12.40)  0.074\n#&gt;   race (%)                 214 (46.4)     344 (60.5)   0.575\n#&gt;                             92 (20.0)      67 (11.8)        \n#&gt;                             90 (19.5)     115 (20.2)        \n#&gt;                             54 (11.7)      26 ( 4.6)        \n#&gt;                             11 ( 2.4)      17 ( 3.0)        \n#&gt;   education (%)             98 (21.3)      97 (17.0)   0.269\n#&gt;                             79 (17.1)      95 (16.7)        \n#&gt;                            104 (22.6)     125 (22.0)        \n#&gt;                            100 (21.7)     161 (28.3)        \n#&gt;                             80 (17.4)      91 (16.0)        \n#&gt;   smoking (%)              162 (35.1)     153 (26.9)   0.216\n#&gt;                            109 (23.6)     196 (34.4)        \n#&gt;                            190 (41.2)     220 (38.7)        \n#&gt;   alcohol (%)              349 (75.7)     427 (75.0)   0.080\n#&gt;                             63 (13.7)      80 (14.1)        \n#&gt;                             49 (10.6)      62 (10.9)        \n#&gt;   carbohyd (mean (SD))  225.58 (87.51) 237.26 (85.11)  0.273\n#&gt;   physical.activity (%)    153 (33.2)     195 (34.3)   0.099\n#&gt;                            177 (38.4)     199 (35.0)        \n#&gt;                            131 (28.4)     175 (30.8)        \n#&gt;   bmi.cat (%)                4 ( 0.9)       4 ( 0.7)   0.181\n#&gt;                             61 (13.2)      80 (14.1)        \n#&gt;                            169 (36.7)     187 (32.9)        \n#&gt;                            132 (28.6)     146 (25.7)        \n#&gt;                             64 (13.9)      95 (16.7)        \n#&gt;                             31 ( 6.7)      57 (10.0)        \n#&gt;   htn (%)                  190 (41.2)     234 (41.1)   0.126\n#&gt;                            271 (58.8)     335 (58.9)        \n#&gt;   macrovascular (%)        353 (76.6)     412 (72.4)   0.066\n#&gt;                            108 (23.4)     157 (27.6)        \n#&gt;   insulin (%)              375 (81.3)     449 (78.9)   0.032\n#&gt;                             86 (18.7)     120 (21.1)        \n#&gt;   survey.cycle (%)          48 (10.4)      64 (11.2)   0.284\n#&gt;                             59 (12.8)      69 (12.1)        \n#&gt;                             48 (10.4)     106 (18.6)        \n#&gt;                             70 (15.2)      65 (11.4)        \n#&gt;                            112 (24.3)     130 (22.8)        \n#&gt;                            124 (26.9)     135 (23.7)\n\nSurvey design\nThe paper analyzed the data separately for males and females. Let us create the survey design:\n\n# Revised weight - weight divided by 6 cycles\ndat.full$svy.weight &lt;- dat.full$survey.weight/6 \ndat$svy.weight &lt;- dat$survey.weight/6 \nsummary(dat$svy.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   223.2  1760.2  3349.3  4555.9  5817.8 25347.4\n\n# Create an indicator variable\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$id %in% dat$id] &lt;- 0\n\n# Set up the design\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~svy.weight, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design\nw.design1 &lt;- subset(w.design0, miss == 0)\n\n# Subset the design for females\nw.design.f &lt;- subset(w.design1, sex == \"Female\")\ndim(w.design.f)\n#&gt; [1] 1875   28\n\n# Subset the design for males\nw.design.m &lt;- subset(w.design1, sex == \"Male\")\ndim(w.design.m)\n#&gt; [1] 1905   28\n\nKaplan-Meier plot\nLet us create the Kaplan-Meier plot for males and females:\n\n# KM for females\nfit0.f &lt;- svykm(Surv(stime, status) ~ caff, design = w.design.f)\nplot(fit0.f)\n\n\n\n\n\n\n\n# KM for males\nfit0.m &lt;- svykm(Surv(stime, status) ~ caff, design = w.design.m)\nplot(fit0.m)\n\n\n\n\n\n\n\nCox PH\nNow, we will fit the Cox proportional hazards model, separately for males and females. Let us run the unadjusted model first.\n\n# Unadjusted Cox PH for females\nfit1.f &lt;- svycoxph(Surv(stime, status) ~ caff, design = w.design.f)\npublish(fit1.f)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Female\")\n#&gt;  Variable          Units HazardRatio       CI.95   p-value \n#&gt;      caff No consumption         Ref                       \n#&gt;              &lt;100 mg/day        0.80 [0.62;1.03]   0.08543 \n#&gt;           100-200 mg/day        0.58 [0.42;0.80]   &lt; 0.001 \n#&gt;              200+ mg/day        0.61 [0.43;0.86]   0.00507\n\n# Unadjusted Cox PH for males\nfit1.m &lt;- svycoxph(Surv(stime, status) ~ caff, design = w.design.m)\npublish(fit1.m)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Male\")\n#&gt;  Variable          Units HazardRatio       CI.95 p-value \n#&gt;      caff No consumption         Ref                     \n#&gt;              &lt;100 mg/day        1.20 [0.88;1.63]   0.246 \n#&gt;           100-200 mg/day        1.10 [0.75;1.59]   0.632 \n#&gt;              200+ mg/day        1.10 [0.76;1.58]   0.607\n\nNow, we will fit the Cox PH model, adjusting for covariates.\n\n# Covariate adjusted Cox PH for females\nfit2.f &lt;- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.f)\npublish(fit2.f)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Female\")\n#&gt;           Variable                     Units HazardRatio       CI.95   p-value \n#&gt;               caff            No consumption         Ref                       \n#&gt;                                  &lt;100 mg/day        0.62 [0.46;0.84]   0.00166 \n#&gt;                               100-200 mg/day        0.44 [0.30;0.64]   &lt; 0.001 \n#&gt;                                  200+ mg/day        0.43 [0.28;0.66]   &lt; 0.001 \n#&gt;                age                                  1.08 [1.07;1.09]   &lt; 0.001 \n#&gt;               race        Non-Hispanic White         Ref                       \n#&gt;                           Non-Hispanic Black        0.81 [0.65;1.01]   0.06322 \n#&gt;                             Mexican American        0.79 [0.61;1.03]   0.07976 \n#&gt;                               Other Hispanic        0.65 [0.41;1.01]   0.05634 \n#&gt;                                   Other race        0.64 [0.38;1.08]   0.09394 \n#&gt;          education       Less than 9th grade         Ref                       \n#&gt;                                 9-11th grade        1.15 [0.84;1.57]   0.37396 \n#&gt;                            High school grade        1.03 [0.75;1.41]   0.84759 \n#&gt;                                 Some college        1.06 [0.79;1.44]   0.69484 \n#&gt;                    College graduate or above        0.61 [0.40;0.93]   0.02192 \n#&gt;            smoking              Never smoker         Ref                       \n#&gt;                               Current smoker        1.84 [1.42;2.38]   &lt; 0.001 \n#&gt;                                Former smoker        1.17 [0.95;1.46]   0.14379 \n#&gt;            alcohol            No consumption         Ref                       \n#&gt;                                &lt;20 grams/day        0.94 [0.65;1.36]   0.75056 \n#&gt;                                20+ grams/day        1.05 [0.58;1.93]   0.86786 \n#&gt;           carbohyd                                  1.00 [1.00;1.00]   0.69680 \n#&gt;  physical.activity                       Low         Ref                       \n#&gt;                                 Intermediate        0.73 [0.57;0.92]   0.00834 \n#&gt;                                         High        0.66 [0.49;0.89]   0.00638 \n#&gt;            bmi.cat                     &lt;20.0         Ref                       \n#&gt;                                20.0 to &lt;25.0        0.39 [0.21;0.73]   0.00301 \n#&gt;                                25.0 to &lt;30.0        0.33 [0.19;0.57]   &lt; 0.001 \n#&gt;                                30.0 to &lt;35.0        0.30 [0.17;0.54]   &lt; 0.001 \n#&gt;                                35.0 to &lt;40.0        0.26 [0.13;0.49]   &lt; 0.001 \n#&gt;                                        40.0+        0.31 [0.16;0.62]   &lt; 0.001 \n#&gt;                htn                        No         Ref                       \n#&gt;                                          Yes        1.00 [0.80;1.24]   0.98084 \n#&gt;      macrovascular                        No         Ref                       \n#&gt;                                          Yes        1.85 [1.48;2.32]   &lt; 0.001 \n#&gt;            insulin                        No         Ref                       \n#&gt;                                          Yes        1.87 [1.50;2.32]   &lt; 0.001 \n#&gt;       survey.cycle                   1999-00         Ref                       \n#&gt;                                      2001-02        0.58 [0.43;0.78]   &lt; 0.001 \n#&gt;                                      2003-04        0.75 [0.54;1.04]   0.08384 \n#&gt;                                      2005-06        0.81 [0.57;1.15]   0.24147 \n#&gt;                                      2007-08        0.88 [0.60;1.28]   0.49860 \n#&gt;                                      2009-10        0.76 [0.56;1.04]   0.08292\n\n# Covariate adjusted Cox PH for males\nfit2.m &lt;- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.m)\npublish(fit2.m)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Male\")\n#&gt;           Variable                     Units HazardRatio       CI.95   p-value \n#&gt;               caff            No consumption         Ref                       \n#&gt;                                  &lt;100 mg/day        1.06 [0.75;1.49]   0.74199 \n#&gt;                               100-200 mg/day        1.04 [0.69;1.57]   0.84938 \n#&gt;                                  200+ mg/day        1.05 [0.71;1.54]   0.81249 \n#&gt;                age                                  1.08 [1.06;1.09]   &lt; 0.001 \n#&gt;               race        Non-Hispanic White         Ref                       \n#&gt;                           Non-Hispanic Black        0.75 [0.61;0.91]   0.00423 \n#&gt;                             Mexican American        0.68 [0.52;0.88]   0.00348 \n#&gt;                               Other Hispanic        0.83 [0.51;1.36]   0.46367 \n#&gt;                                   Other race        1.03 [0.60;1.76]   0.91073 \n#&gt;          education       Less than 9th grade         Ref                       \n#&gt;                                 9-11th grade        1.47 [1.10;1.96]   0.00846 \n#&gt;                            High school grade        1.03 [0.80;1.33]   0.81768 \n#&gt;                                 Some college        1.02 [0.76;1.36]   0.91606 \n#&gt;                    College graduate or above        0.68 [0.47;0.98]   0.03833 \n#&gt;            smoking              Never smoker         Ref                       \n#&gt;                               Current smoker        1.90 [1.38;2.62]   &lt; 0.001 \n#&gt;                                Former smoker        1.03 [0.85;1.26]   0.74101 \n#&gt;            alcohol            No consumption         Ref                       \n#&gt;                                &lt;20 grams/day        0.95 [0.74;1.21]   0.67612 \n#&gt;                                20+ grams/day        1.14 [0.83;1.58]   0.42116 \n#&gt;           carbohyd                                  1.00 [1.00;1.00]   0.37461 \n#&gt;  physical.activity                       Low         Ref                       \n#&gt;                                 Intermediate        0.64 [0.52;0.80]   &lt; 0.001 \n#&gt;                                         High        0.69 [0.56;0.87]   0.00137 \n#&gt;            bmi.cat                     &lt;20.0         Ref                       \n#&gt;                                20.0 to &lt;25.0        0.25 [0.11;0.58]   0.00134 \n#&gt;                                25.0 to &lt;30.0        0.20 [0.09;0.48]   &lt; 0.001 \n#&gt;                                30.0 to &lt;35.0        0.19 [0.08;0.45]   &lt; 0.001 \n#&gt;                                35.0 to &lt;40.0        0.25 [0.10;0.64]   0.00380 \n#&gt;                                        40.0+        0.26 [0.11;0.60]   0.00160 \n#&gt;                htn                        No         Ref                       \n#&gt;                                          Yes        1.18 [0.98;1.42]   0.08442 \n#&gt;      macrovascular                        No         Ref                       \n#&gt;                                          Yes        1.40 [1.17;1.66]   &lt; 0.001 \n#&gt;            insulin                        No         Ref                       \n#&gt;                                          Yes        1.48 [1.21;1.81]   &lt; 0.001 \n#&gt;       survey.cycle                   1999-00         Ref                       \n#&gt;                                      2001-02        1.37 [1.04;1.80]   0.02569 \n#&gt;                                      2003-04        0.98 [0.68;1.43]   0.93245 \n#&gt;                                      2005-06        1.07 [0.77;1.48]   0.67918 \n#&gt;                                      2007-08        1.05 [0.78;1.41]   0.75981 \n#&gt;                                      2009-10        0.83 [0.58;1.18]   0.29625\n\nThe adjusted results are approximately the same as in Table 2 of the article. Caffeine consumption was associated with mortality among women but not among men.\nPH assumption\nNow, we will check the proportional hazard assumption.\n\n# PH assumption among females\ncox.zph(fit2.f)\n#&gt;                      chisq df    p\n#&gt; caff              3.65e-03  3 1.00\n#&gt; age               2.15e-06  1 1.00\n#&gt; race              2.87e-03  4 1.00\n#&gt; education         2.42e-03  4 1.00\n#&gt; smoking           2.44e-03  2 1.00\n#&gt; alcohol           1.14e-03  2 1.00\n#&gt; carbohyd          7.38e-04  1 0.98\n#&gt; physical.activity 3.34e-03  2 1.00\n#&gt; bmi.cat           2.21e-03  5 1.00\n#&gt; htn               1.33e-03  1 0.97\n#&gt; macrovascular     4.24e-04  1 0.98\n#&gt; insulin           2.26e-04  1 0.99\n#&gt; survey.cycle      3.49e-03  5 1.00\n#&gt; GLOBAL            2.55e-02 32 1.00\n\n# PH assumption among males\ncox.zph(fit2.m)\n#&gt;                      chisq df    p\n#&gt; caff              2.70e-03  3 1.00\n#&gt; age               2.45e-03  1 0.96\n#&gt; race              2.40e-03  4 1.00\n#&gt; education         1.13e-03  4 1.00\n#&gt; smoking           1.67e-03  2 1.00\n#&gt; alcohol           1.50e-03  2 1.00\n#&gt; carbohyd          1.33e-03  1 0.97\n#&gt; physical.activity 7.73e-03  2 1.00\n#&gt; bmi.cat           3.60e-03  5 1.00\n#&gt; htn               4.45e-06  1 1.00\n#&gt; macrovascular     4.32e-06  1 1.00\n#&gt; insulin           9.24e-05  1 0.99\n#&gt; survey.cycle      2.21e-03  5 1.00\n#&gt; GLOBAL            2.78e-02 32 1.00\n\nThe large p-values indicate that the proportional hazard assumption was met for both models.",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis: NHANES"
    ]
  },
  {
    "objectID": "nonbinary3.html",
    "href": "nonbinary3.html",
    "title": "Poisson",
    "section": "",
    "text": "In this tutorial, we will see how to use Poisson and negative binomial regression. In practice, we use Poisson regression to model a count outcome. Note that we assume the mean is equal to the variance in Poisson. When the variance is greater than what‚Äôs assumed by the model, overdispersion occurs. Poisson regression of overdispersed data leads to under-estimated or deflated standard errors, which leads to inflated test statistics and p-values. We can use negative binomial regression to model overdispersed data.\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(Publish)\nrequire(survey)\n\nData\n\n# Load\nload(\"Data/nonbinary/OAcvd.RData\")\n\n# Survey weight\nanalytic2$weight &lt;- analytic2$weight/3\n\n# Make fruit.cont as a numeric variable\nanalytic2$fruit.cont &lt;- as.numeric(as.character(analytic2$fruit.cont))\n\n# Make fruit.cont as a integer/count variable\nanalytic2$fruit.cont &lt;- floor(analytic2$fruit.cont) # round\n\n# Factor variables using lapply\nvar.names &lt;- c(\"age\", \"sex\", \"income\", \"race\", \"bmicat\", \"phyact\", \"smoke\",\n               \"fruit\", \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic2[var.names] &lt;- lapply(analytic2[var.names] , factor)\nstr(analytic2)\n#&gt; 'data.frame':    21623 obs. of  17 variables:\n#&gt;  $ CVD       : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ age       : Factor w/ 4 levels \"20-39 years\",..: 2 3 4 1 3 1 3 1 3 3 ...\n#&gt;  $ sex       : Factor w/ 2 levels \"Female\",\"Male\": 2 1 1 1 2 1 2 1 1 2 ...\n#&gt;  $ income    : Factor w/ 4 levels \"$29,999 or less\",..: 3 4 1 2 1 2 2 3 3 4 ...\n#&gt;  $ race      : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ bmicat    : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 1 2 2 2 1 2 1 2 2 ...\n#&gt;  $ phyact    : Factor w/ 3 levels \"Active\",\"Inactive\",..: 3 1 1 2 1 3 2 3 2 3 ...\n#&gt;  $ smoke     : Factor w/ 3 levels \"Current smoker\",..: 2 3 3 2 2 2 2 2 3 2 ...\n#&gt;  $ fruit     : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 3 2 1 3 1 3 2 2 ...\n#&gt;  $ painmed   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 1 1 ...\n#&gt;  $ ht        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ copd      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ diab      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ edu       : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 2 2 1 4 4 4 1 4 4 1 ...\n#&gt;  $ weight    : num  4.8 13.29 4.43 11.1 9.61 ...\n#&gt;  $ OA        : Factor w/ 2 levels \"Control\",\"OA\": 2 1 2 1 1 1 2 1 1 1 ...\n#&gt;  $ fruit.cont: num  3 4 8 3 2 10 1 8 5 3 ...\n#&gt;  - attr(*, \"na.action\")= 'omit' Named int [1:219757] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   ..- attr(*, \"names\")= chr [1:219757] \"3\" \"5\" \"7\" \"9\" ...\n\nSurvey weighted summary\n\n# Survey design\nw.design &lt;- svydesign(id=~1, weights=~weight, data=analytic2)\n\n# Cross-tabulation\nxtabs(~fruit.cont, analytic2)\n#&gt; fruit.cont\n#&gt;    0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n#&gt;  407 1628 3304 4261 3759 2887 1980 1288  809  480  303  174  109   80   51   25 \n#&gt;   16   17   18   19   20   21   22   23   24   25   26   27   29   42 \n#&gt;   21   14    9    4    5    4    2    6    7    1    2    1    1    1\nsvytable(~fruit.cont, w.design)\n#&gt; fruit.cont\n#&gt;            0            1            2            3            4            5 \n#&gt;   9085.00222  39974.10444  88216.88222 119206.45778 108665.00556  82149.23111 \n#&gt;            6            7            8            9           10           11 \n#&gt;  53791.70444  35463.49111  22593.73000  13023.83556   8878.24778   4520.85000 \n#&gt;           12           13           14           15           16           17 \n#&gt;   3476.42667   2210.76000   1911.70556    973.93444    571.37667    414.44444 \n#&gt;           18           19           20           21           22           23 \n#&gt;    270.82556     77.76222    253.13000     24.74778     34.17444    185.70778 \n#&gt;           24           25           26           27           29           42 \n#&gt;    154.67778    109.88444    149.81222     89.61444     14.91111     93.76889\nsvyhist(~fruit.cont, w.design)\n\n\n\n\n\n\nsvyby(~fruit.cont, ~phyact, w.design, svymean, deff = TRUE)\n\n\n  \n\n\n\nPoisson regression\nLet us fit the traditional (not design-adjusted) Poisson regression using the glm function:\n\nrequire(jtools)\n#&gt; Loading required package: jtools\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Poisson regression - crude\nfit1 &lt;- glm(fruit.cont ~phyact2, data=analytic2, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nœá¬≤(2)\n1219.347\n\n\np\n0.000\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.055\n\n\nPseudo-R¬≤ (McFadden)\n0.012\n\n\nAIC\n98570.197\n\n\nBIC\n98594.142\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nz val.\np\n\n\n\n(Intercept)\n1.337\n1.328\n1.347\n267.387\n0.000\n\n\nphyact2Active\n0.274\n0.259\n0.289\n34.983\n0.000\n\n\nphyact2Moderate\n0.136\n0.120\n0.152\n16.741\n0.000\n\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n# Poisson regression - adjusted for covariates\nfit2 &lt;- glm(fruit.cont ~ phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, data=analytic2, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nœá¬≤(17)\n2984.005\n\n\np\n0.000\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.130\n\n\nPseudo-R¬≤ (McFadden)\n0.030\n\n\nAIC\n96835.540\n\n\nBIC\n96979.207\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nz val.\np\nVIF\n\n\n\n(Intercept)\n1.156\n1.123\n1.189\n68.773\n0.000\nNA\n\n\nphyact2Active\n0.250\n0.235\n0.266\n31.432\n0.000\n1.040\n\n\nphyact2Moderate\n0.112\n0.095\n0.128\n13.640\n0.000\n1.040\n\n\nage40-49 years\n0.027\n0.010\n0.043\n3.196\n0.001\n1.102\n\n\nage50-59 years\n0.083\n0.066\n0.100\n9.400\n0.000\n1.102\n\n\nage60-64 years\n0.127\n0.103\n0.151\n10.287\n0.000\n1.102\n\n\nsexMale\n-0.173\n-0.187\n-0.160\n-25.108\n0.000\n1.082\n\n\nincome$30,000-$49,999\n0.039\n0.017\n0.060\n3.557\n0.000\n1.144\n\n\nincome$50,000-$79,999\n0.050\n0.030\n0.070\n4.923\n0.000\n1.144\n\n\nincome$80,000 or more\n0.083\n0.062\n0.103\n7.964\n0.000\n1.144\n\n\nraceWhite\n-0.003\n-0.024\n0.018\n-0.240\n0.811\n1.077\n\n\nbmicatOverweight\n-0.021\n-0.035\n-0.008\n-3.054\n0.002\n1.096\n\n\nbmicatUnderweight\n-0.001\n-0.034\n0.033\n-0.030\n0.976\n1.096\n\n\nsmokeFormer smoker\n0.131\n0.114\n0.148\n15.050\n0.000\n1.132\n\n\nsmokeNever smoker\n0.181\n0.163\n0.199\n19.436\n0.000\n1.132\n\n\nedu2nd grad.\n0.040\n0.016\n0.064\n3.291\n0.001\n1.125\n\n\neduOther 2nd grad.\n0.051\n0.020\n0.083\n3.201\n0.001\n1.125\n\n\neduPost-2nd grad.\n0.122\n0.101\n0.144\n11.219\n0.000\n1.125\n\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nSurvey weighted Poisson regression\nNow, let‚Äôs fit the design-adjusted Poisson regression using the svyglm function:\n\nrequire(jtools)\n# Design\nw.design &lt;- update (w.design , phyact2=relevel(phyact, ref =\"Inactive\"))\n\n# Design-adjusted Poisson - crude\nfit1 &lt;- svyglm(fruit.cont ~phyact2, design=w.design, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.064\n\n\nPseudo-R¬≤ (McFadden)\n0.035\n\n\nAIC\n99087.436\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.373\n1.354\n1.391\n146.410\n0.000\n\n\nphyact2Active\n0.261\n0.231\n0.292\n16.877\n0.000\n\n\nphyact2Moderate\n0.124\n0.095\n0.154\n8.230\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Design-adjusted Poisson - adjusted for covariates\nfit2&lt;-svyglm(fruit.cont ~phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, design=w.design, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nPseudo-R¬≤ (Cragg-Uhler)\n0.137\n\n\nPseudo-R¬≤ (McFadden)\n0.076\n\n\nAIC\n97795.011\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\nVIF\n\n\n\n(Intercept)\n1.175\n1.113\n1.237\n37.230\n0.000\nNA\n\n\nphyact2Active\n0.241\n0.210\n0.272\n15.211\n0.000\n1.346\n\n\nphyact2Moderate\n0.105\n0.074\n0.135\n6.775\n0.000\n1.346\n\n\nage40-49 years\n0.041\n0.010\n0.071\n2.623\n0.009\n1.447\n\n\nage50-59 years\n0.102\n0.071\n0.133\n6.402\n0.000\n1.447\n\n\nage60-64 years\n0.149\n0.096\n0.203\n5.461\n0.000\n1.447\n\n\nsexMale\n-0.136\n-0.161\n-0.111\n-10.500\n0.000\n1.148\n\n\nincome$30,000-$49,999\n0.025\n-0.015\n0.066\n1.217\n0.224\n1.294\n\n\nincome$50,000-$79,999\n0.031\n-0.008\n0.070\n1.559\n0.119\n1.294\n\n\nincome$80,000 or more\n0.057\n0.018\n0.096\n2.874\n0.004\n1.294\n\n\nraceWhite\n0.023\n-0.012\n0.059\n1.289\n0.197\n1.193\n\n\nbmicatOverweight\n-0.009\n-0.035\n0.017\n-0.708\n0.479\n1.331\n\n\nbmicatUnderweight\n0.029\n-0.033\n0.091\n0.928\n0.353\n1.331\n\n\nsmokeFormer smoker\n0.094\n0.059\n0.128\n5.352\n0.000\n1.474\n\n\nsmokeNever smoker\n0.140\n0.102\n0.177\n7.290\n0.000\n1.474\n\n\nedu2nd grad.\n0.026\n-0.020\n0.072\n1.093\n0.274\n1.302\n\n\neduOther 2nd grad.\n0.033\n-0.024\n0.090\n1.136\n0.256\n1.302\n\n\neduPost-2nd grad.\n0.129\n0.085\n0.173\n5.807\n0.000\n1.302\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n\nNegative binomial regression\nLet‚Äôs fit the negative binomial regression model using the glm function. Below, we specify the dispersion parameter (theta) is equal to 1, which suggests that the ratio of mean and variance is assumed to be 1.\n\nrequire(MASS)\n#&gt; Loading required package: MASS\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Negative binomial regression - crude\nfit3&lt;- glm(fruit.cont ~ phyact2, data=analytic2, family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#&gt; Waiting for profiling to be done...\n#&gt;                      2.5 % 97.5 %\n#&gt; (Intercept)     3.81  3.76   3.85\n#&gt; phyact2Active   1.32  1.29   1.34\n#&gt; phyact2Moderate 1.15  1.12   1.17\n\n# Negative binomial regression - adjusted for covariates\nfit4&lt;-glm(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, data=analytic2,\n          family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#&gt; Waiting for profiling to be done...\n#&gt;                            2.5 % 97.5 %\n#&gt; (Intercept)           3.17  3.05   3.30\n#&gt; phyact2Active         1.29  1.26   1.31\n#&gt; phyact2Moderate       1.12  1.10   1.14\n#&gt; age40-49 years        1.03  1.01   1.05\n#&gt; age50-59 years        1.08  1.06   1.11\n#&gt; age60-64 years        1.14  1.10   1.17\n#&gt; sexMale               0.84  0.83   0.86\n#&gt; income$30,000-$49,999 1.05  1.02   1.07\n#&gt; income$50,000-$79,999 1.06  1.03   1.08\n#&gt; income$80,000 or more 1.09  1.07   1.12\n#&gt; raceWhite             0.99  0.97   1.02\n#&gt; bmicatOverweight      0.98  0.96   1.00\n#&gt; bmicatUnderweight     0.99  0.95   1.04\n#&gt; smokeFormer smoker    1.14  1.12   1.16\n#&gt; smokeNever smoker     1.20  1.17   1.23\n#&gt; edu2nd grad.          1.04  1.01   1.07\n#&gt; eduOther 2nd grad.    1.05  1.01   1.09\n#&gt; eduPost-2nd grad.     1.13  1.10   1.16\n\nSurvey weighted negative binomial regression\nNow, let‚Äôs fit the design-adjusted negative binomial regression model:\n\nrequire(sjstats)\n#&gt; Loading required package: sjstats\n#&gt; \n#&gt; Attaching package: 'sjstats'\n#&gt; The following object is masked from 'package:survey':\n#&gt; \n#&gt;     cv\n\n# Design-adjusted negative binomial - crude\nfit3&lt;- svyglm.nb(fruit.cont ~phyact2, design=w.design)\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#&gt;                                2.5 %    97.5 %\n#&gt; theta.(Intercept)   28859.34 5797.98 143646.88\n#&gt; eta.(Intercept)         3.95    3.87      4.02\n#&gt; eta.phyact2Active       1.30    1.26      1.34\n#&gt; eta.phyact2Moderate     1.13    1.10      1.17\n\n# Design-adjusted negative binomial - adjusted for covariates\nfit4&lt;-svyglm.nb(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, \n                design=w.design)\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#&gt;                                        2.5 %     97.5 %\n#&gt; theta.(Intercept)         132340.25 15727.48 1113588.86\n#&gt; eta.(Intercept)                3.24     3.04       3.44\n#&gt; eta.phyact2Active              1.27     1.23       1.31\n#&gt; eta.phyact2Moderate            1.11     1.08       1.14\n#&gt; eta.age40-49 years             1.04     1.01       1.07\n#&gt; eta.age50-59 years             1.11     1.07       1.14\n#&gt; eta.age60-64 years             1.16     1.10       1.22\n#&gt; eta.sexMale                    0.87     0.85       0.90\n#&gt; eta.income$30,000-$49,999      1.03     0.99       1.07\n#&gt; eta.income$50,000-$79,999      1.03     0.99       1.07\n#&gt; eta.income$80,000 or more      1.06     1.02       1.10\n#&gt; eta.raceWhite                  1.02     0.99       1.06\n#&gt; eta.bmicatOverweight           0.99     0.97       1.02\n#&gt; eta.bmicatUnderweight          1.03     0.97       1.09\n#&gt; eta.smokeFormer smoker         1.10     1.06       1.14\n#&gt; eta.smokeNever smoker          1.15     1.11       1.19\n#&gt; eta.edu2nd grad.               1.03     0.98       1.07\n#&gt; eta.eduOther 2nd grad.         1.03     0.98       1.09\n#&gt; eta.eduPost-2nd grad.          1.14     1.09       1.19\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Poisson"
    ]
  },
  {
    "objectID": "nonbinaryF.html",
    "href": "nonbinaryF.html",
    "title": "R functions (N)",
    "section": "",
    "text": "The list of new R functions introduced in this non-binary data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\ncox.zph\nsurvival\nTo assess the proportional hazard assumption\n\n\ncoxph\nsurvival\nTo fit cox regression model\n\n\nggforest\nsurvminer\nTo produce a forest plot\n\n\nmultinom\nnnet\nTo fit multinomial models\n\n\npolr\nMASS\nTo fir ordinal logistic and probit regressions\n\n\nSurv\nsurvival\nTo create a survival object\n\n\nsurvdiff\nsurvival\nTo compare survival times between groups\n\n\nsurvfit\nsurvival\nTo create survival curves\n\n\nsvy_vglm\nsvyVGAM\nTo fit design-based generalised linear models\n\n\nsvycoxph\nsurvey\nTo fit cox regression model for complex survey data\n\n\nsvyglm.nb\nsjstats\nNegative binomial model for complex survey data\n\n\nsvykm\nsurvey\nEstimate survival function for complex survey data\n\n\nsvyolr\nsurvey\nOrdinal logistic for complex survey",
    "crumbs": [
      "Complex outcomes",
      "R functions (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html",
    "href": "nonbinaryQ.html",
    "title": "Quiz (N)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html#live-quiz",
    "href": "nonbinaryQ.html#live-quiz",
    "title": "Quiz (N)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html#download-quiz",
    "href": "nonbinaryQ.html#download-quiz",
    "title": "Quiz (N)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "longitudinal.html",
    "href": "longitudinal.html",
    "title": "Longitudinal data",
    "section": "",
    "text": "Background\nThe chapter focuses on longitudinal data analysis. The first dives into mixed effects models, highlighting their importance in studying repeated measurements, especially in longitudinal or clustered data. These models comprise two essential components: fixed effects, which represent universal trends across subjects or clusters, and random effects, capturing individual attributes of each subject or cluster. The tutorial explains their application using datasets, emphasizing model selection metrics and validation methods. The second tutorial introduces the generalized estimating equation (GEE), suitable for modeling non-normally distributed longitudinal data. GEE differentiates from mixed-effects models in its distribution assumptions and its capability to handle various correlation structures. The tutorial illustrates GEE‚Äôs application, emphasizing its advantages over other regression models for repeated measurements, and ends with a comparison between GEE and mixed models‚Äô random effects.",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal.html#background",
    "href": "longitudinal.html#background",
    "title": "Longitudinal data",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal.html#overview-of-tutorials",
    "href": "longitudinal.html#overview-of-tutorials",
    "title": "Longitudinal data",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nLongitudinal data analysis is specialized for handling data collected over time with repeated measurements on the same subjects. It accounts for within-subject correlation, time trends, and subject-specific effects, making it distinct from traditional regression, propensity score analysis, and machine learning approaches we discussed earlier, which are not designed for the longitudinal aspect of the data. Choosing the appropriate analysis method depends on the specific research question and the nature of the data at hand.\n\nMixed effects models\nThis tutorial provides an in-depth look into mixed effects models, which are instrumental in analyzing repeated measurements, especially in longitudinal or clustered data scenarios. Within the context of such models, there are two core components: fixed effects and random effects. Fixed effects depict broad trends applicable universally across subjects or clusters, offering insights such as average student performance across different subjects. In contrast, random effects spotlight the distinct attributes of each subject or cluster, accounting for variables such as the quality of resources or the experience of teachers in schools. Using a dataset, the tutorial demonstrates the application and visualization of linear mixed effects models. Emphasis is also placed on model selection, using metrics like AIC and BIC, and on validating the assumptions of the model using residual plots and QQ-plots.\n\n\nGEE\nThe tutorial introduces the generalized estimating equation (GEE) as a method for modeling longitudinal or clustered data that can be non-normally distributed, such as binary, count, or skewed data. Unlike mixed-effects models that assume a normal distribution for error terms and beta coefficients, GEE is distribution-free but assumes specific correlation structures within subjects or clusters. The tutorial exemplifies the application of GEE using respiratory data with a binary response variable. While logistic regression and Poisson regression handle binary and count data, they do not account for repeated measurement structures, potentially leading to incorrect standard error estimates. GEE allows for various correlation structures, and the tutorial details several types, including independent, exchangeable, AR-1 autoregressive, and unstructured correlations. The GEE models produce consistent estimates even if the correlation structure is misspecified. The tutorial concludes by comparing GEE with random effects in mixed models using several code examples.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal0.html",
    "href": "longitudinal0.html",
    "title": "Concepts (T)",
    "section": "",
    "text": "Longitudinal Data Analysis\nThe section offers a brief overview of longitudinal data analysis, focusing on key concepts and methods. It explains that longitudinal data involves the repeated measurement of variables of interest over time, often referred to as panel data. The example study data is initially in wide-form, with multiple variables for each time point, and then it is reshaped into long-form data for analysis.\nThe analysis of longitudinal data includes various models, such as linear models, mixed-effect models, and marginal models (GEE). Linear models are introduced initially, where each subject has a common slope and intercept. These ideas can be expanded to incorporate random intercepts, random slopes, and combinations of both in mixed-effect models. Model diagnostics, including AIC, BIC, and -loglik, are discussed for model evaluation. The section briefly discussing different correlation structures and highlights the differences in interpretation between mixed models and marginal models. This section also includes an introduction to marginal structual model, which is a GEE model under a situation when treatment-confounder feedback exists.",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#reading-list",
    "href": "longitudinal0.html#reading-list",
    "title": "Concepts (T)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Faraway 2016; Hothorn and Everitt 2014)\nOptional references: (Karim et al. 2021; Cui and Qian 2007)",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#video-lessons",
    "href": "longitudinal0.html#video-lessons",
    "title": "Concepts (T)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nLongitudinal data formatting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: mixed effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: GEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarginal structural model",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#video-lesson-slides",
    "href": "longitudinal0.html#video-lesson-slides",
    "title": "Concepts (T)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nLongitudinal models\n\n\nMarginal structural model",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#links",
    "href": "longitudinal0.html#links",
    "title": "Concepts (T)",
    "section": "Links",
    "text": "Links\nLongitudinal models\n\nGoogle Slides\nPDF Slides\n\nMarginal structural model\n\nGoogle Slides\nPDF Slides\nGitHub page about marginal structural model simulation",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#references",
    "href": "longitudinal0.html#references",
    "title": "Concepts (T)",
    "section": "References",
    "text": "References\n\n\n\n\nCui, Jianwen, and Guoqing Qian. 2007. ‚ÄúSelection of Working Correlation Structure and Best Model in GEE Analyses of Longitudinal Data.‚Äù Communications in Statistics‚ÄîSimulation and Computation¬Æ 36 (5): 987‚Äì96.\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.\n\n\nKarim, Mohammad Ehsanul, Helen Tremlett, Feng Zhu, John Petkau, and Elaine Kingwell. 2021. ‚ÄúDealing with Treatment-Confounder Feedback and Sparse Follow-up in Longitudinal Studies: Application of a Marginal Structural Model in a Multiple Sclerosis Cohort.‚Äù American Journal of Epidemiology 190 (5): 908‚Äì17.",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal1.html",
    "href": "longitudinal1.html",
    "title": "Mixed effects models",
    "section": "",
    "text": "In this section, we will learn about mixed effects models. Mixed effects models are popular choices for modeling repeated measurements, such as longitudinal or clustered data. Examples of longitudinal data include blood pressure measurements taken over time from the same individuals and CD4 count over time from the same individuals. Examples of clustered data include students within schools and patients within hospitals. There are two components in a mixed effects model: fixed effects and random effects:\n\nFixed effects refer to general trends that are applicable to all subjects/clusters. This implies that we might want to investigate how students perform (on average) in different subjects such as math and history. These effects are commonly observed, i.e., fixed, across all schools.\nRandom Effects capture the unique characteristics of each subject/cluster that differentiate them from one another. For instance, some schools may have better resources, more experienced teachers, or a more supportive learning environment. These differences are specific to each school, and we use random effects to capture them.\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(kableExtra)\nrequire(Matrix)\nrequire(jtools)\n\nRepeated measures\n\n\nA useful reference on repeated measures is Section 9.2 of Faraway (2016).\nIn repeated measurement design, the response variable are measured multiple times for each individuals.\nLinear Mixed Effects Models for Repeated Measures Data\n\n\nA useful reference on linear mixed effects models is Section 12.4 of Hothorn and Everitt (2014).\n\n\n\n\n\n\nImportant\n\n\n\n\nThe linear mixed effect models are based on the idea that the correlation of an individual‚Äôs responses depends on some unobserved individual characteristics.\nIn linear mixed effect models, we treat these unobserved characteristics as random effects in our model. If we could conditional on the random effects, then the repeated measurements can be assumed to be independent.\n\n\n\nData\nWe are going to use the BtheB dataset from the HSAUR2 package to explain the linear mixed effect model:\n\nlibrary(HSAUR2)\n#&gt; Loading required package: tools\ndata(\"BtheB\")\nkable(head(BtheB))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nbdi.2m\nbdi.3m\nbdi.5m\nbdi.8m\n\n\n\nNo\n&gt;6m\nTAU\n29\n2\n2\nNA\nNA\n\n\nYes\n&gt;6m\nBtheB\n32\n16\n24\n17\n20\n\n\nYes\n&lt;6m\nTAU\n25\n20\nNA\nNA\nNA\n\n\nNo\n&gt;6m\nBtheB\n21\n17\n16\n10\n9\n\n\nYes\n&gt;6m\nBtheB\n26\n23\nNA\nNA\nNA\n\n\nYes\n&lt;6m\nBtheB\n7\n0\n0\n0\n0\n\n\n\n\n\n\nA typical form of repeated measurement data from a clinical trial data.\nThe individuals are allocated to different treatments then the response Beck Depression Inventory II were taken at baseline, 2, 3, 5, and 8 months\n\n\n## Box-plot of responses at different time points in treatment and control groups\ndata(\"BtheB\", package = \"HSAUR2\")\nlayout(matrix(1:2, nrow = 1))\nylim &lt;- range(BtheB[,grep(\"bdi\", names(BtheB))],na.rm = TRUE)\ntau &lt;- subset(BtheB, treatment == \"TAU\")[, grep(\"bdi\", names(BtheB))]\nboxplot(tau, main = \"Treated as Usual\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\nbtheb &lt;- subset(BtheB, treatment == \"BtheB\")[, grep(\"bdi\", names(BtheB))]\nboxplot(btheb, main = \"Beat the Blues\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\n\n\n\n\n\n\n\n\nThe side-by-side box plots show the distributions of BDI overtime between control (Treated as Usual) and intervention (Beat the Blues) groups.\nAs time goes, drops in BDI are more obvious in intervention which may indicate the intervention is effective.\nRegular model fixed intercept and slope\nTo compare, we start with fixed effect linear model, i.e., a regular linear model without any random effect:\n\n## To analyze the data, we first need to convert the dataset to a analysis-ready form\nBtheB$subject &lt;- factor(rownames(BtheB))\nnobs &lt;- nrow(BtheB)\nBtheB_long &lt;- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \n                                  \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time &lt;- rep(c(2, 3, 5, 8), rep(nobs, 4))\nkable(head(BtheB_long))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n1.2m\nNo\n&gt;6m\nTAU\n29\n1\n2\n2\n\n\n2.2m\nYes\n&gt;6m\nBtheB\n32\n2\n2\n16\n\n\n3.2m\nYes\n&lt;6m\nTAU\n25\n3\n2\n20\n\n\n4.2m\nNo\n&gt;6m\nBtheB\n21\n4\n2\n17\n\n\n5.2m\nYes\n&gt;6m\nBtheB\n26\n5\n2\n23\n\n\n6.2m\nYes\n&lt;6m\nBtheB\n7\n6\n2\n0\n\n\n\n\nunique(BtheB_long$subject)\n#&gt;   [1] 1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18 \n#&gt;  [19] 19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 \n#&gt;  [37] 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 \n#&gt;  [55] 55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 \n#&gt;  [73] 73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 \n#&gt;  [91] 91  92  93  94  95  96  97  98  99  100\n#&gt; 100 Levels: 1 10 100 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 ... 99\nkable(subset(BtheB_long,  subject == 2))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n2.2m\nYes\n&gt;6m\nBtheB\n32\n2\n2\n16\n\n\n2.3m\nYes\n&gt;6m\nBtheB\n32\n2\n3\n24\n\n\n2.5m\nYes\n&gt;6m\nBtheB\n32\n2\n5\n17\n\n\n2.8m\nYes\n&gt;6m\nBtheB\n32\n2\n8\n20\n\n\n\n\nkable(subset(BtheB_long,  subject == 99))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n99.2m\nNo\n&lt;6m\nTAU\n13\n99\n2\n5\n\n\n99.3m\nNo\n&lt;6m\nTAU\n13\n99\n3\n5\n\n\n99.5m\nNo\n&lt;6m\nTAU\n13\n99\n5\n0\n\n\n99.8m\nNo\n&lt;6m\nTAU\n13\n99\n8\n6\n\n\n\n\n\n\nlmfit &lt;- lm(bdi ~ bdi.pre + time + treatment + drug +length, \n            data = BtheB_long, na.action = na.omit)\nrequire(jtools)\nsumm(lmfit)\n\n\n\n\nObservations\n280 (120 missing obs. deleted)\n\n\nDependent variable\nbdi\n\n\nType\nOLS linear regression\n\n\n\n \n\n\n\nF(5,274)\n35.78\n\n\nR¬≤\n0.40\n\n\nAdj. R¬≤\n0.38\n\n\n\n \n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n(Intercept)\n7.32\n1.73\n4.24\n0.00\n\n\nbdi.pre\n0.57\n0.05\n10.44\n0.00\n\n\ntime\n-0.94\n0.24\n-3.97\n0.00\n\n\ntreatmentBtheB\n-3.32\n1.10\n-3.02\n0.00\n\n\ndrugYes\n-3.57\n1.15\n-3.11\n0.00\n\n\nlength&gt;6m\n1.71\n1.11\n1.54\n0.12\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\nRandom intercept but fixed slope\nLet us start with a model with a random intercept but fixed slope. In this case, the resulting regression line for each individual is parallel (for fixed slope) but have different intercepts (for random intercept).\n\n## Fit a random intercept model with lme4 package\nlibrary(\"lme4\")\nBtheB_lmer1 &lt;- lmer(bdi ~ bdi.pre + time + treatment + drug +length \n                    + (1 | subject), data = BtheB_long, \n                    REML = FALSE, na.action = na.omit)\n\nsummary(BtheB_lmer1)\n#&gt; Linear mixed model fit by maximum likelihood  ['lmerMod']\n#&gt; Formula: bdi ~ bdi.pre + time + treatment + drug + length + (1 | subject)\n#&gt;    Data: BtheB_long\n#&gt; \n#&gt;       AIC       BIC    logLik -2*log(L)  df.resid \n#&gt;    1887.5    1916.6    -935.7    1871.5       272 \n#&gt; \n#&gt; Scaled residuals: \n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.6975 -0.5026 -0.0638  0.4124  3.8203 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  subject  (Intercept) 48.78    6.984   \n#&gt;  Residual             25.14    5.014   \n#&gt; Number of obs: 280, groups:  subject, 97\n#&gt; \n#&gt; Fixed effects:\n#&gt;                Estimate Std. Error t value\n#&gt; (Intercept)     5.59239    2.24244   2.494\n#&gt; bdi.pre         0.63968    0.07789   8.212\n#&gt; time           -0.70476    0.14639  -4.814\n#&gt; treatmentBtheB -2.32908    1.67036  -1.394\n#&gt; drugYes        -2.82495    1.72684  -1.636\n#&gt; length&gt;6m       0.19708    1.63832   0.120\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;             (Intr) bdi.pr time   trtmBB drugYs\n#&gt; bdi.pre     -0.682                            \n#&gt; time        -0.238  0.020                     \n#&gt; tretmntBthB -0.390  0.121  0.018              \n#&gt; drugYes     -0.073 -0.237 -0.022 -0.323       \n#&gt; length&gt;6m   -0.243 -0.242 -0.036  0.002  0.157\nsumm(BtheB_lmer1)\n\n\n\n\nObservations\n280\n\n\nDependent variable\nbdi\n\n\nType\nMixed effects linear regression\n\n\n\n \n\n\n\nAIC\n1887.49\n\n\nBIC\n1916.57\n\n\nPseudo-R¬≤ (fixed effects)\n0.39\n\n\nPseudo-R¬≤ (total)\n0.79\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nEst.\nS.E.\nt val.\nd.f.\np\n\n\n\n\n(Intercept)\n5.59\n2.24\n2.49\n108.98\n0.01\n\n\nbdi.pre\n0.64\n0.08\n8.21\n104.08\n0.00\n\n\ntime\n-0.70\n0.15\n-4.81\n199.32\n0.00\n\n\ntreatmentBtheB\n-2.33\n1.67\n-1.39\n97.12\n0.17\n\n\ndrugYes\n-2.82\n1.73\n-1.64\n98.20\n0.11\n\n\nlength&gt;6m\n0.20\n1.64\n0.12\n100.26\n0.90\n\n\n\n\n p values calculated using Satterthwaite d.f.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nsubject\n(Intercept)\n6.98\n\n\nResidual\n\n5.01\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\nsubject\n97\n0.66\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nAIC, BIC, and -loglik etc are goodness-of-fit statistics, which tells you how well the model fits your data. Since there is no standard to tell what values of these statistics are good, without comparison with other models, they have little information to tell.\nFixed effects: this is the standard output we will have in any fixed-effect model. The interpretation of estimated coefficients will be similar to a regular linear model.\nYou may compare the outputs with the regular linear model, then you will find that lm tends to underestimate the SE for estimated coefficients.\n\n\n\n\nlibrary(ggplot2)\nBtheB_longna &lt;- na.omit(BtheB_long)\ndat &lt;- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer1),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, \n                    color=Subject)) + theme_classic() +\n    geom_line() \n\n\n\n\n\n\n\nAs we can see, for each individual, we have different intercepts but the slope over follow-up time is the same. Next, we will fit the model with random intercept and random slope.\nRandom intercept and random slope\nIn the codes below, we fitted a mixed effects model with both random intercept and random slope:\n\n## We can fit a random slope and intercept model using lme4 package and treat variable time as random slope. \nlibrary(\"lme4\")\nBtheB_lmer2 &lt;- lmer(bdi ~ bdi.pre + time + treatment + drug +length + \n                   (time | subject), data = BtheB_long, REML = FALSE, \n                   na.action = na.omit)\n\nsumm(BtheB_lmer2)\n\n\n\n\nObservations\n280\n\n\nDependent variable\nbdi\n\n\nType\nMixed effects linear regression\n\n\n\n \n\n\n\nAIC\n1891.04\n\n\nBIC\n1927.39\n\n\nPseudo-R¬≤ (fixed effects)\n0.39\n\n\nPseudo-R¬≤ (total)\n0.80\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nEst.\nS.E.\nt val.\nd.f.\np\n\n\n\n\n(Intercept)\n5.61\n2.25\n2.50\n106.79\n0.01\n\n\nbdi.pre\n0.64\n0.08\n8.25\n102.78\n0.00\n\n\ntime\n-0.70\n0.15\n-4.56\n57.69\n0.00\n\n\ntreatmentBtheB\n-2.38\n1.67\n-1.42\n97.12\n0.16\n\n\ndrugYes\n-2.87\n1.73\n-1.66\n98.18\n0.10\n\n\nlength&gt;6m\n0.14\n1.64\n0.09\n100.04\n0.93\n\n\n\n\n p values calculated using Satterthwaite d.f.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nsubject\n(Intercept)\n7.12\n\n\nsubject\ntime\n0.43\n\n\nResidual\n\n4.90\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\nsubject\n97\n0.68\n\n\n\n\n\nThe interpretation of the model outputs will be similar to the model with only random intercepts. Let us plot the data:\n\nlibrary(ggplot2)\nBtheB_longna &lt;- na.omit(BtheB_long)\ndat &lt;- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer2),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, color=Subject)) + \n  theme_classic() +\n    geom_line() \n\n\n\n\n\n\n\nFrom the figure, we can see, we have different intercepts and different slopes over follow-up time for each individual.\nChoice among models\nA common question is to ask should I add random slope to our model or random intercept is good enough. We may want to compare the models in terms of AIC and BIC. Smaller values indicate a better model.\n\nlm usually will not be considered as a competitor of lme as they basically apply to different types of data.\nWhen choosing between random intercept and random slope, a quick solution is to fit all possible models then do likelihood ratio tests.\nFor example, I am not sure whether I should use random intercept only or random intercept + random slope. I could fit both model, then do a likelihood ratio test:\n\n\nanova(BtheB_lmer1, BtheB_lmer2)\n\n\n  \n\n\n## The non-significant p-value shows that the second model is not \n## statistically different from the first model. Therefore, adding a \n## random slope is not necessary\n\nThe p-value is greater than 0.05, which indicate that adding a random slope does not make the fitting significantly better. To keep the model simple, we may just use random intercept.\nPrediction of Random Effects\n\nRef: (Hothorn and Everitt 2014) Section 12.5\n\nIf you have noticed in the R output of linear mixed effect model. Random effects are not estimated in the model.\n\nWe could use the fitted model to predict random effects.\nAlso, the predicted random effects can be used to examine the assumptions we have for linear mixed effect model.\n\nThe ranef function is used to predict the random effect in R\n\nqint &lt;- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqint\n#&gt;  [1] -16.36173411  -2.44276827  -3.81655251   3.05333928   3.45142248\n#&gt;  [6]   7.76598376   2.72263773  -7.22484569   6.84680617  -1.02552084\n#&gt; [11]   2.40417048   1.16611114  -8.55346533  -6.25844483  -2.26117896\n#&gt; [16]  -5.23031336   2.52509381  -1.64263857  -1.07523649   3.93689163\n#&gt; [21]   7.66903473 -16.38156595   1.74883174  -1.31871410  -9.02883854\n#&gt; [26]  -2.75150756   2.38884151   2.72740404  -3.41918542   6.27519245\n#&gt; [31]  -4.52154811  -8.67437225  -0.34470208  -0.63972054  -0.08927194\n#&gt; [36]   7.61914282   2.91050412  -2.58455318   4.24637616 -16.01664364\n#&gt; [41]   5.90993779   3.21014012  -7.04631398   3.09608824   4.71327710\n#&gt; [46]  16.70161338   2.26241522   2.26584476  15.77002952   1.75410838\n#&gt; [51]   6.18741474   1.99791717   0.56774362   4.04107968  10.72891321\n#&gt; [56]  -1.54551028  -5.28330207   2.04552526   2.36472056  -1.56879952\n#&gt; [61]   6.63565636   5.45929329  -4.19695096  -6.39632488  -2.21496288\n#&gt; [66]  -0.95956136  -3.96021851   7.17628719  -1.41644518  -4.99763144\n#&gt; [71]  -2.83374092  -1.28015494   8.79345988  -0.33213428   1.43146353\n#&gt; [76]  -3.13316295  -6.52989461   2.51445573   5.93415004   3.42003755\n#&gt; [81]   4.49954804   3.75178702  -6.44832897  12.88173218  -9.36110814\n#&gt; [86]   2.02028152 -18.09960241  -6.53428535   5.06613705  -3.31816912\n#&gt; [91]   7.26187445   0.03103101  -8.14350181  -4.89326763   2.92437698\n#&gt; [96]   5.24835927  -5.96778945\n## predict random effects using the fitted model\n\nCheck assumptions\nRemember, we have two assumptions in the linear mixed effect model with random intercept:\n\nerror term follows normal distribution\nbeta for subject \\(i\\) follows normal distribution\n\nWe have predict the values of random effect and we could extract residuals from the fitted model. Therefore, we can use QQ-plot to check their normality\n\n# Assumption 1\nqres &lt;- residuals(BtheB_lmer1)\nqqnorm(qres, xlim = c(-3, 3), ylim = c(-20, 20), \n       ylab = \"Estimated residuals\",\n       main = \"Residuals\")\nqqline(qres)\n\n\n\n\n\n\n\n# Assumption 2\nqint &lt;- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqqnorm(qint, ylab = \"Estimated random intercepts\", \n       xlim = c(-3, 3), ylim = c(-20, 20),\n       main = \"Random intercepts\")\nqqline(qint)\n\n\n\n\n\n\n\nSince points are almost on the lines, we can say that the normality assumption is met.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.",
    "crumbs": [
      "Longitudinal data",
      "Mixed effects models"
    ]
  },
  {
    "objectID": "longitudinal2.html",
    "href": "longitudinal2.html",
    "title": "GEE",
    "section": "",
    "text": "In this section, we will learn about generalized estimating equation (GEE). GEE is another popular method for modeling longitudinal or clustered data.\n\n\n\n\n\n\nNote\n\n\n\n\nGEE is a population-averaged (e.g., marginal) model, whereas mixed effects models are subject/cluster-specific. For example, we can use GEE when we are interested in exploring the population average effect. On the other hand, when we are interested in individual/cluster-specific effects, we can use the mixed effects models.\nAlso, in contrast to the mixed effects models where we assume error term and beta coefficients for subject \\(i\\) both follow normal distribution, GEE is distribution free. However, in GEE, we assume some correlation structures for within subjects/clusters. Hence, we can use GEE for non-normal data such as skewed data, binary data, or count data.\n\n\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(HSAUR2)\nlibrary(gee)\nlibrary(MuMIn)\nlibrary(geepack)\nlibrary(\"lme4\")\nrequire(afex)\n\nData Description\nIn addition to BtheB dataset in part 1, we used respiratory data from HSAUR2 package to demonstrate the analysis with non-normal responses:\n\nThe response variable in this dataset is status (the respiratory status), which is a binary response\nOther covariates are: treatment, age, gender, the study center\nThe response has been measured at 0, 1, 2, 3, 4 mths for each subject\n\n\ndata(\"respiratory\", package = \"HSAUR2\")\nhead(respiratory)\n\n\n  \n\n\n\nMethods for Non-normal Distributions\n\nref: (Hothorn and Everitt 2014) Section 13.2\nIn addition to normally distributed response variables, the response variable can also follow non-normal distributions, such as binary or count responses.\nWe have learned logistic regression or Poisson regression (glm) to analyze binary and count data but they are not considering the ‚Äúrepeated measurement‚Äù structure.\nThe consequence of ignoring the longitudinal/repeated measurements structure is to have the wrong estimated standard errors for regression coefficients. Hence, our inference will be invalid. However, the glm still gives consistent estimated beta coefficients.\nThere are many correlation structures in modelling GEE.\nWith non-normal responses, different assumptions about these correlation structures can lead to different interpretations of the beta coefficients. Here, we will introduce two different GEE models: marginal model and conditional model.\nMarginal Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.1\n\nRepeated measurement and longitudinal data has responses taken at different time points. Therefore, we could simply review them as many series of cross-sectional data. Each cross-sectional data can be analyzed using glm introduced in previous lectures. Then a correlation structure can be assumed to connect different ‚Äúcross-sections‚Äù. The common correlation structures are:\nAn independent structure\nAn independent structure which assumes the repeated measures are independent. An example of independent structure is basically is an identity matrix:\n\\[\n\\left(\\begin{array}{ccc}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{array}\\right)\n\\]\nAn exchangeable correlation\nAn exchangeable correlation assumes that for the each individual, the correlation between each pair of repeated measurements is the same, i.e., \\(Cor(Y_{ij},Y_{ik})=\\rho\\). In the correlation matrix, only \\(\\rho\\) is unknown. Therefore, it is a single parameter working correlation matrix. An example of exchangeable correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho\\\\\n\\rho & 1 & \\rho\\\\\n\\rho & \\rho & 1\n\\end{array}\\right)\n\\]\nAn AR-1 autoregressive correlation\nDefined as \\(Cor(Y_{ij},Y_{ik})=\\rho^{|k-j|}\\). If two measurements are taken at two closer time points the correlation is higher than these taken at two farther apart time points. It is also a single parameter working correlation matrix. An example of AR-1 correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho^2\\\\\n\\rho & 1 & \\rho\\\\\n\\rho^2 & \\rho & 1\n\\end{array}\\right)\n\\]\nAn unstructured correlation\nDifferent pairs of observation for each individual have different correlations \\(Cor(Y_{ij},Y_{ik})=\\rho_{jk}\\). Assume that each individual has \\(K\\) pairs of measurements, it is a \\(K(K-1)/2\\) parameters working correlation matrix. An example of unstructured correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho_{12} & \\rho_{13}\\\\\n\\rho_{12} & 1 & \\rho_{23}\\\\\n\\rho_{13} & \\rho_{23} & 1\n\\end{array}\\right)\n\\]\n\nSometimes, specifying a ‚Äúright‚Äù correlation matrix is hard. However, the marginal model (usually we use GEE) gives us consistent estimated coefficients even with misspecified correlation structure.\nConditional Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.2\nIn GEE marginal models, the estimated regression coefficients are marginal (or population-averaged) effects. Therefore, the interpretation are at population-level. It is almost impossible to make inference on any specific individual or cluster.\nOne solution is to do conditional models. The random effect approach in the part 1 can be extended to non-Gaussian response.\nGEE models\nAfter the short introduction of two models, let‚Äôs take a look at real examples\n\nref: (Hothorn and Everitt 2014) Section 13.3\nref: (Faraway 2016) Section 10.2\n\nBinary response\nWe started with binary response using respiratory dataset:\n\nlibrary(gee)\ndata(\"respiratory\", package = \"HSAUR2\")\n## Data manipulation\nresp &lt;- subset(respiratory, month &gt; \"0\")\nresp$baseline &lt;- rep(subset(respiratory, month == \"0\")$status, rep(4, 111))\n## Change the response to 0 and 1\nresp$nstat &lt;- as.numeric(resp$status == \"good\")\nresp$month &lt;- resp$month[, drop = TRUE]\n\nNow we will fit a regular glm, i.e., model without random effect or any correlation structures. For binary outcomes, the estimated coefficients are log odds.\n\n## Regular GLM \nresp_glm &lt;- glm(status ~ centre + treatment + gender + baseline+ age, \n                data = resp, family = \"binomial\")\nsummary(resp_glm)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = status ~ centre + treatment + gender + baseline + \n#&gt;     age, family = \"binomial\", data = resp)\n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)        -0.900171   0.337653  -2.666  0.00768 ** \n#&gt; centre2             0.671601   0.239567   2.803  0.00506 ** \n#&gt; treatmenttreatment  1.299216   0.236841   5.486 4.12e-08 ***\n#&gt; gendermale          0.119244   0.294671   0.405  0.68572    \n#&gt; baselinegood        1.882029   0.241290   7.800 6.20e-15 ***\n#&gt; age                -0.018166   0.008864  -2.049  0.04043 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 608.93  on 443  degrees of freedom\n#&gt; Residual deviance: 483.22  on 438  degrees of freedom\n#&gt; AIC: 495.22\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\nNow we will fit GEE with independent correlation structure:\n\n## GEE with identity matrix\nresp_gee.in &lt;- gee(nstat ~ centre + treatment + gender + baseline + age, \n                 data = resp, family = \"binomial\", \n                 id = subject,corstr = \"independence\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)            centre2 treatmenttreatment         gendermale \n#&gt;        -0.90017133         0.67160098         1.29921589         0.11924365 \n#&gt;       baselinegood                age \n#&gt;         1.88202860        -0.01816588\nsummary(resp_gee.in)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Logit \n#&gt;  Variance to Mean Relation: Binomial \n#&gt;  Correlation Structure:     Independent \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = nstat ~ centre + treatment + gender + baseline + \n#&gt;     age, id = subject, data = resp, family = \"binomial\", corstr = \"independence\", \n#&gt;     scale.fix = TRUE, scale.value = 1)\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#&gt; centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#&gt; gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#&gt; age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\n#&gt; \n#&gt; Estimated Scale Parameter:  1\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    0    0    0\n#&gt; [2,]    0    1    0    0\n#&gt; [3,]    0    0    1    0\n#&gt; [4,]    0    0    0    1\n\n\nThis model assumes an independent correlation structure, the output will be equal to glm.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same as GLM. For binary outcome, you may still interpret them as log odds. Naive SE and z value are estimated directly from this model. Robust SE and z are sandwich estimates.\nThe difference between naive and robust indicates that the correlation structure may not be good.\nWorking Correlation is the correlation structure estimated from the data (identity matrix for independence).\n\nLet fit the GEE model with exchangeable correlation structure:\n\n## GEE with exchangeable matrix\nresp_gee.ex &lt;- gee(nstat ~ centre + treatment + gender + baseline+ age, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)            centre2 treatmenttreatment         gendermale \n#&gt;        -0.90017133         0.67160098         1.29921589         0.11924365 \n#&gt;       baselinegood                age \n#&gt;         1.88202860        -0.01816588\nsummary(resp_gee.ex)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Logit \n#&gt;  Variance to Mean Relation: Binomial \n#&gt;  Correlation Structure:     Exchangeable \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = nstat ~ centre + treatment + gender + baseline + \n#&gt;     age, id = subject, data = resp, family = \"binomial\", corstr = \"exchangeable\", \n#&gt;     scale.fix = TRUE, scale.value = 1)\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#&gt; centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#&gt; gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#&gt; age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n#&gt; \n#&gt; Estimated Scale Parameter:  1\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;           [,1]      [,2]      [,3]      [,4]\n#&gt; [1,] 1.0000000 0.3359883 0.3359883 0.3359883\n#&gt; [2,] 0.3359883 1.0000000 0.3359883 0.3359883\n#&gt; [3,] 0.3359883 0.3359883 1.0000000 0.3359883\n#&gt; [4,] 0.3359883 0.3359883 0.3359883 1.0000000\n\n\nThis model assumes an exchangeable correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same GLM. For binary outcome, you may still interpret them as log odds. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nThe difference between naive and robust is smaller, which indicates that the correlation structure is better specified.\nWorking Correlation is the correlation structure estimated from the data\n\nLet‚Äôs check the estimated coefficients from all three models.\n\nsummary(resp_glm)$coefficients\n#&gt;                       Estimate  Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)        -0.90017133 0.337652992 -2.6659658 7.676750e-03\n#&gt; centre2             0.67160098 0.239566555  2.8034004 5.056684e-03\n#&gt; treatmenttreatment  1.29921589 0.236840962  5.4856047 4.120574e-08\n#&gt; gendermale          0.11924365 0.294671000  0.4046671 6.857223e-01\n#&gt; baselinegood        1.88202860 0.241290163  7.7998563 6.197770e-15\n#&gt; age                -0.01816588 0.008864401 -2.0493065 4.043215e-02\nsummary(resp_gee.in)$coefficients\n#&gt;                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#&gt; centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#&gt; gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#&gt; age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\nsummary(resp_gee.ex)$coefficients\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#&gt; centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#&gt; gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#&gt; age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n# Same estimated coefficients but different SEs\n\nGEE with identity matrix is the same as GLM model. If we change the correlation structure to exchangeable does not change the beta estimates, but the naive SEs are closer to Robust SE, which indicates that the exchangeable correlation structure is a better reflection of the correlation structures.\nGaussian response\nGEE can also be applied to Gaussian response\n\nlibrary(gee)\nlibrary(HSAUR2)\nBtheB$subject &lt;- factor(rownames(BtheB))\nnobs &lt;- nrow(BtheB)\nBtheB_long &lt;- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time &lt;- rep(c(2, 3, 5, 8), rep(nobs, 4))\nosub &lt;- order(as.integer(BtheB_long$subject))\nBtheB_long &lt;- BtheB_long[osub,]\nbtb_gee.ind &lt;- gee(bdi ~ bdi.pre + treatment + length + drug, \n               data = BtheB_long, id = subject, \n               family = gaussian, corstr = \"independence\")\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;    (Intercept)        bdi.pre treatmentBtheB      length&gt;6m        drugYes \n#&gt;      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ind)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Identity \n#&gt;  Variance to Mean Relation: Gaussian \n#&gt;  Correlation Structure:     Independent \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#&gt;     data = BtheB_long, family = gaussian, corstr = \"independence\")\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -21.6497810  -5.8485100   0.1131663   5.5838383  28.1871039 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)     3.5686314  1.4833349  2.405816  2.26947617  1.5724472\n#&gt; bdi.pre         0.5818494  0.0563904 10.318235  0.09156455  6.3545274\n#&gt; treatmentBtheB -3.2372285  1.1295569 -2.865928  1.77459534 -1.8242066\n#&gt; length&gt;6m       1.4577182  1.1380277  1.280916  1.48255866  0.9832449\n#&gt; drugYes        -3.7412982  1.1766321 -3.179667  1.78271179 -2.0986557\n#&gt; \n#&gt; Estimated Scale Parameter:  79.25813\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    0    0    0\n#&gt; [2,]    0    1    0    0\n#&gt; [3,]    0    0    1    0\n#&gt; [4,]    0    0    0    1\n# require(Publish)\n# publish(btb_gee.ind)\n\n\nThis model assumes an independent correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficient will be interpreted the same way as linear model. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation struture estimated from the data (identity matrix for indepedence)\n\nWith exchangeable correlation matrix:\n\nbtb_gee.ex &lt;- gee(bdi ~ bdi.pre + treatment + length + drug,\n                data = BtheB_long, id = subject, \n                family = gaussian, corstr = \"exchangeable\")\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;    (Intercept)        bdi.pre treatmentBtheB      length&gt;6m        drugYes \n#&gt;      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ex)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Identity \n#&gt;  Variance to Mean Relation: Gaussian \n#&gt;  Correlation Structure:     Exchangeable \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#&gt;     data = BtheB_long, family = gaussian, corstr = \"exchangeable\")\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -23.955980  -6.643864  -1.109741   4.257688  25.452310 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Naive S.E.     Naive z Robust S.E.   Robust z\n#&gt; (Intercept)     3.0231602 2.30390185  1.31219140  2.23204410  1.3544357\n#&gt; bdi.pre         0.6479276 0.08228567  7.87412417  0.08351405  7.7583066\n#&gt; treatmentBtheB -2.1692863 1.76642861 -1.22806339  1.73614385 -1.2494854\n#&gt; length&gt;6m      -0.1112910 1.73091679 -0.06429596  1.55092705 -0.0717577\n#&gt; drugYes        -2.9995608 1.82569913 -1.64296559  1.73155411 -1.7322940\n#&gt; \n#&gt; Estimated Scale Parameter:  81.7349\n#&gt; Number of Iterations:  5\n#&gt; \n#&gt; Working Correlation\n#&gt;           [,1]      [,2]      [,3]      [,4]\n#&gt; [1,] 1.0000000 0.6757951 0.6757951 0.6757951\n#&gt; [2,] 0.6757951 1.0000000 0.6757951 0.6757951\n#&gt; [3,] 0.6757951 0.6757951 1.0000000 0.6757951\n#&gt; [4,] 0.6757951 0.6757951 0.6757951 1.0000000\n#publish(btb_gee.ex)\n\n\nThe interpretation of estimated coefficients are similar to LM. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation structure estimated from the data\nWhen we change the structure of correlation, the estimates and naive SE and z changed. A closer naive SE to robust SE indicates that the correlation structure is better specified.\nCompare with Random Effects\n\nref: (Hothorn and Everitt 2014) Section 13.4 We then use the conditional models (i.e., adding random effect) with non-normal response\n\nLet us compare the GEE models with the mixed effect models.\n\n## Generalized mixed effect model model\nlibrary(\"lme4\")\nresp_lmer &lt;- glmer(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.197382 (tol = 0.002, component 1)\n\n\nrequire(afex)\nresp_afex &lt;- mixed(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp, method = \"LRT\")\n#&gt; Contrasts set to contr.sum for the following variables: baseline, month, treatment, gender, centre, subject\n#&gt; Numerical variables NOT centered on 0: age\n#&gt; If in interactions, interpretation of lower order (e.g., main) effects difficult.\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.00495329 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.00478889 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0466122 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0949357 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0416611 (tol = 0.002, component 1)\n\n\n## GEE model\nresp_gee3 &lt;- gee(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)       baselinegood            month.L            month.Q \n#&gt;        -0.90363465         1.88945124        -0.14372490        -0.02455122 \n#&gt;            month.C treatmenttreatment         gendermale                age \n#&gt;        -0.23255161         1.30410916         0.11969528        -0.01823703 \n#&gt;            centre2 \n#&gt;         0.67417628\nlibrary(MuMIn)\nlibrary(geepack)\nresp_gee4 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nresp_gee5 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"independence\", \n                 scale.fix = TRUE)\nresp_gee6 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"ar1\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee5)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  511.14981  499.69791 -240.84895   14.72595    9.00000  512.93199\nQIC(resp_gee6)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  511.81242  500.27657 -241.13829   14.76792    9.00000  514.01242\n# Smaller QIC values for correlation structure represents better models\n# i.e., \"exchangeable\" in our case\n\nresp_gee4a &lt;- geeglm(nstat ~ month + treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4b &lt;- geeglm(nstat ~ treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4c &lt;- geeglm(nstat ~ gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4d &lt;- geeglm(nstat ~ age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4e &lt;- geeglm(nstat ~ centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee4a)[2]\n#&gt;   QICu \n#&gt; 567.35\nQIC(resp_gee4b)[2]\n#&gt;     QICu \n#&gt; 562.6247\nQIC(resp_gee4c)[2]\n#&gt;     QICu \n#&gt; 585.3099\nQIC(resp_gee4d)[2]\n#&gt;     QICu \n#&gt; 586.1581\nQIC(resp_gee4e)[2]\n#&gt;     QICu \n#&gt; 592.3437\nQIC(resp_gee4d)[2]\n#&gt;     QICu \n#&gt; 586.1581\n# Covariates are selected based on the QICu criteria\n\n\n## compare estimates (conditional vs. marginal)\nsummary(resp_lmer)$coefficients # Model failed to converge\n#&gt;                       Estimate Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)        -1.65459986 0.77620476 -2.1316538 3.303531e-02\n#&gt; baselinegood        3.08897132 0.59858148  5.1604860 2.463096e-07\n#&gt; month.L            -0.20348327 0.27957504 -0.7278306 4.667173e-01\n#&gt; month.Q            -0.02821444 0.27907489 -0.1010999 9.194712e-01\n#&gt; month.C            -0.35570944 0.28084948 -1.2665483 2.053168e-01\n#&gt; treatmenttreatment  2.16620464 0.55157208  3.9273283 8.589470e-05\n#&gt; gendermale          0.23836010 0.66606345  0.3578640 7.204451e-01\n#&gt; age                -0.02557320 0.01993984 -1.2825178 1.996611e-01\n#&gt; centre2             1.03849878 0.54182274  1.9166762 5.527908e-02\nsummary(resp_afex)$coefficients # Model failed to converge\n#&gt;                Estimate Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)  1.61366355 0.79598978  2.0272415 4.263772e-02\n#&gt; baseline1   -1.55321951 0.30467553 -5.0979463 3.433581e-07\n#&gt; month1       0.21660822 0.24441676  0.8862249 3.754963e-01\n#&gt; month2      -0.17701748 0.24250584 -0.7299514 4.654199e-01\n#&gt; month3       0.21559489 0.24440444  0.8821235 3.777101e-01\n#&gt; treatment1  -1.09162957 0.28098789 -3.8849701 1.023425e-04\n#&gt; gender1     -0.10313396 0.33932981 -0.3039343 7.611780e-01\n#&gt; age         -0.02576276 0.02031589 -1.2681088 2.047591e-01\n#&gt; centre1     -0.52829986 0.27639204 -1.9114149 5.595128e-02\nsummary(resp_gee3)$coefficients # Marginalized model\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90565507 0.47940039 -1.8891413  0.46042640 -1.9669920\n#&gt; baselinegood        1.86665412 0.34220231  5.4548261  0.35023043  5.3297885\n#&gt; month.L            -0.14330949 0.18044542 -0.7941986  0.18210027 -0.7869812\n#&gt; month.Q            -0.02448267 0.18034926 -0.1357514  0.18329974 -0.1335663\n#&gt; month.C            -0.23187407 0.18073662 -1.2829391  0.15929035 -1.4556693\n#&gt; treatmenttreatment  1.28311415 0.33592099  3.8196903  0.35055323  3.6602548\n#&gt; gendermale          0.11513183 0.41851611  0.2750953  0.44135628  0.2608592\n#&gt; age                -0.01785399 0.01258158 -1.4190585  0.01299686 -1.3737157\n#&gt; centre2             0.68481139 0.34010911  2.0135050  0.35681635  1.9192265\nsummary(resp_gee4)$coefficients # Marginalized model\n\n\n  \n\n\n\nThe significance of variables are similar in both variables, but the estimated coefficients are larger in generalized mixed effect model. However, it does not mean the estimated coefficients are inconsistent. Instead, two models are estimating different parameters. Remember, the mixed effect model is conditional on random effects, while the GEE is a marginalized model.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.",
    "crumbs": [
      "Longitudinal data",
      "GEE"
    ]
  },
  {
    "objectID": "longitudinalF.html",
    "href": "longitudinalF.html",
    "title": "R functions (T)",
    "section": "",
    "text": "The list of new R functions introduced in this longitudinal data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\ngee\ngee\nTo fit a generalized estimation equation model\n\n\ngeeglm\ngeepack\nTo fit a generalized estimation equation model\n\n\nlmer\nlme4\nTo fit linear mixed effects models\n\n\nglmer\nlme4\nTo fit generalized linear mixed effects models\n\n\nmixed\nafex\nTo fit generalized linear mixed effects models\n\n\nqqnorm\nbase/stats\nTo fit a QQ plot\n\n\nranef\nlme5\nTo extract the random effects from a model\n\n\nreshape\nbase/stats\nReshape data, e.g., into wide to long or long to wide format\n\n\nresiduals\nbase/stats\nTo extract residuals of a model",
    "crumbs": [
      "Longitudinal data",
      "R functions (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html",
    "href": "longitudinalQ.html",
    "title": "Quiz (T)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html#live-quiz",
    "href": "longitudinalQ.html#live-quiz",
    "title": "Quiz (T)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html#download-quiz",
    "href": "longitudinalQ.html#download-quiz",
    "title": "Quiz (T)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "Mediation analysis",
    "section": "",
    "text": "Background\nThis chapter provides comprehensive tutorials on mediation analysis. The Baron and Kenny approach explores non-binary outcomes through directed acyclic graphs (DAGs) and regression in big data scenarios, with a focus on both continuous and binary mediators and outcomes. The justification of mediation analysis evaluates the connection between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD), considering various covariates like BMI, smoking status, and associations with diseases like diabetes. The final mediation example centers on decomposing the total effect of OA on CVD through direct and indirect pathways via pain medication, including data preparation, weight computation, and outcome evaluation, accompanied by considerations of non-linearity and potential interactions.",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation.html#background",
    "href": "mediation.html#background",
    "title": "Mediation analysis",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation.html#overview-of-tutorials",
    "href": "mediation.html#overview-of-tutorials",
    "title": "Mediation analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily discussed about total effect of an exposure to the outcome. In this chapter, we will discuss about decomposition of the effect in the presence of a mediator.\n\nBaron and Kenny Approach\nThe chapter, referencing the Baron and Kenny approach, delves into the analysis of non-binary outcomes using directed acyclic graphs (DAGs) and regression techniques for big data scenarios with a million observations. Initially, the chapter focuses on a continuous outcome and continuous mediator, where the true treatment effect is known. Through the data generating process, a DAG is formulated and data simulated, followed by an estimation of effects using generalized linear models. Subsequently, the Baron and Kenny approaches are applied to determine direct, total, and indirect effects. The chapter progresses to explore binary outcomes with both continuous and binary mediators, each time employing a similar approach: creating a DAG, generating data, estimating effects using regression models, and then using the Baron and Kenny methodology to elucidate the relationships.\n\n\nJustification of Mediation Analysis\nIn this chapter, the data analysis process centers on understanding the relationship between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD) using a mediation analysis. Specifically, the analysis seeks to determine if OA, the exposure, is associated with an increased risk of CVD, the outcome. Additionally, it investigates whether pain medication acts as a mediator in this causal pathway. The total effect of OA on CVD risk was found to be significant. Furthermore, OA was observed to significantly influence the use of pain medication, which is the proposed mediator. To facilitate the analysis, the study considers various adjustment covariates, including demographic variables, confounders such as BMI and smoking status, and associations with other diseases like diabetes. The data used in this study is pre-processed, analyzed, and subsequently saved for further use.\n\n\nMediation Example\nIn the chapter, the focus is on decomposing the ‚Äútotal effect‚Äù of a given exposure, OA (\\(A\\)), on the outcome CVD (\\(Y\\)) into its natural direct effect (NDE; \\(A \\rightarrow Y\\)) and a natural indirect effect (NIE) that routes through a mediator, in this case, pain medication (\\(M\\)). Initially, the required data is loaded and preprocessed. The mediation analysis involves several steps: (1) Preparing the data and ensuring it has the necessary variables; (2) Modifying data for different exposures and duplicating it; (3) Computing weights for the mediation based on logistic regressions, where the weights are applied to factor in the mediator‚Äôs effect; (4) Building a weighted outcome model, which is a logistic regression to evaluate the outcome. To quantify these effects, the chapter derives point estimates for the total effect, direct effect, and indirect effect. Furthermore, confidence intervals for these effects are determined using bootstrap methods. The results, including the proportion mediated by pain medication, are visualized using graphs. The chapter also delves into considerations of non-linearity and potential interactions between variables.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation0.html",
    "href": "mediation0.html",
    "title": "Concepts (I)",
    "section": "",
    "text": "Mediation Analysis\nThe section provides an overview of the concept and methods of mediation analysis in the context of epidemiology and statistical modeling. The section discusses total effects, indirect and direct paths, and highlights the limitations of the Baron and Kenny approach (Baron and Kenny 1986). It introduces the counterfactual definition, emphasizing the importance of adjusting for confounders and providing insights into the mechanics of mediation analysis through imputation and weighting methods. The section also covers sensitivity analysis, proportion mediated, and methodological extensions, including multi-category mediators and software references for conducting mediation analysis.",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#reading-list",
    "href": "mediation0.html#reading-list",
    "title": "Concepts (I)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Rochon, Bois, and Lange 2014; Lange et al. 2017)",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#video-lessons",
    "href": "mediation0.html#video-lessons",
    "title": "Concepts (I)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMediation Analysis - Baron and Kenny (1986)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Mediation analysis with counterfactual definitions and why confounding adjustment helps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis mechanism under counterfactual definition (i) Outcome imputation & (ii) Weighting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis using survey data, assumptions, extensions, software and references",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#video-lesson-slides",
    "href": "mediation0.html#video-lesson-slides",
    "title": "Concepts (I)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#links",
    "href": "mediation0.html#links",
    "title": "Concepts (I)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#references",
    "href": "mediation0.html#references",
    "title": "Concepts (I)",
    "section": "References",
    "text": "References\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. ‚ÄúThe Moderator‚ÄìMediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.‚Äù Journal of Personality and Social Psychology 51 (6): 1173.\n\n\nLange, Theis, Kristoffer W Hansen, Rune S√∏rensen, and S√∏ren Galatius. 2017. ‚ÄúApplied Mediation Analyses: A Review and Tutorial.‚Äù Epidemiology and Health 39.\n\n\nRochon, Justine, Andreas du Bois, and Theis Lange. 2014. ‚ÄúMediation Analysis of the Relationship Between Institutional Research Activity and Patient Survival.‚Äù BMC Medical Research Methodology 14 (1): 9.",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation1.html",
    "href": "mediation1.html",
    "title": "Baron and Kenny",
    "section": "",
    "text": "Baron and Kenny (1986) approach 1\nIn Baron and Kenny approach 1 (Baron and Kenny 1986), two regression models are fitted to estimate the total effect, direct effect, and indirect effect.\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] where \\(Y\\) is the outcome, \\(A\\) is the exposure, \\(M\\) is the mediator, and \\(\\alpha, \\beta, \\gamma\\) are regression coefficients. The effects are then calculated as:\n\nTotal effect of A on Y: \\(\\hat{\\beta}_1\\)\n\nDirect effect of A on Y: \\(\\hat{\\beta}_0\\)\n\nIndirect effect of A on Y through M: \\(\\hat{\\beta}_1 - \\hat{\\beta}_0\\),\n\nwhere \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\nBaron and Kenny (1986) approach 2\nIn the second approach, three models are fitted:\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] \\[ M = \\alpha_{2} + \\beta_2 A, \\]\nThe indirect effect of A on Y through M can be calculated as: \\(\\hat{\\beta}_2 \\times \\hat{\\beta}_0\\), where \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\n\n# Load required packages\nrequire(simcausal)\n\nBig data: What if we had 1,000,000 (1 million) observations?\nLet us explore the mediation analysis using Baron and Kenny (1986) approaches. We first simulate a big dataset and then show the results from the mediation analysis.\nContinuous outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n    node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n    node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.55\nfit.am &lt;- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.55\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.55\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.25\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nBinary outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.48\nfit.am &lt;- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.48\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.48\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.18\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nBinary outcome, binary mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rbern\", prob = plogis(0.5 * A)) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.25        1.34\nfit.am &lt;- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.34\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.34\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.04\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nAs we can see, the estimated indirect effects are different from the two approaches. That means Baron and Kenny‚Äôs (1986) approach doesn‚Äôt work for a non-collapsible effect measure such as the odds ratio.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. ‚ÄúThe Moderator‚ÄìMediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.‚Äù Journal of Personality and Social Psychology 51 (6): 1173.",
    "crumbs": [
      "Mediation analysis",
      "Baron and Kenny"
    ]
  },
  {
    "objectID": "mediation2.html",
    "href": "mediation2.html",
    "title": "Justification",
    "section": "",
    "text": "We must show enough justification to do a mediation analysis. A causal diagram would be the first step to conceptualize the mediation analysis problem hypothetically. Then, we can empirically verify where doing the mediation analysis makes sense in that particular context.\n\n# Load required packages\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\n\nPre-processing\nLoad saved data\n\nload(\"Data/mediation/cchs123pain.RData\")\n\nPrepared the data\n\nanalytic.miss$mediator &lt;- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure &lt;- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome &lt;- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nNotation\n\nOutcome (\\(Y\\)): Cardiovascular disease (CVD)\nExposure (\\(A\\)): Osteoarthritis (OA)\nMediator (\\(M\\)): Pain medication\nAdjustment covariates (\\(C\\))\nHypothesis\n\nFor total effect (TE): Is OA (\\(A\\)) associated with CVD (\\(Y\\))?\n\n\n\n\n\n\n\n\n\n\nFor mediation analysis: Does pain-medication (\\(M\\)) play a mediating role in the causal pathway between OA (\\(A\\)) and CVD (\\(Y\\))? Here, we will decompose total effect (TE) to a natural direct effect (NDE) and a natural indirect effect (NIE).\n\n\n\n\n\n\n\n\n\nAdjustment variables (\\(C\\)):\n\nDemographics\n\nage\nsex\nincome\nrace\neducation status\n\n\nImportant confounders\n\nBMI\nphysical activity\nsmoking status\nfruit and vegetable consumption\n\n\nRelation with other diseases\n\nhypertension\nCOPD\ndiabetes\n\n\nAnalysis/empirical exploration\nTotal effect\nOutcome model (weighted) is\n\\(logit [P(Y_{a}=1 | C = c] = \\theta_0 + \\theta_1 a + \\theta_3 c\\)\nSetting Design\n\nrequire(survey)\nsummary(analytic.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Survey design\nw.design0 &lt;- svydesign(id=~1, weights=~weight, data=analytic.miss)\nsummary(weights(w.design0))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#&gt; [1] 80.34263\n\n# Subset the design\nw.design &lt;- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt;    2.053   28.480   53.218   82.472  102.860 1295.970\nsd(weights(w.design))\n#&gt; [1] 91.98946\n\n\n# Model\nTE &lt;- svyglm(outcome ~ exposure + age + sex + income + race + bmi + edu + phyact + \n               smoke + fruit + diab, design = w.design, family = quasibinomial(\"logit\"))\n# painmed is mediator; not included here.\nTE.save &lt;- exp(c(summary(TE)$coef[\"exposure\",1], \n                confint(TE)[\"exposure\",]))\nTE.save\n#&gt;             2.5 %   97.5 % \n#&gt; 1.537005 1.230735 1.919489\n\nExposure to OA has a detrimental effect on CVD risk (significant!).\nEffect on the mediators\n\nfit.m = svyglm(mediator ~ exposure + age + sex + income + race + bmi + edu +\n                 phyact + smoke + fruit + diab, design = w.design,\n               family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nmed.save &lt;- exp(c(summary(fit.m)$coef[\"exposure\",1], confint(fit.m)[\"exposure\",]))\nmed.save\n#&gt;             2.5 %   97.5 % \n#&gt; 2.428463 2.059265 2.863853\n\nExposure to OA has a substantial effect on the mediator (Pain medication) as well (significant!). Hence, it would be interesting to explore a mediation analysis to assess the mediating role.\nSave data\n\nsave(w.design, file = \"Data/mediation/cchs123painW.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Mediation analysis",
      "Justification"
    ]
  },
  {
    "objectID": "mediation3.html",
    "href": "mediation3.html",
    "title": "Mediation Example",
    "section": "",
    "text": "# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(Publish)\n\nWe want to decompose of the ‚Äútotal effect‚Äù of a given exposure OA (\\(A\\)) on the outcome CVD (\\(Y\\)) into\n\na natural direct effect (NDE; \\(A \\rightarrow Y\\)) and\na natural indirect effect (NIE) through a mediator pain medication (\\(M\\)) through (\\(A \\rightarrow M \\rightarrow Y\\)).\n\nStep 0: Build data first\n\nload(\"Data/mediation/cchs123pain.RData\")\nsource(\"Data/mediation/medFunc.R\")\nls()\n#&gt; [1] \"analytic.cc\"        \"analytic.miss\"      \"doEffectDecomp\"    \n#&gt; [4] \"doEffectDecomp.int\"\n\nvarlist &lt;- c(\"age\", \"sex\", \"income\", \"race\", \"bmi\", \"edu\", \"phyact\", \"smoke\", \"fruit\", \"diab\")\nanalytic.miss$mediator &lt;- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure &lt;- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome &lt;- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nPre-run step 3 model\nWe will utilize this fit in step 3\n\n# A = actual exposure (without any change)\nanalytic.miss$exposureTemp &lt;- analytic.miss$exposure\n\n# Design\nw.design0 &lt;- svydesign(id=~1, weights=~weight, data=analytic.miss)\nw.design &lt;- subset(w.design0, miss == 0)\n\n# Replace exposure with exposureTemp. This will be necessary in step 3\nfit.m &lt;- svyglm(mediator ~ exposureTemp + \n                 age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab,\n                design = w.design, family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.m)\n#&gt;      Variable             Units OddsRatio       CI.95     p-value \n#&gt;  exposureTemp                        2.43 [2.06;2.86]     &lt; 1e-04 \n#&gt;           age       20-29 years       Ref                         \n#&gt;                     30-39 years      1.00 [0.88;1.13]   0.9442989 \n#&gt;                     40-49 years      0.93 [0.82;1.06]   0.2651302 \n#&gt;                     50-59 years      0.66 [0.58;0.76]     &lt; 1e-04 \n#&gt;                     60-64 years      0.61 [0.51;0.72]     &lt; 1e-04 \n#&gt;               65 years and over      0.61 [0.52;0.71]     &lt; 1e-04 \n#&gt;           sex            Female       Ref                         \n#&gt;                            Male      0.50 [0.46;0.55]     &lt; 1e-04 \n#&gt;        income   $29,999 or less       Ref                         \n#&gt;                 $30,000-$49,999      1.20 [1.06;1.35]   0.0043533 \n#&gt;                 $50,000-$79,999      1.21 [1.08;1.37]   0.0014914 \n#&gt;                 $80,000 or more      1.28 [1.14;1.45]     &lt; 1e-04 \n#&gt;          race         Non-white       Ref                         \n#&gt;                           White      1.81 [1.62;2.02]     &lt; 1e-04 \n#&gt;           bmi       Underweight       Ref                         \n#&gt;                  healthy weight      1.09 [0.82;1.44]   0.5631582 \n#&gt;                      Overweight      1.33 [1.01;1.77]   0.0449616 \n#&gt;           edu          &lt; 2ndary       Ref                         \n#&gt;                       2nd grad.      1.13 [0.98;1.30]   0.1014986 \n#&gt;                 Other 2nd grad.      1.30 [1.08;1.55]   0.0050596 \n#&gt;                  Post-2nd grad.      1.25 [1.10;1.42]   0.0008252 \n#&gt;        phyact            Active       Ref                         \n#&gt;                        Inactive      1.12 [1.02;1.23]   0.0184447 \n#&gt;                        Moderate      1.12 [1.01;1.25]   0.0364592 \n#&gt;         smoke      Never smoker       Ref                         \n#&gt;                  Current smoker      1.29 [1.16;1.44]     &lt; 1e-04 \n#&gt;                   Former smoker      1.28 [1.17;1.40]     &lt; 1e-04 \n#&gt;         fruit 0-3 daily serving       Ref                         \n#&gt;               4-6 daily serving      0.92 [0.83;1.02]   0.0976967 \n#&gt;                6+ daily serving      0.80 [0.71;0.90]   0.0001979 \n#&gt;          diab                No       Ref                         \n#&gt;                             Yes      1.23 [0.99;1.52]   0.0626501\n\nStep 1 and 2: Replicate data with different exposures\nWe manipulate and duplicate data here\n\ndim(analytic.miss)\n#&gt; [1] 397173     28\ndim(analytic.cc)\n#&gt; [1] 28734    23\nnrow(analytic.miss) - nrow(analytic.cc)\n#&gt; [1] 368439\n\n# Create counterfactual data. This will be necessary in step 3\nd1 &lt;- d2 &lt;- analytic.miss\n\n# Exposed = Exposed\nd1$exposure.counterfactual &lt;- d1$exposure\n\n# Exposed = Not exposed\nd2$exposure.counterfactual &lt;- !d2$exposure \n\n# duplicated data (double the amount)\nnewd &lt;- rbind(d1, d2)\nnewd &lt;- newd[order(newd$ID), ]\ndim(newd)\n#&gt; [1] 794346     29\n\nStep 3: Compute weights for the mediation\nWeight is computed by\n\\(W^{M|C} = \\frac{P(M|A^*, C)}{P(M|A, C)}\\)\nin all data newd (fact d1 + alternative fact d2).\n\n\n\\(P(M|A, C)\\) is computed from a logistic regression of \\(M\\) on \\(A\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta_1 a + \\beta_3 c\\)\n\n\n\n\\(P(M|A^{*}, C)\\) is computed from a logistic regression of \\(M\\) on \\(A^*\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta'_1 a^* + \\beta'_3 c\\)\n\n\n\n\n# First, use original exposure (all A + all A):\n# A = actual exposure (without any change)\n# A = exposure\nnewd$exposureTemp &lt;- newd$exposure\n\n# Probability of M given A + C\nw &lt;- predict(fit.m, newdata=newd, type='response') \ndirect &lt;- ifelse(newd$mediator, w, 1-w)\n\n# Second, use counterfactual exposures (all A + all !A):\n# A* = Opposite (counterfactual) values of the exposure\n# A* = exposure.counterfactual\nnewd$exposureTemp &lt;- newd$exposure.counterfactual\n\n# Probability of M given A* + C\nw &lt;- predict(fit.m, newdata=newd, type='response') \nindirect &lt;- ifelse(newd$mediator, w, 1-w)\n\n# Mediator weights\nnewd$W.mediator &lt;- indirect/direct\nsummary(newd$W.mediator)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.45    1.00    1.00    1.01    1.15    2.25  670758\nhist(newd$W.mediator)\n\n\n\n\n\n\n\nIncorporating the survey weights:\nNote: scaling can often be helpful if there exists extreme weights.\n\n# scale survey weights\n#newd$S.w &lt;- with(newd,(weight)/mean(weight))\nnewd$S.w &lt;- with(newd,weight)\nnewd$S.w[is.na(newd$S.w)]\n#&gt; numeric(0)\nsummary(newd$S.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Multiply mediator weights with scaled survey weights\nnewd$SM.w &lt;- with(newd,(W.mediator * S.w))\n\nsummary(newd$SM.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.40   24.65   46.41   73.92   88.84 3531.63  670758\ntable(newd$miss[is.na(newd$SM.w)])\n#&gt; \n#&gt;      1 \n#&gt; 670758\nnewd$SM.w[is.na(newd$SM.w)] &lt;- 0\nsummary(newd$SM.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     0.0     0.0     0.0    11.5     0.0  3531.6\n\nhist(newd$SM.w, main = \"\", xlab = \"Combined weights\", \n     ylab = \"Frequency\", freq = TRUE)\n\n\n\n\n\n\n\nHere all missing weights are associated with incomplete cases (miss==1)! Hence, doesn‚Äôt matter if they are missing or other value (0) in them.\nStep 4: Weighted outcome Model\nOutcome model is\n\\(logit [P(Y_{a,M(a^*)}=1 | C = c)] = \\theta_0 + \\theta_1 a + \\theta_2 a^* + \\theta_3 c\\)\nafter weighting (combination of mediator weight + sampling weight).\n\n# Outcome analysis\nw.design0 &lt;- svydesign(id=~1, weights=~SM.w, data=newd)\nw.design &lt;- subset(w.design0, miss == 0)\n\n# Fit Y on (A + A* + C)\nfit &lt;- svyglm(outcome ~ exposure + exposure.counterfactual + \n                age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab, \n             design = w.design, family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPoint estimates\nFollowing are the conditional ORs:\n\n\\(OR_{TE}(C=c) = \\exp(\\theta_1 + \\theta_2)\\)\n\\(OR_{NDE}(A=1,M=0,C=c) = \\exp(\\theta_1)\\)\n\\(OR_{NIE}(A^{*}=1,M=0,C=c) = \\exp(\\theta_2)\\)\n\n\n# total effect of A-&gt; Y + A -&gt; M -&gt; Y\nTE &lt;- exp(sum(coef(fit)[c('exposure', 'exposure.counterfactual')])) \nTE \n#&gt; [1] 1.544694\n\n# direct effect of A-&gt; Y (not through M)\nDE &lt;- exp(unname(coef(fit)['exposure']))\nDE \n#&gt; [1] 1.488554\n\n# indirect effect of A-&gt; Y (A -&gt; M -&gt; Y)\nIE &lt;- exp(unname(coef(fit)[c('exposure.counterfactual')])) \nIE \n#&gt; [1] 1.037714\n\n# Product of ORs; same as TE \nDE * IE \n#&gt; [1] 1.544694\n\n# Proportion mediated\nPM &lt;- log(IE) / log(TE)\nPM \n#&gt; [1] 0.08513902\n\nObtaining results fast\nUser-written function doEffectDecomp() (specific to OA-CVD problem):\n\ndoEffectDecomp(analytic.miss, ind = NULL, varlist = varlist)\n#&gt;         TE         DE         IE         PM \n#&gt; 1.54469393 1.48855395 1.03771444 0.08513902\n# function is provided in the appendix\n\nConfidence intervals\nStandard errors and confidence intervals are determined by bootstrap methods.\n\nrequire(boot)\n#&gt; Loading required package: boot\n#&gt; \n#&gt; Attaching package: 'boot'\n#&gt; The following object is masked from 'package:survival':\n#&gt; \n#&gt;     aml\n# I ran the computation on a 24 core computer,\n# hence set ncpus = 5 (keep some free). \n# If you have more / less cores, adjust accordingly. \n# Try parallel package to find how many cores you have.\n# library(parallel)\n# detectCores()\n# doEffectDecomp is a user-written function\n# See appendix for the function\nset.seed(504)\nbootresBin &lt;- boot(data=analytic.miss, statistic=doEffectDecomp, \n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1b &lt;- boot.ci(bootresBin,type = \"perc\",index=1)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2b &lt;- boot.ci(bootresBin,type = \"perc\",index=2)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3b &lt;- boot.ci(bootresBin,type = \"perc\",index=3)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4b &lt;- boot.ci(bootresBin,type = \"perc\",index=4)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\n# Number of bootstraps\nbootresBin$R\n#&gt; [1] 5\n\n# Total Effect\nc(bootresBin$t0[1], bootci1b$percent[4:5])\n#&gt;       TE                   \n#&gt; 1.544694 1.293208 1.894417\n\n# Direct Effect\nc(bootresBin$t0[2], bootci2b$percent[4:5])\n#&gt;       DE                   \n#&gt; 1.488554 1.303554 1.876916\n\n# Indirect Effect\nc(bootresBin$t0[3], bootci3b$percent[4:5])\n#&gt;        IE                     \n#&gt; 1.0377144 0.9738072 1.0093246\n\n# Proportion Mediated\nc(bootresBin$t0[4], bootci4b$percent[4:5])\n#&gt;          PM                         \n#&gt;  0.08513902 -0.08360848  0.01655013\n\nThe proportion mediated through pain medication was about 8.51% on the log odds ratio scale.\nVisualization for main effects\n\nrequire(plotrix)\n#&gt; Loading required package: plotrix\nTEc &lt;- c(bootresBin$t0[1], bootci1b$percent[4:5])\nDEc &lt;- c(bootresBin$t0[2], bootci2b$percent[4:5])\nIEc &lt;- c(bootresBin$t0[3], bootci3b$percent[4:5])\nmat &lt;- rbind(TEc,DEc,IEc)\ncolnames(mat) &lt;- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#&gt;        Point      2.5%    97.5%\n#&gt; TEc 1.544694 1.2932078 1.894417\n#&gt; DEc 1.488554 1.3035540 1.876916\n#&gt; IEc 1.037714 0.9738072 1.009325\n\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2], xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\n\n\n\nNon-linearity\nConsider\n\nnon-linear relationships (polynomials) and interactions between exposure, demographic/baseline covariates and mediators,\nIs misclassification of the mediators possible?\n\nHere we are again using a user-written function doEffectDecomp.int() (including interaction phyact*diab in the mediation model as well as the outcome model):\n\n# doEffectDecomp.int is a user-written function\n# See appendix for the function\ndoEffectDecomp.int(analytic.miss, ind = NULL, varlist = varlist)\n#&gt;         TE         DE         IE         PM \n#&gt; 1.54472021 1.48868864 1.03763821 0.08496674\n# try bootstrap on it?\n\nVisualization for main + interactions\n\nset.seed(504)\nbootresInt &lt;- boot(data=analytic.miss, statistic=doEffectDecomp.int,\n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1i &lt;- boot.ci(bootresInt,type = \"perc\",index=1)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2i &lt;- boot.ci(bootresInt,type = \"perc\",index=2)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3i &lt;- boot.ci(bootresInt,type = \"perc\",index=3)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4i &lt;- boot.ci(bootresInt,type = \"perc\",index=4)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\nbootresInt$R\n#&gt; [1] 5\n# from saved boostrap results: bootresInt \n# (similar as before)\nTEc &lt;- c(bootresInt$t0[1], bootci1i$percent[4:5])\nDEc &lt;- c(bootresInt$t0[2], bootci2i$percent[4:5])\nIEc &lt;- c(bootresInt$t0[3], bootci3i$percent[4:5])\nmat&lt;- rbind(TEc,DEc,IEc)\ncolnames(mat) &lt;- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#&gt;        Point      2.5%    97.5%\n#&gt; TEc 1.544720 1.2931358 1.893575\n#&gt; DEc 1.488689 1.3040953 1.876521\n#&gt; IEc 1.037638 0.9742042 1.009088\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2],\n       xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\n\n\n\nAppendix: OA-CVD Functions for bootstrap\nThese functions are written basically for performing bootstrap for the OA-CVD analysis. However, changing the covariates names/model-specifications should not be too hard, once you understand the basic steps.\n\n# without interactions (binary mediator)\ndoEffectDecomp\n#&gt; function (dat, ind = NULL, varlist) \n#&gt; {\n#&gt;     if (is.null(ind)) \n#&gt;         ind &lt;- 1:nrow(dat)\n#&gt;     d &lt;- dat[ind, ]\n#&gt;     d$mediator &lt;- ifelse(as.character(d$painmed) == \"Yes\", 1, \n#&gt;         0)\n#&gt;     d$exposure &lt;- ifelse(as.character(d$OA) == \"OA\", 1, 0)\n#&gt;     d$outcome &lt;- ifelse(as.character(d$CVD) == \"event\", 1, 0)\n#&gt;     d$exposureTemp &lt;- d$exposure\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~weight, data = d)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit.m &lt;- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp  + \"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     d1 &lt;- d2 &lt;- d\n#&gt;     d1$exposure.counterfactual &lt;- d1$exposure\n#&gt;     d2$exposure.counterfactual &lt;- !d2$exposure\n#&gt;     newd &lt;- rbind(d1, d2)\n#&gt;     newd &lt;- newd[order(newd$ID), ]\n#&gt;     newd$exposureTemp &lt;- newd$exposure\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     direct &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$exposureTemp &lt;- newd$exposure.counterfactual\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     indirect &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$W.mediator &lt;- indirect/direct\n#&gt;     summary(newd$W.mediator)\n#&gt;     newd$S.w &lt;- with(newd, weight)\n#&gt;     summary(newd$S.w)\n#&gt;     newd$SM.w &lt;- with(newd, (W.mediator * S.w))\n#&gt;     newd$SM.w[is.na(newd$SM.w)] &lt;- 0\n#&gt;     summary(newd$SM.w)\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit &lt;- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     TE &lt;- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#&gt;     DE &lt;- exp(unname(coef(fit)[\"exposure\"]))\n#&gt;     IE &lt;- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#&gt;     PM &lt;- log(IE)/log(TE)\n#&gt;     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#&gt; }\n#&gt; &lt;bytecode: 0x0000022e10416e10&gt;\n\n# with interactions (binary mediator)\ndoEffectDecomp.int\n#&gt; function (dat, ind = NULL, varlist) \n#&gt; {\n#&gt;     if (is.null(ind)) \n#&gt;         ind &lt;- 1:nrow(dat)\n#&gt;     d &lt;- dat[ind, ]\n#&gt;     d$exposureTemp &lt;- d$exposure\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~weight, data = d)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit.m &lt;- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp + phyact*diab +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     d1 &lt;- d2 &lt;- d\n#&gt;     d1$exposure.counterfactual &lt;- d1$exposure\n#&gt;     d2$exposure.counterfactual &lt;- !d2$exposure\n#&gt;     newd &lt;- rbind(d1, d2)\n#&gt;     newd &lt;- newd[order(newd$ID), ]\n#&gt;     newd$exposureTemp &lt;- newd$exposure\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     direct &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$exposureTemp &lt;- newd$exposure.counterfactual\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     indirect &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$W.mediator &lt;- indirect/direct\n#&gt;     summary(newd$W.mediator)\n#&gt;     newd$S.w &lt;- with(newd, weight)\n#&gt;     summary(newd$S.w)\n#&gt;     newd$SM.w &lt;- with(newd, (W.mediator * S.w))\n#&gt;     newd$SM.w[is.na(newd$SM.w)] &lt;- 0\n#&gt;     summary(newd$SM.w)\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit &lt;- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     TE &lt;- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#&gt;     DE &lt;- exp(unname(coef(fit)[\"exposure\"]))\n#&gt;     IE &lt;- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#&gt;     PM &lt;- log(IE)/log(TE)\n#&gt;     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#&gt; }\n#&gt; &lt;bytecode: 0x0000022ec5e867f0&gt;\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Mediation analysis",
      "Mediation Example"
    ]
  },
  {
    "objectID": "mediationF.html",
    "href": "mediationF.html",
    "title": "R functions (I)",
    "section": "",
    "text": "The list of new R functions introduced in this mediation analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nboot\nboot\nTo conduct bootstrap resampling\n\n\nboot.ci\nboot\nTo calculate confidence intervals from bootstrap samples",
    "crumbs": [
      "Mediation analysis",
      "R functions (I)"
    ]
  },
  {
    "objectID": "mediationQ.html",
    "href": "mediationQ.html",
    "title": "Quiz (I)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "mediationQ.html#live-quiz",
    "href": "mediationQ.html#live-quiz",
    "title": "Quiz (I)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "mediationQ.html#download-quiz",
    "href": "mediationQ.html#download-quiz",
    "title": "Quiz (I)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select ‚ÄúSave link as‚Ä¶‚Äù from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you‚Äôd like to save the file (e.g., Desktop). Remember this location, as you‚Äôll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don‚Äôt have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio‚Äôs console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you‚Äôll see a ‚ÄúRun Document‚Äù button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "Writing tools",
    "section": "",
    "text": "Background\nThis tutorial provides a step-by-step guide to using R, RStudio, GitHub, and GitHub Desktop for collaborative manuscript development. These tools enable version control, streamline teamwork, and enhance documentation throughout the research and writing process. Additionally, it introduces various supplementary resources, including LaTex, TablesGenerator, and Zotero, to further assist and streamline the research and writing processes.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting.html#background",
    "href": "reporting.html#background",
    "title": "Writing tools",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder\nIf you‚Äôre new to R Markdown, consider reviewing the introductory tutorial.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting.html#overview-of-tutorials",
    "href": "reporting.html#overview-of-tutorials",
    "title": "Writing tools",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nGit and GitHub\nThis tutorial introduces the fundamentals of Git and GitHub as essential tools for version control and collaborative research. It explains how Git tracks changes over time, enabling reproducibility and accountability in manuscript writing, while GitHub serves as a cloud-based platform to host and manage Git repositories.\n\n\nConfigure Git in RStudio\nLearn how to configure Git by linking your GitHub credentials to RStudio. This is a necessary step for making authenticated changes to remote repositories.\n\n\nCreate and Clone a Git Repository\nUnderstand how to create a repository on GitHub and ‚Äúclone‚Äù it locally. This step sets up the shared workspace for manuscript development.\n\n\nUpdating Repository Contents from RStudio\nGain familiarity with essential Git commands‚Äîpull, commit, and push‚Äîfor synchronizing changes and managing version control across collaborators.\n\n\nFormatting Resources\nThis section provides a wealth of tools and platforms beneficial for researchers and writers in managing and creating scientific documents. It encompasses LaTex and ShareLaTeX for document preparation, TablesGenerator for converting tables from MS Word to various formats, and R packages like ‚Äúofficer‚Äù and ‚Äúflextable‚Äù for generating tables and charts. It also introduces draw.io for crafting flow charts, platforms like jane for identifying suitable journals, officetimeline for creating Gantt charts, and Google Docs for real-time collaborative writing. Moreover, it highlights Zotero and ZoteroBib as comprehensive tools for reference management and bibliography creation, facilitating organized, collaborative, and streamlined research and writing processes.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou‚Äôll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting1.html",
    "href": "reporting1.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Git is a free and open-source version control system that tracks changes made to documents over time. It allows you to revisit earlier versions of your manuscript, methods, or scripts‚Äîessential for reproducibility and collaboration.\nGitHub is a cloud-based hosting platform for Git repositories. It is widely used by researchers and developers to facilitate collaboration, version tracking, and code sharing. GitHub features such as Copilot, Issues, branching, and GitHub Pages provide powerful tools for managing projects and showcasing your work.\nThis tutorial series will introduce the fundamentals of using Git and GitHub in conjunction with RStudio. Topics include setting up Git locally, cloning repositories, pushing updates to GitHub, and best practices for managing collaborative work.\nFor more in-depth guidance, consult the UBC Library Git & GitHub workshop or the textbook Happy Git and GitHub for the useR.\n\nVideo Content (Optional)\n\n\n\n\n\n\nTip\n\n\n\nPrefer a video walkthrough? Watch the tutorial below for an overview of collaborative writing with GitHub.\n\n\n\n\n\n\n\n\nUseful Resources\n\nManuscript template (view output)\nUBC thesis template in R Markdown\nApply for a GitHub student license\nFree Tableau license for students\nSet up GitHub Copilot in RStudio\nVideo: Using ChatGPT and Copilot in RStudio\nVideo: Creating and managing GitHub branches\nVideo: Resolving Git conflicts in RStudio\nData Management in Large-Scale Education Research by Crystal Lewis\nusethis package documentation\nIntro Git & GitHub workshop ‚Äì UBC Library\nChapter 8 of Data Management in Large-Scale Education Research for README best practices\nManaging Personal Access Tokens\nGit command-line setup\n.gitignore configuration\nChoosing a license for your project",
    "crumbs": [
      "Writing tools",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "reporting1a.html",
    "href": "reporting1a.html",
    "title": "Configure Git in RStudio",
    "section": "",
    "text": "After creating a GitHub account, you must configure Git on your local machine to enable authentication. This process, known as ‚Äúconfiguring Git,‚Äù ensures that Git operations (e.g., initialize, commit, push, pull) can securely interact with GitHub‚Äôs API. Enabling two-factor authentication (2FA) is strongly recommended for added security.\nThis section walks you through setting up Git in RStudio using the usethis package, a user-friendly interface for configuring Git. Every usethis function corresponds to an underlying Git command. For more detail, refer to the usethis documentation and UBC‚Äôs Git and GitHub workshop.\nAll example files are available here. If you‚Äôre new to R Markdown, review the introductory tutorial.\n\nRequirements and Necessary Software\n\nGitHub Account: Click ‚ÄúSign up‚Äù and follow the instructions.\nR and RStudio (see previous chapter)\nGitHub Desktop: Install and sign in.\n\n\n\nConfigure Git in RStudio\n\nOpen Tools &gt; Global Options in RStudio.\n\nNavigate to the ‚ÄúGit/SVN‚Äù section on the left panel.\nConfirm that the ‚ÄúGit executable‚Äù field correctly identifies the Git installation path.\n\n\nInstall required packages:\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\nSet your Git identity:\nusethis::use_git_config(\n  user.name = \"Jane\",           # Replace with your full name\n  user.email = \"jane@example.com\"  # Use your GitHub email\n)\nVerify your configuration:\nusethis::git_sitrep()\n\n\nYour Git name doesn‚Äôt need to match your GitHub username. Use your full name for clarity. If you use multiple computers, consider including the device name for easier identification.\n\n\nAuthenticate GitHub with a Personal Access Token (PAT). PATs are GitHub‚Äôs secure alternative to passwords.\n\nGenerate a token:\nusethis::create_github_token()\nThis opens a GitHub page in your browser with recommended scopes pre-selected. Describe the token‚Äôs purpose clearly (e.g., ‚ÄúSPPH VM‚Äù or ‚ÄúPersonal Laptop‚Äù).\nClick Generate token, copy the token, and keep the tab open.\nStore the token:\ngitcreds::gitcreds_set()\nIf a previous PAT exists, this command allows you to inspect and update it.\n\n\n\n\nUseful Resources\n\nusethis setup guide\nUBC Git & GitHub workshop\nManaging PATs securely\nGit command-line setup",
    "crumbs": [
      "Writing tools",
      "Configure Git in RStudio"
    ]
  },
  {
    "objectID": "reporting1b.html",
    "href": "reporting1b.html",
    "title": "Create and Clone a Git Repository",
    "section": "",
    "text": "This tutorial provides a step-by-step guide for creating repositories on GitHub and cloning them to your local computer. GitHub repositories are remote storage spaces that allow you to track script changes, collaborate with colleagues, and document your research process. Public repositories can also be cloned by others to facilitate reproducibility and collaboration.\n\nRequirements and Necessary Software\n\nGitHub Account: Click ‚ÄúSign up‚Äù and follow the instructions.\nR and RStudio (see previous chapter)\nGitHub Desktop: Install and sign in.\n\n\n\nCreating a Repository on GitHub\n\nSign in to GitHub. Click the ‚Äú+‚Äù icon at the top right and select New repository.\n\nEnter repository details:\n\nName your repository and provide a brief description.\n\nChoose public or private visibility.\n\nSelect initialization options.\n\n\nA README is recommended for basic project documentation. For formatting tips, see Chapter 8 of Data Management in Large-Scale Education Research.\n.gitignore lets you exclude specific files from version control (e.g., .Rhistory or local .csv files). See Git‚Äôs documentation.\nA license defines how others may use your code. The MIT License is commonly used. For guidance, visit choosealicense.com.\n\nClick Create repository to finish.\n\n\n\n\n\nCloning a Repository via Git Bash\n\nVisit the repository you want to clone.\n\nExample: Advanced Epidemiological Methods by Dr.¬†Ehsan Karim.\n\nClick the green &lt; &gt; Code button and copy the HTTPS link.\n\n\nOpen Git Bash.\n\nNavigate to the target directory:\n\nUse pwd to check your current path.\n\nUse cd to move to your preferred folder.\n\n\n\nThe path after ~ shows your current location.\n\nRun the clone command:\ngit clone https://github.com/ehsanx/EpiMethods.git\n\n\n\n\nCloning a Repository via RStudio\n\nGo to the GitHub repository and copy the HTTPS URL.\n\n\nOpen RStudio: File &gt; New Project\n\nSelect Version Control.\n\n\nThen choose Git.\n\n\nComplete the Git project setup:\n\n\nPaste the HTTPS link under Repository URL.\nOptionally, edit the Project directory name.\nClick Browse to choose a folder (e.g., your Desktop).\n\nClick Create Project to finish.\n\nThe cloned repository will now be available in your selected folder.\n\n\n\n\nUseful Resources\n\nBest practices for structuring a repository and writing a README\n.gitignore guide\nChoosing a license\nUBC Library Git & GitHub workshop\nManaging Personal Access Tokens",
    "crumbs": [
      "Writing tools",
      "Create and Clone a Git Repository"
    ]
  },
  {
    "objectID": "reporting1c.html",
    "href": "reporting1c.html",
    "title": "Updating Repository Contents from RStudio",
    "section": "",
    "text": "This section outlines how to update the contents of a cloned GitHub repository using RStudio. It also introduces key Git commands and common workflows for version control and collaboration.\n\nExample Git Workflow in RStudio\nGit is used through a series of commands. The most commonly used are:\n\npull: Retrieves the most recent version of the repository, ensuring all local files are up to date.\n\n\nAlways begin your session by pulling changes to prevent conflicts. See this video for guidance on resolving merge issues.\n\n\ncommit: Records a snapshot of changes made to local files.\n\n\nWrite clear, concise commit messages to help collaborators understand the purpose of each change.\n\n\npush: Uploads committed changes to the remote repository on GitHub.\n\n\nConflicts may arise if changes contradict those already made in the repository. See this video for resolving conflicts in RStudio.\n\nAdditional Git commands and advanced options are available in the Git documentation.\n\n\nRequirements and Necessary Software\n\nA GitHub account\nR and RStudio (refer to previous chapter)\nGitHub Desktop\n\n\n\nExample: Modify a Test Repository\n\nClone the repository and locate the Git tab in RStudio.\n\nPull the latest version of the repository.\n\nEdit the README file and commit changes.\n\n\n\n\n\nStage the modified files, ensure you are working on the main branch, and add a descriptive commit message.\n\nConfirm the commit.\n\n\n\nPush the committed changes to GitHub.\n\n\nVerify the changes on the GitHub repository page.\n\n\n\n\nBookdown Template in RStudio\n\nGo to File &gt; New Project &gt; New Directory &gt; Book Project using bookdown\nName your project (e.g., \"test1\")\nCopy project files to your main repository folder\nRename the project file as appropriate (e.g., \"template\")\n\n\n\nCompiling\n\nOpen index.Rmd\nModify the YAML header and section titles to match your content\nCompile using the Build tab in RStudio\n\n\n\nPublishing Using GitHub Pages\n\nCreate a folder named docs and move compiled HTML files there\nIn GitHub repository settings, set Pages source to the docs folder\nYour site will be available at username.github.io/repository_name\n\n\n\nInvite Collaborators\nNavigate to your repository &gt; Settings &gt; Manage Access &gt; Invite a collaborator.\nEnter the user‚Äôs name or email, assign appropriate access permissions, and send the invitation.\n\n\nUseful Resources\n\nManuscript template (view output)\nUBC thesis template\nGitHub Copilot in RStudio\nSetup video\nManaging branches\nConflict resolution in RStudio",
    "crumbs": [
      "Writing tools",
      "Updating Repository Contents from RStudio"
    ]
  },
  {
    "objectID": "reporting2.html",
    "href": "reporting2.html",
    "title": "Formatting Tools",
    "section": "",
    "text": "The tutorial elucidates a variety of tools and methodologies aimed at streamlining and enhancing academic writing and presentation creation, discussing topics such as utilizing a typesetting system, converting tables across different formats, employing various R packages for enhanced data visualization and presentation, drawing flow and Gantt charts, crafting HTML5 presentations, enabling collaborative writing and document sharing, managing references efficiently, formatting articles, and employing specific platforms for identifying appropriate journals for publishing academic articles.\nLaTex\nLaTeX is a high-quality software system for typesetting document. LaTeX is designed for the production of technical and scientific documentation, such as book, articles.\nGet an account in ShareLaTeX/Overleaf.\nTable conversion\nFrom MS word to latex / markdown / HTML in TablesGenerator.\nYou can use tableone package to generate csv file, and then import them in the TablesGenerator to convert to HTML to paste to doc file!\n\ntab1x &lt;- print(tab1, quote = FALSE, noSpaces = TRUE, printToggle = FALSE)\nwrite.csv(tab1x, file = \"tab1x.csv\")\n\nFancy table and chart generators:\nR Packages\n\nofficer\nofficedown\nflextable\nmschart\nDrawing flow chart\ndraw.io\nPresentation\nThe ‚Äúxaringan‚Äù package, derived from a love for the Japanese manga and anime ‚ÄúNaruto‚Äù, serves as an R Markdown extension, and it facilitates the creation of distinctively styled HTML5 presentations by leveraging the JavaScript library remark.js. Originating from an intent to produce a unique, though not widely adopted style, due to its potentially challenging pronunciation unless familiar with the anime, ‚Äúxaringan‚Äù offers significant customizability in presentation design and has garnered additional theme contributions from its user community. Despite only supporting Markdown, ‚Äúxaringan‚Äù enhances remark.js by introducing support for R Markdown and additional utilities, simplifying the slide-building and previewing processes. Further insights into ‚Äúxaringan‚Äù, its background, and its utility can be explored through its documentation.\nGantt charts\nofficetimeline\nSimultaneous collaborative writing\nGoogle Docs offers a platform for real-time collaborative writing. Multiple users can edit documents simultaneously, and changes are saved automatically. Note that Google Docs requires sign-in for a Google account.\n\nCommenting and Suggesting: Use the comment and suggest features to provide feedback without altering the original text.\nRevision History: Navigate through the revision history to view changes and revert to previous versions if needed.\nSharing and Permissions: Manage who can view, comment, or edit the document with varied permission levels.\nReference manager\nZotero stands out as a free, open-source reference management software that assists researchers, academics, and students in organizing, managing, and formatting their citations and bibliographies. It‚Äôs not just a reference manager but also a powerful tool for collaborative work on research projects. Try the Zotero desktop manager as well for assisting with reference inserting.\nZotero syncs data across devices, ensuring that users can access their libraries from any location. Users can work offline with Zotero, and any changes made will be synchronized when the internet connection is restored.\nZoteroBib: Use ZoteroBib to generate bibliographies instantly without creating an account or installing software.\nArticle formatting\nThe rticles package in R provides a diverse selection of templates for creating academic articles and is easily accessible directly within the RStudio environment by navigating through File -&gt; New File -&gt; R Markdown, where users can select their desired template. For users not utilizing RStudio, the installation of Pandoc is requisite, with articles being creatable using the rmarkdown::draft() function, and specifying the template and package parameters as needed. Additionally, the package enables viewing a list of available journal names using rticles::journals(). To employ enhanced features, such as automatic figure numbering and cross-referencing of tables, users can utilize functionalities from the bookdown package. This involves adjusting the YAML to use bookdown::pdf_book as the output format, and designating the chosen rticles template as the base_format. Comprehensive details and tutorials regarding the use of the rticles package can be found in its online documentation. The complete array of options can be explored within the R Markdown templates window in RStudio, via the packages‚Äôs GitHub readme or accessed programmatically via the following function:\n\nrticles::journals()\n#&gt;  [1] \"acm\"            \"acs\"            \"aea\"            \"agu\"           \n#&gt;  [5] \"ajs\"            \"amq\"            \"ams\"            \"arxiv\"         \n#&gt;  [9] \"asa\"            \"bioinformatics\" \"biometrics\"     \"copernicus\"    \n#&gt; [13] \"ctex\"           \"elsevier\"       \"frontiers\"      \"glossa\"        \n#&gt; [17] \"ieee\"           \"ims\"            \"informs\"        \"iop\"           \n#&gt; [21] \"isba\"           \"jasa\"           \"jedm\"           \"joss\"          \n#&gt; [25] \"jss\"            \"lipics\"         \"lncs\"           \"mdpi\"          \n#&gt; [29] \"mnras\"          \"oup_v0\"         \"oup_v1\"         \"peerj\"         \n#&gt; [33] \"pihph\"          \"plos\"           \"pnas\"           \"rjournal\"      \n#&gt; [37] \"rsos\"           \"rss\"            \"sage\"           \"sim\"           \n#&gt; [41] \"springer\"       \"tf\"             \"trb\"            \"wellcomeor\"\n\nFind appropriate journals\n\njane\nSee more extensive list here\n\nSpecific Epidemiology-focus journals\n\nSummary\n\n\nCategory\nTool\nDescription\n\n\n\nAcademic Search Engine\nGoogle Scholar\nFreely accessible search engine indexing scholarly literature.\n\n\nArticle Formatting\nrticles (R Package)\nR package providing templates for various academic journals.\n\n\nBrainstorming & Mapping\nMindMeister\nOnline mind mapping tool for brainstorming and collaborative visualization.\n\n\nDocument Creation & Editing\nOverleaf\nCollaborative LaTeX editor online.\n\n\n\nGoogle Docs\nPlatform for real-time collaborative writing.\n\n\n\nofficer (R Package)\nR package for generating Word and PowerPoint files.\n\n\n\nofficedown (R Package)\nR package to produce Word documents with officer.\n\n\nGantt Chart Generators\nOffice Timeline\nOnline tool for creating Gantt charts.\n\n\nJournal Finding\njane\nTool to assist in finding the right journal for publishing.\n\n\n\nCustom List\nExtensive list and guide on finding suitable journals.\n\n\n\nEpidemiology Journals\nSpecific list of epidemiology-focus journals.\n\n\nNote & Research Management\nEvernote\nNote-taking and organization tool for managing research notes and drafts.\n\n\nPresentation & Sharing\nPrezi\nDynamic and visually engaging presentation creation tool.\n\n\n\nSlideShare\nPlatform for sharing presentations and professional documents.\n\n\nPresentation\nxaringan (R Package)\nR Markdown extension for creating presentations using remark.js.\n\n\nProject Management\nAsana\nProject management tool for workflow organization and collaboration.\n\n\n\nTrello\nProject management tool for task tracking and collaboration.\n\n\nReference Management\nEndNote\nReference management software for organizing and integrating references.\n\n\n\nJabRef\nOpen-source bibliography reference manager using BibTeX.\n\n\n\nMendeley\nReference management and academic social network.\n\n\n\nPaperpile\nReference management and academic research library.\n\n\n\nZotero\nReference management and collaborative tool.\n\n\n\nZoteroBib\nQuick bibliography generation tool.\n\n\nResearch Identity Management\nORCID\nProvides a persistent digital identifier to distinguish researchers.\n\n\nTable & Chart Generators\ndraw.io\nOnline diagram software for flow charts and various diagrams.\n\n\n\nTablesGenerator\nConverts tables to LaTeX, markdown, HTML formats.\n\n\n\nflextable (R Package)\nR package for tabular reporting in various formats (Word, HTML, etc.).\n\n\n\nmschart (R Package)\nR package to create PowerPoint charts.\n\n\nWriting & Editing\nAuthorea\nCollaborative platform for writing, citing, and publishing.\n\n\n\nGrammarly\nWriting assistant for grammar and style enhancement.",
    "crumbs": [
      "Writing tools",
      "Formatting Tools"
    ]
  }
]