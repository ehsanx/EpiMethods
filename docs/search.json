[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "",
    "text": "The Project\nWelcome to a place crafted to bridge a unique gap in the health research world. This website offers valuable resources for those who are taking their first steps into health research and advanced statistics. Even if you’re familiar with health research, but advanced statistical methods seem daunting, you’re in the right place. Here, we offer:\nThis hub is a part of an open educational initiative, meaning it’s available to everyone. We hope to uplift the standard of health research methodology through this endeavor."
  },
  {
    "objectID": "index.html#what-we-aim-to-achieve",
    "href": "index.html#what-we-aim-to-achieve",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "What We Aim to Achieve",
    "text": "What We Aim to Achieve\nWe’re on a mission to:\n\nEquip public health learners with hands-on experience.\nTeach the nuances of applying advanced epidemiological methods using real data.\nOffer a unique open textbook that’s enriched with interactive tools and quizzes for a self-paced learning experience."
  },
  {
    "objectID": "index.html#dive-into-our-modules",
    "href": "index.html#dive-into-our-modules",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Dive into Our Modules",
    "text": "Dive into Our Modules\nEmbark on a journey through 10 core learning modules, and one introductory module about R (module 0). Letters in the parentheses (W, A, Q, R, P, D, M, S, L, C, G) are the chapter indicators; these are indicated along with quizzes, R functions, and exercises associated with the corresponding chapters. Only key chapters have exercises.\n\n\n\n\n\n Module \n    Topics.Indicators \n    Descriptions \n  \n\n\n 0 \n    R for Data Wrangling (W) \n    Get to know R. \n  \n\n 1 \n    Accessing (A) Survey Data Resources \n    Understand and source reliable national survey data. \n  \n\n 2 \n    Crafting Analytic Data for Research Questions (Q) \n    Customize data to your research query. \n  \n\n 3 \n    Causal roles (R) \n    Delve into the concept of confounding and its implications. \n  \n\n 4 \n    Predictive (P) issues \n    Introduction to key concepts of prediction modelling. \n  \n\n 5 \n    Complex Survey Data (D) Analysis \n    Handle data sets obtained from complex survey designs. \n  \n\n 6 \n    Missing (M) Data Analysis \n    Understand and tackle missingness in your data. \n  \n\n 7 \n    Propensity Score (S) Analysis \n    Dive deeper into advanced observational data analyses. \n  \n\n 8 \n    Machine Learning (L) \n    Introduction to machine learning algorithms, and applications. \n  \n\n 9 \n    Intergrating Machine Learners in Causal (C) Inference \n    Discusses the potential pitfalls and challenges in merging machine learning with causal inference, and a way forward. \n  \n\n 10 \n    Scientific Writing Tools (G) \n    Tools and guides for scientific writing and collaboration. \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tutorial is designed with a consistent structure across all chapters to provide a cohesive and thorough learning experience. Here’s what you can expect in each chapter:\n\nOverview: The first page of each chapter offers a concise summary that outlines the key learning objectives, topics covered, and what you can expect to gain from the chapter. The overview page will also feature links to the data sources used in the tutorials as well as a form where you can report any bugs or issues you encounter. This helps you quickly grasp the chapter’s essence and set learning expectations.\nTutorial Topics: Immediately following the overview, you’ll find in-depth tutorials that cover each topic in detail. These are designed to provide comprehensive insights and are spread across multiple pages for easier navigation and understanding.\nSummary of R Functions: Each chapter includes a succinct summary of the R functions used in the tutorials. This serves as a quick reference guide for learners to understand the tools they will be applying.\nChapter-Specific Quiz: For those interested in self-assessment, each chapter concludes with an optional quiz. This is a self-paced learning tool to help reinforce the chapter’s key concepts.\nPractice Exercises: Finally, practice exercises are available for selected chapters to help you apply what you’ve learned in a hands-on manner. These exercises are designed to reinforce your understanding and give you practical experience with the chapter’s topics."
  },
  {
    "objectID": "index.html#how-our-content-is-presented",
    "href": "index.html#how-our-content-is-presented",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "How Our Content is Presented",
    "text": "How Our Content is Presented\nAll our resources are hosted on an easy-to-access GitHub page. The format? Engaging text, reproducible software codes, clear analysis outputs, and crisp videos that distill complex topics. And don’t miss our quiz section at the end of each module for a quick self-check on what you’ve learned. This document is created using quarto and R."
  },
  {
    "objectID": "index.html#open-copyright-license",
    "href": "index.html#open-copyright-license",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Open Copyright License",
    "text": "Open Copyright License\nCC-BY"
  },
  {
    "objectID": "index.html#contributor-list",
    "href": "index.html#contributor-list",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Contributor list",
    "text": "Contributor list\nDive into this captivating content, brought to life with the generous support of the UBC OER Fund Implementation Grant and further supported by UBC SPPH. The foundation of this content traces back to the PI’s work over five years while instructing SPPH 604 (2018-2022). That knowledge have now transformed into an open educational resource, thanks to this grant. Meet the innovative minds behind the grant proposal below.\n\n\n\n\n\n Role \n    Team_Member \n    Affiliation \n  \n\n\n Principal Applicant (PI) \n    Dr. M Ehsan Karim \n    UBC School of Population and Public Health \n  \n\n Co-applicant (Co-I) \n    Dr. Suborna Ahmed \n    UBC Department of Forest Resources Management \n  \n\n Trainee co-applicants \n    Md Belal Hossain \n    UBC School of Population and Public Health \n  \n\n  \n    Fardowsa Yusuf \n    UBC School of Population and Public Health \n  \n\n  \n    Hanna Frank \n    UBC School of Population and Public Health \n  \n\n  \n    Dr. Michael Asamoah-Boaheng \n    UBC Department of Emergency Medicine \n  \n\n  \n    Chuyi (Astra) Zheng \n    UBC Faculty of Arts \n  \n\n\n\n\nAdditional earlier contributors to the course material development, who were not part of this current OER grant, include Derek Ouyang, Kate McLeod (both from UBC School of Population and Public Health), and Mohammad Atiquzzaman (UBC Pharmaceutical Sciences). Numerous pieces of student feedback were also incorporated in order to update the content."
  },
  {
    "objectID": "wrangling.html#background",
    "href": "wrangling.html#background",
    "title": "Data wrangling",
    "section": "Background",
    "text": "Background\nThe realm of data science is vast, and one of its foundational pillars is data wrangling. Data wrangling, often known as data munging, is the process of transforming raw data into a more digestible and usable format for analysis. In the context of R, a powerful statistical programming language, data wrangling becomes an essential skill for any data enthusiast. This chapter is dedicated to imparting practical knowledge on various data manipulation, import, and summarization techniques in R. Through a series of meticulously crafted tutorials, you will be equipped with the tools and techniques to handle, transform, and visualize data efficiently.\n\n\nIntroducing R Basics at the outset of an epidemiological methods tutorial book is akin to laying the foundation before building a house. It ensures that all readers, regardless of their prior experience, start on the same page, understanding the fundamental tools and language of R. This foundational knowledge not only smoothens the learning curve but also boosts confidence, allowing learners to focus on complex epidemiological techniques without being bogged down by the intricacies of the R language. In essence, mastering the basics first ensures a more cohesive and effective learning experience as the material advances.\nIn this chapter, we embark on a structured journey through the intricate world of data wrangling in R. We begin by laying a solid foundation with R Basics, ensuring you grasp the essential elements of R programming. Once grounded in the basics, we progress to understanding the core R Data Types, diving deep into matrices, lists, and data frames. With a firm grasp of these structures, we introduce Automating Tasks to empower you with techniques that streamline the handling of vast datasets. Following this, we delve into the practical aspects of Importing Datasets, showcasing various methods to bring data from different formats into R. Building on this, Data Manipulation comes next, where we explore the myriad ways to modify and reshape your datasets to suit analytical needs. We then turn our attention to Importing External Data, offering a hands-on demonstration of how to integrate specific external datasets into your R environment. As we approach the chapter’s culmination, we emphasize the importance of Summary Tables in medical research, teaching you the art and science of data summarization. Finally, we wrap up with R Markdown, providing a comprehensive guide on how to seamlessly document your R code and analytical findings, ensuring your work is both reproducible and presentable.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "wrangling.html#overview-of-tutorials",
    "href": "wrangling.html#overview-of-tutorials",
    "title": "Data wrangling",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nR Basics\nThis tutorial introduces the basics of R programming. It covers topics such as setting up R and RStudio, using R as a calculator, creating variables, working with vectors, plotting data, and accessing help resources.\n\n\nR Data Types\nThis tutorial covers three primary data structures in R: matrices, lists, and data frames. Matrices are two-dimensional arrays with elements of the same type, and their manipulation includes reshaping and combining. Lists in R are versatile collections that can store various R objects, including matrices. Data frames, on the other hand, are akin to matrices but permit columns of diverse data types. The tutorial offers guidance on creating, modifying, and merging data frames and checking their dimensions.\n\n\nAutomating Tasks\nMedical data analysis often grapples with vast and intricate data sets. Manual handling isn’t just tedious; it’s error-prone, especially given the critical decisions hinging on the results. This tutorial introduces automation techniques in R, a leading language for statistical analysis. By learning to use loops and functions, you can automate repetitive tasks, minimize errors, and conduct analyses more efficiently. Dive in to enhance your data handling skills.\n\n\nImporting Dataset\nThis tutorial focuses on importing data into R. It demonstrates how to import data from CSV and SAS formats using functions like read.csv and sasxport.get. It also includes examples of loading specific variables, dropping variables, subsetting observations based on certain criteria, and handling missing values.\n\n\nData Manipulation\nThis tutorial explores various data manipulation techniques in R. It covers topics such as dropping variables from a dataset, keeping specific variables, subsetting observations based on specific criteria, converting variable types (e.g., factors, strings), and handling missing values.\n\n\nImport External Data\nThis tutorial provides examples of importing external data into R. It includes specific examples of importing a CSV file (Employee Salaries - 2017 data) and a SAS file (NHANES 2015-2016 data). It also demonstrates how to save a working dataset in different formats, such as CSV and RData.\n\n\nSummary Tables\nThis tutorial emphasizes the importance of data summarization in medical research and epidemiology, specifically how to summarize medical data using R. It demonstrates creating “Table 1”, a typical descriptive statistics table in research papers, with examples that use the built-in R functions and specialized packages to efficiently summarize and stratify data.\n\n\nR Markdown\nThis beginner-friendly tutorial guides you through working with R Markdown (RMD) files in RStudio, a popular IDE for R. The tutorial covers installing prerequisites, creating a new RMD file, and the basics of “knitting” to compile the document into various formats like HTML, PDF, or Word. It delves into embedding R code chunks and plain text within the RMD file, using the knitr package for document rendering. Tips for troubleshooting common issues and additional resources for further learning are also provided.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "wrangling1a.html",
    "href": "wrangling1a.html",
    "title": "R basics",
    "section": "",
    "text": "Important\n\n\n\nShow/hide code:\nOn every page, at the top, you’ll find a </> button. Click it to toggle the visibility of all R code on the page at once. Alternatively, you can click ‘Show the code’ within individual code chunks to view code on a case-by-case basis.\n\n\nStart using R\nTo get started with R, follow these steps:\n\nDownload and Install R: Grab the newest version from the official R website. > Tip: Download from a Comprehensive R Archive Network (CRAN) server near your geographic location.\nDownload and Install RStudio: You can get it from this link. > Note: RStudio serves as an Integrated Development Environment (IDE) offering a user-friendly interface. It facilitates operations such as executing R commands, preserving scripts, inspecting results, managing data, and more.\nBegin with RStudio: Once you open RStudio, delve into using R. For starters, employ the R syntax for script preservation, allowing future code adjustments and additions.\nBasic syntax\n\n\n\n\n\n\nTip\n\n\n\nR, a versatile programming language for statistics and data analysis, can execute numerous tasks. Let’s break down some of the fundamental aspects of R’s syntax.\n\n\n\nUsing R as a Calculator\n\nSimilar to how you’d use a traditional calculator for basic arithmetic operations, R can perform these functions with ease. For instance:\n\nShow the code# Simple arithmetic\n1 + 1\n#> [1] 2\n\n\nThis is a basic addition, resulting in 2.\nA more intricate calculation:\n\nShow the code# Complex calculation involving \n# multiplication, subtraction, division, powers, and square root\n20 * 5 - 10 * (3/4) * (2^3) + sqrt(25)\n#> [1] 45\n\n\nThis demonstrates R’s capability to handle complex arithmetic operations.\n\nVariable Assignment in R\n\nR allows you to store values in variables, acting like labeled containers that can be recalled and manipulated later. For example,\n\nShow the code# Assigning a value of 2 to variable x1\nx1 <- 2\nprint(x1)\n#> [1] 2\n\n\nSimilarly:\n\nShow the codex2 <- 9\nx2\n#> [1] 9\n\n\n\nCreating New Variables Using Existing Ones\n\nYou can combine and manipulate previously assigned variables to create new ones.\n\nShow the code# Using variable x1 \n# to compute its square and assign to y1\ny1 <- x1^2\ny1\n#> [1] 4\n\n\nYou can also use multiple variables in a single expression:\n\nShow the codey2 <- 310 - x1 + 2*x2 - 5*y1^3\ny2\n#> [1] 6\n\n\n\nCreating Functions\n\nFunctions act as reusable blocks of code. Once defined, they can be called multiple times with different arguments. Here’s how to define a function that squares a number:\n\nShow the codez <- function(x) {x^2}\n\n\nR also comes with a plethora of built-in functions. Examples include exp (exponential function) and rnorm (random number generation from a normal distribution).\n\nUtilizing Built-In Functions\n\nFor instance, using the exponential function:\n\nShow the code# Calling functions\nexp(x1)\n#> [1] 7.389056\nlog(exp(x1))\n#> [1] 2\n\n\nThe rnorm function can generate random samples from a normal distribution: below we are generating 10 random sampling from the normal distribution with mean 0 and standard deviation 1:\n\nShow the codernorm(n = 10, mean = 0, sd = 1)\n#>  [1] -1.35075652 -0.50412761 -0.19264235  0.26622497  1.13099668  0.07959930\n#>  [7]  1.00311899  1.21638989 -0.20415432  0.05776501\n\n\nAs random number generation relies on algorithms, results will differ with each execution.\n\nShow the code# Random sampling (again)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.22434788  0.17886145 -1.91964887 -0.10497858 -1.83003488  0.40758183\n#>  [7]  0.31717725 -0.77952024  0.05435223  0.13119455\n\n\nHowever, by setting a seed, we can reproduce identical random results:\n\nShow the code# Random sampling (again, but with a seed)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\n\nShow the code# random sampling (reproducing the same numbers)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\nAs we can see, when we set the same seed, we get exactly the same random number. This is very important for reproducing the same results. There are many other pre-exiting functions in R.\n\nSeeking Help in R\n\n\n\n\n\n\n\nTip\n\n\n\nR’s help function, invoked with ?function_name, provides detailed documentation on functions, assisting users with unclear or forgotten arguments:\n\n\n\nShow the code# Searching for help if you know \n# the exact name of the function with a question mark\n?curve\n\n\nBelow is an example of using the pre-exiting function for plotting a curve ranging from -10 to 10.\n\nShow the code# Plotting a function\ncurve(z, from = -10, to = 10, xlab = \"x\", ylab = \"Squared x\")\n\n\n\n\nIf some of the arguments are difficult to remember or what else could be done with that function, we could use the help function. For example, we can simply type help(curve) or ?curve to get help on the curve function:\n\n\n\n\n\n\nTip\n\n\n\nIf you’re uncertain about a function’s precise name, two question marks can assist in the search:\n\n\n\nShow the code# Searching for help if don't know \n# the exact name of the function\n??boxplot\n\n\n\nCreating Vectors\n\nVectors are sequences of data elements of the same basic type. Here are some methods to create them:\n\nShow the code# Creating vectors in different ways\nx3 <- c(1, 2, 3, 4, 5)\nprint(x3)\n#> [1] 1 2 3 4 5\n\nx4 <- 1:7\nprint(x4)\n#> [1] 1 2 3 4 5 6 7\n\nx5 <- seq(from = 0, to = 100, by = 10)\nprint(x5)\n#>  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nx6 <- seq(10, 30, length = 7)\nx6\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n\n\n\nPlotting in R\n\nR provides numerous plotting capabilities. For instance, the plot function can create scatter plots and line graphs:\n\nShow the code# Scatter plot\nplot(x5, type = \"p\", main = \"Scatter plot\")\n\n\n\n\n\nShow the code# Line graph\nplot(x = x6, y = x6^2, type = \"l\", main = \"Line graph\")\n\n\n\n\n\nCharacter Vectors Apart from numeric values, R also allows for character vectors. For example, we can create a sex variable coded as females, males and other.\n\n\nShow the code# Character vector\nsex <- c(\"females\", \"males\", \"other\")\nsex\n#> [1] \"females\" \"males\"   \"other\"\n\n\nTo determine a variable’s type, use the mode function:\n\nShow the code# Check data type\nmode(sex)\n#> [1] \"character\"\n\n\nPackage Management\nPackages in R are collections of functions and datasets developed by the community. They enhance the capability of R by adding new functions for data analysis, visualization, data import, and more. Understanding how to install and load packages is essential for effective R programming.\n\nInstalling Packages from CRAN\n\nThe CRAN is a major source of R packages. You can install them directly from within R using the install.packages() function.\n\nShow the code# Installing the 'ggplot2' package\ninstall.packages(\"ggplot2\")\n\n\n\nLoading a Package\n\nAfter a package is installed, it must be loaded to use its functions. This is done with the library() function.\n\nShow the code# Loading the 'ggplot2' package\nlibrary(ggplot2)\n\n\nYou only need to install a package once, but you’ll need to load it every time you start a new R session and want to use its functions.\n\nUpdating Packages\n\nR packages are frequently updated. To ensure you have the latest version of a package, use the update.packages() function.\n\nShow the code# Updating all installed packages\n# could be time consuming!\nupdate.packages(ask = FALSE)  \n# 'ask = FALSE' updates all without asking for confirmation\n\n\n\nListing Installed Packages\n\nYou can view all the installed packages on your R setup using the installed.packages() function.\n\nShow the code# Listing installed packages\ninstalled.packages()[, \"Package\"]\n\n\n\nRemoving a Package\n\nIf you no longer need a package, it can be removed using the remove.packages() function.\n\nShow the code# Removing the 'ggplot2' package\nremove.packages(\"ggplot2\")\n\n\n\nInstalling Packages from Other Sources\n\nWhile CRAN is the primary source, sometimes you might need to install packages from GitHub or other repositories. The devtools package provides a function for this.\n\nShow the code# Installing devtools first\ninstall.packages(\"devtools\")\n# Loading devtools\nlibrary(devtools)\n# Install a package from GitHub\n# https://github.com/ehsanx/simMSM\ninstall_github(\"ehsanx/simMSM\")\n\n\nWhen you are working on a project, it’s a good practice to list and install required packages at the beginning of your R script.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling1b.html",
    "href": "wrangling1b.html",
    "title": "Data types",
    "section": "",
    "text": "Matrix\n\n\n\n\n\n\nTip\n\n\n\nIn R, matrices are two-dimensional rectangular data sets, which can be created using the matrix() function. It’s essential to remember that all the elements of a matrix must be of the same type, such as all numeric or all character.\n\n\nTo construct a matrix, we often start with a vector and specify how we want to reshape it. For instance:\n\nShow the code# Matrix 1\nx <- 1:10\nmatrix1 <- matrix(x, nrow = 5, ncol = 2, byrow = TRUE)\nmatrix1\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nHere, the vector x contains numbers from 1 to 10. We reshape it into a matrix with 5 rows and 2 columns. The byrow = TRUE argument means the matrix will be filled row-wise, with numbers from the vector.\nConversely, if you want the matrix to be filled column-wise, you’d set byrow = FALSE:\n\nShow the code# matrix 2\nmatrix2 <- matrix(x, nrow = 5, ncol = 2, byrow = FALSE)\nmatrix2\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nYou can also combine or concatenate matrices. cbind() joins matrices by columns while rbind() joins them by rows.\n\nShow the code# Merging 2 matrices\ncbind(matrix1, matrix2)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    1    6\n#> [2,]    3    4    2    7\n#> [3,]    5    6    3    8\n#> [4,]    7    8    4    9\n#> [5,]    9   10    5   10\n\n\n\nShow the code# Appending 2 matrices\nrbind(matrix1, matrix2)\n#>       [,1] [,2]\n#>  [1,]    1    2\n#>  [2,]    3    4\n#>  [3,]    5    6\n#>  [4,]    7    8\n#>  [5,]    9   10\n#>  [6,]    1    6\n#>  [7,]    2    7\n#>  [8,]    3    8\n#>  [9,]    4    9\n#> [10,]    5   10\n\n\nList\n\n\n\n\n\n\nTip\n\n\n\nIn R, lists can be seen as a collection where you can store a variety of different objects under a single name. This includes vectors, matrices, or even other lists. It’s very versatile because its components can be of any type of R object.\n\n\nFor instance:\n\nShow the code# List of 2 matrices\nlist1 <- list(matrix1, matrix2)\nlist1\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nLists can also be expanded to include multiple items:\n\nShow the codex6 <- seq(10, 30, length = 7)\nsex <- c(\"females\", \"males\", \"other\")\n# Expanding list to include more items\nlist2 <- list(list1, x6, sex, matrix1)\nlist2 \n#> [[1]]\n#> [[1]][[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[1]][[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n#> \n#> \n#> [[2]]\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n#> \n#> [[3]]\n#> [1] \"females\" \"males\"   \"other\"  \n#> \n#> [[4]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nCombining different types of data into a single matrix converts everything to a character type:\n\nShow the code# A matrix with numeric and character variables\nid <- c(1, 2)\nscore <- c(85, 85)\nsex <- c(\"M\", \"F\")\nnew.matrix <- cbind(id, score, sex)\nnew.matrix\n#>      id  score sex\n#> [1,] \"1\" \"85\"  \"M\"\n#> [2,] \"2\" \"85\"  \"F\"\n\n\nTo check the type of data in your matrix:\n\nShow the codemode(new.matrix)\n#> [1] \"character\"\n\n\nData frame\n\n\n\n\n\n\nTip\n\n\n\nAs we can see combining both numeric and character variables into a matrix ended up with a matrix of character values. To keep the numeric variables as numeric and character variables as character, we can use the data.frame function.\n\n\n\nCreating a data frame\n\n\n\nA data frame is similar to a matrix but allows for columns of different types (numeric, character, factor, etc.). It’s a standard format for storing data sets in R.\n\nShow the codedf <- data.frame(id, score, sex)\ndf\n\n\n\n  \n\n\n\nTo check the mode or type of your data frame:\n\nShow the codemode(df)\n#> [1] \"list\"\n\n\n\nExtract elements\n\nData frames allow easy extraction and modification of specific elements. For example, we can extract the values on the first row and first column as follow:\n\nShow the codedf[1,1]\n#> [1] 1\n\n\nSimilarly, the first column can be extracted as follows:\n\nShow the codedf[,1]\n#> [1] 1 2\n\n\nThe first row can be extracted as follows:\n\nShow the codedf[1,]\n\n\n\n  \n\n\n\n\nModifying values\n\nWe can edit the values in the data frame as well. For example, we can change the score from 85 to 90 for the id 1:\n\nShow the codedf$score[df$id == 1] <- 90\ndf\n\n\n\n  \n\n\n\nWe can also change the name of the variables/columns:\n\nShow the codecolnames(df) <- c(\"Studyid\", \"Grade\", \"Sex\")\ndf\n\n\n\n  \n\n\n\n\nCombining data frames\n\nWe can also merge another data frame with the same variables using the rbind function:\n\nShow the code# Create a new dataset\ndf2 <- data.frame(Studyid = c(10, 15, 50), Grade = c(75, 90, 65), Sex = c(\"F\", \"M\", \"M\"))\n\n# Combining two data frames\ndf.new <- rbind(df, df2)\n\n# Print the first 6 rows\nhead(df.new)\n\n\n\n  \n\n\n\n\nChecking the dimensions\n\nTo see the dimension of the data frame (i.e., number of rows and columns), we can use the dim function:\n\nShow the codedim(df.new)\n#> [1] 5 3\n\n\nAs we can see, we have 5 rows and 3 columns. We can use the nrow and ncol functions respectively for the same output:\n\nShow the codenrow(df.new)\n#> [1] 5\nncol(df.new)\n#> [1] 3"
  },
  {
    "objectID": "wrangling1c.html",
    "href": "wrangling1c.html",
    "title": "Automating tasks",
    "section": "",
    "text": "Repeating a task\n\n\n\n\n\n\nTip\n\n\n\nThe for loop is a control flow statement in R that lets you repeat a particular task multiple times. This repetition is based on a sequence of numbers or values in a vector.\n\n\nConsider a simple real-life analogy: Imagine you are filling water in 10 bottles, one by one. Instead of doing it manually 10 times, you can set a machine to do it in a loop until all 10 bottles are filled.\n\nExample 1\n\n\nShow the code# Looping and adding\nk <- 0\nfor (i in 1:10){\n  k <- k + 5\n  print(k)\n}\n#> [1] 5\n#> [1] 10\n#> [1] 15\n#> [1] 20\n#> [1] 25\n#> [1] 30\n#> [1] 35\n#> [1] 40\n#> [1] 45\n#> [1] 50\n\n\nHere, you’re initiating a counter k at 0. With each iteration of the loop (i.e., every time it “runs”), 5 is added to k. After 10 cycles, the loop will stop, but not before printing k in each cycle.\n\nExample 2\n\nWe create a variable x5 containing the values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. Let us print the first 5 values using the for loop function:\n\nShow the codex5 <- seq(from = 0, to = 100, by = 10)\n# Looping through a vector\nk <- 1:5\nfor (ii in k){\n  print(x5[ii])\n}\n#> [1] 0\n#> [1] 10\n#> [1] 20\n#> [1] 30\n#> [1] 40\n\n\nThis loop cycles through the first five values of a previously created variable x5 and prints them. Each value printed corresponds to the positions 1 to 5 in x5.\n\nExample 3\n\nLet us use the for loop in a more complicated scenario. First, we create a vector of numeric values and square it:\n\nShow the code# Create a vector\nk <- c(1, 3, 6, 2, 0)\nk^2\n#> [1]  1  9 36  4  0\n\n\nThis is just squaring each value in the vector k.\n\nExample 4\n\nWe create the same vector of square values using the for loop function. To do so, (i) we create a null object, (ii) use the loop for each of the elements in the vector (k), (iii) square each of the elements, and (iv) store each of the elements of the new vector. In the example below, the length of k is 5, and the loop will run from the first to the fifth element of k. Also, k.sq[1] is the first stored value for squared-k, and k.sq[2] is the second stored value for squared-k, and so on.\n\nShow the code# Looping through a vector with function\nk.sq <- NULL\nfor (i in 1:length(k)){\n  k.sq[i] <- k[i]^2\n}\n\n# Print the values\nk.sq\n#> [1]  1  9 36  4  0\n\n\nHere, we achieve the same result as the third example but use a for loop. We prepare an empty object k.sq and then use the loop to square each value in k, storing the result in k.sq.\n\nExample 5\n\n\nShow the codedf.new <- data.frame(\n  Studyid = c(1, 2, 10, 15, 50),\n  Grade = c(90, 85, 75, 90, 65),\n  Sex = c('M', 'F', 'F', 'M', 'M')\n)\n# Looping through a data frame\nfor (i in 1:nrow(df.new)){\n  print(df.new[i,\"Sex\"])\n}\n#> [1] \"M\"\n#> [1] \"F\"\n#> [1] \"F\"\n#> [1] \"M\"\n#> [1] \"M\"\n\n\nThis loop prints the “Sex” column value for each row in the df.new data frame.\nFunctions\n\n\n\n\n\n\nTip\n\n\n\nA function in R is a piece of code that can take inputs, process them, and return an output. There are functions built into R, like mean(), which calculates the average of a set of numbers.\n\n\n\nBuilt-in function\n\n\nShow the code# Calculating a mean from a vector\nVector <- 1:100\nmean(Vector)\n#> [1] 50.5\n\n\nHere, we’re using the built-in mean() function to find the average of numbers from 1 to 100.\n\nCustom-made function\n\nTo understand how functions work, sometimes it’s helpful to build our own. Now we will create our own function to calculate the mean, where we will use the following equation to calculate it:\n\\(\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n},\\)\nwhere \\(x_1\\), \\(x_2\\),…, \\(x_n\\) are the values in the vector and \\(n\\) is the sample size. Let us create the function for calculation the mean:\nThis function, mean.own, calculates the average. We add up all the numbers in a vector (Sum <- sum(x)) and divide by the number of items in that vector (n <- length(x)). The result is then returned.\n\nShow the codemean.own <- function(x){\n  Sum <- sum(x)\n  n <- length(x)\n  return(Sum/n)\n}\n\n\nBy using our custom-made function, we calculate the mean of numbers from 1 to 100, getting the same result as the built-in mean() function.\n\nShow the codemean.own(Vector)\n#> [1] 50.5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling2.html",
    "href": "wrangling2.html",
    "title": "Importing dataset",
    "section": "",
    "text": "Introduction to Data Importing\nBefore analyzing data in R, one of the first steps you’ll typically undertake is importing your dataset. R provides numerous methods to do this, depending on the format of your dataset.\nDatasets come in a variety of file formats, with .csv (Comma-Separated Values) and .txt (Text file) being among the most common. While R’s interface offers manual ways to load these datasets, knowing how to code this step ensures better reproducibility and automation.\nImporting .txt files\nA .txt data file can be imported using the read.table function. As an example, consider you have a dataset named grade in the specified path.\nLet’s briefly glance at the file without concerning ourselves with its formatting.\n\nShow the code# Read and print the content of the TXT file\ncontent <- readLines(\"Data/wrangling/grade.txt\")\ncat(content, sep = \"\\n\")\n#> Studyid Grade Sex\n#> 1    90   M\n#> 2    85   F\n#> 10    75   F\n#> 15    90   M\n#> 50    65   M\n\n\nUsing the read.table function, you can load this dataset in R properly. It’s important to specify header = TRUE if the first row of your dataset contains variable names.\n\nTip: Always ensure the header argument matches the structure of your dataset. If your dataset contains variable names, set header = TRUE.\n\n\nShow the code## Read a text dataset\ngrade <- read.table(\"Data/wrangling/grade.txt\", header = TRUE, sep = \"\\t\", quote = \"\\\"\")\n# Display the first few rows of the dataset\nhead(grade)\n\n\n\n  \n\n\n\nImporting .csv files\nSimilarly, .csv files can be loaded using the read.csv function. Here’s how you can load a .csv dataset named mpg:\n\nShow the code## Read a csv dataset\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n# Display the first few rows of the dataset\nhead(mpg)\n\n\n\n  \n\n\n\nWhile we’ve discussed two popular data formats, R can handle a plethora of other formats. For further details, refer to Quick-R (2023). Notably, some datasets come built-in with R packages, like the mpg dataset in the ggplot2 package. To load such a dataset:\n\nShow the codedata(mpg, package = \"ggplot2\")\nhead(mpg)\n\n\n\n  \n\n\n\nTo understand more about the variables and the dataset’s structure, you can consult the documentation:\n\nShow the code?mpg\n\n\nData Screening and Understanding Your Dataset\ndim(), nrow(), ncol(), and str() are incredibly handy functions when initially exploring your dataset.\nOnce your data is in R, the next logical step is to get familiar with it. Knowing the dimensions of your dataset, types of variables, and the first few entries can give you a quick sense of what you’re dealing with.\nFor instance, str (short for structure) is a concise way to display information about your data. It reveals the type of each variable, the first few entries, and the total number of observations:\n\nShow the codestr(mpg)\n#> tibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n#>  $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n#>  $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n#>  $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n#>  $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n#>  $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n#>  $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n#>  $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n#>  $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n#>  $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n#>  $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n#>  $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nIn summary, becoming proficient in data importing and initial screening is a fundamental step in any data analysis process in R. It ensures that subsequent stages of data manipulation and analysis are based on a clear understanding of the dataset at hand.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\nReferences\n\n\n\n\n\n\nQuick-R. 2023. “Importing Data.” https://www.statmethods.net/input/importingdata.html."
  },
  {
    "objectID": "wrangling3.html",
    "href": "wrangling3.html",
    "title": "Data manipulation",
    "section": "",
    "text": "Data manipulation is a foundational skill for data analysis. This guide introduces common methods for subsetting datasets, handling variable types, creating summary tables, and dealing with missing values using R.\nLoad dataset\nUnderstanding the dataset’s structure is the first step in data manipulation. Here, we’re using the mpg dataset, which provides information on various car models:\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\n\nSubset\nOften, you’ll need to subset your data for analysis. Here, we’ll explore different methods to both drop unwanted variables and keep desired observations.\nDrop variables\nSometimes, only part of the variables will be used in your analysis. Therefore, you may want to drop the variables you do not need. There are multiple ways to drop variables from a dataset. Below are two examples without using any package and using the dplyr package.\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[, c(the columns you want to KEEP)]\n\n\nSay, we want to keep only three variables in the mpg dataset: manufacturer, model and cyl. For Option 1 (without package), we can use the following R codes to keep these three variables:\n\nShow the codempg1 <- mpg[, c(\"manufacturer\", \"model\", \"cyl\")]\nhead(mpg1)\n\n\n\n  \n\n\n\nHere mpg1 is a new dataset containing only three variables (manufacturer, model and cyl).\n\n\n\n\n\n\nTip\n\n\n\nOption 2: use select in dplyr\nselect(dataset.name, …(columns names you want to KEEP))\n\n\nFor Option 2, the dplyr package offers the select function, which provides a more intuitive way to subset data.\n\nShow the codempg2 <- select(mpg, c(\"manufacturer\", \"model\", \"cyl\"))\nhead(mpg2)\n\n\n\n  \n\n\n\nWe can also exclude any variables from the dataset by using the minus (-) sign with the select function. For example, we we want to drop trans, drv, and cty from the mpg dataset, we can use the following codes:\n\nShow the codempg3 <- select(mpg, -c(\"trans\", \"drv\", \"cty\"))\nhead(mpg3)\n\n\n\n  \n\n\n\nThis mpg3 is a new dataset from mpg after dropping three variables (trans, drv, and cty).\nKeep observations\nIt often happens that we only want to investigate a subset of a population which only requires a subset of our dataset. In this case, we need to subset the dataset to meet certain requirements. Again, there are multiple ways to do this task. Below is an example without a package and with the dplyr package:\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[the rows you want to KEEP, ]\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 2: No package needed\nsubset(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 3: use select in dplyr\nfilter(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nCommon logical tests are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n X <(=) Y \n    Smaller (equal) than \n  \n\n X >(=) Y \n    Larger (equal) than \n  \n\n X == Y \n    Equal to \n  \n\n X != Y \n    Not equal to \n  \n\n is.na(X) \n    is NA/missing? \n  \n\n\n\n\n\n\nSay, we want to keep the observations for which cars are manufactured in 2008. We can use the following R codes to do it:\n\nShow the codempg4 <- mpg[mpg$year == \"2008\",] # Option 1\nhead(mpg4)\n\n\n\n  \n\n\n\nThe following codes with the subset and filter function will do the same:\n\nShow the codempg5 <- subset(mpg, year == \"2008\") # Option 1\nhead(mpg5)\n\n\n\n  \n\n\n\n\nShow the codempg6 <- filter(mpg, year == \"2008\") # Option 3\nhead(mpg6)\n\n\n\n  \n\n\n\nThe filter function can also work when you have multiple criteria (i.e., multiple logical tests) to satisfy. Here, we need Boolean operators to connect different logical tests.\n\n\n\n\n\n\nTip\n\n\n\nCommon boolean operators are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n & \n    and \n  \n\n | \n    or \n  \n\n ! \n    not \n  \n\n == \n    equals to \n  \n\n != \n    not equal to \n  \n\n > \n    greater than \n  \n\n < \n    less than \n  \n\n >= \n    greater than or equal to \n  \n\n <= \n    less than or equal to \n  \n\n\n\n\n\n\nSay, we want to keep the observations for 6 and 8 cylinders (cyl) and engine displacement (displ) greater than or equal to 4 litres. We can use the following codes to do the task:\n\nShow the codempg7 <- filter(mpg, cyl %in% c(\"6\",\"8\") & displ >= 4)\nhead(mpg7)\n\n\n\n  \n\n\n\n\n\nThe %in% operator is used to determine whether the values of the first argument are present in the second argument.\nHandling Variable Types\n\n\n\n\n\n\nTip\n\n\n\nMost common types of variable in R are\n\nnumbers,\nfactors and\nstrings(or character).\n\nUnderstanding and manipulating these types are crucial for data analysis.\n\n\n\nIdentifying Variable Type\n\nWhen we analyze the data, we usually just deal with numbers and factors. If there are variables are strings, we could convert them to factors using as.factors(variable.name)\n\nShow the codemode(mpg$trans)\n#> [1] \"character\"\n\n\n\nShow the codestr(mpg$trans)\n#>  chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" \"auto(l5)\" ...\n\n\n\nConverting Characters to Factors\n\nSometimes, it’s necessary to treat text data as categorical by converting them into factors. as.numeric() converts other types of variables to numbers. For a factor variable, we usually we want to access the categories (or levels) it has. We can use a build-in function to explore: levels(variable.name)\n\nShow the code# no levels for character\nlevels(mpg$trans)\n#> NULL\n\n\n\nShow the code## Ex check how many different trans the dataset has\nmpg$trans <- as.factor(mpg$trans)\nlevels(mpg$trans)\n#>  [1] \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"   \"auto(l6)\"  \n#>  [6] \"auto(s4)\"   \"auto(s5)\"   \"auto(s6)\"   \"manual(m5)\" \"manual(m6)\"\n\n\nThe levels usually will be ordered alphabetically. The first level is called “baseline”. However, the users may/may not want to keep this baseline and want to relevel/change the reference group. We can do it using the relevel function:\nrelevel(variable.name, ref=)\n\nShow the codempg$trans <- relevel(mpg$trans, ref = \"auto(s6)\")\nlevels(mpg$trans)\n#>  [1] \"auto(s6)\"   \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"  \n#>  [6] \"auto(l6)\"   \"auto(s4)\"   \"auto(s5)\"   \"manual(m5)\" \"manual(m6)\"\nnlevels(mpg$trans)\n#> [1] 10\n\n\nfactor function can be also used to combine factors. If the user want to combine multiple factors to one factors\n\nShow the code## EX re-group trans to \"auto\" and \"manual\"\nlevels(mpg$trans) <- list(auto = c(\"auto(av)\", \"auto(l3)\", \"auto(l4)\", \"auto(l5)\", \"auto(l6)\", \n                                   \"auto(s4)\", \"auto(s5)\", \"auto(s6)\"), \n                          manual = c(\"manual(m5)\", \"manual(m6)\"))\nlevels(mpg$trans)\n#> [1] \"auto\"   \"manual\"\n\n\nYou can also change the order of all factors using the following code: factor(variable.name, levels = c(“new order”))\n\nShow the code## EX. Change the order of trans to manual\nmpg$trans <- factor(mpg$trans, levels = c(\"manual\", \"auto\"))\nlevels(mpg$trans)\n#> [1] \"manual\" \"auto\"\n\n\n\n\nIn R, the use of factors with multiple levels is primarily a memory optimization strategy. While users may not directly see this, R assigns internal numerical identifiers to each level, which is a more memory-efficient way of handling such data. Unlike some other software packages that generate multiple dummy variables to represent a single variable, R’s approach is generally more resource-efficient.\nConvert continuous variables to categorical variables\n\n\n\n\n\n\nTip\n\n\n\nifelse, cut, recode all are helpful functions to convert numerical variables to categorical variables.\n\n\nLet’s see the summary of the cty variable first.\n\nShow the codesummary(mpg$cty)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    9.00   14.00   17.00   16.86   19.00   35.00\n\n\nsay, we may want to change continuous ‘cty’ into groups 0-14, 15-18, and 18-40. Below is an example with the cut function.\n\nShow the code## EX. change the cty into two categories (0,14], (14,18] and (18,40]\nmpg$cty.num <- cut(mpg$cty, c(0, 14, 18, 40), right = TRUE)\ntable(mpg$cty.num)\n#> \n#>  (0,14] (14,18] (18,40] \n#>      73      85      76\n\n\n\nShow the code## Try this: do you see a difference?: [0,14), [14,18) and [18,40)\nmpg$cty.num2 <- cut(mpg$cty, c(0, 14, 18, 40), right = FALSE)\ntable(mpg$cty.num2)\n#> \n#>  [0,14) [14,18) [18,40) \n#>      54      78     102\n\n\n\n\n] stands for closed interval, i.e., right = TRUE. On the other hand, ) means open interval. Hence, there will be a huge difference when setting right = TRUE vs. right = FALSE\nMissing value\n\n\n\n\n\n\nTip\n\n\n\nIncomplete datasets can distort analysis. Identifying and managing these missing values is thus crucial.\n\n\nWe can check how many missing values we have by: table(is.na(variable.name))\nLet’s us check whether the cty variable contains any missing values:\n\nShow the codetable(is.na(mpg$cty))\n#> \n#> FALSE \n#>   234\n\n\nIf you want to return all non-missing values, i.e., complete case values: na.omit(variable.name). For more extensive methods on handling missing values, see subsequent tutorials."
  },
  {
    "objectID": "wrangling4.html",
    "href": "wrangling4.html",
    "title": "Import external data",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(dplyr)\nrequire(Hmisc)\n\n\nWhen dealing with data analysis in R, it’s common to need to import external data. This tutorial will walk you through importing data in different formats.\nCSV format data\nCSV stands for “Comma-Separated Values” and it’s a widely used format for data. We’ll be looking at the “Employee Salaries - 2017” dataset, which contains salary information for permanent employees of Montgomery County in 2017.\n\n\nEmployee Salaries - 2017 data\n\n\n\n\n\n\nTip\n\n\n\nWe’ll be loading the Employee_Salaries_-_2017.csv dataset into R from its saved location at Data/wrangling/. Do note, the directory path might vary for you based on where you’ve stored the downloaded data.\n\n\n\nShow the codedata.download <- read.csv(\"Data/wrangling/Employee_Salaries_-_2017.csv\")\n\n\nHere, the read.csv function reads the data from the CSV file and stores it in a variable called data.download.\nTo understand the structure of our dataset, We can see the number of rows and columns and the names of the columns/variables as follows:\n\nShow the codedim(data.download) # check dimension / row / column numbers\n#> [1] 9398   12\nnrow(data.download) # check row numbers\n#> [1] 9398\nnames(data.download) # check column names\n#>  [1] \"Full.Name\"                \"Gender\"                  \n#>  [3] \"Current.Annual.Salary\"    \"X2017.Gross.Pay.Received\"\n#>  [5] \"X2017.Overtime.Pay\"       \"Department\"              \n#>  [7] \"Department.Name\"          \"Division\"                \n#>  [9] \"Assignment.Category\"      \"Employee.Position.Title\" \n#> [11] \"Position.Under.Filled\"    \"Date.First.Hired\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nhead shows the first 6 elements of an object, giving you a sneak peek into the data you’re dealing with, while tail shows the last 6 elements.\n\n\nWe can see the first see six rows of the dataset as follows:\n\nShow the codehead(data.download)\n\n\n\n  \n\n\n\nNext, for learning purposes, let’s artificially assign all male genders in our dataset as missing:\n\nShow the code# Assigning male gender as missing\ndata.download$Gender[data.download$Gender == \"M\"] <- NA\nhead(data.download)\n\n\n\n  \n\n\n\nThis chunk sets the Gender column’s value to NA (missing) wherever the gender is “M”. This is a form of data manipulation, sometimes used to handle missing or incorrect data. If you want to work with datasets that exclude any missing values:\n\n\n\n\n\n\nTip\n\n\n\nna.omit and complete.cases are useful functions to to create datasets with non-NA values\n\n\n\nShow the code# deleting/dropping missing components\ndata.download2 <- na.omit(data.download)\nhead(data.download2)\n\n\n\n  \n\n\nShow the codedim(data.download2)\n#> [1] 3806   12\n\n\nHere, na.omit is used to remove rows with any missing values. This can be essential when preparing data for certain analyses.\nAlternatively, we could have selected only females to drop all males:\n\nShow the codedata.download3 <- filter(data.download, Gender != \"M\")\nhead(data.download3)\n\n\n\n  \n\n\n\nAnd to check the size of this new dataset:\n\nShow the code# new dimension / row / column numbers\ndim(data.download3)\n#> [1] 3806   12\n\n\nSAS format data\n\n\n\n\n\n\nTip\n\n\n\nSAS is another data format, commonly used in professional statistics and analytics.\n\n\nLet’s explore importing a SAS dataset. We download a SAS formatted dataset from the CDC website.\n\nShow the codeNHANES1516data <- sasxport.get(\"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\")\n#> Processing SAS dataset DEMO_I     ..\ndim(NHANES1516data) # check dimension / row / column numbers\n#> [1] 9971   47\nnrow(NHANES1516data) # check row numbers\n#> [1] 9971\nnames(NHANES1516data)[1:10] # check first 10 names\n#>  [1] \"seqn\"     \"sddsrvyr\" \"ridstatr\" \"riagendr\" \"ridageyr\" \"ridagemn\"\n#>  [7] \"ridreth1\" \"ridreth3\" \"ridexmon\" \"ridexagm\"\n\n\nThe sasxport.get function retrieves the SAS dataset. The following lines, just like before, help understand its structure.\nTo analyze some of the data:\n\nShow the codetable(NHANES1516data$riagendr) # tabulating gender variable\n#> \n#>    1    2 \n#> 4892 5079\n\n\n\n\nVerify these numbers from CDC website\nThis code creates a frequency table of the riagendr variable, which represents gender.\nSaving working dataset\n\n\n\n\n\n\nTip\n\n\n\nOnce you’ve made modifications or conducted some preliminary analysis, it’s important to save your dataset. We can save the dataset in a different format, e.g., CSV, txt, or even R, SAS or other formats.\n\n\nWe can save our working dataset in different formats. Say, we want to save our NHANES1516data dataset in csv format. We can use the write.csv() command:\n\nShow the codewrite.csv(NHANES1516data, \"Data/wrangling/NHANES1516.csv\", row.names = FALSE)\n\n\nWe can also save the dataset in R format:\n\nShow the codesave(NHANES1516data, file = \"Data/wrangling/NHANES1516.RData\")"
  },
  {
    "objectID": "wrangling5.html",
    "href": "wrangling5.html",
    "title": "Summary tables",
    "section": "",
    "text": "Medical research and epidemiology often involve large, complex datasets. Data summarization is a vital step that transforms these vast datasets into concise, understandable insights. In medical contexts, these summaries can highlight patterns, indicate data inconsistencies, and guide further research. This tutorial will teach you how to use R to efficiently summarize medical data.\nIn epidemiology and medical research, “Table 1” typically refers to the first table in a research paper or report that provides descriptive statistics of the study population. It offers a snapshot of the baseline characteristics of the study groups, whether in a cohort study, clinical trial, or any other study design.\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n## Ex create a summary table between manufacturer and drv\ntable(mpg$drv, mpg$manufacturer)\n#>    \n#>     audi chevrolet dodge ford honda hyundai jeep land rover lincoln mercury\n#>   4   11         4    26   13     0       0    8          4       0       4\n#>   f    7         5    11    0     9      14    0          0       0       0\n#>   r    0        10     0   12     0       0    0          0       3       0\n#>    \n#>     nissan pontiac subaru toyota volkswagen\n#>   4      4       0     14     15          0\n#>   f      9       5      0     19         27\n#>   r      0       0      0      0          0\n\n\nThe first line reads a CSV file. It uses the table() function to generate a contingency table (cross-tabulation) between two categorical variables: drv (drive) and manufacturer. It essentially counts how many times each combination of drv and manufacturer appears in the dataset.\n\nShow the code## Get the percentage summary using prop.table\nprop.table(table(mpg$drv, mpg$manufacturer), margin = 2)\n#>    \n#>          audi chevrolet     dodge      ford     honda   hyundai      jeep\n#>   4 0.6111111 0.2105263 0.7027027 0.5200000 0.0000000 0.0000000 1.0000000\n#>   f 0.3888889 0.2631579 0.2972973 0.0000000 1.0000000 1.0000000 0.0000000\n#>   r 0.0000000 0.5263158 0.0000000 0.4800000 0.0000000 0.0000000 0.0000000\n#>    \n#>     land rover   lincoln   mercury    nissan   pontiac    subaru    toyota\n#>   4  1.0000000 0.0000000 1.0000000 0.3076923 0.0000000 1.0000000 0.4411765\n#>   f  0.0000000 0.0000000 0.0000000 0.6923077 1.0000000 0.0000000 0.5588235\n#>   r  0.0000000 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>    \n#>     volkswagen\n#>   4  0.0000000\n#>   f  1.0000000\n#>   r  0.0000000\n## margin = 1 sum across row, 2 across col\n\n\nThis code calculates the column-wise proportion (as percentages) for each combination of drv and manufacturer. The prop.table() function is used to compute the proportions. The margin = 2 argument indicates that the proportions are to be computed across columns (margin = 1 would compute them across rows).\ntableone package\n\n\n\n\n\n\nTip\n\n\n\nCreateTableOne function from tableone package could be a very useful function to see the summary table. Type ?tableone::CreateTableOne to see for more details.\n\n\nThis section introduces the tableone package, which offers the CreateTableOne function. This function helps in creating “Table 1” type summary tables, commonly used in epidemiological studies.\n\nShow the coderequire(tableone)\n#> Loading required package: tableone\nCreateTableOne(vars = c(\"cyl\", \"drv\", \"hwy\", \"cty\"), data = mpg, \n               strata = \"trans\", includeNA = TRUE, test = FALSE)\n#>                  Stratified by trans\n#>                   auto(av)       auto(l3)       auto(l4)      auto(l5)     \n#>   n                   5              2             83            39        \n#>   cyl (mean (SD))  5.20 (1.10)    4.00 (0.00)    6.14 (1.62)   6.56 (1.45) \n#>   drv (%)                                                                  \n#>      4                0 (  0.0)      0 (  0.0)     34 (41.0)     29 (74.4) \n#>      f                5 (100.0)      2 (100.0)     37 (44.6)      8 (20.5) \n#>      r                0 (  0.0)      0 (  0.0)     12 (14.5)      2 ( 5.1) \n#>   hwy (mean (SD)) 27.80 (2.59)   27.00 (4.24)   21.96 (5.64)  20.72 (6.04) \n#>   cty (mean (SD)) 20.00 (2.00)   21.00 (4.24)   15.94 (3.98)  14.72 (3.49) \n#>                  Stratified by trans\n#>                   auto(l6)      auto(s4)      auto(s5)      auto(s6)     \n#>   n                   6             3             3            16        \n#>   cyl (mean (SD))  7.33 (1.03)   5.33 (2.31)   6.00 (2.00)   6.00 (1.59) \n#>   drv (%)                                                                \n#>      4                2 (33.3)      2 (66.7)      1 (33.3)      7 (43.8) \n#>      f                2 (33.3)      1 (33.3)      2 (66.7)      8 (50.0) \n#>      r                2 (33.3)      0 ( 0.0)      0 ( 0.0)      1 ( 6.2) \n#>   hwy (mean (SD)) 20.00 (2.37)  25.67 (1.15)  25.33 (6.66)  25.19 (3.99) \n#>   cty (mean (SD)) 13.67 (1.86)  18.67 (2.31)  17.33 (5.03)  17.38 (3.22) \n#>                  Stratified by trans\n#>                   manual(m5)    manual(m6)   \n#>   n                  58            19        \n#>   cyl (mean (SD))  5.00 (1.30)   6.00 (1.76) \n#>   drv (%)                                    \n#>      4               21 (36.2)      7 (36.8) \n#>      f               33 (56.9)      8 (42.1) \n#>      r                4 ( 6.9)      4 (21.1) \n#>   hwy (mean (SD)) 26.29 (5.99)  24.21 (5.75) \n#>   cty (mean (SD)) 19.26 (4.56)  16.89 (3.83)\n\n\nThe CreateTableOne function is used to create a summary table for the variables cyl, drv, hwy, and cty from the mpg dataset. The strata = trans argument means that the summary is stratified by the trans variable. The includeNA = TRUE argument means that missing values (NAs) are included in the summary. The test = FALSE argument indicates that no statistical tests should be applied to the data (often tests are used to compare groups in the table).\ntable1 package\nThis section introduces another package, table1, which can also be used to create “Table 1” type summary tables.\n\nShow the coderequire(table1)\n#> Loading required package: table1\n#> \n#> Attaching package: 'table1'\n#> The following objects are masked from 'package:base':\n#> \n#>     units, units<-\ntable1(~ cyl + drv + hwy + cty | trans, data=mpg)\n\n\n\n\n\nauto(av)(N=5)\nauto(l3)(N=2)\nauto(l4)(N=83)\nauto(l5)(N=39)\nauto(l6)(N=6)\nauto(s4)(N=3)\nauto(s5)(N=3)\nauto(s6)(N=16)\nmanual(m5)(N=58)\nmanual(m6)(N=19)\nOverall(N=234)\n\n\n\ncyl\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n5.20 (1.10)\n4.00 (0)\n6.14 (1.62)\n6.56 (1.45)\n7.33 (1.03)\n5.33 (2.31)\n6.00 (2.00)\n6.00 (1.59)\n5.00 (1.30)\n6.00 (1.76)\n5.89 (1.61)\n\n\nMedian [Min, Max]\n6.00 [4.00, 6.00]\n4.00 [4.00, 4.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n8.00 [6.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n\n\ndrv\n\n\n\n\n\n\n\n\n\n\n\n\n\nf\n5 (100%)\n2 (100%)\n37 (44.6%)\n8 (20.5%)\n2 (33.3%)\n1 (33.3%)\n2 (66.7%)\n8 (50.0%)\n33 (56.9%)\n8 (42.1%)\n106 (45.3%)\n\n\n4\n0 (0%)\n0 (0%)\n34 (41.0%)\n29 (74.4%)\n2 (33.3%)\n2 (66.7%)\n1 (33.3%)\n7 (43.8%)\n21 (36.2%)\n7 (36.8%)\n103 (44.0%)\n\n\nr\n0 (0%)\n0 (0%)\n12 (14.5%)\n2 (5.1%)\n2 (33.3%)\n0 (0%)\n0 (0%)\n1 (6.3%)\n4 (6.9%)\n4 (21.1%)\n25 (10.7%)\n\n\nhwy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n27.8 (2.59)\n27.0 (4.24)\n22.0 (5.64)\n20.7 (6.04)\n20.0 (2.37)\n25.7 (1.15)\n25.3 (6.66)\n25.2 (3.99)\n26.3 (5.99)\n24.2 (5.75)\n23.4 (5.95)\n\n\nMedian [Min, Max]\n27.0 [25.0, 31.0]\n27.0 [24.0, 30.0]\n22.0 [14.0, 41.0]\n19.0 [12.0, 36.0]\n19.0 [18.0, 23.0]\n25.0 [25.0, 27.0]\n27.0 [18.0, 31.0]\n26.0 [18.0, 29.0]\n26.0 [16.0, 44.0]\n26.0 [12.0, 32.0]\n24.0 [12.0, 44.0]\n\n\ncty\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n20.0 (2.00)\n21.0 (4.24)\n15.9 (3.98)\n14.7 (3.49)\n13.7 (1.86)\n18.7 (2.31)\n17.3 (5.03)\n17.4 (3.22)\n19.3 (4.56)\n16.9 (3.83)\n16.9 (4.26)\n\n\nMedian [Min, Max]\n19.0 [18.0, 23.0]\n21.0 [18.0, 24.0]\n16.0 [11.0, 29.0]\n14.0 [9.00, 25.0]\n13.0 [12.0, 16.0]\n20.0 [16.0, 20.0]\n18.0 [12.0, 22.0]\n17.0 [12.0, 22.0]\n19.0 [11.0, 35.0]\n16.0 [9.00, 23.0]\n17.0 [9.00, 35.0]\n\n\n\n\n\nThe table1() function is used to generate a summary table for the specified variables. The formula-like syntax (~ cyl + drv + hwy + cty | trans) indicates that the summary should be stratified by the trans variable."
  },
  {
    "objectID": "wrangling6.html#introduction",
    "href": "wrangling6.html#introduction",
    "title": "R Markdown",
    "section": "Introduction",
    "text": "Introduction\nWelcome to this tutorial on working with RMD files in RStudio! RStudio is an IDE that makes R programming easier and more efficient, while R Markdown (RMD) is a file format that enables you to create dynamic reports with R code and narrative text. Using R Markdown within RStudio allows you to compile your analyses and reports into a single, easily shareable document in multiple formats like HTML, PDF, or Word.\n\n\nRStudio is an Integrated Development Environment (IDE), which is a software application that provides comprehensive facilities for software development. An IDE typically includes a text editor, tools for building and running code, and debugging utilities."
  },
  {
    "objectID": "wrangling6.html#prerequisites",
    "href": "wrangling6.html#prerequisites",
    "title": "R Markdown",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this tutorial, make sure you have both R and RStudio installed on your computer, as explaied in an earlier tutorial."
  },
  {
    "objectID": "wrangling6.html#knitting-rmd",
    "href": "wrangling6.html#knitting-rmd",
    "title": "R Markdown",
    "section": "Knitting RMD",
    "text": "Knitting RMD\nThis document shows how to work with an RMD file. We can create dynamic documents, e.g., a document with simple plain text combined with R code and its outputs. Note that RMD files are designed to be used with the R package rmarkdown. In RStudio IDE, the rmarkdown package could be already installed.\n\nShow the code# install.packages(\"rmarkdown\")\n\n\nOpen on RStudio\nLet us open an RMD file in RStudio. From the file menu (on the side), we can create a new RMD document. First, we need to click on the + symbol to create a new file, as follows:\n\n\n\n\n\n\n\n\nSecond, we need to click R markdown... to create a new RMD document as follows:\n\n\n\n\n\n\n\n\nWe will see a pop-up window as follows:\n\n\n\n\n\n\n\n\nWe can select whether we want to convert our RMD file to an HTML, PDF, or a Word document. These options can also be selected later. Let us select the default option (HTML) and press OK. We will see the markdown file as shown in the picture below:\n\n\n\n\n\n\n\n\nKnit\nWe use the knit option to create a document (e.g., making a PDF, HTML, or Word document) from the RMD file. Before knitting, we need to save the file. Let us save the file as working.RMD. After saving the file, we can knit it by clicking on the knit option, as shown below:\n\n\nThe term “knit” may sound a bit strange in the context of programming. However, it aptly describes the process of combining your R code and narrative text to produce a cohesive, final document. Think of it as “weaving” your code and text together into various output formats like HTML, PDF, or Word.\n\n\n\n\n\n\n\n\nOnce we knit the file, it will produce an HTML output, since our default option was HTML.\n\n\n\n\n\n\n\n\nFor formats other than HTML (e.g., PDF or Word), we can click on the dropdown menu:\n\n\n\n\n\n\n\n\nLet us select Knit to Word and knit it. Once the file is rendered, RStudio will show us a preview of the output in a word file and save the file in our working directory. We can also see that Word is added as another output:\n\n\nWhen you’re working in RStudio, all your files and outputs will be saved in a ‘working directory.’ This is simply the folder on your computer where RStudio will look for files and save outputs. To find out what your current working directory is, you can run the command getwd() in the R console.\n\n\n\n\n\n\n\n\nIn the R terminal, we can see that a Word document is created, which is stored in our working directory:\n\n\n\n\n\n\n\n\nSimilarly, we can create a pdf by clicking Knit to PDF option from the Knit menu. However, we could see an error message as follows:\n\n\n\n\n\n\n\n\nIt is important to note that RStudio does not build PDF documents from scratch. If we want to create PDF documents using RMD, we must have a LaTeX distribution installed on our computer. There are several options for LaTeX distributions, including MiKTeX, MacTeX, TeX Live, and so on. However, the recommended option for R Markdown users is TinyTeX. We can install TinyTeX using the R package tinytex. To install the package, run the following command: install.packages(\"tinytex\").\n\n\nLaTeX is a typesetting system commonly used for technical and scientific documentation. It is required for converting R Markdown documents to PDF format because it provides the text formatting commands that the rmarkdown package uses behind the scenes.\n\n\n\n\n\n\n\n\nOnce the tinytex package installation is complete, we can type tinytex::install_tinytex() to install the LaTeX distribution on our computer.\n\n\n\n\n\n\n\n\nTinyTeX is a large package (~123 MB). The installation time will vary depending on your machine. Once the installation is complete, we can click Knit to PDF. Similar to the Word file, RStudio will display a preview of the PDF output and save the PDF in our working directory. We will also see that a PDF file has been created:\n\n\n\n\n\n\n\n\nWorking with RMD\nNow we are ready to start writing plain text intermixed with embedded R code. For plain text, we can use the whitespace:\n\n\n\n\n\n\n\n\nOn the other hand, to embed a chunk of R code into our report, we use R code chunks. An R chunk surrounds the code with two lines that each contain three backticks. After the first set of backticks, we include {r}, which alerts knitr that we are going to include a chunk of R code:\n\n\nCode chunks are segments of code that are contained within an R Markdown document. They allow you to run R code within the document itself, making your report dynamic and reproducible.\n\n\n\n\n\n\n\n\nBelow are some codes:\n\n\n\n\n\n\n\n\nWe can knit the file to see the document, which will include plain text, R code, and outputs from the R code. We can also see the output from a code chunk without knitting the entire file. For example, we can click the arrow on the right-hand side to execute the current code chunk:\n\n\n\n\n\n\n\n\nNow we can see the following outputs:\n\n\n\n\n\n\n\n\nPlease also explore the drop down menu under Run to see the further options, including run the current code chunk, run all code chunk above, etc.\n\n\n\n\n\n\n\n\nTo omit the code from the final report while still including the results, add the argument echo = FALSE. This will place a copy of the results into your report.\n\n\nBesides echo = FALSE, there are several other options you can include in your code chunks to control their behavior, like eval = FALSE if you don’t want to evaluate the code, or message = FALSE to hide messages. Take a look at the author’s page of comprehensive list of chunk options.\nIn the final report (e.g., Word or PDF), we often want to omit the code and only show the outputs. To do this, we can add the argument echo = FALSE in the R code chunk:\n\n\n\n\n\n\n\n\nThe resulting output will look as follows:"
  },
  {
    "objectID": "wrangling6.html#tips-and-troubleshooting",
    "href": "wrangling6.html#tips-and-troubleshooting",
    "title": "R Markdown",
    "section": "Tips and Troubleshooting",
    "text": "Tips and Troubleshooting\n\nIf the knit button is grayed out, make sure you have saved your RMD file first.\nEncountering LaTeX errors? Make sure you’ve installed a LaTeX distribution like TinyTeX.\n\n\n\n\n\n\n\nTip\n\n\n\nThe following links could also be useful if you want to learn more:\n\nR Markdown Cheat Sheet\nIntroduction to R Markdown\nR Markdown: The Definitive Guide\nReports with R Markdown\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wranglingF.html",
    "href": "wranglingF.html",
    "title": "R Functions (W)",
    "section": "",
    "text": "This review page provides an extensive list of R functions tailored for data wrangling tasks that we have used in this chapter. Each function is systematically described, highlighting its primary package source and its specific utility.\nTo learn more about these functions, readers can:\n\nUse R’s Built-in Help System: For each function, access its documentation by prefixing the function name with a question mark in the R console, e.g., ?as.factor. This displays the function’s manual page with descriptions, usage, and examples.\nSearch Websites: Simply Google, or visit the CRAN website to search for specific function documentation. Websites like Stack Overflow and RStudio Community often have discussions related to R functions.\nTutorials and Online Courses: Platforms like DataCamp, Coursera, and edX offer R courses that cover many functions in depth. Also there are examples of dedicated R tutorial websites that you might find useful. One example is “Introduction to R for health data analysis” by Ehsan Karim, An Hoang and Qu.\nBooks: There are numerous R programming books, such as “R for Data Science” by Hadley Wickham and “The Art of R Programming” by Norman Matloff.\nWorkshops and Webinars: Institutions and organizations occasionally offer R programming workshops or webinars.\n\nWhenever in doubt, exploring existing resources can be highly beneficial.\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.factor \n    base \n    Converts a variable to factors. `as.factor` is a wrapper for the `factor` function. \n  \n\n cbind \n    base \n    Merges matrices. \n  \n\n CreateTableOne \n    tableone \n    Creates a frequency table. \n  \n\n data.frame \n    base \n    Creates a dataset with both numeric and character variables. Requires unique column names and equal length for all variables. \n  \n\n dim \n    base \n    Returns the dimensions of a data frame (rows x columns). \n  \n\n filter \n    dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n function \n    base \n    Used to define custom functions, e.g., for calculating standard deviation. \n  \n\n head \n    base \n    Displays the first six elements of an object (e.g., a dataset). `tail` displays the last six. \n  \n\n is.na \n    base \n    Checks for missing values in a variable. \n  \n\n levels \n    base \n    Displays the levels of a factor variable. \n  \n\n list \n    base \n    Stores vectors, matrices, or lists of differing types. \n  \n\n mode \n    base \n    Determines the type of a variable. \n  \n\n na.omit \n    base/stats \n    Removes all rows with missing values from a dataset. \n  \n\n names \n    base \n    Displays names of objects, e.g., variable names of a data frame. \n  \n\n nlevels \n    base \n    Shows the number of levels in a factor variable. \n  \n\n nrow \n    base \n    Returns the dimensions of a data frame. `nrow` gives row count and `ncol` gives column count. \n  \n\n plot \n    base/graphics \n    Draws scatter plots or line graphs. \n  \n\n print \n    base \n    Prints the output to console. \n  \n\n prop.table \n    base \n    Displays percentage summary for a table. \n  \n\n rbind \n    base \n    Appends matrices row-wise. \n  \n\n read.csv \n    base/utils \n    Reads data from a CSV file. \n  \n\n relevel \n    base/stats \n    Changes the reference group of a factor variable. \n  \n\n sasxport.get \n    Hmisc \n    Loads data in the SAS format. \n  \n\n save \n    base \n    Saves R objects, such as datasets. \n  \n\n select \n    dplyr \n    Selects specified variables from a dataset. \n  \n\n set.seed \n    base \n    Sets a seed for random number generation ensuring reproducibility. \n  \n\n str \n    base/utils \n    Displays the structure of a dataset, including data type of variables. \n  \n\n subset \n    base, dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n summary \n    base \n    Provides a summary of an object, like variable statistics. \n  \n\n table \n    base \n    Displays frequency counts for a variable. \n  \n\n write.csv \n    base/utils \n    Saves a data frame to a CSV file in a specified directory."
  },
  {
    "objectID": "wranglingQ.html",
    "href": "wranglingQ.html",
    "title": "Quiz (W)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "wranglingE.html#problem-statement",
    "href": "wranglingE.html#problem-statement",
    "title": "Exercise (W)",
    "section": "Problem Statement",
    "text": "Problem Statement\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nA paper you can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\n\nShow the code# Load required packages\nlibrary(dplyr)\nlibrary(tableone)\n\n\n\nShow the code# Data import: name it rhc\n#rhc <- ...(\"Data/wrangling/rhc.csv\", ...)\n\n\nPart (a) Basic Manipulation [60%]\n\nContinuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\n\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\n\n\n\nSet reference: Change the reference category for gender to Male\n\n\n\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\n\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\n\n\n\ncomorbidities:\n\n\ncreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nreport maximim and minimum values of numcom:\n\n\n\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\n“age”, “sex”, “race”,“cat1”, “ca”, “dnr1”, “aps1”, “surv2md1”, “numcom”, “adld3p”, “das2d3pc”, “temp1”, “hrt1”, “meanbp1”, “resp1”, “wblc1”, “pafi1”, “paco21”, “ph1”, “crea1”, “alb1”, “scoma1”, “swang1”, and\nname it rhc2.\n\n\n\n\nPart (b) Table 1 [20%]\n\nRe-produce the sample table from the rhc2 data (see the Table that was provided with this assignment). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\n\n\nTable 1 for subset\n\nProduce a similar table as part (b) but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]\n\n\n\nPart (c) Considering eligibility criteria [20%]\nProduce a similar table as part (b.i) but only for the subjects who meet all of the following eligibility criteria: (i) age is equal to or above 50, (ii) age is below 80 (iii) Glasgow Coma Score is below 61 and (iv) Primary disease categories are either ARF or MOSF. [Hint: droplevels.data.frame can be a useful function]\n\n\n\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\n\n\n\nCount how many NAs does that variable have?\n\n\n\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\n\n\n\n\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nShow the codeVector <- 1:100\n\n#variance.est <- function(?){?}\n\n#variance.est(Vector)\n\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nShow the codevar(Vector)\n#> [1] 841.6667"
  },
  {
    "objectID": "accessing.html#background",
    "href": "accessing.html#background",
    "title": "Data accessing",
    "section": "Background",
    "text": "Background\nSurveys serve as a pivotal tool for collecting and evaluating health-related information on a national scale. More often than not, it’s the governmental bodies that take the lead in gathering this data. Recognizing the value of this information, many governments not only compile and analyze these datasets but also ensure they are accessible to the public, especially for research purposes. In this guide, we will delve into an array of survey methodologies, provide illustrative examples, and guide you through the process of downloading pertinent data from both Canadian and American repositories. To make this more tangible, we will conclude with a hands-on example, showcasing how to replicate findings from an academic paper that leveraged one of these publicly available datasets.\n\n\nDedicating a chapter to accessing and downloading nationally representative survey datasets is pivotal for a hands-on epidemiological tutorial. These datasets, often rich in information and reflective of diverse populations, serve as the backbone for real-world analysis. By guiding learners on how to obtain these datasets, the book ensures that they not only grasp theoretical concepts but also gain practical experience working with authentic, large-scale data. This approach bridges the gap between theory and practice, allowing readers to apply learned techniques on datasets that mirror real-world complexities, thereby enhancing the relevance and applicability of their analytical skills.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "accessing.html#overview-of-tutorials",
    "href": "accessing.html#overview-of-tutorials",
    "title": "Data accessing",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nSurvey data sources\nThe tutorial lists primary complex survey data sources, including the Canadian Community Health Survey and National Health and Nutrition Examination Survey, with several offering dedicated R packages for data access.\n\n\nImporting CCHS to R\nThe section provides detailed steps for importing the Canadian Community Health Survey dataset from the UBC library into RStudio, with processing options using SAS, the free software PSPP, and directly in R.\n\n\nImporting NHANES to R\nThe tutorial guides users on how to access and import the NHANES dataset from the CDC website into RStudio, detailing the dataset’s structure and providing methods both manually and using an R package.\n\n\nReproducing results\nThe tutorial guides users through accessing, processing, and analyzing NHANES data to reproduce the results from a referenced article using R code.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThe subsequent chapter on Research Questions serves as a valuable guide for constructing an analytics-driven data set tailored to your specific research queries. It will cover crucial aspects such as the types of variables to collect and how to set eligibility criteria, followed by approaches to data analysis based on your research questions. It’s important to note that research questions can fall into two main categories: predictive or causal. For a deeper understanding of variable selection and analytical tools suited to these types of questions, the chapters on the Roles of Variables and Predictive Models offer insightful guidance.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "accessing1.html",
    "href": "accessing1.html",
    "title": "Survey data sources",
    "section": "",
    "text": "The tutorial discusses complex survey data and highlights potential data sources. Key datasets with survey features include the Canadian Community Health Survey (CCHS), the National Health and Nutrition Examination Survey (NHANES), and the European Social Survey (ESS), among others. Many of these sources, like NHANES and ESS, have specific R packages for data retrieval. In addition, there are other data sources such as the Vanderbilt Biostatistics Datasets and the World Bank Open Data, with the latter also offering dedicated R packages for data access.\n\nDataset with survey features\n\nCanadian Community Health Survey - Annual Component CCHS\n\nDownload link UBC library\n\nNational Health and Nutrition Examination Survey NHANES\n\nR packages to download data: nhanesA, RNHANES\n\nNational Longitudinal Study of Adolescent to Adult Health [Add Health], 1994-2008 ICPSR 21600\nEuropean Social Survey ESS\n\nR package to download data: essurvey\n\nBehavioral Risk Factor Surveillance System BRFSS\nBureau of Economic Analysis BEA\nUS National Vital Statistics System NVSS\nDemographic and Health Surveys DHS\n\n\n\nOthers\n\nVanderbilt Biostatistics Datasets link\nWorld Bank Open Data WBOD\n\nR packages to download data: wbstats, WDI"
  },
  {
    "objectID": "accessing2.html",
    "href": "accessing2.html",
    "title": "Importing CCHS to R",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(knitr)\n\n\nThis section provides comprehensive instructions on how to import the Canadian Community Health Survey (CCHS) dataset from the UBC library site to the RStudio environment. The process starts with downloading the CCHS data from the UBC library site and includes step-by-step visual guides for each stage. Three primary options are provided to process and format the data:\n\nUsing the commercial software SAS.\nUtilizing the free software PSPP, an alternative to SPSS.\nDirectly processing the data in R.\n\nFor each option, users are guided on how to download, install, access, read, save, and check the dataset. The objective is to help users acquire, visualize, and manipulate the CCHS dataset seamlessly using various software applications.\nDownloading CCHS data from UBC\n\n\nStep 1: Go to dvn.library.ubc.ca, and press ‘log-in’\n\n\n\n\n\n\n\n\nStep 2: Select ‘UBC’ from the dropdown menu\n\n\n\n\n\n\n\n\nStep 3: Enter your CWL or UBC library authentication information\n\n\n\n\n\n\n\n\nStep 4: Once you log-in, search the term ‘cchs’ in the search-box\n\n\n\n\n\n\n\n\nStep 5: For illustrative purposes, let us work with the Cycle 3.1 of the CCHS dataset from the list of results. In that case, type ‘cchs 3.1’\n\n\n\n\n\n\n\n\nStep 6: CCHS Cycle 3.1 information\n\n\n\n\n\n\n\n\nStep 7: Choose the ‘Data: CD’ from the menu\n\n\n\n\n\n\n\n\nStep 8: Download the entire data (about 159 MB) as a zip file\n\n\n\n\n\n\n\n\nStep 9: Accept the ‘terms of use’\n\n\n\n\n\n\n\n\nStep 10: Select a directory to download the zip file. The path of the download directory is important (we need to use this path exactly later). For example, below we are in \"C:\\CCHS\\\" folder, but we will create a “Data” folder there, so that the download path is \"C:\\CCHS\\Data\\\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 11: Extract the zip file\n\n\n\n\n\n\n\n\nStep 12: Be patient with the extraction\n\n\n\n\n\n\n\n\nStep 13: Once extraction is complete, take a look at the folders inside. You will see that there is a folder named ‘SAS_SPSS’\n\n\n\n\n\n\nReading and Formatting the data\nOption 1: Processing data using SAS\nSAS is a commercial software. You may be able to get access to educational version. In case you don’t have access to it, later we outline how to use free packages to read these datasets.\n\n\nStep 1: Inside that ‘SAS_SPSS’ folder, find the file hs_pfe.sas. It is a long file, but we are going to work on part of it. First thing we want to do it to change all the directory names to where you have unzipped the downloaded file (for example, here the zip file was extracted to C:/CCHS/Data/cchs_cycle3-1CD/). We only need the first part of the code (as shown below; only related to data ‘hs’). Delete the rest of the codes for now. The resulting code should like like this:\n\n\nShow the code%include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_pfe.sas\";\n\ndata hs;\n        %let datafid=\"C:\\CCHS\\Data\\cchs_cycle3-1CD\\Data\\hs.txt\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_fmt.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_lbe.sas\";\nrun;\n\n\nOnce the modifications are done, submit the codes in SAS. Note that, the name of the data is ‘hs’.\n\n\n\n\n\n\n\nStep 2: Once you submit the code, you can check the log window in SAS to see how the code submission went. It should tell you how many observations and variables were read.\n\n\n\n\n\n\n\n\nStep 3: If you one to view the dataset, you can go to ‘Explorer’ window within SAS.\n\n\n\n\n\n\n\n\nStep 4: Generally, if you haven’t specified where to load the files, SAS will by default save the data into a library called ‘Work’\n\n\n\n\n\n\n\n\nStep 5: Open that folder, and you will be able to find the dataset ‘Hs’.\n\n\n\n\n\n\n\n\nStep 6: Right click on the data, and click ‘open’ to view the datafile.\n\n\n\n\n\n\n\n\nStep 7: To export the data into a CSV format data (so that we can read this data into other software packages), ckick ‘Menu’.\n\n\n\n\n\n\n\n\nStep 8: then press ‘Export Data’.\n\n\n\n\n\n\n\n\nStep 9: choose the library and the data.\n\n\n\n\n\n\n\n\nStep 10: choose the format in which you may want to save the existing data.\n\n\n\n\n\n\n\n\nStep 11: also specify where you want to save the csv file and the name of that file (e.g., cchs3.csv).\n\n\n\n\n\n\n\n\nStep 12: go to that directory to see the file cchs3.csv\n\n\n\n\n\n\n\n\nStep 13: If you want to save the file in SAS format, you can do so by writing the following sas code into the ‘Editor’ window. Here we are saving the data Hs within the Work library in to a data called cchs3 within the SASLib library. Note that, the directory name has to be where you want to save the output file.\n\n\nShow the codeLIBNAME SASLib \"C:\\CCHS\\Data\";\nDATA SASLib.cchs3;\n    set Work.Hs;\nrun;\n\n\nSubmit these codes into SAS:\n\n\n\n\n\n\n\nStep 13: go to that directory to see the file cchs3.sas7dbat\n\n\n\n\n\n\nOption 2: Processing data using PSPP (Free)\nPSPP is a free package; alternative to commercial software SPSS. We can use the same SPSS codes to read the datafile into PSPP, and save.\n\n\nStep 1: Get the free PSPP software from the website: www.gnu.org/software/pspp/\n\n\nPSPP is available for GNU/Hurd, GNU/Linux, Darwin (Mac OS X), OpenBSD, NetBSD, FreeBSD, and Windows\n\n\n\n\n\nFor windows, download appropriate version.\n\n\n\n\n\nDownload the file\n\n\n\n\n\nInstall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the icon shorcut after installing\n\n\n\n\n\n\n\nStep 2: Open PSPP\n\n\n\n\n\n\n\n\nStep 3: Go to ‘file’ menu and click ‘open’\n\n\n\n\n\n\n\n\nStep 4: Specify the readfile.sps file from the ‘SAS_SPSS’ folder.\n\n\n\n\n\n\nYou will see the following file:\n\n\n\n\n\n\n\nStep 5: Similar to before, change the directories as appropriate. Get rid of the extra lines of codes. Resulting codes are as follows (you can copy and replace the code in the file with the following codes):\n\n\nShow the codefile handle infile/name = 'C:\\CCHS\\Data\\cchs_cycle3-1CD\\DATA\\hs.txt'.\ndata list file = infile notable/.\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvale.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvare.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsmiss.sps\".\nexecute.\n\n\n\n\n\n\n\nFor Mac users, it should be as follows (e.g., username should be your user name, if you are saving under the path \"/Users/username/CCHS/Data/\"):\n\nShow the codefile handle infile/name =\"/Users/username/CCHS/Data/cchs_cycle3-1CD/Data/hs.txt\".\ndata list file = infile notable/.\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hs_i.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvale.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvare.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsmiss.sps\".\n\nexecute.\n\n\n\n\nStep 6: Run the codes.\n\n\n\n\n\n\n\n\nStep 7: This is a large data, and will take some time to load the data into the PSPP data editor. Be patient.\n\n\n\n\n\n\nOnce loading is complete, it will show the ‘output’ and ‘data view’.\n\n\n\n\n\n\n\n\n\n\nNote that, you will get error message, if your files were not in the correct path. In our example, the path was \"C:\\CCHS\\Data\\\" for the zip file content (see the previous steps).\n\n\nStep 7: You can also check the ‘variable view’.\n\n\n\n\n\n\n\n\nStep 8: Save the data by clicking ‘File’ and then ‘save as …’\n\n\n\n\n\n\n\n\nStep 9: Specify the name of the datafile and the location / folder to save the data file.\n\n\n\n\n\n\n\n\nStep 10: See the SAV file saved in the directory.\n\n\n\n\n\n\n\n\nStep 11: To save CSV format data, use the following syntax.\n\n\nShow the codeSAVE TRANSLATE\n  /OUTFILE=\"C:/CCHS/Data/cchs3b.csv\"  \n  /TYPE=CSV\n  /FIELDNAMES      \n  /CELLS=VALUES.\n\n\nNote that, for categorical data, you can either save values or labels. For our purpose, we prefer values, and hence saved with values here.\n\n\n\n\n\n\n\nStep 12: See the CSV file saved in the directory extracted from PSPP.\n\n\n\n\n\n\nOption 3: Processing data using SPSS\nLog into ubc.onthehub.com to download SPSS. With your CWL account, UBC students should be able to download it. UBC IT website for SPSS says:\nThe SPSS software license with UBC specifies that SPSS must only be used by UBC Faculty, Students, and Research Staff and only for Teaching and non-commercial Research purposes related to UBC.\nBoth network (for UBC owened devices) or standalone / home versions (for non-UBC owened devices) should be available. Once downloaded, same process of importing CCHS data in PSPP can also be applied on SPSS (same syntax files should work). Let me know if that is not the case.\nProcessing data in R\nDownload software\n\n\nStep 1: Download either ‘R’ from CRAN www.r-project.org or ‘R open’ from Microsoft mran.microsoft.com/open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download RStudio from www.rstudio.com/\n\n\n\n\n\n\n\n\n\nStep 3: Open RStudio\n\n\n\n\n\n\nImport, export and load data into R\n\n\nStep 1: Set working directory\n\n\nShow the codesetwd(\"C:/CCHS/Data/\") # or something appropriate\n\n\n\n\nStep 2: Read the dataset created from PSPP with cell values. We can also do a small check to see if the cell values are visible. For example, we choose a variable ‘CCCE_05A’, and tabulate it.\n\n\nShow the codeHs <- read.csv(\"cchs3b.csv\", header = TRUE)\ntable(Hs$CCCE_05A)\n\n\n\n\n\n\n\n\n\nStep 3: Save the RData file from R into a folder SurveyData:\n\n\nShow the codesave(Hs, file = \"SurveyData/cchs3.RData\")\n\n\n\n\nStep 4: See the RData file saved in the directory extracted from R.\n\n\n\n\n\n\n\n\nStep 5: Close R / RStudio and restart it. Environment window within RStudio should be empty.\n\n\n\n\n\n\n\n\nStep 6: Load the saved RData into R. Environment window within RStudio should have ‘Hs’ dataset.\n\n\nShow the codeload(\"SurveyData/cchs3.RData\")"
  },
  {
    "objectID": "accessing3.html",
    "href": "accessing3.html",
    "title": "Importing NHANES to R",
    "section": "",
    "text": "This tutorial provides comprehensive instructions on accessing the National Health and Nutrition Examination Survey (NHANES) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment. It covers:\n\nIntroduction to the NHANES dataset, highlighting its significance in evaluating the health and nutritional status of U.S. adults and children.\nSampling Procedure details, explaining the multi-stage sampling strategy and emphasizing the importance of using survey features like weights, strata, and primary sampling units for population-level estimates.\nSurvey History with a visualization representing different NHANES survey cycles.\nNHANES Data Files and Documents:\n\n\nExplains the data’s file format, mostly in SAS transport file format (.xpt).\nBreaks down the NHANES components, which include demographics, dietary, examination, laboratory, and questionnaire data.\nProvides guidelines on combining data from different cycles and handling missing data or outliers.\n\n\nAccessing NHANES Data:\n\n\nDirectly from the CDC website: A step-by-step guide with accompanying images, illustrating how to navigate the CDC website, download the data, and interpret the accompanying codebook.\nUsing R packages, specifically the nhanesA package: A concise guide on how to download and get summaries of the NHANES data using this R package.\n\n\nShow the code# Load required packages\n#devtools::install_github(\"warnes/SASxport\")\nlibrary(SASxport)\nlibrary(foreign)\nlibrary(nhanesA)\nlibrary(knitr)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nuse.saved.chche <- TRUE\n\n\n\n\nBefore installing a package from GitHub, it’s better to check whether you installed the right version of Rtools\nOverview\nNational Center for Health Statistics (NCHS) conducts National Health and Nutrition Examination Survey (NHANES) (CDC,NCHS 2023). These surveys are designed to evaluate the health and nutritional status of U.S. adults and children. These surveys are being administered in two-year cycles or intervals starting from 1999-2000. Prior to 1999, a number of surveys were conducted (e.g., NHANES III), but in our discussion, we will mostly restrict our discussions to continuous NHANES (e.g., NHANES 1999-2000 to NHANES 2017-2018).\n\n\nCDC,NCHS (2023)\nSampling Procedure:\nIt is a probabilistic sample (we know probability of getting selected for all individuals). This sample is unlikely to be representative of the entire population, as some under/oversampling occurs (unlike SRS), and samples may be dependent (due to proximity of some samples). For example, household with the following characteristics may be oversampled in NHANES, e.g., African Americans, Mexican Americans, Low income White Americans, Persons age 60+ years.\n\n\nSampling Procedure:\n\nnot obtained via simple random sample\nmultistage sample designs\nA sample weight is assigned to each sample person where weight = the number of people in the target population represented by that sample person in NHANES\n\nNHANES used multistage sample designs:\n\nStage 1: PSU/clusters = geographically contiguous counties. 50 states - divided into ~3100 counties. Each PSU is assigned to a strata (e.g., urban/rural or PSU size etc.). The counties are randomly/PPS selected using a 2-per-stratum design. Complex sample variance estimation requires PSU + strata (masking involved).\nStage 2: each selected county is broken into segments (with at least ~50-100 housing units). Segments are randomly/PPS selected.\nStage 3: each selected segment is divided into households. Households are randomly selected.\nStage 4: Within each sampled household, an individual is randomly selected.\n\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nSurvey history\nOverall NHANES survey history\n\n\n\n\n\n\n\n\nNHANES datafile and documents\nFile format\nThe Continuous NHANES files are stored in the NHANES website as SAS transport file formats (.xpt). You can import this data in any statistical package that supports this file format.\nContinuous NHANES Components\nContinuous NHANES components separated to reduce the amount of time to download and documentation size:\n\n\nNHANES Tutorials\n\n\n\n\n\n\n\n\n\n\nBroadly, continuous NHANES data are available in 5 categories:\n\nDemographics\nDietary\nExamination\nLaboratory\nQuestionnaire\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHANES. However, NHANES is a cross-sectional data, and identification of the same person accross different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\n\n\nThe following data have not been released on the NHANES website as public release files due to confidentiality concerns:\n\nadolescent data on alcohol use\nsmoking\nsexual behavior\nreproductive health and drug use\n\nWithin the same cycle\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\nMissing data and outliers\nCDC (2023) recommends:\n\n\nCDC (2023)\n\n\n“As a general rule, if 10% or less of your data for a variable are missing from your analytic dataset, it is usually acceptable to continue your analysis without further evaluation or adjustment. However, if more than 10% of the data for a variable are missing, you may need to determine whether the missing values are distributed equally across socio-demographic characteristics, and decide whether further imputation of missing values or use of adjusted weights are necessary.”\n\n\n\n\n“If you fail to identify ‘refusal’ or ‘do not know’ as types of missing data, and treat the assigned values for ‘refused’ or ‘do not know’ as real values, you will get distorted results in your statistical analyses. Therefore, it is important to recode ‘refused’ or ‘don’t know’ responses as missing values (either as a period (.) for numeric variables or as a blank for character variables).”\n\n\n\n\n“Outliers with extremely large weights could have an influential impact on your estimates. You will have to decide whether to keep these influential outliers in your analysis or not. It is up to the analysts to make that decision.”\n\n\nNHANES documents\n\n\n\n\n\n\n\n\n\n\nThe following websites could be helpful: - For more information about NHANES design.\n\nVisit US CDC website and do a variable keyword search based on your research interest (e.g., arthritis).\n\nAccessing NHANES Data Directly from the CDC website\nIn the following example, we will see how to download ‘Demographics’ data, and check associated variable in that dataset.\n\n\n\n\n\n\n\nNHANES 1999-2000 and onward survey datasets are publicly available at wwwn.cdc.gov/nchs/nhanes/\n\n\nStep 1: Say, for example, we are interested about the NHANES 2015-2016 survey. Clicking the associated link in the above Figure gets us to the page for the corresponding cycle (see below).\n\n\n\n\n\n\n\n\nStep 2: There are various types of data available for this survey. Let’s explore the demographic information from this cycle. These data are mostly available in the form of SAS XPT format (see below).\n\n\n\n\n\n\n\n\nStep 3: We can download the XPT data in the local PC folder and read the data into R as as follows:\n\n\nShow the codeDEMO <- read.xport(\"Data/accessing/DEMO_I.XPT\")\n\n\n\n\n\n\n\nStep 4: Once data is imported in RStudio, we will see the DEMO object listed under data window (see below):\n\n\n\n\n\n\n\n\nStep 5: We can also check the variable names in this DEMO dataset as follows:\n\n\nShow the codenames(DEMO)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\n\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the DEMO data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\nStep 7: There is a column name associated with each column, e.g., DMDHSEDU in the first column in the above Figure. To understand what the column names mean in this Figure, we need to take a look at the codebook. To access codebook, click the 'DEMO|Doc' link (in step 2). This will show the data documentation and associated codebook (see the next Figure).\n\n\n\n\n\n\n\n\nStep 8: We can see a link for the column or variable DMDHSEDU in the table of content (in the above Figure). Clicking that link will provide us further information about what this variable means (see the next Figure).\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count and cumulative (from the above Figure) matches with what we get from the DEMO data we just imported (particularly, for the DMDHSEDU variable):\n\n\nShow the codetable(DEMO$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(DEMO$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(DEMO$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nAccessing NHANES Data Using R Packages\nnhanesA package\n\nShow the codelibrary(nhanesA)\n\n\n\n\n\n\n\n\nTip\n\n\n\nR package nhanesA provides a convenient way to download and analyze NHANES survey data.\n\n\n\n\nRNHANES (Susmann 2016) is another packages for downloading the NHANES data easily.\n\n\nStep 1: Witin the CDC website, NHANES data are available in 5 categories\n\nDemographics (DEMO)\nDietary (DIET)\nExamination (EXAM)\nLaboratory (LAB)\nQuestionnaire (Q)\n\n\n\nTo get a list of available variables within a data file, we run the following command (e.g., we check variable names within DEMO data):\n\nShow the codenhanesTables(data_group='DEMO', year=2015)\n\n\n\n  \n\n\n\n\n\nStep 2: We can obtain the summaries of the downloaded data as follows (see below):\n\n\nShow the codedemo <- nhanes('DEMO_I')\nnames(demo)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\ntable(demo$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(demo$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(demo$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nReferences\n\n\n\n\n\n\nCDC. 2023. “NHANES Web Tutorial Frequently Asked Questions (FAQs).” https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/faq.aspx.\n\n\nCDC,NCHS. 2023. “National Health and Nutrition Examination Survey Data.” https://wwwn.cdc.gov/nchs/nhanes/.\n\n\nSusmann, Herb. 2016. RNHANES: Facilitates Analysis of CDC NHANES Data. https://CRAN.R-project.org/package=RNHANES."
  },
  {
    "objectID": "accessing4.html#references",
    "href": "accessing4.html#references",
    "title": "Reproducing results",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDhana, A. 2023. “R & Python for Data Science.” https://datascienceplus.com/.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91."
  },
  {
    "objectID": "accessingF.html",
    "href": "accessingF.html",
    "title": "R Functions (A)",
    "section": "",
    "text": "The section introduces a set of R functions useful for accessing and processing complex survey data, providing their descriptions and the packages they belong to.\n\n\n\n\n\n Function_name \n    Package_name \n    Description \n  \n\n\n apply \n    base \n    Applies a function over an array or matrix. \n  \n\n cut \n    base \n    Converts a numeric variable to a factor variable. \n  \n\n merge \n    base/data.table \n    Merges multiple datasets. \n  \n\n names \n    base \n    Retrieves the names of an object. \n  \n\n nhanes \n    nhanesA \n    Downloads a NHANES datafile. \n  \n\n nhanesTables \n    nhanesA \n    Lists available variables within a datafile. \n  \n\n nhanesTranslate \n    nhanesA \n    Encodes categorical variables to match with certain standards, e.g., CDC website. \n  \n\n recode \n    car \n    Recodes a variable. \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "accessingQ.html",
    "href": "accessingQ.html",
    "title": "Quiz (A)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "accessingE.html#problem-statement",
    "href": "accessingE.html#problem-statement",
    "title": "Exercise (A)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Palis, Marchand, and Oviedo-Joekes (2020), DOI: 10.1080/09638237.2018.1437602.\n\nDownload the CCHS MH topical index\n\nDownload the CCHS MH Data Dictionary"
  },
  {
    "objectID": "accessingE.html#question-i-60-grade",
    "href": "accessingE.html#question-i-60-grade",
    "title": "Exercise (A)",
    "section": "Question I: [60% grade]",
    "text": "Question I: [60% grade]\n1(a) Importing dataset\n\nShow the code# Importing dataset\nload(\"Data/accessing/cchsMH.RData\") \n\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper\n\nIdentify the variable needed for eligibility criteria\nIdentify the outcome variable\nIdentify the explanatory variable\nIdentify the potential confounders\nIdentify the survey weight variable\n\nHint\n\nRead\n\n\nthe first paragraph of Analytic sample (page 2) for the eligibility criteria, and\nfirst and second paragraphs of Study variables for rest of the variables,\nthird paragraph of the Statistical analyses for the survey weights variable.\n\n\neligibility criteria was determined based on one variable. Only work with ‘YES’ category.\nOutcome variable has a category ‘NOT STATED’, but for our analysis, we will omit anyone associated with this category.\nFor explanatory variable, we have categories such as DON’T KNOW, REFUSAL and NOT STATED. We will not use these categories (omit anyone with these categories).\nThere were five potential confounders.\nPotentially useful functions:\n\n\n%in%\nlevels\nrecode\nsubset\nas.factor\nrelevel\n\nor dplyr ways:\n\nfilter\nselect\n\n\n\n\nShow the code# your code here\n\n\n1(c) Retaining necessary variables\nIn the dataset, retain only the variables associated with outcome measure, explanatory variable, potential confounders and survey weight\n\nShow the code# your code here\n\n\n1(d) Creating analytic dataset\n\nAssign missing values for categories such as DON’T KNOW, REFUSAL and NOT STATED.\n‘recode’ the variables as shown in Table 1 (choose any function of your choice). Here is an example (but feel free to use other functions. In R there are many other ways to do this same task):\n\n\nShow the code## your code here\n# levels(your.data.frame$your.age.variable) <- \n#   list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n#        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n#        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n#        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n#        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n#        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n#        \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n\n1(e) Number of columns and variable names\nreport the number of columns in your analytic dataset, and the variable names.\n\nShow the code# your code here"
  },
  {
    "objectID": "accessingE.html#question-ii-20-grade",
    "href": "accessingE.html#question-ii-20-grade",
    "title": "Exercise (A)",
    "section": "Question II: [20% grade]",
    "text": "Question II: [20% grade]\n2(a) Table 1\nReproduce Table 1 presented in the above paper (omit the ‘Main source of income’ variable). If necessary, drop other variables from the analytic dataset that are not presented in Table 1.\nThe table you produce should report numbers as follows:\n\n\n\n\n\n\n\n\n\nSelf-rated Mental Health Variable\nTotal n(%)\nPoor or Fair n(%)\nGood n(%)\nVery good or excellent n(%)\n\n\n\nStudy sample\n2628 (100)\n1002 (38.1)\n885 (33.7)\n741 (28.2)\n\n\nCommunity belonging\n\n\n\n\n\n\n- Very weak\n480 (18.3)\n282 (28.1)\n118 (13.3)a\n80 (10.8)a\n\n\n- Somewhat weak\n857 (32.6)\n358 (35.7)\n309 (34.9)\n190 (25.6)\n\n\n- Somewhat strong\n1005 (38.2)\n288 (28.7)\n362 (40.9)\n355 (47.9)\n\n\n- Very strong\n286 (10.9)\n74 (7.4)a\n96 (10.8)a\n116 (15.7)a\n\n\nSex\n\n\n\n\n\n\n- Females\n1407 (53.5)\n616 (61.5)\n487 (55.0)\n304 (41.0)\n\n\n- Males\n1221 (46.5)\n386 (38.5)\n398 (45.0)\n437 (59.0)\n\n\nAge group\n\n\n\n\n\n\n- 15 to 24 years\n740 (28.2)\n191 (19.1)\n264 (29.8)\n285 (38.5)\n\n\n- 25 to 34 years\n475 (18.1)\n141 (14.1)\n167 (18.9)\n167 (22.5)\n\n\n- 35 to 44 years\n393 (15.0)\n185 (18.5)\n119 (13.4)a\n89 (12.0)a\n\n\n- 45 to 54 years\n438 (16.6)\n220 (22.0)\n139 (15.7)\n79 (10.7)a\n\n\n- 55 to 64 years\n379 (14.4)\n198 (19.7)\n113 (12.8)a\n68 (9.2)a\n\n\n- 65 years or older\n203 (7.7)\n67 (6.6)a\n83 (8.4)a\n53 (7.1)b\n\n\nRace/Ethnicity\n\n\n\n\n\n\n- Non-white\n458 (17.4)\n184 (18.4)\n140 (15.8)\n134 (18.1)\n\n\n- White\n2170 (82.6)\n818 (81.6)\n745 (84.2)\n607 (81.9)\n\n\nMain source of income\n\n\n\n\n\n\n- Employment Income^d\n1054 (40.1)\n289 (28.8)\n386 (43.6)\n379 (51.1)\n\n\n- Worker’s Compensation^e\n160 (6.1)\n91 (9.1)a\n44 (5.0)b\n25 (3.4)c\n\n\n- Senior Benefits^f\n134 (5.1)\n57 (5.7)a\n42 (4.7)b\n35 (4.7)\n\n\n- Other^g\n184 (7.0)\n82 (8.2)a\n60 (6.8)a\n42 (5.7)b\n\n\n- Not applicable^h\n851 (32.4)\n402 (40.1)\n263 (29.7)\n186 (25.1)\n\n\n- Not Stated^i\n245 (9.3)\n81 (8.1)a\n90 (10.2)a\n74 (10.0)\n\n\n\n\\(^a\\) Coefficient of variation between 16.6 and 25.0%. \\(^b\\) Coefficient of variation between 25.1 and 33.3%. \\(^c\\) Coefficient of variation > 33.3%. \\(^d\\) Employment Income: Wages/salaries or self-employment. \\(^e\\) Worker’s compensation: Employment insurance or worker’s compensation or social assistance/welfare. \\(^f\\) Senior Benefits: Benefits from Canada or Quebec Pension Plan or job related retirement pensions, superannuation and annuities or RRSP/RRIF of Old Age Security and Guaranteed Income Supplement. \\(^g\\) Other: Dividends/interest or child tax benefit or child support or alimony or other or no income. \\(^h\\) Not applicable: Respondents who live in a household with only one person. The income variable “main source of personal income” is applicable only to those that live in a household of more than one person. \\(^i\\) Not Stated: Question was not answered (don’t know, refusal, not stated). - Hint - You can produce 1 table with total, and another table stratified by the necessary variable.\n\nShow the code# your code here\nrequire(tableone)"
  },
  {
    "objectID": "accessingE.html#question-iii-20-grade",
    "href": "accessingE.html#question-iii-20-grade",
    "title": "Exercise (A)",
    "section": "Question III: [20% grade]",
    "text": "Question III: [20% grade]\n3(a) Subset\nSubset the dataset excluding ‘Very good or excellent’ responses from the self-rated mental health variable\n\nShow the code# your code here\n\n\n3(b) Recode\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\nShow the code# your code here\n\n\n3(c) Regression\nRun a logistic regression model for finding the relationship between community belonging (Reference: Very weak) and self-rated mental health (Reference: Poor) among respondents with mental or substance use disorders. Adjust the model for three confounders: sex, age, and race/ethnicity.\n\nShow the code# your code here\n\n\n3(d) Reporting odds ratio\nReport the odds ratios and associated confidence intervals (use Publish package).\n\nShow the coderequire(Publish)\n# your code here\n\n\n\n\n\n\n\n\nPalis, Heather, Kirsten Marchand, and Eugenia Oviedo-Joekes. 2020. “The Relationship Between Sense of Community Belonging and Self-Rated Mental Health Among Canadians with Mental or Substance Use Disorders.” Journal of Mental Health 29 (2): 168–75."
  },
  {
    "objectID": "researchquestion.html#background",
    "href": "researchquestion.html#background",
    "title": "Research questions",
    "section": "Background",
    "text": "Background\nWhen we are starting a research project, one of the first steps is to clearly define your research topic or question. According to Thabane et al., there is a structured approach to do this, primarily using the PICOT and FINER frameworks.\n\n\nThabane et al. (2009)\n\n\nPICOT Framework:\n\nThe PICOT framework helps to structure a specific and clear research question by focusing on five key elements:\n\n\n\n\n\n Element \n    Description \n    Example \n  \n\n\n P \n    Population of Interest: Who is the target group you are studying? \n    US adults \n  \n\n I \n    Intervention: What is the main action, treatment, or variable you're looking at? \n    Effect of having rheumatoid arthritis \n  \n\n C \n    Comparison: Are you comparing the intervention against a control group or usual care? \n    People without rheumatoid arthritis \n  \n\n O \n    Outcome of Interest: What specifically do you want to measure? \n    Rate of cardiovascular diseases \n  \n\n T \n    Time Frame: Over what time period will your study take place? \n    1999–2018 \n  \n\n\n\n\n\n\nResearch Question: “In US adults, does having rheumatoid arthritis, compared to those without rheumatoid arthritis, affect the rate of cardiovascular diseases during 1999–2018?” based on Hossain et al. (2022): DOI: 10.1016/j.annepidem.2022.03.005\n\n\nFINER Criteria:\n\nOnce we have formulated your research question with the help of the PICOT elements, we should evaluate it using the FINER criteria:\n\n\n\n\nFINER Criteria\n \n Element \n    Description \n  \n\n\n F \n    Feasible: Is it possible to conduct this research with available resources? \n  \n\n I \n    Interesting: Is the research question intriguing to the scientific community? \n  \n\n N \n    Novel: Is the question original and not already thoroughly researched? \n  \n\n E \n    Ethical: Is the research ethically sound? \n  \n\n R \n    Relevant: Is the research currently needed or will it fill a gap in existing knowledge? \n  \n\n\n\n\nThe key takeaway is: Use the PICOT and FINER frameworks to guide you in framing a compelling, ethical, and achievable research question.\n\n\nRefer to the ‘Scientific Writing for Health Research’ book chapter for more details and examples.\n\n\n\n\n\n\nNote\n\n\n\nWe include 2 types of tutorials that emphasize the critical steps of data preparation and analysis tailored to specific research questions, cosidering the PICOT framework. They underscore the importance of refining and cleaning datasets to ensure their suitability for rigorous analytical procedures. The analyses, while rooted in distinct methodologies, converge on the common goal of deriving meaningful insights and ensuring the integrity and validity of the results obtained from the processed analytical data.\n\n\n\n\nData preparation: Merging, reformatting and recategorizing essential variables to create a dataset suitable for analysis, aligning it with the study’s objectives.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "researchquestion.html#overview-of-tutorials",
    "href": "researchquestion.html#overview-of-tutorials",
    "title": "Research questions",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\nPredictive questions\n\n\nIn this chapter, we’ve embarked on a journey to understand the nuances of different research questions, laying the groundwork for the topics that lie ahead. As we move forward, the next chapter will delve deeper into the challenges associated with causal questions. We’ll explore the complexities of causal associations and discuss the optimal types of variables to include in adjustment models for accurate treatment effect estimation. Following that, we’ll transition to a chapter dedicated entirely to predictive questions, shedding light on their unique attributes and the methodologies best suited for addressing them. Join us as we navigate these intricate terrains of research inquiry.\nThe first tutorial serves to educate the user on how to utilize the RHC dataset to answer a predictive research question: developing a prediction model for the length of stay. The tutorial equips users with the skills to clean and process raw data, transforming it into an analyzable format, and introduces concepts that will be foundational for subsequent analysis.\nThe second tutorial (part a for downloading and part b for analyzing) provides an in-depth guide on how to build a predictive model for Diastolic blood pressure using the NHANES dataset for the years 2013-14.\nCausal questions\nThe third tutorial aims to guide a study on the relationship between Osteoarthritis (OA) and cardiovascular diseases (CVD) among Canadian adults from 2001-2005. Utilizing the Canadian Community Health Survey (CCHS) cycle 1.1-3.1, the study intends to explore whether OA increases (more accurately, whether associated with) the risk of developing CVD.\nThe NHANES dataset was analyzed in this forth tutorial to explore the relationship between health predictors and cholesterol levels (association/causal). After refining the survey design and handling missing data, regression models were built using varying predictors. Standard error computations and p-values were derived, adjusting for the survey’s unique structure.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\nReference\n\n\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nThabane, Lehana, Tara Thomas, Chenglin Ye, and James Paul. 2009. “Posing the Research Question: Not so Simple.” Canadian Journal of Anesthesia/Journal Canadien d’anesthésie 56 (1): 71–79."
  },
  {
    "objectID": "researchquestion1.html",
    "href": "researchquestion1.html",
    "title": "Predictive question-1",
    "section": "",
    "text": "Show the code# Load required packages\nrequire(tableone)\nrequire(Publish)\nrequire(MatchIt)\nrequire(cobalt)\nrequire(ggplot2)\n\n\nWorking with a Predictive question using RHC\nThis tutorial delves into processing and understanding the RHC dataset, which pertains to patients in the intensive care unit. The dataset is particularly centered around the implications of using right heart catheterization (RHC) in the early phases of care, with a focus on comparing two patient groups: those who received the RHC procedure and those who did not. The key outcome being analyzed is the 30-day survival rate. We will use this as an example to explain how to work with a predictive research question to build the analytic data.\n\n\nLink for the RHC dataset\n(Connors et al. 1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as\n\n\nlength of stay (hospital stay; measured continuously)\n\ndeath within certain period (death at any time up to 180 Days; measured as a binary variable)\n\nThe original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above.\nBut we will use this data as a case study for our prediction modelling. Traditional PICOT framework is designed primarily for clinical questions related to interventions, so when applying it to other areas like predictive modeling, some creative adaptation is needed.\n\n\n\n\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nNot applicable, as we are dealing with a prediction model here\n\n\nC\nNot applicable, as we are dealing with a prediction model here\n\n\nO\nin-hospital mortality\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\n\n\n\nWe are interested in developing a prediction model for the length of stay.\nData download\nData is freely available from Vanderbilt Biostatistics, variable list is available here, and the article is freely available from researchgate.\n\n\nRHC Data amd search for right heart catheterization dataset\n\nVariable list\n\nArticle\n\n\nLet us download the dataset and save it for later use.\n\nShow the code# Load the dataset\nObsData <- read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE)\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhc.RDS\")\n\n\nCreating analytic data\nNow, we show the process of preparing analytic data, so that the variables generally match with the way the authors were coded in the original article. Below we show the process of creating the analytic data.\nAdd column for outcome: length of stay\n\nShow the code# Length.of.Stay = date of discharge - study admission date\nObsData$Length.of.Stay <- ObsData$dschdte - ObsData$sadmdte\n\n# Length.of.Stay = date of death - study admission date if date of discharge not available\nObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] <- \n  ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - \n  ObsData$sadmdte[is.na(ObsData$Length.of.Stay)]\n\n\nRecoding column for outcome: death\n\n\n\n\n\n\nTip\n\n\n\nHere we use the ifelse function to create a categorical variable. Other related functions are cut, car.\n\n\nLet us recode our outcome variable as a binary variable:\n\nShow the codeObsData$death <- ifelse(ObsData$death == \"Yes\", 1, 0)\n\n\nRemove unnecessary outcomes\nOur next task is to remove unnecessary outcomes:\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to drop variables from a dataset. E.g., without using any package and using the select function from the dplyr package.\n\n\n\nShow the codeObsData <- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, \n                            t3d30, dth30, surv2md1))\n\n\nRemove unnecessary and problematic variables\nNow we will drop unnecessary and problematic variables:\n\nShow the codeObsData <- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, urin1, cat2))\n\n\nBasic data cleanup\nNow we will do some basic cleanup.\n\n\n\n\n\n\nTip\n\n\n\nWe an use the lapply function to convert all categorical variables to factors at once. Not that a similar function to lapply is sapply. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\n\nShow the code# convert all categorical variables to factors\nfactors <- c(\"cat1\", \"ca\", \"death\", \"cardiohx\", \"chfhx\", \n             \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n             \"transhx\", \"amihx\", \"sex\", \"dnr1\", \"ninsclas\", \n             \"resp\", \"card\", \"neuro\", \"gastr\", \"renal\", \"meta\", \n             \"hema\", \"seps\", \"trauma\", \"ortho\", \"race\", \n             \"income\")\nObsData[factors] <- lapply(ObsData[factors], as.factor)\n\n# convert RHC.use (RHC vs. No RHC) to a binary variable\nObsData$RHC.use <- ifelse(ObsData$swang1 == \"RHC\", 1, 0)\nObsData <- dplyr::select(ObsData, !swang1)\n\n# Categorize the variables to match with the original paper\nObsData$age <- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf),\n                   right=FALSE)\nObsData$race <- factor(ObsData$race, levels=c(\"white\",\"black\",\"other\"))\nObsData$sex <- as.factor(ObsData$sex)\nObsData$sex <- relevel(ObsData$sex, ref = \"Male\")\nObsData$cat1 <- as.factor(ObsData$cat1)\nlevels(ObsData$cat1) <- c(\"ARF\",\"CHF\",\"Other\",\"Other\",\"Other\",\n                          \"Other\",\"Other\",\"MOSF\",\"MOSF\")\nObsData$ca <- as.factor(ObsData$ca)\nlevels(ObsData$ca) <- c(\"Metastatic\",\"None\",\"Localized (Yes)\")\nObsData$ca <- factor(ObsData$ca, levels=c(\"None\", \"Localized (Yes)\",\n                                          \"Metastatic\"))\n\n\nRename variables\n\nShow the code# Rename the variables\nnames(ObsData) <- c(\"Disease.category\", \"Cancer\", \"Death\", \"Cardiovascular\", \n                    \"Congestive.HF\", \"Dementia\", \"Psychiatric\", \"Pulmonary\", \n                    \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\", \n                    \"Immunosupperssion\", \"Transfer.hx\", \"MI\", \"age\", \"sex\", \n                    \"edu\", \"DASIndex\", \"APACHE.score\", \"Glasgow.Coma.Score\", \n                    \"blood.pressure\", \"WBC\", \"Heart.rate\", \"Respiratory.rate\", \n                    \"Temperature\", \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\", \n                    \"Bilirubin\", \"Creatinine\", \"Sodium\", \"Potassium\", \"PaCo2\", \n                    \"PH\", \"Weight\", \"DNR.status\", \"Medical.insurance\", \n                    \"Respiratory.Diag\", \"Cardiovascular.Diag\", \n                    \"Neurological.Diag\", \"Gastrointestinal.Diag\", \"Renal.Diag\",\n                    \"Metabolic.Diag\", \"Hematologic.Diag\", \"Sepsis.Diag\", \n                    \"Trauma.Diag\", \"Orthopedic.Diag\", \"race\", \"income\", \n                    \"Length.of.Stay\", \"RHC.use\")\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhcAnalytic.RDS\")\n\n\nNotations\nlet us introduce with some notations:\n\n\nNotations\nExample in RHC study\n\n\n\n\n\\(Y_1\\): Observed outcome\nlength of stay\n\n\n\n\\(Y_2\\): Observed outcome\ndeath within 3 months\n\n\n\n\\(L\\): Covariates\nSee below\n\n\nBasic data exploration\nDimension\nLet us the how many rows and columns we have:\n\nShow the codedim(ObsData)\n#> [1] 5735   52\n\n\nComprehensive summary\nLet us see the summary statistics of the variables:\n\n\n\n\n\n\nTip\n\n\n\nTo see the comprehensive summary of the variables, we can use the skim function form skimr package or describe function from rms package\n\n\n\nShow the coderequire(skimr)\n#> Loading required package: skimr\n#> Warning: package 'skimr' was built under R version 4.3.1\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nDeath\n0\n1\nFALSE\n2\n1: 3722, 0: 2013\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, > $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\nPredictive vs. causal models\nThe focus of current document is predictive models (e.g., predicting a health outcome).\n\n\n\n\n\nThe original article by Connors et al. (1996) focused on the association of\n\n\nConnors et al. (1996)\n\nright heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and\nthe health-outcomes (such as length of stay).\n\n\n\n\n\n\nThen the PICOT table changes as follows:\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nReceiving a right heart catheterization (RHC)\n\n\nC\nNot receiving a right heart catheterization (RHC)\n\n\nO\nlength of stay\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996."
  },
  {
    "objectID": "researchquestion2a.html#saving-data-for-later-use",
    "href": "researchquestion2a.html#saving-data-for-later-use",
    "title": "Predictive question-2a",
    "section": "Saving data for later use",
    "text": "Saving data for later use\nIt’s a good practice to save your data for future reference.\n\nShow the codesave(analytic.data, file=\"Data/researchquestion/Analytic2013.RData\")"
  },
  {
    "objectID": "researchquestion2a.html#exercise-try-yourself",
    "href": "researchquestion2a.html#exercise-try-yourself",
    "title": "Predictive question-2a",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nFollow the steps in the exercise section to deepen your understanding and broaden the analysis.\n\nThe following variables were not included in the above analysis, that were included in this paper: try including them and then create the new analytic data:\n\n\neducation level\npoverty income ratio\nSodium intake (mg)\nPotassium intake (mg)\n\n\nDownload the NHANES 2015-2016 and append with the NHANES 2013-2014 analytic data with same variables."
  },
  {
    "objectID": "researchquestion2a.html#references",
    "href": "researchquestion2a.html#references",
    "title": "Predictive question-2a",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion2b.html#saving-for-further-use",
    "href": "researchquestion2b.html#saving-for-further-use",
    "title": "Predictive question-2b",
    "section": "Saving for further use",
    "text": "Saving for further use\n\nShow the codesave(analytic.data1, file = \"Data/researchquestion/NHANESanalytic.Rdata\")"
  },
  {
    "objectID": "researchquestion2b.html#regression-summary-optional",
    "href": "researchquestion2b.html#regression-summary-optional",
    "title": "Predictive question-2b",
    "section": "Regression summary (Optional)",
    "text": "Regression summary (Optional)\n\n\nThis is optional content for this chapter. Later in confounding and predictive factor chapters, we will learn more about adjustment.\nDifferent General Linear Models (GLMs) are fit for diastolic blood pressure using variables like gender, marital status, etc.\nBivariate Regression summary (missing values included)\n\nShow the codefit1g <- glm(diastolic ~ gender, data=analytic.data1)\nsummary(fit1g)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -67.579   -7.091    0.421    6.909   50.421  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   71.5789     0.2352 304.299  < 2e-16 ***\n#> genderFemale  -2.4880     0.3278  -7.591 3.76e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 136.3911)\n#> \n#>     Null deviance: 700862  on 5082  degrees of freedom\n#> Residual deviance: 693003  on 5081  degrees of freedom\n#>   (686 observations deleted due to missingness)\n#> AIC: 39415\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codefit1m <- glm(diastolic ~ marital, data=analytic.data1)\nsummary(fit1m)\n#> \n#> Call:\n#> glm(formula = diastolic ~ marital, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -66.750   -6.838    1.162    7.250   51.250  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                70.7500     0.2138 330.901  < 2e-16 ***\n#> maritalNever married       -1.9116     0.4316  -4.429 9.69e-06 ***\n#> maritalPreviously married  -0.3953     0.4140  -0.955     0.34    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 137.5101)\n#> \n#>     Null deviance: 700840  on 5079  degrees of freedom\n#> Residual deviance: 698139  on 5077  degrees of freedom\n#>   (689 observations deleted due to missingness)\n#> AIC: 39434\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codestr(analytic.data1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 NA 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 NA 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 NA NA 1 1 NA 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 NA 3 NA 3 1 1 NA ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nfit13 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data1)\nsummary(fit13)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender + age.centred + race + marital + \n#>     systolic + smoke + alcohol, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -75.142   -6.090    0.811    7.074   33.512  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               30.92372    2.44895  12.627  < 2e-16 ***\n#> genderFemale              -0.34850    0.59830  -0.582 0.560325    \n#> age.centred               -0.13638    0.02142  -6.367 2.56e-10 ***\n#> raceNon-Hispanic Black     1.44736    1.11246   1.301 0.193443    \n#> raceNon-Hispanic White     0.59565    0.96117   0.620 0.535540    \n#> raceOther Hispanic         1.07369    1.29793   0.827 0.408234    \n#> raceOther race             2.02908    1.22998   1.650 0.099216 .  \n#> maritalNever married      -2.92801    0.79123  -3.701 0.000223 ***\n#> maritalPreviously married  0.44754    0.71911   0.622 0.533804    \n#> systolic                   0.31071    0.01763  17.624  < 2e-16 ***\n#> smokeSome days            -0.42177    0.97853  -0.431 0.666513    \n#> smokeNot at all            0.01796    0.65159   0.028 0.978008    \n#> alcohol                    0.17287    0.10994   1.572 0.116060    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 117.7142)\n#> \n#>     Null deviance: 219477  on 1515  degrees of freedom\n#> Residual deviance: 176924  on 1503  degrees of freedom\n#>   (4253 observations deleted due to missingness)\n#> AIC: 11546\n#> \n#> Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "researchquestion2b.html#check-missingness-optional",
    "href": "researchquestion2b.html#check-missingness-optional",
    "title": "Predictive question-2b",
    "section": "Check missingness (optional)",
    "text": "Check missingness (optional)\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nThe plot_missing() function from the DataExplorer package is used to plot missing data.\n\nShow the coderequire(DataExplorer)\nplot_missing(analytic.data1)\n\n\n\n\n\nShow the coderequire(\"tableone\")\nvars = c(\"systolic\", \"smoke\", \"diastolic\", \"race\", \n                       \"age.centred\", \"gender\", \"marital\", \"alcohol\")\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nSetting correct variable types\nThe variables are explicitly set to either numeric or factor types.\nNote: In case any of the variables types are wrong, your table 1 output will be wrong. Better to be sure about what type of variable you want them to be (numeric or factor). For example, systolic should be numeric. Is it defined that way?\n\nShow the codemode(analytic.data1$systolic)\n#> [1] \"numeric\"\n\n\nIn case it wasn’t (often they can get converted to character), then here is the solution:\n\nShow the code# solution 1: one-by-one\nanalytic.data1$systolic <- as.numeric(as.character(analytic.data1$systolic))\nsummary(analytic.data1$systolic)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   110.0   120.0   123.2   134.0   228.0     658\n\n\n\nShow the code# solution 2: fixing all variable types at once\nnumeric.names <- c(\"systolic\", \"diastolic\", \"age.centred\", \"alcohol\")\nfactor.names <- vars[!vars %in% numeric.names]\nfactor.names\n#> [1] \"smoke\"   \"race\"    \"gender\"  \"marital\"\nanalytic.data1[,factor.names] <- lapply(analytic.data1[,factor.names] , factor)\nanalytic.data1[numeric.names] <- apply(X = analytic.data1[numeric.names],\n                                       MARGIN = 2, FUN =function (x) \n                                         as.numeric(as.character(x)))\nlevels(analytic.data1$marital)\n#> [1] \"Married\"            \"Never married\"      \"Previously married\"\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nComplete case analysis\nRemoves all rows containing NA.\n\nShow the codedim(analytic.data1)\n#> [1] 5769   14\nanalytic.data2 <- as.data.frame(na.omit(analytic.data1))\ndim(analytic.data2)\n#> [1] 1516   14\nplot_missing(analytic.data2)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data2, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         1516        \n#>   systolic (mean (SD))    123.29 (17.58)\n#>   smoke (%)                             \n#>      Every day               590 (38.9) \n#>      Some days               159 (10.5) \n#>      Not at all              767 (50.6) \n#>   diastolic (mean (SD))    70.11 (12.04)\n#>   race (%)                              \n#>      Mexican American        162 (10.7) \n#>      Non-Hispanic Black      292 (19.3) \n#>      Non-Hispanic White      778 (51.3) \n#>      Other Hispanic          126 ( 8.3) \n#>      Other race              158 (10.4) \n#>   age.centred (mean (SD))  -0.76 (16.71)\n#>   gender = Female (%)        626 (41.3) \n#>   marital (%)                           \n#>      Married                 858 (56.6) \n#>      Never married           300 (19.8) \n#>      Previously married      358 (23.6) \n#>   alcohol (mean (SD))       3.15 (2.76)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data2)\nrequire(Publish)\npublish(fit23)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          30.92 [26.12;35.72]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -0.35  [-1.52;0.82]   0.5603254 \n#>  age.centred                          -0.14 [-0.18;-0.09]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        1.45  [-0.73;3.63]   0.1934428 \n#>              Non-Hispanic White        0.60  [-1.29;2.48]   0.5355396 \n#>                  Other Hispanic        1.07  [-1.47;3.62]   0.4082336 \n#>                      Other race        2.03  [-0.38;4.44]   0.0992165 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.93 [-4.48;-1.38]   0.0002229 \n#>              Previously married        0.45  [-0.96;1.86]   0.5338035 \n#>     systolic                           0.31   [0.28;0.35]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.42  [-2.34;1.50]   0.6665127 \n#>                      Not at all        0.02  [-1.26;1.30]   0.9780080 \n#>      alcohol                           0.17  [-0.04;0.39]   0.1160603\n\n\nImputed data\nWe will learn about proper missing data analysis at a latter class. Currently, we will do a simple (but rather controversial) single imputation. In here we are simply using a random sampling to impute (probably the worst method, but we are just filling in some gaps for now).\n\nShow the coderequire(mice)\nimputation1 <- mice(analytic.data1,\n                   method = \"sample\",  \n                   m = 1, # Number of multiple imputations. \n                   maxit = 1 # Number of iteration; mostly useful for convergence\n                   )\n#> \n#>  iter imp variable\n#>   1   1  systolic  diastolic  marital  alcohol  smoke\n#> Warning: Number of logged events: 5\nanalytic.data.imputation1 <- complete(imputation1)\ndim(analytic.data.imputation1)\n#> [1] 5769   14\nstr(analytic.data.imputation1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 100 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 66 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 1 2 1 1 4 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 3 3 3 3 1 1 3 ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nplot_missing(analytic.data.imputation1)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data.imputation1, includeNA = TRUE,\n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    122.98 (18.06)\n#>   smoke (%)                             \n#>      Every day              2187 (37.9) \n#>      Some days               517 ( 9.0) \n#>      Not at all             3065 (53.1) \n#>   diastolic (mean (SD))    70.25 (11.80)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3383 (58.6) \n#>      Never married          1113 (19.3) \n#>      Previously married     1273 (22.1) \n#>   alcohol (mean (SD))       2.62 (2.28)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23i <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data.imputation1)\npublish(fit23i)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          38.74 [36.37;41.11]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -1.30 [-1.88;-0.72]     < 1e-04 \n#>  age.centred                          -0.12 [-0.14;-0.10]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        0.97  [-0.04;1.98]   0.0606987 \n#>              Non-Hispanic White        0.68  [-0.21;1.57]   0.1337626 \n#>                  Other Hispanic        0.91  [-0.32;2.13]   0.1471629 \n#>                      Other race        2.00   [0.93;3.08]   0.0002535 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.43 [-3.24;-1.63]     < 1e-04 \n#>              Previously married       -0.12  [-0.87;0.62]   0.7450923 \n#>     systolic                           0.26   [0.24;0.28]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.17  [-1.21;0.88]   0.7572027 \n#>                      Not at all       -0.19  [-0.80;0.42]   0.5387956 \n#>      alcohol                          -0.02  [-0.15;0.10]   0.7070437\n\n\nWe see some changes in the estimates. After imputing compared to complete case analysis, any changes dramatic (e.g., changing conclusion)?\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\n\nShow the coderequire(jtools)\nrequire(ggstance)\nrequire(broom.mixed)\nrequire(huxtable)\nexport_summs(fit23, fit23i)\n\n\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n(Intercept)\n30.92 ***\n38.74 ***\n\n\n\n(2.45)   \n(1.21)   \n\n\ngenderFemale\n-0.35    \n-1.30 ***\n\n\n\n(0.60)   \n(0.30)   \n\n\nage.centred\n-0.14 ***\n-0.12 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nraceNon-Hispanic Black\n1.45    \n0.97    \n\n\n\n(1.11)   \n(0.52)   \n\n\nraceNon-Hispanic White\n0.60    \n0.68    \n\n\n\n(0.96)   \n(0.45)   \n\n\nraceOther Hispanic\n1.07    \n0.91    \n\n\n\n(1.30)   \n(0.63)   \n\n\nraceOther race\n2.03    \n2.00 ***\n\n\n\n(1.23)   \n(0.55)   \n\n\nmaritalNever married\n-2.93 ***\n-2.43 ***\n\n\n\n(0.79)   \n(0.41)   \n\n\nmaritalPreviously married\n0.45    \n-0.12    \n\n\n\n(0.72)   \n(0.38)   \n\n\nsystolic\n0.31 ***\n0.26 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nsmokeSome days\n-0.42    \n-0.17    \n\n\n\n(0.98)   \n(0.53)   \n\n\nsmokeNot at all\n0.02    \n-0.19    \n\n\n\n(0.65)   \n(0.31)   \n\n\nalcohol\n0.17    \n-0.02    \n\n\n\n(0.11)   \n(0.07)   \n\n\nN\n1516       \n5769       \n\n\nAIC\n11545.85    \n43965.77    \n\n\nBIC\n11620.38    \n44059.01    \n\n\nPseudo R2\n0.19    \n0.15    \n\n *** p < 0.001;  ** p < 0.01;  * p < 0.05.\n\n\nShow the codeplot_summs(fit23, fit23i)\n\n\n\nShow the code# plot_summs(fit23, fit23i, plot.distributions = TRUE)"
  },
  {
    "objectID": "researchquestion2b.html#exercise-try-yourself",
    "href": "researchquestion2b.html#exercise-try-yourself",
    "title": "Predictive question-2b",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nIn this lab, we have done multiple steps that could be improved. One of them was single imputation by random sampling. What other ad hoc method you could use to impute the factor variables?"
  },
  {
    "objectID": "researchquestion2b.html#references",
    "href": "researchquestion2b.html#references",
    "title": "Predictive question-2b",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "href": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "title": "Causal question-1",
    "section": "Naive Analysis of combined 3 cycles",
    "text": "Naive Analysis of combined 3 cycles\nIn the current analysis, we will simply consider all of the variables under consideration as ‘confounders’, and include in our analysis. Later we will perform a refined analysis.\nSummary of the analytic data\nIncluding missing values\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic <- c123sub3\ndim(analytic)\n#> [1] 241380     17\n\nrequire(\"tableone\")\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, includeNA = TRUE)\n#>                       \n#>                        Overall       \n#>   n                    241380        \n#>   CVD = event (%)        7044 ( 2.9) \n#>   age (%)                            \n#>      20-39 years       108161 (44.8) \n#>      40-49 years        59690 (24.7) \n#>      50-59 years        52685 (21.8) \n#>      60-64 years        20844 ( 8.6) \n#>   sex = Male (%)       114104 (47.3) \n#>   income (%)                         \n#>      $29,999 or less    48005 (19.9) \n#>      $30,000-$49,999    49496 (20.5) \n#>      $50,000-$79,999    61093 (25.3) \n#>      $80,000 or more    57056 (23.6) \n#>      NA                 25730 (10.7) \n#>   race (%)                           \n#>      Non-white          25840 (10.7) \n#>      White             210307 (87.1) \n#>      NA                  5233 ( 2.2) \n#>   bmicat (%)                         \n#>      Normal            103378 (42.8) \n#>      Overweight        120423 (49.9) \n#>      Underweight         8964 ( 3.7) \n#>      NA                  8615 ( 3.6) \n#>   phyact (%)                         \n#>      Active             57033 (23.6) \n#>      Inactive          117516 (48.7) \n#>      Moderate           60164 (24.9) \n#>      NA                  6667 ( 2.8) \n#>   smoke (%)                          \n#>      Current smoker     71321 (29.5) \n#>      Former smoker      97845 (40.5) \n#>      Never smoker       71397 (29.6) \n#>      NA                   817 ( 0.3) \n#>   fruit (%)                          \n#>      0-3 daily serving  56256 (23.3) \n#>      4-6 daily serving  96177 (39.8) \n#>      6+ daily serving   45861 (19.0) \n#>      NA                 43086 (17.8) \n#>   painmed (%)                        \n#>      No                 11141 ( 4.6) \n#>      Yes                25743 (10.7) \n#>      NA                204496 (84.7) \n#>   ht (%)                             \n#>      No                213432 (88.4) \n#>      Yes                27592 (11.4) \n#>      NA                   356 ( 0.1) \n#>   copd (%)                           \n#>      No                192608 (79.8) \n#>      Yes                 1353 ( 0.6) \n#>      NA                 47419 (19.6) \n#>   diab (%)                           \n#>      No                232486 (96.3) \n#>      Yes                 8811 ( 3.7) \n#>      NA                    83 ( 0.0) \n#>   edu (%)                            \n#>      < 2ndary           37775 (15.6) \n#>      2nd grad.          44376 (18.4) \n#>      Other 2nd grad.    19273 ( 8.0) \n#>      Post-2nd grad.    136031 (56.4) \n#>      NA                  3925 ( 1.6)\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control        OA            p      test\n#>   n                    221029         20351                    \n#>   CVD = event (%)        5429 ( 2.5)   1615 ( 7.9)  <0.001     \n#>   age (%)                                           <0.001     \n#>      20-39 years       106003 (48.0)   2158 (10.6)             \n#>      40-49 years        55569 (25.1)   4121 (20.2)             \n#>      50-59 years        43706 (19.8)   8979 (44.1)             \n#>      60-64 years        15751 ( 7.1)   5093 (25.0)             \n#>   sex = Male (%)       107729 (48.7)   6375 (31.3)  <0.001     \n#>   income (%)                                        <0.001     \n#>      $29,999 or less    42019 (19.0)   5986 (29.4)             \n#>      $30,000-$49,999    45090 (20.4)   4406 (21.7)             \n#>      $50,000-$79,999    56754 (25.7)   4339 (21.3)             \n#>      $80,000 or more    53637 (24.3)   3419 (16.8)             \n#>      NA                 23529 (10.6)   2201 (10.8)             \n#>   race (%)                                          <0.001     \n#>      Non-white          24681 (11.2)   1159 ( 5.7)             \n#>      White             191513 (86.6)  18794 (92.3)             \n#>      NA                  4835 ( 2.2)    398 ( 2.0)             \n#>   bmicat (%)                                        <0.001     \n#>      Normal             96697 (43.7)   6681 (32.8)             \n#>      Overweight        107871 (48.8)  12552 (61.7)             \n#>      Underweight         8490 ( 3.8)    474 ( 2.3)             \n#>      NA                  7971 ( 3.6)    644 ( 3.2)             \n#>   phyact (%)                                        <0.001     \n#>      Active             52942 (24.0)   4091 (20.1)             \n#>      Inactive          106580 (48.2)  10936 (53.7)             \n#>      Moderate           55222 (25.0)   4942 (24.3)             \n#>      NA                  6285 ( 2.8)    382 ( 1.9)             \n#>   smoke (%)                                         <0.001     \n#>      Current smoker     65398 (29.6)   5923 (29.1)             \n#>      Former smoker      88210 (39.9)   9635 (47.3)             \n#>      Never smoker       66663 (30.2)   4734 (23.3)             \n#>      NA                   758 ( 0.3)     59 ( 0.3)             \n#>   fruit (%)                                         <0.001     \n#>      0-3 daily serving  52140 (23.6)   4116 (20.2)             \n#>      4-6 daily serving  87951 (39.8)   8226 (40.4)             \n#>      6+ daily serving   41606 (18.8)   4255 (20.9)             \n#>      NA                 39332 (17.8)   3754 (18.4)             \n#>   painmed (%)                                       <0.001     \n#>      No                 10624 ( 4.8)    517 ( 2.5)             \n#>      Yes                23084 (10.4)   2659 (13.1)             \n#>      NA                187321 (84.7)  17175 (84.4)             \n#>   ht (%)                                            <0.001     \n#>      No                198550 (89.8)  14882 (73.1)             \n#>      Yes                22142 (10.0)   5450 (26.8)             \n#>      NA                   337 ( 0.2)     19 ( 0.1)             \n#>   copd (%)                                          <0.001     \n#>      No                173224 (78.4)  19384 (95.2)             \n#>      Yes                  938 ( 0.4)    415 ( 2.0)             \n#>      NA                 46867 (21.2)    552 ( 2.7)             \n#>   diab (%)                                          <0.001     \n#>      No                213910 (96.8)  18576 (91.3)             \n#>      Yes                 7046 ( 3.2)   1765 ( 8.7)             \n#>      NA                    73 ( 0.0)     10 ( 0.0)             \n#>   edu (%)                                           <0.001     \n#>      < 2ndary           32884 (14.9)   4891 (24.0)             \n#>      2nd grad.          40950 (18.5)   3426 (16.8)             \n#>      Other 2nd grad.    17808 ( 8.1)   1465 ( 7.2)             \n#>      Post-2nd grad.    125772 (56.9)  10259 (50.4)             \n#>      NA                  3615 ( 1.6)    310 ( 1.5)\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLet us investigate why pain medication has so much missing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional content respondent (cycle 3.1):\n\n\n\n\n\nIn cycle 2.1, only 21,755 out of 134,072 responded to optional medication component.\nComplete case analysis\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic2 <- as.data.frame(na.omit(c123sub3))\ndim(analytic2)\n#> [1] 21623    17\n\n\ntab1 <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, includeNA = TRUE)\nprint(tab1, showAllLevels = TRUE)\n#>              \n#>               level             Overall      \n#>   n                             21623        \n#>   CVD (%)     0 event           20917 (96.7) \n#>               event               706 ( 3.3) \n#>   age (%)     20-39 years        7119 (32.9) \n#>               40-49 years        7024 (32.5) \n#>               50-59 years        5457 (25.2) \n#>               60-64 years        2023 ( 9.4) \n#>   sex (%)     Female            10982 (50.8) \n#>               Male              10641 (49.2) \n#>   income (%)  $29,999 or less    4054 (18.7) \n#>               $30,000-$49,999    4461 (20.6) \n#>               $50,000-$79,999    6600 (30.5) \n#>               $80,000 or more    6508 (30.1) \n#>   race (%)    Non-white          2488 (11.5) \n#>               White             19135 (88.5) \n#>   bmicat (%)  Normal             8993 (41.6) \n#>               Overweight        11739 (54.3) \n#>               Underweight         891 ( 4.1) \n#>   phyact (%)  Active             5502 (25.4) \n#>               Inactive          10495 (48.5) \n#>               Moderate           5626 (26.0) \n#>   smoke (%)   Current smoker     5887 (27.2) \n#>               Former smoker      9368 (43.3) \n#>               Never smoker       6368 (29.5) \n#>   fruit (%)   0-3 daily serving  5806 (26.9) \n#>               4-6 daily serving 10730 (49.6) \n#>               6+ daily serving   5087 (23.5) \n#>   painmed (%) No                 6197 (28.7) \n#>               Yes               15426 (71.3) \n#>   ht (%)      No                19014 (87.9) \n#>               Yes                2609 (12.1) \n#>   copd (%)    No                21475 (99.3) \n#>               Yes                 148 ( 0.7) \n#>   diab (%)    No                20760 (96.0) \n#>               Yes                 863 ( 4.0) \n#>   edu (%)     < 2ndary           2998 (13.9) \n#>               2nd grad.          4605 (21.3) \n#>               Other 2nd grad.    1509 ( 7.0) \n#>               Post-2nd grad.    12511 (57.9)\ntab1b <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, strata = \"OA\", includeNA = TRUE)\nprint(tab1b, showAllLevels = TRUE)\n#>              Stratified by OA\n#>               level             Control       OA           p      test\n#>   n                             19459         2164                    \n#>   CVD (%)     0 event           18917 (97.2)  2000 (92.4)  <0.001     \n#>               event               542 ( 2.8)   164 ( 7.6)             \n#>   age (%)     20-39 years        6915 (35.5)   204 ( 9.4)  <0.001     \n#>               40-49 years        6515 (33.5)   509 (23.5)             \n#>               50-59 years        4504 (23.1)   953 (44.0)             \n#>               60-64 years        1525 ( 7.8)   498 (23.0)             \n#>   sex (%)     Female             9521 (48.9)  1461 (67.5)  <0.001     \n#>               Male               9938 (51.1)   703 (32.5)             \n#>   income (%)  $29,999 or less    3413 (17.5)   641 (29.6)  <0.001     \n#>               $30,000-$49,999    3968 (20.4)   493 (22.8)             \n#>               $50,000-$79,999    6023 (31.0)   577 (26.7)             \n#>               $80,000 or more    6055 (31.1)   453 (20.9)             \n#>   race (%)    Non-white          2370 (12.2)   118 ( 5.5)  <0.001     \n#>               White             17089 (87.8)  2046 (94.5)             \n#>   bmicat (%)  Normal             8277 (42.5)   716 (33.1)  <0.001     \n#>               Overweight        10356 (53.2)  1383 (63.9)             \n#>               Underweight         826 ( 4.2)    65 ( 3.0)             \n#>   phyact (%)  Active             4986 (25.6)   516 (23.8)   0.190     \n#>               Inactive           9417 (48.4)  1078 (49.8)             \n#>               Moderate           5056 (26.0)   570 (26.3)             \n#>   smoke (%)   Current smoker     5247 (27.0)   640 (29.6)  <0.001     \n#>               Former smoker      8363 (43.0)  1005 (46.4)             \n#>               Never smoker       5849 (30.1)   519 (24.0)             \n#>   fruit (%)   0-3 daily serving  5290 (27.2)   516 (23.8)  <0.001     \n#>               4-6 daily serving  9686 (49.8)  1044 (48.2)             \n#>               6+ daily serving   4483 (23.0)   604 (27.9)             \n#>   painmed (%) No                 5859 (30.1)   338 (15.6)  <0.001     \n#>               Yes               13600 (69.9)  1826 (84.4)             \n#>   ht (%)      No                17356 (89.2)  1658 (76.6)  <0.001     \n#>               Yes                2103 (10.8)   506 (23.4)             \n#>   copd (%)    No                19359 (99.5)  2116 (97.8)  <0.001     \n#>               Yes                 100 ( 0.5)    48 ( 2.2)             \n#>   diab (%)    No                18751 (96.4)  2009 (92.8)  <0.001     \n#>               Yes                 708 ( 3.6)   155 ( 7.2)             \n#>   edu (%)     < 2ndary           2527 (13.0)   471 (21.8)  <0.001     \n#>               2nd grad.          4173 (21.4)   432 (20.0)             \n#>               Other 2nd grad.    1364 ( 7.0)   145 ( 6.7)             \n#>               Post-2nd grad.    11395 (58.6)  1116 (51.6)"
  },
  {
    "objectID": "researchquestion3.html#save-data-for-later",
    "href": "researchquestion3.html#save-data-for-later",
    "title": "Causal question-1",
    "section": "Save data for later",
    "text": "Save data for later\n\nShow the codesave(analytic, analytic2, cc123a, file = \"Data/researchquestion/OA123CVD.RData\")"
  },
  {
    "objectID": "researchquestion3.html#references",
    "href": "researchquestion3.html#references",
    "title": "Causal question-1",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "researchquestion4.html",
    "href": "researchquestion4.html",
    "title": "Causal question-2",
    "section": "",
    "text": "Working with a causal question using NHANES\nWe are interested in exploring the relationship between diabetes (binary exposure variable defined as whether the doctor ever told the participant has diabetes) and cholesterol (binary outcome variable defined as whether total cholesterol is more than 200 mg/dL). Below is the PICOT:\n\n\nPICOT element\nDescription\n\n\n\nP\nUS adults\n\n\nI\nDiabetes\n\n\nC\nNo diabetes\n\n\nO\nTotal cholesterol > 200 mg/dL\n\n\nT\n2017–2018\n\n\n\nFirst, we will prepare the analytic dataset from NHANES 2017–2018.\nSecond, we will work with subset of data to assess the association between diabetes and cholesterol, and to get proper SE and 95% CI for the estimate. We emphasize the correct usage of the survey’s design features (correct handling of survey design elements, such as stratification, clustering, and weighting) to obtain accurate population-level estimates.\n\nShow the code# Load required packages\nrequire(SASxport)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(nhanesA)\nrequire(survey)\nrequire(Publish)\nrequire(jtools)\n\n\nSteps for creating analytic dataset\nWe will combine multiple components (e.g., demographic, blood pressure) using the unique identifier to create our analytic dataset.\n\n\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\n\n\n\nDownload and Subsetting to retain only the useful variables\nSearch literature for the relevant variables, and then see if some of them are available in the NHANES data.\n\n\nPeters, Fabian, and Levy (2014)\nAn an example, let us assume that variables listed in the following figures are known to be useful. Then we will try to indentify, in which NHANES component we have these variables.\n\n\nRefer to the earlier chapter to get a more detailed understanding of how we search for variables within NHANES.\n\n\n\n\n\n\n\nNHANES Data Components:\n\nDemographic (variables like age, gender, income, etc.)\nBlood Pressure (Diastolic and Systolic pressure)\nBody Measures (BMI, Waist Circumference, etc.)\nSmoking Status (Current smoker or not)\nCholesterol (Total cholesterol in different units)\nBiochemistry Profile (Triglycerides, Uric acid, etc.)\nPhysical Activity (Vigorous work and recreational activities)\nDiabetes (Whether the respondent has been told by a doctor that they have diabetes)\n\nDemographic component:\n\nShow the codedemo <- nhanes('DEMO_J') # Both males and females 0 YEARS - 150 YEARS\ndemo <- demo[c(\"SEQN\", # Respondent sequence number\n                 \"RIAGENDR\", # gender\n                 \"RIDAGEYR\", # Age in years at screening\n                 \"DMDBORN4\", # Country of birth\n                 \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                 \"DMDEDUC3\", # Education level - Children/Youth 6-19\n                 \"DMDEDUC2\", # Education level - Adults 20+\n                 \"DMDMARTL\", # Marital status: 20 YEARS - 150 YEARS\n                 \"INDHHIN2\", # Total household income\n                 \"WTMEC2YR\", \"SDMVPSU\", \"SDMVSTRA\")]\ndemo_vars <- names(demo) # nhanesTableVars('DEMO', 'DEMO_J', namesonly=TRUE)\ndemo1 <- nhanesTranslate('DEMO_J', demo_vars, data=demo)\n#> Translated columns: RIAGENDR DMDBORN4 RIDRETH3 DMDEDUC3 DMDEDUC2 DMDMARTL INDHHIN2\n\n\nBlood pressure component:\n\nShow the codebpx <- nhanes('BPX_J')\nbpx <- bpx[c(\"SEQN\", # Respondent sequence number\n             \"BPXDI1\", #Diastolic: Blood pres (1st rdg) mm Hg\n             \"BPXSY1\" # Systolic: Blood pres (1st rdg) mm Hg\n             )]\nbpx_vars <- names(bpx) \nbpx1 <- nhanesTranslate('BPX_J', bpx_vars, data=bpx)\n#> Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx): No columns were\n#> translated\n\n\nBody measure component:\n\nShow the codebmi <- nhanes('BMX_J')\nbmi <- bmi[c(\"SEQN\", # Respondent sequence number\n               \"BMXWT\", # Weight (kg) \n               \"BMXHT\", # Standing Height (cm)\n               \"BMXBMI\", # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\n               #\"BMDBMIC\", # BMI Category - Children/Youth # 2 YEARS - 19 YEARS\n               \"BMXWAIST\" # Waist Circumference (cm): 2 YEARS - 150 YEARS\n               )]\nbmi_vars <- names(bmi) \nbmi1 <- nhanesTranslate('BMX_J', bmi_vars, data=bmi)\n#> Warning in nhanesTranslate(\"BMX_J\", bmi_vars, data = bmi): No columns were\n#> translated\n\n\nSmoking component:\n\nShow the codesmq <- nhanes('SMQ_J')\nsmq <- smq[c(\"SEQN\", # Respondent sequence number\n               \"SMQ040\" # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\n               )]\nsmq_vars <- names(smq) \nsmq1 <- nhanesTranslate('SMQ_J', smq_vars, data=smq)\n#> Translated columns: SMQ040\n\n\n\nShow the code# alq <- nhanes('ALQ_J')\n# alq <- alq[c(\"SEQN\", # Respondent sequence number\n#                \"ALQ130\" # Avg # alcoholic drinks/day - past 12 mos\n#                # 18 YEARS - 150 YEARS\n#                )]\n# alq_vars <- names(alq) \n# alq1 <- nhanesTranslate('ALQ_J', alq_vars, data=alq)\n\n\nCholesterol component:\n\nShow the codechl <- nhanes('TCHOL_J') # 6 YEARS - 150 YEARS\nchl <- chl[c(\"SEQN\", # Respondent sequence number\n               \"LBXTC\", # Total Cholesterol (mg/dL)\n               \"LBDTCSI\" # Total Cholesterol (mmol/L)\n               )]\nchl_vars <- names(chl) \nchl1 <- nhanesTranslate('TCHOL_J', chl_vars, data=chl)\n#> Warning in nhanesTranslate(\"TCHOL_J\", chl_vars, data = chl): No columns were\n#> translated\n\n\nBiochemistry Profile component:\n\nShow the codetri <- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\ntri <- tri[c(\"SEQN\", # Respondent sequence number\n               \"LBXSTR\", # Triglycerides, refrig serum (mg/dL)\n               \"LBXSUA\", # Uric acid\n               \"LBXSTP\", # total Protein (g/dL)\n               \"LBXSTB\", # Total Bilirubin (mg/dL)\n               \"LBXSPH\", # Phosphorus (mg/dL)\n               \"LBXSNASI\", # Sodium (mmol/L)\n               \"LBXSKSI\", # Potassium (mmol/L)\n               \"LBXSGB\", # Globulin (g/dL)\n               \"LBXSCA\" # Total Calcium (mg/dL)\n               )]\ntri_vars <- names(tri) \ntri1 <- nhanesTranslate('BIOPRO_J', tri_vars, data=tri)\n#> Warning in nhanesTranslate(\"BIOPRO_J\", tri_vars, data = tri): No columns were\n#> translated\n\n\nPhysical activity component:\n\nShow the codepaq <- nhanes('PAQ_J')\npaq <- paq[c(\"SEQN\", # Respondent sequence number\n               \"PAQ605\", # Vigorous work activity \n               \"PAQ650\" # Vigorous recreational activities\n               )]\npaq_vars <- names(paq) \npaq1 <- nhanesTranslate('PAQ_J', paq_vars, data=paq)\n#> Translated columns: PAQ605 PAQ650\n\n\nDiabetes component:\n\nShow the codediq <- nhanes('DIQ_J')\ndiq <- diq[c(\"SEQN\", # Respondent sequence number\n               \"DIQ010\" # Doctor told you have diabetes\n               )]\ndiq_vars <- names(diq) \ndiq1 <- nhanesTranslate('DIQ_J', diq_vars, data=diq)\n#> Translated columns: DIQ010\n\n\nMerging all the datasets\n\n\n\n\n\n\nTip\n\n\n\nWe can use the merge or Reduce function to combine the datasets\n\n\n\nShow the codeanalytic.data7 <- Reduce(function(x,y) merge(x,y,by=\"SEQN\",all=TRUE) ,\n       list(demo1,bpx1,bmi1,smq1,chl1,tri1,paq1,diq1))\ndim(analytic.data7)\n#> [1] 9254   33\n\n\n\n\nAll these datasets are merged into one analytic dataset using the SEQN as the key. This can be done either all at once using the Reduce function or one by one (using merge once at a time).\n\nShow the code# Merging one by one\n# analytic.data0 <- merge(demo1, bpx1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data1 <- merge(analytic.data0, bmi1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data2 <- merge(analytic.data1, smq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data3 <- merge(analytic.data2, alq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data4 <- merge(analytic.data3, chl1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data5 <- merge(analytic.data4, tri1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data6 <- merge(analytic.data5, paq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data7 <- merge(analytic.data6, diq1, by = c(\"SEQN\"), all=TRUE)\n# dim(analytic.data7)\n\n\nCheck Target population and avoid zero-cell cross-tabulation\n\n\nThe dataset is then filtered to only include adults (20 years and older) and avoid zero-cell cross-tabulation.\nSee that marital status variable was restricted to 20 YEARS - 150 YEARS.\n\nShow the codestr(analytic.data7)\n#> 'data.frame':    9254 obs. of  33 variables:\n#>  $ SEQN    : num  93703 93704 93705 93706 93707 ...\n#>  $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 2 1 2 1 1 2 2 2 1 1 ...\n#>  $ RIDAGEYR: num  2 2 66 18 13 66 75 0 56 18 ...\n#>  $ DMDBORN4: Factor w/ 4 levels \"Born in 50 US states or Washingt\",..: 1 1 1 1 1 2 1 1 2 2 ...\n#>  $ RIDRETH3: Factor w/ 6 levels \"Mexican American\",..: 5 3 4 5 6 5 4 3 5 1 ...\n#>  $ DMDEDUC3: Factor w/ 17 levels \"Never attended / kindergarten on\",..: NA NA NA 16 7 NA NA NA NA 13 ...\n#>  $ DMDEDUC2: Factor w/ 7 levels \"Less than 9th grade\",..: NA NA 2 NA NA 1 4 NA 5 NA ...\n#>  $ DMDMARTL: Factor w/ 7 levels \"Married\",\"Widowed\",..: NA NA 3 NA NA 1 2 NA 1 NA ...\n#>  $ INDHHIN2: Factor w/ 16 levels \"$ 0 to $ 4,999\",..: 14 14 3 NA 10 6 2 14 14 4 ...\n#>  $ WTMEC2YR: num  8540 42567 8338 8723 7065 ...\n#>  $ SDMVPSU : num  2 1 2 2 1 2 1 1 2 2 ...\n#>  $ SDMVSTRA: num  145 143 145 134 138 138 136 134 134 147 ...\n#>  $ BPXDI1  : num  NA NA NA 74 38 NA 66 NA 68 68 ...\n#>  $ BPXSY1  : num  NA NA NA 112 128 NA 120 NA 108 112 ...\n#>  $ BMXWT   : num  13.7 13.9 79.5 66.3 45.4 53.5 88.8 10.2 62.1 58.9 ...\n#>  $ BMXHT   : num  88.6 94.2 158.3 175.7 158.4 ...\n#>  $ BMXBMI  : num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>  $ BMXWAIST: num  48.2 50 101.8 79.3 64.1 ...\n#>  $ SMQ040  : Factor w/ 3 levels \"Every day\",\"Some days\",..: NA NA 3 NA NA NA 1 NA NA 2 ...\n#>  $ LBXTC   : num  NA NA 157 148 189 209 176 NA 238 182 ...\n#>  $ LBDTCSI : num  NA NA 4.06 3.83 4.89 5.4 4.55 NA 6.15 4.71 ...\n#>  $ LBXSTR  : num  NA NA 95 92 110 72 132 NA 59 124 ...\n#>  $ LBXSUA  : num  NA NA 5.8 8 5.5 4.5 6.2 NA 4.2 5.8 ...\n#>  $ LBXSTP  : num  NA NA 7.3 7.1 8 7.1 7 NA 7.1 8.1 ...\n#>  $ LBXSTB  : num  NA NA 0.6 0.7 0.7 0.5 0.3 NA 0.3 0.8 ...\n#>  $ LBXSPH  : num  NA NA 4 4 4.3 3.3 3.5 NA 3.4 5.1 ...\n#>  $ LBXSNASI: num  NA NA 141 144 137 144 141 NA 140 141 ...\n#>  $ LBXSKSI : num  NA NA 4 4.4 3.3 4.4 4.1 NA 4.9 4.3 ...\n#>  $ LBXSGB  : num  NA NA 2.9 2.7 2.8 3.2 3.3 NA 3.1 3.3 ...\n#>  $ LBXSCA  : num  NA NA 9.2 9.6 10.1 9.5 9.9 NA 9.4 9.6 ...\n#>  $ PAQ605  : Factor w/ 3 levels \"Yes\",\"No\",\"Don't know\": NA NA 2 2 NA 2 2 NA 2 1 ...\n#>  $ PAQ650  : Factor w/ 2 levels \"Yes\",\"No\": NA NA 2 2 NA 2 2 NA 1 1 ...\n#>  $ DIQ010  : Factor w/ 4 levels \"Yes\",\"No\",\"Borderline\",..: 2 2 2 2 2 3 2 NA 2 2 ...\nhead(analytic.data7)\n\n\n\n  \n\n\nShow the codesummary(analytic.data7$RIDAGEYR)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   11.00   31.00   34.33   58.00   80.00\n\n\n\nShow the codedim(analytic.data7)\n#> [1] 9254   33\nanalytic.data8 <- analytic.data7\nanalytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR < 20] <- NA\n#analytic.data8 <- subset(analytic.data7, RIDAGEYR >= 20)\ndim(analytic.data8)\n#> [1] 9254   33\n\n\nGet rid of variables where target was less than 20 years of age accordingly.\n\nShow the codeanalytic.data8$DMDEDUC3 <- NULL # not relevant for adults\n#analytic.data8$BMDBMIC <- NULL # not relevant for adults\n\n\nGet rid of invalid responses\n\n\nVariables that have “Don’t Know” or “Refused” as responses are set to NA, effectively getting rid of invalid responses.\n\nShow the codefactor.names <- c(\"RIAGENDR\",\"DMDBORN4\",\"RIDRETH3\",\n                  \"DMDEDUC2\",\"DMDMARTL\",\"INDHHIN2\", \n                  \"SMQ040\", \"PAQ605\", \"PAQ650\", \"DIQ010\")\nnumeric.names <- c(\"SEQN\",\"RIDAGEYR\",\"WTMEC2YR\",\n                   \"SDMVPSU\", \"SDMVSTRA\",\n                   \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"BMXHT\",\n                   \"BMXBMI\", \"BMXWAIST\",\n                   \"ALQ130\", \"LBXTC\", \"LBDTCSI\", \n                   \"LBXSTR\", \"LBXSUA\", \"LBXSTP\", \"LBXSTB\", \n                   \"LBXSPH\", \"LBXSNASI\", \"LBXSKSI\",\n                   \"LBXSGB\",\"LBXSCA\")\nanalytic.data8[factor.names] <- apply(X = analytic.data8[factor.names], \n                                      MARGIN = 2, FUN = as.factor)\n# analytic.data8[numeric.names] <- apply(X = analytic.data8[numeric.names], \n#                                        MARGIN = 2, FUN = \n#                                          function (x) as.numeric(as.character(x)))\n\n\n\nShow the codeanalytic.data9 <- analytic.data8\nanalytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == \"Don't Know\"] <- NA\n#analytic.data9 <- subset(analytic.data8, DMDBORN4 != \"Don't Know\")\ndim(analytic.data9)\n#> [1] 9254   32\n\nanalytic.data10 <- analytic.data9\nanalytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == \"Don't Know\"] <- NA\n#analytic.data10 <- subset(analytic.data9, DMDEDUC2 != \"Don't Know\")\ndim(analytic.data10)\n#> [1] 9254   32\n\nanalytic.data11 <- analytic.data10\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Don't Know\"] <- NA\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Refused\"] <- NA\n# analytic.data11 <- subset(analytic.data10, DMDMARTL != \"Don't Know\" & DMDMARTL != \"Refused\")\ndim(analytic.data11)\n#> [1] 9254   32\n\n\nanalytic.data12 <- analytic.data11\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Don't Know\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Refused\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Under $20,000\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"$20,000 and Over\"] <- NA\n# analytic.data12 <- subset(analytic.data11, INDHHIN2 != \"Don't know\" & INDHHIN2 !=  \"Refused\" & INDHHIN2 != \"Under $20,000\" & INDHHIN2 != \"$20,000 and Over\" )\ndim(analytic.data12)\n#> [1] 9254   32\n\n#analytic.data11 <- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )\n#dim(analytic.data11) # this are listed as NA anyway\n\nanalytic.data13 <- analytic.data12\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Don't know\"] <- NA\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Refused\"] <- NA\n# analytic.data13 <- subset(analytic.data12, PAQ605 != \"Don't know\" & PAQ605 != \"Refused\")\ndim(analytic.data13)\n#> [1] 9254   32\n\nanalytic.data14 <- analytic.data13\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Don't know\"] <- NA\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Refused\"] <- NA\n# analytic.data14 <- subset(analytic.data13, PAQ650 != \"Don't Know\" & PAQ650 != \"Refused\")\ndim(analytic.data14)\n#> [1] 9254   32\n\nanalytic.data15 <- analytic.data14\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Don't know\"] <- NA\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Refused\"] <- NA\n# analytic.data15 <- subset(analytic.data14, DIQ010 != \"Don't Know\" & DIQ010 != \"Refused\")\ndim(analytic.data15)\n#> [1] 9254   32\n\n\n# analytic.data15$ALQ130[analytic.data15$ALQ130 > 100] <- NA\n# summary(analytic.data15$ALQ130)\ntable(analytic.data15$SMQ040,useNA = \"always\")\n#> \n#>  Every day Not at all  Some days       <NA> \n#>        805       1338        216       6895\ntable(analytic.data15$PAQ605,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4461 1389 3404\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\n\n\nRecode values\nLet us recode the variables using the recode function:\n\nShow the coderequire(car)\nanalytic.data15$RIDRETH3 <- recode(analytic.data15$RIDRETH3, \n                            \"c('Mexican American','Other Hispanic')='Hispanic'; \n                            'Non-Hispanic White'='White'; \n                            'Non-Hispanic Black'='Black';\n                            c('Non-Hispanic Asian',\n                               'Other Race - Including Multi-Rac')='Other';\n                               else=NA\")\nanalytic.data15$DMDEDUC2 <- recode(analytic.data15$DMDEDUC2, \n                            \"c('Some college or AA degree',\n                             'College graduate or above')='College'; \n                            c('9-11th grade (Includes 12th grad', \n                              'High school graduate/GED or equi')\n                               ='High.School'; \n                            'Less than 9th grade'='School';\n                               else=NA\")\nanalytic.data15$DMDMARTL <- recode(analytic.data15$DMDMARTL, \n                            \"c('Divorced','Separated','Widowed')\n                                ='Previously.married'; \n                            c('Living with partner', 'Married')\n                                ='Married'; \n                            'Never married'='Never.married';\n                               else=NA\")\nanalytic.data15$INDHHIN2 <- recode(analytic.data15$INDHHIN2, \n                            \"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999', \n                                 '$10,000 to $14,999', '$15,000 to $19,999', \n                                 '$20,000 to $24,999')='<25k';\n                            c('$25,000 to $34,999', '$35,000 to $44,999', \n                                 '$45,000 to $54,999') = 'Between.25kto54k';\n                            c('$55,000 to $64,999', '$65,000 to $74,999',\n                                 '$75,000 to $99,999')='Between.55kto99k';\n                            '$100,000 and Over'= 'Over100k';\n                               else=NA\")\nanalytic.data15$SMQ040 <- recode(analytic.data15$SMQ040, \n                            \"'Every day'='Every.day';\n                            'Not at all'='Not.at.all';\n                            'Some days'='Some.days';\n                               else=NA\")\nanalytic.data15$DIQ010 <- recode(analytic.data15$DIQ010, \n                            \"'No'='No';\n                            c('Yes', 'Borderline')='Yes';\n                               else=NA\")\n\n\n\n\nData types for various variables are set correctly; for instance, factor variables are converted to factor data types, and numeric variables to numeric data types.\nCheck missingness\n\n\n\n\n\n\nTip\n\n\n\nWe can use the plot_missing function to plot the profile of missing values, e.g., the percentage of missing per variable\n\n\n\nShow the coderequire(DataExplorer)\nplot_missing(analytic.data15)\n\n\n\n\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nCheck data summaries\n\nShow the codenames(analytic.data15)\n#>  [1] \"SEQN\"     \"RIAGENDR\" \"RIDAGEYR\" \"DMDBORN4\" \"RIDRETH3\" \"DMDEDUC2\"\n#>  [7] \"DMDMARTL\" \"INDHHIN2\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BPXDI1\"  \n#> [13] \"BPXSY1\"   \"BMXWT\"    \"BMXHT\"    \"BMXBMI\"   \"BMXWAIST\" \"SMQ040\"  \n#> [19] \"LBXTC\"    \"LBDTCSI\"  \"LBXSTR\"   \"LBXSUA\"   \"LBXSTP\"   \"LBXSTB\"  \n#> [25] \"LBXSPH\"   \"LBXSNASI\" \"LBXSKSI\"  \"LBXSGB\"   \"LBXSCA\"   \"PAQ605\"  \n#> [31] \"PAQ650\"   \"DIQ010\"\nnames(analytic.data15) <- c(\"ID\", \"gender\", \"age\", \"born\", \"race\", \"education\", \n\"married\", \"income\", \"weight\", \"psu\", \"strata\", \"diastolicBP\", \n\"systolicBP\", \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \n\"cholesterol\", \"cholesterolM2\", \"triglycerides\", \n\"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \n\"potassium\", \"globulin\", \"calcium\", \"physical.work\", \n\"physical.recreational\",\"diabetes\")\nrequire(\"tableone\")\nCreateTableOne(data = analytic.data15, includeNA = TRUE)\n#>                                      \n#>                                       Overall            \n#>   n                                       9254           \n#>   ID (mean (SD))                      98329.50 (2671.54) \n#>   gender = Male (%)                       4557 (49.2)    \n#>   age (mean (SD))                        51.50 (17.81)   \n#>   born (%)                                               \n#>      Born in 50 US states or Washingt     7303 (78.9)    \n#>      Others                               1948 (21.1)    \n#>      Refused                                 2 ( 0.0)    \n#>      NA                                      1 ( 0.0)    \n#>   race (%)                                               \n#>      Black                                2115 (22.9)    \n#>      Hispanic                             2187 (23.6)    \n#>      Other                                1802 (19.5)    \n#>      White                                3150 (34.0)    \n#>   education (%)                                          \n#>      College                              3114 (33.7)    \n#>      High.School                          1963 (21.2)    \n#>      School                                479 ( 5.2)    \n#>      NA                                   3698 (40.0)    \n#>   married (%)                                            \n#>      Married                              3252 (35.1)    \n#>      Never.married                        1006 (10.9)    \n#>      Previously.married                   1305 (14.1)    \n#>      NA                                   3691 (39.9)    \n#>   income (%)                                             \n#>      <25k                                 1998 (21.6)    \n#>      Between.25kto54k                     2460 (26.6)    \n#>      Between.55kto99k                     1843 (19.9)    \n#>      Over100k                             1624 (17.5)    \n#>      NA                                   1329 (14.4)    \n#>   weight (mean (SD))                  34670.71 (43344.00)\n#>   psu (mean (SD))                         1.52 (0.50)    \n#>   strata (mean (SD))                    140.97 (4.20)    \n#>   diastolicBP (mean (SD))                67.84 (16.36)   \n#>   systolicBP (mean (SD))                121.33 (19.98)   \n#>   bodyweight (mean (SD))                 65.14 (32.89)   \n#>   bodyheight (mean (SD))                156.59 (22.26)   \n#>   bmi (mean (SD))                        26.58 (8.26)    \n#>   waist (mean (SD))                      89.93 (22.81)   \n#>   smoke (%)                                              \n#>      Every.day                             805 ( 8.7)    \n#>      Not.at.all                           1338 (14.5)    \n#>      Some.days                             216 ( 2.3)    \n#>      NA                                   6895 (74.5)    \n#>   cholesterol (mean (SD))               179.89 (40.60)   \n#>   cholesterolM2 (mean (SD))               4.65 (1.05)    \n#>   triglycerides (mean (SD))             137.44 (109.13)  \n#>   uric.acid (mean (SD))                   5.40 (1.48)    \n#>   protein (mean (SD))                     7.17 (0.44)    \n#>   bilirubin (mean (SD))                   0.46 (0.28)    \n#>   phosphorus (mean (SD))                  3.66 (0.59)    \n#>   sodium (mean (SD))                    140.32 (2.75)    \n#>   potassium (mean (SD))                   4.09 (0.36)    \n#>   globulin (mean (SD))                    3.09 (0.43)    \n#>   calcium (mean (SD))                     9.32 (0.37)    \n#>   physical.work (%)                                      \n#>      No                                   4461 (48.2)    \n#>      Yes                                  1389 (15.0)    \n#>      NA                                   3404 (36.8)    \n#>   physical.recreational (%)                              \n#>      No                                   4422 (47.8)    \n#>      Yes                                  1434 (15.5)    \n#>      NA                                   3398 (36.7)    \n#>   diabetes (%)                                           \n#>      No                                   7816 (84.5)    \n#>      Yes                                  1077 (11.6)    \n#>      NA                                    361 ( 3.9)\n\n\nCreate complete case data (for now)\n\nShow the codeanalytic.with.miss <- analytic.data15\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, 1,0)\nanalytic <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic)\n#> [1] 1562   33\n\n\nCreating Table 1 from the complete case data\n\nShow the coderequire(\"tableone\")\nCreateTableOne(data = analytic, includeNA = TRUE)\n#>                                  \n#>                                   Overall            \n#>   n                                   1562           \n#>   ID (mean (SD))                  98344.21 (2697.76) \n#>   gender = Male (%)                    959 (61.4)    \n#>   age (mean (SD))                    53.18 (17.18)   \n#>   born = Others (%)                    299 (19.1)    \n#>   race (%)                                           \n#>      Black                             324 (20.7)    \n#>      Hispanic                          284 (18.2)    \n#>      Other                             228 (14.6)    \n#>      White                             726 (46.5)    \n#>   education (%)                                      \n#>      College                           806 (51.6)    \n#>      High.School                       658 (42.1)    \n#>      School                             98 ( 6.3)    \n#>   married (%)                                        \n#>      Married                           921 (59.0)    \n#>      Never.married                     228 (14.6)    \n#>      Previously.married                413 (26.4)    \n#>   income (%)                                         \n#>      <25k                              484 (31.0)    \n#>      Between.25kto54k                  520 (33.3)    \n#>      Between.55kto99k                  331 (21.2)    \n#>      Over100k                          227 (14.5)    \n#>   weight (mean (SD))              48538.53 (54106.24)\n#>   psu (mean (SD))                     1.48 (0.50)    \n#>   strata (mean (SD))                141.18 (4.07)    \n#>   diastolicBP (mean (SD))            72.06 (14.17)   \n#>   systolicBP (mean (SD))            127.06 (19.11)   \n#>   bodyweight (mean (SD))             85.66 (22.41)   \n#>   bodyheight (mean (SD))            168.96 (9.30)    \n#>   bmi (mean (SD))                    29.96 (7.33)    \n#>   waist (mean (SD))                 102.98 (17.15)   \n#>   smoke (%)                                          \n#>      Every.day                         530 (33.9)    \n#>      Not.at.all                        903 (57.8)    \n#>      Some.days                         129 ( 8.3)    \n#>   cholesterol (mean (SD))           188.77 (43.51)   \n#>   cholesterolM2 (mean (SD))           4.88 (1.13)    \n#>   triglycerides (mean (SD))         154.71 (123.00)  \n#>   uric.acid (mean (SD))               5.62 (1.53)    \n#>   protein (mean (SD))                 7.09 (0.43)    \n#>   bilirubin (mean (SD))               0.46 (0.27)    \n#>   phosphorus (mean (SD))              3.53 (0.54)    \n#>   sodium (mean (SD))                140.14 (2.83)    \n#>   potassium (mean (SD))               4.10 (0.38)    \n#>   globulin (mean (SD))                3.03 (0.44)    \n#>   calcium (mean (SD))                 9.29 (0.37)    \n#>   physical.work = Yes (%)              476 (30.5)    \n#>   physical.recreational = Yes (%)      290 (18.6)    \n#>   diabetes = Yes (%)                   330 (21.1)    \n#>   cholesterol.bin (mean (SD))         0.63 (0.48)\n\n\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\nSaving data\n\nShow the code# getwd()\nsave(analytic.with.miss, analytic, file=\"Data/researchquestion/NHANES17.RData\")\n\n\nReferences\n\n\n\n\n\n\nPeters, Junenette L, M Patricia Fabian, and Jonathan I Levy. 2014. “Combined Impact of Lead, Cadmium, Polychlorinated Biphenyls and Non-Chemical Risk Factors on Blood Pressure in NHANES.” Environmental Research 132: 93–99."
  },
  {
    "objectID": "researchquestionF.html",
    "href": "researchquestionF.html",
    "title": "R functions (Q)",
    "section": "",
    "text": "The list of new R functions introduced in this Research question lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.data.frame \n    base \n    To force an object to a data frame \n  \n\n as.formula \n    base/stats \n    To specify a model formula, e.g., formula for an outcome model \n  \n\n confint \n    base/stats \n    To estimate the confidence interval for model parameters \n  \n\n degf \n    survey \n    To see the degrees of freedom for a survey design object \n  \n\n describe \n    DescTools \n    To see the summary statistics of variables \n  \n\n exp \n    base \n    Exponentials \n  \n\n lapply \n    base \n    To apply a function over a list, e.g., to see the summary of a list of variables or to convert a list of categorical variables to factor variables. A similar function is `sapply`. lapply and sapply have the same functionality. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list. \n  \n\n length \n    base \n    To see the length of an object, e.g., number of elements/observations of a variable \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n publish \n    Publish \n    To show/publish regression tables \n  \n\n Reduce \n    base \n    To combine multiple objects, e.g., datasets \n  \n\n round \n    base \n    To round numeric values \n  \n\n saveRDS \n    base \n    To save a single R object. Similarly, readDRS will read an R object \n  \n\n skim \n    skimr \n    To see the summary statistics of variables \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n unique \n    base \n    To see the number of unique elements \n  \n\n weights \n    base/stats \n    To extract model weights, e.g., see the weights from a pre-specified survey design \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "researchquestionQ.html",
    "href": "researchquestionQ.html",
    "title": "Quiz (Q)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "confounding.html#background",
    "href": "confounding.html#background",
    "title": "Causal roles",
    "section": "Background",
    "text": "Background\nThis chapter delves deep into the intricate issues surrounding causal associations, particularly confounding, mediation, and other related biases. In this comprehensive series of tutorials, various aspects of confounding and bias are explored through the lens of Directed Acyclic Graphs (DAGs). Initially, the tutorials guide you through the process of generating large datasets based on these DAGs. They then delve into how the inclusion of different types of variables in adjustment models can skew estimates of treatment effects. We use the R package simcausal in these tutorials to derive empirical estimates from a large dataset.\n\n\nIn the preceding chapter, we delved into the various types of research questions, distinguishing between causal and predictive inquiries. This current chapter focuses primarily on the challenges and intricacies associated with causal questions. Specifically, we will explore which types of variables are most appropriate to incorporate into adjustment models when aiming to estimate treatment effects accurately. In contrast, the subsequent chapter will shift its emphasis towards predictive questions, providing insights into their unique characteristics and considerations.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nWe use the R package simcausal in these tutorials to derive empirical estimates from a large simulated dataset. The simulation is based on data generation based on specified DAGs.\n\n\n\n\nDirected Acyclic Graphs (DAGs) are powerful tools in the realm of causal inference. They are a type of graphical representation used to depict causal relationships between variables. This visual representation of the causal structure among variables makes it easier to understand and communicate complex causal relationships. They provide a visual framework to understand, represent, and analyze complex causal relationships, ensuring that researchers make informed decisions when trying to answer causal questions."
  },
  {
    "objectID": "confounding.html#overview-of-tutorials",
    "href": "confounding.html#overview-of-tutorials",
    "title": "Causal roles",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nConfounding\nThe first tutorial provides a thorough exploration of confounding, with a particular focus on its impact on treatment effect estimates in large datasets. It emphasizes the importance of properly adjusting for confounders to arrive at accurate estimates.\n\n\nMediator\nThis tutorial focuses on the role of mediator variables in estimating treatment effects. It assesses how adjusting for the mediator influences the estimated treatment effect, exploring both scenarios where the true treatment effect is either non-null or null.\n\n\nCollider\nThis tutorial serves as a practical guide for understanding how the inclusion of colliders can affect the estimation of treatment effects in causal models.\n\n\nZ-bias\nThis tutorial explores the concept of Z-bias, a phenomenon that can lead to misleading estimates of treatment effects in observational studies. It demonstrates how failing to properly adjust or not adjust for instrumental variables can result in biased estimates and compares these with the true treatment effect.\n\n\nCollapsibility\nThis tutorial provides a detailed guide on calculating marginal probabilities and measures of association, including Risk Difference (RD), Risk Ratio (RR), and Odds Ratio (OR). It examines the impact of adjusting for various covariates on these measures, highlighting the concept of “collapsibility.”\n\n\nChange-in-estimate\nThis tutorial focuses on the “Change-in-estimate” concept to understand the impact of various variables on measures of effect. For both continuous and binary outcomes, the tutorial reveals that adding a confounder to the model alters the true treatment effect estimate. Conversely, including a variable that is not a confounder but is a pure risk factor can either change or not change the effect estimate, depending on the type of outcome involved. This nuanced approach aids in understanding how different roles of variables can influence results and interpretations in causal inference.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "confounding1.html",
    "href": "confounding1.html",
    "title": "Confounding",
    "section": "",
    "text": "This tutorial aims to delve into the role of confounding variables in data analysis, especially in the context of big data. We will examine each of these using simulations built on Directed Acyclic Graphs (DAGs). The objective is to understand whether a simple regression adjusting for the confounder variable can correctly estimate treatment effects in such a large sample.\n\nShow the code# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n\n\nBig data: What if we had 1,000,000 (one million) observations? Would that give us true result? Let’s try to answer that using DAGs.\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\nTo perform the lab, we’ll need the simcausal R package. This package may not be available on CRAN but can be installed from the author’s GitHub page.\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        1.75\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         0.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, our true treatment effect is 1.3. When we estimate the relationship between A and Y without adjusting for L, we obtain an estimated effect of 1.75. However, this is not the true effect. The true treatment effect of 1.3 is recovered when we adjust for L.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        0.45\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         0.0         0.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this second scenario, the true treatment effect is zero. There is no arrow from A to Y in the DAG, but L remains a common cause for both. Upon analyzing the data without adjusting for L, we observe an induced correlation between A and Y. This correlation disappears, confirming the true null effect, when we adjust for L.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confounding2.html",
    "href": "confounding2.html",
    "title": "Mediator",
    "section": "",
    "text": "Mediators play a crucial role in understanding how a treatment variable affects an outcome. A mediator variable lies in the pathway between the treatment and outcome, essentially transmitting or explaining the effect of the treatment variable. In this expanded tutorial, we’ll delve into more details based on the lecture, specifically focusing on the true direct and indirect effects when a mediator is present.\n\nShow the code# Load required packages\nlibrary(simcausal)\n\n\nLet us consider\n\nM is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nOur true treatment effect is 1.3, and the mediator variable’s effect on the outcome Y is 0.5. It’s important to differentiate between these effects.\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        1.69\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou might notice a total effect that could differ from the true effects. In the lecture example, a crude association showed an effect of 1.69, which is the total effect combining both direct and indirect pathways.\nUpon adjusting for M, the coefficients will show you the direct effect of A on Y and the indirect effect through M. These should align closely with our true effects: a direct effect of 1.3 and an indirect effect of 0.5.\nIn this expanded tutorial, we’ve shown how essential it is to consider mediator variables when estimating treatment effects. We’ve also illustrated how adjusting for mediators allows you to differentiate between true direct and indirect effects, thereby reducing the risk of drawing incorrect conclusions from your data.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        0.39\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         0.0         0.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTotal Effect: If you want to measure the “total effect” of a treatment on an outcome, then you typically don’t adjust for the mediator. The reason is that the total effect captures both the direct effect of the treatment on the outcome and the indirect effect through the mediator.\nDirect and Indirect Effects: If you want to separate out the direct and indirect effects, then you would adjust for the mediator. In essence, when you control for the mediator, what remains is the direct effect of the treatment on the outcome.\nLinearity and Decomposition: In linear models with continuous outcomes, it is more straightforward to decompose total effects into direct and indirect effects. The mathematics get more complicated in non-linear models or when dealing with non-continuous outcomes."
  },
  {
    "objectID": "confounding3.html",
    "href": "confounding3.html",
    "title": "Collider",
    "section": "",
    "text": "In causal inference, understanding the role of colliders is crucial. A collider is a variable that is a common effect of two or more variables. Adjusting for a collider can introduce bias into your estimates.\n\nShow the code# Load required packages\nlibrary(simcausal)\n\n\nIn a DAG, a collider is a variable influenced by two or more other variables. In our case, L is a collider because it is affected by both A (the treatment) and Y (the outcome). When you adjust for a collider like L, you could introduce bias into your estimates, as demonstrated in the examples below.\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 1.3 * A, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00        1.29\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00        0.58        0.05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen not adjusting for L, we recover the true effect close to 1.3. Adjusting for L introduces bias, making the estimate unreliable.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00       -0.01\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00       -0.07        0.05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen the true effect is null, not adjusting for L shows an estimate close to zero. Adjusting for L moves the estimate away from the null value, introducing bias.\n\n\nEven 1,000,000 observations were not enough to recover true treatment effect! But we are close enough."
  },
  {
    "objectID": "confounding4.html",
    "href": "confounding4.html",
    "title": "Z-bias",
    "section": "",
    "text": "Z-bias occurs in the context of causal inference, specifically when using instrumental variables to estimate causal effects. Instrumental variables (IVs) are used to isolate the variation in the treatment variable that is unrelated to the confounding factors, thus providing a pathway to estimate causal effects.\n\nShow the code# Load required packages\nlibrary(simcausal)\n\n\nContinuous Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"age\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"gender\", distr = \"rbern\", prob = plogis(0.25)) +\n  node(\"education\", distr = \"rbern\", prob = plogis(3 + 5* age)) +\n  node(\"diet\", distr = \"rbern\", prob = plogis(13 + 7 * education)) +\n  node(\"income\", distr = \"rbern\", prob = plogis(2 + 1.4 * education + 2 * age)) +\n  node(\"smoking\", distr = \"rbern\", prob = plogis(1 + 1.2 * gender + 2 * age)) +\n  node(\"hypertension\", distr = \"rnorm\", mean = 3 * diet + 1.3 * age + 2 * smoking + 0.5 * gender, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the codeObs.Data$income <- as.factor(Obs.Data$income)\n# True data generating mechanism \n# (unattainable as U is unmeasured)\nfit0 <- glm(hypertension ~ diet + age + smoking + gender, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)        diet         age     smoking      gender \n#>  -2024169.9   2024172.9         1.3         2.0         0.5\n\nrequire(Publish)\nfit1 <- glm(hypertension ~ diet + age + smoking*income + gender, family=\"gaussian\", data=Obs.Data)\npublish(fit1)\n#>            Variable Units Coefficient                     CI.95  p-value \n#>         (Intercept)       -2024807.39 [-13733931.24;9684316.46]   0.7347 \n#>                diet        2024810.39 [-9684313.46;13733934.24]   0.7347 \n#>                 age              1.30               [1.30;1.30]   <1e-04 \n#>              gender              0.50               [0.50;0.50]   <1e-04 \n#>  smoking: income(0)              2.00               [1.99;2.00]   <1e-04 \n#>  smoking: income(1)              2.00               [2.00;2.00]   <1e-04\n\n\nBinary Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is binary outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rbern\", prob = plogis(-1 + 3 * U + 1.3 * A))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# True data generating mechanism (unattainable as U is unmeasured)\nfit0 <- glm(Y ~ A + U, family=\"binomial\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A           U \n#>       -0.99        1.30        3.01\n\n# Unadjusted effect (Z not controlled)\nfit1 <- glm(Y ~ A, family=\"binomial\", data=Obs.Data)\nround(coef(fit1),2)\n#> (Intercept)           A \n#>        0.40        3.02\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#>        A \n#> 1.716482\n\n# Adjusted effect (Z  controlled)\nfit2 <- glm(Y ~ A + Z, family=\"binomial\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           Z \n#>        0.51        3.29       -0.18\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#>        A \n#> 1.991396"
  },
  {
    "objectID": "confounding5.html",
    "href": "confounding5.html",
    "title": "Collapsibility",
    "section": "",
    "text": "Explanation of collapsibility property of an estimate (RD, RR and OR: conditional or marginal) in absence of confounding\n\nShow the code# Load required packages\nlibrary(simcausal)\nlibrary(tableone)\nlibrary(Publish)\nlibrary(lawstat)\n\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"gender\", distr = \"rbern\", \n       prob = 0.7) +\n  node(\"age\", distr = \"rnorm\", \n       mean = 2, sd = 4) +\n  node(\"smoking\", distr = \"rbern\", \n       prob = plogis(.1)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3.5) * smoking \n                     + log(.1) * gender  \n                       + log(7) * age))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the codeObs.Data <- sim(DAG = Dset, n = 100000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nBalance check\n\nShow the coderequire(tableone)\nCreateTableOne(data = Obs.Data, \n               strata = \"smoking\", \n               vars = c(\"gender\", \"age\"))\n#>                     Stratified by smoking\n#>                      0            1            p      test\n#>   n                  47720        52280                   \n#>   gender (mean (SD))  0.70 (0.46)  0.70 (0.46)  0.403     \n#>   age (mean (SD))     2.02 (4.02)  2.01 (4.00)  0.690\n\n\nConditional and crude RD\nFull list of risk factors for outcome (2 variables)\n\nShow the code## RD\nrequire(Publish)\nfitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator …robust variance estimator (or bootstrap) should be used to obtain valid standard errors.”)\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"],\n             coef(fitx3)[\"smoking\"],\n             coef(fitx4)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nConditional and crude RR\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk ratio, one may use a GLM with a Poisson distribution and log link function …. one should use the robust (or sandwich) variance estimator to obtain valid standard errors (the bootstrap can also be used)”).\n\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 1.156387\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the codefitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.077402\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nConditional and crude OR\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 2.180804\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\",\n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.290429\n\n\nMantel-Haenszel adjusted ORs with 1 variable\n\nShow the codetabx <- xtabs( ~ hypertension + smoking + gender, data = Obs.Data)\nftable(tabx)    \n#>                      gender     0     1\n#> hypertension smoking                   \n#> 0            0               3788 12401\n#>              1               3400 11504\n#> 1            0              10547 20984\n#>              1              12178 25198\n# require(samplesizeCMH)\n# apply(tabx, 3, odds.ratio)\n\nlibrary(lawstat)\ncmh.test(tabx)\n#> \n#>  Cochran-Mantel-Haenszel Chi-square Test\n#> \n#> data:  tabx\n#> CMH statistic = NA, df = 1.0000, p-value = NA, MH Estimate = 1.2924,\n#> Pooled Odd Ratio = 1.2876, Odd Ratio of level 1 = 1.2864, Odd Ratio of\n#> level 2 = 1.2945\n# mantelhaen.test(tabx, exact = TRUE)\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nMarginal RD, RR and OR\nBelow we show a procedure for calculating marginal probabilities \\(p_1\\) (for treated) and \\(p_0\\) (for untreated).\nAdjustment of 2 variables\n\nShow the codefitx3 <- glm(hypertension ~ smoking + gender + age, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx3)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  3.37 \n#> RR (ZY)=  1.08\n\n\nAdjustment of 1 variable\n\nShow the codefitx2 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx2)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nNo adjustment\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx1)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nBootstrap could be used to estimate confidence intervals.\nRef:\n\n\n(Kleinman and Norton 2009) (“this paper demonstrates how to move from a nonlinear model to estimates of marginal effects that are quantified as the adjusted risk ratio or adjusted risk difference”)\n\n(Austin 2010) (“clinically meaningful measures of treatment effect using logistic regression model”)\n\n(Luijken et al. 2022) (“marginal odds ratio”)\n\n(Muller and MacLehose 2014) (“marginal standardization”)\n\n(Greenland 2004) (“standardized / population-averaged”)\n\n(Bieler et al. 2010) (“standardized /population-averaged risk from the logistic model”)\nSummary\nHere are the summary of the results based on a scenario where confounding was absent:\n\n\n\n\n\n\n\n\nModelling strategy\nRD (conditional)\nRR (conditional)\nOR (conditional)\n\n\n\nage + gender in regression\n0.06 [0.05;0.06]\n1.08 [1.08;1.09]\n3.37 [3.17;3.58]\n\n\nstratified by age and gender (mean)\n0.05 (0.11, 0.1,0.01,0)\n1.16 (1.41, 1.21, 1.01,1)\n2.18 (unweighted; 1.65, 1.49, 3.45, 2.14)\n\n\ngender in regression\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.26;1.33]\n\n\nstratified by gender (mean)\n0.05 (0.6,0.5)\n1.08 (1.09, 1.06)\n1.29 (1.29, 1.29; M-H 1.29)\n\n\nMarginal estimates\n\n\n\n\n\ncrude\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.25;1.32]\n\n\nBased on marginal probabilities (any variable combination)\n0.05\n1.08\n1.29\n\n\n\nLet us assume we have a regression of hypertension (\\(Y\\)), smoking (\\(A\\)) and a risk factor for outcome, gender (\\(L\\)). Then let us set up 2 regression models:\n\n1st regression model is \\(Y \\sim \\beta \\times A + \\alpha \\times L\\). Here we are conditioning on gender (\\(L\\)).\n2nd regression model is \\(Y \\sim \\beta' \\times A\\)\n\n\nThen regression is collapsible for \\(\\beta\\) over \\(L\\) if \\(\\beta = \\beta'\\) from the 2nd regression omitting \\(L\\). \\(\\beta \\ne \\beta'\\) would mean non-collapsibility. A measure of association (say, risk difference) is collapsible if the marginal measure of association is equal to a weighted average of the stratum-specific measures of association. Non-collapsibility is also knows as Simpson’s Paradox (in absence of confoinding of course): a statistical phenomenon where an association between two factors (say, hypertension and smoking) in a population (we are talking about marginal estimate here) is different than the associations of same relationship in subpopulations (conditional on some other factor, say, age; hence talking about conditional estimates).\nOdds ratio can be non-collapsible. It can produce different treatment effect estimate for different covariate adjustment sets (see our above example of when adjusting form age and sex vs. when adjusting none). This is true even in the absence of confounding. However, according to our definition here, OR is collapsible when we consider gender in the adjustment set.\nNote that, OR non-collapsibility is a consequence of the fact that it is estimated via a logit link function (nonlinearity of the logistic transformation).\nRef:\n\n(Greenland, Pearl, and Robins 1999)\n(Mansournia and Greenland 2015)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nAustin, Peter C. 2010. “Absolute Risk Reductions, Relative Risks, Relative Risk Reductions, and Numbers Needed to Treat Can Be Obtained from a Logistic Regression Model.” Journal of Clinical Epidemiology 63 (1): 2–6.\n\n\nBieler, Gayle S, G Gordon Brown, Rick L Williams, and Donna J Brogan. 2010. “Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios from Complex Survey Data.” American Journal of Epidemiology 171 (5): 618–23.\n\n\nGreenland, Sander. 2004. “Model-Based Estimation of Relative Risks and Other Epidemiologic Measures in Studies of Common Outcomes and in Case-Control Studies.” American Journal of Epidemiology 160 (4): 301–5.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Confounding and Collapsibility in Causal Inference.” Statistical Science 14 (1): 29–46.\n\n\nKleinman, Lawrence C, and Edward C Norton. 2009. “What’s the Risk? A Simple Approach for Estimating Adjusted Risk Measures from Nonlinear Models Including Logistic Regression.” Health Services Research 44 (1): 288–302.\n\n\nLuijken, Kim, Rolf HH Groenwold, Maarten van Smeden, Susanne Strohmaier, and Georg Heinze. 2022. “A Comparison of Full Model Specification and Backward Elimination of Potential Confounders When Estimating Marginal and Conditional Causal Effects on Binary Outcomes from Observational Data.” Biometrical Journal.\n\n\nMansournia, Mohammad Ali, and Sander Greenland. 2015. “The Relation of Collapsibility and Confounding to Faithfulness and Stability.” Epidemiology 26 (4): 466–72.\n\n\nMuller, Clemma J, and Richard F MacLehose. 2014. “Estimating Predicted Probabilities from Logistic Regression: Different Methods Correspond to Different Target Populations.” International Journal of Epidemiology 43 (3): 962–70.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is a confounder",
    "text": "Adjusting for a variable that is a confounder\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.85\n\nfit2 <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis( 1.1 * L + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node L, order:1\n#> node A, order:2\n#> node P, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.68\n\nfit2 <- glm(Y ~ A + L, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (simplified)",
    "text": "Adjusting for a variable that is not a confounder (simplified)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>         0.0         1.3\n\nfit2 <- glm(Y ~ A + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.06\n\nfit2 <- glm(Y ~ A + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (Complex)",
    "text": "Adjusting for a variable that is not a confounder (Complex)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\nfit2 <- glm(Y ~ A + M + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>         0.0         1.3         0.5         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>        0.00        1.06        0.41\n\nfit2 <- glm(Y ~ A + M + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>        0.00        1.29        0.50        1.10\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confoundingF.html",
    "href": "confoundingF.html",
    "title": "R functions (R)",
    "section": "",
    "text": "The list of new R functions introduced in this Confounding and bias lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n cmh.test \n    lawstat \n    To conduct the Mantel-Haenszel Chi-square test \n  \n\n DAG.empty \n    simcausal \n    To initialize an empty DAG \n  \n\n ftable \n    base/stats \n    To create a flat contingency table \n  \n\n plotDAG \n    simcausal \n    To visualize a DAG \n  \n\n set.DAG \n    simcausal \n    To create a DAG \n  \n\n sim \n    simcausal \n    To simulate data using a DAG"
  },
  {
    "objectID": "confoundingQ.html",
    "href": "confoundingQ.html",
    "title": "Quiz (R)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "predictivefactors.html#background",
    "href": "predictivefactors.html#background",
    "title": "Prediction ideas",
    "section": "Background",
    "text": "Background\nThe chapter provides a comprehensive guide to prediction modeling for cholesterol levels, focusing on the challenges and solutions involved in building robust prediction models. It begins by addressing the issue of collinearity among predictors and progresses to cover the intricacies of modeling both continuous and binary outcomes. Special attention is given to diagnosing and preventing model overfitting through various techniques such as data splitting and cross-validation. Advanced topics in model validation like bootstrapping are also explored. The overarching theme is to equip data analysts with the tools and methods needed to build, assess, and improve predictive models while addressing common challenges like collinearity and overfitting.\n\n\nAs we’ve journeyed through the previous chapters, we’ve gained a comprehensive understanding of various research questions, particularly distinguishing between causal and predictive inquiries. While the prior chapter delved into the intricacies of causal questions and the challenges they present, this chapter shifts the spotlight to the realm of prediction. Predictive questions have their own set of complexities and methodologies, distinct from those of causal inquiries. Here, we’ll explore the art and science of making accurate predictions, understanding the factors that influence them, and the tools and techniques best suited for predictive analysis.\nFurthermore, this chapter serves as a precursor to our upcoming exploration of machine learning. While prediction provides the foundation, machine learning offers advanced tools and algorithms to refine and enhance our predictive capabilities. By building on the foundational knowledge from the preceding chapters and setting the stage for the machine learning chapter, we aim to provide a holistic view of how prediction and machine learning intertwine in the broader landscape of research inquiry.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "predictivefactors.html#overview-of-tutorials",
    "href": "predictivefactors.html#overview-of-tutorials",
    "title": "Prediction ideas",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nIdentify collinear predictors\nThis tutorial focuses on identifying collinear predictors in a dataset related to cholesterol levels from the NHANES 2015 collection. The tutorial guides you through summarizing its structure, and applying methods for variable clustering to detect collinear predictors. The tutorial is practical for data analysts aiming to improve model accuracy by identifying and addressing redundant variables.\n\n\nExplore relationships for continuous outcome variable\nThis comprehensive tutorial walks you through the process of analyzing a dataset on cholesterol levels, focusing on exploring relationships for a continuous outcome variable. It starts by generating a correlation plot. Multiple methods for examining descriptive associations are provided, including stratification by key predictors. The tutorial also covers linear regression modeling, diagnosing data issues like outliers and leverage, and refitting the model after cleaning the data. Additionally, the tutorial delves into more complex modeling techniques like polynomial regression and multiple covariates, and addresses issues of collinearity using Variance Inflation Factors (VIF).\n\n\nExplore relationships for binary outcome variable\nA binary outcome variable is created to classify cholesterol levels as ‘healthy’ or ‘unhealthy’. This transformed variable is then modeled using logistic regression. Various predictors including demographic variables, vital statistics, and other health parameters are considered in the model. The performance of the model is evaluated using Variance Inflation Factor (VIF) for multicollinearity and Area Under the Curve (AUC) for classification accuracy. Two models are fitted, and their respective AUCs are calculated to assess the predictive power.\n\n\nOverfitting and performance\nThe tutorial focus is on addressing overfitting and assessing model performance. A linear regression model is fitted using a comprehensive set of predictors. Various statistical metrics such as the design matrix dimensions, Sum of Squares for Error (SSE), Total Sum of Squares (SST), R-squared (R2), Root Mean Square Error (RMSE), and Adjusted R2 are calculated to evaluate the model’s predictive power and fit. Functions are also created to streamline the calculation of these metrics, allowing for more dynamic and customizable performance assessment. One such function, perform, encapsulates the entire process, outputting key performance indicators including R2, adjusted R2, and RMSE, and it can be applied to new data sets for validation.\n\n\nData spliting\nThe tutorial focuses on splitting data into training and testing sets to prevent model overfitting. We allocate approximately 70% of the data to the training set and the remaining 30% to the test set. The linear regression model is then fitted using the training data. Performance metrics are extracted using the previously defined perform function, which is applied not only to the training and test sets but also to the entire dataset for comprehensive performance evaluation. This data splitting approach allows for more robust model validation by assessing how well the model generalizes to unseen data.\n\n\nCross-vaildation\nThe tutorial outlines the process of implementing k-fold cross-validation to validate a linear regression model’s performance, aiming to predict cholesterol levels. The dataset is divided into 5 folds, by turn used as training (to fit the model), and test sets (used for prediction and performance evaluation). Performance metrics such as R-squared are calculated for each fold. The process can also be automated , which helps in fitting the model across all folds and summarizing the results, including calculating the mean and standard deviation of the R-squared values to understand the model’s consistency and reliability.\n\n\nBootstrap\nThe tutorial outlines methods for implementing various bootstrapping techniques in statistical analysis. It demonstrates resampling methods using vectors and matrices. The idea of bootstrapping is emphasized as a useful technique for estimating the standard deviation (SD) of a statistic (e.g., mean), when the distribution of the data is unknown. This SD is then used to calculate confidence intervals. Different variations of bootstrap methods, such as “boot,” “boot632,” and “Optimism corrected bootstrap,” are demonstrated for linear regression and logistic regression models. They are used to obtain performance metrics like R-squared for regression models and the Receiver Operating Characteristic (ROC) curve for classification models. The tutorial also includes an example of calculating the Brier Score. The examples aim to offer various strategies for model evaluation, from the basics of resampling a vector to applying complex methods like ‘Optimism corrected bootstrap’ on real-world data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "predictivefactors1.html",
    "href": "predictivefactors1.html",
    "title": "Collinear predictors",
    "section": "",
    "text": "In this tutorial, we’ll continue with the data analysis. We’ll focus on an analysis of an NHANES data. This data contains over 1200 observations and 33 variables. These variables come in various types: numeric, categorical, and binary. Our primary goal is to fit a linear regression model to predict cholesterol levels.\n\nShow the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\n\n\nLoad data\nLet us load the dataset and see structure of the variables:\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n#head(analytic)\nstr(analytic)\n#> 'data.frame':    1267 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83741 83747 83750 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n#>  $ age                  : num  62 53 22 46 45 30 60 69 24 70 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Others\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"Black\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"College\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Never.married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"Between.25kto54k\" \"<25k\" ...\n#>  $ weight               : num  135630 25282 39353 35674 97002 ...\n#>  $ psu                  : num  1 1 2 1 1 1 1 2 1 2 ...\n#>  $ strata               : num  125 125 128 121 125 124 128 120 130 132 ...\n#>  $ diastolicBP          : num  70 88 70 94 70 50 74 70 72 54 ...\n#>  $ systolicBP           : num  128 146 110 144 116 104 142 146 126 144 ...\n#>  $ bodyweight           : num  94.8 90.4 76.6 86.2 76.2 71.2 75.6 84 89.2 81.7 ...\n#>  $ bodyheight           : num  184 171 165 177 178 ...\n#>  $ bmi                  : num  27.8 30.8 28 27.6 24.1 26.6 35.9 31 26.9 27 ...\n#>  $ waist                : num  101.1 107.9 86.6 104.3 90.1 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Some.days\" \"Every.day\" ...\n#>  $ alcohol              : num  1 6 8 1 3 2 1 1 2 2 ...\n#>  $ cholesterol          : num  173 265 164 242 181 184 205 287 126 192 ...\n#>  $ cholesterolM2        : num  4.47 6.85 4.24 6.26 4.68 4.76 5.3 7.42 3.26 4.97 ...\n#>  $ triglycerides        : num  158 170 77 497 63 62 169 245 95 64 ...\n#>  $ uric.acid            : num  4.2 7 6 6.5 5.4 5.5 5.1 4.3 7.6 7.1 ...\n#>  $ protein              : num  7.5 7.4 7.4 6.8 7.4 6.7 7.4 6.8 7.3 7.2 ...\n#>  $ bilirubin            : num  0.5 0.6 0.2 0.5 0.7 0.8 0.4 0.6 1.2 1.2 ...\n#>  $ phosphorus           : num  4.7 4.4 5.3 3.6 3.9 3.4 3.9 4.4 3.2 3 ...\n#>  $ sodium               : num  136 140 139 138 138 136 139 140 140 139 ...\n#>  $ potassium            : num  4.3 4.55 4.16 4.27 3.91 3.97 3.99 4.25 3.8 4.63 ...\n#>  $ globulin             : num  2.9 2.9 3 2.6 2.8 2.5 3.2 2.3 2.7 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.3 9.3 9.3 9.4 9.6 9.6 9.6 9.6 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"Yes\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:3739] 3 4 5 6 8 9 13 14 15 16 ...\n#>   ..- attr(*, \"names\")= chr [1:3739] \"3\" \"4\" \"5\" \"6\" ...\n\n\nDescribe the data\n\nShow the coderequire(rms)\ndescribe(analytic) \n#> analytic \n#> \n#>  33  Variables      1267  Observations\n#> --------------------------------------------------------------------------------\n#> ID \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1267        1    88660     3366    84250    84687 \n#>      .25      .50      .75      .90      .95 \n#>    86019    88692    91252    92670    93089 \n#> \n#> lowest : 83732 83733 83741 83747 83750, highest: 93617 93633 93643 93659 93685\n#> --------------------------------------------------------------------------------\n#> gender \n#>        n  missing distinct \n#>     1267        0        2 \n#>                         \n#> Value      Female   Male\n#> Frequency     496    771\n#> Proportion  0.391  0.609\n#> --------------------------------------------------------------------------------\n#> age \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       61        1    49.91    19.18       24       27 \n#>      .25      .50      .75      .90      .95 \n#>       36       51       63       72       78 \n#> \n#> lowest : 20 21 22 23 24, highest: 76 77 78 79 80\n#> --------------------------------------------------------------------------------\n#> born \n#>        n  missing distinct \n#>     1267        0        2 \n#>                                                                             \n#> Value      Born in 50 US states or Washingt                           Others\n#> Frequency                               991                              276\n#> Proportion                            0.782                            0.218\n#> --------------------------------------------------------------------------------\n#> race \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                               \n#> Value         Black Hispanic    Other    White\n#> Frequency       246      337      132      552\n#> Proportion    0.194    0.266    0.104    0.436\n#> --------------------------------------------------------------------------------\n#> education \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                               \n#> Value          College High.School      School\n#> Frequency          648         523          96\n#> Proportion       0.511       0.413       0.076\n#> --------------------------------------------------------------------------------\n#> married \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                                                    \n#> Value                 Married      Never.married Previously.married\n#> Frequency                 751                226                290\n#> Proportion              0.593              0.178              0.229\n#> --------------------------------------------------------------------------------\n#> income \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                                                               \n#> Value                  <25k Between.25kto54k Between.55kto99k         Over100k\n#> Frequency               344              435              297              191\n#> Proportion            0.272            0.343            0.234            0.151\n#> --------------------------------------------------------------------------------\n#> weight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1184        1    48904    44337     9158    11549 \n#>      .25      .50      .75      .90      .95 \n#>    19540    30335    63822   121803   151546 \n#> \n#> lowest :   5470.041   5948.955   6197.660   6480.947   6703.837\n#> highest: 203562.855 207197.232 213611.345 218138.797 224891.623\n#> --------------------------------------------------------------------------------\n#> psu \n#>        n  missing distinct     Info     Mean      Gmd \n#>     1267        0        2     0.75    1.493   0.5003 \n#>                       \n#> Value          1     2\n#> Frequency    642   625\n#> Proportion 0.507 0.493\n#> --------------------------------------------------------------------------------\n#> strata \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       15    0.994    126.3    4.792      120      121 \n#>      .25      .50      .75      .90      .95 \n#>      123      126      130      132      133 \n#> \n#> lowest : 119 120 121 122 123, highest: 129 130 131 132 133\n#>                                                                             \n#> Value        119   120   121   122   123   124   125   126   127   128   129\n#> Frequency     47    74   118    63    77    66   114   104   107    65    53\n#> Proportion 0.037 0.058 0.093 0.050 0.061 0.052 0.090 0.082 0.084 0.051 0.042\n#>                                   \n#> Value        130   131   132   133\n#> Frequency     99   120    95    65\n#> Proportion 0.078 0.095 0.075 0.051\n#> --------------------------------------------------------------------------------\n#> diastolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       41    0.997    70.37    13.99       52       54 \n#>      .25      .50      .75      .90      .95 \n#>       62       70       78       86       92 \n#> \n#> lowest :   0  26  34  38  40, highest: 104 106 108 110 112\n#> --------------------------------------------------------------------------------\n#> systolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       56    0.998    126.5     19.3    102.0    106.0 \n#>      .25      .50      .75      .90      .95 \n#>    114.0    124.0    136.0    148.8    160.0 \n#> \n#> lowest :  84  88  90  92  94, highest: 194 196 206 218 236\n#> --------------------------------------------------------------------------------\n#> bodyweight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      615        1    84.95    23.56    56.29    61.10 \n#>      .25      .50      .75      .90      .95 \n#>    69.70    81.40    97.00   113.44   127.47 \n#> \n#> lowest :  39.7  39.8  40.7  42.6  42.7, highest: 161.9 166.3 175.7 175.9 178.4\n#> --------------------------------------------------------------------------------\n#> bodyheight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      376        1    169.2    10.66    153.8    157.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.6    169.3    176.2    181.1    184.2 \n#> \n#> lowest : 143.8 144.2 145.2 145.9 146.2, highest: 194.6 195.1 195.6 198.4 201.0\n#> --------------------------------------------------------------------------------\n#> bmi \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      284        1    29.58    7.403    20.60    22.06 \n#>      .25      .50      .75      .90      .95 \n#>    24.80    28.60    33.30    38.24    42.00 \n#> \n#> lowest : 16.3 17.5 17.6 17.7 17.9, highest: 57.2 57.6 59.4 60.7 64.5\n#> --------------------------------------------------------------------------------\n#> waist \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      544        1    101.8    18.47     77.1     81.4 \n#>      .25      .50      .75      .90      .95 \n#>     90.5    100.3    111.2    122.8    132.5 \n#> \n#> lowest :  65.0  65.5  66.5  68.2  68.7, highest: 159.2 159.8 160.2 160.5 161.5\n#> --------------------------------------------------------------------------------\n#> smoke \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                            \n#> Value       Every.day Not.at.all  Some.days\n#> Frequency         448        665        154\n#> Proportion      0.354      0.525      0.122\n#> --------------------------------------------------------------------------------\n#> alcohol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       14    0.952    3.109    2.419        1        1 \n#>      .25      .50      .75      .90      .95 \n#>        1        2        4        6        8 \n#> \n#> lowest :  1  2  3  4  5, highest: 10 11 12 14 15\n#>                                                                             \n#> Value          1     2     3     4     5     6     7     8     9    10    11\n#> Frequency    336   371   189   106    79    95    10    26     4    20     1\n#> Proportion 0.265 0.293 0.149 0.084 0.062 0.075 0.008 0.021 0.003 0.016 0.001\n#>                             \n#> Value         12    14    15\n#> Frequency     23     1     6\n#> Proportion 0.018 0.001 0.005\n#> --------------------------------------------------------------------------------\n#> cholesterol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    193.1    47.47    132.0    142.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.5    191.0    217.0    248.0    268.0 \n#> \n#> lowest :  81  93  97 100 101, highest: 345 348 349 358 545\n#> --------------------------------------------------------------------------------\n#> cholesterolM2 \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    4.994    1.228    3.410    3.670 \n#>      .25      .50      .75      .90      .95 \n#>    4.205    4.940    5.610    6.410    6.930 \n#> \n#> lowest :  2.09  2.40  2.51  2.59  2.61, highest:  8.92  9.00  9.03  9.26 14.09\n#> --------------------------------------------------------------------------------\n#> triglycerides \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      361        1    165.8    124.1     48.0     59.0 \n#>      .25      .50      .75      .90      .95 \n#>     84.0    127.0    201.5    309.0    396.6 \n#> \n#> lowest :   18   21   24   25   31, highest:  964 1020 1157 1253 3061\n#> --------------------------------------------------------------------------------\n#> uric.acid \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       84        1    5.598    1.626     3.43     3.80 \n#>      .25      .50      .75      .90      .95 \n#>     4.60     5.50     6.50     7.40     8.00 \n#> \n#> lowest :  1.6  2.2  2.3  2.4  2.5, highest: 10.2 10.3 11.7 12.2 18.0\n#> --------------------------------------------------------------------------------\n#> protein \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       32    0.995    7.126   0.5095      6.4      6.6 \n#>      .25      .50      .75      .90      .95 \n#>      6.8      7.1      7.4      7.7      7.9 \n#> \n#> lowest : 5.7 5.8 5.9 6.0 6.1, highest: 8.4 8.5 8.6 8.8 9.0\n#> --------------------------------------------------------------------------------\n#> bilirubin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       31    0.984   0.5467   0.2949      0.2      0.2 \n#>      .25      .50      .75      .90      .95 \n#>      0.4      0.5      0.7      0.9      1.0 \n#> \n#> lowest : 0.00 0.01 0.02 0.03 0.04, highest: 1.80 2.00 2.10 2.60 3.30\n#> --------------------------------------------------------------------------------\n#> phosphorus \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       37    0.996    3.642    0.593      2.8      3.0 \n#>      .25      .50      .75      .90      .95 \n#>      3.3      3.6      4.0      4.3      4.5 \n#> \n#> lowest : 1.8 2.0 2.2 2.3 2.4, highest: 5.2 5.3 5.4 5.6 6.1\n#> --------------------------------------------------------------------------------\n#> sodium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       20    0.977    138.5    2.383      135      136 \n#>      .25      .50      .75      .90      .95 \n#>      137      139      140      141      142 \n#> \n#> lowest : 124 126 129 130 131, highest: 142 143 144 146 148\n#>                                                                             \n#> Value        124   126   129   130   131   132   133   134   135   136   137\n#> Frequency      1     1     1     1     5     4    11    23    46    93   176\n#> Proportion 0.001 0.001 0.001 0.001 0.004 0.003 0.009 0.018 0.036 0.073 0.139\n#>                                                                 \n#> Value        138   139   140   141   142   143   144   146   148\n#> Frequency    235   260   206   112    55    29     6     1     1\n#> Proportion 0.185 0.205 0.163 0.088 0.043 0.023 0.005 0.001 0.001\n#> --------------------------------------------------------------------------------\n#> potassium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      175        1    3.985   0.3725     3.45     3.57 \n#>      .25      .50      .75      .90      .95 \n#>     3.78     3.98     4.19     4.40     4.54 \n#> \n#> lowest : 2.60 2.92 2.96 3.07 3.09, highest: 5.15 5.21 5.36 5.37 5.51\n#> --------------------------------------------------------------------------------\n#> globulin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       29    0.994    2.799   0.4536      2.2      2.3 \n#>      .25      .50      .75      .90      .95 \n#>      2.5      2.8      3.0      3.3      3.5 \n#> \n#> lowest : 1.6 1.8 1.9 2.0 2.1, highest: 4.1 4.2 4.3 4.5 5.5\n#> --------------------------------------------------------------------------------\n#> calcium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       25    0.991    9.335   0.3786      8.8      8.9 \n#>      .25      .50      .75      .90      .95 \n#>      9.1      9.3      9.6      9.7      9.9 \n#> \n#> lowest :  8.4  8.5  8.6  8.7  8.8, highest: 10.4 10.5 10.7 11.0 11.1\n#> --------------------------------------------------------------------------------\n#> physical.work \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency    895   372\n#> Proportion 0.706 0.294\n#> --------------------------------------------------------------------------------\n#> physical.recreational \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency   1002   265\n#> Proportion 0.791 0.209\n#> --------------------------------------------------------------------------------\n#> diabetes \n#>        n  missing distinct \n#>     1267        0        2 \n#>                     \n#> Value        No  Yes\n#> Frequency  1064  203\n#> Proportion 0.84 0.16\n#> --------------------------------------------------------------------------------\n\n\nCollinearity\nAvoiding collinear variables can result in a more interpretable, stable, and efficient predictive model. Collinearity refers to a situation in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with substantial accuracy. Collinearity poses several issues for predictive analysis:\nReduced Interpretability: When predictor variables are highly correlated, it becomes challenging to isolate the impact of individual predictors on the response variable. In other words, it is difficult to determine which predictor is genuinely influential in explaining variance in the response variable. This reduces the interpretability of the model.\nUnstable Coefficients: Collinearity can lead to inflated standard errors of the regression coefficients. This means that the coefficients can be very sensitive to small changes in the data, making the model unstable and less generalizable to new, unseen data.\nOverfitting: When predictors are collinear, the model is more likely to fit to the noise in the data rather than the actual signal. This is a manifestation of overfitting, where the model becomes too complex and captures random noise. Overfit models will perform poorly on new, unseen data.\nInefficiency: Including redundant variables (collinear variables) does not add additional information to the model. This could be inefficient, especially when dealing with large datasets, as it increases computational costs without improving model performance.\nMulticollinearity Diagnostics\nSeveral techniques are available for diagnosing multicollinearity, including:\n\nVariance Inflation Factor (VIF)\nEigenvalues and Eigenvectors of the correlation/covariance matrix\nPairwise correlation matrices\nRemedies\nSome common ways to handle collinearity include:\n\nRemoving one of the correlated predictors\nCombining correlated predictors into a single composite predictor\nUsing regularization techniques like Ridge Regression, which can help handle collinearity by adding a penalty term\nIdentify collinear predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can also use hclust and varclus or variable clustering, i.e., to identify collinear predictors\n\n\n\n\nhclust is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.\n\nShow the coderequire(Hmisc)\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.cluster <- varclus(~., data = analytic[sel.names])\n# var.cluster\nplot(var.cluster)\n\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "predictivefactors2.html",
    "href": "predictivefactors2.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Let us focus on issues related to predictive modeling for continuous outcomes in 4 steps:\n\nDiagnosis Phase: Identifies outliers, leverage points, and residuals that could affect the model.\nCleaning Phase: Deletes problematic data based on predefined conditions.\nModeling Phase: Various models are fitted including polynomial models and a comprehensive model with multiple predictors.\nColinearity Check: A rule of thumb is used to check for multicollinearity in the comprehensive model, and problematic variables are flagged.\n\nExplore relationships for continuous outcome variable\nFirst, load several R packages for statistical modeling, data manipulation, and visualization.\n\nShow the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(car)\nlibrary(corrplot)\nlibrary(olsrr)\n\n\nLoad data\nHere, a dataset is loaded into the R environment from an RData file.\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n\n\nCorrelation plot\n\n\n\n\n\n\nTip\n\n\n\nWe can use the cor function to see the correlation between numeric variables and then use the corrplot function to plot the cor object. The plot helps in understanding the linear or nonlinear relationships between different numerical variables.\n\n\n\nShow the coderequire(corrplot)\nnumeric.names <- c(\"age\", \"diastolicBP\", \"systolicBP\", \n                   \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"alcohol\", \n                   \"cholesterol\", \"triglycerides\", \"uric.acid\", \n                   \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n                   \"globulin\", \"calcium\")\ncorrelationMatrix <- cor(analytic[numeric.names])\nmat.num <- round(correlationMatrix,2)\nmat.num[mat.num>0.8 & mat.num < 1]\n#> [1] 0.89 0.90 0.89 0.91 0.90 0.91\ncorrplot(correlationMatrix, method=\"number\", type=\"upper\")\n\n\n\n\nExamine descriptive associations\nLet us examine the descriptive associations with the dependent variable by stratifying separately by key predictors\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to examine the descriptive associations by strata/groups, e.g., summarize, aggregate, describeBy, tapply, summary\n\n\nThe code calculates and explores various ways to describe the cholesterol levels, stratified by key predictors such as gender.\n\nShow the codemean(analytic$cholesterol)\n#> [1] 193.1002\n\n# Process 1\nmean(analytic$cholesterol[analytic$gender == \"Male\"])\n#> [1] 190.7626\nmean(analytic$cholesterol[analytic$gender == \"Female\"])\n#> [1] 196.7339\n\n# Process 2\nlibrary(dplyr)\nanalytic %>%\n  group_by(gender) %>%\n  summarize(mean.ch=mean(cholesterol), .groups = 'drop') \n\n\n\n  \n\n\nShow the code\n# process 3\nwith(analytic, aggregate( analytic$cholesterol, by=list(gender) , FUN=summary))\n\n\n\n  \n\n\nShow the code\n# process 4\npsych::describeBy(analytic$cholesterol, analytic$gender)\n#> \n#>  Descriptive statistics by group \n#> group: Female\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 496 196.73 43.26  194.5  194.44 40.77 100 358   258 0.57     0.56 1.94\n#> ------------------------------------------------------------ \n#> group: Male\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 771 190.76 43.06    188  188.54 40.03  81 545   464  1.1     5.76 1.55\n\n# process 5\ntapply(analytic$cholesterol, analytic$gender, summary)\n#> $Female\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   100.0   166.8   194.5   196.7   220.2   358.0 \n#> \n#> $Male\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   161.0   188.0   190.8   215.0   545.0\n\n# A general process\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.summ <- summary(cholesterol~ ., data = analytic[sel.names])\nvar.summ\n#> cholesterol      N= 1267  \n#> \n#> +---------------------+--------------------------------+----+-----------+\n#> |                     |                                |   N|cholesterol|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               gender|                          Female| 496|   196.7339|\n#> |                     |                            Male| 771|   190.7626|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  age|                         [20,37)| 342|   182.4854|\n#> |                     |                         [37,52)| 313|   200.1661|\n#> |                     |                         [52,64)| 315|   199.7873|\n#> |                     |                         [64,80]| 297|   190.7845|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 born|Born in 50 US states or Washingt| 991|   190.9253|\n#> |                     |                          Others| 276|   200.9094|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 race|                           Black| 246|   187.3740|\n#> |                     |                        Hispanic| 337|   193.5490|\n#> |                     |                           Other| 132|   191.8561|\n#> |                     |                           White| 552|   195.6757|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            education|                         College| 648|   192.5478|\n#> |                     |                     High.School| 523|   193.4532|\n#> |                     |                          School|  96|   194.9062|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              married|                         Married| 751|   194.0306|\n#> |                     |                   Never.married| 226|   182.8761|\n#> |                     |              Previously.married| 290|   198.6586|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               income|                            <25k| 344|   191.9564|\n#> |                     |                Between.25kto54k| 435|   191.9310|\n#> |                     |                Between.55kto99k| 297|   195.7508|\n#> |                     |                        Over100k| 191|   193.7016|\n#> +---------------------+--------------------------------+----+-----------+\n#> |          diastolicBP|                        [ 0, 64)| 336|   186.7649|\n#> |                     |                        [64, 72)| 321|   189.3458|\n#> |                     |                        [72, 80)| 319|   195.7085|\n#> |                     |                        [80,112]| 291|   201.6976|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           systolicBP|                       [ 84,116)| 340|   186.2765|\n#> |                     |                       [116,126)| 317|   190.6372|\n#> |                     |                       [126,138)| 335|   196.9881|\n#> |                     |                       [138,236]| 275|   199.6400|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyweight|                    [39.7, 69.8)| 319|   193.8903|\n#> |                     |                    [69.8, 81.5)| 316|   197.1424|\n#> |                     |                    [81.5, 97.2)| 317|   192.4984|\n#> |                     |                    [97.2,178.4]| 315|   188.8508|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyheight|                       [144,163)| 317|   198.7003|\n#> |                     |                       [163,169)| 320|   193.7750|\n#> |                     |                       [169,176)| 314|   189.8790|\n#> |                     |                       [176,201]| 316|   190.0000|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  bmi|                     [16.3,24.9)| 322|   188.8043|\n#> |                     |                     [24.9,28.7)| 315|   198.5016|\n#> |                     |                     [28.7,33.4)| 317|   197.5016|\n#> |                     |                     [33.4,64.5]| 313|   187.6262|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                waist|                   [ 65.0, 90.7)| 320|   188.9688|\n#> |                     |                   [ 90.7,100.4)| 315|   199.6413|\n#> |                     |                   [100.4,111.3)| 316|   197.3892|\n#> |                     |                   [111.3,161.5]| 316|   186.4747|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                smoke|                       Every.day| 448|   191.5938|\n#> |                     |                      Not.at.all| 665|   194.6451|\n#> |                     |                       Some.days| 154|   190.8117|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              alcohol|                               1| 336|   191.0387|\n#> |                     |                               2| 371|   192.0809|\n#> |                     |                          [3, 5)| 295|   195.9356|\n#> |                     |                          [5,15]| 265|   193.9849|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        triglycerides|                      [ 18,  85)| 320|   172.2344|\n#> |                     |                      [ 85, 128)| 319|   185.6834|\n#> |                     |                      [128, 203)| 314|   199.4140|\n#> |                     |                      [203,3061]| 314|   215.5860|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            uric.acid|                      [1.6, 4.7)| 348|   188.9310|\n#> |                     |                      [4.7, 5.6)| 305|   191.8033|\n#> |                     |                      [5.6, 6.6)| 307|   195.7720|\n#> |                     |                      [6.6,18.0]| 307|   196.4430|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              protein|                       [5.7,6.9)| 336|   189.8631|\n#> |                     |                       [6.9,7.2)| 328|   192.3201|\n#> |                     |                       [7.2,7.5)| 310|   193.4258|\n#> |                     |                       [7.5,9.0]| 293|   197.3413|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            bilirubin|                       [0.0,0.5)| 506|   195.7391|\n#> |                     |                             0.5| 212|   192.2264|\n#> |                     |                       [0.6,0.8)| 310|   192.0645|\n#> |                     |                       [0.8,3.3]| 239|   189.6318|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           phosphorus|                       [1.8,3.4)| 362|   188.0387|\n#> |                     |                       [3.4,3.7)| 309|   192.5405|\n#> |                     |                       [3.7,4.1)| 323|   195.5542|\n#> |                     |                       [4.1,6.1]| 273|   197.5421|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               sodium|                       [124,138)| 362|   191.9420|\n#> |                     |                       [138,140)| 495|   194.2929|\n#> |                     |                             140| 206|   191.7864|\n#> |                     |                       [141,148]| 204|   193.5882|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            potassium|                     [2.60,3.79)| 320|   191.5375|\n#> |                     |                     [3.79,3.99)| 328|   192.3628|\n#> |                     |                     [3.99,4.20)| 308|   196.9643|\n#> |                     |                     [4.20,5.51]| 311|   191.6592|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             globulin|                       [1.6,2.6)| 350|   189.9429|\n#> |                     |                       [2.6,2.9)| 388|   199.0052|\n#> |                     |                       [2.9,3.1)| 230|   193.3783|\n#> |                     |                       [3.1,5.5]| 299|   188.9197|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              calcium|                      [8.4, 9.2)| 371|   186.0323|\n#> |                     |                      [9.2, 9.4)| 294|   188.3605|\n#> |                     |                      [9.4, 9.7)| 395|   197.4430|\n#> |                     |                      [9.7,11.1]| 207|   204.2126|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        physical.work|                              No| 895|   194.0078|\n#> |                     |                             Yes| 372|   190.9167|\n#> +---------------------+--------------------------------+----+-----------+\n#> |physical.recreational|                              No|1002|   193.5359|\n#> |                     |                             Yes| 265|   191.4528|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             diabetes|                              No|1064|   194.8036|\n#> |                     |                             Yes| 203|   184.1724|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              Overall|                                |1267|   193.1002|\n#> +---------------------+--------------------------------+----+-----------+\nplot(var.summ)\n\n\n\nShow the code\nsummary(analytic$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   62.00   70.00   70.37   78.00  112.00\n\nanalytic$diastolicBP[analytic$diastolicBP == 0] <- NA\n\n# Bivariate Summaries Computed Separately by a Series of Predictors\nvar.summ2 <- spearman2(cholesterol~ ., data = analytic[sel.names])\nplot(var.summ2)\n\n\n\n\nRegression: Linear regression\nA linear regression model is fitted to explore the association between cholesterol levels and triglycerides. Various summary statistics are also generated for the model.\n\n\n\n\n\n\nTip\n\n\n\nWe use lm function to fit the linear regression\n\n\n\nShow the code# set up formula with just 1 variable\nformula0 <- as.formula(\"cholesterol~triglycerides\")\n\n# fitting regression on the analytic2 data\nfit0 <- lm(formula0,data = analytic2)\n\n# extract results\nsummary(fit0)\n#> \n#> Call:\n#> lm(formula = formula0, data = analytic2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -111.651  -26.157   -2.661   22.549  166.752 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.716e+02  1.127e+00  152.23   <2e-16 ***\n#> triglycerides 1.275e-01  5.456e-03   23.37   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 37.38 on 2632 degrees of freedom\n#> Multiple R-squared:  0.1718, Adjusted R-squared:  0.1715 \n#> F-statistic:   546 on 1 and 2632 DF,  p-value: < 2.2e-16\n\n# extract just the coefficients/estimates\ncoef(fit0)\n#>   (Intercept) triglycerides \n#>   171.6147531     0.1274909\n\n# extract confidence intervals\nconfint(fit0)\n#>                     2.5 %      97.5 %\n#> (Intercept)   169.4042284 173.8252779\n#> triglycerides   0.1167919   0.1381899\n\n# residual plots\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\nDiagnosis\nIdentifying problematic data\nOutliers: We can begin by plotting cholesterol against triglycerides to visualize any potential outliers. We can then identify data points where triglycerides are high.\nLeverage: It calculates and plots leverage points. Leverage points that have values greater than 0.05 are isolated for inspection.\nResiduals: Studentized residuals are computed for each data point to identify potential outliers. Those with values less than -5 are identified.\n\nShow the coderequire(olsrr)\n# Outlier\nplot(cholesterol ~ triglycerides, data = analytic2)\n\n\n\nShow the codesubset(analytic2, triglycerides > 1500)\n\n\n\n  \n\n\nShow the code\n# leverage\nols_plot_resid_lev(fit0)\n\n\n\nShow the codeanalytic2$lev <- hat(model.matrix(fit0))\nplot(analytic2$lev)\n\n\n\nShow the codesummary(analytic2$lev)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.0003796 0.0004062 0.0004773 0.0007593 0.0005831 0.1800021\nwhich(analytic2$lev > 0.05)\n#> [1] 1102\nsubset(analytic2, lev > 0.05)\n\n\n\n  \n\n\nShow the code\n# Residual\nanalytic2$rstudent.values <- rstudent(fit0)\nplot(analytic2$rstudent.values)\n\n\n\nShow the codewhich(analytic2$rstudent.values < -5)\n#> integer(0)\n# Heteroskedasticity: Test for constant variance\n#ols_test_breusch_pagan(fit0, rhs = TRUE)\n\n\nDeleting suspicious data\nWe then delete observations based on two conditions: triglycerides > 1500 and leverage > 0.05.\n\nShow the code# condition 1: triglycerides above 1500 needs deleting\nanalytic2b <- subset(analytic2, triglycerides < 1500)\ndim(analytic2b)\n#> [1] 2632   34\n\n# condition 2: leverage above 0.05 needs deleting\nanalytic3 <- subset(analytic2b, lev < 0.05)\ndim(analytic3)\n#> [1] 2632   34\n\n# Check how many observations are deleted\nnrow(analytic2)-nrow(analytic3)\n#> [1] 2\n\n\nRefitting in cleaned data\nWe refit the linear model on this cleaned data, and diagnostic plots are generated.\n\nShow the code### Re-fit in data analytic3 (without problematic data)\nformula0\n#> cholesterol ~ triglycerides\nfit0 <- lm(formula0,data = analytic3)\n\nrequire(Publish)\npublish(fit0)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\nShow the code\nrequire(car)\n# component+residual plot or partial-residual plot\ncrPlots(fit0)\n\n\n\n\npolynomial order 2\nWe fit polynomial models of orders 2 and 3 to explore non-linear relationships between cholesterol and triglycerides.\n\nShow the codeformula1 <- as.formula(\"cholesterol~poly(triglycerides,2)\")\nformula1 <- as.formula(\"cholesterol~triglycerides^2\")\nfit1 <- lm(formula1,data = analytic3)\npublish(fit1)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit1)\n\n\n\nShow the code\n# compare fit0 and fit1 models\nanova(fit0,fit1)\n\n\n\n  \n\n\n\npolynomial order 3\n\nShow the code# Fit a polynomial of order 3\nformula2 <- as.formula(\"cholesterol~poly(triglycerides,3)\")\nformula2 <- as.formula(\"cholesterol~triglycerides^3\")\nfit2 <- lm(formula2,data = analytic3)\npublish(fit2)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit2)\n\n\n\nShow the code\n# compare fit1 and fit2 models\nanova(fit1,fit2)\n\n\n\n  \n\n\n\nMultiple covariates\nWe add more covariates.\n\nShow the code# include everything!\nformula3 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit3 <- lm(formula3, data = analytic3)\npublish(fit3)\n#>               Variable                            Units Coefficient           CI.95     p-value \n#>            (Intercept)                                       280.02 [133.42;426.62]   0.0001852 \n#>                 gender                           Female         Ref                             \n#>                                                    Male      -11.99  [-16.41;-7.57]     < 1e-04 \n#>                    age                                         0.35     [0.23;0.47]     < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                             \n#>                                                  Others        7.52    [3.68;11.36]   0.0001270 \n#>                   race                            Black         Ref                             \n#>                                                Hispanic       -6.15  [-10.87;-1.44]   0.0106253 \n#>                                                   Other       -5.37   [-10.92;0.18]   0.0579281 \n#>                                                   White       -0.95    [-5.21;3.30]   0.6603698 \n#>              education                          College         Ref                             \n#>                                             High.School        2.90    [-0.28;6.08]   0.0743132 \n#>                                                  School       -2.54    [-8.61;3.54]   0.4134016 \n#>                married                          Married         Ref                             \n#>                                           Never.married       -5.72   [-9.63;-1.81]   0.0041887 \n#>                                      Previously.married        0.31    [-3.54;4.17]   0.8730460 \n#>                 income                             <25k         Ref                             \n#>                                        Between.25kto54k       -0.97    [-4.87;2.93]   0.6261315 \n#>                                        Between.55kto99k        2.29    [-1.98;6.56]   0.2928564 \n#>                                                Over100k        2.44    [-2.27;7.14]   0.3099380 \n#>            diastolicBP                                         0.38     [0.25;0.50]     < 1e-04 \n#>             systolicBP                                         0.02    [-0.08;0.12]   0.6668119 \n#>                    bmi                                        -2.55   [-4.29;-0.81]   0.0041392 \n#>             bodyweight                                         0.82     [0.19;1.45]   0.0105518 \n#>             bodyheight                                        -0.89   [-1.55;-0.24]   0.0074286 \n#>                  waist                                        -0.02    [-0.29;0.26]   0.9020424 \n#>          triglycerides                                         0.12     [0.11;0.14]     < 1e-04 \n#>              uric.acid                                         1.27     [0.08;2.47]   0.0369190 \n#>                protein                                         4.99   [-0.77;10.74]   0.0897748 \n#>              bilirubin                                        -5.43  [-10.53;-0.33]   0.0370512 \n#>             phosphorus                                        -0.18    [-2.81;2.45]   0.8939361 \n#>                 sodium                                        -0.97   [-1.66;-0.29]   0.0052516 \n#>              potassium                                         1.04    [-3.44;5.52]   0.6487979 \n#>               globulin                                        -2.25    [-8.22;3.71]   0.4591138 \n#>                calcium                                        12.02    [6.98;17.07]     < 1e-04 \n#>          physical.work                               No         Ref                             \n#>                                                     Yes       -0.45    [-3.68;2.79]   0.7858787 \n#>  physical.recreational                               No         Ref                             \n#>                                                     Yes        1.35    [-1.94;4.65]   0.4210703 \n#>               diabetes                               No         Ref                             \n#>                                                     Yes      -19.11 [-23.37;-14.85]     < 1e-04\n\n\nColinearity\nWe finally check for multicollinearity among predictors using the Variance Inflation Factor (VIF).\n\n\nRule of thumb: variables with VIF > 4 needs further investigation\n\nShow the codecar::vif(fit3)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.694171  1        1.641393\n#> age                     2.164388  1        1.471186\n#> born                    1.611478  1        1.269440\n#> race                    2.463445  3        1.162137\n#> education               1.435876  2        1.094660\n#> married                 1.481141  2        1.103187\n#> income                  1.402249  3        1.057964\n#> diastolicBP             1.271126  1        1.127442\n#> systolicBP              1.594986  1        1.262928\n#> bmi                    81.811969  1        9.044997\n#> bodyweight            101.102349  1       10.054966\n#> bodyheight             21.863188  1        4.675809\n#> waist                  11.913719  1        3.451626\n#> triglycerides           1.219331  1        1.104233\n#> uric.acid               1.603290  1        1.266211\n#> protein                 3.622385  1        1.903256\n#> bilirubin               1.185035  1        1.088593\n#> phosphorus              1.116982  1        1.056874\n#> sodium                  1.120920  1        1.058735\n#> potassium               1.178381  1        1.085533\n#> globulin                3.371211  1        1.836086\n#> calcium                 1.591677  1        1.261617\n#> physical.work           1.087315  1        1.042744\n#> physical.recreational   1.226830  1        1.107624\n#> diabetes                1.210715  1        1.100325\ncollinearity <- ols_vif_tol(fit3)\ncollinearity\n\n\n\n  \n\n\nShow the code\n# VIF > 4\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + # bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit4 <- lm(formula4, data = analytic3)\npublish(fit4)\n#>               Variable                            Units Coefficient           CI.95    p-value \n#>            (Intercept)                                       136.87  [34.96;238.79]   0.008533 \n#>                 gender                           Female         Ref                            \n#>                                                    Male      -13.06  [-16.60;-9.53]    < 1e-04 \n#>                    age                                         0.35     [0.24;0.46]    < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                            \n#>                                                  Others        7.88    [4.06;11.69]    < 1e-04 \n#>                   race                            Black         Ref                            \n#>                                                Hispanic       -5.79  [-10.34;-1.24]   0.012740 \n#>                                                   Other       -4.88   [-10.33;0.57]   0.079497 \n#>                                                   White       -0.85    [-5.02;3.33]   0.690720 \n#>              education                          College         Ref                            \n#>                                             High.School        2.85    [-0.32;6.02]   0.078008 \n#>                                                  School       -2.45    [-8.49;3.60]   0.427694 \n#>                married                          Married         Ref                            \n#>                                           Never.married       -5.74   [-9.65;-1.83]   0.004088 \n#>                                      Previously.married        0.34    [-3.52;4.20]   0.861981 \n#>                 income                             <25k         Ref                            \n#>                                        Between.25kto54k       -0.87    [-4.77;3.03]   0.663123 \n#>                                        Between.55kto99k        2.46    [-1.79;6.71]   0.256585 \n#>                                                Over100k        2.63    [-2.07;7.32]   0.272886 \n#>            diastolicBP                                         0.37     [0.25;0.50]    < 1e-04 \n#>             systolicBP                                         0.03    [-0.07;0.13]   0.544971 \n#>                    bmi                                        -0.31   [-0.54;-0.08]   0.009302 \n#>          triglycerides                                         0.12     [0.11;0.14]    < 1e-04 \n#>              uric.acid                                         1.36     [0.16;2.55]   0.025926 \n#>                protein                                         4.77   [-0.98;10.51]   0.104059 \n#>              bilirubin                                        -6.06  [-11.14;-0.98]   0.019519 \n#>             phosphorus                                        -0.08    [-2.71;2.55]   0.954561 \n#>                 sodium                                        -1.03   [-1.71;-0.35]   0.003175 \n#>              potassium                                         0.89    [-3.58;5.37]   0.695615 \n#>               globulin                                        -2.20    [-8.15;3.75]   0.469150 \n#>                calcium                                        12.20    [7.16;17.25]    < 1e-04 \n#>          physical.work                               No         Ref                            \n#>                                                     Yes       -0.44    [-3.68;2.80]   0.790297 \n#>  physical.recreational                               No         Ref                            \n#>                                                     Yes        1.24    [-2.03;4.51]   0.457666 \n#>               diabetes                               No         Ref                            \n#>                                                     Yes      -19.03 [-23.26;-14.80]    < 1e-04\n\n# check if there is still any problematic variable\n# with high collinearity problem\ncollinearity <- ols_vif_tol(fit4)\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")"
  },
  {
    "objectID": "predictivefactors3.html",
    "href": "predictivefactors3.html",
    "title": "Binary outcome",
    "section": "",
    "text": "We focuse on statistical analysis and modeling of a binary outcome, cholesterol levels that are categorized as either “healthy” or “unhealthy.”\nExplore relationships for binary outcome variable\n\n\n\nLoad data\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")\n\n\nCreating binary variable\nBinary categorization: The cholesterol variable is converted into a binary outcome (“healthy” or “unhealthy”) using the ifelse function based on a threshold value of 200.\nRe-leveling: The reference category for the binary variable is changed to “unhealthy.”\n\nShow the code# Binary variable\nanalytic3$cholesterol.bin <- ifelse(analytic3$cholesterol < 200, \"healthy\", \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>      1586      1046\n\n# Changing the reference category\nanalytic3$cholesterol.bin <- as.factor(analytic3$cholesterol.bin)\nanalytic3$cholesterol.bin <- relevel(analytic3$cholesterol.bin, ref = \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#> unhealthy   healthy \n#>      1046      1586\n\n\nModelling data\nA logistic regression is fitted to predict the binary cholesterol outcome from multiple predictor variables.\n\n\n\n\n\n\nTip\n\n\n\nWe use the glm function to run generalized linear models. The default family is gaussian with identity link. Setting binomial family with logit link (logit link is default for binomial family) means fitting logistic regression.\n\n\n\nShow the code# Regression model\nformula5x <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\n\n# Summary\nfit5x <- glm(formula5x, family = binomial(), data = analytic3)\npublish(fit5x)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.68 [1.27;2.23]   0.000313 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.69 [0.54;0.88]   0.002636 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.29 [0.95;1.75]   0.107104 \n#>                                                   Other      1.24 [0.87;1.79]   0.234062 \n#>                                                   White      1.10 [0.83;1.44]   0.505147 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.082168 \n#>                                                  School      1.23 [0.84;1.82]   0.292070 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.67]   0.052969 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.345688 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.01 [0.78;1.29]   0.957537 \n#>                                        Between.55kto99k      0.90 [0.69;1.19]   0.462854 \n#>                                                Over100k      0.90 [0.66;1.21]   0.472137 \n#>            diastolicBP                                       0.98 [0.97;0.98]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.029513 \n#>                    bmi                                       1.11 [0.99;1.24]   0.065627 \n#>             bodyweight                                       0.96 [0.92;1.00]   0.045338 \n#>             bodyheight                                       1.05 [1.00;1.09]   0.030995 \n#>                  waist                                       1.01 [0.99;1.02]   0.464825 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.273792 \n#>                protein                                       0.61 [0.42;0.89]   0.009192 \n#>              bilirubin                                       1.19 [0.86;1.66]   0.292632 \n#>             phosphorus                                       0.96 [0.81;1.13]   0.610931 \n#>                 sodium                                       1.06 [1.02;1.11]   0.007980 \n#>              potassium                                       0.95 [0.71;1.26]   0.729218 \n#>               globulin                                       1.38 [0.94;2.01]   0.101667 \n#>                calcium                                       0.64 [0.47;0.89]   0.007026 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.392539 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.05 [0.85;1.29]   0.681388 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.68 [2.02;3.56]    < 1e-04\n\n# VIF\ncar::vif(fit5x)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.735258  1        1.653862\n#> age                     2.121098  1        1.456399\n#> born                    1.664094  1        1.289998\n#> race                    2.585539  3        1.171544\n#> education               1.458430  2        1.098933\n#> married                 1.432595  2        1.094034\n#> income                  1.426911  3        1.061043\n#> diastolicBP             1.297308  1        1.138994\n#> systolicBP              1.614374  1        1.270580\n#> bmi                    81.928815  1        9.051454\n#> bodyweight            103.125772  1       10.155086\n#> bodyheight             22.647853  1        4.758976\n#> waist                  11.493710  1        3.390237\n#> triglycerides           1.258340  1        1.121758\n#> uric.acid               1.636512  1        1.279262\n#> protein                 3.684816  1        1.919587\n#> bilirubin               1.186181  1        1.089119\n#> phosphorus              1.117915  1        1.057315\n#> sodium                  1.123193  1        1.059808\n#> potassium               1.181358  1        1.086903\n#> globulin                3.427401  1        1.851324\n#> calcium                 1.543019  1        1.242183\n#> physical.work           1.090958  1        1.044490\n#> physical.recreational   1.218558  1        1.103883\n#> diabetes                1.212365  1        1.101074\n\n\nAUC\nThe Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) is calculated to assess the model’s predictive performance. Let us measure the accuracy for classification models fit5x.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the roc function to build a ROC curve and auc function to calculate the AUC (are under the ROC curve) value.\n\n\n\nShow the coderequire(pROC)\npred.y <- predict(fit5x, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7411\n\nauc(rocobj)\n#> Area under the curve: 0.7411\n\n\nRe-modelling\nLet us re-fit the model and measure the AUC. VIF is calculated again for this new model.\n\nShow the codeformula5 <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\npublish(fit5)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.86 [1.48;2.33]    < 1e-04 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.67 [0.52;0.85]   0.001233 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.26 [0.94;1.69]   0.128165 \n#>                                                   Other      1.21 [0.85;1.73]   0.284083 \n#>                                                   White      1.11 [0.85;1.45]   0.460652 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.083806 \n#>                                                  School      1.22 [0.83;1.80]   0.305514 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.68]   0.049983 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.339401 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.00 [0.78;1.28]   0.999445 \n#>                                        Between.55kto99k      0.90 [0.68;1.17]   0.425427 \n#>                                                Over100k      0.89 [0.66;1.20]   0.447012 \n#>            diastolicBP                                       0.98 [0.97;0.99]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.042769 \n#>                    bmi                                       1.01 [0.99;1.02]   0.496430 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.242942 \n#>                protein                                       0.62 [0.43;0.89]   0.010343 \n#>              bilirubin                                       1.24 [0.89;1.72]   0.203993 \n#>             phosphorus                                       0.95 [0.80;1.12]   0.539847 \n#>                 sodium                                       1.06 [1.02;1.11]   0.006777 \n#>              potassium                                       0.96 [0.72;1.28]   0.790080 \n#>               globulin                                       1.37 [0.94;2.00]   0.102430 \n#>                calcium                                       0.64 [0.46;0.88]   0.005772 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.382281 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.04 [0.85;1.29]   0.682962 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.69 [2.03;3.57]    < 1e-04\n\n# VIF\ncar::vif(fit5)\n#>                           GVIF Df GVIF^(1/(2*Df))\n#> gender                1.749947  1        1.322856\n#> age                   1.850160  1        1.360206\n#> born                  1.640947  1        1.280994\n#> race                  2.345460  3        1.152669\n#> education             1.430721  2        1.093676\n#> married               1.432015  2        1.093923\n#> income                1.409064  3        1.058819\n#> diastolicBP           1.289411  1        1.135523\n#> systolicBP            1.605248  1        1.266984\n#> bmi                   1.477795  1        1.215646\n#> triglycerides         1.246395  1        1.116421\n#> uric.acid             1.624039  1        1.274378\n#> protein               3.648367  1        1.910070\n#> bilirubin             1.177643  1        1.085193\n#> phosphorus            1.114298  1        1.055603\n#> sodium                1.117463  1        1.057101\n#> potassium             1.176914  1        1.084857\n#> globulin              3.395946  1        1.842809\n#> calcium               1.542486  1        1.241969\n#> physical.work         1.089742  1        1.043907\n#> physical.recreational 1.197719  1        1.094404\n#> diabetes              1.200402  1        1.095629\n\n\nThe AUC for this new model is also calculated.\n\nShow the code#### AUC\npred.y <- predict(fit5, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7406\nauc(rocobj)\n#> Area under the curve: 0.7406\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nReferences"
  },
  {
    "objectID": "predictivefactors4.html",
    "href": "predictivefactors4.html",
    "title": "Overfitting and performance",
    "section": "",
    "text": "The following tutorial extends the work from the previous lab and focuses on understanding overfitting, evaluating performance, and function writing in the context of linear modeling for a continuous outcome variable, cholesterol levels.\n\nShow the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nNow we will fit the final model that we decided at the end of previous part of the lab.\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4 <- lm(formula4, data = analytic3)\nsummary(fit4)\n#> \n#> Call:\n#> lm(formula = formula4, data = analytic3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -115.465  -23.695   -2.598   20.017  177.264 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               136.871606  51.998527   2.632  0.00853 ** \n#> genderMale                -13.064857   1.802099  -7.250 5.48e-13 ***\n#> age                         0.351838   0.056116   6.270 4.22e-10 ***\n#> bornOthers                  7.877420   1.947498   4.045 5.39e-05 ***\n#> raceHispanic               -5.790547   2.323010  -2.493  0.01274 *  \n#> raceOther                  -4.879882   2.781673  -1.754  0.07950 .  \n#> raceWhite                  -0.847635   2.130149  -0.398  0.69072    \n#> educationHigh.School        2.851633   1.617435   1.763  0.07801 .  \n#> educationSchool            -2.446765   3.084409  -0.793  0.42769    \n#> marriedNever.married       -5.739509   1.997152  -2.874  0.00409 ** \n#> marriedPreviously.married   0.342206   1.968165   0.174  0.86198    \n#> incomeBetween.25kto54k     -0.867063   1.990253  -0.436  0.66312    \n#> incomeBetween.55kto99k      2.462130   2.169757   1.135  0.25658    \n#> incomeOver100k              2.626046   2.394560   1.097  0.27289    \n#> diastolicBP                 0.374971   0.062238   6.025 1.93e-09 ***\n#> systolicBP                  0.029976   0.049515   0.605  0.54497    \n#> bmi                        -0.309530   0.118927  -2.603  0.00930 ** \n#> triglycerides               0.124806   0.006427  19.419  < 2e-16 ***\n#> uric.acid                   1.357242   0.609012   2.229  0.02593 *  \n#> protein                     4.767008   2.931636   1.626  0.10406    \n#> bilirubin                  -6.060791   2.593508  -2.337  0.01952 *  \n#> phosphorus                 -0.076472   1.341957  -0.057  0.95456    \n#> sodium                     -1.026686   0.347679  -2.953  0.00318 ** \n#> potassium                   0.893507   2.283488   0.391  0.69561    \n#> globulin                   -2.198037   3.036091  -0.724  0.46915    \n#> calcium                    12.202366   2.574400   4.740 2.25e-06 ***\n#> physical.workYes           -0.439108   1.651078  -0.266  0.79030    \n#> physical.recreationalYes    1.238756   1.667670   0.743  0.45767    \n#> diabetesYes               -19.032748   2.158825  -8.816  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 35.22 on 2603 degrees of freedom\n#> Multiple R-squared:  0.2415, Adjusted R-squared:  0.2334 \n#> F-statistic: 29.61 on 28 and 2603 DF,  p-value: < 2.2e-16\n\n\nDesign Matrix\nExpands factors to a set of dummy variables.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the model.matrix function to construct a design/model matrix, such as expand factor variables to a matrix of dummy variable\n\n\nThe dimensions of the model matrix are obtained, and the total number of model parameters (p) is calculated.\n\nShow the codehead(model.matrix(fit4))\n#>    (Intercept) genderMale age bornOthers raceHispanic raceOther raceWhite\n#> 1            1          1  62          0            0         0         1\n#> 2            1          1  53          1            0         0         1\n#> 4            1          0  56          0            0         0         1\n#> 5            1          0  42          0            0         0         0\n#> 10           1          1  22          0            0         0         0\n#> 11           1          0  32          1            1         0         0\n#>    educationHigh.School educationSchool marriedNever.married\n#> 1                     0               0                    0\n#> 2                     1               0                    0\n#> 4                     0               0                    0\n#> 5                     0               0                    0\n#> 10                    0               0                    1\n#> 11                    0               0                    0\n#>    marriedPreviously.married incomeBetween.25kto54k incomeBetween.55kto99k\n#> 1                          0                      0                      1\n#> 2                          1                      0                      0\n#> 4                          0                      0                      1\n#> 5                          1                      1                      0\n#> 10                         0                      1                      0\n#> 11                         0                      1                      0\n#>    incomeOver100k diastolicBP systolicBP  bmi triglycerides uric.acid protein\n#> 1               0          70        128 27.8           158       4.2     7.5\n#> 2               0          88        146 30.8           170       7.0     7.4\n#> 4               0          72        132 42.4            93       5.4     6.1\n#> 5               0          70        100 20.3            52       3.3     7.7\n#> 10              0          70        110 28.0            77       6.0     7.4\n#> 11              0          70        120 28.2           295       5.2     7.4\n#>    bilirubin phosphorus sodium potassium globulin calcium physical.workYes\n#> 1        0.5        4.7    136      4.30      2.9     9.8                0\n#> 2        0.6        4.4    140      4.55      2.9     9.8                0\n#> 4        0.3        3.8    141      4.08      2.3     8.9                0\n#> 5        0.3        3.2    136      3.50      3.4     9.3                0\n#> 10       0.2        5.3    139      4.16      3.0     9.3                0\n#> 11       0.4        3.1    138      4.31      2.9    10.3                0\n#>    physical.recreationalYes diabetesYes\n#> 1                         0           1\n#> 2                         0           0\n#> 4                         0           0\n#> 5                         0           0\n#> 10                        1           0\n#> 11                        0           0\n\n# Dimension of the model matrix\ndim(model.matrix(fit4))\n#> [1] 2632   29\n\n# Number of parameters = intercept + slopes\np <- dim(model.matrix(fit4))[2] \np\n#> [1] 29\n\n\nCheck prediction\nThe observed and predicted cholesterol values are summarized.\n\nShow the codeobs.y <- analytic3$cholesterol\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   163.0   189.0   191.5   216.0   362.0\n\n# Predict the above fit on analytic3 data\npred.y <- predict(fit4, analytic3)\nsummary(pred.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   136.3   178.2   189.4   191.5   202.4   337.6\nn <- length(pred.y)\nn\n#> [1] 2632\nplot(obs.y,pred.y)\nlines(lowess(obs.y,pred.y), col = \"red\")\n\n\n\nShow the code\n# Prediction on a new data: fictitious.data\nstr(fictitious.data)\n#> 'data.frame':    4121 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83734 83735 83736 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n#>  $ age                  : num  62 53 78 56 42 72 22 32 56 46 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"High.School\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"<25k\" \"Between.55kto99k\" ...\n#>  $ weight               : num  135630 25282 12576 102079 18235 ...\n#>  $ psu                  : num  1 1 1 1 2 1 2 1 2 1 ...\n#>  $ strata               : num  125 125 131 131 126 128 128 125 126 121 ...\n#>  $ diastolicBP          : num  70 88 46 72 70 58 70 70 116 94 ...\n#>  $ systolicBP           : num  128 146 138 132 100 116 110 120 178 144 ...\n#>  $ bodyweight           : num  94.8 90.4 83.4 109.8 55.2 ...\n#>  $ bodyheight           : num  184 171 170 161 165 ...\n#>  $ bmi                  : num  27.8 30.8 28.8 42.4 20.3 28.6 28 28.2 33.6 27.6 ...\n#>  $ waist                : num  101.1 107.9 116.5 110.1 80.4 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Not.at.all\" \"Not.at.all\" ...\n#>  $ alcohol              : num  1 6 0 1 1 0 8 1 0 1 ...\n#>  $ cholesterol          : num  173 265 229 174 204 190 164 190 145 242 ...\n#>  $ cholesterolM2        : num  4.47 6.85 5.92 4.5 5.28 4.91 4.24 4.91 3.75 6.26 ...\n#>  $ triglycerides        : num  158 170 299 93 52 52 77 295 121 497 ...\n#>  $ uric.acid            : num  4.2 7 7.3 5.4 3.3 4.9 6 5.2 4.8 6.5 ...\n#>  $ protein              : num  7.5 7.4 7.3 6.1 7.7 7.1 7.4 7.4 6.9 6.8 ...\n#>  $ bilirubin            : num  0.5 0.6 0.5 0.3 0.3 0.5 0.2 0.4 0.4 0.5 ...\n#>  $ phosphorus           : num  4.7 4.4 3.6 3.8 3.2 3.7 5.3 3.1 4.1 3.6 ...\n#>  $ sodium               : num  136 140 140 141 136 140 139 138 140 138 ...\n#>  $ potassium            : num  4.3 4.55 4.7 4.08 3.5 4.2 4.16 4.31 4.5 4.27 ...\n#>  $ globulin             : num  2.9 2.9 2.8 2.3 3.4 3 3 2.9 2.9 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.7 8.9 9.3 9.3 9.3 10.3 9.5 9.3 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:885] 16 30 39 48 50 58 61 65 67 68 ...\n#>   ..- attr(*, \"names\")= chr [1:885] \"27\" \"68\" \"90\" \"112\" ...\npred.y.new1 <- predict(fit4, fictitious.data)\nsummary(pred.y.new1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   128.7   178.9   190.6   192.5   203.3   557.4\n\n\nMeasuring prediction error\nContinuous outcomes\nR2\nThe Sum of Squares of Errors (SSE) and the Total Sum of Squares (SST) are calculated. The proportion of variance explained by the model is then calculated as R2.\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find SSE\nSSE <- sum( (obs.y - pred.y)^2 )\nSSE\n#> [1] 3228460\n\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 4256586\n\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.2415378\n\nrequire(caret)\nR2(pred.y, obs.y)\n#> [1] 0.2415378\n\n\nRMSE\nThe Root Mean Square Error is calculated to measure the average magnitude of the errors between predicted and observed values.\n\n\nSee Wikipedia (2023b)\n\nShow the code# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 35.21767\n\nRMSE(pred.y, obs.y)\n#> [1] 35.02311\n\n\nAdj R2\nIt provides a measure of how well the model generalizes and adjusts R2 based on the number of predictors.\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p))\nadjR2\n#> [1] 0.2333791\n\n\nWriting function\nSyntax for Writing Functions\n\nShow the codefunc_name <- function (argument) {\n  A statement or multiple lines of statements\n  return(output)\n}\n\n\nExample of a simple function\n\nShow the codef1 <- function(a,b){\n  result <- a + b\n  return(result)\n}\nf1(a=1,b=3)\n#> [1] 4\nf1(a=1,b=6)\n#> [1] 7\n# setting default values\nf1 <- function(a=1,b=1){\n  result <- a + b\n  return(result)\n}\nf1()\n#> [1] 2\nf1(b = 10)\n#> [1] 11\n\n\nA bit more complicated\n\nShow the code# one argument\nmodel.fit <- function(data.for.fitting){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#> 184.3131838  -7.8095595   0.2225745  11.1557140\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#> 176.1286576  -4.8256829   0.3375009   7.7186190\n\n\n\nShow the code# adding one more argument: digits\nmodel.fit <- function(data.for.fitting, digits=2){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  result <- round(result,digits)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#>      184.31       -7.81        0.22       11.16\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#>      176.13       -4.83        0.34        7.72\n\n\nFunction that gives performance measures\nlet us create a function that will give us the performance measures:\n\nShow the codeperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  \n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  \n  # sample size\n  n <- length(pred.y)\n  \n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  \n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  \n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  \n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  \n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  \n  # returning object\n  return(res)\n}\nperform(new.data = analytic3, y.name = \"cholesterol\", model.fit = fit4)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 2632 29 0.242 0.233 35.023\n\n\nReferences\n\n\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "predictivefactors5.html",
    "href": "predictivefactors5.html",
    "title": "Data spliting",
    "section": "",
    "text": "This tutorial is focused on a crucial aspect of model building: splitting your data into training and test sets to avoid overfitting. Overfitting occurs when your model learns the noise in the data, rather than the underlying trend. As a result, the model performs well on the training data but poorly on new, unseen data. To mitigate this, you often split your data.\nLoad data anf files\nInitially, several libraries are loaded to facilitate data manipulation and analysis.\n\nShow the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nThen, previously saved data related to cholesterol and other factors is loaded for further use.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nData spliting to avoid model overfitting\nYou start by setting a random seed to ensure that the random splitting of data is reproducible. A specified function is then used to partition the data, taking as arguments the outcome variable (cholesterol level in this case) and the percentage of data that you want to allocate to the training set (70% in this example).\n\n\nKDnuggets (2023)\n\nKuhn (2023a)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use the createDataPartition function to split a dataset into training and testing datasets. The function will return the row indices that should go into the training set. These indices are stored in a variable, and its dimensions are displayed to provide an understanding of the size of the training set that will be created. Additionally, you can calculate what 70% of your entire dataset would look like to verify the approximation of the training data size, as well as what the remaining 30% (for the test set) would look like.\n\n\n\nShow the code# Using a seed to randomize in a reproducible way \nset.seed(123)\nsplit <- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:1844, 1] 3 4 5 8 9 13 14 16 20 21 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 1844    1\n\n# Approximate train data\ndim(analytic3)*.7 \n#> [1] 1842.4   24.5\n\n# Approximate test data\ndim(analytic3)*(1-.7) \n#> [1] 789.6  10.5\n\n\nSplit the data\nAfter determining how to partition the data, the next step is to actually create the training and test datasets. The indices are used to subset the original dataset into these two new datasets. The dimensions of each dataset are displayed to confirm their sizes.\n\nShow the code# Create train data\ntrain.data <- analytic3[split,]\ndim(train.data)\n#> [1] 1844   35\n\n# Create test data\ntest.data <- analytic3[-split,]\ndim(test.data)\n#> [1] 788  35\n\n\nOur next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.\nTrain the model\nOnce the training dataset is created, you can proceed to train the model using the training data. A previously defined formula containing the predictor variables is used in a linear regression model. After fitting the model, a summary is generated to display key statistics that help in evaluating the model’s performance.\n\nShow the codeformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4.train1 <- lm(formula4, data = train.data)\nsummary(fit4.train1)\n#> \n#> Call:\n#> lm(formula = formula4, data = train.data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -91.973 -23.719  -1.563  20.586 178.542 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                72.716792  59.916086   1.214  0.22504    \n#> genderMale                -11.293629   2.136545  -5.286 1.40e-07 ***\n#> age                         0.306235   0.066376   4.614 4.23e-06 ***\n#> bornOthers                  7.220858   2.300658   3.139  0.00172 ** \n#> raceHispanic               -6.727473   2.709718  -2.483  0.01313 *  \n#> raceOther                  -4.865771   3.237066  -1.503  0.13298    \n#> raceWhite                  -1.468522   2.494981  -0.589  0.55621    \n#> educationHigh.School        1.626097   1.920289   0.847  0.39722    \n#> educationSchool            -4.853095   3.585185  -1.354  0.17602    \n#> marriedNever.married       -5.298265   2.332033  -2.272  0.02321 *  \n#> marriedPreviously.married   1.202448   2.305191   0.522  0.60199    \n#> incomeBetween.25kto54k     -1.736495   2.360385  -0.736  0.46202    \n#> incomeBetween.55kto99k      0.170505   2.565896   0.066  0.94703    \n#> incomeOver100k              1.712359   2.860226   0.599  0.54946    \n#> diastolicBP                 0.355813   0.074380   4.784 1.86e-06 ***\n#> systolicBP                  0.037464   0.059848   0.626  0.53140    \n#> bmi                        -0.282881   0.139160  -2.033  0.04222 *  \n#> triglycerides               0.123797   0.007613  16.261  < 2e-16 ***\n#> uric.acid                   1.006499   0.712871   1.412  0.15815    \n#> protein                     1.721623   3.468969   0.496  0.61975    \n#> bilirubin                  -6.143411   3.006858  -2.043  0.04118 *  \n#> phosphorus                  0.093824   1.575489   0.060  0.95252    \n#> sodium                     -0.604286   0.400694  -1.508  0.13170    \n#> potassium                  -0.583525   2.715189  -0.215  0.82986    \n#> globulin                   -0.278970   3.614404  -0.077  0.93849    \n#> calcium                    15.679677   3.054968   5.133 3.17e-07 ***\n#> physical.workYes           -1.099540   1.960321  -0.561  0.57494    \n#> physical.recreationalYes    0.834737   1.953960   0.427  0.66928    \n#> diabetesYes               -19.932101   2.580138  -7.725 1.83e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 34.68 on 1815 degrees of freedom\n#> Multiple R-squared:  0.2433, Adjusted R-squared:  0.2316 \n#> F-statistic: 20.84 on 28 and 1815 DF,  p-value: < 2.2e-16\n\n\nExtract performance measures\nYou can use a saved function to measure the performance of the trained model. The function will return performance metrics like R-squared, RMSE, etc. This function is applied not just on the training data but also on the test data, the full dataset, and a separate, fictitious dataset.\n\n\n\n\n\n\nTip\n\n\n\nBelow we use the perform function that we saved to evaluate the model performances\n\n\n\nShow the codeperform(new.data = train.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma   logLik      AIC\n#> [1,] 1844 29        1815 2182509 2884109 0.243 0.232 34.677 -9140.98 18341.96\n#>           BIC\n#> [1,] 18507.55\nperform(new.data = test.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>        n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 788 29         759 1057454 1372214 0.229 0.201 37.326 -3955.936 7971.873\n#>           BIC\n#> [1,] 8111.958\nperform(new.data = analytic3,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2 sigma    logLik      AIC\n#> [1,] 2632 29        2603 3239962 4256586 0.239 0.231 35.28 -13098.82 26257.64\n#>           BIC\n#> [1,] 26433.91\nperform(new.data = fictitious.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 4121 29        4092 5306559 6912485 0.232 0.227 36.011 -20601.92 41263.84\n#>           BIC\n#> [1,] 41453.55\n\n\nEvaluating the model’s performance on the test data provides insights into how well the model will generalize to new, unseen data. Comparing the performance metrics across different datasets can give you a robust view of your model’s predictive power and reliability.\n\n\nFor more on model training and tuning, see Kuhn (2023b)\nReferences\n\n\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023a. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.\n\n\n———. 2023b. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html."
  },
  {
    "objectID": "predictivefactors6.html",
    "href": "predictivefactors6.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Cross-validation is another important technique used to assess the performance of machine learning models and mitigate the risk of overfitting. This tutorial focuses on k-fold cross-validation as a strategy to obtain a more generalized and robust assessment of the model’s performance. It shows both manual calculations for individual folds and an automated approach using the caret package. This ensures that you aren’t simply fitting your model well to a specific subset of your data but are achieving good performance in a general sense.\n\nShow the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nk-fold cross-vaildation\n\n\nSee Wikipedia (2023)\nWe can set the number of folds to 5 (k = 5). A random seed is used for reproducibility. We use the function createFolds to create the folds. The data is divided based on the cholesterol levels, with each fold having approximately equal numbers of data points. The resulting structure contains training indices for each fold.\nWe can also examine the approximate size of training and test sets for each fold. The dimensions are displayed to understand the partitioning, and you can examine the length of indices in each fold to confirm the size of the training sets.\n\nShow the codek = 5\ndim(analytic3)\n#> [1] 2632   35\nset.seed(567)\n\n# Create folds (based on the outcome)\nfolds <- createFolds(analytic3$cholesterol, k = k, list = TRUE, \n                     returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\n\n# Approximate training data size\ndim(analytic3)*4/5\n#> [1] 2105.6   28.0\n\n# Approximate test data size\ndim(analytic3)/5  \n#> [1] 526.4   7.0\n\nlength(folds[[1]])\n#> [1] 2105\nlength(folds[[2]])\n#> [1] 2107\nlength(folds[[3]])\n#> [1] 2106\nlength(folds[[4]])\n#> [1] 2105\nlength(folds[[5]])\n#> [1] 2105\n\nstr(folds[[1]])\n#>  int [1:2105] 1 3 5 6 8 10 11 12 13 14 ...\nstr(folds[[2]])\n#>  int [1:2107] 1 2 3 4 5 6 7 8 9 12 ...\nstr(folds[[3]])\n#>  int [1:2106] 2 4 5 7 8 9 10 11 12 14 ...\nstr(folds[[4]])\n#>  int [1:2105] 1 2 3 4 6 7 8 9 10 11 ...\nstr(folds[[5]])\n#>  int [1:2105] 1 2 3 4 5 6 7 9 10 11 ...\n\n\nCalculation for Fold 1\nThe first fold is used as an example. The indices for the training data in the first fold are extracted and used to subset the main data set into training and test sets for that fold. Then a linear regression model is fitted using the training data, and predictions are made on the test set. The model’s performance is evaluated using the same performance function as before.\n\nShow the codefold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1]  1  3  5  6  8 10\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\nmodel.fit <- lm(formula4, data = fold1.train)\npredictions <- predict(model.fit, newdata = fold1.test)\n\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual      SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 527 29         498 637317.5 830983.2 0.233  0.19 35.774 -2618.471 5296.942\n#>           BIC\n#> [1,] 5424.958\n\n\nCalculation for Fold 2\nThe same process is repeated for the second fold. This way, you can manually evaluate how the model performs on different subsets of the data, making the performance assessment more robust.\n\nShow the codefold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 3 4 5 6\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\n\nmodel.fit <- lm(formula4, data = fold1.train)\n\npredictions <- predict(model.fit, newdata = fold1.test)\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual    SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 525 29         496 615243 785326.6 0.217 0.172 35.219 -2600.282 5260.564\n#>           BIC\n#> [1,] 5388.466\n\n\nUsing caret package to automate\n\n\nSee Kuhn (2023)\nInstead of manually running the process for each fold, the caret package can be used to automate k-fold cross-validation. A control object is set up specifying that 5-fold cross-validation should be used. Then, the train function from the caret package can be used to fit the linear regression model on each fold.\nAfter fitting, you can access summary results for each fold in the resampling results. This summary provides performance metrics such as R-squared for each fold. You can calculate the mean and standard deviation of these metrics to get an overall sense of the model’s performance.\nAdditionally, an adjusted R-squared can be calculated to consider the number of predictors in the model, giving a more accurate sense of the model’s explanatory power when you have multiple predictors.\n\nShow the code# Using Caret package\nset.seed(567)\n\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n\n# fit the model with formula = formula4\n# use training method lm\nfit4.cv<-train(formula4, trControl = ctrl,\n               data = analytic3, method = \"lm\")\nfit4.cv\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2105, 2106, 2105, 2106 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.62758  0.2194187  27.85731\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# extract results from each test data \nsummary.res <- fit4.cv$resample\nsummary.res\n\n\n\n  \n\n\nShow the codemean(fit4.cv$resample$Rsquared)\n#> [1] 0.2194187\nsd(fit4.cv$resample$Rsquared)\n#> [1] 0.02755561\n\n# # extract adj R2\n# k <- 5\n# p <- 2\n# n <- round(nrow(analytic3)/k)\n# summary.res$adjR2 <- 1-(1-fit4.cv$resample$Rsquared)*((n-1)/(n-p))\n# summary.res\n\n\nReferences\n\n\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "predictivefactors7.html",
    "href": "predictivefactors7.html",
    "title": "Bootstrap",
    "section": "",
    "text": "The tutorial is on bootstrapping methods, mainly using R. Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling, with replacement, from the observed data points. It is a way to quantify the uncertainty associated with a given estimator or statistical measure, such as the mean, median, variance, or correlation coefficient, among others. Bootstrapping is widely applicable and very straightforward to implement, which has made it a popular choice for statistical inference when analytical solutions are not available or are difficult to derive.\n\n\n\n\n\n\nImportant\n\n\n\nBootstrapping is a powerful statistical tool for making inferences by empirically estimating the sampling distribution of a statistic. It is especially useful when the underlying distribution is unknown or when an analytical solution is difficult to obtain.\n\n\n\nShow the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nResampling a vector\nHere, the document introduces basic resampling of a simple vector. The code creates a new sample using the sample function with replacement. It also discusses “out-of-bag” samples which are the samples not chosen during the resampling.\n\nShow the codefake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n\n\n\nShow the coderesampled.fake.data <- sample(fake.data, size = length(fake.data), replace = TRUE)\nresampled.fake.data\n#> [1] 4 1 3 3 1\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 4 1 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 2 5\n\n\nThe samples not selected are known as the out-of-bag samples\n\nShow the codeB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 1 4 4 1 4\n#> [1] 5 3 1 3 3\n#> [1] 2 5 4 5 3\n#> [1] 4 4 3 5 4\n#> [1] 4 5 3 5 5\n#> [1] 3 5 3 3 2\n#> [1] 4 4 5 3 2\n#> [1] 1 4 2 4 3\n#> [1] 2 5 5 4 4\n#> [1] 2 3 3 5 4\n\n\nCalculating SD of a statistics\nWe introduce the concept of calculating confidence intervals (CIs) using bootstrapping when the distribution of data is not known. It uses resampling to create multiple bootstrap samples, then calculates means and standard deviations (SD) for those samples.\nIdea:\n\nNot sure about what distribution is appropriate to make inference?\nIf that is the case, calculating CI is hard.\nresample and get a new bootstrap sample\ncalculate a statistic (say, mean) from that sample\nfind SD of those statistic (say, means)\nUse those SD to calculate CI\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 4 4 2 3 5\n#>  $ : int [1:5] 4 2 2 1 3\n#>  $ : int [1:5] 2 2 1 2 2\n#>  $ : int [1:5] 4 4 3 1 4\n#>  $ : int [1:5] 3 2 4 4 2\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 3.6 2.4 1.8 3.2 3.0\nmean(B.means)\n#> [1] 2.8\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.7071068\n\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 3.018\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6366337\nmean(B.medians)\n#> [1] 3.05\nhist(B.means)\n\n\n\nShow the code\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 0.996224\nhist(B.medians)\n\n\n\n\nResampling a data or matrix\nWe show how to resample a data frame or a matrix, and how to identify which rows have been selected and which haven’t, introducing the concept of “out-of-bag samples” for matrices.\n\nShow the codeanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n1\n83732\nMale\n62\n\n\n2\n83733\nMale\n53\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n21\n83752\nFemale\n30\n\n\n\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n1\n83732\nMale\n62\n\n\n21.1\n83752\nFemale\n30\n\n\n1.1\n83732\nMale\n62\n\n\n1.2\n83732\nMale\n62\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83732\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83741 83747 83750\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n21.1\n83752\nFemale\n30\n\n\n21.2\n83752\nFemale\n30\n\n\n21.3\n83752\nFemale\n30\n\n\n19\n83750\nMale\n45\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83750\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732 83741 83747\n\n\nThe caret package / boot\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time). We introduce the trainControl and train functions from the caret package. It sets up a linear model and demonstrates how bootstrapping can be done to estimate the variability in R-squared, a measure of goodness-of-fit for the model.\n\nShow the codeset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n\n\nMethod boot632\nA specific bootstrapping method called “boot632”, which aims to reduce bias but can provide unstable results if the sample size is small.\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n\n\nMethod boot632 for stepwise\nWe discuss the use of stepwise regression models in conjunction with the “boot632” method. It highlights the trade-offs and explains that models could be unstable depending on the data.\nA stable model\n\n\nSee Kuhn (2023)\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\nShow the codectrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n\n\nAn unstable model\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n\n\nNote that SD should be higher for larger B.\nOptimism corrected bootstrap\nWe discuss a specific type of bootstrap called the “Optimism corrected bootstrap”. It’s a way to adjust performance metrics for the optimism that is often present when a model is tested on the data used to create it.\n\n\nSee Bondarenko and Consulting (2023)\nSteps:\n\nFit a model M to entire data D and estimate predictive ability R2.\nIterate from b=1 to B:\n\nTake a resample from the original data, and name it D.star\nFit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\nUse the bootstrap model M.star to get predictive ability on D, R2.fullData\n\n\nOptimism Opt is calculated as mean(R2.boot - R2.fullData)\nCalculate optimism corrected performance as R2-Opt.\n\n\nShow the codeR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n\n\nBinary outcome\nHere, bootstrapping and cross-validation are used for a logistic regression model. It calculates the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), a measure for the performance of classification models.\nAUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\nAUC = 1 (perfect classification) AUC = 0.5 (random classification)\n\nShow the codeset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n\n\n\n  \n\n\nShow the codemean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n\n\nBrier Score is another metric for evaluating the performance of binary classification models.\n\nShow the coderequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nBondarenko, Vadim, and FI Consulting. 2023. “The Bootstrap Approach to Managing Model Uncertainty.” https://rstudio-pubs-static.s3.amazonaws.com/90467_c70206f3dc864d53bf36072207ee011d.html.\n\n\nKuhn, Max. 2023. “Available Models.” https://topepo.github.io/caret/available-models.html."
  },
  {
    "objectID": "predictivefactorsF.html",
    "href": "predictivefactorsF.html",
    "title": "R functions (P)",
    "section": "",
    "text": "The list of new R functions introduced in this Predictive factors lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggregate \n    base/stats \n    To see summary by groups, e.g., by gender \n  \n\n anova \n    base/stats \n    To compare models \n  \n\n auc \n    pROC \n    To compute the AUC (area under the ROC curve) value \n  \n\n BrierScore \n    DescTools \n    To calculate the Brier score \n  \n\n coef \n    base/stats \n    To see the coefficients of a fitted model \n  \n\n cor \n    base/stats \n    To see the correlation between numeric variables \n  \n\n corrplot \n    corrplot \n    To visualize a correlation matrix \n  \n\n createDataPartition \n    caret \n    To split a dataset into training and testing sets \n  \n\n createFolds \n    caret \n    To create k folds based on the outcome variable \n  \n\n crPlots \n    car \n    To see partial residual plot \n  \n\n describeBy \n    psych \n    To see summary by groups, e.g., by gender \n  \n\n glm \n    base/stats \n    To run generalized linear models \n  \n\n group_by \n    dplyr \n    To group by variables \n  \n\n hat \n    base/stats \n    To return a hat matrix \n  \n\n ifelse \n    base \n    To set an condition, e.g., creating a categorical variable from a numerical variable based on a condition \n  \n\n kable \n    knitr \n    To create a nice table \n  \n\n layout \n    base/graphics \n    To specify plot arrangement \n  \n\n lines \n    base/graphics \n    To draw a line graph \n  \n\n lm \n    base/stats \n    To fit a linear regression \n  \n\n lowess \n    base/stats \n    To smooth a scatter plot \n  \n\n model.matrix \n    base/stats \n    To construct a design/model matrix, e.g., a matrix with covariate values \n  \n\n ols_plot_resid_lev \n    olsrr \n    To visualize the residuals vs leverage plot \n  \n\n ols_vif_tol \n    olsrr \n    To calculate tolerance and variance inflation factor \n  \n\n predict \n    base/stats \n    `predict` is a generic function that is used for prediction, e.g., predicting probability of an event from a model \n  \n\n R2 \n    caret \n    To calculate the R-squared value \n  \n\n RMSE \n    caret \n    To calculate the RMSE value \n  \n\n roc \n    pROC \n    To build a ROC curve \n  \n\n sample \n    base \n    To take/draw random samples with or without replacement \n  \n\n save.image \n    base \n    To save an R object \n  \n\n spearman2 \n    Hmisc \n    To compute the square of Spearman's rank correlation \n  \n\n summarize \n    dplyr \n    To see summary \n  \n\n tapply \n    base \n    To apply a function over an array, e.g., to see the summary of a variable by gender \n  \n\n train \n    caret \n    To fit the model with tuning hyperparameters \n  \n\n trainControl \n    caret \n    To tune the hyperparameters, i.e., controlling the parameters to train the model \n  \n\n varclus \n    Hmisc \n    We use the `varclus` function to identify collinear predictors with cluster analysis \n  \n\n vif \n    car \n    To calculate variance inflation factor \n  \n\n which \n    base \n    To see which indices are TRUE"
  },
  {
    "objectID": "predictivefactorsQ.html",
    "href": "predictivefactorsQ.html",
    "title": "Quiz (P)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydata.html#background",
    "href": "surveydata.html#background",
    "title": "Survey data analysis",
    "section": "Background",
    "text": "Background\nThe chapter consists of a series of tutorials focused on conducting rigorous analyses of complex survey data, mainly using Canadian Community Health Survey (CCHS) and National Health and Nutrition Examination Survey (NHANES) datasets. The tutorials guide users through various stages of survey data analysis: formulating research questions via the PICOT framework, data preparation, quality assessment, and handling missing data. They cover both bivariate and multivariable statistical methods, such as logistic and linear regressions, emphasizing the need to account for complex survey design elements like weights, strata, and clusters to avoid biased estimates. Advanced statistical techniques like backward elimination and interaction effect assessments are also discussed. Predictive model performance is evaluated using metrics like AIC, pseudo R-squared, and ROC curves, along with specialized tests like Archer and Lemeshow Goodness of Fit. The tutorials serve as a comprehensive guide for anyone looking to delve deep into the intricacies of analyzing complex survey data effectively.\n\n\nThe foundation we’ve built in understanding various research questions, especially the distinction between causal and predictive inquiries, will be instrumental in our next phase. Survey data, with its rich and diverse information, often presents opportunities to address both causal and predictive questions. By leveraging the knowledge we’ve garnered about the intricacies of causality and the methodologies of prediction, we’ll be better equipped to extract meaningful insights from nationally representative survey data. This holistic approach ensures that we not only comprehend the underlying theories but also effectively apply them in real-world contexts, making our analysis robust, relevant, and impactful.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "surveydata.html#overview-of-tutorials",
    "href": "surveydata.html#overview-of-tutorials",
    "title": "Survey data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCCHS: Revisiting PICOT\nThe tutorial focuses on revisiting a research question concerning the relationship between osteoarthritis (OA) and cardiovascular disease (CVD) in Canadian adults, utilizing data from the Canadian Community Health Survey (CCHS) spanning from 2001 to 2005. The approach follows the PICOT framework, which specifies the target population, outcome, exposure and control groups, and timeline. The tutorial provides detailed steps for data preparation and analysis, from loading the necessary R packages to subsetting data based on a comprehensive set of variables like age, sex, marital status, and income among others. The variables are recoded into broader categories for easier analysis. The tutorial then combines different cycles of CCHS data into one comprehensive dataset. Potential confounders are also identified to better understand the relationship between OA and CVD.\n\n\nCCHS: Assessing data\nThis tutorial provides a comprehensive guide to data preparation and quality assessment. It emphasizes the importance of checking for missing data and visualizing it, creating summary tables to look for zero-cells in variables, and generating frequency tables for various variables to examine data distribution. Specific attention is given to the presence of problematic variables. Data dictionaries from different cycles are also consulted to ensure variable compatibility. After identifying and modifying problematic data, the tutorial also explains how to set appropriate reference levels for factors in the dataset and offers an option to create a new dataset that excludes missing values (although this is not generally recommended).\n\n\nCCHS: Bivariate analysis\nThis tutorial outlines how to examine relationships between two variables using R. The tutorial covers data preparation steps such as accumulating survey weights across cycles. It also highlights the handling of missing data and survey design specifications for weighted analyses. Descriptive weighted statistics are generated in tables, stratified by exposure and outcome, to provide insights for survey weighted logistic regression analysis. Additionally, proportions and design effects are calculated to account for the complex survey design’s impact on statistical estimates. The tutorial employs specialized chi-square tests, such as, Rao-Scott and Thomas-Rao modifications, to assess associations between variables, accounting for the survey’s complex design.\n\n\nCCHS: Regression analysis\nThis tutorial offers a comprehensive guide on conducting complex regression analyses on survey data using R. The tutorial starts by conducting basic data checks. It then performs both simple and multivariable logistic regression to explore the relationship between cardiovascular disease and osteoarthritis. Model fit is assessed using Akaike Information Criterion (AIC) and pseudo R-squared metrics. Variable selection techniques such as backward elimination and stepwise regression guided by AIC are applied to hone the model. The tutorial also delves into assessing interaction effects among variables like age, sex, and diabetes, incorporating significant interactions into the final model.\n\n\nCCHS: Model performance\nThe tutorial guides users through the process of evaluating logistic regression models fitted to complex survey data in R, focusing primarily on the Receiver Operating Characteristic (ROC) curves and Archer and Lemeshow Goodness of Fit tests. It introduces a specialized function for plotting ROC curves and calculating the Area Under the Curve (AUC) to gauge the model’s predictive accuracy, while taking survey weights into account. Grading guidelines for AUC values are provided for model discrimination quality. For model fit, the Archer and Lemeshow test is used. The tutorial also covers additional functionalities for dealing with strata and clusters in the survey data.\n\n\nNHANES: Predicting blood pressure\nThe tutorial provides a comprehensive guide for analyzing health survey data with a focus on how demographic factors like race, age, gender, and marital status relate to blood pressure levels using NHANES dataset. The tutorial constructs both bivariate and multivariate regression models. Additionally, the tutorial incorporates complex survey designs by creating a new survey design object that factors in sampling weight, strata, and clusters. It also generates box plots and summary statistics to visualize variations in blood pressure across different demographic groups, considering survey design. The tutorial emphasizes the importance of accounting for survey design features to avoid biased estimates and discusses the challenges of model overfitting and optimism when shifting from inference to prediction, recommending optimism-correction techniques.\n\n\nNHANES: Predicting cholesterol level\nIn the study using NHANES data, the goal was to predict cholesterol levels in adults based on various predictors such as gender, country of birth, race, education, marital status, income, BMI, and diabetes. The data was filtered to include only adults 18 years and older, and multiple statistical tests were conducted. Linear regression and logistic regression models were fitted, with results suggesting an association between gender and cholesterol level. Various statistical tests, including Wald tests and backward elimination, were employed to optimize the model. The study found that income was not a significant predictor for cholesterol levels, and interaction terms did not improve the model. Despite utilizing survey design features, the model had poor discriminatory power. However, Archer-Lemeshow Goodness of Fit test showed that the model fits the data well. The inclusion of age as an additional predictor led to different odds ratios, and the AIC value suggested that adding age improved the model.\n\n\nNHANES: Properly subsetting a design object\nThe tutorial provides a comprehensive guide on how to handle and analyze a subset of complex survey data from the NHANES study using R. It begins by checking for missing data. The focus is on subsetting data based on complete information, emphasizing the importance of accounting for the full complex survey design to obtain unbiased variance estimates. Logistic regression is then run on this subset, with the tutorial explicitly differentiating between correct and incorrect approaches to consider the survey design. Finally, variable selection methods like backward elimination are discussed to determine significant predictors, emphasizing the retention of variables deemed important based on prior research.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "surveydata1.html#references",
    "href": "surveydata1.html#references",
    "title": "CCHS: Revisiting PICOT",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nCanada, Statistics. 2005. “Canadian Community Health Survey (CCHS), Cycle 3.1.” Author Ottawa.\n\n\nKarim, Ehsan. 2023. “Case Study 2: Risk of Cardiovascular Disease Among Osteoarthritis Patients.” https://ssc.ca/en/case-study/case-study-2-risk-cardiovascular-disease-among-osteoarthritis-patients.\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "surveydata2.html",
    "href": "surveydata2.html",
    "title": "CCHS: Assessing data",
    "section": "",
    "text": "Let us load all the necessary packages for data manipulation, statistical analysis, and plotting.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\n\n\nLoad data\nData loading that we saved earlier:\n\nShow the codeload(\"Data/surveydata/cchs123.RData\")\nls()\n#> [1] \"analytic\" \"cc123a\"\n\n\nChecking\nCheck the data for missingness\nChecks the dimensions of the data and runs functions to explore missing data, stratifying by some variables. Additionally, it plots the missing data for visualization.\n\nShow the codedim(analytic)\n#> [1] 397173     24\nrequire(\"tableone\")\n#CreateTableOne(data = analytic, includeNA = TRUE)\nCreateTableOne(data = analytic, strata = \"CVD\", includeNA = TRUE)\n#>                       Stratified by CVD\n#>                        event                 no event              p      test\n#>   n                        25524                371121                        \n#>   CVD (%)                                                             NaN     \n#>      event                 25524 (100.0)             0 (  0.0)                \n#>      no event                  0 (  0.0)        371121 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years             330 (  1.3)         48293 ( 13.0)                \n#>      30-39 years             580 (  2.3)         63194 ( 17.0)                \n#>      40-49 years            1498 (  5.9)         63549 ( 17.1)                \n#>      50-59 years            3635 ( 14.2)         57300 ( 15.4)                \n#>      60-64 years            2720 ( 10.7)         22497 (  6.1)                \n#>      65 years and over     16496 ( 64.6)         64198 ( 17.3)                \n#>      teen                    265 (  1.0)         52090 ( 14.0)                \n#>   sex = Male (%)           12506 ( 49.0)        169776 ( 45.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single            13287 ( 52.1)        188687 ( 50.8)                \n#>      single                12207 ( 47.8)        181811 ( 49.0)                \n#>      NA                       30 (  0.1)           623 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white              1276 (  5.0)         37323 ( 10.1)                \n#>      White                 23629 ( 92.6)        325178 ( 87.6)                \n#>      NA                      619 (  2.4)          8620 (  2.3)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              11547 ( 45.2)        112678 ( 30.4)                \n#>      2nd grad.              3310 ( 13.0)         61355 ( 16.5)                \n#>      Other 2nd grad.        1323 (  5.2)         27643 (  7.4)                \n#>      Post-2nd grad.         8744 ( 34.3)        163052 ( 43.9)                \n#>      NA                      600 (  2.4)          6393 (  1.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       11664 ( 45.7)         89506 ( 24.1)                \n#>      $30,000-$49,999        4871 ( 19.1)         72994 ( 19.7)                \n#>      $50,000-$79,999        3193 ( 12.5)         81861 ( 22.1)                \n#>      $80,000 or more        1905 (  7.5)         73768 ( 19.9)                \n#>      NA                     3891 ( 15.2)         52992 ( 14.3)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight             504 (  2.0)          9600 (  2.6)                \n#>      healthy weight         7176 ( 28.1)        141200 ( 38.0)                \n#>      Overweight            12104 ( 47.4)        153887 ( 41.5)                \n#>      NA                     5740 ( 22.5)         66434 ( 17.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                 3642 ( 14.3)         94844 ( 25.6)                \n#>      Inactive              15494 ( 60.7)        174976 ( 47.1)                \n#>      Moderate               4928 ( 19.3)         88480 ( 23.8)                \n#>      NA                     1460 (  5.7)         12821 (  3.5)                \n#>   doctor (%)                                                       <0.001     \n#>      No                     1134 (  4.4)         57425 ( 15.5)                \n#>      Yes                   24384 ( 95.5)        313282 ( 84.4)                \n#>      NA                        6 (  0.0)           414 (  0.1)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed      20041 ( 78.5)        266358 ( 71.8)                \n#>      stressed               5184 ( 20.3)         76986 ( 20.7)                \n#>      NA                      299 (  1.2)         27777 (  7.5)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker         4481 ( 17.6)         93253 ( 25.1)                \n#>      Former smoker         13927 ( 54.6)        143421 ( 38.6)                \n#>      Never smoker           6981 ( 27.4)        132891 ( 35.8)                \n#>      NA                      135 (  0.5)          1556 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker       15852 ( 62.1)        279583 ( 75.3)                \n#>      Former driker          6820 ( 26.7)         48373 ( 13.0)                \n#>      Never drank            2421 (  9.5)         38195 ( 10.3)                \n#>      NA                      431 (  1.7)          4970 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving      4284 ( 16.8)         79088 ( 21.3)                \n#>      4-6 daily serving     10527 ( 41.2)        148684 ( 40.1)                \n#>      6+ daily serving       5047 ( 19.8)         73729 ( 19.9)                \n#>      NA                     5666 ( 22.2)         69620 ( 18.8)                \n#>   bp (%)                                                           <0.001     \n#>      No                    12611 ( 49.4)        315344 ( 85.0)                \n#>      Yes                   12857 ( 50.4)         55037 ( 14.8)                \n#>      NA                       56 (  0.2)           740 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                    23378 ( 91.6)        267481 ( 72.1)                \n#>      Yes                    1449 (  5.7)          3043 (  0.8)                \n#>      NA                      697 (  2.7)        100597 ( 27.1)                \n#>   diab (%)                                                         <0.001     \n#>      No                    20461 ( 80.2)        353817 ( 95.3)                \n#>      Yes                    5038 ( 19.7)         17138 (  4.6)                \n#>      NA                       25 (  0.1)           166 (  0.0)                \n#>   province = South (%)     25271 ( 99.0)        363659 ( 98.0)     <0.001     \n#>   weight (mean (SD))      152.58 (181.69)       203.40 (244.28)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                     7968 ( 31.2)        122798 ( 33.1)                \n#>      21                     9027 ( 35.4)        124838 ( 33.6)                \n#>      31                     8529 ( 33.4)        123485 ( 33.3)                \n#>   ID (mean (SD))       199839.07 (114705.35) 198466.74 (114661.51)  0.064     \n#>   OA (%)                                                           <0.001     \n#>      Control               12655 ( 49.6)        301675 ( 81.3)                \n#>      OA                     6522 ( 25.6)         34346 (  9.3)                \n#>      NA                     6347 ( 24.9)         35100 (  9.5)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years             2295 (  9.0)         24409 (  6.6)                \n#>      not immigrant         21342 ( 83.6)        316353 ( 85.2)                \n#>      recent                  159 (  0.6)         10476 (  2.8)                \n#>      NA                     1728 (  6.8)         19883 (  5.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND            519 (  2.0)          7398 (  2.0)                \n#>      PEI                     567 (  2.2)          7172 (  1.9)                \n#>      NOVA SCOTIA            1308 (  5.1)         14015 (  3.8)                \n#>      NEW BRUNSWICK          1223 (  4.8)         13786 (  3.7)                \n#>      QU\\xc9BEC              1380 (  5.4)         20625 (  5.6)                \n#>      ONTARIO                8596 ( 33.7)        115053 ( 31.0)                \n#>      MANITOBA               1339 (  5.2)         22074 (  5.9)                \n#>      SASKATCHEWAN           1542 (  6.0)         21782 (  5.9)                \n#>      ALBERTA                1837 (  7.2)         38238 ( 10.3)                \n#>      BRITISH COLUMBIA       2847 ( 11.2)         46834 ( 12.6)                \n#>      YUKON/NWT/NUNAVT        173 (  0.7)          4884 (  1.3)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                 3839 ( 15.0)         52850 ( 14.2)                \n#>      NFLD & LAB.             274 (  1.1)          3832 (  1.0)                \n#>      YUKON/NWT/NUNA.          80 (  0.3)          2578 (  0.7)\nCreateTableOne(data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control               OA                    p      test\n#>   n                       314542                 40943                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 12655 (  4.0)          6522 ( 15.9)                \n#>      no event             301675 ( 95.9)         34346 ( 83.9)                \n#>      NA                      212 (  0.1)            75 (  0.2)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           46805 ( 14.9)           537 (  1.3)                \n#>      30-39 years           59233 ( 18.8)          1622 (  4.0)                \n#>      40-49 years           55598 ( 17.7)          4128 ( 10.1)                \n#>      50-59 years           43746 ( 13.9)          8994 ( 22.0)                \n#>      60-64 years           15772 (  5.0)          5100 ( 12.5)                \n#>      65 years and over     41661 ( 13.2)         20436 ( 49.9)                \n#>      teen                  51727 ( 16.4)           126 (  0.3)                \n#>   sex = Male (%)          153889 ( 48.9)         11627 ( 28.4)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           158065 ( 50.3)         21794 ( 53.2)                \n#>      single               155952 ( 49.6)         19099 ( 46.6)                \n#>      NA                      525 (  0.2)            50 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             34028 ( 10.8)          1803 (  4.4)                \n#>      White                273378 ( 86.9)         38241 ( 93.4)                \n#>      NA                     7136 (  2.3)           899 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              92831 ( 29.5)         14539 ( 35.5)                \n#>      2nd grad.             52077 ( 16.6)          6291 ( 15.4)                \n#>      Other 2nd grad.       24099 (  7.7)          2484 (  6.1)                \n#>      Post-2nd grad.       140400 ( 44.6)         16887 ( 41.2)                \n#>      NA                     5135 (  1.6)           742 (  1.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       68530 ( 21.8)         16233 ( 39.6)                \n#>      $30,000-$49,999       61697 ( 19.6)          8360 ( 20.4)                \n#>      $50,000-$79,999       72657 ( 23.1)          6348 ( 15.5)                \n#>      $80,000 or more       67458 ( 21.4)          4191 ( 10.2)                \n#>      NA                    44200 ( 14.1)          5811 ( 14.2)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8660 (  2.8)           715 (  1.7)                \n#>      healthy weight       123416 ( 39.2)         12631 ( 30.9)                \n#>      Overweight           123898 ( 39.4)         20715 ( 50.6)                \n#>      NA                    58568 ( 18.6)          6882 ( 16.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                84269 ( 26.8)          6968 ( 17.0)                \n#>      Inactive             143058 ( 45.5)         23604 ( 57.7)                \n#>      Moderate              75703 ( 24.1)          9176 ( 22.4)                \n#>      NA                    11512 (  3.7)          1195 (  2.9)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53335 ( 17.0)          2221 (  5.4)                \n#>      Yes                  260802 ( 82.9)         38717 ( 94.6)                \n#>      NA                      405 (  0.1)             5 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     223212 ( 71.0)         31769 ( 77.6)                \n#>      stressed              63923 ( 20.3)          8998 ( 22.0)                \n#>      NA                    27407 (  8.7)           176 (  0.4)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        79521 ( 25.3)          8087 ( 19.8)                \n#>      Former smoker        117745 ( 37.4)         20267 ( 49.5)                \n#>      Never smoker         116006 ( 36.9)         12428 ( 30.4)                \n#>      NA                     1270 (  0.4)           161 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      239223 ( 76.1)         28622 ( 69.9)                \n#>      Former driker         37042 ( 11.8)          8668 ( 21.2)                \n#>      Never drank           34185 ( 10.9)          3128 (  7.6)                \n#>      NA                     4092 (  1.3)           525 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     68629 ( 21.8)          6571 ( 16.0)                \n#>      4-6 daily serving    125177 ( 39.8)         17214 ( 42.0)                \n#>      6+ daily serving      62121 ( 19.7)          9123 ( 22.3)                \n#>      NA                    58615 ( 18.6)          8035 ( 19.6)                \n#>   bp (%)                                                           <0.001     \n#>      No                   275443 ( 87.6)         25551 ( 62.4)                \n#>      Yes                   38442 ( 12.2)         15341 ( 37.5)                \n#>      NA                      657 (  0.2)            51 (  0.1)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213719 ( 67.9)         39007 ( 95.3)                \n#>      Yes                    2131 (  0.7)          1214 (  3.0)                \n#>      NA                    98692 ( 31.4)           722 (  1.8)                \n#>   diab (%)                                                         <0.001     \n#>      No                   301943 ( 96.0)         36211 ( 88.4)                \n#>      Yes                   12442 (  4.0)          4705 ( 11.5)                \n#>      NA                      157 (  0.0)            27 (  0.1)                \n#>   province = South (%)    307761 ( 97.8)         40507 ( 98.9)     <0.001     \n#>   weight (mean (SD))      211.50 (251.46)       159.00 (188.84)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106231 ( 33.8)         12052 ( 29.4)                \n#>      21                   104530 ( 33.2)         14750 ( 36.0)                \n#>      31                   103781 ( 33.0)         14141 ( 34.5)                \n#>   ID (mean (SD))       197003.20 (115147.95) 204459.43 (113014.25) <0.001     \n#>   OA (%)                                                              NaN     \n#>      Control              314542 (100.0)             0 (  0.0)                \n#>      OA                        0 (  0.0)         40943 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            19385 (  6.2)          3622 (  8.8)                \n#>      not immigrant        268962 ( 85.5)         34509 ( 84.3)                \n#>      recent                10187 (  3.2)           151 (  0.4)                \n#>      NA                    16008 (  5.1)          2661 (  6.5)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6315 (  2.0)           725 (  1.8)                \n#>      PEI                    5892 (  1.9)           817 (  2.0)                \n#>      NOVA SCOTIA           11081 (  3.5)          1880 (  4.6)                \n#>      NEW BRUNSWICK         11517 (  3.7)          1693 (  4.1)                \n#>      QU\\xc9BEC             19111 (  6.1)          2035 (  5.0)                \n#>      ONTARIO               95651 ( 30.4)         13669 ( 33.4)                \n#>      MANITOBA              18050 (  5.7)          2272 (  5.5)                \n#>      SASKATCHEWAN          17941 (  5.7)          2166 (  5.3)                \n#>      ALBERTA               32207 ( 10.2)          3608 (  8.8)                \n#>      BRITISH COLUMBIA      40034 ( 12.7)          4873 ( 11.9)                \n#>      YUKON/NWT/NUNAVT       4446 (  1.4)           273 (  0.7)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                46817 ( 14.9)          6366 ( 15.5)                \n#>      NFLD & LAB.            3145 (  1.0)           403 (  1.0)                \n#>      YUKON/NWT/NUNA.        2335 (  0.7)           163 (  0.4)\n\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLook for zero-cells\nCreates two new variables based on age groups and generates summary tables. It also comments on the presence of ‘zero cells’ in one of the variables, which might require further handling.\n\nShow the codeanalytic$age.65p <- analytic$age.teen <- 0\nanalytic$age.teen[analytic$age == \"teen\"] <- 1\nanalytic$age.65p[analytic$age == \"65 years and over\"] <- 1\nCreateTableOne(data = analytic, strata = \"age.teen\", includeNA = TRUE)\n#>                       Stratified by age.teen\n#>                        0                     1                     p      test\n#>   n                       344786                 52387                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 25259 ( 7.3)            265 (  0.5)                \n#>      no event             319031 (92.5)          52090 ( 99.4)                \n#>      NA                      496 ( 0.1)             32 (  0.1)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (14.1)              0 (  0.0)                \n#>      30-39 years           63810 (18.5)              0 (  0.0)                \n#>      40-49 years           65111 (18.9)              0 (  0.0)                \n#>      50-59 years           61035 (17.7)              0 (  0.0)                \n#>      60-64 years           25265 ( 7.3)              0 (  0.0)                \n#>      65 years and over     80913 (23.5)              0 (  0.0)                \n#>      teen                      0 ( 0.0)          52387 (100.0)                \n#>   sex = Male (%)          155980 (45.2)          26543 ( 50.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           201528 (58.5)            685 (  1.3)                \n#>      single               142639 (41.4)          51660 ( 98.6)                \n#>      NA                      619 ( 0.2)             42 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             31107 ( 9.0)           7534 ( 14.4)                \n#>      White                305497 (88.6)          43725 ( 83.5)                \n#>      NA                     8182 ( 2.4)           1128 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              83649 (24.3)          40776 ( 77.8)                \n#>      2nd grad.             59205 (17.2)           5548 ( 10.6)                \n#>      Other 2nd grad.       24580 ( 7.1)           4420 (  8.4)                \n#>      Post-2nd grad.       170707 (49.5)           1265 (  2.4)                \n#>      NA                     6645 ( 1.9)            378 (  0.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       93630 (27.2)           7701 ( 14.7)                \n#>      $30,000-$49,999       69798 (20.2)           8142 ( 15.5)                \n#>      $50,000-$79,999       73596 (21.3)          11512 ( 22.0)                \n#>      $80,000 or more       63697 (18.5)          12018 ( 22.9)                \n#>      NA                    44065 (12.8)          13014 ( 24.8)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            7277 ( 2.1)           2839 (  5.4)                \n#>      healthy weight       138611 (40.2)           9922 ( 18.9)                \n#>      Overweight           163701 (47.5)           2520 (  4.8)                \n#>      NA                    35197 (10.2)          37106 ( 70.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                74738 (21.7)          23833 ( 45.5)                \n#>      Inactive             176573 (51.2)          14166 ( 27.0)                \n#>      Moderate              82158 (23.8)          11349 ( 21.7)                \n#>      NA                    11317 ( 3.3)           3039 (  5.8)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    49874 (14.5)           8749 ( 16.7)                \n#>      Yes                  294763 (85.5)          43342 ( 82.7)                \n#>      NA                      149 ( 0.0)            296 (  0.6)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     265391 (77.0)          21353 ( 40.8)                \n#>      stressed              78044 (22.6)           4253 (  8.1)                \n#>      NA                     1351 ( 0.4)          26781 ( 51.1)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88986 (25.8)           8866 ( 16.9)                \n#>      Former smoker        150004 (43.5)           7566 ( 14.4)                \n#>      Never smoker         104332 (30.3)          35685 ( 68.1)                \n#>      NA                     1464 ( 0.4)            270 (  0.5)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      268269 (77.8)          27464 ( 52.4)                \n#>      Former driker         50929 (14.8)           4370 (  8.3)                \n#>      Never drank           20754 ( 6.0)          19916 ( 38.0)                \n#>      NA                     4834 ( 1.4)            637 (  1.2)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72392 (21.0)          11049 ( 21.1)                \n#>      4-6 daily serving    139753 (40.5)          19627 ( 37.5)                \n#>      6+ daily serving      66831 (19.4)          12026 ( 23.0)                \n#>      NA                    65810 (19.1)           9685 ( 18.5)                \n#>   bp (%)                                                           <0.001     \n#>      No                   276318 (80.1)          51846 ( 99.0)                \n#>      Yes                   67763 (19.7)            308 (  0.6)                \n#>      NA                      705 ( 0.2)            233 (  0.4)                \n#>   copd (%)                                                         <0.001     \n#>      No                   291191 (84.5)              0 (  0.0)                \n#>      Yes                    4508 ( 1.3)              0 (  0.0)                \n#>      NA                    49087 (14.2)          52387 (100.0)                \n#>   diab (%)                                                         <0.001     \n#>      No                   322448 (93.5)          52141 ( 99.5)                \n#>      Yes                   22032 ( 6.4)            199 (  0.4)                \n#>      NA                      306 ( 0.1)             47 (  0.1)                \n#>   province = South (%)    338450 (98.2)          51001 ( 97.4)     <0.001     \n#>   weight (mean (SD))      201.76 (245.97)       189.09 (205.24)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   113323 (32.9)          17557 ( 33.5)                \n#>      21                   115548 (33.5)          18524 ( 35.4)                \n#>      31                   115915 (33.6)          16306 ( 31.1)                \n#>   ID (mean (SD))       199143.77 (114810.36) 194922.59 (113553.38) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              262815 (76.2)          51727 ( 98.7)                \n#>      OA                    40817 (11.8)            126 (  0.2)                \n#>      NA                    41154 (11.9)            534 (  1.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            25976 ( 7.5)            770 (  1.5)                \n#>      not immigrant        289651 (84.0)          48427 ( 92.4)                \n#>      recent                 8710 ( 2.5)           1934 (  3.7)                \n#>      NA                    20449 ( 5.9)           1256 (  2.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6646 ( 1.9)           1278 (  2.4)                \n#>      PEI                    6802 ( 2.0)            942 (  1.8)                \n#>      NOVA SCOTIA           13337 ( 3.9)           2004 (  3.8)                \n#>      NEW BRUNSWICK         13057 ( 3.8)           1968 (  3.8)                \n#>      QU\\xc9BEC             19186 ( 5.6)           2826 (  5.4)                \n#>      ONTARIO              107768 (31.3)          16053 ( 30.6)                \n#>      MANITOBA              20362 ( 5.9)           3092 (  5.9)                \n#>      SASKATCHEWAN          20160 ( 5.8)           3201 (  6.1)                \n#>      ALBERTA               34293 ( 9.9)           5834 ( 11.1)                \n#>      BRITISH COLUMBIA      43431 (12.6)           6336 ( 12.1)                \n#>      YUKON/NWT/NUNAVT       4170 ( 1.2)            894 (  1.7)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                49806 (14.4)           6958 ( 13.3)                \n#>      NFLD & LAB.            3602 ( 1.0)            509 (  1.0)                \n#>      YUKON/NWT/NUNA.        2166 ( 0.6)            492 (  0.9)                \n#>   age.teen (mean (SD))      0.00 (0.00)           1.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.23 (0.42)           0.00 (0.00)      <0.001\n# copd has zero cells\n# analytic$age[analytic$age == 'teen'] <- NA (will set this if we use copd)\n\n\n\nShow the codeCreateTableOne(data = analytic, strata = \"age.65p\", includeNA = TRUE)\n#>                       Stratified by age.65p\n#>                        0                     1                     p      test\n#>   n                       316260                 80913                        \n#>   CVD (%)                                                          <0.001     \n#>      event                  9028 ( 2.9)          16496 ( 20.4)                \n#>      no event             306923 (97.0)          64198 ( 79.3)                \n#>      NA                      309 ( 0.1)            219 (  0.3)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (15.4)              0 (  0.0)                \n#>      30-39 years           63810 (20.2)              0 (  0.0)                \n#>      40-49 years           65111 (20.6)              0 (  0.0)                \n#>      50-59 years           61035 (19.3)              0 (  0.0)                \n#>      60-64 years           25265 ( 8.0)              0 (  0.0)                \n#>      65 years and over         0 ( 0.0)          80913 (100.0)                \n#>      teen                  52387 (16.6)              0 (  0.0)                \n#>   sex = Male (%)          150152 (47.5)          32371 ( 40.0)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           163660 (51.7)          38553 ( 47.6)                \n#>      single               152077 (48.1)          42222 ( 52.2)                \n#>      NA                      523 ( 0.2)            138 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             35329 (11.2)           3312 (  4.1)                \n#>      White                274000 (86.6)          75222 ( 93.0)                \n#>      NA                     6931 ( 2.2)           2379 (  2.9)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              84832 (26.8)          39593 ( 48.9)                \n#>      2nd grad.             53974 (17.1)          10779 ( 13.3)                \n#>      Other 2nd grad.       25305 ( 8.0)           3695 (  4.6)                \n#>      Post-2nd grad.       147385 (46.6)          24587 ( 30.4)                \n#>      NA                     4764 ( 1.5)           2259 (  2.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       62513 (19.8)          38818 ( 48.0)                \n#>      $30,000-$49,999       62296 (19.7)          15644 ( 19.3)                \n#>      $50,000-$79,999       77283 (24.4)           7825 (  9.7)                \n#>      $80,000 or more       72566 (22.9)           3149 (  3.9)                \n#>      NA                    41602 (13.2)          15477 ( 19.1)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8588 ( 2.7)           1528 (  1.9)                \n#>      healthy weight       124932 (39.5)          23601 ( 29.2)                \n#>      Overweight           136225 (43.1)          29996 ( 37.1)                \n#>      NA                    46515 (14.7)          25788 ( 31.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                85384 (27.0)          13187 ( 16.3)                \n#>      Inactive             144019 (45.5)          46720 ( 57.7)                \n#>      Moderate              76602 (24.2)          16905 ( 20.9)                \n#>      NA                    10255 ( 3.2)           4101 (  5.1)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53972 (17.1)           4651 (  5.7)                \n#>      Yes                  261866 (82.8)          76239 ( 94.2)                \n#>      NA                      422 ( 0.1)             23 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     215454 (68.1)          71290 ( 88.1)                \n#>      stressed              73402 (23.2)           8895 ( 11.0)                \n#>      NA                    27404 ( 8.7)            728 (  0.9)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88068 (27.8)           9784 ( 12.1)                \n#>      Former smoker        115111 (36.4)          42459 ( 52.5)                \n#>      Never smoker         111879 (35.4)          28138 ( 34.8)                \n#>      NA                     1202 ( 0.4)            532 (  0.7)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      245156 (77.5)          50577 ( 62.5)                \n#>      Former driker         35401 (11.2)          19898 ( 24.6)                \n#>      Never drank           31888 (10.1)           8782 ( 10.9)                \n#>      NA                     3815 ( 1.2)           1656 (  2.0)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72908 (23.1)          10533 ( 13.0)                \n#>      4-6 daily serving    124621 (39.4)          34759 ( 43.0)                \n#>      6+ daily serving      61855 (19.6)          17002 ( 21.0)                \n#>      NA                    56876 (18.0)          18619 ( 23.0)                \n#>   bp (%)                                                           <0.001     \n#>      No                   282174 (89.2)          45990 ( 56.8)                \n#>      Yes                   33346 (10.5)          34725 ( 42.9)                \n#>      NA                      740 ( 0.2)            198 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213221 (67.4)          77970 ( 96.4)                \n#>      Yes                    1791 ( 0.6)           2717 (  3.4)                \n#>      NA                   101248 (32.0)            226 (  0.3)                \n#>   diab (%)                                                         <0.001     \n#>      No                   305027 (96.4)          69562 ( 86.0)                \n#>      Yes                   10974 ( 3.5)          11257 ( 13.9)                \n#>      NA                      259 ( 0.1)             94 (  0.1)                \n#>   province = South (%)    309016 (97.7)          80435 ( 99.4)     <0.001     \n#>   weight (mean (SD))      215.36 (255.33)       140.38 (160.88)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106647 (33.7)          24233 ( 29.9)                \n#>      21                   105506 (33.4)          28566 ( 35.3)                \n#>      31                   104107 (32.9)          28114 ( 34.7)                \n#>   ID (mean (SD))       197072.98 (115035.66) 204504.77 (112956.66) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              272881 (86.3)          41661 ( 51.5)                \n#>      OA                    20507 ( 6.5)          20436 ( 25.3)                \n#>      NA                    22872 ( 7.2)          18816 ( 23.3)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            17607 ( 5.6)           9139 ( 11.3)                \n#>      not immigrant        273622 (86.5)          64456 ( 79.7)                \n#>      recent                10325 ( 3.3)            319 (  0.4)                \n#>      NA                    14706 ( 4.6)           6999 (  8.7)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6665 ( 2.1)           1259 (  1.6)                \n#>      PEI                    5993 ( 1.9)           1751 (  2.2)                \n#>      NOVA SCOTIA           11896 ( 3.8)           3445 (  4.3)                \n#>      NEW BRUNSWICK         11856 ( 3.7)           3169 (  3.9)                \n#>      QU\\xc9BEC             18128 ( 5.7)           3884 (  4.8)                \n#>      ONTARIO               97660 (30.9)          26161 ( 32.3)                \n#>      MANITOBA              17967 ( 5.7)           5487 (  6.8)                \n#>      SASKATCHEWAN          17507 ( 5.5)           5854 (  7.2)                \n#>      ALBERTA               33445 (10.6)           6682 (  8.3)                \n#>      BRITISH COLUMBIA      39394 (12.5)          10373 ( 12.8)                \n#>      YUKON/NWT/NUNAVT       4765 ( 1.5)            299 (  0.4)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                45226 (14.3)          11538 ( 14.3)                \n#>      NFLD & LAB.            3279 ( 1.0)            832 (  1.0)                \n#>      YUKON/NWT/NUNA.        2479 ( 0.8)            179 (  0.2)                \n#>   age.teen (mean (SD))      0.17 (0.37)           0.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.00 (0.00)           1.00 (0.00)      <0.001\nanalytic$age.65p <- analytic$age.teen <- NULL\n\n\nProduces frequency tables for multiple variable combinations to check the distribution of the data and identify issues.\n\nShow the codetable(analytic$province.check,analytic$fruit)\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  2991              3401             1237\n#>   PEI                           2280              3654             1506\n#>   NOVA SCOTIA                   3089              4804             1974\n#>   NEW BRUNSWICK                 2989              4730             1880\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      28752             59466            30746\n#>   MANITOBA                      4561              7669             3095\n#>   SASKATCHEWAN                  4173              7390             3003\n#>   ALBERTA                      10828             18901             8251\n#>   BRITISH COLUMBIA             10726             24422            12390\n#>   YUKON/NWT/NUNAVT              1829              2045             1023\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\ntable(analytic$age)\n#> \n#>       20-29 years       30-39 years       40-49 years       50-59 years \n#>             48652             63810             65111             61035 \n#>       60-64 years 65 years and over              teen \n#>             25265             80913             52387\ntable(analytic$copd, analytic$age)\n#>      \n#>       20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   No            0       63645       64735       60203       24638\n#>   Yes           0         117         320         768         586\n#>      \n#>       65 years and over  teen\n#>   No              77970     0\n#>   Yes              2717     0\ntable(analytic$stress, analytic$age) \n#>                   \n#>                    20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   Not too stressed       37117       45494       45316       45226       20948\n#>   stressed               11472       18197       19639       15623        4218\n#>                   \n#>                    65 years and over  teen\n#>   Not too stressed             71290 21353\n#>   stressed                      8895  4253\n\n\n\nuniverse 15 + is not an issue for stress as age starts from 20\n\ncopd is problematic!\n\nCreates tables to look at the distribution of a specific variable across different cycles (time periods) of the survey. Notes differences and issues.\n\n\nfruit variable measured in an optional component (not available in all cycles)\n\n\nShow the codetable(analytic$province.check[analytic$cycle==11],\n      analytic$fruit[analytic$cycle==11])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1512              1643              689\n#>   PEI                           1084              1738              773\n#>   NOVA SCOTIA                   1732              2500             1036\n#>   NEW BRUNSWICK                 1663              2363              934\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      10437             19478             8809\n#>   MANITOBA                      2604              4214             1526\n#>   SASKATCHEWAN                  2386              3957             1387\n#>   ALBERTA                       4391              7050             2664\n#>   BRITISH COLUMBIA              4321              9350             4278\n#>   YUKON/NWT/NUNAVT               999              1014              448\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n\n\n\nShow the codetable(analytic$province.check[analytic$cycle==21],\n      analytic$fruit[analytic$cycle==21])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1479              1758              548\n#>   PEI                            615               947              344\n#>   NOVA SCOTIA                   1357              2304              938\n#>   NEW BRUNSWICK                 1326              2367              946\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       9365             20356            10933\n#>   MANITOBA                      1957              3455             1569\n#>   SASKATCHEWAN                  1787              3433             1616\n#>   ALBERTA                       3326              6376             3046\n#>   BRITISH COLUMBIA              3186              7727             4224\n#>   YUKON/NWT/NUNAVT               830              1031              575\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# a different QUEBEC spelling used\n\n\n\nShow the codetable(analytic$province.check[analytic$cycle==31],\n      analytic$fruit[analytic$cycle==31])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                     0                 0                0\n#>   PEI                            581               969              389\n#>   NOVA SCOTIA                      0                 0                0\n#>   NEW BRUNSWICK                    0                 0                0\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       8950             19632            11004\n#>   MANITOBA                         0                 0                0\n#>   SASKATCHEWAN                     0                 0                0\n#>   ALBERTA                       3111              5475             2541\n#>   BRITISH COLUMBIA              3219              7345             3888\n#>   YUKON/NWT/NUNAVT                 0                 0                0\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# The real problem!\n\n\n\nLook at data dictionaries in all cycles\n\ncycle 1.1 FVCADTOT Universe: All respondents\ncycle 2.1 FVCCDTOT Universe: All respondents\ncycle 3.1 FVCEDTOT Universe: Respondents with FVCEFOPT = 1\n\n\n\nBelow we delete or modify problematic data, and removes unnecessary variables. Checks the dimensions before and after data cleanup.\n\nShow the codedim(analytic)\n#> [1] 397173     24\nanalytic1 <- analytic\n# analytic1$South[analytic1$province.check == \"NFLD & LAB.\"] <- NA\n# analytic1$South[analytic1$province.check == \"YUKON/NWT/NUNA.\"] <- NA\n# analytic1 <- subset(analytic, province.check != \"NFLD & LAB.\" & \n#                       province.check != \"YUKON/NWT/NUNA.\" )\ndim(analytic1)\n#> [1] 397173     24\n\nanalytic1$copd <- NULL # will bring this later for missing data analysis\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n# analytic1 <- droplevels.data.frame(analytic1)\nanalytic1$province.check <- NULL # we already have simplified province variable\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n\n\nSet appropriate reference\nSave the original data (with missing values)!\n\nShow the codeanalytic.miss <- analytic1\n\n\nRelevels factors in the dataset so that a specific level is set as the reference level. This is often needed for statistical analysis.\n\nShow the codeanalytic.miss$smoke <- relevel(as.factor(analytic.miss$smoke), ref='Never smoker')\nanalytic.miss$drink <- relevel(as.factor(analytic.miss$drink), ref='Never drank')\nanalytic.miss$province <- relevel(as.factor(analytic.miss$province), ref='South')\nanalytic.miss$immigrate <- relevel(as.factor(analytic.miss$immigrate), ref='not immigrant')\n\n\nComplete data options\nCreates a new dataset that omits all rows containing any missing values. This is generally not recommended for most data analysis, as it can introduce bias.\n\nShow the code# Wrong thing to do for survey data analysis!!\nanalytic2 <- as.data.frame(na.omit(analytic1)) \ndim(analytic2)\n#> [1] 185613     22\n# tab1 <- CreateTableOne(data = analytic2, strata = \"OA\", includeNA = TRUE)\n# print(tab1, test=FALSE, showAllLevels = TRUE)\n\n\nSaving dataset\nLet us check the dimensions of multiple data objects and then save them to a file for future use.\n\nShow the codedim(cc123a)\n#> [1] 397173     25\ndim(analytic)\n#> [1] 397173     24\ndim(analytic.miss)\n#> [1] 397173     22\ndim(analytic2)\n#> [1] 185613     22\nsave(analytic.miss, analytic2, file = \"Data/surveydata/cchs123b.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata3.html",
    "href": "surveydata3.html",
    "title": "CCHS: Bivariate analysis",
    "section": "",
    "text": "The following tutorial is performing bivariate analysis on our CCHS analytic dataset to examine relationships between two variables (association question).\nWe load several R packages required for bivariate analysis, statistical tests, and data visualization.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\n\n\nLoad data\nWe load the dataset into the R environment and lists all available variables and objects.\n\nShow the codeload(\"Data/surveydata/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"\n\n\nPreparing data\nWeights\nHere, the weights of survey respondents are accumulated, to account for the combination of different cycles of the data.\n\nShow the codeanalytic.miss$weight <- analytic.miss$weight/3 # 3 cycles combined\n\n\nFixing variable types\nWe convert several variables to categorical or “factor” types, which are better suited for some statistical analysis when variables have categories.\n\nShow the codevar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"OA\", \"immigrate\")\nanalytic.miss[var.names] <- lapply(analytic.miss[var.names] , factor)\nstr(analytic.miss)\n#> 'data.frame':    397173 obs. of  22 variables:\n#>  $ CVD      : Factor w/ 2 levels \"event\",\"no event\": 1 2 2 2 2 2 2 2 2 2 ...\n#>  $ age      : Factor w/ 7 levels \"20-29 years\",..: 6 6 2 6 1 6 3 7 1 1 ...\n#>  $ sex      : Factor w/ 2 levels \"Female\",\"Male\": 1 1 2 1 1 2 2 2 1 2 ...\n#>  $ married  : Factor w/ 2 levels \"not single\",\"single\": 2 2 1 2 2 1 1 2 2 2 ...\n#>  $ race     : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ edu      : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 2 4 4 4 4 4 4 1 4 4 ...\n#>  $ income   : Factor w/ 4 levels \"$29,999 or less\",..: 1 1 4 1 2 2 1 1 NA 4 ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#>  $ phyact   : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 2 2 2 2 2 1 1 2 3 ...\n#>  $ doctor   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ stress   : Factor w/ 2 levels \"Not too stressed\",..: 1 1 2 1 1 1 1 NA 1 1 ...\n#>  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#>  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#>  $ bp       : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ diab     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ weight   : num  47.6 23.8 56.1 23.8 65.4 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ OA       : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\n\nThe code identifies rows where data is missing and labels them for later analyses.\n\nShow the codeanalytic.miss$miss <- 1\nhead(analytic.miss$ID) # full data\n#> [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#> [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#> [1]  3  5  7 10 11 13\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] <- 0\ntable(analytic.miss$miss)\n#> \n#>      0      1 \n#> 185613 211560\n\n\nSetting Design\nThe code sets up the survey design, specifying weights (but no specific clustering and stratification, as they are unavailable for CCHS public access data), for use in survey-weighted analyses.\n\nShow the coderequire(survey)\nsummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#> [1] 80.34263\n\n\nThis creates a subset of the data where there are no missing values. Note that subset was done to the design object w.design0, not the data analytic.miss.\n\nShow the codew.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsd(weights(w.design))\n#> [1] 84.97819\n\n\nBivariate analysis\nTable 1 (weighted)\nStratified by exposure\nThese tables contain descriptive statistics, stratified by different categories. They can be useful for understanding how variables relate to the exposure or outcome in the data.\n\nShow the coderequire(tableone)\nvar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"OA\"\n# tab1 <- CreateTableOne(var = var.names, strata= \"OA\", data=analytic.miss, test = TRUE)\n# print(tab1)\ntab2 <- svyCreateTableOne(var = var.names, strata= \"OA\", \n                          data=w.design, test = TRUE)\nprint(tab2)\n#>                        Stratified by OA\n#>                         Control            OA                p      test\n#>   n                     12124961.5         1153392.2                    \n#>   CVD = no event (%)    11786450.6 (97.2)  1020965.5 (88.5)  <0.001     \n#>   age (%)                                                    <0.001     \n#>      20-29 years         2668880.8 (22.0)    28317.5 ( 2.5)             \n#>      30-39 years         3009426.7 (24.8)    77159.3 ( 6.7)             \n#>      40-49 years         3108300.9 (25.6)   211515.1 (18.3)             \n#>      50-59 years         1900845.3 (15.7)   350264.7 (30.4)             \n#>      60-64 years          546041.8 ( 4.5)   163724.7 (14.2)             \n#>      65 years and over    624706.7 ( 5.2)   322033.4 (27.9)             \n#>      teen                 266759.3 ( 2.2)      377.4 ( 0.0)             \n#>   sex = Male (%)         6374765.5 (52.6)   379850.5 (32.9)  <0.001     \n#>   married = single (%)   4120611.0 (34.0)   367647.7 (31.9)  <0.001     \n#>   race = White (%)      10312228.2 (85.0)  1081778.6 (93.8)  <0.001     \n#>   edu (%)                                                    <0.001     \n#>      < 2ndary            1752318.3 (14.5)   309652.8 (26.8)             \n#>      2nd grad.           2314713.1 (19.1)   203437.5 (17.6)             \n#>      Other 2nd grad.     1078645.2 ( 8.9)    79255.1 ( 6.9)             \n#>      Post-2nd grad.      6979284.9 (57.6)   561046.8 (48.6)             \n#>   income (%)                                                 <0.001     \n#>      $29,999 or less     2051640.6 (16.9)   353862.9 (30.7)             \n#>      $30,000-$49,999     2436063.7 (20.1)   272484.1 (23.6)             \n#>      $50,000-$79,999     3495902.5 (28.8)   275115.8 (23.9)             \n#>      $80,000 or more     4141354.6 (34.2)   251929.4 (21.8)             \n#>   bmi (%)                                                    <0.001     \n#>      Underweight          346004.9 ( 2.9)    22064.6 ( 1.9)             \n#>      healthy weight      6019004.1 (49.6)   431570.2 (37.4)             \n#>      Overweight          5759952.5 (47.5)   699757.4 (60.7)             \n#>   phyact (%)                                                 <0.001     \n#>      Active              3037314.2 (25.1)   216879.5 (18.8)             \n#>      Inactive            5982492.3 (49.3)   647856.2 (56.2)             \n#>      Moderate            3105154.9 (25.6)   288656.5 (25.0)             \n#>   doctor = Yes (%)      10087473.8 (83.2)  1090763.9 (94.6)  <0.001     \n#>   stress = stressed (%)  3123770.9 (25.8)   301895.2 (26.2)   0.420     \n#>   smoke (%)                                                  <0.001     \n#>      Never smoker        4043479.9 (33.3)   320323.7 (27.8)             \n#>      Current smoker      3219168.6 (26.5)   275835.7 (23.9)             \n#>      Former smoker       4862313.0 (40.1)   557232.7 (48.3)             \n#>   drink (%)                                                  <0.001     \n#>      Never drank          678435.7 ( 5.6)    66085.0 ( 5.7)             \n#>      Current drinker    10297713.4 (84.9)   887808.2 (77.0)             \n#>      Former driker       1148812.3 ( 9.5)   199498.9 (17.3)             \n#>   fruit (%)                                                  <0.001     \n#>      0-3 daily serving   3214156.0 (26.5)   236483.8 (20.5)             \n#>      4-6 daily serving   6001124.3 (49.5)   588323.8 (51.0)             \n#>      6+ daily serving    2909681.1 (24.0)   328584.5 (28.5)             \n#>   bp = Yes (%)           1212548.2 (10.0)   347269.8 (30.1)  <0.001     \n#>   diab = Yes (%)          377876.2 ( 3.1)   104541.0 ( 9.1)  <0.001     \n#>   province = North (%)     27124.3 ( 0.2)     1825.1 ( 0.2)  <0.001     \n#>   immigrate (%)                                              <0.001     \n#>      not immigrant       9898636.6 (81.6)   994682.5 (86.2)             \n#>      > 10 years          1384672.6 (11.4)   146879.8 (12.7)             \n#>      recent               841652.3 ( 6.9)    11829.8 ( 1.0)\n\n\nStratified by outcome\nThis table is generally useful for logistic regression analysis\n\nShow the codevar.names <- c(\"OA\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"CVD\"\ntab3 <- svyCreateTableOne(var = var.names, strata= \"CVD\", data=w.design, test = TRUE)\nprint(tab3)\n#>                        Stratified by CVD\n#>                         event            no event           p      test\n#>   n                     470937.5         12807416.1                    \n#>   OA = OA (%)           132426.7 (28.1)   1020965.5 ( 8.0)  <0.001     \n#>   age (%)                                                   <0.001     \n#>      20-29 years         14966.0 ( 3.2)   2682232.3 (20.9)             \n#>      30-39 years         24105.5 ( 5.1)   3062480.5 (23.9)             \n#>      40-49 years         63520.1 (13.5)   3256296.0 (25.4)             \n#>      50-59 years        122613.0 (26.0)   2128497.0 (16.6)             \n#>      60-64 years         72328.3 (15.4)    637438.1 ( 5.0)             \n#>      65 years and over  172742.2 (36.7)    773997.8 ( 6.0)             \n#>      teen                  662.3 ( 0.1)    266474.4 ( 2.1)             \n#>   sex = Male (%)        267743.8 (56.9)   6486872.2 (50.6)  <0.001     \n#>   married = single (%)  143747.0 (30.5)   4344511.8 (33.9)  <0.001     \n#>   race = White (%)      434591.6 (92.3)  10959415.2 (85.6)  <0.001     \n#>   edu (%)                                                   <0.001     \n#>      < 2ndary           147338.4 (31.3)   1914632.6 (14.9)             \n#>      2nd grad.           77705.6 (16.5)   2440445.0 (19.1)             \n#>      Other 2nd grad.     30921.3 ( 6.6)   1126979.0 ( 8.8)             \n#>      Post-2nd grad.     214972.2 (45.6)   7325359.4 (57.2)             \n#>   income (%)                                                <0.001     \n#>      $29,999 or less    164929.4 (35.0)   2240574.2 (17.5)             \n#>      $30,000-$49,999    109988.2 (23.4)   2598559.6 (20.3)             \n#>      $50,000-$79,999    103091.1 (21.9)   3667927.1 (28.6)             \n#>      $80,000 or more     92928.7 (19.7)   4300355.3 (33.6)             \n#>   bmi (%)                                                   <0.001     \n#>      Underweight          8844.4 ( 1.9)    359225.0 ( 2.8)             \n#>      healthy weight     173475.1 (36.8)   6277099.2 (49.0)             \n#>      Overweight         288617.9 (61.3)   6171091.9 (48.2)             \n#>   phyact (%)                                                <0.001     \n#>      Active              85140.3 (18.1)   3169053.4 (24.7)             \n#>      Inactive           274968.8 (58.4)   6355379.7 (49.6)             \n#>      Moderate           110828.4 (23.5)   3282983.0 (25.6)             \n#>   doctor = Yes (%)      445493.3 (94.6)  10732744.5 (83.8)  <0.001     \n#>   stress = stressed (%) 113282.5 (24.1)   3312383.7 (25.9)   0.023     \n#>   smoke (%)                                                 <0.001     \n#>      Never smoker       119434.6 (25.4)   4244368.9 (33.1)             \n#>      Current smoker      97328.0 (20.7)   3397676.3 (26.5)             \n#>      Former smoker      254174.9 (54.0)   5165370.9 (40.3)             \n#>   drink (%)                                                 <0.001     \n#>      Never drank         29444.3 ( 6.3)    715076.4 ( 5.6)             \n#>      Current drinker    344405.1 (73.1)  10841116.6 (84.6)             \n#>      Former driker       97088.1 (20.6)   1251223.1 ( 9.8)             \n#>   fruit (%)                                                  0.001     \n#>      0-3 daily serving  111803.5 (23.7)   3338836.3 (26.1)             \n#>      4-6 daily serving  233403.5 (49.6)   6356044.7 (49.6)             \n#>      6+ daily serving   125730.4 (26.7)   3112535.1 (24.3)             \n#>   bp = Yes (%)          209257.0 (44.4)   1350561.0 (10.5)  <0.001     \n#>   diab = Yes (%)         78762.9 (16.7)    403654.4 ( 3.2)  <0.001     \n#>   province = North (%)     702.8 ( 0.1)     28246.6 ( 0.2)   0.005     \n#>   immigrate (%)                                             <0.001     \n#>      not immigrant      389553.0 (82.7)  10503766.2 (82.0)             \n#>      > 10 years          69008.0 (14.7)   1462544.4 (11.4)             \n#>      recent              12376.5 ( 2.6)    841105.5 ( 6.6)\n\n\nHow did they calculate the p-values? Hint: svychisq (see below).\nProportions and Design Effect\nThis part computes proportions and design effects, which help understand the influence of the sampling design on the estimated statistics.\n\nShow the coderequire(survey)\n# Computing survey statistics on subsets of a survey defined by factor(s).\nfit0a <- svyby(~CVD,~OA,design=w.design, svymean,deff=TRUE)\nfit0a\n\n\n\n  \n\n\nShow the codeconfint(fit0a)\n#>                          2.5 %    97.5 %\n#> Control:CVDevent    0.02681661 0.0290204\n#> OA:CVDevent         0.10847000 0.1211599\n#> Control:CVDno event 0.97097960 0.9731834\n#> OA:CVDno event      0.87884010 0.8915300\n# 7.45% OA patients estimated to have CVD event.\n# 95% CI:  (0.067, 0.0816)\n\n\nLet\n\n\n\\(\\theta\\) = parameter (population slope) and\n\n\n\\(\\hat(\\theta)\\) = statistic (estimated slope).\n\n\\(b = \\frac{\\sum[w (y_i-\\bar{y}) (x_i-\\bar{x})]}{\\sum[w (x_i-\\bar{x})^2]}\\)\nDE = Effect of complex survey on the SEs, relative to a SRS of equal size.\n\n\\(D^2(\\hat{\\theta}) = \\frac{Var(\\hat{\\theta})_{Complex Survey}}{Var(\\hat{\\theta})_{SRS}}\\)\n\\(D^2(\\hat{\\theta}) = \\frac{SE(\\hat{\\theta})^2_{Complex Survey}}{SE(\\hat{\\theta})^2_{SRS}}\\)\n\nNote:\n\nSE increases as value of weight increases (CCHS).\nNHANES has more things to worry about (strata, PSU)\n\nDEFF = 2 means that the variance of the sample proportion, when choosing the sample by complex survey sampling, is nearly 2 times as large as the variance of the same estimator under simple random sampling/SRS.\n\nShow the codefit0b <- svyby(~CVD,~diab,design=w.design, svymean,deff=TRUE)\nfit0b\n\n\n\n  \n\n\nShow the codeconfint(fit0b)\n#>                     2.5 %     97.5 %\n#> No:CVDevent     0.0295633 0.03173344\n#> Yes:CVDevent    0.1505917 0.17594247\n#> No:CVDno event  0.9682666 0.97043670\n#> Yes:CVDno event 0.8240575 0.84940825\n\n\nTesting association\nHere, Chi-square tests are conducted to test the association between different variables. Two variants of the test are used: Rao-Scott and Thomas-Rao modifications. These adaptations are used when the data come from a complex survey design.\n\nTests for hypothesis\n\nRao-Scott modifications (chi-sq)\nThomas-Rao modifications (F)\n\n\n\n\nShow the code# Rao-Scott modifications (chi-sq)\nsvychisq(~CVD+OA,design=w.design, statistic=\"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"Chisq\")\n#> X-squared = 3249.7, df = 1, p-value < 2.2e-16\n\n# Thomas-Rao modifications (F)\nsvychisq(~CVD+OA,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"F\")\n#> F = 1863, ndf = 1, ddf = 185612, p-value < 2.2e-16\n\n# Both provide strong evidence to reject the null hypothesis.\n# Conclusion: there is a significant (at 5%) association \n# between CVD prevalence and OA.\nsvychisq(~CVD+fruit,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + fruit, design = w.design, statistic = \"F\")\n#> F = 7.1241, ndf = 1.9758e+00, ddf = 3.6673e+05, p-value = 0.0008503\nsvychisq(~CVD+province,design=w.design, statistic=\"Chisq\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + province, design = w.design, statistic = \"Chisq\")\n#> X-squared = 1.4848, df = 1, p-value = 0.00492\n\n\nSaving data\nFinally, the dataset, along with any new variables or subsets created during the analysis, is saved for future use.\n\nShow the codesave(w.design, analytic.miss, analytic2, file = \"Data/surveydata/cchs123w.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata4.html",
    "href": "surveydata4.html",
    "title": "CCHS: Regression",
    "section": "",
    "text": "This tutorial is for a complex data analysis, specifically using regression techniques to analyze survey data.\nLoads necessary R packages for the analysis\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(MASS)\n\n\nLoad data\nLoads a dataset and provides some quick data checks, like the dimensions and summary of weights.\n\nShow the codeload(\"Data/surveydata/cchs123w.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"     \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\nsummary(weights(w.design))\n#> Length  Class   Mode \n#>      0   NULL   NULL\n\n\nLogistic for complex survey\nPerforms a simple logistic regression using the complex survey data, focusing on the relationship between cardiovascular disease and osteoarthritis.\n\nShow the codeformula0 <- as.formula(I(CVD==\"event\") ~ OA)\n\n## Crude regression\nfit2 <- svyglm(formula0, \n              design = w.design, \n              family = binomial(logit))\nrequire(Publish)\npublish(fit2)\n#>  Variable   Units OddsRatio       CI.95 p-value \n#>        OA Control       Ref                     \n#>                OA      4.52 [4.19;4.87]  <1e-04\n\n\nMultivariable analysis\nRuns a more complex logistic regression model, adding multiple covariates to better understand the relationship.\n\nShow the codeformula1 <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race + \n              edu + income + bmi + phyact + doctor + stress + \n              smoke + drink + fruit + bp + diab + province + immigrate)\n\nfit3 <- svyglm(formula1, \n              design = w.design, \n              family = binomial(logit))\npublish(fit3)\n#>   Variable             Units OddsRatio         CI.95     p-value \n#>         OA           Control       Ref                           \n#>                           OA      1.52   [1.40;1.66]     < 1e-04 \n#>        age       20-29 years       Ref                           \n#>                  30-39 years      1.29   [0.98;1.69]   0.0707636 \n#>                  40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                  50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                  60-64 years      9.71  [7.68;12.29]     < 1e-04 \n#>            65 years and over     15.85 [12.57;20.00]     < 1e-04 \n#>                         teen      0.46   [0.20;1.07]   0.0707295 \n#>        sex            Female       Ref                           \n#>                         Male      1.73   [1.60;1.88]     < 1e-04 \n#>    married        not single       Ref                           \n#>                       single      0.97   [0.90;1.05]   0.5209444 \n#>       race         Non-white       Ref                           \n#>                        White      1.44   [1.19;1.75]   0.0002219 \n#>        edu          < 2ndary       Ref                           \n#>                    2nd grad.      0.90   [0.80;1.00]   0.0512330 \n#>              Other 2nd grad.      0.97   [0.83;1.13]   0.6737665 \n#>               Post-2nd grad.      0.93   [0.85;1.02]   0.1016562 \n#>     income   $29,999 or less       Ref                           \n#>              $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>              $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>              $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>        bmi       Underweight       Ref                           \n#>               healthy weight      0.86   [0.67;1.10]   0.2350526 \n#>                   Overweight      0.88   [0.69;1.12]   0.3033213 \n#>     phyact            Active       Ref                           \n#>                     Inactive      1.21   [1.10;1.34]   0.0001345 \n#>                     Moderate      1.08   [0.97;1.21]   0.1771985 \n#>     doctor                No       Ref                           \n#>                          Yes      1.75   [1.49;2.06]     < 1e-04 \n#>     stress  Not too stressed       Ref                           \n#>                     stressed      1.30   [1.18;1.42]     < 1e-04 \n#>      smoke      Never smoker       Ref                           \n#>               Current smoker      1.18   [1.05;1.32]   0.0050518 \n#>                Former smoker      1.21   [1.11;1.33]     < 1e-04 \n#>      drink       Never drank       Ref                           \n#>              Current drinker      0.82   [0.68;0.98]   0.0290605 \n#>                Former driker      1.13   [0.93;1.36]   0.2133779 \n#>      fruit 0-3 daily serving       Ref                           \n#>            4-6 daily serving      0.94   [0.86;1.03]   0.1758214 \n#>             6+ daily serving      1.09   [0.97;1.23]   0.1311029 \n#>         bp                No       Ref                           \n#>                          Yes      2.35   [2.16;2.55]     < 1e-04 \n#>       diab                No       Ref                           \n#>                          Yes      1.86   [1.66;2.07]     < 1e-04 \n#>   province             South       Ref                           \n#>                        North      1.21   [0.90;1.62]   0.2103030 \n#>  immigrate     not immigrant       Ref                           \n#>                   > 10 years      1.02   [0.89;1.16]   0.8057243 \n#>                       recent      1.06   [0.73;1.53]   0.7651069\n\n\nModel fit assessment\nVariability explained\nPseudo-R-square values indicate how much of the total variability in the outcomes is explainable by the fitted model (analogous to R-square)\n\nCox/Snell (never reaches max 1)\nNagelkerke R-square (scaled to max 1)\n\n\nThe larger Cox & Snell estimate is the better the model.\nThese Pseudo-R-square values should be interpreted with caution (if not ignored).\nThey offer little confidence in interpreting the model fit.\nSurvey weighted version of them are available.\nNot trivial to decide which statistic to use under complex surveys.\n\nEvaluates the model fit using Akaike Information Criterion (AIC) and pseudo R-squared metrics.\n\nShow the codefit3 <- svyglm(formula1, \n              design = w.design, \n              family = quasibinomial(logit)) # publish does not work\nAIC(fit3) \n#>        eff.p          AIC     deltabar \n#>    67.752064 45362.706032     2.053093\n\n# AIC for survey weighted regressions\npsrsq(fit3, method = \"Cox-Snell\")\n#> [1] 0.06091896\npsrsq(fit3, method = \"Nagelkerke\")\n#> [1] 0.2307586\n# Nagelkerke and Cox-Snell pseudo-rsquared statistics\n\n\nBackward Elimination\n\nModel comparisons\n\nLRT-aprroximation\nWald-based\n\n\n\nChecking one by one\nChecks the significance of each variable one by one and removes those that are not statistically significant.\n\nShow the coderound(sort(summary(fit3)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.                ageteen \n#>                   0.03                   0.05                   0.07 \n#>         age30-39 years      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.18                   0.18                   0.21 \n#>     drinkFormer driker      bmihealthy weight          bmiOverweight \n#>                   0.21                   0.24                   0.30 \n#>          marriedsingle     eduOther 2nd grad.        immigraterecent \n#>                   0.52                   0.67                   0.77 \n#>    immigrate> 10 years \n#>                   0.81\n# bmiOverweight is associated with largest p-value\n# but what about other categories?\n\nregTermTest(fit3,~bmi) # coef of all bmi cat = 0\n#> Wald test for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> F =  0.7591291  on  2  and  185579  df: p= 0.46808\nfit4 <- update(fit3, .~. -bmi) \n\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> Working 2logLR =  1.424634 p= 0.49071 \n#> (scale factors:  1.1 0.93 );  denominator df= 185579\n# high p-value (in both wald and Anova) makes it more likely that you should exclude bmi\nAIC(fit3,fit4) \n#>         eff.p      AIC deltabar\n#> [1,] 67.75206 45362.71 2.053093\n#> [2,] 64.30460 45358.26 2.074342\nround(sort(summary(fit4)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.00 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.17                   0.17                   0.21 \n#>     drinkFormer driker          marriedsingle     eduOther 2nd grad. \n#>                   0.21                   0.53                   0.67 \n#>        immigraterecent    immigrate> 10 years \n#>                   0.76                   0.81\n\n\nUsing AIC to automate\nUses stepwise regression guided by AIC to automatically select the most important variables.\n\nShow the coderequire(MASS)\nformula1b <- as.formula(I(CVD==\"event\") ~ OA + age + sex)\nfit1b <- svyglm(formula1b, \n              design = w.design, \n              family = binomial(logit))\nfit5 <- stepAIC(fit1b, direction = \"backward\")\n#> Start:  AIC=47384.51\n#> I(CVD == \"event\") ~ OA + age + sex\n#> \n#>        Df Deviance   AIC\n#> <none>       47353 47385\n#> - sex   1    47634 47658\n#> - OA    1    47679 47702\n#> - age   6    54414 54291\n\n\n\nShow the codepublish(fit5)\n#>  Variable             Units OddsRatio         CI.95   p-value \n#>        OA           Control       Ref                         \n#>                          OA      1.81   [1.66;1.97]   < 1e-04 \n#>       age       20-29 years       Ref                         \n#>                 30-39 years      1.40   [1.07;1.84]   0.01374 \n#>                 40-49 years      3.39   [2.69;4.27]   < 1e-04 \n#>                 50-59 years      9.42  [7.55;11.76]   < 1e-04 \n#>                 60-64 years     17.78 [14.22;22.23]   < 1e-04 \n#>           65 years and over     33.82 [27.26;41.97]   < 1e-04 \n#>                        teen      0.45   [0.20;1.02]   0.05462 \n#>       sex            Female       Ref                         \n#>                        Male      1.57   [1.46;1.69]   < 1e-04\nround(sort(summary(fit5)$coef[,\"Pr(>|t|)\"]),2)\n#>          (Intercept) age65 years and over       age60-64 years \n#>                 0.00                 0.00                 0.00 \n#>       age50-59 years                 OAOA              sexMale \n#>                 0.00                 0.00                 0.00 \n#>       age40-49 years       age30-39 years              ageteen \n#>                 0.00                 0.01                 0.05\n\n\nUsing AIC, but keeping importants\nSimilar to the previous step but ensures certain important variables remain in the model.\n\nShow the codeformula1c <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate)\nscope <- list(upper = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate,\n              lower = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab)\n\nfit1c <- svyglm(formula1c, design = w.design, family = binomial(logit))\n\nfitstep <- step(fit1c, scope = scope, trace = FALSE, k = 2, direction = \"backward\")\n# k = 2 gives the genuine AIC\n\n\n\nShow the codepublish(fitstep)\n#>  Variable             Units OddsRatio         CI.95     p-value \n#>        OA           Control       Ref                           \n#>                          OA      1.52   [1.40;1.66]     < 1e-04 \n#>       age       20-29 years       Ref                           \n#>                 30-39 years      1.29   [0.98;1.70]   0.0697699 \n#>                 40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                 50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                 60-64 years      9.71  [7.68;12.28]     < 1e-04 \n#>           65 years and over     15.85 [12.57;19.98]     < 1e-04 \n#>                        teen      0.46   [0.20;1.07]   0.0705660 \n#>       sex            Female       Ref                           \n#>                        Male      1.73   [1.60;1.88]     < 1e-04 \n#>   married        not single       Ref                           \n#>                      single      0.97   [0.90;1.05]   0.5042496 \n#>      race         Non-white       Ref                           \n#>                       White      1.41   [1.18;1.70]   0.0001986 \n#>       edu          < 2ndary       Ref                           \n#>                   2nd grad.      0.90   [0.80;1.00]   0.0524940 \n#>             Other 2nd grad.      0.97   [0.83;1.13]   0.6732446 \n#>              Post-2nd grad.      0.93   [0.85;1.02]   0.1045975 \n#>    income   $29,999 or less       Ref                           \n#>             $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>             $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>             $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>       bmi       Underweight       Ref                           \n#>              healthy weight      0.86   [0.67;1.10]   0.2316915 \n#>                  Overweight      0.88   [0.69;1.12]   0.2982852 \n#>    phyact            Active       Ref                           \n#>                    Inactive      1.22   [1.10;1.34]   0.0001227 \n#>                    Moderate      1.08   [0.97;1.21]   0.1754422 \n#>     fruit 0-3 daily serving       Ref                           \n#>           4-6 daily serving      0.94   [0.86;1.03]   0.1807666 \n#>            6+ daily serving      1.09   [0.97;1.23]   0.1295281 \n#>        bp                No       Ref                           \n#>                         Yes      2.35   [2.16;2.55]     < 1e-04 \n#>      diab                No       Ref                           \n#>                         Yes      1.85   [1.66;2.07]     < 1e-04 \n#>    doctor                No       Ref                           \n#>                         Yes      1.75   [1.49;2.05]     < 1e-04 \n#>    stress  Not too stressed       Ref                           \n#>                    stressed      1.30   [1.18;1.42]     < 1e-04 \n#>     smoke      Never smoker       Ref                           \n#>              Current smoker      1.17   [1.05;1.31]   0.0053412 \n#>               Former smoker      1.21   [1.10;1.33]     < 1e-04 \n#>     drink       Never drank       Ref                           \n#>             Current drinker      0.82   [0.68;0.98]   0.0254942 \n#>               Former driker      1.12   [0.93;1.36]   0.2205315\nround(sort(summary(fitstep)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#>         phyactModerate fruit4-6 daily serving     drinkFormer driker \n#>                   0.18                   0.18                   0.22 \n#>      bmihealthy weight          bmiOverweight          marriedsingle \n#>                   0.23                   0.30                   0.50 \n#>     eduOther 2nd grad. \n#>                   0.67\n\n\nAssess interactions\nCheck biologically interesting ones.\nCheck one by one\nChecks if there is a significant interaction effect between ‘age’ and ‘sex’.\n\nShow the codefit8a <- update(fitstep, .~. + interaction(age,sex))\nanova(fitstep, fit8a) # keep interaction\n#> Working (Rao-Scott+F) LRT for interaction(age, sex)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(age, sex), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  40.16528 p= 1.2167e-06 \n#> (scale factors:  1.3 1.2 1.2 0.93 0.78 0.71 );  denominator df= 185576\n\n\nChecks if there is a significant interaction effect between ‘sex’ and ‘diabetes’.\n\nShow the codefit8b <- update(fitstep, .~. + interaction(sex,diab))\nanova(fitstep, fit8b) # Do not keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(sex, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(sex, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  0.4591456 p= 0.49597 \n#> df=1;  denominator df= 185581\n\n\nChecks if there is a significant interaction effect between ‘BMI’ and ‘diabetes’.\n\nShow the codefit8c <- update(fitstep, .~. + interaction(bmi,diab))\nanova(fitstep, fit8c) # keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(bmi, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(bmi, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  7.92727 p= 0.02533 \n#> (scale factors:  1.4 0.6 );  denominator df= 185580\n\n\nAdd all significant interactions in 1 model\nUpdates the model to include significant interaction terms.\nNote that we have 0 effect modifier, 2 interactions\n\nShow the codefit9 <- update(fitstep, .~. + interaction(age,sex) + interaction(bmi,diab))\nrequire(jtools)\nsumm(fit9, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    185613 \n  \n\n Dependent variable \n    I(CVD == \"event\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.233 \n  \n\n Pseudo-R² (McFadden) \n    0.207 \n  \n\n AIC \n    41548.160 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    -5.590 \n    -6.046 \n    -5.135 \n    -24.069 \n    0.000 \n  \n\n OAOA \n    0.438 \n    0.352 \n    0.523 \n    10.043 \n    0.000 \n  \n\n age30-39 years \n    0.299 \n    -0.118 \n    0.717 \n    1.405 \n    0.160 \n  \n\n age40-49 years \n    1.158 \n    0.794 \n    1.522 \n    6.236 \n    0.000 \n  \n\n age50-59 years \n    2.181 \n    1.831 \n    2.531 \n    12.212 \n    0.000 \n  \n\n age60-64 years \n    2.619 \n    2.263 \n    2.976 \n    14.395 \n    0.000 \n  \n\n age65 years and over \n    3.033 \n    2.680 \n    3.387 \n    16.833 \n    0.000 \n  \n\n ageteen \n    -0.695 \n    -1.704 \n    0.314 \n    -1.350 \n    0.177 \n  \n\n sexMale \n    -0.028 \n    -0.447 \n    0.390 \n    -0.133 \n    0.895 \n  \n\n marriedsingle \n    -0.016 \n    -0.096 \n    0.064 \n    -0.382 \n    0.703 \n  \n\n raceWhite \n    0.348 \n    0.165 \n    0.531 \n    3.724 \n    0.000 \n  \n\n edu2nd grad. \n    -0.107 \n    -0.217 \n    0.003 \n    -1.906 \n    0.057 \n  \n\n eduOther 2nd grad. \n    -0.039 \n    -0.192 \n    0.114 \n    -0.498 \n    0.618 \n  \n\n eduPost-2nd grad. \n    -0.081 \n    -0.173 \n    0.010 \n    -1.746 \n    0.081 \n  \n\n income$30,000-$49,999 \n    -0.304 \n    -0.400 \n    -0.208 \n    -6.215 \n    0.000 \n  \n\n income$50,000-$79,999 \n    -0.444 \n    -0.552 \n    -0.336 \n    -8.034 \n    0.000 \n  \n\n income$80,000 or more \n    -0.555 \n    -0.684 \n    -0.426 \n    -8.424 \n    0.000 \n  \n\n bmihealthy weight \n    -1.406 \n    -2.677 \n    -0.135 \n    -2.167 \n    0.030 \n  \n\n bmiOverweight \n    -1.110 \n    -2.375 \n    0.154 \n    -1.721 \n    0.085 \n  \n\n phyactInactive \n    0.194 \n    0.094 \n    0.293 \n    3.817 \n    0.000 \n  \n\n phyactModerate \n    0.077 \n    -0.034 \n    0.187 \n    1.353 \n    0.176 \n  \n\n fruit4-6 daily serving \n    -0.062 \n    -0.155 \n    0.031 \n    -1.313 \n    0.189 \n  \n\n fruit6+ daily serving \n    0.094 \n    -0.023 \n    0.211 \n    1.579 \n    0.114 \n  \n\n bpYes \n    0.861 \n    0.778 \n    0.944 \n    20.354 \n    0.000 \n  \n\n diabYes \n    1.745 \n    0.462 \n    3.027 \n    2.667 \n    0.008 \n  \n\n doctorYes \n    0.535 \n    0.372 \n    0.697 \n    6.447 \n    0.000 \n  \n\n stressstressed \n    0.256 \n    0.164 \n    0.347 \n    5.483 \n    0.000 \n  \n\n smokeCurrent smoker \n    0.152 \n    0.039 \n    0.266 \n    2.628 \n    0.009 \n  \n\n smokeFormer smoker \n    0.177 \n    0.082 \n    0.272 \n    3.654 \n    0.000 \n  \n\n drinkCurrent drinker \n    -0.205 \n    -0.381 \n    -0.029 \n    -2.277 \n    0.023 \n  \n\n drinkFormer driker \n    0.116 \n    -0.072 \n    0.303 \n    1.210 \n    0.226 \n  \n\n interaction(age, sex)30-39 years.Female \n    -0.083 \n    -0.622 \n    0.457 \n    -0.300 \n    0.764 \n  \n\n interaction(age, sex)40-49 years.Female \n    -0.302 \n    -0.766 \n    0.161 \n    -1.278 \n    0.201 \n  \n\n interaction(age, sex)50-59 years.Female \n    -0.810 \n    -1.258 \n    -0.362 \n    -3.543 \n    0.000 \n  \n\n interaction(age, sex)60-64 years.Female \n    -0.787 \n    -1.237 \n    -0.337 \n    -3.428 \n    0.001 \n  \n\n interaction(age, sex)65 years and over.Female \n    -0.603 \n    -1.035 \n    -0.170 \n    -2.733 \n    0.006 \n  \n\n interaction(age, sex)teen.Female \n    -0.129 \n    -1.806 \n    1.548 \n    -0.151 \n    0.880 \n  \n\n interaction(bmi, diab)healthy weight.No \n    1.380 \n    0.085 \n    2.675 \n    2.089 \n    0.037 \n  \n\n interaction(bmi, diab)Overweight.No \n    1.085 \n    -0.204 \n    2.373 \n    1.650 \n    0.099 \n  \n\n\n Standard errors: Robust\n\n\n\n\nShow the codefit9 <- update(fitstep, .~. + age:sex + bmi:diab)\npublish(fit9)\n#>                                            Variable             Units OddsRatio         CI.95     p-value \n#>                                                  OA           Control       Ref                           \n#>                                                                    OA      1.55   [1.42;1.69]     < 1e-04 \n#>                                             married        not single       Ref                           \n#>                                                                single      0.98   [0.91;1.07]   0.7026897 \n#>                                                race         Non-white       Ref                           \n#>                                                                 White      1.42   [1.18;1.70]   0.0001960 \n#>                                                 edu          < 2ndary       Ref                           \n#>                                                             2nd grad.      0.90   [0.81;1.00]   0.0566536 \n#>                                                       Other 2nd grad.      0.96   [0.83;1.12]   0.6183383 \n#>                                                        Post-2nd grad.      0.92   [0.84;1.01]   0.0807393 \n#>                                              income   $29,999 or less       Ref                           \n#>                                                       $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>                                                       $50,000-$79,999      0.64   [0.58;0.71]     < 1e-04 \n#>                                                       $80,000 or more      0.57   [0.50;0.65]     < 1e-04 \n#>                                              phyact            Active       Ref                           \n#>                                                              Inactive      1.21   [1.10;1.34]   0.0001349 \n#>                                                              Moderate      1.08   [0.97;1.21]   0.1760803 \n#>                                               fruit 0-3 daily serving       Ref                           \n#>                                                     4-6 daily serving      0.94   [0.86;1.03]   0.1892549 \n#>                                                      6+ daily serving      1.10   [0.98;1.23]   0.1142759 \n#>                                                  bp                No       Ref                           \n#>                                                                   Yes      2.37   [2.18;2.57]     < 1e-04 \n#>                                              doctor                No       Ref                           \n#>                                                                   Yes      1.71   [1.45;2.01]     < 1e-04 \n#>                                              stress  Not too stressed       Ref                           \n#>                                                              stressed      1.29   [1.18;1.42]     < 1e-04 \n#>                                               smoke      Never smoker       Ref                           \n#>                                                        Current smoker      1.16   [1.04;1.30]   0.0085960 \n#>                                                         Former smoker      1.19   [1.09;1.31]   0.0002579 \n#>                                               drink       Never drank       Ref                           \n#>                                                       Current drinker      0.81   [0.68;0.97]   0.0227934 \n#>                                                         Former driker      1.12   [0.93;1.35]   0.2264702 \n#>               age(20-29 years): sex(Male vs Female)                        0.97   [0.64;1.48]   0.8945322 \n#>               age(30-39 years): sex(Male vs Female)                        1.06   [0.75;1.49]   0.7583565 \n#>               age(40-49 years): sex(Male vs Female)                        1.32   [1.07;1.62]   0.0091527 \n#>               age(50-59 years): sex(Male vs Female)                        2.18   [1.85;2.58]     < 1e-04 \n#>               age(60-64 years): sex(Male vs Female)                        2.14   [1.80;2.53]     < 1e-04 \n#>         age(65 years and over): sex(Male vs Female)                        1.78   [1.58;1.99]     < 1e-04 \n#>                      age(teen): sex(Male vs Female)                        1.11   [0.22;5.62]   0.9033924 \n#>        sex(Female): age(30-39 years vs 20-29 years)                        1.24   [0.87;1.77]   0.2276609 \n#>        sex(Female): age(40-49 years vs 20-29 years)                        2.35   [1.75;3.16]     < 1e-04 \n#>        sex(Female): age(50-59 years vs 20-29 years)                        3.94   [2.95;5.27]     < 1e-04 \n#>        sex(Female): age(60-64 years vs 20-29 years)                        6.25   [4.66;8.39]     < 1e-04 \n#>  sex(Female): age(65 years and over vs 20-29 years)                       11.37  [8.60;15.02]     < 1e-04 \n#>               sex(Female): age(teen vs 20-29 years)                        0.44   [0.11;1.69]   0.2320840 \n#>          sex(Male): age(30-39 years vs 20-29 years)                        1.35   [0.89;2.05]   0.1599938 \n#>          sex(Male): age(40-49 years vs 20-29 years)                        3.18   [2.21;4.58]     < 1e-04 \n#>          sex(Male): age(50-59 years vs 20-29 years)                        8.85  [6.24;12.56]     < 1e-04 \n#>          sex(Male): age(60-64 years vs 20-29 years)                       13.73  [9.61;19.61]     < 1e-04 \n#>    sex(Male): age(65 years and over vs 20-29 years)                       20.77 [14.59;29.57]     < 1e-04 \n#>                 sex(Male): age(teen vs 20-29 years)                        0.50   [0.18;1.37]   0.1769155 \n#>                   bmi(Underweight): diab(Yes vs No)                        5.72  [1.59;20.63]   0.0076561 \n#>                bmi(healthy weight): diab(Yes vs No)                        1.44   [1.19;1.75]   0.0002221 \n#>                    bmi(Overweight): diab(Yes vs No)                        1.93   [1.70;2.20]     < 1e-04 \n#>        diab(No): bmi(healthy weight vs Underweight)                        0.97   [0.76;1.25]   0.8394972 \n#>            diab(No): bmi(Overweight vs Underweight)                        0.97   [0.76;1.25]   0.8400722 \n#>       diab(Yes): bmi(healthy weight vs Underweight)                        0.25   [0.07;0.87]   0.0302066 \n#>           diab(Yes): bmi(Overweight vs Underweight)                        0.33   [0.09;1.17]   0.0852377\n\n\n\nShow the codebasic.model <- eval(fit5$call[[2]])\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\n\naic.int.model <- eval(fit9$call[[2]])\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\n\n\nSaving data\nSaves the final regression models for future use.\n\nShow the codesave(basic.model, aic.int.model, file = \"Data/surveydata/cchs123w2.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata5.html",
    "href": "surveydata5.html",
    "title": "CCHS: Performance",
    "section": "",
    "text": "The tutorial outlines the process for evaluating the performance of logistic regression models fitted to complex survey data using R. It focuses on two major aspects: creating Receiver Operating Characteristic (ROC) curves and conducting Archer and Lemeshow Goodness of Fit tests. Here AUC is a measure to evaluate the predictive accuracy of the model, and Archer and Lemeshow test is a statistical test to evaluate how well your model fits the observed data.\nWe start by importing the required R packages.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\n\nLoad data\nIt loads two datasets from the specified paths.\n\nShow the codeload(\"Data/surveydata/cchs123w.RData\")\nload(\"Data/surveydata/cchs123w2.RData\")\nls()\n#> [1] \"aic.int.model\" \"analytic.miss\" \"analytic2\"     \"basic.model\"  \n#> [5] \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\n\n\nThree different logistic regression models are fitted to the data:\n\nShow the codelibrary(survey)\nsimple.model <- as.formula(I(CVD==\"event\") ~ OA)\nfit0 <- svyglm(simple.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\nfit5 <- svyglm(basic.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\nfit9 <- svyglm(aic.int.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nModel performance\nROC curve\nThis section defines a function, svyROCw, to plot the ROC curves and calculate the area under the curve (AUC). The function can handle both weighted and unweighted survey data.\n\nThe appropriateness of the fitted logistic regression model needs to be examined before it is accepted for use.\nPlotting the pairs of - sensitivities vs - 1-specificities on a scatter plot provides a Receiver Operating Characteristic (ROC) curve.\nThe area under the ROC curve = AUC / C-statistic.\nROC/AUC should consider weights for complex surveys.\n\nGrading Guidelines for AUC values:\n\n0.90-1.0 excellent discrimination (unusual)\n0.80-0.90 good discrimination\n0.70-0.80 fair discrimination\n0.60-0.70 poor discrimination\n0.50-0.60 failed discrimination\n\n\nShow the coderequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit=fit,outcome=analytic2$CVD==\"event\", weight = NULL){\n  # ROC curve for\n  # Survey Data with Logistic Regression\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { # library(WeightedROC)\n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\n\n\n\nShow the codesummary(analytic2$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   71.56  137.95  214.61  261.91 7154.95\nanalytic2$corrected.weight <- weights(w.design)\nsummary(analytic2$corrected.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsvyROCw(fit=fit0,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the codesvyROCw(fit=fit5,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the codesvyROCw(fit=fit9,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the code# This function does not take in to account of strata/cluster\n\n\nArcher and Lemeshow test\nThis test helps to evaluate how well the model fits the data. A Goodness of Fit (GOF) function AL.gof is defined. If the p-value from this test is greater than a certain threshold (e.g., 0.05), the model fit is considered acceptable.\n\nHosmer Lemeshow-type tests are most useful as a very crude way to screen for fit problems, and should not be taken as a definitive diagnostic of a ‘good’ fit.\n\nproblem in small sample size\nDependent on G (group)\n\n\nArcher and Lemeshow (2006) extended the standard Hosmer and Lemeshow GOF test for complex surveys.\nAfter fitting the survey weighted logistic regression, the F-adjusted mean residual goodness-of-fit test could suggest\n\nno evidence of lack of fit (if P-value > a reasonable cut-point, e.g., 0.05)\nevidence of lack of fit (if P-value < a reasonable cut-point, e.g., 0.05)\n\n\n\n\nShow the codeAL.gof <- function(fit=fit, data = analytic2, \n                   weight = \"corrected.weight\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=~1, \n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\n\nShow the codeAL.gof(fit0, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.20807e-22  on  1  and  185611  df: p= 1\nAL.gof(fit5, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.795204  on  8  and  185604  df: p= 0.0042898\nAL.gof(fit9, analytic2, weight = \"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.650332  on  9  and  185603  df: p= 0.0045417\n\n\nAdditional function\nIf the survey data contains strata and cluster, then the following function will be useful:\n\nShow the codeAL.gof2 <- function(fit=fit7, data = analytic, \n                   weight = \"corrected.weight\", psu = \"psu\", strata= \"strata\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata6.html#a-note-about-predictive-models",
    "href": "surveydata6.html#a-note-about-predictive-models",
    "title": "NHANES: Blood Pressure",
    "section": "A note about Predictive models",
    "text": "A note about Predictive models\nIn statistical analyses involving survey data, it’s crucial to account for the survey’s design features. These features can include sampling weights, stratification, and clustering, among others. Ignoring these could lead to biased estimates and incorrect conclusions. In the tutorial you mentioned, such survey design features are considered, making the analysis more robust and reliable in terms of inference.\nHowever, when the goal shifts from inference to prediction, additional challenges come into play. Specifically, the model may perform well on the data used to fit it (the “training” data) but not generalize well to new, unseen data. This discrepancy between training performance and generalization to new data is often referred to as “overfitting,” and the optimism of the model refers to the extent to which it overestimates its predictive performance on new data based on its performance on the training data.\nOptimism-correction techniques are methodologies designed to address this issue. They allow you to evaluate how well your model is likely to perform on new data, not just the data you used to build it. Methods for optimism correction often involve techniques like cross-validation, bootstrapping, or specialized types of model validation that help in estimating the ‘true’ predictive performance of the model. Some of these techniques were discussed in the predictive modelling chapter.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata7.html",
    "href": "surveydata7.html",
    "title": "NHANES: Cholesterol",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(tableone)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(dplyr)\n\n\nPreprocessing\nAnalytic data set\nWe will use cholesterolNHANES15part1.RData in this prediction goal question (predicting cholesterol in adults).\nFor this exercise, we are assuming that:\n\noutcome: cholesterol\n\npredictors:\n\ngender\nwhether born in US\nrace\neducation\nwhether married\nincome level\nBMI\nwhether has diabetes\n\n\n\nsurvey features:\n\nsurvey weights\nstrata\ncluster/PSU; where strata is nested within clusters\n\n– restrict to those participants who are of 18 years of age or older\n\n\n\nShow the codeload(\"Data/surveydata/cholesterolNHANES15part1.rdata\") #Loading the dataset\nls()\n#>  [1] \"analytic\"           \"analytic.with.miss\" \"analytic1\"         \n#>  [4] \"analytic2\"          \"analytic2b\"         \"analytic3\"         \n#>  [7] \"collinearity\"       \"correlationMatrix\"  \"diff.boot\"         \n#> [10] \"extract.boot.fun\"   \"extract.fit\"        \"extract.lm.fun\"    \n#> [13] \"fictitious.data\"    \"fit0\"               \"fit1\"              \n#> [16] \"fit2\"               \"fit3\"               \"fit4\"              \n#> [19] \"fit5\"               \"formula0\"           \"formula1\"          \n#> [22] \"formula2\"           \"formula3\"           \"formula4\"          \n#> [25] \"formula5\"           \"k.folds\"            \"numeric.names\"     \n#> [28] \"perform\"            \"pred.y\"             \"rocobj\"            \n#> [31] \"sel.names\"          \"var.cluster\"        \"var.summ\"          \n#> [34] \"var.summ2\"\n\n\nRetaining only useful variables\n\nShow the code# Data dimensions\ndim(analytic)\n#> [1] 1267   33\n\n# Variable names\nnames(analytic)\n#>  [1] \"ID\"                    \"gender\"                \"age\"                  \n#>  [4] \"born\"                  \"race\"                  \"education\"            \n#>  [7] \"married\"               \"income\"                \"weight\"               \n#> [10] \"psu\"                   \"strata\"                \"diastolicBP\"          \n#> [13] \"systolicBP\"            \"bodyweight\"            \"bodyheight\"           \n#> [16] \"bmi\"                   \"waist\"                 \"smoke\"                \n#> [19] \"alcohol\"               \"cholesterol\"           \"cholesterolM2\"        \n#> [22] \"triglycerides\"         \"uric.acid\"             \"protein\"              \n#> [25] \"bilirubin\"             \"phosphorus\"            \"sodium\"               \n#> [28] \"potassium\"             \"globulin\"              \"calcium\"              \n#> [31] \"physical.work\"         \"physical.recreational\" \"diabetes\"\n\n#Subsetting dataset with variables needed:\nrequire(dplyr)\nanadata <- select(analytic, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\n# new data sizes\ndim(anadata)\n#> [1] 1267   13\n\n# retained variable names\nnames(anadata)\n#>  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#>  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#> [11] \"weight\"      \"psu\"         \"strata\"\n\n#Restricting to participants who are 18 or older\nsummary(anadata$age) #The age range is already 20-80\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   20.00   36.00   51.00   49.91   63.00   80.00\n\n#Recoding the born variable\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born in 50 US states or Washingt                           Others \n#>                              991                              276 \n#>                             <NA> \n#>                                0\nlevels(anadata$born)\n#> NULL\nanadata$born <- car::recode(anadata$born,\n                            \"'Born in 50 US states or Washingt' = 'Born.in.US';\n                            'Others' = 'Others';\n                            else=NA\")\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born.in.US     Others       <NA> \n#>        991        276          0\n\n\nChecking the data for missing\n\nShow the coderequire(DataExplorer)\nplot_missing(anadata) #no missing data\n\n\n\n\nPreparing factor and continuous variables appropriately\n\nShow the codevars = c(\"cholesterol\", \"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\nnumeric.names <- c(\"cholesterol\", \"bmi\")\nfactor.names <- vars[!vars %in% numeric.names] \n\nanadata[factor.names] <- apply(X = anadata[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanadata[numeric.names] <- apply(X = anadata[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\n\n\nTable 1\n\nShow the codelibrary(tableone)\ntab1 <- CreateTableOne(data = anadata, includeNA = TRUE, vars = vars)\nprint(tab1, showAllLevels = TRUE,  varLabels = TRUE)\n#>                          \n#>                           level              Overall       \n#>   n                                            1267        \n#>   cholesterol (mean (SD))                    193.10 (43.22)\n#>   gender (%)              Female                496 (39.1) \n#>                           Male                  771 (60.9) \n#>   born (%)                Born.in.US            991 (78.2) \n#>                           Others                276 (21.8) \n#>   race (%)                Black                 246 (19.4) \n#>                           Hispanic              337 (26.6) \n#>                           Other                 132 (10.4) \n#>                           White                 552 (43.6) \n#>   education (%)           College               648 (51.1) \n#>                           High.School           523 (41.3) \n#>                           School                 96 ( 7.6) \n#>   married (%)             Married               751 (59.3) \n#>                           Never.married         226 (17.8) \n#>                           Previously.married    290 (22.9) \n#>   income (%)              <25k                  344 (27.2) \n#>                           Between.25kto54k      435 (34.3) \n#>                           Between.55kto99k      297 (23.4) \n#>                           Over100k              191 (15.1) \n#>   bmi (mean (SD))                             29.58 (6.84) \n#>   diabetes (%)            No                   1064 (84.0) \n#>                           Yes                   203 (16.0)\n\n\nLinear regression when cholesterol is continuous\nFit a linear regression, and report the VIFs.\n\nShow the code#Fitting initial regression\n\nfit0 <- lm(cholesterol ~ gender + born + race + education +\n              married + income + bmi + diabetes,\n            data = anadata)\n\nlibrary(Publish)\npublish(fit0)\n#>     Variable              Units Coefficient           CI.95    p-value \n#>  (Intercept)                         198.90 [184.82;212.97]    < 1e-04 \n#>       gender             Female         Ref                            \n#>                            Male       -6.82  [-11.76;-1.89]   0.006854 \n#>         born         Born.in.US         Ref                            \n#>                          Others       15.65    [8.54;22.75]    < 1e-04 \n#>         race              Black         Ref                            \n#>                        Hispanic       -2.75   [-10.61;5.10]   0.492333 \n#>                           Other       -3.95   [-13.61;5.72]   0.423740 \n#>                           White        5.36   [-1.20;11.92]   0.109403 \n#>    education            College         Ref                            \n#>                     High.School        3.51    [-1.61;8.63]   0.179871 \n#>                          School        0.31   [-9.63;10.24]   0.951841 \n#>      married            Married         Ref                            \n#>                   Never.married      -11.05  [-17.67;-4.44]   0.001082 \n#>              Previously.married        4.72   [-1.43;10.86]   0.132468 \n#>       income               <25k         Ref                            \n#>                Between.25kto54k       -0.48    [-6.72;5.75]   0.879480 \n#>                Between.55kto99k        3.41   [-3.60;10.43]   0.340491 \n#>                        Over100k        2.24   [-6.02;10.51]   0.595131 \n#>          bmi                          -0.21    [-0.56;0.15]   0.257105 \n#>     diabetes                 No         Ref                            \n#>                             Yes      -10.61  [-17.21;-4.02]   0.001652\n\n#Checking VIFs\ncar::vif(fit0) \n#>               GVIF Df GVIF^(1/(2*Df))\n#> gender    1.065810  1        1.032381\n#> born      1.578258  1        1.256288\n#> race      1.684064  3        1.090753\n#> education 1.280113  2        1.063683\n#> married   1.225520  2        1.052156\n#> income    1.277005  3        1.041595\n#> bmi       1.086953  1        1.042570\n#> diabetes  1.073619  1        1.036156\n\n\nAll VIFs are small.\nTest of association when cholesterol is binary\nDichotomize the outcome such that cholesterol<200 is labeled as ‘healthy’; otherwise label it as ‘unhealthy’, and name it ‘cholesterol.bin’. Test the association between this binary variable and gender.\n\nShow the code#Creating binary variable for cholesterol\nanadata$cholesterol.bin <- ifelse(anadata$cholesterol <200, \"healthy\", \"unhealthy\")\n#If cholesterol is <200, then \"healthy\", if not, \"unhealthy\"\n\ntable(anadata$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>       738       529\nanadata$cholesterol.bin <- as.factor(anadata$cholesterol.bin)\nanadata$cholesterol.bin <- relevel(anadata$cholesterol.bin, ref = \"unhealthy\")\n\n\nTest of association between cholesterol and gender (no survey features)\n\nShow the code# Simple Chi-square testing\nchisq.chol.gen <- chisq.test(anadata$cholesterol.bin, anadata$gender)\nchisq.chol.gen\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  anadata$cholesterol.bin and anadata$gender\n#> X-squared = 5.1321, df = 1, p-value = 0.02349\n\n\nSetting up survey design\n\nShow the coderequire(survey)\nsummary(anadata$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\nw.design <- svydesign(id = ~psu, weights = ~weight, strata = ~strata,\n                      nest = TRUE, data = anadata)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\n\n\nTest of association accounting for survey design\n\nShow the code#Rao-Scott modifications (chi-sq)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> X-squared = 11.092, df = 1, p-value = 0.02365\n\n#Thomas-Rao modifications (F)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\")\n#> F = 5.1205, ndf = 1, ddf = 15, p-value = 0.03891\n\n\nAll three tests indicate strong evidence to reject the H0. There seems to be an association between gender and cholesterol level (healthy/unhealthy)\nTable 1\nCreate a Table 1 (summarizing the covariates) stratified by the binary outcome: cholesterol.bin, utilizing the above survey features.\n\nShow the code# Creating Table 1 stratified by binary outcome (cholesterol)\n# Using the survey features\n\nvars2 = c(\"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\n\n\nkableone <- function(x, ...) {\n  capture.output(x <- print(x, showAllLevels= TRUE, padColnames = TRUE, insertLevel = TRUE))\n  knitr::kable(x, ...)\n}\nkableone(svyCreateTableOne(var = vars2, strata= \"cholesterol.bin\", data=w.design, test = TRUE)) \n\n\n\n\n\n\n\n\n\n\n\n\nlevel\nunhealthy\nhealthy\np\ntest\n\n\n\nn\n\n27369732.3\n34591444.0\n\n\n\n\ngender (%)\nFemale\n13573865.5 (49.6)\n13917447.5 (40.2)\n0.039\n\n\n\n\nMale\n13795866.8 (50.4)\n20673996.5 (59.8)\n\n\n\n\nborn (%)\nBorn.in.US\n23772751.7 (86.9)\n31532673.3 (91.2)\n0.028\n\n\n\n\nOthers\n3596980.6 (13.1)\n3058770.7 ( 8.8)\n\n\n\n\nrace (%)\nBlack\n1832118.3 ( 6.7)\n3696893.4 (10.7)\n0.015\n\n\n\n\nHispanic\n3263992.3 (11.9)\n3921344.6 (11.3)\n\n\n\n\n\nOther\n1887156.6 ( 6.9)\n2601870.3 ( 7.5)\n\n\n\n\n\nWhite\n20386465.2 (74.5)\n24371335.7 (70.5)\n\n\n\n\neducation (%)\nCollege\n15855712.5 (57.9)\n20945710.7 (60.6)\n0.522\n\n\n\n\nHigh.School\n10615218.7 (38.8)\n12434827.2 (35.9)\n\n\n\n\n\nSchool\n898801.1 ( 3.3)\n1210906.1 ( 3.5)\n\n\n\n\nmarried (%)\nMarried\n17489306.2 (63.9)\n21170020.0 (61.2)\n0.005\n\n\n\n\nNever.married\n3086474.4 (11.3)\n7175237.2 (20.7)\n\n\n\n\n\nPreviously.married\n6793951.8 (24.8)\n6246186.8 (18.1)\n\n\n\n\nincome (%)\n<25k\n4760281.8 (17.4)\n6364208.6 (18.4)\n0.915\n\n\n\n\nBetween.25kto54k\n8682481.6 (31.7)\n10786198.6 (31.2)\n\n\n\n\n\nBetween.55kto99k\n6939847.0 (25.4)\n9190388.2 (26.6)\n\n\n\n\n\nOver100k\n6987121.9 (25.5)\n8250648.6 (23.9)\n\n\n\n\nbmi (mean (SD))\n\n29.35 (6.13)\n29.64 (7.05)\n0.593\n\n\n\ndiabetes (%)\nNo\n25080412.0 (91.6)\n30006523.6 (86.7)\n0.012\n\n\n\n\nYes\n2289320.3 ( 8.4)\n4584920.4 (13.3)\n\n\n\n\n\n\n\nLogistic regression model\nRun a logistic regression model using the same variables, utilizing the survey features. Report the corresponding odds ratios and the 95% confidence intervals.\n\nShow the codeformula1 <- as.formula(I(cholesterol.bin==\"unhealthy\") ~ gender + born +\n                         race + education + married + income + bmi +\n                         diabetes)\n\nfit1 <- svyglm(formula1,\n               design = w.design, \n               family = binomial(link = \"logit\"))\n\npublish(fit1)\n#>   Variable              Units OddsRatio       CI.95  p-value \n#>     gender             Female       Ref                      \n#>                          Male      0.70 [0.49;0.98]   0.2866 \n#>       born         Born.in.US       Ref                      \n#>                        Others      2.10 [1.41;3.13]   0.1707 \n#>       race              Black       Ref                      \n#>                      Hispanic      1.15 [0.80;1.67]   0.5871 \n#>                         Other      1.11 [0.69;1.80]   0.7406 \n#>                         White      1.46 [1.00;2.14]   0.3003 \n#>  education            College       Ref                      \n#>                   High.School      1.21 [0.96;1.52]   0.3563 \n#>                        School      0.86 [0.52;1.43]   0.6712 \n#>    married            Married       Ref                      \n#>                 Never.married      0.54 [0.32;0.90]   0.2526 \n#>            Previously.married      1.31 [0.92;1.87]   0.3704 \n#>     income               <25k       Ref                      \n#>              Between.25kto54k      1.03 [0.61;1.73]   0.9408 \n#>              Between.55kto99k      1.02 [0.66;1.56]   0.9525 \n#>                      Over100k      1.12 [0.73;1.72]   0.6920 \n#>        bmi                         1.00 [0.97;1.03]   0.9361 \n#>   diabetes                 No       Ref                      \n#>                           Yes      0.62 [0.41;0.95]   0.2720\n\n\nWald test (survey version)\nPerform a Wald test (survey version) to test the null hypothesis that all coefficients associated with the income variable are zero, and interpret.\n\nShow the code#Testing the H0 that all coefficients associated with the income variable are zero\nregTermTest(fit1, ~income)\n#> Wald test for income\n#>  in svyglm(formula = formula1, design = w.design, family = binomial(link = \"logit\"))\n#> F =  0.1050099  on  3  and  1  df: p= 0.94611\n\n\nThe Wald test here gives a large p-value; We do not have evidence to reject the H0 of coefficient being 0. If the coefficient for income variable is 0, this means that the outcome in the model (cholesterol) is not affected by income. This suggests that removing income from the model does not statistically improve the model fit. So we can remove income variable from the model.\nBackward elimination\nRun a backward elimination (using the AIC criteria) on the above logistic regression fit (keeping important variables gender, race, bmi, diabetes in the model), and report the odds ratios and the 95% confidence intervals from the resulting final logistic regression fit.\n\nShow the code#Running backward elimination based on AIC\nrequire(MASS)\nscope <- list(upper = ~ gender + born + race + education + \n                married + income + bmi + diabetes,\n              lower = ~ gender + race + bmi + diabetes)\n\nfit3 <- step(fit1, scope = scope, trace = FALSE,\n                k = 2, direction = \"backward\")\n\n#Odds Ratios\npublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\n\n\nBorn and married are also found to be useful on top of gender + race + bmi + diabetes.\nInteraction terms\nChecking interaction terms\n– gender and whether married\n– gender and whether born in the US\n– gender and diabetes\n– whether married and diabetes\n\nShow the code#gender and married\nfit4 <- update(fit3, .~. + interaction(gender, married))\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for interaction(gender, married)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     married), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.7461308 p= 0.70903 \n#> (scale factors:  1.1 0.93 );  denominator df= 4\n\n\nDo not include interaction term\n\nShow the code#gender and born in us\nfit5 <- update(fit3, .~. + interaction(gender, born))\nanova(fit3, fit5)\n#> Working (Rao-Scott+F) LRT for interaction(gender, born)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     born), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.4635299 p= 0.52441 \n#> df=1;  denominator df= 5\n\n\nDo not include interaction term\n\nShow the code#gender and diabetes\nfit6 <- update(fit3, .~. + interaction(gender, diabetes))\nanova(fit3, fit6)\n#> Working (Rao-Scott+F) LRT for interaction(gender, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  1.222596 p= 0.32211 \n#> df=1;  denominator df= 5\n\n\nDo not include interaction term\n\nShow the code#married and diabetes\nfit7 <- update(fit3, .~. + interaction(married, diabetes))\nanova(fit3, fit7)\n#> Working (Rao-Scott+F) LRT for interaction(married, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(married, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.3207507 p= 0.84547 \n#> (scale factors:  1.4 0.62 );  denominator df= 4\n\n\nDo not include interaction term\nNone of the interaction terms are improving the model fit.\nAUC\nReport AUC of the final model (only using weight argument) and interpret.\nAUC of the final model (only using weight argument) and interpret\n\nShow the coderequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight){\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { \n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\nsvyROCw(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight)\n\n\n\n\nThe area under the curve in the final model is 0.611, using the survey weighted ROC. The AUC of 0.611 indicates that this model has poor discrimination.\nArcher-Lemeshow Goodness of fit\nReport Archer-Lemeshow Goodness of fit test and interpret (utilizing all the survey features).\n\nShow the code#Archer-Lemeshow Goodness of fit test utilizing all survey features\nAL.gof2 <- function(fit = fit3, data = anadata, \n                   weight = \"weight\", psu = \"psu\", strata = \"strata\"){\n  r <- residuals(fit, type = \"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f, (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                         data=data2g, nest = TRUE)\n  decilemodel <- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nAL.gof2(fit3, anadata, weight = \"weight\", psu = \"psu\", strata = \"strata\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  0.7569326  on  9  and  6  df: p= 0.66075\n\n\nArcher and Lemeshow GoF test was used to test the fit of this model. The p-value of 0.3043, which is greater than 0.05. This means that there is no evidence of lack of fit to this model.\nAdd age as a predictor for linear regression\nFit another logistic regression (similar to Q1) with the above-mentioned predictors (as obtained in Q7) and age, utilizing the survey features. What difference do you see from the previous fit results?\n\nShow the codeaic.int.model <- eval(fit3$call[[2]])\naic.int.model\n#> I(cholesterol.bin == \"unhealthy\") ~ gender + born + race + married + \n#>     bmi + diabetes\n\nformula3 <- as.formula(cholesterol.bin ~ gender + born + race + married + bmi + diabetes + age)\nfit9 <- svyglm(formula3,\n               design = w.design,\n               family = binomial(link=\"logit\"))\nsummary(fit9)\n#> \n#> Call:\n#> svyglm(formula = formula3, design = w.design, family = binomial(link = \"logit\"))\n#> \n#> Survey design:\n#> svydesign(id = ~psu, weights = ~weight, strata = ~strata, nest = TRUE, \n#>     data = anadata)\n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)                1.0657919  0.6260889   1.702   0.1494  \n#> genderMale                 0.3821902  0.1703394   2.244   0.0749 .\n#> bornOthers                -0.6912102  0.2026407  -3.411   0.0190 *\n#> raceHispanic              -0.2442019  0.1823190  -1.339   0.2381  \n#> raceOther                 -0.1570271  0.2306208  -0.681   0.5262  \n#> raceWhite                 -0.3638735  0.2029676  -1.793   0.1330  \n#> marriedNever.married       0.4029107  0.2637962   1.527   0.1872  \n#> marriedPreviously.married -0.2096009  0.1620478  -1.293   0.2524  \n#> bmi                       -0.0002237  0.0134117  -0.017   0.9873  \n#> diabetesYes                0.6534019  0.2456333   2.660   0.0449 *\n#> age                       -0.0151364  0.0042038  -3.601   0.0155 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1.000456)\n#> \n#> Number of Fisher Scoring iterations: 4\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\n\n\nComparing with previous model fit\n\nShow the codepublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\nAIC(fit3)\n#>       eff.p         AIC    deltabar \n#>   11.121795 1706.792543    1.235755\nAIC(fit9)\n#>      eff.p        AIC   deltabar \n#>   12.14310 1694.15974    1.21431\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata8.html",
    "href": "surveydata8.html",
    "title": "NHANES: Subsetting",
    "section": "",
    "text": "The tutorial demonstrates how to work with subset of complex survey data, specifically focusing on an NHANES example.\nThe required packages are loaded.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\n\n\nLoad data\nSurvey data is loaded into the R environment.\n\nShow the codeload(\"Data/surveydata/NHANES17.RData\")\nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\n\n\nCheck missingness\nA subset of variables is selected, and the presence of missing data is visualized.\n\nShow the codeVars <- c(\"ID\", \n          \"weight\", \n          \"psu\", \n          \"strata\", \n          \"gender\", \n          \"born\", \n          \"race\", \n          \"bmi\", \n          \"cholesterol\", \n          \"diabetes\")\nanalytic.full.data <- analytic.with.miss[,Vars]\n\n\nA new variable is also created to categorize cholesterol levels as “healthy” or “unhealthy.”\n\nShow the codeanalytic.full.data$cholesterol.bin <- ifelse(analytic.full.data$cholesterol <200, \"healthy\", \"unhealthy\")\nanalytic.full.data$cholesterol <- NULL\n\nrequire(DataExplorer)\nplot_missing(analytic.full.data)\n\n\n\n\nSubsetting Complex Survey data\nWe are subsetting based on whether the subjects have missing observation (e.g., only retaining those with complete information). This is often an eligibility criteria in studies. In missing data analysis, we will learn more about more appropriate approaches.\n\nShow the codedim(analytic.full.data)\n#> [1] 9254   10\nhead(analytic.full.data$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\nanalytic.complete.case.only <- as.data.frame(na.omit(analytic.full.data))\ndim(analytic.complete.case.only)\n#> [1] 6636   10\nhead(analytic.complete.case.only$ID) # complete case\n#> [1] 93705 93706 93707 93708 93709 93711\nhead(analytic.full.data$ID[analytic.full.data$ID %in% analytic.complete.case.only$ID])\n#> [1] 93705 93706 93707 93708 93709 93711\n\n\nBelow we show how to identify who has missing observations vs not based on full (analytic.full.data) and complete case (analytic.complete.case.only) data. See Heeringa et al (2010) book page 114 (section 4.5.3 “Preparation for Subclass analyses”) and also page 218 (section 7.5.4 “appropriate analysis: incorporating all Sample Design Features”). This is done for 2 reasons:\n\nfull complex survey design structure is taken into account, so that variance estimation is done correctly. If one or more PSU were excluded because none of the complete cases were observed in those PSU, the sub-population (complete cases) will not have complete information of how many PSU were actually present in the original complex design. Then in the population, a reduced number of PSUs would be used to calculate variance (number of SPU is a component of the variance calculation formula, see equation (5.2) in Heeringa et al (2010) textbook. Same is true for strata.), and will result in a wrong/biased variance estimate. Also see West et al. doi: 10.1177/1536867X0800800404\nsize of sub-population (here, those with complete cases) is recognized as a random variable; not just a fixed size.\n\n\nShow the code# assign missing indicator\nanalytic.full.data$miss <- 1 \n# assign missing indicator = 0 if the observation is available\nanalytic.full.data$miss[analytic.full.data$ID %in% analytic.complete.case.only$ID] <- 0\n\n\n\nShow the codetable(analytic.full.data$miss)\n#> \n#>    0    1 \n#> 6636 2618\n# IDs not in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==1])\n#> [1] 93703 93704 93710 93720 93724 93725\n# IDs in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==0])\n#> [1] 93705 93706 93707 93708 93709 93711\n\n\nLogistic regression on sub-population\nA logistic regression model is run on the subset of data that has no missing values. Here, it distinguishes between correct and incorrect approaches to account for the complex survey design.\n\nShow the coderequire(survey)\nrequire(Publish)\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\n\nWrong approach\n\nShow the codew.design.wrong <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.complete.case.only, #wrong!!\n                       nest = TRUE)\n\n\nCorrect approach\n\nShow the codew.design0 <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.full.data, \n                       nest = TRUE)\n\n# retain only those that have complete observation / no missing\nw.design <- subset(w.design0, miss == 0)# this is the subset design\n\n\nFull model\n\nShow the codefit <- svyglm(model.formula, family = quasibinomial, \n              design = w.design) # subset design\npublish(fit)\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\n\nVariable selection\nFinally, we discuss variable selection methods. We employ backward elimination to determine which variables are significant predictors while retaining an important variable in the model. If unsure about usefulness of some (gender, born, race, bmi) variables in predicting the outcome, check via backward elimination while keeping important variable (diabetes, say, that has been established in the literature) in the model\n\nShow the codemodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nscope <- list(upper = ~ diabetes+gender+born+race+bmi, lower = ~ diabetes)\n\nfit <- svyglm(model.formula, design=w.design, # subset design\n              family=quasibinomial)\n\nfitstep <- step(fit,  scope = scope, trace = FALSE, direction = \"backward\")\npublish(fitstep) # final model\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\n\nAlso see (Stata 2023) for further details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nStata. 2023. “How Can i Analyze a Subpopulation of My Survey Data in Stata?” https://stats.oarc.ucla.edu/stata/faq/how-can-i-analyze-a-subpopulation-of-my-survey-data-in-stata/."
  },
  {
    "objectID": "surveydataF.html",
    "href": "surveydataF.html",
    "title": "R functions (D)",
    "section": "",
    "text": "The list of new R functions introduced in this Survey data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n AIC \n    base/stats \n    To extract the AIC value of a model \n  \n\n as.character \n    base \n    To create a character vector \n  \n\n as.numeric \n    base \n    To create a numeric vector \n  \n\n eval \n    base \n    To evaluate an expression \n  \n\n fitted \n    base/stats \n    To extract fitted values of a model \n  \n\n ls \n    base \n    To see the list of objects \n  \n\n psrsq \n    survey \n    To compute the Nagelkerke and Cox-Snell pseudo R-squared statistics for survey data \n  \n\n regTermTest \n    survey \n    To test for an additional variable in a regression model \n  \n\n residuals \n    base/stats \n    To extract residuals of a model \n  \n\n stepAIC \n    MASS \n    To choose a model by stepwise AIC \n  \n\n step \n    base/stats \n    To choose a model by stepwise AIC but it can keep the pre-specified variables in the model \n  \n\n summ \n    jtools \n    To show/publish regression tables \n  \n\n svyboxplot \n    survey \n    To produce a box plot for survey data \n  \n\n svyby \n    survey \n    To see the summary statistics for a survey design \n  \n\n svychisq \n    survey \n    To test the bivariate assocaition between two categorical variables for survey data \n  \n\n svyCreateTableOne \n    tableone \n    To create a frequency table with a survey design \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n update \n    base/stats \n    To update and re-fit a regression model"
  },
  {
    "objectID": "surveydataQ.html",
    "href": "surveydataQ.html",
    "title": "Quiz (D)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydataE.html#problem-statement",
    "href": "surveydataE.html#problem-statement",
    "title": "Exercise (D)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will revisit the article by Flegal et al. (2016). Our primary aim this time is to execute the survey data analysis more rigorously, specifically by incorporating essential survey features into our analysis.\n\n\nThis is the same article that we discussed in our data access chapter!\nWe will reproduce some results from the article. The authors used NHANES 2013-14 dataset to create their main analytic dataset. The dataset contains 10,175 subjects with 12 relevant variables:\n\nSEQN: Respondent sequence number\nRIDAGEYR: Age in years at screening\nRIAGENDR: Gender\nDMDEDUC2: Education level\nRIDRETH3: Race/ethnicity\nRIDEXPRG: Pregnancy status at exam\nWTINT2YR: Full sample 2 year weights\nSDMVPSU: Masked variance pseudo-PSU\nSDMVSTRA: Masked variance pseudo-stratum\nBMXBMI: Body mass index in kg/m**2\nSMQ020: Whether smoked at least 100 cigarettes in life\nSMQ040: Current status of smoking (Do you now smoke cigarettes?)"
  },
  {
    "objectID": "surveydataE.html#question-1-creating-data-and-table",
    "href": "surveydataE.html#question-1-creating-data-and-table",
    "title": "Exercise (D)",
    "section": "Question 1: Creating data and table",
    "text": "Question 1: Creating data and table\n1(a) Importing dataset\n\nShow the code# you have to download the data in the same folder\nload(\"Data/surveydata/Flegal2016.RData\")\nls()\n#> [1] \"dat.full\"\nnames(dat.full)\n#>  [1] \"SEQN\"     \"RIDAGEYR\" \"RIAGENDR\" \"DMDEDUC2\" \"RIDRETH3\" \"RIDEXPRG\"\n#>  [7] \"WTINT2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BMXBMI\"   \"SMQ020\"   \"SMQ040\"\n\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria described in the second paragraph of the Methods section.\n\nHint: The authors restricted their study to\n\nadults aged 20 years and more,\nnon-missing body mass index, and\nnon-pregnant.\n\n\n\nYour analytic sample size should be 5,455, as described in the first sentence in the Results section.\n\nShow the code# 20+\ndat.analytic <- subset(dat.full, RIDAGEYR>=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic <- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ndat.analytic <- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\n\ndim(dat.analytic)\n#> [1] 5455   12\n\n\n1(c) Reproduce Table 1\nReproduce Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Please be advised to order the categories as shown in the table. tableone package could be helpful.\nHint 2: the authors did not show the results for the Other race category. But in your table, you could include all race categories.\n\n\nShow the codelibrary(tableone)\n\ndat <- dat.analytic\n\n# Age\ndat$age <- cut(dat$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat$gender <- dat$RIAGENDR\n\n# Race/Hispanic origin group\ndat$race <- dat$RIDRETH3\ndat$race <- car::recode(dat$race, \" 'Non-Hispanic White'='White'; 'Non-Hispanic Black'=\n                        'Black'; 'Non-Hispanic Asian'='Asian'; c('Mexican American',\n                        'Other Hispanic')='Hispanic'; 'Other Race - Including Multi-Rac'=\n                        'Other'; else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                      \"Hispanic\", \"Other\"))\n\n# Table 1: Overall \ntab11 <- CreateTableOne(vars = \"age\", strata = \"race\", data = dat, test = F, \n                        addOverall = T)\n\n# Table 1: Male\ntab12 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(dat, gender == \"Male\"))\n\n# Table 1: Female\ntab13 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(dat, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1a <- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1a, format = \"f\") # Showing only frequencies \n#> $Overall\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           5455    2343  1115  623   1214     160  \n#>   age                                                 \n#>      [20,40)  1810     734   362  216    412      86  \n#>      [40,60)  1896     759   383  251    449      54  \n#>      [60,Inf) 1749     850   370  156    353      20  \n#> \n#> $Male\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2638    1130  556   300   573      79   \n#>   age                                                 \n#>      [20,40)   909     386  182   106   189      46   \n#>      [40,60)   897     360  179   120   215      23   \n#>      [60,Inf)  832     384  195    74   169      10   \n#> \n#> $Female\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2817    1213  559   323   641      81   \n#>   age                                                 \n#>      [20,40)   901     348  180   110   223      40   \n#>      [40,60)   999     399  204   131   234      31   \n#>      [60,Inf)  917     466  175    82   184      10"
  },
  {
    "objectID": "surveydataE.html#question-2",
    "href": "surveydataE.html#question-2",
    "title": "Exercise (D)",
    "section": "Question 2",
    "text": "Question 2\n2(a) Reproduce Table 1 with survey features [15% grade]\nNot in this article but in many other articles, you would see n comes from the analytic sample and % comes from the survey design that accounts for survey features such as strata, clusters and survey weights. In Question 1, you see how n comes from the analytic sample. Your task for Question 2(a) is to create % part of the Table 1 with survey features, i.e., % should come from the survey design that accounts for strata, clusters and survey weights.\n\nHint 1: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 2: Generate age, gender, and race variable in your full data (codes shown in Question 1 could be helpful).\nHint 3: Subset the design.\nHint 4: Reproduce Table 1 with the design. svyCreateTableOne could be a helpful function.\n\n\nShow the code## Create all variables in the full data\n# Age\ndat.full$age <- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat.full$gender <- dat.full$RIAGENDR\n\n# Race/Hispanic origin group\ndat.full$race <- dat.full$RIDRETH3\ndat.full$race <- car::recode(dat.full$race, \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black'='Black'; 'Non-Hispanic Asian'='Asian'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             'Other Race - Including Multi-Rac'='Other'; \n                             else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                  \"Hispanic\", \"Other\"))\n\n## Subset the design\n# your codes here\n\n\n## Table 1 \n# your codes here\n\n#print(tab1b, format = \"p\") # Showing only percentages  \n\n\n2(b) Reproduce Table 3 [50% grade]\nReproduce the first column of Table 3 of the article (i.e., among men, explore the relationship between obesity and four predictors shown in the table).\n\nHint 1: If necessary, re-level or re-order the levels. Use Publish package to report the estimates.\nHint 2: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 3: The authors used SAS to produce the results vs. We are using R. The estimates could be slightly different (in second decimal point) from the estimates presented in Table 3, but they should be approximately similar.\nHint 4: You need to generate two variables, smoking status and education. The unweighted frequencies should be matched with the frequencies in eTable 1 and eTable 2.\n\nYour odds ratios could be look like as follows:\n\n\n\n\n\n\nShow the code## Recode Obese, Smoking status, Education - work on full data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Reproduce Table 3 - column 1\n# your codes here\n\n\n2(c) Model selection [25% grade]\nFrom the literature, you know that age and race needs to be adjusted in the model, but you are not sure about smoking and education. Run an AIC based backward selection process to figure out whether you want to add smoking or education, or both in the final model in 2(b). What is your conclusion [Expected answer: one short sentence]?\n\nHint 1: You need to make sure your design (that is based on eligibility) is free from missing values. Even after applying eligibility criteria, you may have some missing values on multiple variables (see eTable 1 and eTable 2). This is especially important for model selection process.\nHint 2: Work with the analytic data, keep only the relevant variables, and then remove missing values. Finally, subset the design and then select your final model.\n\n\nShow the code## Recode Obese, Smoking status, Education - work on analytic data\n# your codes here\n\n\n## Remove missing values - work on analytic data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Model selection \n# your codes here\n\n\n2(d) Testing for interactions [10% grade]\nCheck whether the interaction between age and smoking should be added in the 2(b) model (yes or no answer required, along with the code and p-value):\n\nShow the code# your codes here"
  },
  {
    "objectID": "missingdata.html#background",
    "href": "missingdata.html#background",
    "title": "Missing data analysis",
    "section": "Background",
    "text": "Background\nThe chapter provides a comprehensive guide on missing data analysis, emphasizing various imputation techniques to address data gaps. It begins by introducing the concept of imputation and the different types of missing data: MCAR, MAR, and MNAR. The tutorial then delves into multiple imputation methods for complex survey data, highlighting the importance of visualizing missing data patterns, creating multiple imputed datasets, and pooling results for a consolidated analysis. The challenges of imputing dependent and exposure variables are addressed, with a focus on the benefits of using auxiliary variables. The guide also explores the estimation of model performance in datasets with missing values, using metrics like the AUC and the Archer-Lemeshow test. Special attention is given to handling subpopulations with missing observations, testing the MCAR assumption empirically, and understanding effect modification within multiple imputation.\n\n\nIn the vast landscape of survey data analysis, one challenge consistently emerges as both a hurdle and an opportunity: missing data. As we delve into this chapter, we’ll confront the often-encountered issue of incomplete or absent data points in survey datasets. Missing data isn’t just a challenge; it’s an invitation to refine our analytical techniques. This chapter will equip you with the tools and methodologies to handle missing data adeptly, ensuring that our survey data analysis remains robust and reliable.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "missingdata.html#overview-of-tutorials",
    "href": "missingdata.html#overview-of-tutorials",
    "title": "Missing data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nMissing data and imputation\nImputation is a technique used to replace missing data with substituted values. In health research, missing data is a common issue, and imputation helps in ensuring datasets are complete, leading to more accurate analyses. There are three types of missing data: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR). The type of missingness determines how the missing data should be handled. Various single imputation methods, such as mean imputation, regression imputation, and predictive mean matching, are used based on the nature of the missing data. Multiple imputation is a process where the incomplete dataset is imputed multiple times, and the results are pooled together for more accurate analyses. Variable selection is crucial when analyzing missing data, and methods like majority, stack, and Wald are used to determine the best model. It’s also essential to assess the impact of missing values on model fitting (convergence and diagnostics) to ensure the reliability of the results.\n\n\nMultiple imputation in complex survey data\nIn the tutorial involve understanding how to assess the missing data patterns and visualize to understand the extent of missingness. Multiple imputations are then performed to address the missing data, creating multiple versions of the dataset with varying imputations. After imputation, new variables are created or modified for analysis, and the integrity of the imputed data is checked both visually. The tutorial also emphasizes the importance of combining multiple imputed datasets for analysis. Logistic regression is applied to the imputed datasets, and the results are pooled to get a single set of estimates. The tutorial concludes with a variable selection process to identify the most relevant variables for the model.\n\n\nMultiple imputation then deletion (MID)\nThis tutorial emphasizes the challenges of imputing dependent and exposure variables. The tutorial underscores the potential benefits of using auxiliary variables in the imputation process. While traditional Multiple Imputation (MI) and MID can yield similar results, MID is particularly advantageous when there’s a significant percentage of missing values in the outcome variable. The tutorial walks through the process of data loading, identifying missing values, performing standard imputations, and adding missing indicators. Subsequent steps involve structuring the data for survey design, fitting statistical models to each imputed dataset, and pooling the results for a consolidated analysis. The final stages focus on calculating and presenting odds ratios to interpret the relationships between variables.\n\n\nModel performance from multiple imputed datasets\nIn the context of survey data analysis, the provided tutorial outlines the process of estimating model performance, particularly when dealing with weighted data that has missing values. The focus is about estimating treatment effects, both individually and in a pooled manner. Model performance is gauged using the Area Under the Curve (AUC) and the Archer-Lemeshow (AL) test. This is done for models with and without interactions. The results provide insights into the model’s accuracy and fit, with the AUC offering a measure of the model’s discriminative ability and the AL test indicating the model’s goodness of fit to the data. The appendix provides a closer look at the user-defined functions used throughout the analysis.\n\n\nDealing with subpopulations with missing observations\nThe primary objective is to showcase how to handle missing data analysis with multiple imputation in the backdrop of complex surveys, particularly when we are interested in subpopulations. The process involves working with the analytic data, imputing missing values from this dataset, accounting for ineligible subjects from the complete data, and reincorporating these ineligible subjects into the imputed datasets. This ensures that the survey’s features can be utilized and the design subsetted accordingly. After importing and inspecting the dataset, the analysis subsets the data based on eligibility criteria, imputes missing values, and prepares the survey design. The subsequent steps involve design-adjusted logistic regression and pooling of estimates using Rubin’s rule.\n\n\nTesting MCAR assumption empirically in the data\nThe tutorial discusses the process of testing for Missing Completely At Random (MCAR) in datasets. Initially, essential packages are loaded to facilitate the analysis. A DAG is defined to represent the causal relationships between dataset variables, and this DAG is used to simulate a dataset. The DAG is then visualized, and the simulated dataset undergoes random data omission to mimic MCAR scenarios. Various visualizations, such as margin plots, are employed to understand the distribution of missing values in relation to other variables. Little’s MCAR test, a statistical method, is applied to determine if the data is indeed MCAR. The test’s limitations are also discussed. Additionally, tests for multivariate normality and homoscedasticity are conducted. In a subsequent section, data is intentionally set to missing based on a specific rule, and similar analyses and visualizations are performed to understand the nature of this missingness.\n\n\nEffect modification within multiple imputation\nThe tutorial delves into the intricacies of effect modification within the realm of survey data analysis. A dataset is comprising several imputed datasets is used. The primary objective is to investigate how two specific variables interact in predicting a particular outcome. To this end, logistic regression models are constructed for each level of a categorical variable. Emphasis is placed on the significance of Odds Ratios (ORs) in interpreting these interactions. Subsequently, simple slopes analyses are performed for each imputed dataset, shedding light on the relationship between the predictor and the outcome at distinct levels of a moderating variable. The outcomes from each imputed dataset are then pooled to offer a comprehensive understanding of the effect modification.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "missingdata1.html",
    "href": "missingdata1.html",
    "title": "Imputation",
    "section": "",
    "text": "What is imputation?\nImputation is the process of replacing missing data with substituted values. In health research, it’s common to have missing data. This tutorial teaches you how to handle and replace these missing values using the mice package in R.\nWhy is imputation important?\nMissing data can lead to biased or incorrect results. Imputation helps in making the dataset complete, which can lead to more accurate analyses.\nKey reference\nIn this discussion, our primary guide and source of information is the work titled “Flexible Imputation of Missing Data” by Stef van Buuren, denoted here as (Van Buuren 2018). This book is an invaluable resource for anyone looking to delve deeper into the intricacies of handling missing data, especially in the context of statistical analyses. Below we also cited the relevant section numbers.\nFirst, you need to load the necessary libraries:\n\nShow the code# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(mitools)\n\n\nType of missing data\n\nRef: (Van Buuren 2018), Section 1.2\n\nIn this section, we are going to introduce three types of missing data that we will encounter in data analysis.\n\n\nMissing Completely at Random (MCAR):\n\nThe reason data is missing is completely random and not related to any measured or unmeasured variables. It’s often an unrealistic assumption.\n\n\nMissing at Random (MAR):\n\nThe missing data is related to variables that are observed.\n\n\nMissing Not at Random (MNAR):\n\nThe missing data is related to variables that are not observed.\nWhy does missingness type matter?\nThe type of missingness affects how you handle the missing data:\n\nIf data is MCAR, you can still analyze the complete cases without introducing bias.\nIf data is MAR, you can use imputation to replace the missing values.\nIf data is MNAR, it’s challenging to address, and estimates will likely be biased. We could do some sensitivity analyses to check the impact.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nData imputation\nGetting to know the data\nBefore imputing, you should understand the data. The tutorial uses the analytic.with.miss dataset from NHANES. Various plots and functions are used to inspect the missing data pattern and relationships between variables.\n\n\n\n\n\n\nImportant\n\n\n\n\nTake a look here for those who are interested in how the analytic data was created.\nFor the purposes of this lab, we are just going to treat the data as SRS, and not going to deal with intricacies of survey data analysis.\n\n\n\n\nShow the coderequire(VIM)\nload(\"Data/missingdata/NHANES17.RData\")\nNHANES17s <- analytic.with.miss[1:30,c(\"age\", \"bmi\", \"cholesterol\",\"diastolicBP\")]\nNHANES17s\n\n\n\n  \n\n\nShow the codeNHANES17s[complete.cases(NHANES17s),]\n\n\n\n  \n\n\nShow the codemd.pattern(NHANES17s) \n\n\n\n#>    bmi cholesterol age diastolicBP   \n#> 15   1           1   1           1  0\n#> 4    1           1   1           0  1\n#> 4    1           1   0           1  1\n#> 1    1           0   1           1  1\n#> 4    1           0   0           0  3\n#> 2    0           0   0           0  4\n#>      2           7  10          10 29\n# Inspect the missing data pattern (each row = pattern)\n# possible missingness (0,1) pattern and counts\n# last col = missing counts for each variables\n# last row = how many variable values missing in the row\n# First col: Frequency of the pattern \n# e,g, 2 cases missing for bmi\n\nrequire(DataExplorer)\nplot_missing(NHANES17s)\n\n\n\nShow the code# check the missingness\n\nrequire(VIM)\nmarginplot(NHANES17s[, c(\"diastolicBP\", \"bmi\")])\n\n\n\nShow the codemarginplot(NHANES17s[, c(\"diastolicBP\", \"cholesterol\")])\n\n\n\nShow the codemarginplot(NHANES17s[, c(\"cholesterol\", \"bmi\")])\n\n\n\nShow the code# distribution of observed data given the other variable is observed\n# for MCAR, blue and red box plots should be similar\n\n\nSingle imputation\n\nRef: (Van Buuren 2018), Section 1.3\n\nImpute NA only once. Below are some examples (Van Buuren and Groothuis-Oudshoorn 2011):\nMean imputation\n\nRef: (Van Buuren 2018), Section 1.3.3, and (Buuren and Groothuis-Oudshoorn 2010)\n\n\nMean imputation is a straightforward method where missing values in a dataset are replaced with the mean of the observed values. While it’s simple and intuitive, this approach can reduce the overall variability of the data, leading to an underestimation of variance. This can be problematic in statistical analyses where understanding data spread is crucial.\n\nShow the code# Replace missing values by mean \nimputation1 <- mice(NHANES17s, \n                   method = \"mean\", # Replace by mean of the other values\n                   m = 1, # Number of multiple imputations. \n                   maxit = 1) # Number of iteration; mostly useful for convergence\n#> \n#>  iter imp variable\n#>   1   1  age  bmi  cholesterol  diastolicBP\nimputation1$imp\n#> $age\n#>       1\n#> 1  57.4\n#> 2  57.4\n#> 4  57.4\n#> 5  57.4\n#> 8  57.4\n#> 10 57.4\n#> 17 57.4\n#> 18 57.4\n#> 22 57.4\n#> 23 57.4\n#> \n#> $bmi\n#>           1\n#> 8  25.12857\n#> 18 25.12857\n#> \n#> $cholesterol\n#>           1\n#> 1  178.8261\n#> 2  178.8261\n#> 8  178.8261\n#> 18 178.8261\n#> 22 178.8261\n#> 23 178.8261\n#> 30 178.8261\n#> \n#> $diastolicBP\n#>       1\n#> 1  67.6\n#> 2  67.6\n#> 3  67.6\n#> 6  67.6\n#> 8  67.6\n#> 12 67.6\n#> 18 67.6\n#> 22 67.6\n#> 23 67.6\n#> 25 67.6\ncomplete(imputation1, action = 1) # this is a function from mice\n\n\n\n  \n\n\nShow the code# there is another function in tidyr with the same name!\n# use mice::complete() to avoid conflict\n## the imputed dataset\n\n\nRegression Imputation\n\nRef: (Van Buuren 2018), Section 1.3.4\n\nRegression imputation offers a more nuanced approach, especially when dealing with interrelated variables. By building a regression model using observed data, missing values are predicted based on the relationships between variables. This method can provide more accurate estimates for missing values by leveraging the inherent correlations within the data, making it a preferred choice in many scenarios over mean imputation.\n\\(Y \\sim X\\)\n\\(age \\sim bmi + cholesterol + diastolicBP\\)\n\nShow the codeimputation2 <- mice(NHANES17s, \n            method = \"norm.predict\", # regression imputation\n            seed = 1,\n            m = 1, \n            print = FALSE)\n\n# look at all imputed values\nimputation2$imp\n#> $age\n#>           1\n#> 1  55.32215\n#> 2  54.99604\n#> 4  55.65437\n#> 5  53.68539\n#> 8  56.70424\n#> 10 55.79329\n#> 17 54.38372\n#> 18 56.70422\n#> 22 54.81486\n#> 23 55.06851\n#> \n#> $bmi\n#>           1\n#> 8  25.12857\n#> 18 25.12857\n#> \n#> $cholesterol\n#>           1\n#> 1  183.3772\n#> 2  184.2347\n#> 8  179.7431\n#> 18 179.7431\n#> 22 184.7111\n#> 23 184.0442\n#> 30 183.4399\n#> \n#> $diastolicBP\n#>           1\n#> 1  66.48453\n#> 2  66.24254\n#> 3  68.82463\n#> 6  67.47980\n#> 8  67.51011\n#> 12 68.91318\n#> 18 67.51011\n#> 22 66.10810\n#> 23 66.29631\n#> 25 67.93401\n\n# examine the correlation between age and bmi before and after imputation\nfit1 <- lm(age ~ bmi, NHANES17s) \n\nsummary(fit1) ## original data\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = NHANES17s)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.383  -5.194   3.168   9.444  15.965 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#> bmi           0.1462     0.6063   0.241  0.81219   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.51 on 18 degrees of freedom\n#>   (10 observations deleted due to missingness)\n#> Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#> F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nsqrt(summary(fit1)$r.squared)\n#> [1] 0.05674047\n\nfit2 <- lm(age ~ bmi, mice::complete(imputation2)) \nsummary(fit2) ## imputed complete data\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = mice::complete(imputation2))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.152  -1.407   0.000   8.026  15.989 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  52.1516     9.2485   5.639 4.86e-06 ***\n#> bmi           0.1812     0.3568   0.508    0.616    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.45 on 28 degrees of freedom\n#> Multiple R-squared:  0.009127,   Adjusted R-squared:  -0.02626 \n#> F-statistic: 0.2579 on 1 and 28 DF,  p-value: 0.6155\nsqrt(summary(fit2)$r.squared)\n#> [1] 0.09553283\n## Relationship become stronger before imputation. \n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation2), cor(age, bmi))\n#> [1] 0.09553283\n\n\nStochastic regression imputation\n\nRef: (Van Buuren 2018), Section 1.3.5\n\nRegression imputation, while powerful, has an inherent limitation. When it employs the fitted model to predict missing values, it does so without incorporating the error terms. This means that the imputed values are precisely on the regression line, leading to an overly perfect fit. As a result, the natural variability present in real-world data is not captured, causing the imputed dataset to exhibit biased correlations and reduced variance. Essentially, the data becomes too “clean,” and this lack of variability can mislead subsequent analyses, making them overly optimistic or even erroneous.\nRecognizing this limitation, stochastic regression imputation was introduced as an enhancement. Instead of merely using the fitted model, it adds a randomly drawn error term during the imputation process. This error term reintroduces the natural variability that the original regression imputation method missed. By doing so, the imputed values are scattered around the regression line, more accurately reflecting the true correlations and distributions in the dataset. This method, therefore, offers a more realistic representation of the data, ensuring that subsequent analyses are grounded in a dataset that mirrors genuine variability and relationships.\n\\(Y \\sim X + e\\)\n\\(age \\sim bmi + cholesterol + diastolicBP + error\\)\n\nShow the codeimputation3 <- mice(NHANES17s, method = \"norm.nob\", # stochastic regression imputation\n                    m = 1, maxit = 1, seed = 504, print = FALSE)\n\n# look at all imputed values\nimputation3$imp\n#> $age\n#>           1\n#> 1  79.59513\n#> 2  53.94601\n#> 4  73.76486\n#> 5  43.69817\n#> 8  49.70112\n#> 10 43.81002\n#> 17 29.72590\n#> 18 61.15172\n#> 22 58.75506\n#> 23 78.39545\n#> \n#> $bmi\n#>           1\n#> 8  27.53270\n#> 18 31.52568\n#> \n#> $cholesterol\n#>           1\n#> 1  252.2928\n#> 2  209.7680\n#> 8  169.2450\n#> 18 107.7585\n#> 22 181.8617\n#> 23 239.9008\n#> 30 131.8489\n#> \n#> $diastolicBP\n#>           1\n#> 1  75.02181\n#> 2  44.45935\n#> 3  86.69637\n#> 6  60.54256\n#> 8  63.80884\n#> 12 60.03311\n#> 18 73.94575\n#> 22 36.70323\n#> 23 73.95647\n#> 25 65.84012\n#mice::complete(imputation3)\n\n# examine the correlation between age and bmi before and after imputation\nfit1 <- lm(age ~ bmi, NHANES17s) \nsummary(fit1) \n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = NHANES17s)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.383  -5.194   3.168   9.444  15.965 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#> bmi           0.1462     0.6063   0.241  0.81219   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.51 on 18 degrees of freedom\n#>   (10 observations deleted due to missingness)\n#> Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#> F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nfit3 <- lm(age ~ bmi, mice::complete(imputation3)) \nsummary(fit3)\n#> \n#> Call:\n#> lm(formula = age ~ bmi, data = mice::complete(imputation3))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.089  -6.691   3.183  10.104  21.288 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  60.4173    11.4671   5.269 1.33e-05 ***\n#> bmi          -0.1206     0.4371  -0.276    0.785    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 15.53 on 28 degrees of freedom\n#> Multiple R-squared:  0.002712,   Adjusted R-squared:  -0.03291 \n#> F-statistic: 0.07614 on 1 and 28 DF,  p-value: 0.7846\n## Fitted coefficients of bmi are much closer before and after imputation\n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation3), cor(age, bmi))\n#> [1] -0.05207502\n# see the direction change?\n\n\nPredictive mean matching\nPredictive Mean Matching (PMM) is an advanced imputation technique that aims to provide more realistic imputations for missing data. Let’s break it down:\nIn this context, we’re trying to fill in missing values for the variable ‘age’. To do this, we use other variables like ‘bmi’, ‘cholesterol’, and ‘diastolicBP’ to predict ‘age’. First, a regression model is run using the available data to estimate the relationship between ‘age’ and the predictor variables. From this model, we get a coefficient, which is then adjusted slightly to introduce some randomness. Using this adjusted coefficient, we predict the missing ‘age’ values for all subjects. For example, if ‘subject 19’ has a missing age value, we might predict it to be 45.5 years. Instead of using this predicted value directly, we look for other subjects who have actual age values and whose predicted ages are close to 45.5 years. From these subjects, one is randomly chosen, and their real age is used as the imputed value for ‘subject 19’. In this way, PMM ensures that the imputed values are based on real, observed data from the dataset.\n\n\n\n\n\n\nTip\n\n\n\n\nAssume \\(Y\\) = age, a variable with some missing values. \\(X\\) (say, bmi, cholesterol, diastolicBP) are predictors of \\(Y\\).\nEstimate beta coef \\(\\beta\\) from complete case running \\(Y \\sim X + e\\)\n\ngenerate new \\(\\beta* \\sim Normal(b,se_b)\\).\nusing \\(\\beta*\\), predict new \\(\\hat{Y}\\) predicted age for all subjects (those with missing and observed age):\n\nIf subject 19 (say) has missing values in age variable, find out his predicted age \\(\\hat{Y}\\) (say, 45.5).\nFind others subjects, subjects 2, 15, 24 (say) who has their age measured and their predicted age \\(\\hat{Y}\\) (say, predicted ages 43.9,45.7,46.1 with real ages 43,45,46 respectively) are close to subject 19 (predicted age 45.5).\nRandomly select subject 2 with real/observed age 43, and impute 43 for subject 19’s missing age.\n\n\n\n\n\nThe strength of PMM lies in its approach. Instead of imputing a potentially artificial value based on a prediction, it imputes a real, observed value from the dataset. This ensures that the imputed data retains the original data’s characteristics and doesn’t introduce any unrealistic values. It offers a safeguard against extrapolation, ensuring that the imputed values are always within the plausible range of the dataset.\n\nShow the codeimputation3b <- mice(NHANES17s, method = \"pmm\", \n                    m = 1, maxit = 1,\n                    seed = 504, print = FALSE)\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#> [1] 0.05674047\nwith(data = mice::complete(imputation3b), cor(age, bmi))\n#> [1] -0.08029207\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nMultiple imputation and workflow\n\nRef: (Van Buuren 2018), Sections 1.4 and 5.1\nRef: (Buuren and Groothuis-Oudshoorn 2010)\n\n\nWe have learned different methods of imputation. In this section, we will introduce how to incorporate the data imputation into data analysis. In multiple imputation data analysis, three steps will be taken:\n\n\nStep 0: Set imputation model: Before starting the imputation process, it’s crucial to determine the appropriate imputation model based on the nature of the missing data and the relationships between variables. This model will guide how the missing values are estimated. For instance, if the data is missing at random, a linear regression model might be used for continuous data, while logistic regression might be used for binary data. The choice of model can significantly impact the quality of the imputed data, so it’s essential to understand the underlying mechanisms causing the missingness and select a model accordingly.\n\nStep 1: The incomplete dataset will be imputed \\(m\\) times: In this step, the incomplete dataset is imputed multiple times, resulting in \\(m\\) different “complete” datasets. The reason for creating multiple datasets is to capture the uncertainty around the missing values. Each of these datasets will have slightly different imputed values, reflecting the variability and uncertainty in the imputation process. The number of imputations, \\(m\\), is typically chosen based on the percentage of missing data and the desired level of accuracy. Common choices for \\(m\\) range from 5 to 50, but more imputations provide more accurate results, especially when the percentage of missing data is high.\n\nStep 2: Each \\(m\\) complete datasets will be analyzed separately by standard analysis (e.g., regression model): Once the \\(m\\) complete datasets are generated, each one is analyzed separately using standard statistical methods. For example, if the research question involves understanding the relationship between two variables, a regression model might be applied to each dataset. This step produces \\(m\\) sets of analysis results, one for each imputed dataset.\n\nStep 3: The analysis results will be pooled / aggregated together by Rubin’s rules (1987): The final step involves combining the results from the \\(m\\) separate analyses into a single set of results. This is done using Rubin’s rules (1987), which provide a way to aggregate the estimates and adjust for the variability between the imputed datasets. The pooled results give a more accurate and robust estimate than analyzing a single imputed dataset. Rubin’s rules ensure that the combined results reflect both the within-imputation variability (the variability in results from analyzing each dataset separately) and the between-imputation variability (the differences in results across the imputed datasets).\n\nStep 0\nSet imputation model:\n\nShow the codeini <- mice(data = NHANES17s, maxit = 0, print = FALSE)\npred <- ini$pred\npred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           1   0           1           1\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n# A value of 1 indicates that column variables (say, bmi, cholesterol, diastolicBP) \n# are used as a predictor to impute the a row variable (say, age).\npred[,\"diastolicBP\"] <- 0 \n# if you believe 'diastolicBP' should not be a predictor in any imputation model\npred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           0\n#> bmi           1   0           1           0\n#> cholesterol   1   1           0           0\n#> diastolicBP   1   1           1           0\n# for cholesterol: bmi and age used to predict cholesterol (diastolicBP is not a predictor)\n# for diastolicBP: bmi, age and cholesterol used to predict diastolicBP \n# (diastolicBP itself is not a predictor) \n\n\nSet imputation method:\nSee Table 1 of (Van Buuren and Groothuis-Oudshoorn 2011).\n\nShow the codemeth <- ini$meth\nmeth\n#>         age         bmi cholesterol diastolicBP \n#>       \"pmm\"       \"pmm\"       \"pmm\"       \"pmm\"\n# pmm is generally a good method, \n# but let's see how to work with other methods\n# just as an example.\n# Specifying imputation method:\nmeth[\"bmi\"] <- \"mean\" \n# for BMI: no predictor used in mean method \n# (only average of observed bmi)\nmeth[\"cholesterol\"] <- \"norm.predict\" \nmeth[\"diastolicBP\"] <- \"norm.nob\"\nmeth\n#>            age            bmi    cholesterol    diastolicBP \n#>          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\"\n\n\nSet imputation model based on correlation alone:\n\nShow the codepredictor.selection <- quickpred(NHANES17s, \n                                 mincor=0.1, # absolute correlation \n                                 minpuc=0.1) # proportion of usable cases\npredictor.selection\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n\n\nStep 1\n\nShow the code# Step 1 Impute the incomplete data m=10 times\nimputation4 <- mice(data=NHANES17s, \n                    seed=504,\n                    method = meth,\n                    predictorMatrix = predictor.selection,\n                    m=10, # imputation will be done 10 times (i.e., 10 imputed datasets)\n                    maxit=3)\n#> \n#>  iter imp variable\n#>   1   1  age  bmi  cholesterol  diastolicBP\n#>   1   2  age  bmi  cholesterol  diastolicBP\n#>   1   3  age  bmi  cholesterol  diastolicBP\n#>   1   4  age  bmi  cholesterol  diastolicBP\n#>   1   5  age  bmi  cholesterol  diastolicBP\n#>   1   6  age  bmi  cholesterol  diastolicBP\n#>   1   7  age  bmi  cholesterol  diastolicBP\n#>   1   8  age  bmi  cholesterol  diastolicBP\n#>   1   9  age  bmi  cholesterol  diastolicBP\n#>   1   10  age  bmi  cholesterol  diastolicBP\n#>   2   1  age  bmi  cholesterol  diastolicBP\n#>   2   2  age  bmi  cholesterol  diastolicBP\n#>   2   3  age  bmi  cholesterol  diastolicBP\n#>   2   4  age  bmi  cholesterol  diastolicBP\n#>   2   5  age  bmi  cholesterol  diastolicBP\n#>   2   6  age  bmi  cholesterol  diastolicBP\n#>   2   7  age  bmi  cholesterol  diastolicBP\n#>   2   8  age  bmi  cholesterol  diastolicBP\n#>   2   9  age  bmi  cholesterol  diastolicBP\n#>   2   10  age  bmi  cholesterol  diastolicBP\n#>   3   1  age  bmi  cholesterol  diastolicBP\n#>   3   2  age  bmi  cholesterol  diastolicBP\n#>   3   3  age  bmi  cholesterol  diastolicBP\n#>   3   4  age  bmi  cholesterol  diastolicBP\n#>   3   5  age  bmi  cholesterol  diastolicBP\n#>   3   6  age  bmi  cholesterol  diastolicBP\n#>   3   7  age  bmi  cholesterol  diastolicBP\n#>   3   8  age  bmi  cholesterol  diastolicBP\n#>   3   9  age  bmi  cholesterol  diastolicBP\n#>   3   10  age  bmi  cholesterol  diastolicBP\nimputation4$pred\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n## look at the variables used for imputation\nmice::complete(imputation4, action = 1) # 1 imputed data  \n\n\n\n  \n\n\nShow the codeall <- mice::complete(imputation4, action=\"long\") # combine all 5 imputed datasets\ndim(all)\n#> [1] 300   6\nhead(all)\n\n\n\n  \n\n\nShow the code## you can change the way of displaying the data\ndata_hori <- mice::complete(imputation4, action=\"broad\") # display five imputations horizontally\n#> New names:\n#> • `age` -> `age...1`\n#> • `bmi` -> `bmi...2`\n#> • `cholesterol` -> `cholesterol...3`\n#> • `diastolicBP` -> `diastolicBP...4`\n#> • `age` -> `age...5`\n#> • `bmi` -> `bmi...6`\n#> • `cholesterol` -> `cholesterol...7`\n#> • `diastolicBP` -> `diastolicBP...8`\n#> • `age` -> `age...9`\n#> • `bmi` -> `bmi...10`\n#> • `cholesterol` -> `cholesterol...11`\n#> • `diastolicBP` -> `diastolicBP...12`\n#> • `age` -> `age...13`\n#> • `bmi` -> `bmi...14`\n#> • `cholesterol` -> `cholesterol...15`\n#> • `diastolicBP` -> `diastolicBP...16`\n#> • `age` -> `age...17`\n#> • `bmi` -> `bmi...18`\n#> • `cholesterol` -> `cholesterol...19`\n#> • `diastolicBP` -> `diastolicBP...20`\n#> • `age` -> `age...21`\n#> • `bmi` -> `bmi...22`\n#> • `cholesterol` -> `cholesterol...23`\n#> • `diastolicBP` -> `diastolicBP...24`\n#> • `age` -> `age...25`\n#> • `bmi` -> `bmi...26`\n#> • `cholesterol` -> `cholesterol...27`\n#> • `diastolicBP` -> `diastolicBP...28`\n#> • `age` -> `age...29`\n#> • `bmi` -> `bmi...30`\n#> • `cholesterol` -> `cholesterol...31`\n#> • `diastolicBP` -> `diastolicBP...32`\n#> • `age` -> `age...33`\n#> • `bmi` -> `bmi...34`\n#> • `cholesterol` -> `cholesterol...35`\n#> • `diastolicBP` -> `diastolicBP...36`\n#> • `age` -> `age...37`\n#> • `bmi` -> `bmi...38`\n#> • `cholesterol` -> `cholesterol...39`\n#> • `diastolicBP` -> `diastolicBP...40`\n\ndim(data_hori)\n#> [1] 30 40\nhead(data_hori)\n\n\n\n  \n\n\nShow the code\n## Compare means of each imputed dataset\ncolMeans(data_hori)\n#>          age.1          bmi.1  cholesterol.1  diastolicBP.1          age.2 \n#>       61.06667       25.12857      179.47152       68.06998       58.20000 \n#>          bmi.2  cholesterol.2  diastolicBP.2          age.3          bmi.3 \n#>       25.12857      179.71210       66.61289       57.40000       25.12857 \n#>  cholesterol.3  diastolicBP.3          age.4          bmi.4  cholesterol.4 \n#>      180.28681       68.77854       54.86667       25.12857      179.58873 \n#>  diastolicBP.4          age.5          bmi.5  cholesterol.5  diastolicBP.5 \n#>       66.13461       52.80000       25.12857      179.35478       66.74254 \n#>          age.6          bmi.6  cholesterol.6  diastolicBP.6          age.7 \n#>       60.00000       25.12857      179.22499       66.76592       58.43333 \n#>          bmi.7  cholesterol.7  diastolicBP.7          age.8          bmi.8 \n#>       25.12857      178.99900       66.82760       56.20000       25.12857 \n#>  cholesterol.8  diastolicBP.8          age.9          bmi.9  cholesterol.9 \n#>      179.63656       67.31160       59.06667       25.12857      179.58295 \n#>  diastolicBP.9         age.10         bmi.10 cholesterol.10 diastolicBP.10 \n#>       66.37078       55.63333       25.12857      179.69042       67.60832\n\n\nStep 2\n\nShow the codeimputation4\n#> Class: mids\n#> Number of multiple imputations:  10 \n#> Imputation methods:\n#>            age            bmi    cholesterol    diastolicBP \n#>          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\" \n#> PredictorMatrix:\n#>             age bmi cholesterol diastolicBP\n#> age           0   1           1           1\n#> bmi           0   0           0           0\n#> cholesterol   1   1           0           1\n#> diastolicBP   1   1           1           0\n\n\n\nShow the codeimputation4[[1]]\n\n\n\n  \n\n\n\n\nShow the codemice::complete(imputation4, action = 1)\n\n\n\n  \n\n\n\n\nShow the codemice::complete(imputation4, action = 10)\n\n\n\n  \n\n\n\n\nShow the code# Step 2 Analyze the imputed data\nfit4 <- with(data = imputation4, exp = lm(cholesterol ~ age + bmi + diastolicBP))\n## fit model with each of 10 datasets separately\nfit4\n#> call :\n#> with.mids(data = imputation4, expr = lm(cholesterol ~ age + bmi + \n#>     diastolicBP))\n#> \n#> call1 :\n#> mice(data = NHANES17s, m = 10, method = meth, predictorMatrix = predictor.selection, \n#>     maxit = 3, seed = 504)\n#> \n#> nmis :\n#>         age         bmi cholesterol diastolicBP \n#>          10           2           7          10 \n#> \n#> analyses :\n#> [[1]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   210.68650     -0.19085     -0.52112     -0.09498  \n#> \n#> \n#> [[2]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   185.56395      0.06366     -0.49154      0.04196  \n#> \n#> \n#> [[3]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   188.01460     -0.07922     -0.52325      0.14493  \n#> \n#> \n#> [[4]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    210.2814       0.1096      -0.4602      -0.3802  \n#> \n#> \n#> [[5]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    167.8965       0.7171      -0.7613      -0.1090  \n#> \n#> \n#> [[6]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    187.4324       0.1727      -0.3452      -0.1482  \n#> \n#> \n#> [[7]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   203.45344     -0.22713     -0.37039     -0.02806  \n#> \n#> \n#> [[8]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>  213.721570    -0.003491    -0.517870    -0.310132  \n#> \n#> \n#> [[9]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>   205.74248     -0.16224     -0.46983     -0.07188  \n#> \n#> \n#> [[10]]\n#> \n#> Call:\n#> lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#> \n#> Coefficients:\n#> (Intercept)          age          bmi  diastolicBP  \n#>    181.2751       0.0698      -0.5425       0.1208\n\n\nStep 3\nUnderstanding the pooled results\nWe will show the result of entire pool later. First we want to show the pooled results for the age variable only an an example.\n\nShow the coderequire(dplyr)\nres10 <- summary(fit4) %>% as_tibble %>% print(n=40)\n#> # A tibble: 40 × 6\n#>    term         estimate std.error statistic   p.value  nobs\n#>    <chr>           <dbl>     <dbl>     <dbl>     <dbl> <int>\n#>  1 (Intercept) 211.         53.6     3.93    0.000561     30\n#>  2 age          -0.191       0.452  -0.422   0.676        30\n#>  3 bmi          -0.521       0.943  -0.553   0.585        30\n#>  4 diastolicBP  -0.0950      0.563  -0.169   0.867        30\n#>  5 (Intercept) 186.         47.9     3.88    0.000644     30\n#>  6 age           0.0637      0.459   0.139   0.891        30\n#>  7 bmi          -0.492       0.939  -0.524   0.605        30\n#>  8 diastolicBP   0.0420      0.533   0.0787  0.938        30\n#>  9 (Intercept) 188.         48.9     3.85    0.000694     30\n#> 10 age          -0.0792      0.470  -0.169   0.867        30\n#> 11 bmi          -0.523       0.926  -0.565   0.577        30\n#> 12 diastolicBP   0.145       0.547   0.265   0.793        30\n#> 13 (Intercept) 210.         38.6     5.44    0.0000105    30\n#> 14 age           0.110       0.372   0.295   0.771        30\n#> 15 bmi          -0.460       0.950  -0.485   0.632        30\n#> 16 diastolicBP  -0.380       0.534  -0.712   0.483        30\n#> 17 (Intercept) 168.         39.6     4.24    0.000253     30\n#> 18 age           0.717       0.299   2.39    0.0241       30\n#> 19 bmi          -0.761       0.898  -0.848   0.404        30\n#> 20 diastolicBP  -0.109       0.500  -0.218   0.829        30\n#> 21 (Intercept) 187.         57.1     3.28    0.00294      30\n#> 22 age           0.173       0.437   0.395   0.696        30\n#> 23 bmi          -0.345       0.943  -0.366   0.717        30\n#> 24 diastolicBP  -0.148       0.569  -0.260   0.797        30\n#> 25 (Intercept) 203.         48.5     4.20    0.000278     30\n#> 26 age          -0.227       0.390  -0.583   0.565        30\n#> 27 bmi          -0.370       0.921  -0.402   0.691        30\n#> 28 diastolicBP  -0.0281      0.536  -0.0523  0.959        30\n#> 29 (Intercept) 214.         51.5     4.15    0.000313     30\n#> 30 age          -0.00349     0.450  -0.00776 0.994        30\n#> 31 bmi          -0.518       0.927  -0.559   0.581        30\n#> 32 diastolicBP  -0.310       0.525  -0.590   0.560        30\n#> 33 (Intercept) 206.         47.8     4.30    0.000213     30\n#> 34 age          -0.162       0.392  -0.414   0.682        30\n#> 35 bmi          -0.470       0.921  -0.510   0.614        30\n#> 36 diastolicBP  -0.0719      0.523  -0.137   0.892        30\n#> 37 (Intercept) 181.         44.4     4.08    0.000379     30\n#> 38 age           0.0698      0.353   0.198   0.845        30\n#> 39 bmi          -0.543       0.970  -0.559   0.581        30\n#> 40 diastolicBP   0.121       0.558   0.216   0.830        30\nm10 <- res10[res10$term == \"age\",]\nm10\n\n\n\n  \n\n\n\nLet us describe the components of a pool for the age variable only:\n\nShow the codem.number <- 10\n# estimate = pooled estimate \n# = sum of (m “beta-hat” estimates) / m (mean of m estimated statistics)\nestimate <- mean(m10$estimate)\nestimate\n#> [1] 0.04699243\n# ubar = sum of (m variance[beta] estimates) / m \n# = within-imputation variance (mean of estimated variances)\nubar.var <- mean(m10$std.error^2)\nubar.var\n#> [1] 0.1686837\n# b =  variance of (m “beta-hat” estimates) \n# = between-imputation variance \n# (degree to which estimated statistic / \n# “beta-hat” varies across m imputed datasets). \n# This b is not available for single imputation when m = 1.\nb.var <- var(m10$estimate)\nb.var\n#> [1] 0.07372796\n# t = ubar + b + b/m = total variance according to Rubin’s rules \n# (within-imputation & between imputation variation)\nt.var <- ubar.var + b.var + b.var/m.number\nt.var\n#> [1] 0.2497845\n# riv = relative increase in variance\nriv = (b.var + b.var/m.number)/ubar.var\nriv\n#> [1] 0.4807859\n# lambda = proportion of variance to due nonresponse\nlambda = (b.var + b.var/m.number)/t.var\nlambda\n#> [1] 0.3246829\n# df (approximate for large sample without correction)\ndf.large.sample <- (m.number - 1)/lambda^2\ndf.large.sample\n#> [1] 85.37359\n# df (hypothetical complete data)\ndfcom <- m10$nobs[1] - 4 # n = 30, # parameters = 4\ndfcom\n#> [1] 26\n# df (Barnard-Rubin correction)\ndf.obs <- (dfcom + 1)/(dfcom + 3) * dfcom * (1 - lambda)\ndf.c <- df.large.sample * df.obs/(df.large.sample + df.obs)\ndf.c\n#> [1] 13.72019\n# fmi = fraction of missing information per parameter\nfmi = (riv + 2/(df.large.sample +3)) / (1 + riv)\nfmi # based on large sample approximation\n#> [1] 0.3399662\nfmi = (riv + 2/(df.c +3)) / (1 + riv)\nfmi # Barnard-Rubin correction\n#> [1] 0.4054616\n\n\nPooled estimate\nCompare above results with the pooled table from mice below. Note that df is based on Barnard-Rubin correction and fmi value is calculated based on that corrected df.\n\nShow the code# Step 3 pool the analysis results\nest1 <- mice::pool(fit4)\n## pool all estimated together using Rubin's rule \nest1\n\n\nClass: mipo    m = 10 (transposed version to accommodate space)\n\n\nTerm\n(Intercept)\nage\nbmi\ndiastolicBP\n\n\n\nm\n10\n10\n10\n10\n\n\nEstimate\n195.40679314\n0.04699243\n-0.50032666\n-0.08347279\n\n\n\\(\\bar{u}\\)\n2313.6362339\n0.1686837\n0.8722547\n0.2909291\n\n\nb\n237.04075365\n0.07372796\n0.01274940\n0.02857675\n\n\nt\n2574.3810629\n0.2497845\n0.8862790\n0.3223635\n\n\ndf_com\n26\n26\n26\n26\n\n\ndf\n21.22870\n13.72019\n23.80807\n21.35356\n\n\nRIV\n0.11269915\n0.48078595\n0.01607826\n0.10804843\n\n\n\\(\\lambda\\)\n0.10128447\n0.32468295\n0.01582384\n0.09751237\n\n\nFMI\n0.17547051\n0.40546159\n0.08924771\n0.17162783\n\n\n\nHere:\n\ndfcom = df for complete data\ndf = df with Barnard-Rubin correction\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSpecial case: Variable selection\nVariable selection in analyzing missing data\n\nRef: (Van Buuren 2018), Section 5.4\n\nThe common workflow for analyzing missing data are (as mentioned above):\n\nImputing the data \\(m\\) times\nAnalyzing the \\(m\\) dataset\nPool all analysis together\n\nWe could apply variable selection in step 2, especially when we have no idea what is the best model to analyze the data. Howevere, it may become challenging when we pull all data together. With different dataset, the final model may or may not be the same.\nWe present the three method of variable selection on each imputed dataset presented by Buuren:\n\nMajority: perform the model selection separately with m dataset and choose the variables that appears at least m/2 times\nStack: combine m datasets into a single dataset, and perform variable selection on this dataset\nWald (Rubin’s rule): model selection was performed at model fitting step and combine the estimates using Rubin’s rules. This is considered as gold standard.\n\nMajority using NHANES17s\n\nShow the codedata <- NHANES17s\nimp <- mice(data, seed = 504, m = 100, print = FALSE)\n## Multiple imputation with 100 imputations, resulting in 100 imputed datasets\nscope0 <- list(upper = ~ age + bmi + cholesterol, lower = ~1)\nexpr <- expression(f1 <- lm(diastolicBP ~ age),\n                   f2 <- step(f1, scope = scope0, trace = FALSE))\nfit5 <- with(imp, expr)\n\n## apply stepwise on each of the imputed dataset separately\nformulas <- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms <- lapply(formulas, terms)\nvotes <- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#> votes\n#>         age         bmi cholesterol \n#>           6          12           1\n\n\n\nShow the code## Set up the stepwise variable selection, from null model to full model\nscope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\n## Set up the stepwise variable selection, from important only model to full model\nexpr <- expression(f1 <- lm(diastolicBP ~ age),\n                   f2 <- step(f1, scope = scope, trace = FALSE))\nfit5 <- with(imp, expr)\n## apply stepwise on each of the imputed dataset separately\nformulas <- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms <- lapply(formulas, terms)\nvotes <- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#> votes\n#>         age         bmi cholesterol \n#>         100          11           1\n\n\nStack using NHANES17s\n\nShow the codeStack.data <- mice::complete(imp, action=\"long\")\nhead(Stack.data)\n\n\n\n  \n\n\nShow the codetail(Stack.data)\n\n\n\n  \n\n\nShow the codefitx <- lm(diastolicBP ~ age + bmi + cholesterol, data = Stack.data)\nfity <- step(fitx, scope = scope0, trace = FALSE)\nrequire(Publish)\n#> Loading required package: Publish\n#> Loading required package: prodlim\npublish(fity)\n#>     Variable Units Coefficient         CI.95 p-value \n#>  (Intercept)             63.70 [62.12;65.28] < 1e-04 \n#>          bmi              0.14   [0.08;0.20] < 1e-04\n\n\nWald using NHANES17s\n\nShow the code# m = 100\nfit7 <- with(data=imp, expr=lm(diastolicBP ~ 1))\nnames(fit7)\n#> [1] \"call\"     \"call1\"    \"nmis\"     \"analyses\"\nfit7$analyses[[1]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ 1)\n#> \n#> Coefficients:\n#> (Intercept)  \n#>       68.47\nfit7$analyses[[100]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ 1)\n#> \n#> Coefficients:\n#> (Intercept)  \n#>       65.47\nfit8 <- with(data=imp, expr=lm(diastolicBP ~ bmi))\nfit8$analyses[[45]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ bmi)\n#> \n#> Coefficients:\n#> (Intercept)          bmi  \n#>    63.93092      0.09209\nfit8$analyses[[99]]\n#> \n#> Call:\n#> lm(formula = diastolicBP ~ bmi)\n#> \n#> Coefficients:\n#> (Intercept)          bmi  \n#>    68.34740      0.01797\n# The D1-statistics is the multivariate Wald test.\nstat <- D1(fit8, fit7)\n## use Wald test to see if we should add bmi into the model\nstat\n#>    test statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.1215668   1 22.70437    28 0.7305539 0.5550013\n# which indicates that adding bmi into our model might not be useful\n\n\n\nShow the codefit9 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi))\nstat <- D1(fit9, fit8)\n## use Wald test to see if we should add age into the model\nstat\n#>    test   statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.006608523   1 22.46746    27 0.9359289 0.4545242\n# which indicates that adding age into our model might not be useful\n\n\n\nShow the codefit10 <- with(data=imp, expr=lm(diastolicBP ~ age + bmi + cholesterol))\nstat <- D1(fit10, fit9)\n## use Wald test to see if we should add cholesterol into the model\nstat\n#>    test    statistic df1      df2 dfcom   p.value       riv\n#>  1 ~~ 2 0.0003547819   1 22.14158    26 0.9851409 0.3615345\n# which indicates that adding cholesterol into our model might not be useful\n\n\nTry method=\"likelihood\" as well.\n\nShow the codestat <- pool.compare(fit10, fit7, method = \"likelihood\", data=imp)\n## test to see if we should add all 3 variables into the model\nstat$pvalue\n#> [1] 0.9432629\n# which indicates that adding none of the variables into our model might be useful\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nAssess the impact of missing values in model fitting\nWhen working with datasets, missing values are a common challenge. These gaps in data can introduce biases and uncertainties, especially when we try to fit models to the data. To address this, researchers often use imputation methods to fill in the missing values based on the observed information. However, imputation itself can introduce uncertainties. Therefore, it’s essential to assess the impact of these missing values on model fitting. Buuren, as referenced in (Van Buuren 2018) Section 5.4.3, provides methods to do this. Out of the four methods presented by Buuren, the following two are the most commonly used:\n\nMultiple imputation with more number of imputations (i.e., 200). Perform variable selection on each imputed dataset. The differences are attributed to the missing values\nBootstrapping the data from a single imputed dataset and do variable selection for each bootstrapping sample. We could evaluate sampling variation using this method\n\nBootstrap using NHANES17s\n\nShow the codeimpx <- mice(NHANES17s, seed = 504, m=1, print=FALSE)\ncompletedata <- mice::complete(impx)\n  \nset.seed(504) \nvotes <-c()\nformula0 <- as.formula(\"diastolicBP ~ age + bmi + cholesterol\")\nscope <- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\nfor (i in 1:200){\n     ind <- sample(1:nrow(completedata),replace = TRUE)\n     newdata <- completedata[ind,]\n     full.model <- glm(formula0, data = newdata)\n     f2 <- MASS::stepAIC(full.model, \n                   scope = scope, trace = FALSE)\n     formulas <- as.formula(f2)\n     temp <- unlist(labels(terms(formulas)))\n     votes <- c(votes,temp)\n }\n table(votes)\n#> votes\n#>         age         bmi cholesterol \n#>         200          59          17\n ## among 200 bootstrap samples how many times that each \n ## variable appears in the final model. Models have different\n ## variables are attributed to sampling variation\n\n\nConvergence and diagnostics\nConvergence\n\nRef: (Van Buuren 2018), Section 6.5.2\nMCMC Algorithm in MICE: The MICE package implements a MCMC algorithm for imputation. The coefficients should be converged and irrelevant to the order which variable is imputed first.\nUnderstanding pattern: For convergence to be achieved, these chains should mix well with each other, meaning their paths should overlap and crisscross freely. If they show distinct, separate trends or paths, it indicates a lack of convergence, suggesting that the imputation may not be reliable.\nVisualizing Convergence: We could plot the imputation object to see the streams.\n\n\nShow the code## Recall the imputation we have done before\nimputation5 <- mice(NHANES17s, seed = 504, \n                   m=10,\n                   maxit = 5,\n                   print=FALSE) \nplot(imputation5)\n\n\n\n\n\n\nShow the code## We hope to see no pattern in the trace lines\n## Sometimes to comfirm this we may want to run with more iterations\nimputation5_2 <- mice(NHANES17s, seed = 504, \n                    m=10,\n                    maxit = 50,\n                    print=FALSE) \nplot(imputation5_2)\n\n\n\n\n\n\n\nDiagnostics\n\nRef: (Van Buuren 2018), Section 6.6\n\nModel diagnostics plays a pivotal role in ensuring the robustness and accuracy of model fitting. Particularly in the realm of missing value imputations, where observed data serves as the foundation for estimating absent values, it becomes imperative to rigorously assess the imputation process. A straightforward diagnostic technique involves comparing the distributions of the observed data with the imputed values, especially when segmented or conditioned based on the variables that originally had missing entries. This comparison helps in discerning any discrepancies or biases introduced during the imputation, ensuring that the filled values align well with the inherent patterns of the observed data.\n\nShow the code## We could compare the imputed and observed data using Density plots\ndensityplot(imputation5, layout = c(2, 2))\n\n\n\nShow the codeimputation5_3 <- mice(NHANES17s, seed = 504, \n                    m=50,\n                    maxit = 50,\n                    print=FALSE)\ndensityplot(imputation5_3)\n\n\n\nShow the code## a subjective judgment on whether you think if there is significant discrepancy\nbwplot(imputation5, age + bmi + cholesterol +diastolicBP ~ .imp, layout = c(2, 2))\n\n\n\nShow the codebwplot(imputation5_3)\n\n\n\nShow the code## Plot a box plot to compare the imputed and observed values\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45: 1–67."
  },
  {
    "objectID": "missingdata2.html",
    "href": "missingdata2.html",
    "title": "Imputation in NHANES",
    "section": "",
    "text": "This tutorial provides a comprehensive guide on handling and analyzing complex survey data with missing values. In analyzing complex survey data, a distinct approach is required compared to handling regular datasets. Specifically, the intricacies of survey design necessitate the consideration of primary sampling units/cluster, sampling weights, and stratification factors. These elements ensure that the analysis accurately reflects the survey’s design and the underlying population structure. Recognizing and incorporating these factors is crucial for obtaining valid and representative insights from the data. As we delve into this tutorial, we’ll explore how to effectively integrate these components into our missing data analysis process.\nComplex Survey data\nIn the initial chunk, we load all the necessary libraries that will be used throughout the tutorial. These libraries provide functions and tools for data exploration, visualization, imputation, and analysis.\n\nShow the code# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\n\nNext, we load a dataset that contains survey data with some missing values. We then select specific columns or variables from this dataset that we are interested in analyzing. To understand the extent and pattern of missingness in our data, we visualize it and display the missing data pattern.\n\nShow the codeload(\"Data/missingdata/NHANES17.RData\")\n\nVars <- c(\"ID\", \"weight\", \"psu\", \"strata\", \n          \"gender\", \"born\", \"race\", \n          \"bmi\", \"cholesterol\", \"diabetes\")\nanalyticx <- analytic.with.miss[,Vars]\nplot_missing(analyticx)\n\n\n\nShow the codemd.pattern(analyticx)\n\n\n\n#>      ID weight psu strata gender race born diabetes  bmi cholesterol     \n#> 6636  1      1   1      1      1    1    1        1    1           1    0\n#> 1364  1      1   1      1      1    1    1        1    1           0    1\n#> 97    1      1   1      1      1    1    1        1    0           1    1\n#> 795   1      1   1      1      1    1    1        1    0           0    2\n#> 4     1      1   1      1      1    1    1        0    1           1    1\n#> 357   1      1   1      1      1    1    1        0    0           0    3\n#> 1     1      1   1      1      1    1    0        1    1           1    1\n#>       0      0   0      0      0    0    1      361 1249        2516 4127\n\nImputing\nIn the following chunk, we address the missing data by performing multiple imputations. This means that instead of filling in each missing value with a single estimate, we create multiple versions (datasets) where each missing value is filled in differently based on a specified algorithm. This helps in capturing the uncertainty around the missing values. The chunk sets up the parameters for multiple imputations, ensuring reproducibility and efficiency, and then performs the imputations on the dataset with missing values.\n\nShow the code# imputation <- mice(analyticx, m=5, maxit=5, seed = 504007)\nset.seed(504)\nimputation <- parlmice(analyticx, m=5, maxit=5, cluster.seed=504007)\n\n\n\n\nData Input: The primary input for the imputation function is the dataset with missing values. This dataset is what we aim to impute.\n\nNumber of Imputations: The option m=5 indicates that we want to create 5 different imputed datasets. Each of these datasets will have the missing values filled in slightly differently, based on the underlying imputation algorithm and the randomness introduced.\n\nMaximum Iterations: The imputation process is iterative, meaning it refines its estimates over several rounds. The option maxit=5 specifies that the algorithm should run for a maximum of 5 iterations. This helps in achieving more accurate imputations, especially when the missing data mechanism is complex.\n\nSetting Seed: In computational processes that involve randomness, it’s often useful to set a “seed” value. This ensures that the random processes are reproducible. If you run the imputation multiple times with the same seed, you’ll get the same results each time. Two seed values are set in the chunk: one using the general set.seed() function and another specifically for the imputation function as cluster.seed.\n\nParallel Processing: The function parlmice used for imputation indicates that the imputations are done in parallel. This means that instead of imputing one dataset after the other, the function tries to impute multiple datasets simultaneously (if the computational resources allow). This can speed up the process, especially when dealing with large datasets or when creating many imputed datasets.\nCreate new variable\nAfter imputation, we might want to create new variables or modify existing ones to better suit our analysis. Here, we transform one of the variables into a binary category based on a threshold. This can help in simplifying the analysis or making the results more interpretable.\n\nShow the codeimpdata <- complete(imputation, action=\"long\") #stacked data\nimpdata$cholesterol.bin <- ifelse(impdata$cholesterol < 200, \"healthy\", \"unhealthy\")\nimpdata$cholesterol.bin <- as.factor(impdata$cholesterol.bin)\ndim(impdata)\n#> [1] 46270    13\nhead(impdata)\n\n\n\n  \n\n\n\nChecking the data\nAfter imputation, it’s crucial to ensure that the imputed data maintains the integrity and structure of the original dataset. The following chunks are designed to help you visually and programmatically inspect the imputed data.\nVisual Inspection of Missing Data:\nIn this chunk, we visually inspect the imputed datasets to see if there are any remaining missing values. We specifically look at the first two imputed datasets. Visualization tools like these can quickly show if the imputation process was successful in filling all missing values.\n\nShow the codeplot_missing(subset(impdata, subset=.imp==1))\n\n\n\nShow the codeplot_missing(subset(impdata, subset=.imp==2))\n\n\n\n\nComparing Original and Imputed Data (First Imputed Dataset):\n\nIn this chunk, we focus on the first imputed dataset. We extract this dataset and display the initial entries to get a sense of the data.\nWe then remove any remaining missing values (if any) and display the initial entries of this complete dataset.\nNext, we compare the IDs (or unique identifiers) of the entries in the complete dataset with the original dataset to see which entries had missing values.\nWe then create a new variable that indicates whether an entry had missing values or not and tabulate this information.\n\n\nShow the codeanalytic.miss1 <- subset(impdata, subset=.imp==1)\nhead(analytic.miss1$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic1 <- as.data.frame(na.omit(analytic.miss1))\nhead(analytic1$ID) # complete case\n#> [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss1$ID[analytic.miss1$ID %in% analytic1$ID])\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss1$miss <- 1\nanalytic.miss1$miss[analytic.miss1$ID %in% analytic1$ID] <- 0\ntable(analytic.miss1$miss)\n#> \n#>    0    1 \n#> 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 102840 102862 102919 102927 102928 102942\n\n\nComparing Original and Imputed Data (Second Imputed Dataset):\nThe this chunk is similar to the above but focuses on the second imputed dataset. We perform the same steps: extracting the dataset, removing missing values, comparing IDs, and creating a variable to indicate missingness.\n\nShow the codeanalytic.miss2 <- subset(impdata, subset=.imp==2)\nhead(analytic.miss2$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic2 <- as.data.frame(na.omit(analytic.miss2))\nhead(analytic2$ID) # complete case\n#> [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss2$ID[analytic.miss2$ID %in% analytic2$ID])\n#> [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss2$miss <- 1\nanalytic.miss2$miss[analytic.miss2$ID %in% analytic2$ID] <- 0\ntable(analytic.miss2$miss)\n#> \n#>    0    1 \n#> 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#> [1] 102840 102862 102919 102927 102928 102942\n\n\nAggregating Missingness Information Across All Imputed Datasets:\n\nIn the fourth chunk, we aim to consolidate the missingness information across all imputed datasets. We initialize a variable in the main dataset to indicate missingness.\nWe then loop through each of the imputed datasets and update the main dataset’s missingness variable based on the missingness information from each imputed dataset. This gives us a consolidated view of which entries had missing values across all imputed datasets.\n\n\nShow the codeimpdata$miss<-1\nm <- 5\nfor (i in 1:m){\n  impdata$miss[impdata$.imp == i] <- analytic.miss2$miss\n  print(table(impdata$miss[impdata$.imp == i]))\n}\n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362 \n#> \n#>    0    1 \n#> 8892  362\n\n\nCombining data\nSince we have multiple versions of the imputed dataset, we need a way to combine them for analysis. In the next chunks, we use a method to merge these datasets into a single list, making it easier to apply subsequent analyses on all datasets simultaneously.\n\nShow the codelibrary(mitools) \nallImputations <- imputationList(list(\n  subset(impdata, subset=.imp==1),\n  subset(impdata, subset=.imp==2),\n  subset(impdata, subset=.imp==3),\n  subset(impdata, subset=.imp==4), \n  subset(impdata, subset=.imp==5)))\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\n\nCombining data efficiently\n\nShow the codem <- 5\nset.seed(123)\nallImputations <-  imputationList(lapply(1:m, \n                                         function(n)  \n                                           subset(impdata, subset=.imp==n)))\n                                           #mice::complete(imputation, action = n)))\nsummary(allImputations)\n#>             Length Class  Mode\n#> imputations 5      -none- list\n#> call        2      -none- call\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#>   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#>   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#>   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#>   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#>   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#>  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\n\nLogistic regression\nWith our imputed datasets ready, we proceed to fit a statistical model. Here, we use logistic regression as an example. We fit the model to each imputed dataset separately and then extract relevant statistics like odds ratios and confidence intervals.\n\nShow the coderequire(jtools)\nrequire(survey)\ndata.list <- vector(\"list\", m)\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~diabetes+gender+born+race+bmi\")\n\n\n\nShow the codesummary(allImputations$imputations[[1]]$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12347   21060   34671   37562  419763\nsum(allImputations$imputations[[1]]$weight==0)\n#> [1] 550\nw.design0 <- svydesign(ids=~psu, weights=~weight, strata=~strata,\n                           data = allImputations, nest = TRUE)\nw.design <- subset(w.design0, miss == 0)\nfits <- with(w.design, svyglm(model.formula, family=quasibinomial))\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n# Estimate from first data\nexp(coef(fits[[1]]))[2]\n#> diabetesYes \n#>    1.409246\nexp(confint(fits[[1]]))[2,]\n#>    2.5 %   97.5 % \n#> 1.129997 1.757503\n# Estimate from second data\nexp(coef(fits[[2]]))[2]\n#> diabetesYes \n#>    1.437951\nexp(confint(fits[[2]]))[2,]\n#>    2.5 %   97.5 % \n#> 1.171160 1.765518\n\n\nPooled / averaged estimates\nAfter analyzing each imputed dataset separately, we need to combine the results to get a single set of estimates. This is done using a method that pools the results, taking into account the variability between the different imputed datasets.\n\nShow the codepooled.estimates <- MIcombine(fits)\nsum.pooled <- summary(pooled.estimates)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(fits)\n#>                   results          se        (lower       upper) missInfo\n#> (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#> diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#> genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#> bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#> bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#> raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#> raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#> raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#> bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nexp(sum.pooled[,1])\n#> [1] 8.458294e+00 1.422037e+00 1.193362e+00 5.507777e-01 4.405626e-06\n#> [6] 1.149279e+00 8.407373e-01 6.989885e-01 9.606814e-01\nOR <- round(exp(pooled.estimates$coefficients),2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)),2)\nsig <- (CI[,1] < 1 & CI[,2] > 1)\nsig <- ifelse(sig==FALSE, \"*\", \"\")\nOR <- cbind(OR,CI,sig)\nOR\n\n\n\n  \n\n\n\nStep-by-step example\nThis segment offers a hands-on approach to understanding the imputation process. Here’s a breakdown:\n\n\nFitting Models to Individual Imputed Datasets:\n\nA list is initialized to store the results of models fitted to each imputed dataset.\nFor every dataset, the specific imputed data is extracted.\nA survey design is established, considering factors like primary sampling units, stratification, and weights. This ensures the analysis aligns with the survey’s design.\nThis design is then refined to only consider complete data entries.\nA logistic regression model is then applied to this refined data.\nThe results of this modeling are stored and displayed for review.\n\n\n\nPooling Results from All Models:\n\nAfter individual analysis, the next step is to combine or ‘pool’ these results.\nA special function is used to merge the results from all the models. This function accounts for variations between datasets and offers a combined estimate.\nA summary of this combined data is then displayed, offering insights like coefficients, standard errors, and more. Another version of this summary, focusing on log-effects, is also presented for deeper insights.\n\n\n\n\nShow the codefits2 <- vector(\"list\", m)\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design0.i <- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i <- subset(w.design0.i, miss == 0)\n  fit <- svyglm(model.formula, design=w.design.i, \n                family = quasibinomial(\"logit\"))\n  print(summ(fit))\n  fits2[[i]] <- fit\n}\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.26   0.20    11.32   0.00\n#> diabetesYes            0.34   0.09     3.67   0.01\n#> genderMale             0.17   0.09     1.88   0.10\n#> bornOthers            -0.62   0.10    -6.36   0.00\n#> bornRefused          -12.35   0.70   -17.63   0.00\n#> raceHispanic           0.18   0.14     1.27   0.25\n#> raceOther             -0.20   0.11    -1.77   0.12\n#> raceWhite             -0.38   0.13    -2.83   0.03\n#> bmi                   -0.04   0.01    -7.92   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.08   0.23     9.17   0.00\n#> diabetesYes            0.36   0.09     4.19   0.00\n#> genderMale             0.20   0.08     2.49   0.04\n#> bornOthers            -0.63   0.08    -7.41   0.00\n#> bornRefused          -12.33   0.71   -17.25   0.00\n#> raceHispanic           0.13   0.14     0.95   0.37\n#> raceOther             -0.16   0.10    -1.57   0.16\n#> raceWhite             -0.37   0.14    -2.71   0.03\n#> bmi                   -0.04   0.01    -6.05   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 1\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.04\n#> Pseudo-R² (McFadden) = 0.02\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.03   0.22     9.22   0.00\n#> diabetesYes            0.35   0.08     4.30   0.00\n#> genderMale             0.17   0.09     1.88   0.10\n#> bornOthers            -0.56   0.08    -7.21   0.00\n#> bornRefused          -12.30   0.70   -17.49   0.00\n#> raceHispanic           0.10   0.13     0.76   0.47\n#> raceOther             -0.19   0.10    -1.81   0.11\n#> raceWhite             -0.34   0.13    -2.59   0.04\n#> bmi                   -0.04   0.01    -6.58   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.18   0.21    10.57   0.00\n#> diabetesYes            0.36   0.09     3.75   0.01\n#> genderMale             0.16   0.09     1.78   0.12\n#> bornOthers            -0.57   0.10    -5.98   0.00\n#> bornRefused          -12.35   0.71   -17.32   0.00\n#> raceHispanic           0.12   0.14     0.83   0.43\n#> raceOther             -0.16   0.10    -1.61   0.15\n#> raceWhite             -0.34   0.13    -2.55   0.04\n#> bmi                   -0.04   0.01    -7.11   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> MODEL INFO:\n#> Observations: 8892\n#> Dependent Variable: I(cholesterol.bin == \"healthy\")\n#> Type: Analysis of complex survey design \n#>  Family: quasibinomial \n#>  Link function: logit \n#> \n#> MODEL FIT:\n#> Pseudo-R² (Cragg-Uhler) = 0.05\n#> Pseudo-R² (McFadden) = 0.03\n#> AIC =  NA \n#> \n#> --------------------------------------------------\n#>                        Est.   S.E.   t val.      p\n#> ------------------ -------- ------ -------- ------\n#> (Intercept)            2.12   0.22     9.67   0.00\n#> diabetesYes            0.35   0.09     4.02   0.01\n#> genderMale             0.19   0.08     2.30   0.06\n#> bornOthers            -0.60   0.10    -6.06   0.00\n#> bornRefused          -12.33   0.71   -17.40   0.00\n#> raceHispanic           0.17   0.14     1.20   0.27\n#> raceOther             -0.17   0.10    -1.64   0.14\n#> raceWhite             -0.36   0.13    -2.78   0.03\n#> bmi                   -0.04   0.01    -6.65   0.00\n#> --------------------------------------------------\n#> \n#> Estimated dispersion parameter = 0.99\n\n\n\nShow the codepooled.estimates <- MIcombine(fits2)\nsummary(pooled.estimates)\n#> Multiple imputation results:\n#>       MIcombine.default(fits2)\n#>                   results          se        (lower       upper) missInfo\n#> (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#> diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#> genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#> bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#> bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#> raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#> raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#> raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#> bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nsummary(pooled.estimates,logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fits2)\n#>              results      se  (lower  upper) missInfo\n#> (Intercept)  8.5e+00 2.0e+00 5.3e+00 1.3e+01     19 %\n#> diabetesYes  1.4e+00 1.3e-01 1.2e+00 1.7e+00      1 %\n#> genderMale   1.2e+00 1.1e-01 1.0e+00 1.4e+00      5 %\n#> bornOthers   5.5e-01 5.3e-02 4.6e-01 6.7e-01     11 %\n#> bornRefused  4.4e-06 3.1e-06 1.1e-06 1.8e-05      0 %\n#> raceHispanic 1.1e+00 1.6e-01 8.7e-01 1.5e+00      6 %\n#> raceOther    8.4e-01 8.9e-02 6.8e-01 1.0e+00      5 %\n#> raceWhite    7.0e-01 9.4e-02 5.4e-01 9.1e-01      2 %\n#> bmi          9.6e-01 6.3e-03 9.5e-01 9.7e-01     20 %\n\n\nVariable selection\nSometimes, not all variables in the dataset are relevant for our analysis. In the final chunks, we apply a method to select the most relevant variables for our model. This can help in simplifying the model and improving its interpretability.\n\nShow the coderequire(jtools)\nrequire(survey)\ndata.list <- vector(\"list\", m)\nmodel.formula <- as.formula(\"cholesterol~diabetes+gender+born+race+bmi\")\nscope <- list(upper = ~ diabetes+gender+born+race+bmi,\n              lower = ~ diabetes)\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design0.i <- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i <- subset(w.design0.i, miss == 0)\n  fit <- svyglm(model.formula, design=w.design.i)\n  fitstep <- step(fit, scope = scope, trace = FALSE,\n                              direction = \"backward\")\n  data.list[[i]] <- fitstep\n}\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(g): observations with zero weight not used for\n#> calculating dispersion\n#> Warning in summary.glm(glm.object): observations with zero weight not used for\n#> calculating dispersion\n\n\nCheck out the variables selected\n\nShow the codex <- all.vars(formula(fit))\nfor (i in 1:m) x <- c(x, all.vars(formula(data.list[[i]])))\ntable(x)-1\n#> x\n#>         bmi        born cholesterol    diabetes      gender        race \n#>           5           5           5           5           5           5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "missingdata3.html",
    "href": "missingdata3.html",
    "title": "Missing in outcome",
    "section": "",
    "text": "This section provides a theoretical background on the concept of Multiple Imputation and then Deletion (MID). It highlights the challenges of imputing dependent and exposure variables and introduces the idea of using auxiliary variables to aid imputation. The section also contrasts the results of traditional MI with MID, especially when the number of imputed datasets is high.\n\nOften researchers are reluctant to impute values in the dependent variable (and exposure variable). Particularly, for dependent variable, imputation might not help too much.\nHowever, if you have a good auxiliary variable (e.g., strongly correlated predictor, that are not used in the main analysis), often multiple imputation method can help. Use of auxiliary variables is one of the greatest strengths of MI methods.\nMI algorithm generally do not have any special treatment for dependent variable in its original form, and hence ignoring dependent variable completely may not be a good idea in many scenarios.\nMultiple imputation followed by deletion of imputed outcomes is known as MID. This is very popular, especially when you have high percentage missing values in the outcome variable (e.g., 20%-50%). For low missing % in outcome, the advantage can be minimal.\nWe are extending this idea to deletion of imputed exposures as well (researchers are often reluctant to impute primary exposure of interest).\nOriginal MI and MID may result in similar results when m (number of imputed datasets) is higher.\n\nData\nIn the initial chunk, we load several packages that provide functions and tools necessary for the subsequent analysis. These packages facilitate multiple imputation, data visualization, and statistical modeling among other tasks.\n\nShow the code# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\n\nWe load the necessary data:\n\nShow the codeload(\"Data/missingdata/NHANES17.RData\")\n\n\nThe data is briefly inspected to understand its structure. An identifier column is added to uniquely identify each row or observation in the dataset.\n\nShow the coderequire(mice)\nnhanes2\n\n\n\n  \n\n\nShow the codenhanes2$id <- 1:nrow(nhanes2)\nnhanes2\n\n\n\n  \n\n\n\nOutcome and exposure has missing\nThis chunk focuses on identifying which rows have missing values in both the outcome and exposure variables. The outcome and exposure variables are crucial for the analysis, so understanding where they are missing is essential.\n\nShow the code# assume outcome = bmi and exposure = chl \nnhanes2.excludingYA <- subset(nhanes2, !is.na(bmi) & !is.na(chl) )\nnhanes2.excludingYA # data without missing A and Y\n\n\n\n  \n\n\nShow the code# identify ids of subjects with missing A & Y \nnhanes2.excludingYA$id\n#>  [1]  2  5  7  8  9 13 14 17 18 19 22 23 25\n\n\nImpute as usual\nUsing the entire dataset, missing values are imputed. This is done by first initializing an imputation model and then performing the imputation to create multiple datasets where missing values are filled in. The result is a list of datasets with imputed values. That means, we impute Y and A for now, as well as other covariates with missing values.\n\nShow the code# use full data to impute \nini <- mice(nhanes2, pri = FALSE)\nini$method\n#>      age      bmi      hyp      chl       id \n#>       \"\"    \"pmm\" \"logreg\"    \"pmm\"       \"\"\npred <- ini$predictorMatrix\npred\n#>     age bmi hyp chl id\n#> age   0   1   1   1  1\n#> bmi   1   0   1   1  1\n#> hyp   1   1   0   1  1\n#> chl   1   1   1   0  1\n#> id    1   1   1   1  0\npred[,\"id\"] <- 0 # as this is not a predictor\nm <- 5\nimp <- mice(data=nhanes2, m=m, maxit=3, seed=504007)\n#> \n#>  iter imp variable\n#>   1   1  bmi  hyp  chl\n#>   1   2  bmi  hyp  chl\n#>   1   3  bmi  hyp  chl\n#>   1   4  bmi  hyp  chl\n#>   1   5  bmi  hyp  chl\n#>   2   1  bmi  hyp  chl\n#>   2   2  bmi  hyp  chl\n#>   2   3  bmi  hyp  chl\n#>   2   4  bmi  hyp  chl\n#>   2   5  bmi  hyp  chl\n#>   3   1  bmi  hyp  chl\n#>   3   2  bmi  hyp  chl\n#>   3   3  bmi  hyp  chl\n#>   3   4  bmi  hyp  chl\n#>   3   5  bmi  hyp  chl\n# list format in m data\nimpdata <- mice::complete(imp, action = \"all\")\nimpdata # all IDs are present\n#> $`1`\n#>      age  bmi hyp chl id\n#> 1  20-39 20.4  no 187  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 27.4  no 187  3\n#> 4  60-99 25.5 yes 204  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.5 yes 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 20.4  no 199 10\n#> 11 20-39 27.5  no 199 11\n#> 12 40-59 26.3  no 284 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 187 15\n#> 16 20-39 35.3 yes 204 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 284 20\n#> 21 20-39 35.3 yes 184 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`2`\n#>      age  bmi hyp chl id\n#> 1  20-39 28.7 yes 238  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 30.1  no 187  3\n#> 4  60-99 22.5 yes 218  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 20.4 yes 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 22.5 yes 238 10\n#> 11 20-39 26.3 yes 118 11\n#> 12 40-59 20.4  no 199 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 187 15\n#> 16 20-39 24.9  no 238 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 218 20\n#> 21 20-39 27.5  no 187 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`3`\n#>      age  bmi hyp chl id\n#> 1  20-39 25.5  no 229  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 22.0  no 187  3\n#> 4  60-99 20.4  no 199  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.7  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 27.4  no 184 10\n#> 11 20-39 30.1  no 238 11\n#> 12 40-59 22.0  no 186 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 229 15\n#> 16 20-39 22.0  no 118 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 218 20\n#> 21 20-39 29.6  no 131 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 218 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`4`\n#>      age  bmi hyp chl id\n#> 1  20-39 24.9  no 187  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 27.4  no 187  3\n#> 4  60-99 24.9 yes 206  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 22.5  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 26.3 yes 187 10\n#> 11 20-39 29.6  no 229 11\n#> 12 40-59 27.4  no 204 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 186 15\n#> 16 20-39 35.3  no 184 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 199 20\n#> 21 20-39 27.5  no 187 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 284 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> $`5`\n#>      age  bmi hyp chl id\n#> 1  20-39 27.2  no 238  1\n#> 2  40-59 22.7  no 187  2\n#> 3  20-39 24.9  no 187  3\n#> 4  60-99 20.4  no 229  4\n#> 5  20-39 20.4  no 113  5\n#> 6  60-99 21.7  no 184  6\n#> 7  20-39 22.5  no 118  7\n#> 8  20-39 30.1  no 187  8\n#> 9  40-59 22.0  no 238  9\n#> 10 40-59 20.4 yes 187 10\n#> 11 20-39 25.5  no 118 11\n#> 12 40-59 21.7  no 187 12\n#> 13 60-99 21.7  no 206 13\n#> 14 40-59 28.7 yes 204 14\n#> 15 20-39 29.6  no 199 15\n#> 16 20-39 27.5  no 187 16\n#> 17 60-99 27.2 yes 284 17\n#> 18 40-59 26.3 yes 199 18\n#> 19 20-39 35.3  no 218 19\n#> 20 60-99 25.5 yes 206 20\n#> 21 20-39 33.2  no 206 21\n#> 22 20-39 33.2  no 229 22\n#> 23 20-39 27.5  no 131 23\n#> 24 60-99 24.9  no 204 24\n#> 25 40-59 27.4  no 186 25\n#> \n#> attr(,\"class\")\n#> [1] \"mild\" \"list\"\n\n\nInclude a missing indicator (Y & A)\nFor each imputed dataset, a new column is added to indicate whether the outcome and exposure variables were originally missing. This “missing indicator” column will be used later to subset the data.\n\nShow the code# Define formula (making binary Y)\nformula <- as.formula(\"I(bmi>25) ~ chl + hyp\")\ndata.list <- vector(\"list\", m)\n# subset the data without Y and A's that had missing values\n# and record those subset data\nfor (i in 1:m) {\n  analytic.i <- impdata[[i]]\n  analytic.i$miss <- 1\n  analytic.i$miss[analytic.i$id %in% nhanes2.excludingYA$id] <- 0\n  data.list[[i]] <- analytic.i\n}\ndata.list  # only relevant IDs are present\n#> [[1]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 20.4  no 187  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 27.4  no 187  3    1\n#> 4  60-99 25.5 yes 204  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.5 yes 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 20.4  no 199 10    1\n#> 11 20-39 27.5  no 199 11    1\n#> 12 40-59 26.3  no 284 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 187 15    1\n#> 16 20-39 35.3 yes 204 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 284 20    1\n#> 21 20-39 35.3 yes 184 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[2]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 28.7 yes 238  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 30.1  no 187  3    1\n#> 4  60-99 22.5 yes 218  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 20.4 yes 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 22.5 yes 238 10    1\n#> 11 20-39 26.3 yes 118 11    1\n#> 12 40-59 20.4  no 199 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 187 15    1\n#> 16 20-39 24.9  no 238 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 218 20    1\n#> 21 20-39 27.5  no 187 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[3]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 25.5  no 229  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 22.0  no 187  3    1\n#> 4  60-99 20.4  no 199  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.7  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 27.4  no 184 10    1\n#> 11 20-39 30.1  no 238 11    1\n#> 12 40-59 22.0  no 186 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 229 15    1\n#> 16 20-39 22.0  no 118 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 218 20    1\n#> 21 20-39 29.6  no 131 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 218 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[4]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 24.9  no 187  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 27.4  no 187  3    1\n#> 4  60-99 24.9 yes 206  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 22.5  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 26.3 yes 187 10    1\n#> 11 20-39 29.6  no 229 11    1\n#> 12 40-59 27.4  no 204 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 186 15    1\n#> 16 20-39 35.3  no 184 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 199 20    1\n#> 21 20-39 27.5  no 187 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 284 24    1\n#> 25 40-59 27.4  no 186 25    0\n#> \n#> [[5]]\n#>      age  bmi hyp chl id miss\n#> 1  20-39 27.2  no 238  1    1\n#> 2  40-59 22.7  no 187  2    0\n#> 3  20-39 24.9  no 187  3    1\n#> 4  60-99 20.4  no 229  4    1\n#> 5  20-39 20.4  no 113  5    0\n#> 6  60-99 21.7  no 184  6    1\n#> 7  20-39 22.5  no 118  7    0\n#> 8  20-39 30.1  no 187  8    0\n#> 9  40-59 22.0  no 238  9    0\n#> 10 40-59 20.4 yes 187 10    1\n#> 11 20-39 25.5  no 118 11    1\n#> 12 40-59 21.7  no 187 12    1\n#> 13 60-99 21.7  no 206 13    0\n#> 14 40-59 28.7 yes 204 14    0\n#> 15 20-39 29.6  no 199 15    1\n#> 16 20-39 27.5  no 187 16    1\n#> 17 60-99 27.2 yes 284 17    0\n#> 18 40-59 26.3 yes 199 18    0\n#> 19 20-39 35.3  no 218 19    0\n#> 20 60-99 25.5 yes 206 20    1\n#> 21 20-39 33.2  no 206 21    1\n#> 22 20-39 33.2  no 229 22    0\n#> 23 20-39 27.5  no 131 23    0\n#> 24 60-99 24.9  no 204 24    1\n#> 25 40-59 27.4  no 186 25    0\n# record the fits from each data\n\n\nDesign, subset and fit\nFor each imputed dataset, a statistical model is fitted. Before fitting, the data is structured to account for survey design features. Only rows without originally missing outcome and exposure values are used for model fitting. The results of the model fitting for each dataset are stored for later analysis.\n\nShow the coderequire(survey)\nfit.list <- vector(\"list\", 5)\nfor (i in 1:m) {\n  analytic.i <- data.list[[i]]\n  # assigning survey features = 1\n  w.design0 <- svydesign(id=~1, weights=~1,\n                        data=analytic.i)\n  w.design <- subset(w.design0, miss == 0)\n  fit <- svyglm(formula, design=w.design, family=binomial)\n  fit.list[[i]] <-  fit\n}\n\n\nPooled results\nAfter fitting models to each imputed dataset, the results are combined or “pooled”. This pooled result provides a more robust estimate by considering the variability across the imputed datasets.\n\nShow the coderequire(mitools)\npooled.estimates <- MIcombine(fit.list)\npooled.estimates\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>                  results         se\n#> (Intercept) -1.769918773 2.73723080\n#> chl          0.009747869 0.01499608\n#> hypyes      18.128613035 1.05775401\n\n# or you can do it this way\nbetas<-MIextract(fit.list,fun=coef)\nvars<-MIextract(fit.list, fun=vcov)\nsummary(MIcombine(betas,vars))\n#> Multiple imputation results:\n#>       MIcombine.default(betas, vars)\n#>                  results         se      (lower      upper) missInfo\n#> (Intercept) -1.769918773 2.73723080 -7.13479255  3.59495501      0 %\n#> chl          0.009747869 0.01499608 -0.01964391  0.03913964      0 %\n#> hypyes      18.128613035 1.05775401 16.05545326 20.20177281      0 %\n\n# report beta coef\nsum.pooled <- summary(pooled.estimates, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>             results    se (lower upper) missInfo\n#> (Intercept) -1.7699 2.737  -7.13  3.595      0 %\n#> chl          0.0097 0.015  -0.02  0.039      0 %\n#> hypyes      18.1286 1.058  16.06 20.202      0 %\nsum.pooled\n\n\n\n  \n\n\n\nReport OR\nThe pooled results are further processed to calculate and report odds ratios, which provide insights into the relationships between variables in the context of logistic regression.\n\nShow the codesum.pooled.OR <- summary(pooled.estimates, logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.list)\n#>             results      se  (lower  upper) missInfo\n#> (Intercept) 1.7e-01 4.7e-01 8.0e-04 3.6e+01      0 %\n#> chl         1.0e+00 1.5e-02 9.8e-01 1.0e+00      0 %\n#> hypyes      7.5e+07 7.9e+07 9.4e+06 5.9e+08      0 %\nsum.pooled.OR\n\n\n\n  \n\n\n\nUsing publish package may be possible, but requires complicated process. Look for publish.MIresult (but this can be complicated)."
  },
  {
    "objectID": "missingdata4.html",
    "href": "missingdata4.html",
    "title": "Performance with NA",
    "section": "",
    "text": "This is a tutorial of how to estimate model performance while analyzing survey data with missing values (for predictive goals).\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\n\nUseful functions\n\nShow the codeknitr::opts_chunk$set(echo = TRUE)\nsvyROCw3 <- function(fit=fit7,outcome=analytic2$CVD==\"event\", weight = NULL,plot=FALSE){\n  # ROC curve for\n  # Survey Data with Logistic Regression\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  if (plot == TRUE){\n    roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n    with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  }\n  } else { # library(WeightedROC)\n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  if (plot == TRUE){\n    with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n  }\n  return(auc)\n}\nAL.gof3 <- function(fit=fit7, data = analytic2, weight = \"weight\", psu = \"psu\", strata= \"strata\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  if (is.null(psu)){\n    newdesign <- svydesign(id=~1,\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  if (!is.null(psu)) {\n    newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(as.numeric(res$p)) \n}\n\n\nLoad imputed 5 sets of data\nSaved at the end of Lab 6 part 2.\n\nShow the code# Saved from last lab\nload(\"Data/missingdata/missOA123CVDnorth.RData\")\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 5\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 2 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 2 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 4 4 4 4 4 4 4 4 4 4 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 1 2 1 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>   ..$ :'data.frame': 135448 obs. of  19 variables:\n#>   .. ..$ .imp   : int [1:135448] 5 5 5 5 5 5 5 5 5 5 ...\n#>   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#>   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#>   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#>   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#>   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#>   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#>   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#>   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#>   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 2 2 2 ...\n#>   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#>   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ edu    : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#>   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#>   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#>  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\n\nEstimating treatment effect\nIndividual beta estimates\n\nShow the codelibrary(survey)\nw.design <- svydesign(ids=~1, weights=~weight,\n                           data = allImputations)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nestimates <- with(w.design, svyglm(model.formula, family=quasibinomial))\nestimates\n#> [[1]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6809                  1.1063                  0.7911  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6233                  2.0076                  0.6278  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4347                 -0.6031                 -0.6805  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2136                  0.8277                  1.0344  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9143                  0.8166                 -0.8183  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1404                 -0.7286                 -0.9360  \n#>        sexMale:copdYes  \n#>                 0.6167  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31100     AIC: NA\n#> \n#> [[2]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.4837                  0.8766                  0.7781  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6090                  1.9901                  0.6083  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4415                 -0.5969                 -0.6840  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2374                  0.5877                  1.0413  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.8837                  0.8054                 -0.5388  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1303                 -0.6944                 -0.8545  \n#>        sexMale:copdYes  \n#>                 0.5857  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31260     AIC: NA\n#> \n#> [[3]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6365                  1.0389                  0.7815  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6140                  1.9986                  0.6180  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4334                 -0.5945                 -0.6843  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2042                  0.7963                  1.0386  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9468                  0.8020                 -0.7330  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1185                 -0.7775                 -0.9414  \n#>        sexMale:copdYes  \n#>                 0.5548  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31130     AIC: NA\n#> \n#> [[4]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.6811                  1.2937                  0.7838  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6248                  2.0097                  0.6285  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4431                 -0.6012                 -0.6877  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2325                  0.8179                  1.0401  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9368                  0.8022                 -1.0624  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.0381                 -0.7407                 -0.9356  \n#>        sexMale:copdYes  \n#>                 0.5422  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31100     AIC: NA\n#> \n#> [[5]]\n#> Independent Sampling design (with replacement)\n#> svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#>     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#>     data = d, pps = pps, ...)\n#> \n#> Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#> \n#> Coefficients:\n#>            (Intercept)                    OAOA          age40-49 years  \n#>                -5.4487                  0.7569                  0.7752  \n#>         age50-59 years          age60-64 years                 sexMale  \n#>                 1.6021                  1.9847                  0.6107  \n#>  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#>                -0.4393                 -0.5977                 -0.6788  \n#>              raceWhite              painmedYes                   htYes  \n#>                 0.2351                  0.5457                  1.0421  \n#>                copdYes                 diabYes         OAOA:painmedYes  \n#>                 1.9235                  0.8057                 -0.3884  \n#> age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#>                -1.1275                 -0.7370                 -0.9152  \n#>        sexMale:copdYes  \n#>                 0.5871  \n#> \n#> Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#> Null Deviance:       36680 \n#> Residual Deviance: 31290     AIC: NA\n#> \n#> attr(,\"call\")\n#> with(w.design, svyglm(model.formula, family = quasibinomial))\n\n\nPooled / averaged estimates for beta and OR\n\nShow the codelibrary(\"mitools\")\npooled.estimates <- MIcombine(estimates)\npooled.estimates\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(estimates)\n#>                           results         se\n#> (Intercept)            -5.5861842 0.19239878\n#> OAOA                    1.0144758 0.27097789\n#> age40-49 years          0.7819429 0.10165218\n#> age50-59 years          1.6146502 0.09905167\n#> age60-64 years          1.9981358 0.10380099\n#> sexMale                 0.6186653 0.05484262\n#> income$30,000-$49,999  -0.4383959 0.06787453\n#> income$50,000-$79,999  -0.5987066 0.06593903\n#> income$80,000 or more  -0.6830429 0.06907307\n#> raceWhite               0.2245784 0.10567063\n#> painmedYes              0.7150386 0.16508180\n#> htYes                   1.0393147 0.05522682\n#> copdYes                 1.9210219 0.65240500\n#> diabYes                 0.8063590 0.08169615\n#> OAOA:painmedYes        -0.7081707 0.32575000\n#> age40-49 years:copdYes -1.1109564 0.69441469\n#> age50-59 years:copdYes -0.7356335 0.67352566\n#> age60-64 years:copdYes -0.9165474 0.65626790\n#> sexMale:copdYes         0.5772949 0.30888502\n\nsum.pooled <- summary(pooled.estimates,logeffect=TRUE, digits = 2)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = quasibinomial))\n#>       MIcombine.default(estimates)\n#>                        results      se (lower  upper) missInfo\n#> (Intercept)             0.0037 0.00072 0.0025  0.0056     45 %\n#> OAOA                    2.7579 0.74733 1.4778  5.1469     76 %\n#> age40-49 years          2.1857 0.22218 1.7909  2.6676      0 %\n#> age50-59 years          5.0261 0.49785 4.1392  6.1031      1 %\n#> age60-64 years          7.3753 0.76556 6.0175  9.0394      1 %\n#> sexMale                 1.8564 0.10181 1.6672  2.0672      4 %\n#> income$30,000-$49,999   0.6451 0.04378 0.5647  0.7369      0 %\n#> income$50,000-$79,999   0.5495 0.03623 0.4829  0.6253      0 %\n#> income$80,000 or more   0.5051 0.03489 0.4411  0.5783      0 %\n#> raceWhite               1.2518 0.13228 1.0176  1.5399      2 %\n#> painmedYes              2.0443 0.33747 1.3628  3.0664     86 %\n#> htYes                   2.8273 0.15614 2.5372  3.1505      0 %\n#> copdYes                 6.8279 4.45458 1.9009 24.5255      0 %\n#> diabYes                 2.2397 0.18298 1.9083  2.6287      1 %\n#> OAOA:painmedYes         0.4925 0.16045 0.2275  1.0663     81 %\n#> age40-49 years:copdYes  0.3292 0.22863 0.0844  1.2841      0 %\n#> age50-59 years:copdYes  0.4792 0.32275 0.1280  1.7940      0 %\n#> age60-64 years:copdYes  0.3999 0.26244 0.1105  1.4473      0 %\n#> sexMale:copdYes         1.7812 0.55019 0.9723  3.2632      1 %\nsum.pooled\n\n\n\n  \n\n\n\nEstimating model performance (AUC and AL)\nIndividual AUC estimates (with interactions)\n\nShow the codelibrary(ROCR)\nlibrary(WeightedROC)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nAL.scalar <- AUC.scalar <- vector(\"list\", 5)\nfor (i in 1:5) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design <- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit <- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc <- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL <- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] <- AL\n  AUC.scalar[[i]] <- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#> AUC calculated for data 1 \n#> AUC calculated for data 2 \n#> AUC calculated for data 3 \n#> AUC calculated for data 4 \n#> AUC calculated for data 5\nstr(AUC.scalar)\n#> List of 5\n#>  $ : num 0.8\n#>  $ : num 0.795\n#>  $ : num 0.798\n#>  $ : num 0.8\n#>  $ : num 0.794\nAL.scalar\n#> [[1]]\n#> [1] 0.01243738\n#> \n#> [[2]]\n#> [1] 0.5471927\n#> \n#> [[3]]\n#> [1] 0.13081\n#> \n#> [[4]]\n#> [1] 0.644286\n#> \n#> [[5]]\n#> [1] 0.6049137\n\n\nAveraged estimates for AUC (with interactions)\n\nShow the code# summary of AUC\nmean(unlist(AUC.scalar))\n#> [1] 0.7973746\nsd(unlist(AUC.scalar))\n#> [1] 0.002688964\nround(range(unlist(AUC.scalar)),3)\n#> [1] 0.794 0.800\n# p-values (from AL) by majority\nsum(AL.scalar>0.05)\n#> [1] 4\n\n\nModel performance without interactions\nIndividual AUC estimates / AL p-values\n\nShow the codelibrary(ROCR)\nlibrary(WeightedROC)\nAL.scalar <- AUC.scalar <- vector(\"list\", 5)\nmodel.formula <- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab\")\nfor (i in 1:5) {\n  analytic.i <- allImputations$imputations[[i]]\n  w.design <- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit <- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc <- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL <- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] <- AL\n  AUC.scalar[[i]] <- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#> AUC calculated for data 1 \n#> AUC calculated for data 2 \n#> AUC calculated for data 3 \n#> AUC calculated for data 4 \n#> AUC calculated for data 5\nstr(AUC.scalar)\n#> List of 5\n#>  $ : num 0.798\n#>  $ : num 0.794\n#>  $ : num 0.797\n#>  $ : num 0.798\n#>  $ : num 0.794\nAL.scalar\n#> [[1]]\n#> [1] 0.01839624\n#> \n#> [[2]]\n#> [1] 0.2836256\n#> \n#> [[3]]\n#> [1] 0.02439659\n#> \n#> [[4]]\n#> [1] 0.8333214\n#> \n#> [[5]]\n#> [1] 0.2050434\n\n\nAveraged estimates for AUC / majority of AL p-values\n\nShow the code# summary of AUC\nmean(unlist(AUC.scalar))\n#> [1] 0.7964061\nsd(unlist(AUC.scalar))\n#> [1] 0.002002636\nround(range(unlist(AUC.scalar)),3)\n#> [1] 0.794 0.798\n# p-values (from AL) by majority\nsum(AL.scalar>0.05)\n#> [1] 3\n\n\nAppendix\nUser-written svyROCw3 and AL.gof3 functions\n\nShow the codesvyROCw3\n#> function(fit=fit7,outcome=analytic2$CVD==\"event\", weight = NULL,plot=FALSE){\n#>   # ROC curve for\n#>   # Survey Data with Logistic Regression\n#>   if (is.null(weight)){ # require(ROCR)\n#>     prob <- predict(fit, type = \"response\")\n#>   pred <- prediction(as.vector(prob), outcome)\n#>   perf <- performance(pred, \"tpr\", \"fpr\")\n#>   auc <- performance(pred, measure = \"auc\")\n#>   auc <- auc@y.values[[1]]\n#>   if (plot == TRUE){\n#>     roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n#>       model = \"Logistic\")\n#>     with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n#>      xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n#>      main = paste(\"AUC = \", round(auc,3))))\n#>   mtext(\"Unweighted ROC\")\n#>   abline(0,1, lty=2)\n#>   }\n#>   } else { # library(WeightedROC)\n#>     outcome <- as.numeric(outcome)\n#>   pred <- predict(fit, type = \"response\")\n#>   tp.fp <- WeightedROC(pred, outcome, weight)\n#>   auc <- WeightedAUC(tp.fp)\n#>   if (plot == TRUE){\n#>     with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n#>      xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n#>      main = paste(\"AUC = \", round(auc,3))))\n#>   abline(0,1, lty=2)\n#>   mtext(\"Weighted ROC\")\n#>   }\n#>   }\n#>   return(auc)\n#> }\n#> <bytecode: 0x0000020309999a40>\nAL.gof3\n#> function(fit=fit7, data = analytic2, weight = \"weight\", psu = \"psu\", strata= \"strata\"){\n#>   # Archer-Lemeshow Goodness of Fit Test for\n#>   # Survey Data with Logistic Regression\n#>   r <- residuals(fit, type=\"response\") \n#>   f<-fitted(fit) \n#>   breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n#>   breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n#>   g<- cut(f, breaks.g)\n#>   data2g <- cbind(data,r,g)\n#>   if (is.null(psu)){\n#>     newdesign <- svydesign(id=~1,\n#>                          weights=as.formula(paste0(\"~\",weight)), \n#>                         data=data2g, nest = TRUE)\n#>   }\n#>   if (!is.null(psu)) {\n#>     newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n#>                          strata=as.formula(paste0(\"~\",strata)),\n#>                          weights=as.formula(paste0(\"~\",weight)), \n#>                         data=data2g, nest = TRUE)\n#>   }\n#>   decilemodel<- svyglm(r~g, design=newdesign) \n#>   res <- regTermTest(decilemodel, ~g)\n#>   return(as.numeric(res$p)) \n#> }\n#> <bytecode: 0x0000020309c11030>"
  },
  {
    "objectID": "missingdata5.html",
    "href": "missingdata5.html",
    "title": "Subpopulations",
    "section": "",
    "text": "This tutorial demonstrates how to manage missing data in complex surveys using multiple imputation, focusing on specific subpopulations defined by the study’s eligibility criteria.\nPurpose\nLet us we are interested in exploring the relationship between rheumatoid arthritis and cardiovascular disease (CVD) among US adults aged 20 years or more. For that, we will use NHANES 2017–2018 dataset, which follows a complex survey design.\n\n\nIn this tutorial, we used a similar approach to the one in a published article by Hossain et al. (2022), but we used less data (restricted to only 2017–2018) to speed up the analysis. Ref link.\nThe purpose of this example is to demonstrate how to do the missing data analysis with multiple imputation in the context of complex surveys.\nThe main idea is:\n\nworking with the analytic data\nimputing missing values based on that analytic dataset\nkeep count of the ineligible subjects from the full data who are not included in the analytic data\nadding those ineligible subjects back in the imputed datasets, so that we can utilize the survey features and subset the design.\n\n\nShow the code# Load required packages\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tableone)\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\nlibrary(mice)\nlibrary(mitools)\n\n\nLet us import the dataset:\n\nShow the codeload(\"Data/missingdata/MIexample.RData\")\nls()\n#> [1] \"dat.full\"\n\n\n\nShow the codedim(dat.full)\n#> [1] 9254   15\nhead(dat.full)\n\n\n\n  \n\n\n\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nAnalytic dataset\nSubsetting according to eligibility\nLet us create an analytic dataset for\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\nShow the code# Drop < 20 years\ndat.with.miss <- subset(dat.full, age >= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#> \n#>   No  Yes <NA> \n#> 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#> \n#>   No  Yes <NA> \n#> 3857  337 1375\n\n# Drop missing in outcome and exposure - dataset with missing values only in covariates\ndat.analytic <- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#> [1] 4191\n\n\nAs we can see, we have 4,191 participants aged 20 years or more without missing values in outcome or exposure. Let us count the ineligible subjects from the full data and create an indicator variable.\n\nShow the codedat.full$ineligible <- 1\ndat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] <- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#> \n#>    0    1 <NA> \n#> 4191 5063    0\n\n\nWe have 4,191 eligible and 5,063 ineligible subjects based on the eligibility criteria.\nGeneral strategy of solution:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nlater we will include 5,063 ineligible subjects in the data so that we can utilize survey features.\nTable 1\nLet us see the summary statistics, i.e., create Table 1 stratified by outcome (cvd). Before that, we will categorize age and recode rheumatoid:\n\nShow the code# Categorical age\ndat.analytic$age.cat <- with(dat.analytic, ifelse(age >= 20 & age < 50, \"20-49\", \n                                  ifelse(age >= 50 & age < 65, \"50-64\", \"65+\")))\ndat.analytic$age.cat <- factor(dat.analytic$age.cat, levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.analytic$age.cat, useNA = \"always\")\n#> \n#> 20-49 50-64   65+  <NA> \n#>  2280  1097   814     0\n\n# Recode rheumatoid to arthritis\ndat.analytic$arthritis <- car::recode(dat.analytic$rheumatoid, \" 'No' = 'No arthritis';\n                                      'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.analytic$arthritis, useNA = \"always\")\n#> \n#>         No arthritis Rheumatoid arthritis                 <NA> \n#>                 3854                  337                    0\n\n# Keep only relevant variables\nvars <-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\", \"arthritis\", \"age.cat\", \n           \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 <- dat.analytic[, vars]\n\n\n\nShow the code# Create Table 1\nvars <- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 <- CreateTableOne(vars = vars, strata = \"cvd\", data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#>                  Stratified by cvd\n#>                   level                     Overall      No          \n#>   n                                          4191         3823       \n#>   arthritis       No arthritis               3854         3580       \n#>                   Rheumatoid arthritis        337          243       \n#>   age.cat         20-49                      2280         2240       \n#>                   50-64                      1097          979       \n#>                   65+                         814          604       \n#>   sex             Male                       2126         1884       \n#>                   Female                     2065         1939       \n#>   education       Less than high school       828          728       \n#>                   High school                2292         2094       \n#>                   College graduate or above  1063          993       \n#>   race            White                      1275         1113       \n#>                   Black                       998          898       \n#>                   Hispanic                   1015          958       \n#>                   Others                      903          854       \n#>   income          less than $20,000           659          557       \n#>                   $20,000 to $74,999         1967         1796       \n#>                   $75,000 and Over           1143         1079       \n#>   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#>   smoking         Never smoker               2570         2427       \n#>                   Previous smoker             882          726       \n#>                   Current smoker              739          670       \n#>   htn             No                         1424         1380       \n#>                   Yes                        2415         2107       \n#>   diabetes        No                         3622         3396       \n#>                   Yes                         566          424       \n#>                  Stratified by cvd\n#>                   Yes         \n#>   n                 368       \n#>   arthritis         274       \n#>                      94       \n#>   age.cat            40       \n#>                     118       \n#>                     210       \n#>   sex               242       \n#>                     126       \n#>   education         100       \n#>                     198       \n#>                      70       \n#>   race              162       \n#>                     100       \n#>                      57       \n#>                      49       \n#>   income            102       \n#>                     171       \n#>                      64       \n#>   bmi (mean (SD)) 30.09 (7.29)\n#>   smoking           143       \n#>                     156       \n#>                      69       \n#>   htn                44       \n#>                     308       \n#>   diabetes          226       \n#>                     142\n\n\nCheck missingness using a plot\nNow we will see the percentage of missing values in the variables.\n\nShow the codeDataExplorer::plot_missing(dat.analytic2)\n\n\n\n\nWe have about 10% missing values in income, followed by hypertension (8.4%), bmi (6.8%), education (0.2%), and diabetes (0.1%).\nDealing with missing values in covariates\n\nNow we will perform multiple imputation to deal with missing values only in covariates. We will use the dat.analytic2 dataset that contains missing values in the covariates but no missing values in the outcome or exposure.\nFor this exercise, we will consider 5 imputed datasets, 3 iterations, and the predictive mean matching method for bmi and income.\n\nWe have already set up the data such that the variables are of appropriate types, e.g., numeric bmi, factor age, sex, and so on.\nWe will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nNow we will set up the initial model and set up the methods and predictor matrix before imputing 5 datasets.\n\n\n\nStep 0: Set up the imputation model\n\nShow the code# Step 0: Set imputation model\nini <- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred <- ini$pred\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",] <- 0\n\n# Do not use survey weight or PSU variable as auxiliary variables\npred[,\"studyid\"] <- pred[\"studyid\",] <- 0\npred[,\"psu\"] <- pred[\"psu\",] <- 0\npred[,\"survey.weight\"] <- pred[\"survey.weight\",] <- 0\n\n# Set imputation method\nmeth <- ini$meth\nmeth[\"bmi\"] <- \"pmm\"\nmeth[\"income\"] <- \"pmm\"\nmeth\n#>       studyid survey.weight           psu        strata           cvd \n#>            \"\"            \"\"            \"\"            \"\"            \"\" \n#>     arthritis       age.cat           sex     education          race \n#>            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#>        income           bmi       smoking           htn      diabetes \n#>         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\n\n\nThere is no missing for studyid, survey.weight, psu, strata, cvd, arthritis, age, sex, race, smoking. Hence, no method is assigned for these variables.\nFor education, polyreg (Polytomous logistic regression) will be used.\nSimilarly, we will use pmm (Predictive mean matching) for bmi, income and used logreg (Logistic regression) for htn, diabetes.\nStep 1: Imputing missing values using mice\n1.1 Imputing dataset for eligible subjects\n\nShow the code# Step 1: impute the incomplete data\nimputation <- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nShow the codeimpdata <- mice::complete(imputation, action=\"long\")\n\ntable(impdata$age.cat)\n#> \n#> 20-49 50-64   65+ \n#> 11400  5485  4070\n\n\nNote that age has no missing values, and everyone is above 20.\n\nShow the code#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id <- NULL\n\n# Create an indicator of eligible subjects \nimpdata$ineligible <- 0\n\n# Number of subjects\nnrow(impdata)\n#> [1] 20955\n\n\nLet’s see whether there is any missing value after imputation:\n\nShow the codeDataExplorer::plot_missing(impdata)\n\n\n\n\n\nThere is no missing value after imputation.\nAs we can see, there is an additional variable (.imp) in the imputed dataset. This .imp goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\n1.2 Preparing dataset for ineligible subjects\nThe next task is adding the ineligible subjects in the imputed datasets, so that we can set up the survey design on the full dataset and then subset the design.\n\nShow the code# Number of ineligible subjects\n#dat.full$ineligible <- 1\n#dat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] <- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#> \n#>    0    1 <NA> \n#> 4191 5063    0\n\n\nNow we will subset the data for ineligible subjects and create m = 5 copies.\n\nShow the code# Subset for ineligible\ndat.ineligible <- subset(dat.full, ineligible == 1)\n\n# Create m = 5 datasets with .imp 1 to m = 5\ndat31 <- dat.ineligible; dat31$.imp <- 1\ndat32 <- dat.ineligible; dat32$.imp <- 2\ndat33 <- dat.ineligible; dat33$.imp <- 3\ndat34 <- dat.ineligible; dat34$.imp <- 4\ndat35 <- dat.ineligible; dat35$.imp <- 5\n\n\nThe next step is combining ineligible datasets. Before merging the stacked dataset for ineligible subjects to the imputed stacked dataset for eligible subjects, we must ensure the variable names are the same.\n\nShow the code# Stacked data for ineligible subjects\ndat.ineligible.stacked <- rbind(dat31, dat32, dat33, dat34, dat35)\n\n\nWe should have missing value in this ineligible part of the data:\n\nShow the codeDataExplorer::plot_missing(dat.ineligible.stacked)\n\n\n\n\n1.3 Combining eligible (imputed) and ineligible (unimputed) subjects\n\nShow the codenames(impdata)\n#>  [1] \".imp\"          \"studyid\"       \"survey.weight\" \"psu\"          \n#>  [5] \"strata\"        \"cvd\"           \"arthritis\"     \"age.cat\"      \n#>  [9] \"sex\"           \"education\"     \"race\"          \"income\"       \n#> [13] \"bmi\"           \"smoking\"       \"htn\"           \"diabetes\"     \n#> [17] \"ineligible\"\n\n\n\nShow the codenames(dat.ineligible.stacked)\n#>  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#>  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#>  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#> [13] \"smoking\"       \"htn\"           \"diabetes\"      \"ineligible\"   \n#> [17] \".imp\"\n\n\nAs we can see, the variable names are different in the two datasets. Particularly, arthritis and age.cat variables are not available in the dat.ineligible.stacked dataset. Now we will recode these variables in the same format as done for impdata:\n\nShow the code# Categorical age\nsummary(dat.ineligible.stacked$age)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00    5.00   12.00   23.25   41.00   80.00\ndat.ineligible.stacked$age.cat <- with(dat.ineligible.stacked, \n                                       ifelse(age >= 20 & age < 50, \"20-49\", \n                                              ifelse(age >= 50 & age < 65, \"50-64\", \n                                                    ifelse(age >= 65, \"65+\", NA))))\ndat.ineligible.stacked$age.cat <- factor(dat.ineligible.stacked$age.cat, \n                                         levels = c(\"20-49\", \"50-64\", \"65+\"))\n\n\nNote that, we are assigning anyone with less than 20 age as missing.\n\nShow the codetable(dat.ineligible.stacked$age.cat, useNA = \"always\")\n#> \n#> 20-49 50-64   65+  <NA> \n#>  1100  2360  3430 18425\n# Recode arthritis\ndat.ineligible.stacked$arthritis <- car::recode(dat.ineligible.stacked$rheumatoid, \n                                                \" 'No' = 'No arthritis'; 'Yes' = \n                                                'Rheumatoid arthritis' \", as.factor = T)\n\n\nNote: In the above step, we could also create two variables with missing values rather than recoding. The reason is that we will subset the design; no matter whether we recode or create missing values for ineligible, the only information we need from ineligible subjects is their survey features when creating the design.\nThe next step is to combine these two datasets (impdata and dat.ineligible.stacked).\n\nShow the code# Variable names in the imputed dataset\nvars <- names(impdata) \n\n# Set up the dataset for ineligible - same variables as impdata\ndat.ineligible.stacked <- dat.ineligible.stacked[, vars]\n\n\nNow we will merge ineligible and eligible subjects to make the full dataset of 5 \\(\\times\\) 9,254 = 46,270 subjects.\n\nShow the codeimpdata2 <- rbind(impdata, dat.ineligible.stacked)\nimpdata2 <- impdata2[order(impdata2$.imp, impdata2$studyid),]\ndim(impdata2)\n#> [1] 46270    17\n\n\n1.4 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on full dataset [with eligible (imputed) and ineligible (unimputed) subjects] of 5 \\(\\times\\) 9,254 = 46,270 subjects and subset the design for 5 \\(\\times\\) 4,716 = 23,580 subjects.\n\nShow the codem <- 5\nallImputations <- imputationList(lapply(1:m, function(n) subset(impdata2, subset=.imp==n)))\n\n# Step 2: Survey data analysis\nw.design0 <- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) # Design on full data\nw.design <- subset(w.design0, ineligible == 0) # Subset the design\n\n\nWe can see the length of the subsetted design:\n\nShow the codedim(w.design)\n#> [1] 4191   17    5\n\n\nThe subsetted design contains 4,191 subjects with 17 variables and 5 imputed datasets. Now we will run the design-adjusted logistic regression on and pool the estimate using Rubin’s rule:\nStep 2: Design adjusted regression analysis\n\nShow the code# Design-adjusted logistic regression\nfit <- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\nres <- exp(as.data.frame(cbind(coef(fit[[1]]),\n      coef(fit[[2]]),\n      coef(fit[[3]]),\n      coef(fit[[4]]),\n      coef(fit[[5]]))))\nnames(res) <- paste(\"OR from m =\", 1:5)\nround(t(res),2)\n#>               (Intercept) arthritisRheumatoid arthritis age.cat50-64 age.cat65+\n#> OR from m = 1        0.02                          2.03         4.71      13.08\n#> OR from m = 2        0.02                          2.04         4.74      13.21\n#> OR from m = 3        0.02                          2.12         4.58      11.88\n#> OR from m = 4        0.02                          2.10         4.59      12.45\n#> OR from m = 5        0.02                          2.07         4.47      12.25\n#>               sexFemale educationHigh school educationCollege graduate or above\n#> OR from m = 1      0.46                 0.75                               0.75\n#> OR from m = 2      0.45                 0.76                               0.79\n#> OR from m = 3      0.47                 0.71                               0.70\n#> OR from m = 4      0.46                 0.70                               0.71\n#> OR from m = 5      0.45                 0.75                               0.77\n#>               raceBlack raceHispanic raceOthers income$20,000 to $74,999\n#> OR from m = 1      0.96         0.54       0.86                     0.48\n#> OR from m = 2      0.97         0.54       0.88                     0.48\n#> OR from m = 3      0.98         0.54       0.85                     0.54\n#> OR from m = 4      0.96         0.53       0.86                     0.51\n#> OR from m = 5      0.99         0.55       0.87                     0.53\n#>               income$75,000 and Over  bmi smokingPrevious smoker\n#> OR from m = 1                   0.33 1.02                   1.55\n#> OR from m = 2                   0.32 1.03                   1.55\n#> OR from m = 3                   0.36 1.01                   1.59\n#> OR from m = 4                   0.34 1.02                   1.56\n#> OR from m = 5                   0.34 1.02                   1.55\n#>               smokingCurrent smoker htnYes diabetesYes\n#> OR from m = 1                  1.43   1.35        3.01\n#> OR from m = 2                  1.47   1.30        3.01\n#> OR from m = 3                  1.45   1.48        3.13\n#> OR from m = 4                  1.44   1.46        3.03\n#> OR from m = 5                  1.45   1.47        3.05\n\n\nStep 3: Pooling estimates\n\nShow the code# Step 3: Pooled estimates\npooled.estimates <- MIcombine(fit)\nOR <- round(exp(pooled.estimates$coefficients), 2)\nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR\n\n\n\n  \n\n\n\nConclusion\nAmong US adults aged 20 years or more, the odds of CVD was approximately twice among those with rheumatoid arthritis than no arthritis.\nReferences\n\n\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30."
  },
  {
    "objectID": "missingdata6.html",
    "href": "missingdata6.html",
    "title": "MCAR tests",
    "section": "",
    "text": "MCAR tests are essential tools in the data analysis process. They help researchers understand the nature of missingness in their datasets, guide appropriate imputation strategies, and ensure the validity and reliability of statistical analyses.\nMCAR data\nIn the initial chunk, several packages are loaded. These packages provide functions and tools necessary for the subsequent analysis, including multiple imputation, statistical modeling, and data visualization.\n\nShow the code# Load required packages\nrequire(mice)\nrequire(mitools)\nrequire(survey)\nrequire(remotes)\nrequire(simcausal)\n\n\nData generating process\nA Directed Acyclic Graph (DAG) is defined, which represents a causal model of how different variables in the dataset relate to each other. This DAG is used to simulate data based on the relationships defined. We generate L as a function of P and B.\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"B\", distr = \"rnorm\", mean = 0, sd = 1) +\n  node(\"P\", distr = \"rnorm\", mean = 0, sd = .7) +\n  node(\"L\", distr = \"rnorm\", mean = 2 + 2 * P + 3 * B, sd = 3) + \n  node(\"A\", distr = \"rnorm\", mean = 0.5 + L + 2 * P, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A + B + 2 * P, sd = .5)\nDset <- set.DAG(D)\n\n\nGenerate DAG\nThe previously defined DAG is visualized, providing a graphical representation of the relationships between variables.\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\nUsing the DAG, a dataset is simulated. This dataset will be used for subsequent analysis.\n\nShow the codeObs.Data <- sim(DAG = Dset, n = 10000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\nShow the codeObs.Data.original <- Obs.Data\n\n\nRandomly set some data to missing\nSome values in the dataset are randomly set to missing (i.e., randomly assign some L values to missing). This simulates a scenario where data might be missing completely at random (MCAR).\n\nShow the codeset.seed(123)\nObs.Data$L[sample(1:length(Obs.Data$L), size = 1000)] <- NA\nsummary(Obs.Data$L)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#> -17.116  -1.096   1.896   1.968   4.996  20.129    1000\n\n\nThe missing data patterns in the dataset are visualized. This helps in understanding which variables have missing values and how they might be related.\n\nShow the coderequire(VIM)\nres <- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\nVisualize via margin plots\nMargin plots are used to compare the distributions of variables when a particular variable is missing versus when it is observed. This provides insights into how missingness might be related to the values of other variables.\n\nThe red boxplot depicts the distribution of a variable in the data where L has a missing value.\nThe blue boxplot depicts the distribution of the values of a variable in the data where L has an observed value.\n\nSame median and spread (range) may mean no difference in the distribution.\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\nLittle’s MCAR test\nA statistical test is conducted to determine if data is missing entirely at random (MCAR). The outcome of this test offers a deeper understanding of the reasons behind the missing data.\nLittle’s 1988 chi-squared test evaluates if data is MCAR by checking for significant differences in the means of various missing-value patterns (Little 1988). Based on the test’s statistic and p-value, we can infer if the data is MCAR. The null hypothesis for this test is that the data is MCAR.\n\nThe essence of this test is to compare means across groups with different missing data patterns. It uses a likelihood ratio test and assumes the data follows a multivariate normal distribution.\nIf this test is rejected (indicated by a low p-value or a high statistic), it suggests the data might not be MCAR.\n\nHowever, this test has several limitations (rdrr 2023):\n\nThe test doesn’t pinpoint which specific variables don’t adhere to MCAR, meaning it doesn’t highlight potential variables associated with missingness.\nIt assumes multivariate normality. If this assumption isn’t met, especially with non-normal or categorical variables, the test might not be reliable unless there’s a large sample size.\nThe test assumes that all missing data patterns have the same covariance matrix. This means it can’t detect deviations from MCAR that are based on covariance, especially if the data is Missing at Random (MAR) or Missing Not at Random (MNAR).\nResearch has shown that Little’s MCAR test might have low power, especially when few variables don’t follow MCAR, the association between the data and its missingness is weak, or if the data is MNAR.\nThe test can only reject the MCAR assumption but can’t confirm it. A non-significant result doesn’t necessarily confirm that the data is MCAR.\nEven if the test result is significant, it doesn’t rule out the possibility of the data being MNAR.\n\n\nShow the coderequire(naniar)\nmcar_test(Obs.Data)\n\n\n\n  \n\n\nShow the code\nrequire(misty)\nna.test(Obs.Data)\n#>  Little's MCAR Test\n#> \n#>       n nIncomp nPattern chi2 df  pval \n#>   10000    1000        2 2.60  5 0.761\n\n\nMCAR and normality test\nHawkins, in 1981, introduced a method to assess both multivariate normality and the consistency in variances, known as homoscedasticity. This method not only checks for consistent variances but also for mean equality (Hawkins 1981).\nIn 2010, Jamshidian and Jalal suggested a technique to compare covariances among groups with the same missing data patterns. They utilized the Hawkins test for data assumed to be normal and a non-parametric approach for other data types (Jamshidian and Jalal 2010).\nThe following package (Jamshidian and Jalal 2010) tests multivariate normality and homoscedasticity in the context of missing data.\n\nShow the code#library(devtools)\n#install_github(\"cran/MissMech\")\nlibrary(MissMech)\ntest.result <- TestMCARNormality(data = Obs.Data)\ntest.result\n#> Call:\n#> TestMCARNormality(data = Obs.Data)\n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1   NA   1   1              1000\n#> group.2    1   1   1    1   1   1              9000\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#> \n#>     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#>     Provided that normality can be assumed, the hypothesis of MCAR is \n#>     rejected at 0.05 significance level. \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0.4019219 \n#> \n#>     Reject Normality at 0.05 significance level.\n#>     There is not sufficient evidence to reject MCAR at 0.05 significance level.\n\nsummary(test.result)\n#> \n#> Number of imputation:  1 \n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1   NA   1   1              1000\n#> group.2    1   1   1    1   1   1              9000\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0.4019219\nboxplot(test.result)\n\n\n\n\nNon-MCAR data\nSet some data to missing based on a rule\nIn this section, the original dataset is restored. Then, for a specific column, any value greater than a certain threshold is set to ‘missing’. A summary of this column is then provided to understand the distribution of missing values.\n\nShow the codeObs.Data <- Obs.Data.original\nObs.Data$L[Obs.Data$L > 7.79] <- NA\nsummary(Obs.Data$L)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#> -17.116  -1.482   1.330   1.050   3.954   7.787    1035\n\n\nThe dataset’s missing data patterns are visualized. This visualization helps in understanding which data points are missing and their distribution across the dataset.\n\nShow the coderes <- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\nVisualize via margin plots\nMargin plots are used to visualize the relationship between two variables, especially when one of them has missing values. Here, the relationship of a specific column with missing values is visualized against other columns in the dataset. This helps in understanding how the missingness in one variable might relate to other variables.\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\nShow the codemarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\nLittle’s MCAR test\nLittle’s MCAR test is applied to the dataset to check if the data is missing completely at random. This test provides a statistic and a p-value to determine the nature of missingness in the data.\n\nShow the codemcar_test(Obs.Data)\n\n\n\n  \n\n\nShow the codena.test(Obs.Data)\n#>  Little's MCAR Test\n#> \n#>       n nIncomp nPattern    chi2 df  pval \n#>   10000    1035        2 3511.08  5 0.000\n\n\nMCAR and normality test\nA test is conducted to check if the data follows a multivariate normal distribution and if the variances across different groups are consistent (homoscedasticity). The results of this test, along with a summary and a boxplot visualization, are provided to understand the distribution and characteristics of the data.\n\nShow the codetest.result <- TestMCARNormality(data = Obs.Data)\ntest.result\n#> Call:\n#> TestMCARNormality(data = Obs.Data)\n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1    1   1   1              8965\n#> group.2    1   1   1   NA   1   1              1035\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#> \n#>     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#>     Provided that normality can be assumed, the hypothesis of MCAR is \n#>     rejected at 0.05 significance level. \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0 \n#> \n#>     Hypothesis of MCAR is rejected at  0.05 significance level.\n#>     The multivariate normality test is inconclusive.\nsummary(test.result)\n#> \n#> Number of imputation:  1 \n#> \n#> Number of Patterns:  2 \n#> \n#> Total number of cases used in the analysis:  10000 \n#> \n#>  Pattern(s) used:\n#>           ID   B   P    L   A   Y   Number of cases\n#> group.1    1   1   1    1   1   1              8965\n#> group.2    1   1   1   NA   1   1              1035\n#> \n#> \n#>     Test of normality and Homoscedasticity:\n#>   -------------------------------------------\n#> \n#> Hawkins Test:\n#> \n#>     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#> \n#> Non-Parametric Test:\n#> \n#>     P-value for the non-parametric test of homoscedasticity:  0\nboxplot(test.result)\n\n\n\n\n\n\n\n\n\n\nHawkins, Douglas M. 1981. “A New Test for Multivariate Normality and Homoscedasticity.” Technometrics 23 (1): 105–10.\n\n\nJamshidian, Mortaza, and Siavash Jalal. 2010. “Tests of Homoscedasticity, Normality, and Missing Completely at Random for Incomplete Multivariate Data.” Psychometrika 75 (4): 649–74.\n\n\nLittle, Roderick JA. 1988. “A Test of Missing Completely at Random for Multivariate Data with Missing Values.” Journal of the American Statistical Association 83 (404): 1198–1202.\n\n\nrdrr. 2023. “Na.test: Little’s Missing Completely at Random (MCAR) Test.” https://rdrr.io/cran/misty/man/na.test.html."
  },
  {
    "objectID": "missingdata7.html",
    "href": "missingdata7.html",
    "title": "Effect modification",
    "section": "",
    "text": "In this tutorial, we delve into the concept of effect modification.\n\n\n\n\n\n\nImportant\n\n\n\nEffect modification and interaction are two distinct concepts in epidemiology. Effect modification occurs when the causal effect of an exposure (A) on an outcome (Y) varies based on the levels of a third factor (B). In this scenario, the association between the exposure and the outcome differs within the strata of a second exposure, which acts as the effect modifier. For instance, the impact of alcohol (A) on oral cancer (Y) might differ based on tobacco smoking (B). On the other hand, interaction refers to the joint causal effect of two exposures (A and B) on an outcome (Y). It examines how the combination of multiple exposures influences the outcome, such as the combined effect of alcohol (A) and tobacco smoking (B) on oral cancer (Y). In essence, while effect modification looks at how a third factor influences the relationship between an exposure and an outcome, interaction focuses on the combined effect of two exposures on the outcome. To revisit or deepen your grasp of these two concepts, consider reviewing this external tutorial.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nWe start by loading the necessary packages that will aid in the analysis. We also define a function tidy.pool_mi to streamline the pooling process for multiple imputation results.\n\nShow the code# Load required packages\nlibrary(survey)\nrequire(interactions)\nrequire(mitools)\nrequire(mice)\nrequire(miceadds)\nlibrary(modelsummary)\n\ntidy.pool_mi <- function(x, ...) {\n  msg <- capture.output(out <- summary(x, ...))\n  out$term <- row.names(out)\n  colnames(out) <- c(\"estimate\", \"std.error\", \"statistic\", \"p.value\",\n                     \"conf.low\", \"conf.high\", \"miss\", \"term\")\n  return(out)\n}\n\n\nData\nWe load a dataset named smi. This dataset is a list of multiple imputed datasets, which is evident from the structure and the way we access its elements.\n\nShow the coderequire(mitools)\ndata(smi)\nlength(smi)\n#> [1] 2\nlength(smi[[1]])\n#> [1] 5\nhead(smi[[1]][[1]])\n#>       id wave mmetro parsmk         drkfre         alcdos alcdhi           smk\n#> 1 920006    1      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 2 920006    2      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 3 920006    3      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 4 920006    4      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 5 920006    5      1      0    Non drinker    Non drinker      0 non/ex-smoker\n#> 6 920006    6      1      0 not in last wk not in last wk      0 non/ex-smoker\n#>   cistot mdrkfre sex drinkreg\n#> 1      0       0   1    FALSE\n#> 2      0       0   1    FALSE\n#> 3      0       0   1    FALSE\n#> 4      1       0   1    FALSE\n#> 5      4       0   1    FALSE\n#> 6      3       0   1    FALSE\n\n\nModel with interaction and ORs\nWe’re interested in understanding how the variable wave interacts with sex in predicting drinkreg. We fit two logistic regression models, one for each level of the sex variable, to understand this interaction. wave is exposure here.\nFor effect modifier sex = 0\n\nShow the codemodels <- with(smi, glm(drinkreg~ wave + sex + wave*sex, family = binomial()))\nsummary(pool(models, rule = \"rubin1987\"), conf.int = TRUE, exponentiate = TRUE)[2,]\n#>   term estimate  std.error statistic       df      p.value   2.5 %   97.5 %\n#> 2 wave 1.271952 0.06587423  3.651693 234.4974 0.0003212903 1.11714 1.448217\n\n\nFor effect modifier sex = 1 (just changing reference)\n\nShow the codemodels2<-with(smi, glm(drinkreg~ wave + I(sex==0) + wave*I(sex==0),family=binomial()))\nsummary(pool(models2, rule = \"rubin1987\"),conf.int = TRUE, exponentiate = TRUE)[2,]\n#>   term estimate  std.error statistic       df      p.value    2.5 %   97.5 %\n#> 2 wave 1.225438 0.05876369   3.45959 240.3053 0.0006399352 1.091487 1.375828\n\n\n\nNotice the ORs for wave in the above 2 analyses. These are basically our target.\nFor proper survey data analysis, you will have to work with design and make sure you subset your subpopulation (those eligible) appropriately.\nSimple slopes analyses\nWe perform a simple slopes analysis for each imputed dataset. This analysis helps in understanding the relationship between the predictor and the outcome at specific levels of the moderator.\n\nShow the codea1 <- sim_slopes(models[[1]], pred = wave, modx = sex)\na2 <- sim_slopes(models[[2]], pred = wave, modx = sex)\na3 <- sim_slopes(models[[3]], pred = wave, modx = sex)\na4 <- sim_slopes(models[[4]], pred = wave, modx = sex)\na5 <- sim_slopes(models[[5]], pred = wave, modx = sex)\n\n\nAfter obtaining the results from each imputed dataset, we pool them to get a consolidated result. This is done separately for each level of the sex variable.\nPooled results for sex = 0\n\nShow the code# For sex = 0\nef.lev <- 1\nest <- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse <- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr <- se^2\nOR <- exp(est)\nOR.se <- OR * se\nOR.v <- OR.se^2\n\nmod_pooled <- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n#>   estimate  std.error statistic      p.value conf.low conf.high   miss term\n#> 1 1.272164 0.08386552  15.16909 6.987679e-39 1.107118   1.43721 12.2 %    1\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#> Multiple imputation results:\n#>       MIcombine.default(as.list(OR), as.list(OR.v))\n#>    results         se   (lower  upper) missInfo\n#> 1 1.272164 0.08386552 1.107118 1.43721     12 %\n\n\nPooled results for sex = 1\n\nShow the code# For sex = 1\nef.lev <- 2\nest <- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse <- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr <- se^2\nOR <- exp(est)\nOR.se <- OR * se\nOR.v <- OR.se^2\n\nmod_pooled <- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n#>   estimate  std.error statistic      p.value conf.low conf.high   miss term\n#> 1 1.225598 0.07204679  17.01113 2.282427e-46 1.083838  1.367357 11.9 %    1\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#> Multiple imputation results:\n#>       MIcombine.default(as.list(OR), as.list(OR.v))\n#>    results         se   (lower   upper) missInfo\n#> 1 1.225598 0.07204679 1.083838 1.367357     12 %"
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R functions (M)",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggr \n    VIM \n    To calculate/plot the missing values in the variables \n  \n\n boxplot \n    base/graphics \n    To produce a box plot \n  \n\n bwplot \n    mice \n    To produce box plot to compare the imputed and observed values \n  \n\n colMeans \n    base \n    To compute the column-wise mean, i.e., mean for each variable/column \n  \n\n complete \n    mice \n    To extract the imputed dataset \n  \n\n complete.cases \n    base/stats \n    To select the complete cases, i.e., observations without missing values \n  \n\n D1 \n    mice \n    To conduct the multivariate Wald test with D1-statistic \n  \n\n densityplot \n    mice \n    To produce desnsity plots \n  \n\n expression \n    base \n    To set/create an expression \n  \n\n imputationList \n    mice \n    To combine multiple imputed datasets \n  \n\n marginplot \n    VIM \n    To draw a scatterplot with additional information when there are missing values \n  \n\n mcar_test \n    naniar \n    To conduct Little's MCAR test \n  \n\n md.pattern \n    mice \n    To see the pattern of the missing data \n  \n\n mice \n    mice \n    To impute missing data where the argument m represents the number of multiple imputation \n  \n\n MIcombine \n    mitools \n    To combine/pool the results using Rubin's rule \n  \n\n MIextract \n    mitools \n    To extract parameters from a list of outputs \n  \n\n na.test \n    misty \n    To conduct Little's MCAR test \n  \n\n parlmice \n    mice \n    To run `mice` function in parallel, i.e., parallel computing of mice \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n pool \n    mice \n    To pool the results using Rubin's rule \n  \n\n pool.compare \n    mice \n    To compare two nested models \n  \n\n pool_mi \n    miceadds \n    To combine/pool the results using Rubin's rule \n  \n\n quickpred \n    mice \n    To set imputation model based on the correlation \n  \n\n sim_slopes \n    interactions \n    To perform simple slope analyses \n  \n\n TestMCARNormality \n    MissMech \n    To test multivariate normality and homoscedasticity in the context of missing data \n  \n\n unlist \n    base \n    To convert a list to a vector"
  },
  {
    "objectID": "missingdataQ.html",
    "href": "missingdataQ.html",
    "title": "Quiz (M)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "missingdataE.html#problem-statement",
    "href": "missingdataE.html#problem-statement",
    "title": "Exercise (M)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n\nid: Respondent sequence number\nsurvey.weight: Full sample 4 year interview weight\npsu: Masked pseudo PSU\nstrata: Masked pseudo strata (strata is nested within PSU)\n\n4 Outcome variables\n\nweight.loss.behavior: doing lifestyle behavior changes - controlling or losing weight\nexercise.behavior: doing lifestyle behavior changes - increasing exercise\nsalt.behavior: doing lifestyle behavior changes - reducing salt in diet\nfat.behavior: doing lifestyle behavior changes - reducing fat in diet\n\n4 predictors (i.e., exposure variables)\n\nweight.loss.advice: told by a doctor or health professional - to control/lose weight\nexercise.advice: told by a doctor or health professional - to exercise\nsalt.advice: told by a doctor or health professional - to reduce salt in diet\nfat.advice: told by a doctor or health professional - to reduce fat/calories\n\nConfounders and other variables\n\ngender: Gender\nage: Age in years at screening\nincome: The ratio of family income to federal poverty level\nrace: Race/Ethnicity\nbmi: Body Mass Index in kg/m\\(^2\\)\n\ncomorbidity: Comorbidity index\nDIQ010: Self-report to have been informed by a provider to have diabetes\nBPQ020: Self-report to have been informed by a provider to have hypertension"
  },
  {
    "objectID": "missingdataE.html#question-1-analytic-dataset",
    "href": "missingdataE.html#question-1-analytic-dataset",
    "title": "Exercise (M)",
    "section": "Question 1: Analytic dataset",
    "text": "Question 1: Analytic dataset\n1(a) Importing dataset\n\nShow the code# download the data in the same folder\nload(\"Data/missingdata/Williams2021.RData\")\n\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\nShow the code# Drop < 18 years\ndat <- dat.full\ndat <- dat[dat$age >= 18,] \n\n# Eligibility\ndat <- dat[dat$DIQ010==\"Yes\" | dat$BPQ020==\"Yes\",] \n\n# Dataset with missing values in outcomes, predictors, and confounders\ndat.with.miss <- dat\nnrow(dat.with.miss) # N = 4,746\n#> [1] 4746\n\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the “Self-reported behavior change and receipt of advice” paragraph.\n\n\nShow the codedat <- dat.with.miss\n\n# Drop missing or don't know outcomes \ndat <- dat[complete.cases(dat$weight.loss.behavior),]\ndat <- dat[complete.cases(dat$exercise.behavior),]\ndat <- dat[complete.cases(dat$salt.behavior),]\ndat <- dat[complete.cases(dat$fat.behavior),]\n\n# Drop missing or don't know predictors\ndat <- dat[complete.cases(dat$weight.loss.advice),]\ndat <- dat[complete.cases(dat$exercise.advice),]\ndat <- dat[complete.cases(dat$salt.advice),]\ndat <- dat[complete.cases(dat$fat.advice),] \n\n# Dataset without missing in outcomes and predictors but missing in confounders \ndat.with.miss2 <- dat\nnrow(dat.with.miss2) # N = 4,716\n#> [1] 4716\n\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nHint 2: You may need to generate the Condition variable.\nHint 3: age and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\nShow the codedat <- dat.with.miss2\n\n# Create the condition variable\ndat$condition <- NA\ndat$condition[dat$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat$condition[dat$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat$condition[dat$BPQ020 == \"Yes\" & dat$DIQ010 == \"Yes\"] <- \"Both\"\ndat$condition <- factor(dat$condition, levels=c(\"Hypertension Only\", \"Diabetes Only\",\n                                                \"Both\"))\ntable(dat$condition, useNA = \"always\")\n#> \n#> Hypertension Only     Diabetes Only              Both              <NA> \n#>              3004               533              1179                 0\n\n\n\nShow the code# First column of Table 1\nvars <- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 <- CreateTableOne(vars = vars, data = dat, includeNA = F)\nprint(tab1, format = \"f\")\n#>                          \n#>                           Overall      \n#>   n                        4716        \n#>   gender = Male            2332        \n#>   age (mean (SD))         59.94 (14.96)\n#>   income                               \n#>      <100%                  881        \n#>      100-199%              1193        \n#>      200-299%               672        \n#>      300-399%               424        \n#>      400+%                  930        \n#>   race                                 \n#>      Hispanic              1161        \n#>      Non-Hispanic white    1630        \n#>      Non-Hispanic black    1239        \n#>      Others                 686        \n#>   bmi                                  \n#>      Reference              753        \n#>      Overweight            1372        \n#>      Obese                 2287        \n#>   condition                            \n#>      Hypertension Only     3004        \n#>      Diabetes Only          533        \n#>      Both                  1179        \n#>   comorbidity (mean (SD))  1.29 (1.45)"
  },
  {
    "objectID": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "href": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "title": "Exercise (M)",
    "section": "Question 2: Dealing with missing values in confoudners [100% grade]",
    "text": "Question 2: Dealing with missing values in confoudners [100% grade]\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nHint 1: There are four outcome variables and four predictor variables used in the study.\nHint 2: The authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\nShow the code# Create the condition variable in the analytic dataset\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat.with.miss2$condition[dat.with.miss2$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\" & \n                          dat.with.miss2$DIQ010 == \"Yes\"] <- \"Both\"\ndat.with.miss2$condition <- factor(dat.with.miss2$condition, \n                                  levels=c(\"Hypertension Only\", \"Diabetes Only\", \"Both\"))\n\n# Variables of interest\nvars <- c(\n  # Outcome\n  \"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\",\n  \n  #Predictors\n  \"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\",\n   \n  # Confounders       \n  \"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\n\n# Plot missing values using DataExplorer\nplot_missing(dat.with.miss2[,vars])\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nPerform multiple imputations to deal with missing values only in confounders. Use the dataset created in Dataset with missing values only in confounders (dat.with.miss2). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to lose weights, i.e., create only the first column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types. lapply function could be helpful.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 5: Set your seed to 123.\nHint 6: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 7: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 8: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\nShow the code## Setup the data such that the variables are of appropriate types\nfactor.names <- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \n                  \"fat.behavior\", \"weight.loss.advice\", \"exercise.advice\", \n                  \"salt.advice\", \"fat.advice\", \"gender\", \"income\", \"race\", \"bmi\", \n                  \"condition\")\n# your codes \n\n\n## Change the reference categories\n# your codes\n\n\n## Imputation model set up\n# your codes\n\n\n## Regression analysis\n# your codes\n\n\n## Pooled estimates\n# your codes"
  },
  {
    "objectID": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "href": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "title": "Exercise (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Include all 4 outcomes and 4 predictors in your imputation model.\nHint 5: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 6: Set your seed to 123.\nHint 7: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 8: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 9: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\nShow the code## Create a missing indicator so that MID can be applied\n# your codes here\n\n## MID\n# your codes here"
  },
  {
    "objectID": "propensityscore.html#background",
    "href": "propensityscore.html#background",
    "title": "Propensity score",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "propensityscore.html#overview-of-tutorials",
    "href": "propensityscore.html#overview-of-tutorials",
    "title": "Propensity score",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCovariate matching using CCHS: example of OA-CVD\nThe tutorial illustrate a comprehensive data analysis workflow using R, focusing on matching methods to estimate treatment effects with the CCHS data. Initially, we conduct data pre-processing steps to handle categorical variables and missing data. Subsequent sections delve into setting up design objects for survey-weighted analyses and conducting preliminary analyses to explore variable distributions and treatment effects. The core of the analysis involves implementing matching techniques, starting with a single variable and progressively including more variables to refine the matching. Various matching scenarios are explored, each followed by logistic regression models to estimate treatment effects.\n\n\nPropensity score matching using CCHS: revisiting example of OA-CVD\nThe tutorial provides a thorough walkthrough of implementing Propensity Score Matching (PSM) in R, specifically in the context of an OA - CVD health study from the CCHS. PSM is utilized to mitigate bias from confounding variables in observational studies by pairing treated and control units with analogous propensity scores. The guide underscores that PSM is iterative, often requiring refinement of the matching strategy to achieve satisfactory covariate balance in the matched sample. Various strategies for estimating treatment effects in the matched sample are explored, each with distinct assumptions and implications. The tutorial also delves into different matching strategies, such as nearest-neighbor matching with and without calipers, matching with different ratios, and matching with replacement, all while emphasizing the importance of assessing and re-assessing covariate balance at each step using both graphical and numerical methods.\n\n\nPropensity score matching using NHANES: example of OA - CVD\nThe provided text outlines methodologies for conducting PSM using the NHANES dataset, with a particular emphasis on handling survey design and weights in the analysis. Three distinct approaches, attributed to Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), are delineated, each with a structured four-step process: 1) specifying the propensity score model, 2) matching treated and untreated subjects based on estimated propensity scores, 3) comparing baseline characteristics between matched groups, and 4) estimating treatment effects using the matched sample. The procedures utilize various R packages and functions to manipulate data, visualize missing data patterns, format variables, and perform analyses, ensuring that survey weights and design are appropriately considered to avoid bias in population-level effect estimates. The text underscores the importance of incorporating survey design into at least propensity score outcome analysis (e.g., during step 4: treatment effect estimation), as neglecting survey weights can significantly impact the estimates of population-level effects.\n\n\nPropensity score matching using NHANES: example of BMI - diabetes\nThe tutorial provides a comprehensive guide on implementing PSM in R, utilizing the NHANES dataset, with a specific focus on diabetes as an outcome and body mass index (BMI) as an exposure variable. The methodology encompasses ensuring accurate and reproducible results in PSM. The tutorial, again, meticulously follows three distinct approaches for PSM, as recommended by Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), each providing a unique perspective on handling and analyzing variables within the propensity score model. Notably, the tutorial introduces a nuanced approach to variable handling, model specifications, and matching steps, ensuring a thorough understanding of implementing PSM with varied methodologies. Furthermore, the tutorial introduces a “double adjustment” step in each approach, providing a robust estimate of the treatment effect while adjusting for covariates, thereby offering readers a holistic view on conducting PSM with a different set of variables and methodologies in the analysis steps.\n\n\nPropensity score matching using NHANES when some variables have missing observations\nThis tutorial offers a clear and straightforward guide on how to use Propensity Score Matching (PSM) and Multiple Imputation (MI) in R, using the NHANES dataset for practical illustration. The main goal is to explore the relationship between “diabetes” (outcome) and being “born in the US” (exposure), while effectively managing missing data through MI. The first part of the tutorial, focusing on logistic regression, explains how to perform multiple imputations, fit a logistic regression model to all imputed datasets, and then obtain pooled Odds Ratios (OR) and 95% confidence intervals. Following this, the PSM analysis section carefully applies the PSM method, following Zanutto E. L. (2006), to all imputed datasets, and presents the pooled OR estimates and 95% confidence intervals. The tutorial emphasizes the crucial role of managing missing data through multiple imputation and provides a detailed, step-by-step guide, including code and thorough explanations, to ensure a deep understanding and ability to replicate the PSM with MI process in epidemiological research. This resource is invaluable for researchers and data analysts looking to strengthen their analyses when dealing with missing data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "propensityscore1.html",
    "href": "propensityscore1.html",
    "title": "Exact Matching (CCHS)",
    "section": "",
    "text": "In the following code chunk, we load the necessary R libraries for our analysis. MatchIt is used for matching methods to find comparable control units, tableone for creating Table 1 to describe baseline characteristics, Publish for generating readable output of regression analysis, and survey for analyzing complex survey samples.\n\nShow the code# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(Publish)\nrequire(survey)\n\n\nLoad data\nIn the following code chunk, we load the CCHS dataset which is related to the Canadian Community Health Survey (CCHS). We then use ls() to list all objects in the workspace and str to display the structure of the data frame, providing a quick overview of the data and checking for any character variables.\n\nShow the codeload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"\nstr(analytic.miss) # is there any character variable?\n#> 'data.frame':    397173 obs. of  22 variables:\n#>  $ CVD      : chr  \"event\" \"no event\" \"no event\" \"no event\" ...\n#>  $ age      : chr  \"65 years and over\" \"65 years and over\" \"30-39 years\" \"65 years and over\" ...\n#>  $ sex      : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n#>  $ married  : chr  \"single\" \"single\" \"not single\" \"single\" ...\n#>  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" ...\n#>  $ income   : chr  \"$29,999 or less\" \"$29,999 or less\" \"$80,000 or more\" \"$29,999 or less\" ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#>  $ phyact   : chr  \"Inactive\" \"Inactive\" \"Inactive\" \"Inactive\" ...\n#>  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n#>  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"stressed\" \"Not too stressed\" ...\n#>  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#>  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#>  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ weight   : num  142.8 71.4 168.3 71.4 196.1 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ OA       : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\n\nData pre-pocessing\nIn the following code chunk, we define a vector containing the names of variables of interest that needs to be converted to factor variables. We then convert these variables to factors, ensuring they are treated as categorical in subsequent analyses. We also recode the Osteoarthritis (OA) variable into a numeric binary format and display the frequency table of OA before and after the transformation.\n\nShow the codevar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"doctor\", \"drink\", \"bp\", \"province\",\n               \"immigrate\", \"fruit\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic.miss[var.names] <- lapply(analytic.miss[var.names] , factor)\ntable(analytic.miss$OA)\n#> \n#> Control      OA \n#>  314542   40943\nanalytic.miss$OA <- as.numeric(analytic.miss$OA==\"OA\") \ntable(analytic.miss$OA)\n#> \n#>      0      1 \n#> 314542  40943\n\n\nIdentify subjects with missing\nIn the following code chunk, we create a new variable miss and initially assign all its values to 1 in the full dataset (that contains some missing observations). We then adjust this assignment by setting miss to 0 for observations that are also present in another complete case dataset. That means any row with miss equal to 0 means that row has no missing observations. Finally, we display the frequency table of the miss variable to check the number of missing and non-missing observations.\n\nShow the codeanalytic.miss$miss <- 1\nhead(analytic.miss$ID) # full data\n#> [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#> [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#> [1]  3  5  7 10 11 13\n# if associated with complete case, assign miss <- 0\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] <- 0\ntable(analytic.miss$miss)\n#> \n#>      0      1 \n#> 185613 211560\n\n\nSetting Design\nUnconditional design\nIn the following code chunk, we explore the summary of the weight variable and establish an unconditional survey design object w.design0 using the svydesign function, which will be used for subsequent survey-weighted analyses. We then explore the summary, standard deviation, and sum of the weights within our design object.\n\nShow the codesummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   65.28  126.63  200.09  243.21 7154.95\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   65.28  126.63  200.09  243.21 7154.95\nsd(weights(w.design0))\n#> [1] 241.0279\nsum(weights(w.design0))\n#> [1] 79468929\n\n\nConditioning the design\nIn the following code chunk, we create a new survey design object w.design by subsetting w.design0 to only include observations without missing data (miss == 0). We then explore the summary, standard deviation, and sum of the weights within this new design object.\n\nShow the codew.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   71.56  137.95  214.61  261.91 7154.95\nsd(weights(w.design))\n#> [1] 254.9346\nsum(weights(w.design))\n#> [1] 39835061\n\n\nSubset data (more!)\nWe subset the data for fast results (less computation). We will only work with cycle 1.1, and the people from Northern provinces in Canada.\n\nShow the codew.design1 <- subset(w.design, cycle == 11 & province == \"North\")\nsum(weights(w.design1))\n#> [1] 42786.28\n\n\nPriliminary analysis\nTable 1\nIn the following code chunk, we define a new variable vector var.names and create a categorical table using svyCreateCatTable to explore the distribution of age and sex across strata of OA within our subsetted design object w.design1. We then print the table with standardized mean differences (SMD) to assess the balance of these variables across strata.\n\nShow the codevar.names <- c(\"age\", \"sex\")\ntab0 <- svyCreateCatTable(var = var.names, strata= \"OA\", data=w.design1,test=FALSE)\nprint(tab0, smd = TRUE)\n#>                       Stratified by OA\n#>                        0               1              SMD   \n#>   n                    40691.2         2095.1               \n#>   age (%)                                              1.084\n#>      20-29 years       10889.4 (26.8)   120.9 ( 5.8)        \n#>      30-39 years       12251.7 (30.1)   237.8 (11.3)        \n#>      40-49 years       11094.0 (27.3)   572.7 (27.3)        \n#>      50-59 years        5346.6 (13.1)  1092.4 (52.1)        \n#>      60-64 years        1109.4 ( 2.7)    71.4 ( 3.4)        \n#>      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#>      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#>   sex = Male (%)       20824.6 (51.2)  1050.8 (50.2)   0.020\n\n\nTreatment effect\nIn the following code chunk, we fit a logistic regression model using svyglm to estimate the effect of OA and other covariates on the binary outcome CVD (cardiovascular disease). We then use publish to display the results in a readable format.\n\nShow the codefit.outcome <- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design1,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#>   Variable             Units OddsRatio         CI.95    p-value \n#>         OA                        0.89   [0.17;4.59]   0.887411 \n#>        age       20-29 years       Ref                          \n#>                  30-39 years      2.62  [0.29;23.43]   0.389521 \n#>                  40-49 years      4.89  [0.59;40.73]   0.142280 \n#>                  50-59 years     17.95 [2.59;124.68]   0.003550 \n#>                  60-64 years     23.95 [3.41;168.27]   0.001439 \n#>        sex            Female       Ref                          \n#>                         Male      1.32   [0.64;2.71]   0.456222 \n#>     stress  Not too stressed       Ref                          \n#>                     stressed      0.54   [0.21;1.39]   0.198815 \n#>    married        not single       Ref                          \n#>                       single      0.75   [0.31;1.80]   0.513807 \n#>     income   $29,999 or less       Ref                          \n#>              $30,000-$49,999      0.72   [0.24;2.16]   0.556703 \n#>              $50,000-$79,999      0.95   [0.27;3.40]   0.939104 \n#>              $80,000 or more      0.47   [0.10;2.15]   0.332557 \n#>       race         Non-white       Ref                          \n#>                        White      0.33   [0.11;0.94]   0.038131 \n#>        bmi       Underweight       Ref                          \n#>               healthy weight      0.29   [0.03;3.20]   0.310237 \n#>                   Overweight      0.44   [0.04;4.77]   0.503130 \n#>     phyact            Active       Ref                          \n#>                     Inactive      0.84   [0.30;2.40]   0.751345 \n#>                     Moderate      1.02   [0.32;3.27]   0.979528 \n#>      smoke      Never smoker       Ref                          \n#>               Current smoker      0.98   [0.26;3.76]   0.981454 \n#>                Former smoker      0.71   [0.18;2.71]   0.612518 \n#>  immigrate     not immigrant       Ref                          \n#>                   > 10 years      0.14   [0.03;0.78]   0.025010 \n#>                       recent      0.00   [0.00;0.00]    < 1e-04 \n#>      fruit 0-3 daily serving       Ref                          \n#>            4-6 daily serving      1.15   [0.52;2.56]   0.725722 \n#>             6+ daily serving      0.68   [0.17;2.71]   0.583752 \n#>       diab                No       Ref                          \n#>                          Yes      3.08  [0.93;10.23]   0.066677 \n#>        edu          < 2ndary       Ref                          \n#>                    2nd grad.      4.12  [0.87;19.43]   0.074178 \n#>              Other 2nd grad.      3.04  [0.63;14.67]   0.167135 \n#>               Post-2nd grad.      3.00  [0.82;10.98]   0.096939\n\n\nMatching: Estimating treatment effect\nGoing back to the data (not working on design here while matching)\nIn the following code chunk, we create a new dataset by omitting NA values from analytic.miss and converting it to a data frame. We then create a subset analytic11n which includes only observations from cycle 1.1 and the Northern provinces. We display the dimensions of this subset, as well as frequency tables of OA and a cross-tabulation of OA and age to understand the distribution of our target variable and a key covariate.\n\nShow the code# Create the dataset without design features\nanalytic2 <- as.data.frame(na.omit(analytic.miss))\nanalytic11n <- subset(analytic2, cycle == 11 & province == \"North\")\ndim(analytic11n)\n#> [1] 1424   23\ntable(analytic11n$OA)\n#> \n#>    0    1 \n#> 1357   67\ntable(analytic11n$OA,analytic11n$age)\n#>    \n#>     20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   0         345         432         358         177          45\n#>   1           4          11          18          31           3\n#>    \n#>     65 years and over teen\n#>   0                 0    0\n#>   1                 0    0\n\n\nMatching by 1 matching variable\nIn the following code chunk, we perform exact matching using a single variable, age. We define the matching formula and apply the matchit function to create matched sets of treated and control units. The resulting matching.obj object is displayed to summarize the matching results.\n\nShow the codematch.formula <- as.formula(\"OA ~ age\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1424 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age\n\n\nMatching by 2 matching variables\nIn the following code chunk, we extend the matching to include two variables, age and sex. We create a new variable var.comb that concatenates these two variables and display its frequency table and the number of unique combinations. We then perform exact matching using both variables and display the resulting object.\n\nShow the codevar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex')])\ntable(var.comb)\n#> var.comb\n#> 20-29 yearsFemale   20-29 yearsMale 30-39 yearsFemale   30-39 yearsMale \n#>               184               165               220               223 \n#> 40-49 yearsFemale   40-49 yearsMale 50-59 yearsFemale   50-59 yearsMale \n#>               187               189               101               107 \n#> 60-64 yearsFemale   60-64 yearsMale \n#>                24                24\nlength(table(var.comb))\n#> [1] 10\nmatch.formula <- as.formula(\"OA ~ age + sex\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1424 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex\n\n\nMatching by 3 matching variables\nIn the following code chunk, we further extend the matching to include three variables: age, sex, and stress. We explore the unique combinations of these variables and their distribution across levels of OA. We then perform exact matching using these three variables and display the resulting object.\n\nShow the codevar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex', 'stress')])\ntable(var.comb)\n#> var.comb\n#> 20-29 yearsFemaleNot too stressed         20-29 yearsFemalestressed \n#>                               157                                27 \n#>   20-29 yearsMaleNot too stressed           20-29 yearsMalestressed \n#>                               147                                18 \n#> 30-39 yearsFemaleNot too stressed         30-39 yearsFemalestressed \n#>                               170                                50 \n#>   30-39 yearsMaleNot too stressed           30-39 yearsMalestressed \n#>                               183                                40 \n#> 40-49 yearsFemaleNot too stressed         40-49 yearsFemalestressed \n#>                               142                                45 \n#>   40-49 yearsMaleNot too stressed           40-49 yearsMalestressed \n#>                               141                                48 \n#> 50-59 yearsFemaleNot too stressed         50-59 yearsFemalestressed \n#>                                72                                29 \n#>   50-59 yearsMaleNot too stressed           50-59 yearsMalestressed \n#>                                78                                29 \n#> 60-64 yearsFemaleNot too stressed         60-64 yearsFemalestressed \n#>                                18                                 6 \n#>   60-64 yearsMaleNot too stressed           60-64 yearsMalestressed \n#>                                20                                 4\nlength(table(var.comb))\n#> [1] 20\ntable(var.comb,analytic11n$OA)\n#>                                    \n#> var.comb                              0   1\n#>   20-29 yearsFemaleNot too stressed 156   1\n#>   20-29 yearsFemalestressed          27   0\n#>   20-29 yearsMaleNot too stressed   144   3\n#>   20-29 yearsMalestressed            18   0\n#>   30-39 yearsFemaleNot too stressed 168   2\n#>   30-39 yearsFemalestressed          49   1\n#>   30-39 yearsMaleNot too stressed   178   5\n#>   30-39 yearsMalestressed            37   3\n#>   40-49 yearsFemaleNot too stressed 130  12\n#>   40-49 yearsFemalestressed          42   3\n#>   40-49 yearsMaleNot too stressed   138   3\n#>   40-49 yearsMalestressed            48   0\n#>   50-59 yearsFemaleNot too stressed  65   7\n#>   50-59 yearsFemalestressed          22   7\n#>   50-59 yearsMaleNot too stressed    67  11\n#>   50-59 yearsMalestressed            23   6\n#>   60-64 yearsFemaleNot too stressed  17   1\n#>   60-64 yearsFemalestressed           5   1\n#>   60-64 yearsMaleNot too stressed    19   1\n#>   60-64 yearsMalestressed             4   0\nmatch.formula <- as.formula(\"OA ~ age + sex + stress\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 1327 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress\n\n\nMatching by 4 matching variables\nThe process of matching by 4 variables involves creating combinations of the 4 variables, exploring their distributions, and performing exact matching.\n\nShow the codevar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income')])\n#table(var.comb)\nlength(table(var.comb))\n#> [1] 76\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 900 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income\n\n\nMatching by 5 matching variables\n\nShow the codevar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race')])\nlength(table(var.comb))\n#> [1] 146\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income + race\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 616 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income, race\n\n\nMatching by 6 matching variables\n\nShow the codevar.comb <- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race','edu')])\nlength(table(var.comb))\n#> [1] 354\nmatch.formula <- as.formula(\"OA ~ age + sex + stress + income + race + edu\")\nmatching.obj <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 399 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, income, race, edu\nOACVD.match.11n <- match.data(matching.obj)\nvar.names <- c(\"age\", \"sex\", \"stress\", \"income\", \"race\", \"edu\")\ntab1 <- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n,test=FALSE)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     337         62               \n#>   age (%)                                       0.565\n#>      20-29 years         63 (18.7)   4 ( 6.5)        \n#>      30-39 years         61 (18.1)  11 (17.7)        \n#>      40-49 years        127 (37.7)  17 (27.4)        \n#>      50-59 years         82 (24.3)  28 (45.2)        \n#>      60-64 years          4 ( 1.2)   2 ( 3.2)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        211 (62.6)  29 (46.8)   0.322\n#>   stress = stressed (%)  42 (12.5)  17 (27.4)   0.381\n#>   income (%)                                    0.115\n#>      $29,999 or less     69 (20.5)  11 (17.7)        \n#>      $30,000-$49,999     57 (16.9)  13 (21.0)        \n#>      $50,000-$79,999     69 (20.5)  12 (19.4)        \n#>      $80,000 or more    142 (42.1)  26 (41.9)        \n#>   race = White (%)      242 (71.8)  43 (69.4)   0.054\n#>   edu (%)                                       0.146\n#>      < 2ndary            73 (21.7)  11 (17.7)        \n#>      2nd grad.            5 ( 1.5)   2 ( 3.2)        \n#>      Other 2nd grad.      0 ( 0.0)   0 ( 0.0)        \n#>      Post-2nd grad.     259 (76.9)  49 (79.0)\n\n\nTreatment effect\nConvert data to design\nIn the following code chunk, we create a new variable matched in the analytic.miss dataset to indicate whether an observation was included in the matched dataset OACVD.match.11n. We then create a new survey design object w.design.m that includes only the matched observations for subsequent analyses.\n\nShow the codeanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match.11n$ID) # matched data\n#> [1] 399\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n$ID])\n#> [1] 399\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match.11n$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396774    399\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nOutcome analysis\nThe subsequent code chunks involve fitting logistic regression models to estimate the treatment effect, both in a crude and adjusted manner, respectively. The models are fitted using the matched survey design object and the results are displayed in a readable format.\nCrude\n\nShow the codefit.outcome <- svyglm(I(CVD==\"event\") ~ OA,\n                   design = w.design.m,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#>  Variable Units OddsRatio        CI.95  p-value \n#>        OA            3.14 [0.80;12.40]   0.1025\n\n\nAdjusted\n\nShow the codefit.outcome <- svyglm(I(CVD==\"event\") ~ OA + \n                        age + sex + stress + income + race + edu,\n                   design = w.design.m,\n                   family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\npublish(fit.outcome)\n#>  Variable            Units    OddsRatio                        CI.95   p-value \n#>        OA                          2.04                 [0.34;12.16]   0.43593 \n#>       age      20-29 years          Ref                                        \n#>                30-39 years         0.54                  [0.08;3.51]   0.51962 \n#>                40-49 years  30148597.85    [7758796.44;117149349.12]   < 1e-04 \n#>                50-59 years  63290825.96   [12589873.89;318170673.23]   < 1e-04 \n#>                60-64 years         0.31                  [0.01;9.33]   0.49735 \n#>       sex           Female          Ref                                        \n#>                       Male         1.58                  [0.29;8.62]   0.59729 \n#>    stress Not too stressed          Ref                                        \n#>                   stressed         0.15                  [0.01;1.80]   0.13666 \n#>    income  $29,999 or less          Ref                                        \n#>            $30,000-$49,999         0.20                  [0.01;3.84]   0.28640 \n#>            $50,000-$79,999         0.20                  [0.02;1.95]   0.16543 \n#>            $80,000 or more         0.08                  [0.01;0.68]   0.02122 \n#>      race        Non-white          Ref                                        \n#>                      White         1.02                  [0.11;9.45]   0.98723 \n#>       edu         < 2ndary          Ref                                        \n#>                  2nd grad. 845233466.89 [44642865.50;16002996347.89]   < 1e-04 \n#>             Post-2nd grad.  69867459.42    [9660579.88;505296985.12]   < 1e-04\n\n\nQuestions for the students\n\nLook at all the ORs. Some of them are VERY high. Why?\nLook at the CI in the above table. Some of them are Inf. Why?\nShould we match matching variables in the regression?\nMatching by a lot of variables\nThe code chunks involve performing matching using a large number of variables and estimating the treatment effect using the matched data. The process involves creating matched datasets, converting them to survey design objects, and fitting logistic regression models.\nMatching part in data\n\nShow the codematch.formula <- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu\")\nmatching.obj2 <- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj2\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 1424 (original), 22 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, immigrate, fruit, diab, edu\nOACVD.match.11n2 <- match.data(matching.obj2)\nvar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"immigrate\", \"fruit\", \"diab\", \"edu\")\ntab2 <- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n2,test=FALSE)\nprint(tab2, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     11         11               \n#>   age (%)                                     <0.001\n#>      20-29 years         3 (27.3)   3 (27.3)        \n#>      30-39 years         1 ( 9.1)   1 ( 9.1)        \n#>      40-49 years         4 (36.4)   4 (36.4)        \n#>      50-59 years         3 (27.3)   3 (27.3)        \n#>      60-64 years         0 ( 0.0)   0 ( 0.0)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)         6 (54.5)   6 (54.5)  <0.001\n#>   stress = stressed (%)  1 ( 9.1)   1 ( 9.1)  <0.001\n#>   married = single (%)   3 (27.3)   3 (27.3)  <0.001\n#>   income (%)                                  <0.001\n#>      $29,999 or less     1 ( 9.1)   1 ( 9.1)        \n#>      $30,000-$49,999     2 (18.2)   2 (18.2)        \n#>      $50,000-$79,999     2 (18.2)   2 (18.2)        \n#>      $80,000 or more     6 (54.5)   6 (54.5)        \n#>   race = White (%)      10 (90.9)  10 (90.9)  <0.001\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      4 (36.4)   4 (36.4)        \n#>      Overweight          7 (63.6)   7 (63.6)        \n#>   phyact (%)                                  <0.001\n#>      Active              3 (27.3)   3 (27.3)        \n#>      Inactive            5 (45.5)   5 (45.5)        \n#>      Moderate            3 (27.3)   3 (27.3)        \n#>   smoke (%)                                   <0.001\n#>      Never smoker        3 (27.3)   3 (27.3)        \n#>      Current smoker      2 (18.2)   2 (18.2)        \n#>      Former smoker       6 (54.5)   6 (54.5)        \n#>   immigrate (%)                               <0.001\n#>      not immigrant      10 (90.9)  10 (90.9)        \n#>      > 10 years          1 ( 9.1)   1 ( 9.1)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                   <0.001\n#>      0-3 daily serving   3 (27.3)   3 (27.3)        \n#>      4-6 daily serving   6 (54.5)   6 (54.5)        \n#>      6+ daily serving    2 (18.2)   2 (18.2)        \n#>   diab = Yes (%)         0 ( 0.0)   0 ( 0.0)  <0.001\n#>   edu (%)                                     <0.001\n#>      < 2ndary            1 ( 9.1)   1 ( 9.1)        \n#>      2nd grad.           0 ( 0.0)   0 ( 0.0)        \n#>      Other 2nd grad.     0 ( 0.0)   0 ( 0.0)        \n#>      Post-2nd grad.     10 (90.9)  10 (90.9)\n\n\nTreatment effect estimation in design\nCreate design\n\nShow the codeanalytic.miss$matched2 <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match.11n2$ID) # matched data\n#> [1] 22\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n2$ID])\n#> [1] 22\nanalytic.miss$matched2[analytic.miss$ID %in% OACVD.match.11n2$ID] <- 1\ntable(analytic.miss$matched2)\n#> \n#>      0      1 \n#> 397151     22\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m2 <- subset(w.design0, matched2 == 1)\n\n\noutcome analysis\n\nShow the codefit.outcome <- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married +\n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design.m2,\n                   family = binomial(logit))\npublish(fit.outcome)\n# Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : \n#   contrasts can be applied only to factors with 2 or more levels\n\n\nQuestions for the students\n\nWhy the above model not fitting?\nSave data for later use\n\nShow the codesave(analytic11n, analytic2, analytic.miss, file=\"Data/propensityscore/cchs123c.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "propensityscore2.html",
    "href": "propensityscore2.html",
    "title": "PSM in OA-CVD (CCHS)",
    "section": "",
    "text": "This tutorial is a comprehensive guide on implementing Propensity Score Matching (PSM) using R, particularly focusing on a OA - CVD health study from the Canadian Community Health Survey (CCHS). This PSM method is used to reduce bias due to confounding variables in observational studies by matching treated and control units with similar propensity scores. The tutorial illustrates that PSM is an iterative process, where researchers may need to refine their matching strategy to achieve satisfactory balance in the matched sample. Different strategies for estimating the treatment effect in the matched sample are explored, each with its own assumptions and implications.\nLoad packages\nAt first, various R packages are loaded to utilize their functions for data manipulation, statistical analysis, and visualization.\n\nShow the code# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\n\n\nLoad data\nThe dataset is loaded into the R environment. Variables are renamed to avoid conflicts in subsequent analyses.\n\nShow the codeload(file=\"Data/propensityscore/cchs123c.RData\")\nhead(analytic11n)\n\n\n\n  \n\n\nShow the code\n# later we will create another variable called weights\n# hence to avoid any conflict/ambiguity,\n# renaming weight variable to survey.weight\nanalytic.miss$survey.weight <- analytic.miss$weight\nanalytic11n$survey.weight <- analytic11n$weight\nanalytic.miss$weight <- analytic11n$weight <- NULL\n\n\nAnalysis\nWe are going to apply propensity score analysis (Matching) in our OA - CVD problem from CCHS. For computation considerations, we will only work with cycle 1.1, and the people from Northern provinces in Canada (analytic11n data).\nStep 1\nSpecify PS\nA logistic regression model formula is specified to calculate the propensity scores (PS), which is the probability of receiving the treatment given the observed covariates.\n\nShow the codeps.formula <- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                        doctor + drink + bp + \n                         immigrate + fruit + diab + edu\")\nvar.names <- c(\"age\", \"sex\", \"stress\", \"married\", \n               \"income\", \"race\", \"bmi\", \"phyact\", \"smoke\", \n               \"doctor\", \"drink\", \"bp\", \n               \"immigrate\", \"fruit\", \"diab\", \"edu\")\n\n\nFit model\nThe software fits the PS model using a logistic regression by default. This package actually performs step 1 and 2 with one command matchit.\nLook at the website for arguments of matchit (RDocumentation 2023)]. It looks like this\n\nShow the codematchit(formula, data, model=\"logit\", discard=0, reestimate=FALSE, nearest=TRUE,\n                 replace=FALSE, m.order=2, ratio=1, caliper=0, calclosest=FALSE,\n                 subclass=0, sub.by=\"treat\", mahvars=NULL, exact=FALSE, counter=TRUE, full=FALSE, full.options=list(),...)\n\n\n\n\n\n\n\n\nTip\n\n\n\nNearest-Neighbor Matching:\nNearest-neighbor matching is a widely used technique in PSM to pair treated and control units based on the proximity of their propensity scores. It is straightforward and computationally efficient, making it a popular choice in many applications of PSM. Nearest-neighbor matching is often termed a “greedy” algorithm because it matches units in order, without considering the global distribution of propensity scores. Once a match is made, it is not revisited, even if a later unit would have been a better match. The method seeks to minimize bias by creating closely matched pairs but can increase variance if the pool of potential matches is reduced too much (e.g., using a very narrow caliper). It is essential to ensure that there is a common support region where the distributions of propensity scores for treated and control units overlap, ensuring comparability.\n\n\nStep 2\nMatch subjects by PS\nWe are going to match using a Nearest neighbor algorithm. This is a greedy matching algorithm. Note that we are not even defining any caliper.\n\n\n\n\n\n\nTip\n\n\n\nCaliper:\nIn the context of PSM, a caliper is a predefined maximum allowable difference between the propensity scores of matched units. Essentially, it sets a threshold for how dissimilar matched units can be in terms of their propensity scores. When a caliper is used, a treated unit is only matched with a control unit if the absolute difference in their propensity scores is less than or equal to the specified caliper width. The caliper is used to avoid bad matches and thereby minimize bias in the estimated treatment effect. The size of the caliper is crucial. Too wide a caliper may allow poor matches, while too narrow a caliper may result in many units going unmatched. Implementing a caliper involves a trade-off between bias and efficiency. Using a caliper reduces bias by avoiding poor matches but may increase variance by reducing the number of matched pairs available for analysis. Therefore, the use of a caliper in PSM is a strategic decision to enhance the quality of matches and thereby improve the validity of causal inferences drawn from observational data. It is a practical tool to ensure that matched units are sufficiently similar in terms of their propensity scores, reducing the likelihood of bias due to poor matches.\n\n\n\nShow the code# set seed\nset.seed(123)\n# match\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 1424 (original), 134 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\n# create the \"matched\"\" data\nOACVD.match <- match.data(matching.obj)\n# see the dimension\ndim(analytic11n)\n#> [1] 1424   23\ndim(OACVD.match)\n#> [1] 134  26\n\n\nLet’s try to understand how this is working.\nExtract matched IDs\n\nShow the codem.mat<-matching.obj$match.matrix\nhead(m.mat)\n#>       [,1]    \n#> 17864 \"96719\" \n#> 17921 \"17846\" \n#> 18191 \"97168\" \n#> 18256 \"111999\"\n#> 18264 \"17989\" \n#> 18383 \"108197\"\n\n\nExtract the matched treated IDs\n\nShow the codetreated.id<-as.numeric(row.names(m.mat))\ntreated.id # basically row names\n#>  [1]  17864  17921  18191  18256  18264  18383  18389  18475  39105  96344\n#> [11]  96364  96407  96424  96460  96484  96571  96582  96625  96632  96641\n#> [21]  96657  96686  96693  96696  96705  96734  96795  96840  96913  97027\n#> [31]  97065  97125 108178 108183 108185 108192 111809 111813 111856 111859\n#> [41] 111895 111896 111920 111942 112014 112026 112046 112083 112086 112114\n#> [51] 112122 112151 112167 112189 112197 112215 112232 112245 112275 112284\n#> [61] 112289 112290 112300 112325 112375 126477 126522\n\n\nExtract the matched untreated IDs\n\nShow the codeuntreated.id <- as.numeric(m.mat)\nuntreated.id # basically row names\n#>  [1]  96719  17846  97168 111999  17989 108197 112384  17909 126561 111880\n#> [11] 112184  18117  96865  18120  97023 112379  97017 126562  96356 126470\n#> [21] 126385  96374  18203  18262  96972 111924  96354  96983  18235  96882\n#> [31] 112054  18321 112349  18426  38996 126516 111814 112087  96569 111932\n#> [41] 126539  18315  96665  18225 112052 112324 112165  18329  96609 126376\n#> [51]  96474 126570 126547 126343  96680  96558 111931  96718  96533 111823\n#> [61] 112177  17953  17904 111908 111962  96644  96576\n\n\nExtract the matched treated data\n\nShow the codetx <- analytic11n[rownames(analytic11n) %in% treated.id,]\nhead(tx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n\n  \n\n\n\nExtract the matched untreated data\n\nShow the codeutx <- analytic11n[rownames(analytic11n) %in% untreated.id,]\nhead(utx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n\n  \n\n\n\nExtract the matched data altogether\nSimply using match.data is enough (as done earlier).\n\nShow the codeOACVD.match <- match.data(matching.obj)\n\n\nAssign match ID\n\nShow the codeOACVD.match$match.ID <- NA\nOACVD.match$match.ID[rownames(OACVD.match) %in% treated.id] <- 1:length(treated.id)\nOACVD.match$match.ID[rownames(OACVD.match) %in% untreated.id] <- 1:length(untreated.id)\ntable(OACVD.match$match.ID)\n#> \n#>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#> 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#> 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n#>  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n\n\nTake a look at individual matches for the first match\n\nShow the codena.omit(OACVD.match[OACVD.match$match.ID == 1,])\n\n\n\n  \n\n\n\nTake a look at individual matches for the second match\n\nShow the codena.omit(OACVD.match[OACVD.match$match.ID == 2,])\n\n\n\n  \n\n\n\nStep 3\nBoth graphical and numerical methods are used to assess the quality of the matches and the balance of covariates in the matched sample.\nExamining PS graphically\nVisually inspect the PS and assess the balance of covariates in the matched sample. Various plots are generated to visualize the distribution of PS across treatment groups and to check the balance of covariates before and after matching.\nmatchit package\n\nShow the code# plot(matching.obj) # covariate balance\nplot(matching.obj, type = \"jitter\") # propensity score locations\n\n\n\n#> To identify the units, use first mouse button; to stop, use second.\nplot(matching.obj, type = \"hist\") #check matched treated vs matched control\n\n\n\nShow the codesummrize.output <- summary(matching.obj, standardize = TRUE)\nplot(summrize.output)\n\n\n\n\nOveralp check\n\nShow the code# plot propensity scores by exposure group\nplot(density(OACVD.match$distance[OACVD.match$OA==1]), \n     col = \"red\", main = \"\")\nlines(density(OACVD.match$distance[OACVD.match$OA==0]), \n      col = \"blue\", lty = 2)\nlegend(\"topright\", c(\"Non-arthritis\",\"OA\"), \n       col = c(\"red\", \"blue\"), lty=1:2)\n\n\n\n\ncobalt package\nOverlap check in a more convenient way\n\nShow the code# different badwidth\nbal.plot(matching.obj, var.name = \"distance\")\n\n\n\n\nLook at the data\n\nShow the code# what is distance variable here?\nhead(OACVD.match)\n\n\n\n  \n\n\n\nNumerical values of PS\n\nShow the codesummary(OACVD.match$distance)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.044834 0.099094 0.138969 0.200576 0.611166\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.047344 0.099215 0.135042 0.199279 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.047346 0.098973 0.142897 0.198741 0.611166\n\n\nQuestion for the students\n\nAre you happy with the matching after reviewing the plots?\nCovariate balance in matched sample\nCovariate balance is assessed numerically using standardized mean differences (SMD).\n\n\n\n\n\n\nTip\n\n\n\nStandardized mean differences: SMD is a versatile and widely used statistical measure that facilitates the comparison of groups in research by providing a scale-free metric of difference and balance. In the context of propensity score matching, achieving low SMD values for covariates after matching is crucial to ensuring the validity of causal inferences drawn from the matched sample.\nBenifits:\n\nSMD is not influenced by the scale of the measured variable, making it suitable for comparing the balance of different variables measured on different scales.\nUnlike hypothesis testing, SMD is not affected by sample size, making it a reliable measure for assessing balance in matched samples.\n\n\n\n\nShow the codetab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     67         67               \n#>   age (%)                                      0.190\n#>      20-29 years         4 ( 6.0)   4 ( 6.0)        \n#>      30-39 years        16 (23.9)  11 (16.4)        \n#>      40-49 years        16 (23.9)  18 (26.9)        \n#>      50-59 years        28 (41.8)  31 (46.3)        \n#>      60-64 years         3 ( 4.5)   3 ( 4.5)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        28 (41.8)  32 (47.8)   0.120\n#>   stress = stressed (%) 20 (29.9)  21 (31.3)   0.032\n#>   married = single (%)  22 (32.8)  23 (34.3)   0.032\n#>   income (%)                                   0.183\n#>      $29,999 or less    11 (16.4)  13 (19.4)        \n#>      $30,000-$49,999    19 (28.4)  15 (22.4)        \n#>      $50,000-$79,999    14 (20.9)  12 (17.9)        \n#>      $80,000 or more    23 (34.3)  27 (40.3)        \n#>   race = White (%)      43 (64.2)  44 (65.7)   0.031\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight     18 (26.9)  18 (26.9)        \n#>      Overweight         49 (73.1)  49 (73.1)        \n#>   phyact (%)                                   0.041\n#>      Active             16 (23.9)  16 (23.9)        \n#>      Inactive           40 (59.7)  39 (58.2)        \n#>      Moderate           11 (16.4)  12 (17.9)        \n#>   smoke (%)                                    0.258\n#>      Never smoker       14 (20.9)   9 (13.4)        \n#>      Current smoker     20 (29.9)  27 (40.3)        \n#>      Former smoker      33 (49.3)  31 (46.3)        \n#>   doctor = Yes (%)      51 (76.1)  47 (70.1)   0.135\n#>   drink (%)                                    0.116\n#>      Never drank         2 ( 3.0)   2 ( 3.0)        \n#>      Current drinker    54 (80.6)  51 (76.1)        \n#>      Former driker      11 (16.4)  14 (20.9)        \n#>   bp = Yes (%)           7 (10.4)   5 ( 7.5)   0.105\n#>   immigrate (%)                                0.180\n#>      not immigrant      64 (95.5)  61 (91.0)        \n#>      > 10 years          3 ( 4.5)   6 ( 9.0)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                    0.146\n#>      0-3 daily serving  19 (28.4)  19 (28.4)        \n#>      4-6 daily serving  28 (41.8)  32 (47.8)        \n#>      6+ daily serving   20 (29.9)  16 (23.9)        \n#>   diab = Yes (%)         1 ( 1.5)   4 ( 6.0)   0.238\n#>   edu (%)                                      0.105\n#>      < 2ndary           14 (20.9)  13 (19.4)        \n#>      2nd grad.           1 ( 1.5)   2 ( 3.0)        \n#>      Other 2nd grad.     1 ( 1.5)   1 ( 1.5)        \n#>      Post-2nd grad.     51 (76.1)  51 (76.1)\n\n\nQuestion for the students\n\nAll SMD < 0.20?\nOther balance measures\nIndividual categories\nIf you want to check balance at each category (not very useful in general situations). We are generally interested if the variables are balanced or not (not categories).\n\nShow the codebaltab <- bal.tab(matching.obj)\nbaltab\n#> Balance Measures\n#>                             Type Diff.Adj\n#> distance                Distance   0.0597\n#> age_20-29 years           Binary   0.0000\n#> age_30-39 years           Binary  -0.0746\n#> age_40-49 years           Binary   0.0299\n#> age_50-59 years           Binary   0.0448\n#> age_60-64 years           Binary   0.0000\n#> sex_Male                  Binary   0.0597\n#> stress_stressed           Binary   0.0149\n#> married_single            Binary   0.0149\n#> income_$29,999 or less    Binary   0.0299\n#> income_$30,000-$49,999    Binary  -0.0597\n#> income_$50,000-$79,999    Binary  -0.0299\n#> income_$80,000 or more    Binary   0.0597\n#> race_White                Binary   0.0149\n#> bmi_Underweight           Binary   0.0000\n#> bmi_healthy weight        Binary   0.0000\n#> bmi_Overweight            Binary   0.0000\n#> phyact_Active             Binary   0.0000\n#> phyact_Inactive           Binary  -0.0149\n#> phyact_Moderate           Binary   0.0149\n#> smoke_Never smoker        Binary  -0.0746\n#> smoke_Current smoker      Binary   0.1045\n#> smoke_Former smoker       Binary  -0.0299\n#> doctor_Yes                Binary  -0.0597\n#> drink_Never drank         Binary   0.0000\n#> drink_Current drinker     Binary  -0.0448\n#> drink_Former driker       Binary   0.0448\n#> bp_Yes                    Binary  -0.0299\n#> immigrate_not immigrant   Binary  -0.0448\n#> immigrate_> 10 years      Binary   0.0448\n#> immigrate_recent          Binary   0.0000\n#> fruit_0-3 daily serving   Binary   0.0000\n#> fruit_4-6 daily serving   Binary   0.0597\n#> fruit_6+ daily serving    Binary  -0.0597\n#> diab_Yes                  Binary   0.0448\n#> edu_< 2ndary              Binary  -0.0149\n#> edu_2nd grad.             Binary   0.0149\n#> edu_Other 2nd grad.       Binary   0.0000\n#> edu_Post-2nd grad.        Binary   0.0000\n#> \n#> Sample sizes\n#>           Control Treated\n#> All          1357      67\n#> Matched        67      67\n#> Unmatched    1290       0\n\n\nIndividual plots\nYou could plot each variables individually\n\nShow the codebal.plot(matching.obj, var.name = \"income\")\n\n\n\nShow the codebal.plot(matching.obj, var.name = \"age\")\n\n\n\nShow the codebal.plot(matching.obj, var.name = \"race\")\n\n\n\nShow the codebal.plot(matching.obj, var.name = \"diab\")\n\n\n\nShow the codebal.plot(matching.obj, var.name = \"immigrate\")\n\n\n\n\nLove plot\n\nShow the code# Individual categories again\nlove.plot(baltab, threshold = .2)\n#> Warning: Unadjusted values are missing. This can occur when `un = FALSE` and\n#> `quick = TRUE` in the original call to `bal.tab()`.\n#> Warning: Standardized mean differences and raw mean differences are present in the same plot. \n#> Use the `stars` argument to distinguish between them and appropriately label the x-axis.\n\n\n\n\nRepeat of Step 1-3 again\nCovariate balance is reassessed in each step to ensure the quality of the match.\nAdd caliper\nThe matching process is repeated, this time introducing a caliper to ensure that matches are only made within a specified range of PS.\n\nShow the codelogitPS <-  -log(1/OACVD.match$distance - 1) \n# logit of the propensity score\n.2*sd(logitPS) # suggested in the literature\n#> [1] 0.2334615\n\n\n# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1,\n                        caliper = .2*sd(logitPS))\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.015)\n#>  - number of obs.: 1424 (original), 128 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n\n\n\nShow the code# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041740 0.094963 0.124665 0.184103 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0          1          SMD   \n#>   n                     64         64               \n#>   age (%)                                      0.196\n#>      20-29 years         4 ( 6.2)   4 ( 6.2)        \n#>      30-39 years        16 (25.0)  11 (17.2)        \n#>      40-49 years        16 (25.0)  18 (28.1)        \n#>      50-59 years        25 (39.1)  28 (43.8)        \n#>      60-64 years         3 ( 4.7)   3 ( 4.7)        \n#>      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#>      teen                0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        27 (42.2)  30 (46.9)   0.094\n#>   stress = stressed (%) 18 (28.1)  18 (28.1)  <0.001\n#>   married = single (%)  21 (32.8)  23 (35.9)   0.066\n#>   income (%)                                   0.204\n#>      $29,999 or less    11 (17.2)  13 (20.3)        \n#>      $30,000-$49,999    19 (29.7)  14 (21.9)        \n#>      $50,000-$79,999    13 (20.3)  12 (18.8)        \n#>      $80,000 or more    21 (32.8)  25 (39.1)        \n#>   race = White (%)      40 (62.5)  42 (65.6)   0.065\n#>   bmi (%)                                     <0.001\n#>      Underweight         0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight     18 (28.1)  18 (28.1)        \n#>      Overweight         46 (71.9)  46 (71.9)        \n#>   phyact (%)                                   0.096\n#>      Active             14 (21.9)  16 (25.0)        \n#>      Inactive           39 (60.9)  36 (56.2)        \n#>      Moderate           11 (17.2)  12 (18.8)        \n#>   smoke (%)                                    0.267\n#>      Never smoker       14 (21.9)   9 (14.1)        \n#>      Current smoker     19 (29.7)  26 (40.6)        \n#>      Former smoker      31 (48.4)  29 (45.3)        \n#>   doctor = Yes (%)      48 (75.0)  44 (68.8)   0.139\n#>   drink (%)                                    0.123\n#>      Never drank         2 ( 3.1)   2 ( 3.1)        \n#>      Current drinker    52 (81.2)  49 (76.6)        \n#>      Former driker      10 (15.6)  13 (20.3)        \n#>   bp = Yes (%)           7 (10.9)   5 ( 7.8)   0.107\n#>   immigrate (%)                                0.260\n#>      not immigrant      62 (96.9)  58 (90.6)        \n#>      > 10 years          2 ( 3.1)   6 ( 9.4)        \n#>      recent              0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                    0.116\n#>      0-3 daily serving  19 (29.7)  19 (29.7)        \n#>      4-6 daily serving  27 (42.2)  30 (46.9)        \n#>      6+ daily serving   18 (28.1)  15 (23.4)        \n#>   diab = Yes (%)         0 ( 0.0)   3 ( 4.7)   0.314\n#>   edu (%)                                      0.108\n#>      < 2ndary           14 (21.9)  13 (20.3)        \n#>      2nd grad.           1 ( 1.6)   2 ( 3.1)        \n#>      Other 2nd grad.     1 ( 1.6)   1 ( 1.6)        \n#>      Post-2nd grad.     48 (75.0)  48 (75.0)\n\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\nShow the code# what is weights variable for pair matching?\nhead(OACVD.match)\n\n\n\n  \n\n\nShow the codesummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       1       1       1       1       1       1\n\n\nStep 4\nEstimate treatment effect for matched data\nDifferent models (e.g., unconditional logistic regression, survey design) are fitted to estimate the treatment effect in the matched sample.\nUnconditional logistic\n\nShow the code# Wrong model for population!!\noutcome.model <- glm(CVD ~ OA, data = OACVD.match, family = binomial())\npublish(outcome.model)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.48 [0.09;2.74]   0.4119\n\n\nSurvey design\nConvert data to design\nThe matched data is converted to a survey design object to account for the matched pairs in the analysis.\n\nShow the codeanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 128\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 128\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 397045    128\nw.design0 <- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\nBalance in matched population?\n\nShow the codetab1 <- svyCreateTableOne(strata = \"OA\", data = w.design.m, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0              1              SMD   \n#>   n                     1783.6         2002.1               \n#>   age (%)                                              0.307\n#>      20-29 years         131.0 ( 7.3)   120.9 ( 6.0)        \n#>      30-39 years         388.0 (21.8)   237.8 (11.9)        \n#>      40-49 years         518.3 (29.1)   572.7 (28.6)        \n#>      50-59 years         680.1 (38.1)   999.3 (49.9)        \n#>      60-64 years          66.1 ( 3.7)    71.4 ( 3.6)        \n#>      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#>      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#>   sex = Male (%)         852.4 (47.8)   985.1 (49.2)   0.028\n#>   stress = stressed (%)  544.6 (30.5)   531.8 (26.6)   0.088\n#>   married = single (%)   419.8 (23.5)   427.5 (21.4)   0.052\n#>   income (%)                                           0.222\n#>      $29,999 or less     266.6 (14.9)   352.4 (17.6)        \n#>      $30,000-$49,999     462.8 (25.9)   348.8 (17.4)        \n#>      $50,000-$79,999     298.6 (16.7)   315.7 (15.8)        \n#>      $80,000 or more     755.5 (42.4)   985.2 (49.2)        \n#>   race = White (%)      1129.2 (63.3)  1364.9 (68.2)   0.103\n#>   bmi (%)                                              0.045\n#>      Underweight           0.0 ( 0.0)     0.0 ( 0.0)        \n#>      healthy weight      483.3 (27.1)   583.3 (29.1)        \n#>      Overweight         1300.2 (72.9)  1418.8 (70.9)        \n#>   phyact (%)                                           0.054\n#>      Active              448.0 (25.1)   493.9 (24.7)        \n#>      Inactive           1075.2 (60.3)  1176.6 (58.8)        \n#>      Moderate            260.3 (14.6)   331.5 (16.6)        \n#>   smoke (%)                                            0.288\n#>      Never smoker        400.2 (22.4)   265.8 (13.3)        \n#>      Current smoker      548.8 (30.8)   836.6 (41.8)        \n#>      Former smoker       834.6 (46.8)   899.6 (44.9)        \n#>   doctor = Yes (%)      1376.0 (77.1)  1430.1 (71.4)   0.131\n#>   drink (%)                                            0.194\n#>      Never drank          44.0 ( 2.5)   112.6 ( 5.6)        \n#>      Current drinker    1464.1 (82.1)  1510.6 (75.5)        \n#>      Former driker       275.4 (15.4)   378.9 (18.9)        \n#>   bp = Yes (%)           166.6 ( 9.3)   153.5 ( 7.7)   0.060\n#>   immigrate (%)                                        0.235\n#>      not immigrant      1694.8 (95.0)  1774.1 (88.6)        \n#>      > 10 years           88.8 ( 5.0)   228.0 (11.4)        \n#>      recent                0.0 ( 0.0)     0.0 ( 0.0)        \n#>   fruit (%)                                            0.293\n#>      0-3 daily serving   426.1 (23.9)   480.8 (24.0)        \n#>      4-6 daily serving   748.1 (41.9)  1082.3 (54.1)        \n#>      6+ daily serving    609.4 (34.2)   439.0 (21.9)        \n#>   diab = Yes (%)           0.0 ( 0.0)    83.6 ( 4.2)   0.295\n#>   edu (%)                                              0.172\n#>      < 2ndary            324.9 (18.2)   342.8 (17.1)        \n#>      2nd grad.            18.8 ( 1.1)    47.6 ( 2.4)        \n#>      Other 2nd grad.      15.2 ( 0.9)    52.2 ( 2.6)        \n#>      Post-2nd grad.     1424.7 (79.9)  1559.4 (77.9)\n\n\nOutcome analysis\n\nShow the codefit.design <- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.50 [0.08;3.09]   0.4535\n\n\nMatched data with increase ratio\nThe matching process is repeated with a different ratio (e.g., 1:5) to explore how changing the ratio affects the covariate balance and treatment effect estimation.\n\nShow the code# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 5:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.013)\n#>  - number of obs.: 1424 (original), 349 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004643 0.039181 0.084421 0.101576 0.146403 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     285         64               \n#>   age (%)                                       0.217\n#>      20-29 years         24 ( 8.4)   4 ( 6.2)        \n#>      30-39 years         59 (20.7)  11 (17.2)        \n#>      40-49 years         94 (33.0)  18 (28.1)        \n#>      50-59 years         98 (34.4)  28 (43.8)        \n#>      60-64 years         10 ( 3.5)   3 ( 4.7)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        132 (46.3)  30 (46.9)   0.011\n#>   stress = stressed (%)  81 (28.4)  18 (28.1)   0.007\n#>   married = single (%)   91 (31.9)  23 (35.9)   0.085\n#>   income (%)                                    0.065\n#>      $29,999 or less     64 (22.5)  13 (20.3)        \n#>      $30,000-$49,999     65 (22.8)  14 (21.9)        \n#>      $50,000-$79,999     51 (17.9)  12 (18.8)        \n#>      $80,000 or more    105 (36.8)  25 (39.1)        \n#>   race = White (%)      169 (59.3)  42 (65.6)   0.131\n#>   bmi (%)                                       0.097\n#>      Underweight          0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      68 (23.9)  18 (28.1)        \n#>      Overweight         217 (76.1)  46 (71.9)        \n#>   phyact (%)                                    0.136\n#>      Active              57 (20.0)  16 (25.0)        \n#>      Inactive           178 (62.5)  36 (56.2)        \n#>      Moderate            50 (17.5)  12 (18.8)        \n#>   smoke (%)                                     0.097\n#>      Never smoker        46 (16.1)   9 (14.1)        \n#>      Current smoker     123 (43.2)  26 (40.6)        \n#>      Former smoker      116 (40.7)  29 (45.3)        \n#>   doctor = Yes (%)      183 (64.2)  44 (68.8)   0.096\n#>   drink (%)                                     0.051\n#>      Never drank         10 ( 3.5)   2 ( 3.1)        \n#>      Current drinker    212 (74.4)  49 (76.6)        \n#>      Former driker       63 (22.1)  13 (20.3)        \n#>   bp = Yes (%)           22 ( 7.7)   5 ( 7.8)   0.003\n#>   immigrate (%)                                 0.100\n#>      not immigrant      266 (93.3)  58 (90.6)        \n#>      > 10 years          19 ( 6.7)   6 ( 9.4)        \n#>      recent               0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                     0.149\n#>      0-3 daily serving  104 (36.5)  19 (29.7)        \n#>      4-6 daily serving  124 (43.5)  30 (46.9)        \n#>      6+ daily serving    57 (20.0)  15 (23.4)        \n#>   diab = Yes (%)         12 ( 4.2)   3 ( 4.7)   0.023\n#>   edu (%)                                       0.137\n#>      < 2ndary            67 (23.5)  13 (20.3)        \n#>      2nd grad.            9 ( 3.2)   2 ( 3.1)        \n#>      Other 2nd grad.      9 ( 3.2)   1 ( 1.6)        \n#>      Post-2nd grad.     200 (70.2)  48 (75.0)\n\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\nShow the code# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n\n  \n\n\nShow the codesummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.8906  0.8906  0.8906  1.0000  0.8906  4.4531\n\n\nCombining matching weights\nDifferent approaches to incorporating weights (e.g., matching weights, survey weights) are explored.\nNot incorporating matching weights\n\nShow the codeanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396824    349\nw.design0 <- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\n\nShow the codefit.design <- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95 p-value \n#>        OA            0.80 [0.23;2.80]   0.722\n\n\nIncorporating matching weights\n\nShow the codeanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396824    349\n\n\n\nShow the code# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight <- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] <-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 <- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\n\nShow the codefit.design <- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            1.14 [0.32;4.07]   0.8419\n\n\nMatched with replacement\nMatching is performed with replacement, allowing control units to be used in more than one match.\n\nShow the code# Step 1 and 2\nmatching.obj <- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2,\n                        replace = TRUE)\n# see how many matched\nmatching.obj\n#> A matchit object\n#>  - method: 5:1 nearest neighbor matching with replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.013)\n#>  - number of obs.: 1424 (original), 308 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match <- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#> OACVD.match$OA: 0\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004643 0.034244 0.067224 0.100845 0.148958 0.418206 \n#> ------------------------------------------------------------ \n#> OACVD.match$OA: 1\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.004671 0.042322 0.097877 0.129743 0.189255 0.424895\ntab1 <- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#>                        Stratified by OA\n#>                         0           1          SMD   \n#>   n                     243         65               \n#>   age (%)                                       0.266\n#>      20-29 years         22 ( 9.1)   4 ( 6.2)        \n#>      30-39 years         57 (23.5)  11 (16.9)        \n#>      40-49 years         74 (30.5)  18 (27.7)        \n#>      50-59 years         82 (33.7)  29 (44.6)        \n#>      60-64 years          8 ( 3.3)   3 ( 4.6)        \n#>      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#>      teen                 0 ( 0.0)   0 ( 0.0)        \n#>   sex = Male (%)        113 (46.5)  31 (47.7)   0.024\n#>   stress = stressed (%)  67 (27.6)  19 (29.2)   0.037\n#>   married = single (%)   75 (30.9)  23 (35.4)   0.096\n#>   income (%)                                    0.079\n#>      $29,999 or less     53 (21.8)  13 (20.0)        \n#>      $30,000-$49,999     57 (23.5)  14 (21.5)        \n#>      $50,000-$79,999     44 (18.1)  12 (18.5)        \n#>      $80,000 or more     89 (36.6)  26 (40.0)        \n#>   race = White (%)      146 (60.1)  42 (64.6)   0.094\n#>   bmi (%)                                       0.088\n#>      Underweight          0 ( 0.0)   0 ( 0.0)        \n#>      healthy weight      58 (23.9)  18 (27.7)        \n#>      Overweight         185 (76.1)  47 (72.3)        \n#>   phyact (%)                                    0.160\n#>      Active              45 (18.5)  16 (24.6)        \n#>      Inactive           155 (63.8)  37 (56.9)        \n#>      Moderate            43 (17.7)  12 (18.5)        \n#>   smoke (%)                                     0.095\n#>      Never smoker        40 (16.5)   9 (13.8)        \n#>      Current smoker     101 (41.6)  26 (40.0)        \n#>      Former smoker      102 (42.0)  30 (46.2)        \n#>   doctor = Yes (%)      152 (62.6)  45 (69.2)   0.141\n#>   drink (%)                                     0.059\n#>      Never drank          9 ( 3.7)   2 ( 3.1)        \n#>      Current drinker    181 (74.5)  50 (76.9)        \n#>      Former driker       53 (21.8)  13 (20.0)        \n#>   bp = Yes (%)           19 ( 7.8)   5 ( 7.7)   0.005\n#>   immigrate (%)                                 0.115\n#>      not immigrant      228 (93.8)  59 (90.8)        \n#>      > 10 years          15 ( 6.2)   6 ( 9.2)        \n#>      recent               0 ( 0.0)   0 ( 0.0)        \n#>   fruit (%)                                     0.147\n#>      0-3 daily serving   87 (35.8)  19 (29.2)        \n#>      4-6 daily serving  109 (44.9)  31 (47.7)        \n#>      6+ daily serving    47 (19.3)  15 (23.1)        \n#>   diab = Yes (%)         11 ( 4.5)   3 ( 4.6)   0.004\n#>   edu (%)                                       0.175\n#>      < 2ndary            58 (23.9)  13 (20.0)        \n#>      2nd grad.            8 ( 3.3)   2 ( 3.1)        \n#>      Other 2nd grad.      9 ( 3.7)   1 ( 1.5)        \n#>      Post-2nd grad.     168 (69.1)  49 (75.4)\n\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\nShow the code# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n\n  \n\n\nShow the codesummary(OACVD.match$weights)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.7477  0.7477  0.7477  1.0000  1.0000  7.4769\n\n\nSurvey design\nThe matched data is converted into a survey design object, and the treatment effect is estimated while accounting for the complex survey design.\n\nShow the codeanalytic.miss$matched <- 0\nlength(analytic.miss$ID) # full data\n#> [1] 397173\nlength(OACVD.match$ID) # matched data\n#> [1] 308\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#> [1] 308\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] <- 1\ntable(analytic.miss$matched)\n#> \n#>      0      1 \n#> 396865    308\n\n\n\nShow the code# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight <- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] <-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 <- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m <- subset(w.design0, matched == 1)\n\n\n\nShow the codefit.design <- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#>  Variable Units OddsRatio       CI.95  p-value \n#>        OA            0.99 [0.26;3.72]   0.9909\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\nRDocumentation. 2023. “Matchit: Matchit: Matching Software for Causal Inference.” https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit."
  },
  {
    "objectID": "propensityscore3.html",
    "href": "propensityscore3.html",
    "title": "PSM in OA-CVD (US)",
    "section": "",
    "text": "Pre-processing\nLoad data\nLoad the dataset and inspect its structure and variables.\n\nShow the codeload(file=\"Data/propensityscore/NHANES17.RData\") \nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\n\n\nVisualize missing data patterns.\n\nShow the codelibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:data.table':\n#> \n#>     between, first, last\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nanalytic.with.miss <- dplyr::select(analytic.with.miss, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, \n                  married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\ndim(analytic.with.miss)\n#> [1] 9254   13\nstr(analytic.with.miss)\n#> 'data.frame':    9254 obs. of  13 variables:\n#>  $ cholesterol: 'labelled' int  NA NA 157 148 189 209 176 NA 238 182 ...\n#>   ..- attr(*, \"label\")= chr \"Total Cholesterol (mg/dL)\"\n#>  $ gender     : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n#>  $ age        : 'labelled' int  NA NA 66 NA NA 66 75 NA 56 NA ...\n#>   ..- attr(*, \"label\")= chr \"Age in years at screening\"\n#>  $ born       : chr  \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race       : chr  \"Other\" \"White\" \"Black\" \"Other\" ...\n#>  $ education  : chr  NA NA \"High.School\" NA ...\n#>  $ married    : chr  NA NA \"Previously.married\" NA ...\n#>  $ income     : chr  \"Over100k\" \"Over100k\" \"<25k\" NA ...\n#>  $ bmi        : 'labelled' num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>   ..- attr(*, \"label\")= chr \"Body Mass Index (kg/m**2)\"\n#>  $ diabetes   : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ weight     : 'labelled' num  8540 42567 8338 8723 7065 ...\n#>   ..- attr(*, \"label\")= chr \"Full sample 2 year MEC exam weight\"\n#>  $ psu        : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>  $ strata     : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\nnames(analytic.with.miss)\n#>  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#>  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#> [11] \"weight\"      \"psu\"         \"strata\"\n\nlibrary(DataExplorer)\nplot_missing(analytic.with.miss)\n\n\n\n\nFormatting variables\nRename variables to avoid conflicts. Recode variables into binary or categorical as needed. Ensure variable types (factor, numeric) are appropriate.\n\nShow the code# to avaoid any confusion later\n# rename weight variable as weights \n# is reserved for matching weights\nanalytic.with.miss$survey.weight <- analytic.with.miss$weight\nanalytic.with.miss$weight <- NULL\n\n#Creating binary variable for cholesterol\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, \n                                             1, #\"healthy\",\n                                             0) #\"unhealthy\")\n# exposure recoding\nanalytic.with.miss$diabetes <- ifelse(analytic.with.miss$diabetes == \"Yes\", 1, 0)\n\n# ID\nanalytic.with.miss$ID <- 1:nrow(analytic.with.miss)\n\n# covariates\nanalytic.with.miss$born <- ifelse(analytic.with.miss$born == \"Other\", \n                                             0,\n                                             1)\n\nvars = c(\"gender\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\")\n\nnumeric.names <- c(\"cholesterol\", \"bmi\")\nfactor.names <- vars[!vars %in% numeric.names] \n\nanalytic.with.miss[factor.names] <- apply(X = analytic.with.miss[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanalytic.with.miss[numeric.names] <- apply(X = analytic.with.miss[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\nanalytic.with.miss$income <- factor(analytic.with.miss$income, \n                                    ordered = TRUE, \n                                levels = c(\"<25k\", \"Between.25kto54k\", \n                                           \"Between.55kto99k\", \n                                           \"Over100k\"))\n\n# features\ntable(analytic.with.miss$strata)\n#> \n#> 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#> 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\ntable(analytic.with.miss$psu)\n#> \n#>    1    2 \n#> 4464 4790\ntable(analytic.with.miss$strata,analytic.with.miss$psu)\n#>      \n#>         1   2\n#>   134 215 295\n#>   135 316 322\n#>   136 320 375\n#>   137 306 248\n#>   138 308 297\n#>   139 278 375\n#>   140 315 297\n#>   141 282 411\n#>   142 349 386\n#>   143 232 319\n#>   144 351 338\n#>   145 339 270\n#>   146 277 327\n#>   147 335 261\n#>   148 241 269\n\n# impute\n# require(mice)\n# imputation1 <- mice(analytic.with.miss, seed = 123,\n#                    m = 1, # Number of multiple imputations. \n#                    maxit = 10 # Number of iteration; mostly useful for convergence\n#                    )\n# analytic.with.miss <- complete(imputation1)\n# plot_missing(analytic.with.miss)\n\n\nComplete case data\nCreate a dataset (analytic.data) without NA values for analysis. This is done for simplified analysis, but this approach has it’s own challenges. In a next tutorial, we will appropriately deal with missing observations in a propensity score modelling.\n\nShow the codedim(analytic.with.miss)\n#> [1] 9254   15\nanalytic.data <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic.data) # complete case\n#> [1] 4167   15\n\n\nZanutto (2006)\n\nRef: (Zanutto 2006)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSet seed\n\nShow the codeset.seed(123)\n\n\n\n“it is not necessary to use survey-weighted estimation for the propensity score model”\n\nPropensity score analysis in 4 steps:\nStep 1\nSpecify the propensity score model to estimate propensity scores\n\nShow the codeps.formula <- as.formula(diabetes ~ gender + born +\n                         race + education + married + income + bmi)\n\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores. Perform nearest-neighbor matching using the propensity scores. Visualize the distribution of propensity scores before and after matching.\n\nShow the coderequire(MatchIt)\nset.seed(123)\n# This function fits propensity score model (using logistic \n# regression as above) when specified distance = 'logit'\n# performs nearest-neighbor (NN) matching, \n# without replacement \n# with caliper = .2*SD of propensity score  \n# within which to draw control units \n# with 1:1 ratio (pair-matching)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\n# see matchit function options here\n# https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02901 0.12255 0.17658 0.18982 0.23876 0.82164\nplot(match.obj, type = \"jitter\")\n\n\n\n#> To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\nShow the codetapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02901 0.11509 0.16687 0.17949 0.22816 0.75968 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.04793 0.16489 0.21300 0.23395 0.27768 0.82164\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.019)\n#>  - number of obs.: 4167 (original), 1564 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data <- match.data(match.obj)\n\n\nStep 3\ncompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nShow the coderequire(tableone)\nbaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1 <- CreateTableOne(strata = \"diabetes\", vars = baselinevars,\n                       data = analytic.data, test = FALSE)\nprint(tab1, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                      3376           791               \n#>   gender = Male (%)      1578 (46.7)    434 (54.9)   0.163\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.060\n#>      Black                728 (21.6)    183 (23.1)        \n#>      Hispanic             727 (21.5)    170 (21.5)        \n#>      Other                642 (19.0)    159 (20.1)        \n#>      White               1279 (37.9)    279 (35.3)        \n#>   education (%)                                      0.185\n#>      College             1992 (59.0)    415 (52.5)        \n#>      High.School         1174 (34.8)    290 (36.7)        \n#>      School               210 ( 6.2)     86 (10.9)        \n#>   married (%)                                        0.316\n#>      Married             2027 (60.0)    488 (61.7)        \n#>      Never.married        631 (18.7)     70 ( 8.8)        \n#>      Previously.married   718 (21.3)    233 (29.5)        \n#>   income (%)                                         0.092\n#>      <25k                 830 (24.6)    225 (28.4)        \n#>      Between.25kto54k    1064 (31.5)    244 (30.8)        \n#>      Between.55kto99k     778 (23.0)    173 (21.9)        \n#>      Over100k             704 (20.9)    149 (18.8)        \n#>   bmi (mean (SD))       29.29 (7.11)  32.31 (8.03)   0.399\n\n\n\nShow the codetab1m <- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                       782           782               \n#>   gender = Male (%)       422 (54.0)    430 (55.0)   0.021\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.068\n#>      Black                180 (23.0)    179 (22.9)        \n#>      Hispanic             151 (19.3)    170 (21.7)        \n#>      Other                171 (21.9)    156 (19.9)        \n#>      White                280 (35.8)    277 (35.4)        \n#>   education (%)                                      0.077\n#>      College              441 (56.4)    411 (52.6)        \n#>      High.School          262 (33.5)    286 (36.6)        \n#>      School                79 (10.1)     85 (10.9)        \n#>   married (%)                                        0.044\n#>      Married              502 (64.2)    486 (62.1)        \n#>      Never.married         63 ( 8.1)     69 ( 8.8)        \n#>      Previously.married   217 (27.7)    227 (29.0)        \n#>   income (%)                                         0.065\n#>      <25k                 202 (25.8)    218 (27.9)        \n#>      Between.25kto54k     236 (30.2)    244 (31.2)        \n#>      Between.55kto99k     187 (23.9)    171 (21.9)        \n#>      Over100k             157 (20.1)    149 (19.1)        \n#>   bmi (mean (SD))       32.12 (8.29)  32.05 (7.58)   0.009\n\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample. Use the matched sample to estimate the treatment effect, considering survey design.\nIncorporating the survey design into both linear regression and propensity score analysis is crucial. Neglecting the survey weights can significantly impact the estimates, altering the representation of population-level effects.\n\nShow the coderequire(survey)\n# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data$ID) # matched data\n#> [1] 1564\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#> [1] 1564\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7690 1564\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m <- subset(w.design0, matched == 1)\n\n\n\nShow the codeout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial(logit), design = w.design.m)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1564 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.02 \n  \n\n Pseudo-R² (McFadden) \n    0.01 \n  \n\n AIC \n    1925.24 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.33 \n    0.97 \n    1.80 \n    1.79 \n    0.09 \n  \n\n diabetes \n    1.68 \n    1.17 \n    2.41 \n    2.84 \n    0.01 \n  \n\n\n Standard errors: Robust\n\n\n\nDuGoff et al. (2014)\n\nRef: (DuGoff, Schuler, and Stuart 2014)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Similar to Zanutto but includes additional covariates in the model.\n\nShow the code# response = exposure variable\n# independent variables = baseline covariates\nps.formula <- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi+\n                           psu+strata+survey.weight)\n\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores\n\nShow the coderequire(MatchIt)\nset.seed(123)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.00363 0.11341 0.17509 0.18982 0.24765 0.80853\nplot(match.obj, type = \"jitter\")\n\n\n\n#> To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\nShow the codetapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.00363 0.10394 0.16243 0.17690 0.23461 0.73143 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01182 0.17245 0.22748 0.24500 0.29948 0.80853\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score [caliper]\n#>              - estimated with logistic regression\n#>  - caliper: <distance> (0.021)\n#>  - number of obs.: 4167 (original), 1570 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi, psu, strata, survey.weight\n# extract matched data\nmatched.data <- match.data(match.obj)\n\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nShow the coderequire(tableone)\nbaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\", \n                  \"psu\", \"strata\", \"survey.weight\")\nmatched.data$survey.weight <- as.numeric(as.character(matched.data$survey.weight))\nmatched.data$strata <- as.numeric(as.character(matched.data$strata))\ntab1m <- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                            Stratified by diabetes\n#>                             0                   1                   SMD   \n#>   n                              785                 785                  \n#>   gender = Male (%)              433 (55.2)          431 (54.9)      0.005\n#>   born (mean (SD))              1.00 (0.00)         1.00 (0.00)     <0.001\n#>   race (%)                                                           0.048\n#>      Black                       193 (24.6)          180 (22.9)           \n#>      Hispanic                    163 (20.8)          170 (21.7)           \n#>      Other                       163 (20.8)          158 (20.1)           \n#>      White                       266 (33.9)          277 (35.3)           \n#>   education (%)                                                      0.032\n#>      College                     403 (51.3)          412 (52.5)           \n#>      High.School                 300 (38.2)          288 (36.7)           \n#>      School                       82 (10.4)           85 (10.8)           \n#>   married (%)                                                        0.030\n#>      Married                     473 (60.3)          484 (61.7)           \n#>      Never.married                71 ( 9.0)           70 ( 8.9)           \n#>      Previously.married          241 (30.7)          231 (29.4)           \n#>   income (%)                                                         0.035\n#>      <25k                        232 (29.6)          222 (28.3)           \n#>      Between.25kto54k            236 (30.1)          242 (30.8)           \n#>      Between.55kto99k            176 (22.4)          173 (22.0)           \n#>      Over100k                    141 (18.0)          148 (18.9)           \n#>   bmi (mean (SD))              31.92 (8.33)        32.09 (7.52)      0.020\n#>   psu = 2 (%)                    382 (48.7)          394 (50.2)      0.031\n#>   strata (mean (SD))          140.84 (4.24)       140.97 (4.23)      0.031\n#>   survey.weight (mean (SD)) 35647.01 (37699.98) 35596.81 (45212.82)  0.001\n\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample\n\nShow the code# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data$ID) # matched data\n#> [1] 1570\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#> [1] 1570\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7684 1570\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m <- subset(w.design0, matched == 1)\n\n\n\nShow the codeout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial, design = w.design.m)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1570 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.01 \n  \n\n Pseudo-R² (McFadden) \n    0.00 \n  \n\n AIC \n    1918.09 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.69 \n    1.33 \n    2.15 \n    4.24 \n    0.00 \n  \n\n diabetes \n    1.32 \n    0.99 \n    1.77 \n    1.86 \n    0.08 \n  \n\n\n Standard errors: Robust\n\n\n\nAustin et al. (2018)\n\nRef: (Austin, Jembere, and Chiu 2018)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Use survey logistic regression to account for survey design in propensity score estimation.\n\nShow the code# response = exposure variable\n# independent variables = baseline covariates\nps.formula <- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi)\nrequire(survey)\nanalytic.design <- svydesign(id=~psu,weights=~survey.weight, \n                             strata=~strata,\n                             data=analytic.data, nest=TRUE)\nps.fit <- svyglm(ps.formula, design=analytic.design, family=quasibinomial)\nanalytic.data$PS <- fitted(ps.fit)\nsummary(analytic.data$PS)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\n\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores. Two methods are explored: using the Matching package and the MatchIt package.\n\nShow the coderequire(Matching)\n#> Loading required package: Matching\n#> Warning: package 'Matching' was built under R version 4.3.1\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> ## \n#> ##  Matching (Version 4.10-14, Build Date: 2023-09-13)\n#> ##  See https://www.jsekhon.com for additional documentation.\n#> ##  Please cite software as:\n#> ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n#> ##   Software with Automated Balance Optimization: The Matching package for R.''\n#> ##   Journal of Statistical Software, 42(7): 1-52. \n#> ##\nmatch.obj2 <- Match(Y=analytic.data$cholesterol, \n                    Tr=analytic.data$diabetes, \n                    X=analytic.data$PS, \n                    M=1, \n                    estimand = \"ATT\",\n                    replace=FALSE, \n                    caliper = 0.2)\nsummary(match.obj2)\n#> \n#> Estimate...  -15.287 \n#> SE.........  2.118 \n#> T-stat.....  -7.2175 \n#> p.val......  5.2958e-13 \n#> \n#> Original number of observations..............  4167 \n#> Original number of treated obs...............  791 \n#> Matched number of observations...............  781 \n#> Matched number of observations  (unweighted).  781 \n#> \n#> Caliper (SDs)........................................   0.2 \n#> Number of obs dropped by 'exact' or 'caliper'  10\nmatched.data2 <- analytic.data[c(match.obj2$index.treated, \n                                 match.obj2$index.control),]\ndim(matched.data2)\n#> [1] 1562   16\n\n\n\nShow the coderequire(MatchIt)\nset.seed(123)\nmatch.obj <- matchit(ps.formula, data = analytic.data,\n                     distance = analytic.data$PS, \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS <- match.obj$distance\nsummary(match.obj$distance)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\nplot(match.obj, type = \"jitter\")\n\n\n\n#> To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\nShow the codetapply(analytic.data$PS, analytic.data$diabetes, summary)\n#> $`0`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.01352 0.08182 0.12644 0.14119 0.18267 0.75047 \n#> \n#> $`1`\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.02352 0.12362 0.16998 0.19389 0.23171 0.86375\n# check how many matched\nmatch.obj\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.019)\n#>  - number of obs.: 4167 (original), 1568 (matched)\n#>  - target estimand: ATT\n#>  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data2 <- match.data(match.obj)\ndim(matched.data2)\n#> [1] 1568   19\n\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD < 0.2 or not.\n\nShow the codebaselinevars <- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1m <- CreateTableOne(strata = \"diabetes\", \n                           vars = baselinevars,\n                           data = matched.data2, test = FALSE)\nprint(tab1m, smd = TRUE)\n#>                        Stratified by diabetes\n#>                         0             1             SMD   \n#>   n                       784           784               \n#>   gender = Male (%)       405 (51.7)    431 (55.0)   0.067\n#>   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  <0.001\n#>   race (%)                                           0.134\n#>      Black                163 (20.8)    182 (23.2)        \n#>      Hispanic             139 (17.7)    170 (21.7)        \n#>      Other                171 (21.8)    157 (20.0)        \n#>      White                311 (39.7)    275 (35.1)        \n#>   education (%)                                      0.040\n#>      College              428 (54.6)    413 (52.7)        \n#>      High.School          274 (34.9)    288 (36.7)        \n#>      School                82 (10.5)     83 (10.6)        \n#>   married (%)                                        0.070\n#>      Married              509 (64.9)    485 (61.9)        \n#>      Never.married         59 ( 7.5)     70 ( 8.9)        \n#>      Previously.married   216 (27.6)    229 (29.2)        \n#>   income (%)                                         0.063\n#>      <25k                 220 (28.1)    220 (28.1)        \n#>      Between.25kto54k     226 (28.8)    242 (30.9)        \n#>      Between.55kto99k     192 (24.5)    173 (22.1)        \n#>      Over100k             146 (18.6)    149 (19.0)        \n#>   bmi (mean (SD))       31.93 (8.16)  32.11 (7.61)   0.023\n\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample.\n\nShow the code# setup the design with survey features\nanalytic.with.miss$matched <- 0\nlength(analytic.with.miss$ID) # full data\n#> [1] 9254\nlength(matched.data2$ID) # matched data\n#> [1] 1568\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data2$ID])\n#> [1] 1568\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data2$ID] <- 1\ntable(analytic.with.miss$matched)\n#> \n#>    0    1 \n#> 7686 1568\nw.design0 <- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m2 <- subset(w.design0, matched == 1)\n\n\n\nShow the codeout.formula <- as.formula(cholesterol.bin ~ diabetes)\nsfit <- svyglm(out.formula,family=binomial, design = w.design.m2)\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\n Observations \n    1568 \n  \n\n Dependent variable \n    cholesterol.bin \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.01 \n  \n\n Pseudo-R² (McFadden) \n    0.01 \n  \n\n AIC \n    1919.53 \n  \n\n\n\n   \n    exp(Est.) \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    1.46 \n    1.15 \n    1.86 \n    3.11 \n    0.01 \n  \n\n diabetes \n    1.52 \n    1.21 \n    1.90 \n    3.65 \n    0.00 \n  \n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore4.html#references",
    "href": "propensityscore4.html#references",
    "title": "PSM in BMI-diabetes",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91."
  },
  {
    "objectID": "propensityscore5.html",
    "href": "propensityscore5.html",
    "title": "PSM with MI",
    "section": "",
    "text": "The tutorial provides a detailed walkthrough of implementing Propensity Score Matching (PSM) combined with Multiple Imputation (MI) in a statistical analysis, focusing on handling missing data and mitigating bias in observational studies.\nThe initial chunk is dedicated to loading various R packages that will be utilized throughout the tutorial. These libraries provide functions and tools that facilitate data manipulation, statistical modeling, visualization, and more.\n\nShow the code# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\nrequire(data.table)\nrequire(jtools)\nrequire(ggstance)\nrequire(DataExplorer)\nrequire(mitools)\nlibrary(kableExtra)\nlibrary(mice)\n\n\nProblem Statement\nLogistic regression\n\nPerform multiple imputation to deal with missing values; with 3 imputed datasets, 5 iterations,\nfit survey featured logistic regression in all of the 3 imputed datasets, and\nobtain the pooled OR (adjusted) and the corresponding 95% confidence intervals.\n\nHints\n\nUse the covariates (listed below) in the imputation model.\n\nImputation model covariates can be different than the original analysis covariates. You are encouraged to use variables in the imputation model that can be predictive of the variables with missing observations. In this example, we use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nAlso the imputation model specification can be modified. For example, we use pmm method for bmi in the imputation model.\nRemove any subject ID variable from the imputation model, if created in an intermediate step. Indeed ID variables should not be in the imputation model, if they are not predictive of the variables with missing observations.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPredictive Mean Matching:\nThe “Predictive Mean Matching” (PMM) method in Multiple Imputation (MI) is a widely used technique to handle missing data, particularly well-suited for continuous variables. PMM operates by first creating a predictive model for the variable with missing data, using observed values from other variables in the dataset. For each missing value, PMM identifies a set of observed values with predicted scores that are close to the predicted score for the missing value, derived from the predictive model. Then, instead of imputing a predicted score directly, PMM randomly selects one of the observed values from this set and assigns it as the imputed value. This method retains the original distribution of the imputed variable since it only uses observed values for imputation, and it also tends to preserve relationships between variables. PMM is particularly advantageous when the normality assumption of the imputed variable is questionable, providing a robust and practical approach to managing missing data in various research contexts.\n\n\nPropensity score matching (Zanutto, 2006)\n\nUse the propensity score matching as per Zanutto E. L. (2006)’s recommendation in all of the imputed datasets.\nReport the pooled OR estimates (adjusted) and corresponding 95% confidence intervals (adjusted OR).\nData and variables\nAnalytic data\nThe analytic dataset is saved as NHANES17.RData.\nVariables\nWe are primarily interested in outcome diabetes and exposure whether born in the US (born).\nVariables under consideration:\n\nsurvey features\n\nPSU\nstrata\nsurvey weight\n\n\nCovariates\n\nrace\nage\nmarriage\neducation\ngender\nBMI\nsystolic blood pressure\n\n\nPre-processing\nThe data is loaded and variables of interest are identified.\n\nShow the codeload(file=\"Data/propensityscore/NHANES17.RData\") # read data\nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\ndim(analytic.with.miss)\n#> [1] 9254   34\nvars <- c(\"ID\", # ID\n          \"psu\", \"strata\", \"weight\", # Survey features \n          \"race\", \"age\", \"married\",\"education\",\"gender\",\"bmi\",\"systolicBP\", # Covariates\n          \"born\", # Exposure\n          \"diabetes\") # Outcome\n\n\nSubset the dataset\nThe dataset is then subsetted to retain only the relevant variables, ensuring that subsequent analyses are focused and computationally efficient.\n\nShow the codedat.with.miss <- analytic.with.miss[,vars]\ndim(analytic.with.miss)\n#> [1] 9254   34\n\n\nInspect weights\nThe weights of the observations are inspected and adjusted to avoid issues in subsequent analyses.\n\nShow the codesummary(dat.with.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12347   21060   34671   37562  419763\n# weight = 0 would create problem in the analysis\n# ad-hoc solution to 0 weight problem\ndat.with.miss$weight[dat.with.miss$weight == 0] <- 0.00000001\n\n\nRecode the exposure variable\nThe exposure variable is recoded for clarity and ease of interpretation in results.\n\nShow the codedat.with.miss$born <- car::recode(dat.with.miss$born, \nrecodes = \" 'Born in 50 US states or Washingt' = \n'Born in US'; 'Others' = 'Others'; else = NA \" )\ndat.with.miss$born <- factor(dat.with.miss$born, levels = c(\"Born in US\", \"Others\"))\n\n\nvariable types\nVariable types are set, ensuring that each variable is treated appropriately in the analyses.\n\nShow the codefactor.names <- c(\"race\", \"married\", \"education\", \"gender\", \"diabetes\")\ndat.with.miss[,factor.names] <- lapply(dat.with.miss[,factor.names], factor)\n\n\nInspect extent of missing data problem\nA visualization is generated to explore the extent and pattern of missing data in the dataset, which informs the strategy for handling them.\n\nShow the coderequire(DataExplorer)\nplot_missing(dat.with.miss)\n\n\n\n\nNote that, multiple imputation then delete (MID) approach can be applied if the outcome had some missing values. Due to the small number of missingness, MICE may not impute the outcomes BTW.\n\n\n\n\n\n\nTip\n\n\n\nMultiple imputation then delete (MID):\nMID is a specific approach used in the context of multiple imputation (MI) when dealing with missing outcome data. All missing values, including those in the outcome variable, are imputed to create several complete datasets. In subsequent analyses, the imputed values for the outcome variable are deleted, so that only observed outcome values are analyzed. Each dataset (with observed outcome values and imputed predictor values) is analyzed separately, and results are pooled to provide a single estimate.\n\n\nLogistic regression\nInitialization\nThe MI process is initialized, setting up the framework for subsequent imputations.\n\nShow the codeimputation <- mice(data = dat.with.miss, maxit = 0, print = FALSE)\n\n\nSetting imputation model covariates\nThe predictor matrix is adjusted to specify which variables will be used to predict missing values in the imputation model. Setting strata as auxiliary variable:\n\nShow the codepred <- imputation$pred\npred\n#>            ID psu strata weight race age married education gender bmi\n#> ID          0   1      1      1    1   1       1         1      1   1\n#> psu         1   0      1      1    1   1       1         1      1   1\n#> strata      1   1      0      1    1   1       1         1      1   1\n#> weight      1   1      1      0    1   1       1         1      1   1\n#> race        1   1      1      1    0   1       1         1      1   1\n#> age         1   1      1      1    1   0       1         1      1   1\n#> married     1   1      1      1    1   1       0         1      1   1\n#> education   1   1      1      1    1   1       1         0      1   1\n#> gender      1   1      1      1    1   1       1         1      0   1\n#> bmi         1   1      1      1    1   1       1         1      1   0\n#> systolicBP  1   1      1      1    1   1       1         1      1   1\n#> born        1   1      1      1    1   1       1         1      1   1\n#> diabetes    1   1      1      1    1   1       1         1      1   1\n#>            systolicBP born diabetes\n#> ID                  1    1        1\n#> psu                 1    1        1\n#> strata              1    1        1\n#> weight              1    1        1\n#> race                1    1        1\n#> age                 1    1        1\n#> married             1    1        1\n#> education           1    1        1\n#> gender              1    1        1\n#> bmi                 1    1        1\n#> systolicBP          0    1        1\n#> born                1    0        1\n#> diabetes            1    1        0\npred[,\"ID\"] <- pred[\"ID\",] <- 0\npred[,\"psu\"] <- pred[\"psu\",] <- 0\npred[,\"weight\"] <- pred[\"weight\",] <- 0\npred[\"strata\",] <- 0\npred\n#>            ID psu strata weight race age married education gender bmi\n#> ID          0   0      0      0    0   0       0         0      0   0\n#> psu         0   0      0      0    0   0       0         0      0   0\n#> strata      0   0      0      0    0   0       0         0      0   0\n#> weight      0   0      0      0    0   0       0         0      0   0\n#> race        0   0      1      0    0   1       1         1      1   1\n#> age         0   0      1      0    1   0       1         1      1   1\n#> married     0   0      1      0    1   1       0         1      1   1\n#> education   0   0      1      0    1   1       1         0      1   1\n#> gender      0   0      1      0    1   1       1         1      0   1\n#> bmi         0   0      1      0    1   1       1         1      1   0\n#> systolicBP  0   0      1      0    1   1       1         1      1   1\n#> born        0   0      1      0    1   1       1         1      1   1\n#> diabetes    0   0      1      0    1   1       1         1      1   1\n#>            systolicBP born diabetes\n#> ID                  0    0        0\n#> psu                 0    0        0\n#> strata              0    0        0\n#> weight              0    0        0\n#> race                1    1        1\n#> age                 1    1        1\n#> married             1    1        1\n#> education           1    1        1\n#> gender              1    1        1\n#> bmi                 1    1        1\n#> systolicBP          0    1        1\n#> born                1    0        1\n#> diabetes            1    1        0\n\n\nSetting imputation model specification\nThe method for imputing a particular variable is specified (e.g., using Predictive Mean Matching). Here, we add pmm for bmi:\n\nShow the codemeth <- imputation$meth\nmeth[\"bmi\"] <- \"pmm\"\n\n\nImpute incomplete data\nMultiple datasets are imputed, each providing a different “guess” at the missing values, based on observed data. We are imputing m = 3 times.\n\nShow the codeimputation <- mice(data = dat.with.miss, \n                   seed = 123, \n                   predictorMatrix = pred,\n                   method = meth, \n                   m = 3, \n                   maxit = 5, \n                   print = FALSE)\nimpdata <- mice::complete(imputation, action=\"long\")\nimpdata$.id <- NULL\nm <- 3\nset.seed(123)\nallImputations <-  imputationList(lapply(1:m, \n                                         function(n)\n                                           subset(impdata, \n                                                  subset=.imp==n)))\nstr(allImputations)\n#> List of 2\n#>  $ imputations:List of 3\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 57 46 66 50 23 66 75 49 56 36 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 2 1 3 3 1 1 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 2 2 1 1 3 1 2 1 3 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 31.2 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 108 96 200 112 128 124 120 122 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 24 49 66 32 34 66 75 80 56 28 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 2 1 3 2 1 1 3 3 1 2 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 2 3 1 1 1 1 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 24.9 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 102 104 136 112 128 120 120 120 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>   ..$ :'data.frame': 9254 obs. of  14 variables:\n#>   .. ..$ .imp      : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#>   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#>   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#>   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#>   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#>   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#>   .. ..$ age       : int [1:9254] 47 71 66 71 45 66 75 37 56 47 ...\n#>   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 1 1 3 1 1 1 ...\n#>   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 1 3 1 1 1 2 ...\n#>   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#>   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 15.9 21.3 19.7 ...\n#>   .. ..$ systolicBP: int [1:9254] 100 114 162 112 128 166 120 116 108 112 ...\n#>   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#>   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#>  - attr(*, \"class\")= chr \"imputationList\"\n\n\nDesign\nA survey design object is created, ensuring that subsequent analyses appropriately account for the survey design.\n\nShow the codew.design <- svydesign(ids = ~psu, weights = ~weight, strata = ~strata,\n                      data = allImputations, nest = TRUE)\n\n\nSurvey data analysis\nA logistic regression model is fitted to each imputed dataset.\n\nShow the codemodel.formula <- as.formula(I(diabetes == 'Yes') ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\nfit.from.logistic <- with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nPooled estimates\nResults from models across all imputed datasets are pooled to provide a single estimate, accounting for the uncertainty due to missing data.\n\nShow the codepooled.estimates <- MIcombine(fit.from.logistic)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#> Multiple imputation results:\n#>       with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#>       MIcombine.default(fit.from.logistic)\n#>                           results      se  (lower  upper) missInfo\n#> (Intercept)               0.00013 7.5e-05 3.9e-05 0.00041     22 %\n#> bornOthers                1.44729 2.7e-01 1.0e+00 2.07384      0 %\n#> raceHispanic              0.81619 1.1e-01 6.3e-01 1.05882      0 %\n#> raceOther                 1.43817 2.6e-01 1.0e+00 2.04954      3 %\n#> raceWhite                 0.86411 1.3e-01 6.5e-01 1.14994      3 %\n#> age                       1.06157 3.6e-03 1.1e+00 1.06874      6 %\n#> marriedNever.married      0.83242 1.6e-01 5.7e-01 1.20809     10 %\n#> marriedPreviously.married 0.88401 1.1e-01 6.8e-01 1.14163     11 %\n#> educationHigh.School      1.16331 1.9e-01 8.4e-01 1.60803      0 %\n#> educationSchool           1.41943 2.4e-01 1.0e+00 1.98397      7 %\n#> genderMale                1.53458 1.8e-01 1.2e+00 1.94217      3 %\n#> bmi                       1.10597 1.2e-02 1.1e+00 1.12956      1 %\n#> systolicBP                1.00325 3.2e-03 1.0e+00 1.01001     39 %\nOR <- round(exp(pooled.estimates$coefficients), 2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR[2,]\n\n\n\n  \n\n\n\nPropensity score matching analysis\nInitialization\nThe MI process is re-initialized to facilitate PSM in the context of MI.\n\nShow the codeimputation <- mice(data = dat.with.miss, maxit = 0, print = FALSE)\nimpdata <- mice::complete(imputation, action=\"long\")\nm <- 3\nallImputations <- imputationList(lapply(1:m, \n                                        function(n) \n                                          subset(impdata, \n                                                 subset=.imp==n)))\n\n\nZanutto E. L. (2006) under multiple imputation\n\n\n\n\n\n\nTip\n\n\n\nAn iterative process is performed within each imputed dataset, which involves:\n\nEstimating propensity scores.\nMatching treated and untreated subjects based on these scores.\nExtracting matched data and checking the balance of covariates across matched groups.\nFitting outcome models to the survey weighted matched data and estimating treatment effects.\n\n\n\nNotice that we are performing multi-step process within MI\n\nShow the codematch.statm <- SMDm <- tab1m <- vector(\"list\", m) \nfit.from.PS <- vector(\"list\", m)\n\nfor (i in 1:m) {\n  analytic.i <- allImputations$imputations[[i]]\n  # Rename the weight variable into survey.weight\n  names(analytic.i)[names(analytic.i) == \"weight\"] <- \"survey.weight\"\n  \n  # Specify the PS model to estimate propensity scores\n  ps.formula <- as.formula(I(born==\"Others\") ~ \n                             race + age + married + education + \n                             gender + bmi + systolicBP)\n\n  # Propensity scores\n  ps.fit <- glm(ps.formula, data = analytic.i, family = binomial(\"logit\"))\n  analytic.i$PS <- fitted(ps.fit)\n  \n  # Match exposed and unexposed subjects \n  set.seed(123)\n  match.obj <- matchit(ps.formula, data = analytic.i, \n                       distance = analytic.i$PS, \n                       method = \"nearest\", \n                       replace = FALSE,\n                       caliper = 0.2, \n                       ratio = 1)\n  match.statm[[i]] <- match.obj\n  analytic.i$PS <- match.obj$distance\n  \n  # Extract matched data\n  matched.data <- match.data(match.obj) \n  \n  # Balance checking\n  cov <- c(\"race\", \"age\", \"married\", \"education\", \"gender\", \"bmi\", \"systolicBP\")\n  \n  tab1m[[i]] <- CreateTableOne(strata = \"born\", \n                               vars = cov, data = matched.data, \n                               test = FALSE, smd = TRUE)\n  SMDm[[i]] <- ExtractSmd(tab1m[[i]])\n  \n  # Setup the design with survey features\n  analytic.i$matched <- 0\n  analytic.i$matched[analytic.i$ID %in% matched.data$ID] <- 1\n  \n  # Survey setup for full data\n  w.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                         data = analytic.i, nest = TRUE)\n  \n  # Subset matched data\n  w.design.m <- subset(w.design0, matched == 1)\n  \n  # Outcome model (double adjustment)\n  out.formula <- as.formula(I(diabetes == \"Yes\") ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\n  fit.from.PS[[i]] <- svyglm(out.formula, design = w.design.m, \n                     family = quasibinomial(\"logit\"))\n}\n\n\nCheck matched data\nThe matched data is inspected to ensure that matching was successful and appropriate.\n\nShow the codematch.statm\n#> [[1]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3590 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n#> \n#> [[2]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3598 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n#> \n#> [[3]]\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: User-defined [caliper]\n#>  - caliper: <distance> (0.044)\n#>  - number of obs.: 9254 (original), 3594 (matched)\n#>  - target estimand: ATT\n#>  - covariates: race, age, married, education, gender, bmi, systolicBP\n\n\nCheck balance in matched data\nThe balance of covariates across matched groups is assessed to ensure that matching has successfully reduced bias.\n\nShow the codeSMDm\n#> [[1]]\n#>                 1 vs 2\n#> race       0.028883793\n#> age        0.033614763\n#> married    0.007318561\n#> education  0.117536503\n#> gender     0.040145831\n#> bmi        0.043350560\n#> systolicBP 0.054549772\n#> \n#> [[2]]\n#>                 1 vs 2\n#> race       0.019901420\n#> age        0.016267050\n#> married    0.017196043\n#> education  0.128811588\n#> gender     0.003338016\n#> bmi        0.057014434\n#> systolicBP 0.071553721\n#> \n#> [[3]]\n#>                1 vs 2\n#> race       0.04490482\n#> age        0.01959377\n#> married    0.03687394\n#> education  0.13301810\n#> gender     0.01225625\n#> bmi        0.03697878\n#> systolicBP 0.10025529\n\n\nPooled estimate\nFinally, the treatment effect estimates from the matched analyses across all imputed datasets are pooled to provide a single, overall estimate, ensuring that the final result appropriately accounts for the uncertainty due to both the matching process and the imputation of missing data.\n\nShow the codepooled.estimates <- MIcombine(fit.from.PS)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#> Multiple imputation results:\n#>       MIcombine.default(fit.from.PS)\n#>                           results      se  (lower  upper) missInfo\n#> (Intercept)               8.9e-05 4.9e-05 0.00003 0.00026      8 %\n#> bornOthers                2.0e+00 3.1e-01 1.47719 2.73325     16 %\n#> raceHispanic              7.0e-01 1.7e-01 0.42593 1.15504     27 %\n#> raceOther                 1.4e+00 4.1e-01 0.77209 2.53278     26 %\n#> raceWhite                 4.9e-01 2.7e-01 0.15853 1.52308     28 %\n#> age                       1.1e+00 4.6e-03 1.04472 1.06298      8 %\n#> marriedNever.married      5.9e-01 2.0e-01 0.28824 1.18926     38 %\n#> marriedPreviously.married 1.0e+00 2.5e-01 0.62417 1.64438     11 %\n#> educationHigh.School      1.4e+00 3.0e-01 0.89403 2.10852      2 %\n#> educationSchool           1.3e+00 3.4e-01 0.81042 2.21438      7 %\n#> genderMale                1.3e+00 2.3e-01 0.86695 1.83880     31 %\n#> bmi                       1.1e+00 1.1e-02 1.08062 1.12285      5 %\n#> systolicBP                1.0e+00 3.0e-03 1.00314 1.01494      3 %\nOR <- round(exp(pooled.estimates$coefficients), 2) \nOR <- as.data.frame(OR)\nCI <- round(exp(confint(pooled.estimates)), 2)\nOR <- cbind(OR, CI)\nOR[2,]"
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R functions (S)",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n bal.plot \n    cobalt \n    To produce a overalp/balance plot for propensity scoes \n  \n\n bal.tab \n    cobalt \n    To check the balance at each category of covariates \n  \n\n CreateCatTable \n    tableone \n    To create a frequency table with categorical variables only \n  \n\n do.call \n    base \n    To execute a function call \n  \n\n love.plot \n    cobalt \n    To plot the standardized mean differences at each category of covariates \n  \n\n match.data \n    MatchIt \n    To extract the matched dataste from a matchit object \n  \n\n matchit \n    MatchIt \n    To match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m. \n  \n\n rownames \n    base \n    Names of the rows"
  },
  {
    "objectID": "propensityscoreQ.html",
    "href": "propensityscoreQ.html",
    "title": "Quiz (S)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "propensityscoreE.html#problem-statement",
    "href": "propensityscoreE.html#problem-statement",
    "title": "Exercise (S)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Moon et al. (2021):\nWe will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\n\nSEQN: Respondent sequence number\nstrata: Masked pseudo strata (strata is nested within PSU)\npsu: Masked pseudo PSU\nsurvey.weight: Full sample 8 year interview weight divided by 4\nsurvey.cycle: NHANES cycle\n\nOutcome variable\n\ncvd: Cardiovascular disease\n\nExposure\n\nnocturia: Binary nocturia\n\nConfounders and other variables\n\nage: Age in years at screening\ngender: Gender\nrace: Race/Ethnicity\nsmoking: 100+ cigarettes in life\nalcohol: Alcohol consumption (12+ drinks in 1 year)\nsleep: Sleep duration, h\nbmi: Body Mass Index in kg/m\\(^2\\)\n\nsystolic: Systolic blood pressure, mmHg\ndiastolic: Diastolic blood pressure, mmHg\ntcholesterol: Total cholesterol, mg/dl\ntriglycerides: Triglycerides, mg/dl\nhdl: HDL‐cholesterol, mg/dl\ndiabetes: Diabetes mellitus\nhypertension: Hypertension\n\nTwo important warnings before we start:\n\nIn this paper, there is insufficient information to create the analytic dataset. This is mainly because of not sufficiently defining the covariates and not explicitly explaining the inclusion/exclusion criteria.\nThe authors did incorrect analyses. For example, they didn’t consider survey features. Since we will utilize survey features in our analysis, our results will likely be different than the results shown by the authors in Table 2."
  },
  {
    "objectID": "propensityscoreE.html#question-1-0-grade",
    "href": "propensityscoreE.html#question-1-0-grade",
    "title": "Exercise (S)",
    "section": "Question 1: [0% grade]",
    "text": "Question 1: [0% grade]\n1(a) Importing dataset\n\nShow the codeload(file = \"Data/propensityscore/Moon2021.RData\")\n\n\n1(b) Subsetting according to eligibility\n\nShow the code# Age 20+\ndat.analytic <- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic <- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars <- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic <- dat.analytic[,vars]\n\n# Complete case\ndat.analytic <- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#> [1] 15404    21\n\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, hypertension, diabetes mellitus, and survey cycles.\n\nHint 1: the authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nHint 2: Adjust the model for age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, and survey cycle.\nHint 3: Use Publish package to report the odds ratio with the 95% CI and p-value.\n\n\nShow the code# Create an indicator variable in the full data\ndat.full$miss <- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] <- 0\n\n# Design setup\nsvy.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design <- subset(svy.design0, miss == 0)\n\n# Design-adjusted logistic\nfit.logit <- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#>       Variable              Units OddsRatio         CI.95     p-value \n#>       nocturia                 <2       Ref                           \n#>                                2+      1.44   [1.21;1.71]   0.0001496 \n#>            age            [20,40)       Ref                           \n#>                           [40,60)      4.21   [3.05;5.82]     < 1e-04 \n#>                           [60,80)     11.46  [7.89;16.64]     < 1e-04 \n#>                          [80,Inf)     25.28 [17.51;36.50]     < 1e-04 \n#>         gender               Male       Ref                           \n#>                            Female      0.68   [0.58;0.79]     < 1e-04 \n#>           race          Hispanics       Ref                           \n#>                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#>                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#>                       Other races      1.55   [1.05;2.30]   0.0319116 \n#>            bmi                         1.02   [1.01;1.03]   0.0003273 \n#>        smoking                 No       Ref                           \n#>                               Yes      1.74   [1.46;2.07]     < 1e-04 \n#>        alcohol                 No       Ref                           \n#>                               Yes      0.92   [0.59;1.45]   0.7273627 \n#>          sleep                         0.96   [0.90;1.01]   0.1146287 \n#>   tcholesterol                         0.99   [0.99;0.99]     < 1e-04 \n#>  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#>            hdl                         0.99   [0.98;1.00]   0.0416900 \n#>   hypertension                 No       Ref                           \n#>                               Yes      2.73   [2.27;3.29]     < 1e-04 \n#>       diabetes                 No       Ref                           \n#>                               Yes      1.83   [1.51;2.22]     < 1e-04 \n#>   survey.cycle            2005-06       Ref                           \n#>                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#>                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#>                           2011-11      0.82   [0.68;0.99]   0.0398975"
  },
  {
    "objectID": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "href": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "title": "Exercise (S)",
    "section": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]",
    "text": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Question 1) using the propensity score 1:1 matching analysis as per DuGoff et al. (2014) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia <2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare your results with the results reported by the authors. [Expected answer: 2-3 sentences]\n\n\nShow the code# your codes here"
  },
  {
    "objectID": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "href": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "title": "Exercise (S)",
    "section": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]",
    "text": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Questions 1 and 2) using the propensity score 1:4 matching analysis as per Austin et al. (2018) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia <2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing. Remember, you need to multiply matching weights and survey weights to get survey-based estimates.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare the results with Question 2. What’s the overall conclusion? [Expected answer: 2-3 sentences]\n\n\nShow the code# your codes here"
  },
  {
    "objectID": "machinelearning.html#background",
    "href": "machinelearning.html#background",
    "title": "Machine learning",
    "section": "Background",
    "text": "Background\nThe chapter encompasses a series of instructional content that sequentially explores various facets of predictive modeling and machine learning, connecting them with a previous chapter. Beginning with a tutorial that revisits the application of regression for predicting continuous outcomes, it underscores the importance of understanding prediction error and overfitting, and introduces foundational machine learning concepts. The subsequent tutorial emphasizes the pivotal role of data splitting in predictive modeling, illustrating how to partition data into training and test sets and evaluate model performance across different data scenarios. Moving forward, the concept of cross-validation is explored, detailing the k-fold cross-validation method and demonstrating its implementation both manually and using the caret package. Another tutorial navigates through predicting binary outcomes using logistic regression, evaluating model performance using various metrics, and employing k-fold cross-validation.\nThe series then delves into supervised learning, exploring regularization techniques, decision trees, and ensemble methods, while employing various model evaluation metrics and cross-validation techniques. Lastly, unsupervised learning is introduced with a focus on the k-means clustering algorithm, discussing its implementation, determining the optimal number of clusters, and addressing associated challenges. Throughout, the tutorials provide practical examples, code snippets, and visual aids, offering a comprehensive and applied exploration of predictive modeling and machine learning concepts.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "machinelearning.html#key-references",
    "href": "machinelearning.html#key-references",
    "title": "Machine learning",
    "section": "Key References",
    "text": "Key References\n\n\n\n\n\n(Bi et al. 2019)\n(Liu et al. 2019)\n(Kuhn et al. 2013)\n\nFollowing ate optional but useful references\n\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)"
  },
  {
    "objectID": "machinelearning.html#overview-of-tutorials",
    "href": "machinelearning.html#overview-of-tutorials",
    "title": "Machine learning",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nRevisiting: Explore relationships for continuous outcome variable\nIn this tutorial, the focus is on utilizing regression to predict continuous outcomes, specifically employing multiple linear regression to construct an initial prediction model. The tutorial revisits fundamental concepts related to prediction and introduces foundational ideas pertinent to machine learning, all while using a distinct dataset compared to previous tutorials. The process involves loading a dataset, defining variables, and fitting a model using linear regression. Subsequent sections delve into the creation of a design matrix, obtaining predictions, and measuring prediction error through various metrics like R^2 and RMSE. The tutorial also addresses the critical concept of overfitting, discussing its causes, consequences, and potential solutions, such as internal and external validation methods.\n\n\nRevisiting: Data spliting\nThis tutorial emphasizes the crucial concept of data splitting in the context of predictive modeling and machine learning, utilizing a different dataset than previous tutorials. The process begins with loading a dataset and then strategically splitting it into training and test subsets, ensuring a robust approach to model validation. A model is trained using the training data, and its performance is evaluated using various metrics, such as R^2 and RMSE, through a custom function that extracts these performance measures. This function facilitates the evaluation of the model’s predictive accuracy and fit by applying it to different datasets (training, test, and the entire dataset), thereby enabling a comprehensive understanding of the model’s performance across different data scenarios.\n\n\nRevisiting: Cross-vaildation\nThis tutorial delves into the concept of cross-validation, a pivotal technique in predictive modeling and machine learning, using a distinct dataset for illustrative purposes. The process of k-fold cross-validation is explored, wherein the data is partitioned into ‘k’ subsets, and the model is trained ‘k’ times, each time using a different subset as the test set and the remaining data as the training set. This method is employed to assess the model’s predictive performance and to mitigate the risk of results being dependent on the initial data split. The tutorial demonstrates both manual calculations for individual folds and the utilization of the caret package to automate the cross-validation process, thereby providing a comprehensive overview of the method.\n\n\nRevisiting: Explore relationships for binary outcome variable\nThe tutorial navigates through the concept of predicting binary outcomes using logistic regression, emphasizing the application of various model evaluation metrics and methodologies in a machine learning context. It begins by ensuring that the outcome variable is treated as a factor and then proceeds to model fitting, where logistic regression is applied to predict a binary outcome. The model’s predictive performance is evaluated using metrics like the Area Under the Curve (AUC) and the Brier Score, which respectively assess the model’s classification accuracy and the mean squared difference between predicted probabilities and the actual outcomes. Furthermore, the tutorial explores k-fold cross-validation using the caret package, providing a robust method to assess the model’s predictive performance while avoiding overfitting. It also touches upon variable selection using stepwise regression with the Akaike Information Criterion (AIC) as a selection criterion.\n\n\nSupervised learning\nThis tutorial delves into the realm of supervised learning, exploring beyond statistical regression and introducing various machine learning methods tailored for both continuous and binary outcomes. The tutorial explores different regularization techniques, such as LASSO, Ridge, and Elastic Net, which are used to prevent overfitting by penalizing large coefficients in regression models. It also introduces decision trees (CART), which provide a flexible, hierarchical approach to modeling data, and can automatically incorporate non-linear effects and interactions. The tutorial further explores ensemble methods, which combine predictions from multiple models to improve predictive accuracy. Two types of ensemble methods are discussed: Type I, which trains the same model on different samples of the data (e.g., bagging and boosting), and Type II, which trains different models on the same data (e.g., Super Learner). Various model evaluation metrics and cross-validation techniques are utilized throughout to assess and enhance the predictive performance of the models.\n\n\nUnsupervised learning\nThe tutorial introduces unsupervised learning, with a focus on clustering, a technique that categorizes data into distinct groups based on similarity without using predefined labels. The k-means clustering algorithm is highlighted, which partitions data into k groups by minimizing within-cluster variation, typically using the sum of squares of Euclidean distances. The algorithm iteratively assigns data points to clusters based on the mean of the data points in each cluster and recalculates the cluster means until the cluster assignments no longer change. Various examples illustrate how to apply k-means clustering to different datasets and variable combinations. The tutorial also discusses determining the optimal number of clusters, k, and addresses challenges such as the influence of outliers and the sensitivity to the initial assignment of cluster means.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences\n\n\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. “What Is Machine Learning? A Primer for the Epidemiologist.” American Journal of Epidemiology 188 (12): 2222–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. “Over-Fitting and Model Tuning.” Applied Predictive Modeling, 61–92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. “How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature.” Jama 322 (18): 1806–16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. “Predictor Selection.” Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395–429."
  },
  {
    "objectID": "machinelearning1.html",
    "href": "machinelearning1.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction model.\nLoad dataset\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n\n  \n\n\n\nPrediction for length of stay\nNow, we show the regression fitting when outcome is continuous (length of stay).\nVariables\n\nShow the codebaselinevars <- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#>  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#>  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#>  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#> [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#> [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#> [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#> [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#> [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#> [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#> [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#> [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#> [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#> [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#> [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#> [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#> [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#> [49] \"income\"                \"RHC.use\"\n\n\nModel\n\nShow the code# adjust covariates\nout.formula1 <- as.formula(paste(\"Length.of.Stay~ \", \n                               paste(baselinevars, \n                                     collapse = \"+\")))\nsaveRDS(out.formula1, file = \"Data/machinelearning/form1.RDS\")\nfit1 <- lm(out.formula1, data = ObsData)\nrequire(Publish)\nadj.fit1 <- publish(fit1, digits=1)$regressionTable\n\n\n\nShow the codeout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nadj.fit1\n\n\n\n  \n\n\n\nDesign Matrix\n\nNotations\n\nn is number of observations\np is number of covariates\n\n\n\nExpands factors to a set of dummy variables.\n\nShow the codedim(ObsData)\n#> [1] 5735   52\nlength(attr(terms(out.formula1), \"term.labels\"))\n#> [1] 50\n\n\n\nShow the codehead(model.matrix(fit1))\n#>   (Intercept) Disease.categoryCHF Disease.categoryOther Disease.categoryMOSF\n#> 1           1                   0                     1                    0\n#> 2           1                   0                     0                    1\n#> 3           1                   0                     0                    1\n#> 4           1                   0                     0                    0\n#> 5           1                   0                     0                    1\n#> 6           1                   0                     1                    0\n#>   CancerLocalized (Yes) CancerMetastatic Cardiovascular1 Congestive.HF1\n#> 1                     1                0               0              0\n#> 2                     0                0               1              1\n#> 3                     1                0               0              0\n#> 4                     0                0               0              0\n#> 5                     0                0               0              0\n#> 6                     0                0               0              1\n#>   Dementia1 Psychiatric1 Pulmonary1 Renal1 Hepatic1 GI.Bleed1 Tumor1\n#> 1         0            0          1      0        0         0      1\n#> 2         0            0          0      0        0         0      0\n#> 3         0            0          0      0        0         0      1\n#> 4         0            0          0      0        0         0      0\n#> 5         0            0          0      0        0         0      0\n#> 6         0            0          1      0        0         0      0\n#>   Immunosupperssion1 Transfer.hx1 MI1 age[50,60) age[60,70) age[70,80)\n#> 1                  0            0   0          0          0          1\n#> 2                  1            1   0          0          0          1\n#> 3                  1            0   0          0          0          0\n#> 4                  1            0   0          0          0          1\n#> 5                  0            0   0          0          1          0\n#> 6                  0            0   0          0          0          0\n#>   age[80, Inf) sexFemale       edu DASIndex APACHE.score Glasgow.Coma.Score\n#> 1            0         0 12.000000 23.50000           46                  0\n#> 2            0         1 12.000000 14.75195           50                  0\n#> 3            0         1 14.069916 18.13672           82                  0\n#> 4            0         1  9.000000 22.92969           48                  0\n#> 5            0         0  9.945259 21.05078           72                 41\n#> 6            1         1  8.000000 17.50000           38                  0\n#>   blood.pressure         WBC Heart.rate Respiratory.rate Temperature\n#> 1             41 22.09765620        124               10    38.69531\n#> 2             63 28.89843750        137               38    38.89844\n#> 3             57  0.04999542        130               40    36.39844\n#> 4             55 23.29687500         58               26    35.79688\n#> 5             65 29.69921880        125               27    34.79688\n#> 6            115 18.00000000        134               36    39.19531\n#>   PaO2vs.FIO2  Albumin Hematocrit Bilirubin Creatinine Sodium Potassium PaCo2\n#> 1     68.0000 3.500000   58.00000 1.0097656  1.1999512    145  4.000000    40\n#> 2    218.3125 2.599609   32.50000 0.6999512  0.5999756    137  3.299805    34\n#> 3    275.5000 3.500000   21.09766 1.0097656  2.5996094    146  2.899902    16\n#> 4    156.6562 3.500000   26.29688 0.3999634  1.6999512    117  5.799805    30\n#> 5    478.0000 3.500000   24.00000 1.0097656  3.5996094    126  5.799805    17\n#> 6    184.1875 3.099609   30.50000 1.0097656  1.3999023    138  5.399414    68\n#>         PH   Weight DNR.statusYes Medical.insuranceMedicare\n#> 1 7.359375 64.69995             0                         1\n#> 2 7.329102 45.69998             0                         0\n#> 3 7.359375  0.00000             0                         0\n#> 4 7.459961 54.59998             0                         0\n#> 5 7.229492 78.39996             1                         1\n#> 6 7.299805 54.89999             0                         1\n#>   Medical.insuranceMedicare & Medicaid Medical.insuranceNo insurance\n#> 1                                    0                             0\n#> 2                                    0                             0\n#> 3                                    0                             0\n#> 4                                    0                             0\n#> 5                                    0                             0\n#> 6                                    0                             0\n#>   Medical.insurancePrivate Medical.insurancePrivate & Medicare\n#> 1                        0                                   0\n#> 2                        0                                   1\n#> 3                        1                                   0\n#> 4                        0                                   1\n#> 5                        0                                   0\n#> 6                        0                                   0\n#>   Respiratory.DiagYes Cardiovascular.DiagYes Neurological.DiagYes\n#> 1                   1                      1                    0\n#> 2                   0                      0                    0\n#> 3                   0                      1                    0\n#> 4                   1                      0                    0\n#> 5                   0                      1                    0\n#> 6                   1                      0                    0\n#>   Gastrointestinal.DiagYes Renal.DiagYes Metabolic.DiagYes Hematologic.DiagYes\n#> 1                        0             0                 0                   0\n#> 2                        0             0                 0                   0\n#> 3                        0             0                 0                   0\n#> 4                        0             0                 0                   0\n#> 5                        0             0                 0                   0\n#> 6                        0             0                 0                   0\n#>   Sepsis.DiagYes Trauma.DiagYes Orthopedic.DiagYes raceblack raceother\n#> 1              0              0                  0         0         0\n#> 2              1              0                  0         0         0\n#> 3              0              0                  0         0         0\n#> 4              0              0                  0         0         0\n#> 5              0              0                  0         0         0\n#> 6              0              0                  0         0         0\n#>   income$25-$50k income> $50k incomeUnder $11k RHC.use\n#> 1              0            0                1       0\n#> 2              0            0                1       1\n#> 3              1            0                0       1\n#> 4              0            0                0       0\n#> 5              0            0                1       1\n#> 6              0            0                1       0\ndim(model.matrix(fit1))\n#> [1] 5735   64\np <- dim(model.matrix(fit1))[2] # intercept + slopes\np\n#> [1] 64\n\n\nObtain prediction\n\nShow the codeobs.y <- ObsData$Length.of.Stay\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    2.00    7.00   14.00   21.56   25.00  394.00\n# Predict the above fit on ObsData data\npred.y1 <- predict(fit1, ObsData)\nsummary(pred.y1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  -32.76   16.62   21.96   21.56   26.73   42.67\nn <- length(pred.y1)\nn\n#> [1] 5735\nplot(obs.y,pred.y1)\nlines(lowess(obs.y,pred.y1), col = \"red\")\n\n\n\n\nMeasuring prediction error\nPrediction error measures how well the model can predict the outcome for new data that were not used in developing the prediction model.\n\nBias reduced for models with more variables\nUnimportant variables lead to noise / variability\nBias variance trade-off / need penalization\n\nR2\nThe provided information describes a statistical context involving a dataset of n values, \\(y_1, ..., y_n\\) (referred to as \\(y_i\\) or as a vector \\(y = [y_1,...,y_n]^T\\)), each paired with a fitted value \\(f_1,...,f_n\\) (denoted as \\(f_i\\) or sometimes \\(\\hat{y_i}\\), and as a vector \\(f\\)). The residuals, represented as \\(e_i\\), are defined as the differences between the observed and the fitted values: $ e_i = y_i − f_i$\nThe mean of the observed data is denoted by \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i \\]\nThe variability of the dataset can be quantified using two sums of squares formulas: 1. Residual Sum of Squares (SSres) or SSE: It quantifies the variance remaining in the data after fitting a model, calculated as: \\[ SS_{res} = \\sum_{i}(y_i - f_i)^2 = \\sum_{i}e_i^2 \\] 2. Total Sum of Squares (SStot) or SST: It represents the total variance in the observed data, calculated as: \\[ SS_{tot} = \\sum_{i}(y_i - \\bar{y})^2 \\]\nThe Coefficient of Determination (R²) or R.2, which provides a measure of how well the model’s predictions match the observed data, is defined as: \\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nIn the ideal scenario where the model fits the data perfectly, we have \\(SS_{res} = 0\\) and thus \\(R^2 = 1\\). Conversely, a baseline model, which always predicts the mean \\(\\bar{y}\\) of the observed data, would yield \\(R^2 = 0\\). Models performing worse than this baseline model would result in a negative R² value. This metric is widely utilized in regression analysis to evaluate model performance, where a higher R² indicates a better fit of the model to the data.\n\nShow the code# Find SSE\nSSE <- sum( (obs.y - pred.y1)^2 )\nSSE\n#> [1] 3536398\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 3836690\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.07826832\nrequire(caret)\ncaret::R2(pred.y1, obs.y)\n#> [1] 0.07826832\n\n\nref\nRMSE\n\nShow the code# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 24.97185\ncaret::RMSE(pred.y1, obs.y)\n#> [1] 24.83212\n\n\nSee (Wikipedia 2023b)\nAdj R2\nThe Adjusted R² statistic modifies the \\(R^2\\) value to counteract the automatic increase of \\(R^2\\) when extra explanatory variables are added to a model, even if they do not improve the model fit. This adjustment is crucial for ensuring that the metric offers a reliable indication of the explanatory power of the model, especially in multiple regression where several predictors are involved.\nThe commonly used formula is defined as:\n\\[\n\\bar{R}^{2} = 1 - \\frac{SS_{\\text{res}} / df_{\\text{res}}}{SS_{\\text{tot}} / df_{\\text{tot}}}\n\\]\nWhere:\n\n\n\\(SS_{\\text{res}}\\) and \\(SS_{\\text{tot}}\\) represent the residual and total sums of squares respectively.\n\n\\(df_{\\text{res}}\\) and \\(df_{\\text{tot}}\\) refer to the degrees of freedom of the residual and total sums of squares. Usually, \\(df_{\\text{res}} = n - p\\) and \\(df_{\\text{tot}} = n - 1\\), where:\n\n\n\\(n\\) signifies the sample size.\n\n\\(p\\) denotes the number of variables in the model.\n\n\n\nThis metric plays a vital role in model selection and safeguards against overfitting by penalizing the inclusion of non-informative variables\nThe alternate formula is:\n\\[\n\\bar{R}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n\\]\nThis formula modifies the \\(R^2\\) value, accounting for the number of predictors and offering a more parsimonious model fit measure.\n\nShow the code# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p-1))\nadjR2\n#> [1] 0.06786429\n\n\nSee (Wikipedia 2023a)\nOverfitting and Optimism\n\nModel usually performs very well in the empirical data where the model was fitted in the same data (optimistic)\nModel performs poorly in the new data (generalization is not as good)\n\nCauses\n\nModel determined by data at hand without expert opinion\nToo many model parameters (\\(age\\), \\(age^2\\), \\(age^3\\)) / predictors\nToo small dataset (training) / data too noisy\nConsequences\n\nOverestimation of effects of predictors\nReduction in model performance in new observations\nProposed solutions\nWe generally use procedures such as\n\nInternal validation\n\nsample splitting\ncross-validation\nbootstrap\n\n\nExternal validation\n\nTemporal\nGeographical\nDifferent data source to calculate same variable\nDifferent disease\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "machinelearning2.html",
    "href": "machinelearning2.html",
    "title": "Data spliting",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nLoad dataset\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n\n  \n\n\n\nSee (KDnuggets 2023; Kuhn 2023)\n\nShow the code# Using a seed to randomize in a reproducible way \nset.seed(123)\nrequire(caret)\nsplit<-createDataPartition(y = ObsData$Length.of.Stay, \n                           p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:4017, 1] 1 2 3 4 5 6 7 8 9 10 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 4017    1\ndim(ObsData)*.7 # approximate train data\n#> [1] 4014.5   36.4\ndim(ObsData)*(1-.7) # approximate train data\n#> [1] 1720.5   15.6\n\n\nSplit the data\n\nShow the code# create train data\ntrain.data<-ObsData[split,]\ndim(train.data)\n#> [1] 4017   52\n# create test data\ntest.data<-ObsData[-split,]\ndim(test.data)\n#> [1] 1718   52\n\n\nTrain the model\n\nShow the codeout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nfit.train1<-lm(out.formula1, data = train.data)\n# summary(fit.train1)\n\n\nFunction that gives performance measures\n\nShow the codeperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  # sample size\n  n <- length(pred.y)\n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  # returning object\n  return(res)\n}\n\n\nExtract performance measures\n\nShow the codeperform(new.data=train.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 4017 64 0.081 0.067 24.647\nperform(new.data=test.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 1718 64 0.056  0.02 25.488\nperform(new.data=ObsData,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 5735 64 0.073 0.063 24.902\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html."
  },
  {
    "objectID": "machinelearning3.html",
    "href": "machinelearning3.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nNow, we will describe the ideas of cross-validation.\nLoad previously saved data\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\n\n\nk-fold cross-vaildation\nSee (Wikipedia 2023)\n\n\n\n\n\n\nShow the codek = 5\ndim(ObsData)\n#> [1] 5735   52\nset.seed(567)\n# create folds (based on outcome)\nfolds <- createFolds(ObsData$Length.of.Stay, k = k, \n                     list = TRUE, returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\ndim(ObsData)*4/5 # approximate training data size\n#> [1] 4588.0   41.6\ndim(ObsData)/5  # approximate test data size\n#> [1] 1147.0   10.4\nlength(folds[[1]])\n#> [1] 4588\nlength(folds[[5]])\n#> [1] 4587\nstr(folds[[1]])\n#>  int [1:4588] 1 2 4 6 7 8 9 10 11 13 ...\nstr(folds[[5]])\n#>  int [1:4587] 1 3 5 6 7 8 10 11 12 13 ...\n\n\nCalculation for Fold 1\n\nShow the codefold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 4 6 7 8\nfold1.train <- ObsData[fold1.train.ids,]\nfold1.test <- ObsData[-fold1.train.ids,]\nout.formula1\n#> Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#>     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#>     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#>     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#>     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#>     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#>     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nmodel.fit <- lm(out.formula1, data = fold1.train)\npredictions <- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#>         n  p    R2  adjR2  RMSE\n#> [1,] 1147 64 0.051 -0.004 24.86\n\n\nCalculation for Fold 2\n\nShow the codefold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 2 3 4 5 6 7\nfold1.train <- ObsData[fold1.train.ids,]\nfold1.test <- ObsData[-fold1.train.ids,]\nmodel.fit <- lm(out.formula1, data = fold1.train)\npredictions <- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 1147 64 0.066 0.011 24.714\n\n\nUsing caret package to automate\nSee (Kuhn 2023)\n\nShow the code# Using Caret package\nset.seed(504)\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n# fit the model with formula = out.formula1\n# use training method lm\nfit.cv<-train(out.formula1, trControl = ctrl,\n               data = ObsData, method = \"lm\")\nfit.cv\n#> Linear Regression \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.05478  0.05980578  15.19515\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n# extract results from each test data \nsummary.res <- fit.cv$resample\nsummary.res\n\n\n\n  \n\n\nShow the codemean(fit.cv$resample$Rsquared)\n#> [1] 0.05980578\nsd(fit.cv$resample$Rsquared)\n#> [1] 0.01204451\nmean(fit.cv$resample$RMSE)\n#> [1] 25.05478\nsd(fit.cv$resample$RMSE)\n#> [1] 2.240366\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "machinelearning4.html",
    "href": "machinelearning4.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of binary outcomes. We will use logistic regression to build the first prediction model.\nRead previously saved data\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\n\nOutcome levels (factor)\n\nLabel\n\nPossible values of outcome\n\n\n\n\nShow the codelevels(ObsData$Death)=c(\"No\",\"Yes\") # this is useful for caret\n# ref: https://tinyurl.com/caretbin\nclass(ObsData$Death)\n#> [1] \"factor\"\ntable(ObsData$Death)\n#> \n#>   No  Yes \n#> 2013 3722\n\n\nMeasuring prediction error\n\nBrier score\n\nBrier score 0 means perfect prediction, and\nclose to zero means better prediction,\n1 being the worst prediction.\nLess accurate forecasts get higher score in Brier score.\n\n\nAUC\n\nThe area under a ROC curve is called as a c statistics.\nc being 0.5 means random prediction and\n1 indicates perfect prediction\n\n\nPrediction for death\nIn this section, we show the regression fitting when outcome is binary (death).\nVariables\n\nShow the codebaselinevars <- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#>  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#>  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#>  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#> [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#> [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#> [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#> [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#> [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#> [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#> [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#> [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#> [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#> [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#> [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#> [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#> [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#> [49] \"income\"                \"RHC.use\"\n\n\nModel\n\nShow the code# adjust covariates\nout.formula2 <- as.formula(paste(\"Death~ \", paste(baselinevars, collapse = \"+\")))\nsaveRDS(out.formula2, file = \"Data/machinelearning/form2.RDS\")\nfit2 <- glm(out.formula2, data = ObsData, \n            family = binomial(link = \"logit\"))\nrequire(Publish)\nadj.fit2 <- publish(fit2, digits=1)$regressionTable\n\n\n\nShow the codeout.formula2\n#> Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#>     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#>     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#>     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#>     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#>     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#>     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nadj.fit2\n\n\n\n  \n\n\n\nMeasuring prediction error\nAUC\n\nShow the coderequire(pROC)\n#> Loading required package: pROC\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\nobs.y2<-ObsData$Death\npred.y2 <- predict(fit2, type = \"response\")\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.7682\nplot(rocobj)\n\n\n\nShow the codeauc(rocobj)\n#> Area under the curve: 0.7682\n\n\nBrier Score\n\nShow the coderequire(DescTools)\n#> Loading required package: DescTools\nBrierScore(fit2)\n#> [1] 0.1812502\n\n\nCross-validation using caret\nBasic setup\n\nShow the code# Using Caret package\nset.seed(504)\n\n# make a 5-fold CV\nrequire(caret)\n#> Loading required package: caret\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n#> \n#> Attaching package: 'caret'\n#> The following objects are masked from 'package:DescTools':\n#> \n#>     MAE, RMSE\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n\n# fit the model with formula = out.formula2\n# use training method glm (have to specify family)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\")\nfit.cv.bin\n#> Generalized Linear Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7545115  0.4659618  0.8535653\n\n\nExtract results from each test data\n\nShow the codesummary.res <- fit.cv.bin$resample\nsummary.res\n\n\n\n  \n\n\nShow the codemean(fit.cv.bin$resample$ROC)\n#> [1] 0.7545115\nsd(fit.cv.bin$resample$ROC)\n#> [1] 0.01651437\n\n\nMore options\n\nShow the codectrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\",\n              preProc = c(\"center\", \"scale\"))\nfit.cv.bin\n#> Generalized Linear Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> Pre-processing: centered (63), scaled (63) \n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7548047  0.4629717  0.8530367\n\n\nVariable selection\nWe can also use stepwise regression that uses AIC as a criterion.\n\nShow the codeset.seed(504)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin.aic<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmStepAIC\",\n               direction =\"backward\",\n              family = binomial(),\n              metric=\"ROC\")\n\n\n\nShow the codefit.cv.bin.aic\n#> Generalized Linear Model with Stepwise Feature Selection \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens      Spec     \n#>   0.7540424  0.464468  0.8562535\nsummary(fit.cv.bin.aic)\n#> \n#> Call:\n#> NULL\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.8626  -0.9960   0.5052   0.8638   1.9578  \n#> \n#> Coefficients:\n#>                                          Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)                             1.0783624  0.7822168   1.379 0.168019\n#> Disease.categoryOther                   0.4495099  0.0919860   4.887 1.03e-06\n#> `CancerLocalized (Yes)`                 1.8942512  0.5501880   3.443 0.000575\n#> CancerMetastatic                        3.2703316  0.5858715   5.582 2.38e-08\n#> Cardiovascular1                         0.2386749  0.0939617   2.540 0.011081\n#> Congestive.HF1                          0.4539010  0.0971624   4.672 2.99e-06\n#> Dementia1                               0.2380213  0.1162903   2.047 0.040679\n#> Hepatic1                                0.3593093  0.1541762   2.331 0.019779\n#> Tumor1                                 -1.2455123  0.5542624  -2.247 0.024630\n#> Immunosupperssion1                      0.2174294  0.0730803   2.975 0.002928\n#> Transfer.hx1                           -0.1849029  0.0945679  -1.955 0.050555\n#> `age[50,60)`                            0.3621248  0.0984288   3.679 0.000234\n#> `age[60,70)`                            0.6941924  0.0968434   7.168 7.60e-13\n#> `age[70,80)`                            0.6804939  0.1126637   6.040 1.54e-09\n#> `age[80, Inf)`                          0.9833851  0.1410563   6.972 3.13e-12\n#> sexFemale                              -0.2805950  0.0653527  -4.294 1.76e-05\n#> DASIndex                               -0.0429272  0.0062191  -6.902 5.11e-12\n#> APACHE.score                            0.0174907  0.0020017   8.738  < 2e-16\n#> Glasgow.Coma.Score                      0.0093657  0.0012563   7.455 9.00e-14\n#> WBC                                     0.0044518  0.0030090   1.479 0.139009\n#> Temperature                            -0.0524703  0.0192757  -2.722 0.006487\n#> PaO2vs.FIO2                             0.0004741  0.0003054   1.552 0.120548\n#> Hematocrit                             -0.0154796  0.0041593  -3.722 0.000198\n#> Bilirubin                               0.0313087  0.0094004   3.331 0.000867\n#> Weight                                 -0.0031548  0.0011213  -2.813 0.004902\n#> DNR.statusYes                           0.9347360  0.1326924   7.044 1.86e-12\n#> Medical.insuranceMedicare               0.4764895  0.1257582   3.789 0.000151\n#> `Medical.insuranceMedicare & Medicaid`  0.3364916  0.1584757   2.123 0.033729\n#> `Medical.insuranceNo insurance`         0.3711345  0.1568820   2.366 0.017996\n#> Medical.insurancePrivate                0.2632637  0.1139805   2.310 0.020903\n#> `Medical.insurancePrivate & Medicare`   0.2819715  0.1313101   2.147 0.031764\n#> Respiratory.DiagYes                     0.1393974  0.0769026   1.813 0.069886\n#> Cardiovascular.DiagYes                  0.1804967  0.0836679   2.157 0.030982\n#> Neurological.DiagYes                    0.4320266  0.1189357   3.632 0.000281\n#> Gastrointestinal.DiagYes                0.2819563  0.1092206   2.582 0.009836\n#> Hematologic.DiagYes                     0.9734424  0.1651363   5.895 3.75e-09\n#> Sepsis.DiagYes                          0.1539651  0.0943235   1.632 0.102614\n#> `incomeUnder $11k`                      0.2151437  0.0689392   3.121 0.001804\n#> RHC.use                                 0.3552053  0.0713632   4.977 6.44e-07\n#>                                           \n#> (Intercept)                               \n#> Disease.categoryOther                  ***\n#> `CancerLocalized (Yes)`                ***\n#> CancerMetastatic                       ***\n#> Cardiovascular1                        *  \n#> Congestive.HF1                         ***\n#> Dementia1                              *  \n#> Hepatic1                               *  \n#> Tumor1                                 *  \n#> Immunosupperssion1                     ** \n#> Transfer.hx1                           .  \n#> `age[50,60)`                           ***\n#> `age[60,70)`                           ***\n#> `age[70,80)`                           ***\n#> `age[80, Inf)`                         ***\n#> sexFemale                              ***\n#> DASIndex                               ***\n#> APACHE.score                           ***\n#> Glasgow.Coma.Score                     ***\n#> WBC                                       \n#> Temperature                            ** \n#> PaO2vs.FIO2                               \n#> Hematocrit                             ***\n#> Bilirubin                              ***\n#> Weight                                 ** \n#> DNR.statusYes                          ***\n#> Medical.insuranceMedicare              ***\n#> `Medical.insuranceMedicare & Medicaid` *  \n#> `Medical.insuranceNo insurance`        *  \n#> Medical.insurancePrivate               *  \n#> `Medical.insurancePrivate & Medicare`  *  \n#> Respiratory.DiagYes                    .  \n#> Cardiovascular.DiagYes                 *  \n#> Neurological.DiagYes                   ***\n#> Gastrointestinal.DiagYes               ** \n#> Hematologic.DiagYes                    ***\n#> Sepsis.DiagYes                            \n#> `incomeUnder $11k`                     ** \n#> RHC.use                                ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 7433.3  on 5734  degrees of freedom\n#> Residual deviance: 6198.0  on 5696  degrees of freedom\n#> AIC: 6276\n#> \n#> Number of Fisher Scoring iterations: 5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "machinelearning5.html",
    "href": "machinelearning5.html",
    "title": "Supervised learning",
    "section": "",
    "text": "In this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods.\nIn the first code chunk, we load necessary R libraries that will be utilized throughout the chapter for various machine learning methods and data visualization.\nRead previously saved data\nThe second chunk is dedicated to reading previously saved data and formulas from specified file paths, ensuring that the dataset and predefined formulas are available for subsequent analyses.\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nlevels(ObsData$Death)=c(\"No\",\"Yes\")\nout.formula1 <- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula2 <- readRDS(file = \"Data/machinelearning/form2.RDS\")\n\n\nContinuous outcome\nCross-validation LASSO\n\n\n\n\n\nIn this code chunk, we implement a machine learning model training process with a focus on utilizing cross-validation and tuning parameters to optimize the model. Cross-validation is a technique used to assess how well the model will generalize to an independent dataset by partitioning the original dataset into a training set to train the model, and a test set to evaluate it. Here, we specify that we are using a particular type of cross-validation, denoted as “cv”, and that we will be creating 5 folds (or partitions) of the data, as indicated by number = 5.\nThe model being trained is specified to use a method known as “glmnet”, which is capable of performing lasso, ridge, and elastic net regularization regressions. Tuning parameters are crucial in controlling the behavior of our learning algorithm. In this instance, we specify lambda and alpha as our tuning parameters, which control the amount of regularization applied to the model and the mixing percentage between lasso and ridge regression, respectively. The tuneGrid argument is used to specify the exact values of alpha and lambda that the model should consider during training. The verbose = FALSE argument ensures that additional model training details are not printed during the training process. Finally, the trained model is stored in an object for further examination and use.\n\nShow the codectrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <- train(out.formula1, \n                    trControl = ctrl,\n                    data = ObsData, method = \"glmnet\",\n                    lambda= 0,\n                    tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                    verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.08162  0.05943728  15.17943\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\n\nCross-validation Ridge\nSubsequent code chunks explore Ridge regression and Elastic Net, employing similar methodologies but adjusting tuning parameters accordingly.\n\nShow the codectrl <- trainControl(method = \"cv\", number = 5)\nfit.cv.con <-train(out.formula1, \n                   trControl = ctrl,\n                   data = ObsData, method = \"glmnet\",\n                   lambda= 0,\n                   tuneGrid = expand.grid(alpha = 0, lambda = 0),\n                   verbose = FALSE)\nfit.cv.con\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4587, 4588, 4589, 4588 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared    MAE     \n#>   25.17466  0.05556026  15.22318\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\n\nBinary outcome\nCross-validation LASSO\nWe then shift to binary outcomes, exploring LASSO and Ridge regression with similar implementations but adjusting for the binary nature of the outcome variable.\n\nShow the codectrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, \n                  trControl = ctrl,\n                  data = ObsData, \n                  method = \"glmnet\",\n                  lambda= 0,\n                  tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                  verbose = FALSE,\n                  metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4588, 4588, 4588, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7539886  0.4684678  0.8549188\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 1\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\n\n\nNot okay to select variables from a shrinkage model, and then use them in a regular regression\nCross-validation Ridge\n\nShow the codectrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               lambda= 0,\n               tuneGrid = expand.grid(alpha = 0,  \n                                      lambda = 0),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7527498  0.4625026  0.8519672\n#> \n#> Tuning parameter 'alpha' was held constant at a value of 0\n#> Tuning\n#>  parameter 'lambda' was held constant at a value of 0\n\n\nCross-validation Elastic net\n\nAlpha = mixing parameter\nLambda = regularization or tuning parameter\nWe can use expand.grid for model tuning\n\n\nShow the codectrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  \n                                      lambda = seq(0.05,0.3,by = 0.05)),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> glmnet \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4589, 4587, 4588, 4589, 4587 \n#> Resampling results across tuning parameters:\n#> \n#>   alpha  lambda  ROC        Sens          Spec     \n#>   0.10   0.05    0.7506071  0.3701097490  0.8968301\n#>   0.10   0.10    0.7473587  0.2742318186  0.9365934\n#>   0.10   0.15    0.7427385  0.1753688135  0.9674894\n#>   0.10   0.20    0.7374742  0.0914089602  0.9868355\n#>   0.10   0.25    0.7309831  0.0223584312  0.9959703\n#>   0.10   0.30    0.7245844  0.0009937904  0.9994624\n#>   0.15   0.05    0.7496521  0.3502388800  0.9067699\n#>   0.15   0.10    0.7432272  0.2170956631  0.9527131\n#>   0.15   0.15    0.7345819  0.1063121119  0.9836115\n#>   0.15   0.20    0.7238424  0.0188832512  0.9957018\n#>   0.15   0.25    0.7166706  0.0004962779  1.0000000\n#>   0.15   0.30    0.7110471  0.0000000000  1.0000000\n#>   0.20   0.05    0.7481001  0.3283853684  0.9169806\n#>   0.20   0.10    0.7377727  0.1639433107  0.9680284\n#>   0.20   0.15    0.7234677  0.0382541387  0.9908656\n#>   0.20   0.20    0.7140924  0.0009937904  0.9994627\n#>   0.20   0.25    0.7074353  0.0000000000  1.0000000\n#>   0.20   0.30    0.6970420  0.0000000000  1.0000000\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were alpha = 0.1 and lambda = 0.05.\nplot(fit.cv.bin)\n\n\n\n\nDecision tree\nDecision trees are then introduced and implemented, with visualizations and evaluation metrics provided to assess their performance.\n\nDecision tree\n\nReferred to as Classification and regression trees or CART\nCovers\n\nClassification (categorical outcome)\nRegression (continuous outcome)\n\n\nFlexible to incorporate non-linear effects automatically\n\nNo need to specify higher order terms / interactions\n\n\nUnstable, prone to overfitting, suffers from high variance\n\n\n\nSimple CART\n\nShow the coderequire(rpart)\nsummary(ObsData$DASIndex) # Duke Activity Status Index\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   11.00   16.06   19.75   20.50   23.43   33.00\ncart.fit <- rpart(Death~DASIndex, data = ObsData)\npar(mfrow = c(1,1), xpd = NA)\nplot(cart.fit)\ntext(cart.fit, use.n = TRUE)\n\n\n\nShow the codeprint(cart.fit)\n#> n= 5735 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 5735 2013 Yes (0.3510026 0.6489974)  \n#>   2) DASIndex>=24.92383 1143  514 No (0.5503062 0.4496938)  \n#>     4) DASIndex>=29.14648 561  199 No (0.6452763 0.3547237) *\n#>     5) DASIndex< 29.14648 582  267 Yes (0.4587629 0.5412371) *\n#>   3) DASIndex< 24.92383 4592 1384 Yes (0.3013937 0.6986063) *\nrequire(rattle)\nrequire(rpart.plot)\nrequire(RColorBrewer)\nfancyRpartPlot(cart.fit, caption = NULL)\n\n\n\n\nAUC\n\nShow the coderequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5912\nplot(rocobj)\n\n\n\nShow the codeauc(rocobj)\n#> Area under the curve: 0.5912\n\n\nComplex CART\nMore variables\n\nShow the codeout.formula2\n#> Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#>     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#>     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#>     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#>     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#>     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#>     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#>     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#>     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#>     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#>     RHC.use\nrequire(rpart)\ncart.fit <- rpart(out.formula2, data = ObsData)\n\n\nCART Variable importance\n\nShow the codecart.fit$variable.importance\n#>            DASIndex              Cancer               Tumor                 age \n#>         123.2102455          33.4559400          32.5418433          24.0804860 \n#>   Medical.insurance                 WBC                 edu Cardiovascular.Diag \n#>          14.5199953           5.6673997           3.7441554           3.6449371 \n#>          Heart.rate      Cardiovascular         Trauma.Diag               PaCo2 \n#>           3.4059248           3.1669125           0.5953098           0.2420672 \n#>           Potassium              Sodium             Albumin \n#>           0.2420672           0.2420672           0.1984366\n\n\nAUC\n\nShow the coderequire(pROC)\nobs.y2<-ObsData$Death\npred.y2 <- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj <- roc(obs.y2, pred.y2)\n#> Setting levels: control = No, case = Yes\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = obs.y2, predictor = pred.y2)\n#> \n#> Data: pred.y2 in 2013 controls (obs.y2 No) < 3722 cases (obs.y2 Yes).\n#> Area under the curve: 0.5981\nplot(rocobj)\n\n\n\nShow the codeauc(rocobj)\n#> Area under the curve: 0.5981\n\n\nCross-validation CART\n\nShow the codeset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"rpart\",\n              metric=\"ROC\")\nfit.cv.bin\n#> CART \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   cp           ROC        Sens       Spec     \n#>   0.007203179  0.6304911  0.2816488  0.9086574\n#>   0.039741679  0.5725283  0.2488649  0.8981807\n#>   0.057128664  0.5380544  0.1287804  0.9473284\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final value used for the model was cp = 0.007203179.\n# extract results from each test data \nsummary.res <- fit.cv.bin$resample\nsummary.res\n\n\n\n  \n\n\n\nEnsemble methods (Type I)\nWe explore ensemble methods, specifically bagging and boosting, through implementation and evaluation in the context of binary outcomes.\nTraining same model to different samples (of the same data)\nCross-validation bagging\n\nBagging or bootstrap aggregation\n\nindependent bootstrap samples (sampling with replacement, B times),\napplies CART on each i (no prunning)\nAverage the resulting predictions\nReduces variance as a result of using bootstrap\n\n\n\n\nShow the codeset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"bag\",\n               bagControl = bagControl(fit = ldaBag$fit, \n                                       predict = ldaBag$pred, \n                                       aggregate = ldaBag$aggregate),\n               metric=\"ROC\")\n#> Warning: executing %dopar% sequentially: no parallel backend registered\nfit.cv.bin\n#> Bagged Model \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7506666  0.4485809  0.8602811\n#> \n#> Tuning parameter 'vars' was held constant at a value of 63\n\n\n\nBagging improves prediction accuracy\n\nover prediction using a single tree\n\n\nLooses interpretability\n\nas this is an average of many diagrams now\n\n\nBut we can get a summary of the importance of each variable\n\nBagging Variable importance\n\nShow the codecaret::varImp(fit.cv.bin, scale = FALSE)\n#> ROC curve variable importance\n#> \n#>   only 20 most important variables shown (out of 50)\n#> \n#>                    Importance\n#> age                    0.6159\n#> APACHE.score           0.6140\n#> DASIndex               0.5962\n#> Cancer                 0.5878\n#> Creatinine             0.5835\n#> Tumor                  0.5807\n#> blood.pressure         0.5697\n#> Glasgow.Coma.Score     0.5656\n#> Disease.category       0.5641\n#> Temperature            0.5584\n#> DNR.status             0.5572\n#> Hematocrit             0.5525\n#> Weight                 0.5424\n#> Bilirubin              0.5397\n#> income                 0.5319\n#> Immunosupperssion      0.5278\n#> RHC.use                0.5263\n#> Dementia               0.5252\n#> Congestive.HF          0.5250\n#> Hematologic.Diag       0.5250\n\n\nCross-validation boosting\n\nBoosting\n\nsequentially updated/weighted bootstrap based on previous learning\n\n\n\n\nShow the codeset.seed(504)\nrequire(caret)\nctrl<-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin<-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"gbm\",\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#> Stochastic Gradient Boosting \n#> \n#> 5735 samples\n#>   50 predictor\n#>    2 classes: 'No', 'Yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  ROC        Sens       Spec     \n#>   1                   50      0.7218938  0.2145970  0.9505647\n#>   1                  100      0.7410292  0.2980581  0.9234228\n#>   1                  150      0.7483014  0.3487142  0.9030028\n#>   2                   50      0.7414513  0.2960631  0.9263816\n#>   2                  100      0.7534264  0.3869684  0.8917212\n#>   2                  150      0.7575826  0.4187512  0.8777477\n#>   3                   50      0.7496078  0.3626125  0.9070358\n#>   3                  100      0.7579645  0.4078244  0.8764076\n#>   3                  150      0.7637074  0.4445909  0.8702298\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were n.trees = 150, interaction.depth =\n#>  3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\nShow the codeplot(fit.cv.bin)\n\n\n\n\nEnsemble methods (Type II)\nWe introduce the concept of Super Learner, providing external resources for further exploration.\nTraining different models on the same data\nSuper Learner\n\nLarge number of candidate learners (CL) with different strengths\n\nParametric (logistic)\nNon-parametric (CART)\n\n\nCross-validation: CL applied on training data, prediction made on test data\nFinal prediction uses a weighted version of all predictions\n\nWeights = coef of Observed outcome ~ prediction from each CL\n\n\nSteps\nRefer to this tutorial for steps and examples!\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "machinelearning6.html",
    "href": "machinelearning6.html",
    "title": "Unsupervised learning",
    "section": "",
    "text": "In this chapter, we will talk about unsupervised learning.\nIn the initial code chunk, we load a specific library that will be utilized for publishing-related functionality throughout the chapter.\nClustering\nClustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity.\nGroup characteristics include (to the extent that is possible)\n\nlow inter-class similarity: observation from different clusters would be dissimilar\nhigh intra-class similarity: observation from the same cluster would be similar\n\nWithin-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances (Wikipedia 2023a)\n\n\n\n\n\nK-means\nK-means is a very popular clustering algorithm, that partitions the data into \\(k\\) groups.\nAlgorithm:\n\nDetermine a number \\(k\\) (e.g., could be 3)\nrandomly select \\(k\\) subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster.\nBy Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong.\ncompute new mean value for each cluster.\nbased on this new mean, try to determine again in which cluster the data points belong.\nprocess continues until the data points do not change cluster membership.\nRead previously saved data\nWe read a previously saved dataset from a specified file path.\n\nShow the codeObsData <- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\n\nIn the next few code chunks, we implement k-means clustering on various subsets of the data, visualizing the results and displaying the cluster centers. The first example uses two variables, the second example uses three, and in the third example, a larger subset of variables is selected but not immediately utilized in the clustering. In the subsequent code chunk, we apply k-means clustering to the larger subset of variables, displaying various results and aggregating data by cluster to display mean and standard deviation values for each variable within each cluster.\nExample 1\n\nShow the codedatax0 <- ObsData[c(\"Heart.rate\", \"edu\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   Heart.rate      edu\n#> 1  134.96277 11.75466\n#> 2   54.55138 11.44494\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\nExample 2\n\nShow the codedatax0 <- ObsData[c(\"blood.pressure\", \"Heart.rate\", \"Respiratory.rate\")]\nkres0 <- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#>   blood.pressure Heart.rate Respiratory.rate\n#> 1       73.71684   54.95789         22.76723\n#> 2       80.10812  135.08956         29.85267\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\nExample with many variables\n\nShow the codedatax <- ObsData[c(\"edu\", \"blood.pressure\", \"Heart.rate\", \n                   \"Respiratory.rate\" , \"Temperature\",\n                   \"PH\", \"Weight\", \"Length.of.Stay\")]\n\n\n\nShow the codekres <- kmeans(datax, centers = 3)\n#kres\nhead(kres$cluster)\n#> [1] 1 1 1 3 1 2\nkres$size\n#> [1] 2795 1688 1252\nkres$centers\n#>        edu blood.pressure Heart.rate Respiratory.rate Temperature       PH\n#> 1 11.85665       54.28086  136.34597         29.75277    37.85056 7.385267\n#> 2 11.54214      128.33886  126.12026         29.36611    37.68129 7.401027\n#> 3 11.46447       65.46446   53.17332         22.66717    37.01512 7.378432\n#>     Weight Length.of.Stay\n#> 1 68.67307       23.41789\n#> 2 66.68351       20.68128\n#> 3 67.48365       18.59425\naggregate(datax, by = list(cluster = kres$cluster), mean)\n\n\n\n  \n\n\nShow the codeaggregate(datax, by = list(cluster = kres$cluster), sd)\n\n\n\n  \n\n\n\nOptimal number of clusters\nNext, we explore determining the optimal number of clusters, visualizing the total within-cluster sum of squares for different values of k and indicating a chosen value of k with a vertical line on the plot.\n\nShow the coderequire(factoextra)\nfviz_nbclust(datax, kmeans, method = \"wss\")+\n  geom_vline(xintercept=3,linetype=3)\n\n\n\n\nHere the vertical line is chosen based on elbow method (Wikipedia 2023b).\nDiscussion\n\nWe need to supply a number, \\(k\\): but we can test different \\(k\\)s to identify optimal value\nClustering can be influenced by outliners, so median based clustering is possible\nmere ordering can influence clustering, hence we should choose different initial means (e.g., nstart should be greater than 1).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nWikipedia. 2023a. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n\n\n———. 2023b. “Elbow Method (Clustering).” https://en.wikipedia.org/wiki/Elbow_method_(clustering)."
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R functions (L)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n fancyRpartPlot \n    rattle \n    To plot an rpart object \n  \n\n fviz_nbclust \n    factoextra \n    To visualize the optimal number of clusters \n  \n\n kmeans \n    base/stats \n    To conduct K-Means cluster analysis \n  \n\n lowess \n    base/stats \n    To perform scatter plot smoothing aka lowess smoothing \n  \n\n rpart \n    rpart \n    To fit a classification tree (CART) \n  \n\n terms \n    base/stats \n    To extarct terms objects \n  \n\n varImp \n    caret \n    To calculate the variable importance measure"
  },
  {
    "objectID": "machinelearningCausal.html#background",
    "href": "machinelearningCausal.html#background",
    "title": "Causal learning",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "machinelearningCausal.html#overview-of-tutorials",
    "href": "machinelearningCausal.html#overview-of-tutorials",
    "title": "Causal learning",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\nThis chapter is going to be about:\n\nMachine learning and their use in causal inference (Karim and Frank 2021)\nTMLE in medical research (Frank and Karim 2023)\n\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\nReferences\n\n\n\n\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. “Implementing TMLE in the Presence of a Continuous Outcome.” Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nKarim, Ehsan, and Hanna Frank. 2021. ehsanx/TMLEworkshop: R Guide for TMLE in Medical Research (version v1.1). Zenodo. https://doi.org/10.5281/zenodo.5246085."
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R functions (C)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\n Function.name \n    Package.name \n    Use \n  \n\n\n ExtractSmd \n    tableone \n    To extract the standardized mean differences of a tableone object \n  \n\n listWrappers \n    SuperLearner \n    To see the list of wrapper functions, i.e., list of learners, in SuperLearner \n  \n\n Match \n    Matching \n    To match an exposed/treated to M unexposed/controls"
  },
  {
    "objectID": "reporting.html#background",
    "href": "reporting.html#background",
    "title": "Writing Tools",
    "section": "Background",
    "text": "Background\nThe tutorial offers a detailed guide on using R, RStudio, and GitHub for collaborative manuscript writing and management, covering aspects from software installation, through manuscript creation and editing, to publishing and sharing via GitHub Pages. Additionally, it introduces various supplementary resources, including LaTex, TablesGenerator, and Zotero, to further assist and streamline the research and writing processes.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder"
  },
  {
    "objectID": "reporting.html#overview-of-tutorials",
    "href": "reporting.html#overview-of-tutorials",
    "title": "Writing Tools",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nScientific Writing Collaboratively\nIn this tutorial, we navigated through a comprehensive guide on utilizing R, RStudio, and GitHub for collaborative manuscript writing. Beginning with the installation of R and RStudio, we explored creating a GitHub repository and managing it using GitHub Desktop. Subsequently, we delved into using RStudio to create a manuscript with a Bookdown template, compiling it, and updating the manuscript content. The tutorial also covered updating the GitHub repository with manuscript changes and publishing the manuscript using GitHub Pages, enabling a seamless, collaborative, and organized manuscript writing and sharing process, ensuring efficiency and ease in academic writing collaborations.\n\n\nFormatting Resources\nThis section provides a wealth of tools and platforms beneficial for researchers and writers in managing and creating scientific documents. It encompasses LaTex and ShareLaTeX for document preparation, TablesGenerator for converting tables from MS Word to various formats, and R packages like “officer” and “flextable” for generating tables and charts. It also introduces draw.io for crafting flow charts, platforms like jane for identifying suitable journals, officetimeline for creating Gantt charts, and Google Docs for real-time collaborative writing. Moreover, it highlights Zotero and ZoteroBib as comprehensive tools for reference management and bibliography creation, facilitating organized, collaborative, and streamlined research and writing processes.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "reporting1.html",
    "href": "reporting1.html",
    "title": "Collaborative Writing",
    "section": "",
    "text": "This tutorial provides a step-by-step guide to using R, RStudio, GitHub, and GitHub Desktop for collaborative manuscript writing. These tools facilitate collaboration and help keep track of the manuscript writing progress.\nAll necessary files for this tutorial are available here. If you’re new to Rmarkdown, consider revisiting the introductory tutorial.\n\nGitHub Account\n\nVisit GitHub and click “Sign up”, and follow the instructions to create your account.\nOn GitHub, click the “+” or “new” icon on the top right and select “New repository”.\nName your repository, add a description, initialize it with a README, and click “Create repository”.\n\n\n\nNecessary Software\n\nR and RStudio (revisit previous chapter)\nGitHub Desktop. Install and sign in with your GitHub credentials.\n\n\n\nCloning Repository\n\nOn GitHub Desktop, go to “File” > “Clone repository”,\nchoose the repository you want to clone and select the local path,\nand click “Clone”.\nNavigate to the local path and ensure all files are cloned.\n\n\n\nBookdown Template in RStudio\n\nIn RStudio, go to “File” > “New Project” > “New Directory” > “Book Project using bookdown”.\nName your project (e.g., “test1”) and create it.\nNavigate to the project directory, copy all files, and paste them into your original repository folder.\nRename the project file to a suitable name (e.g., “template”).\n\n\n\nCompiling\n\nOpen the index file in your RStudio project.\nUpdate the YAML header and chapter/section names as per your manuscript.\nCompile the document into HTML and/or PDF using the “Build” tab in RStudio.\n\n\n\nUpdating GitHub Repository\n\nMake changes to your manuscript files in RStudio.\nOpen GitHub Desktop, review changes, commit them with a descriptive message, and push to the origin.\n\n\n\nPublishing Using GitHub Pages\n\nIn your GitHub repository, create a new folder named “docs” and copy your compiled HTML files into it.\nGo to the settings of your GitHub repository, navigate to “GitHub Pages”, and set the source to the “docs” folder.\nYour manuscript will be available at “username.github.io/repository_name”.\n\n\n\nInvite collaborators\nTo invite collaborators to a GitHub repository, begin by navigating to your desired repository and clicking on the “Settings” tab. In the left sidebar, select “Manage Access” and click “Invite a collaborator.” Enter the username, full name, or email of the person you wish to invite and select them from the suggestions. Assign a role to define their access level and send the invitation. The invitee can accept via the email link sent or directly through their GitHub account. You can review and manage all collaborators and pending invitations in the “Manage Access” section.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\nUseful resources\n\nManuscript template. The output website can be accessed here"
  },
  {
    "objectID": "reporting2.html",
    "href": "reporting2.html",
    "title": "Formatting Tools",
    "section": "",
    "text": "The tutorial elucidates a variety of tools and methodologies aimed at streamlining and enhancing academic writing and presentation creation, discussing topics such as utilizing a typesetting system, converting tables across different formats, employing various R packages for enhanced data visualization and presentation, drawing flow and Gantt charts, crafting HTML5 presentations, enabling collaborative writing and document sharing, managing references efficiently, formatting articles, and employing specific platforms for identifying appropriate journals for publishing academic articles.\nLaTex\nGet an account in ShareLaTeX/Overleaf.\nTable conversion\nFrom MS word to latex / markdown / HTML in TablesGenerator.\nYou can use tableone package to generate csv file, and then import them in the TablesGenerator to convert to HTML to paste to doc file!\n\nShow the codetab1x <- print(tab1, quote = FALSE, noSpaces = TRUE, printToggle = FALSE)\nwrite.csv(tab1x, file = \"tab1x.csv\")\n\n\nFancy table and chart generators:\nR Packages\n\nofficer\nofficedown\nflextable\nmschart\nDrawing flow chart\ndraw.io\nPresentation\nThe “xaringan” package, derived from a love for the Japanese manga and anime “Naruto”, serves as an R Markdown extension, and it facilitates the creation of distinctively styled HTML5 presentations by leveraging the JavaScript library remark.js. Originating from an intent to produce a unique, though not widely adopted style, due to its potentially challenging pronunciation unless familiar with the anime, “xaringan” offers significant customizability in presentation design and has garnered additional theme contributions from its user community. Despite only supporting Markdown, “xaringan” enhances remark.js by introducing support for R Markdown and additional utilities, simplifying the slide-building and previewing processes. Further insights into “xaringan”, its background, and its utility can be explored through its documentation.\nGantt charts\nofficetimeline\nSimultaneous collaborative writing\nGoogle Docs offers a platform for real-time collaborative writing. Multiple users can edit documents simultaneously, and changes are saved automatically.\n\nCommenting and Suggesting: Use the comment and suggest features to provide feedback without altering the original text.\nRevision History: Navigate through the revision history to view changes and revert to previous versions if needed.\nSharing and Permissions: Manage who can view, comment, or edit the document with varied permission levels.\nReference manager\nZotero stands out as a free, open-source reference management software that assists researchers, academics, and students in organizing, managing, and formatting their citations and bibliographies. It’s not just a reference manager but also a powerful tool for collaborative work on research projects. Try the Zotero desktop manager as well for assisting with reference inserting.\nZotero syncs data across devices, ensuring that users can access their libraries from any location. Users can work offline with Zotero, and any changes made will be synchronized when the internet connection is restored.\nZoteroBib: Use ZoteroBib to generate bibliographies instantly without creating an account or installing software.\nArticle formatting\nThe rticles package in R provides a diverse selection of templates for creating academic articles and is easily accessible directly within the RStudio environment by navigating through File -> New File -> R Markdown, where users can select their desired template. For users not utilizing RStudio, the installation of Pandoc is requisite, with articles being creatable using the rmarkdown::draft() function, and specifying the template and package parameters as needed. Additionally, the package enables viewing a list of available journal names using rticles::journals(). To employ enhanced features, such as automatic figure numbering and cross-referencing of tables, users can utilize functionalities from the bookdown package. This involves adjusting the YAML to use bookdown::pdf_book as the output format, and designating the chosen rticles template as the base_format. Comprehensive details and tutorials regarding the use of the rticles package can be found in its online documentation. The complete array of options can be explored within the R Markdown templates window in RStudio, via the packages’s GitHub readme or accessed programmatically via the following function:\n\nShow the coderticles::journals()\n#>  [1] \"acm\"            \"acs\"            \"aea\"            \"agu\"           \n#>  [5] \"ajs\"            \"amq\"            \"ams\"            \"arxiv\"         \n#>  [9] \"asa\"            \"bioinformatics\" \"biometrics\"     \"copernicus\"    \n#> [13] \"ctex\"           \"elsevier\"       \"frontiers\"      \"glossa\"        \n#> [17] \"ieee\"           \"ims\"            \"informs\"        \"iop\"           \n#> [21] \"isba\"           \"jasa\"           \"jedm\"           \"joss\"          \n#> [25] \"jss\"            \"lipics\"         \"lncs\"           \"mdpi\"          \n#> [29] \"mnras\"          \"oup_v0\"         \"oup_v1\"         \"peerj\"         \n#> [33] \"pihph\"          \"plos\"           \"pnas\"           \"rjournal\"      \n#> [37] \"rsos\"           \"rss\"            \"sage\"           \"sim\"           \n#> [41] \"springer\"       \"tf\"             \"trb\"            \"wellcomeor\"\n\n\nFind appropriate journals\n\njane\nSee more extensive list here\n\nSpecific Epidemiology-focus journals\n\nSummary\n\n\nCategory\nTool\nDescription\n\n\n\nAcademic Search Engine\nGoogle Scholar\nFreely accessible search engine indexing scholarly literature.\n\n\nArticle Formatting\nrticles (R Package)\nR package providing templates for various academic journals.\n\n\nBrainstorming & Mapping\nMindMeister\nOnline mind mapping tool for brainstorming and collaborative visualization.\n\n\nDocument Creation & Editing\nOverleaf\nCollaborative LaTeX editor online.\n\n\n\nGoogle Docs\nPlatform for real-time collaborative writing.\n\n\n\nofficer (R Package)\nR package for generating Word and PowerPoint files.\n\n\n\nofficedown (R Package)\nR package to produce Word documents with officer.\n\n\nGantt Chart Generators\nOffice Timeline\nOnline tool for creating Gantt charts.\n\n\nJournal Finding\njane\nTool to assist in finding the right journal for publishing.\n\n\n\nCustom List\nExtensive list and guide on finding suitable journals.\n\n\n\nEpidemiology Journals\nSpecific list of epidemiology-focus journals.\n\n\nNote & Research Management\nEvernote\nNote-taking and organization tool for managing research notes and drafts.\n\n\nPresentation & Sharing\nPrezi\nDynamic and visually engaging presentation creation tool.\n\n\n\nSlideShare\nPlatform for sharing presentations and professional documents.\n\n\nPresentation\nxaringan (R Package)\nR Markdown extension for creating presentations using remark.js.\n\n\nProject Management\nAsana\nProject management tool for workflow organization and collaboration.\n\n\n\nTrello\nProject management tool for task tracking and collaboration.\n\n\nReference Management\nEndNote\nReference management software for organizing and integrating references.\n\n\n\nJabRef\nOpen-source bibliography reference manager using BibTeX.\n\n\n\nMendeley\nReference management and academic social network.\n\n\n\nPaperpile\nReference management and academic research library.\n\n\n\nZotero\nReference management and collaborative tool.\n\n\n\nZoteroBib\nQuick bibliography generation tool.\n\n\nResearch Identity Management\nORCID\nProvides a persistent digital identifier to distinguish researchers.\n\n\nTable & Chart Generators\ndraw.io\nOnline diagram software for flow charts and various diagrams.\n\n\n\nTablesGenerator\nConverts tables to LaTeX, markdown, HTML formats.\n\n\n\nflextable (R Package)\nR package for tabular reporting in various formats (Word, HTML, etc.).\n\n\n\nmschart (R Package)\nR package to create PowerPoint charts.\n\n\nWriting & Editing\nAuthorea\nCollaborative platform for writing, citing, and publishing.\n\n\n\nGrammarly\nWriting assistant for grammar and style enhancement."
  }
]