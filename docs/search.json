[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "",
    "text": "The Project\nWelcome to a place crafted to bridge a unique gap in the health research world. This website offers valuable resources for those who are taking their first steps into health research and advanced statistics. Even if you’re familiar with health research, but advanced statistical methods seem daunting, you’re in the right place. Here, we offer:\nThis hub is a part of an open educational initiative, meaning it’s available to everyone. We hope to uplift the standard of health research methodology through this endeavor.\nWe’re on a mission to:"
  },
  {
    "objectID": "index.html#dive-into-our-modules",
    "href": "index.html#dive-into-our-modules",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Dive into Our Modules",
    "text": "Dive into Our Modules\nEmbark on a journey through 10 core learning modules, including one introductory module about R:\n\n\n\n\n\n Module \n    Topics \n    Descriptions \n  \n\n\n 0 \n    R Data Wrangling \n    Get to know R. \n  \n\n 1 \n    Survey Data Resources \n    Understand and source reliable national survey data. \n  \n\n 2 \n    Crafting Data for Research \n    Customize data to your research query. \n  \n\n 3 \n    Grasping confounding \n    Delve into the concept of confounding and its implications. \n  \n\n 4 \n    Adjustment Techniques \n    Adjusting for covariates in a multivariate analysis. \n  \n\n 5 \n    Complex Survey Analysis \n    Handle intricate data sets. \n  \n\n 6 \n    Missing Data \n    Understand and tackle gaps in your data. \n  \n\n 7 \n    Propensity Score Analysis \n    Dive deeper into advanced analyses. \n  \n\n 8 \n    Reporting Guideline \n    Guidelines to share your findings. \n  \n\n 9 \n    Machine Learning \n    Introduction to key concepts, algorithms, and applications. \n  \n\n 10 \n    Machine Learners in Causal Inference \n    Discusses the potential pitfalls and challenges in merging machine learning with causal inference, and a way forward."
  },
  {
    "objectID": "index.html#how-our-content-is-presented",
    "href": "index.html#how-our-content-is-presented",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "How Our Content is Presented",
    "text": "How Our Content is Presented\nAll our resources are hosted on an easy-to-access GitHub page. The format? Engaging text, reproducible software codes, clear analysis outputs, and crisp videos that distill complex topics. And don’t miss our quiz section at the end of each module for a quick self-check on what you’ve learned. This document is created using quarto and R."
  },
  {
    "objectID": "index.html#open-copyright-license",
    "href": "index.html#open-copyright-license",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Open Copyright License",
    "text": "Open Copyright License\nCC-BY"
  },
  {
    "objectID": "index.html#contributor-list",
    "href": "index.html#contributor-list",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Contributor list",
    "text": "Contributor list\n\n\n\n\n\n\nTip\n\n\n\nDive into this captivating content, brought to life with the generous support of the UBC OER Fund Implementation Grant and further supported by UBC SPPH. The foundation of this content traces back to the PI’s work over five years while instructing SPPH 604 (2018-2022). That knowledge have now transformed into an open educational resource, thanks to this grant. Meet the innovative minds behind the grant proposal below.\n\n\n\n\n\n\n\n Role \n    Team_Member \n    Affiliation \n  \n\n\n Principal Applicant (PI) \n    Dr. M Ehsan Karim \n    UBC School of Population and Public Health \n  \n\n Co-applicant (Co-I) \n    Dr. Suborna Ahmed \n    UBC Department of Forest Resources Management \n  \n\n Trainee co-applicants \n    Md Belal Hossain \n    UBC School of Population and Public Health \n  \n\n  \n    Fardowsa Yusuf \n    UBC School of Population and Public Health \n  \n\n  \n    Hanna Frank \n    UBC School of Population and Public Health \n  \n\n  \n    Dr. Michael Asamoah-Boaheng \n    UBC Department of Emergency Medicine \n  \n\n  \n    Chuyi (Astra) Zheng \n    UBC Faculty of Arts"
  },
  {
    "objectID": "wrangling.html#description",
    "href": "wrangling.html#description",
    "title": "Data wrangling",
    "section": "Description",
    "text": "Description\nTutorials in this chapter offer step-by-step instructions and code examples to help you understand and implement various data manipulation and import techniques in R.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder\n\n\n\nR Basics\nThis tutorial introduces the basics of R programming. It covers topics such as setting up R and RStudio, using R as a calculator, creating variables, working with vectors, plotting data, and accessing help resources.\n\n\nR Data Types\nThis tutorial covers three primary data structures in R: matrices, lists, and data frames. Matrices are two-dimensional arrays with elements of the same type, and their manipulation includes reshaping and combining. Lists in R are versatile collections that can store various R objects, including matrices. Data frames, on the other hand, are akin to matrices but permit columns of diverse data types. The tutorial offers guidance on creating, modifying, and merging data frames and checking their dimensions.\n\n\nAutomating Tasks\nMedical data analysis often grapples with vast and intricate data sets. Manual handling isn’t just tedious; it’s error-prone, especially given the critical decisions hinging on the results. This tutorial introduces automation techniques in R, a leading language for statistical analysis. By learning to use loops and functions, you can automate repetitive tasks, minimize errors, and conduct analyses more efficiently. Dive in to enhance your data handling skills.\n\n\nImporting Dataset\nThis tutorial focuses on importing data into R. It demonstrates how to import data from CSV and SAS formats using functions like read.csv and sasxport.get. It also includes examples of loading specific variables, dropping variables, subsetting observations based on certain criteria, and handling missing values.\n\n\nData Manipulation\nThis tutorial explores various data manipulation techniques in R. It covers topics such as dropping variables from a dataset, keeping specific variables, subsetting observations based on specific criteria, converting variable types (e.g., factors, strings), and handling missing values.\n\n\nImport External Data\nThis tutorial provides examples of importing external data into R. It includes specific examples of importing a CSV file (Employee Salaries - 2017 data) and a SAS file (NHANES 2015-2016 data). It also demonstrates how to save a working dataset in different formats, such as CSV and RData.\n\n\nSummary Tables\nThis tutorial emphasizes the importance of data summarization in medical research and epidemiology, specifically how to summarize medical data using R. It demonstrates creating “Table 1”, a typical descriptive statistics table in research papers, with examples that use the built-in R functions and specialized packages to efficiently summarize and stratify data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "wrangling1a.html",
    "href": "wrangling1a.html",
    "title": "R basics",
    "section": "",
    "text": "Important\n\n\n\nShow/hide code:\nOn every page, at the top, you’ll find a </> button. Click it to toggle the visibility of all R code on the page at once. Alternatively, you can click ‘Show the code’ within individual code chunks to view code on a case-by-case basis.\n\n\nStart using R\nTo get started with R, follow these steps:\n\nDownload and Install R: Grab the newest version from the official R website. > Tip: Download from a Comprehensive R Archive Network (CRAN) server near your geographic location.\nDownload and Install RStudio: You can get it from this link. > Note: RStudio serves as an Integrated Development Environment (IDE) offering a user-friendly interface. It facilitates operations such as executing R commands, preserving scripts, inspecting results, managing data, and more.\nBegin with RStudio: Once you open RStudio, delve into using R. For starters, employ the R syntax for script preservation, allowing future code adjustments and additions.\nBasic syntax\n\n\n\n\n\n\nTip\n\n\n\nR, a versatile programming language for statistics and data analysis, can execute numerous tasks. Let’s break down some of the fundamental aspects of R’s syntax.\n\n\n\nUsing R as a Calculator\n\nSimilar to how you’d use a traditional calculator for basic arithmetic operations, R can perform these functions with ease. For instance:\n\nShow the code# Simple arithmetic\n1 + 1\n#> [1] 2\n\n\nThis is a basic addition, resulting in 2.\nA more intricate calculation:\n\nShow the code# Complex calculation involving \n# multiplication, subtraction, division, powers, and square root\n20 * 5 - 10 * (3/4) * (2^3) + sqrt(25)\n#> [1] 45\n\n\nThis demonstrates R’s capability to handle complex arithmetic operations.\n\nVariable Assignment in R\n\nR allows you to store values in variables, acting like labeled containers that can be recalled and manipulated later. For example,\n\nShow the code# Assigning a value of 2 to variable x1\nx1 <- 2\nprint(x1)\n#> [1] 2\n\n\nSimilarly:\n\nShow the codex2 <- 9\nx2\n#> [1] 9\n\n\n\nCreating New Variables Using Existing Ones\n\nYou can combine and manipulate previously assigned variables to create new ones.\n\nShow the code# Using variable x1 \n# to compute its square and assign to y1\ny1 <- x1^2\ny1\n#> [1] 4\n\n\nYou can also use multiple variables in a single expression:\n\nShow the codey2 <- 310 - x1 + 2*x2 - 5*y1^3\ny2\n#> [1] 6\n\n\n\nCreating Functions\n\nFunctions act as reusable blocks of code. Once defined, they can be called multiple times with different arguments. Here’s how to define a function that squares a number:\n\nShow the codez <- function(x) {x^2}\n\n\nR also comes with a plethora of built-in functions. Examples include exp (exponential function) and rnorm (random number generation from a normal distribution).\n\nUtilizing Built-In Functions\n\nFor instance, using the exponential function:\n\nShow the code# Calling functions\nexp(x1)\n#> [1] 7.389056\nlog(exp(x1))\n#> [1] 2\n\n\nThe rnorm function can generate random samples from a normal distribution: below we are generating 10 random sampling from the normal distribution with mean 0 and standard deviation 1:\n\nShow the codernorm(n = 10, mean = 0, sd = 1)\n#>  [1] -1.35075652 -0.50412761 -0.19264235  0.26622497  1.13099668  0.07959930\n#>  [7]  1.00311899  1.21638989 -0.20415432  0.05776501\n\n\nAs random number generation relies on algorithms, results will differ with each execution.\n\nShow the code# Random sampling (again)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.22434788  0.17886145 -1.91964887 -0.10497858 -1.83003488  0.40758183\n#>  [7]  0.31717725 -0.77952024  0.05435223  0.13119455\n\n\nHowever, by setting a seed, we can reproduce identical random results:\n\nShow the code# Random sampling (again, but with a seed)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\n\nShow the code# random sampling (reproducing the same numbers)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\nAs we can see, when we set the same seed, we get exactly the same random number. This is very important for reproducing the same results. There are many other pre-exiting functions in R.\n\nSeeking Help in R\n\n\n\n\n\n\n\nTip\n\n\n\nR’s help function, invoked with ?function_name, provides detailed documentation on functions, assisting users with unclear or forgotten arguments:\n\n\n\nShow the code# Searching for help if you know \n# the exact name of the function with a question mark\n?curve\n\n\nBelow is an example of using the pre-exiting function for plotting a curve ranging from -10 to 10.\n\nShow the code# Plotting a function\ncurve(z, from = -10, to = 10, xlab = \"x\", ylab = \"Squared x\")\n\n\n\n\nIf some of the arguments are difficult to remember or what else could be done with that function, we could use the help function. For example, we can simply type help(curve) or ?curve to get help on the curve function:\n\n\n\n\n\n\nTip\n\n\n\nIf you’re uncertain about a function’s precise name, two question marks can assist in the search:\n\n\n\nShow the code# Searching for help if don't know \n# the exact name of the function\n??boxplot\n\n\n\nCreating Vectors\n\nVectors are sequences of data elements of the same basic type. Here are some methods to create them:\n\nShow the code# Creating vectors in different ways\nx3 <- c(1, 2, 3, 4, 5)\nprint(x3)\n#> [1] 1 2 3 4 5\n\nx4 <- 1:7\nprint(x4)\n#> [1] 1 2 3 4 5 6 7\n\nx5 <- seq(from = 0, to = 100, by = 10)\nprint(x5)\n#>  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nx6 <- seq(10, 30, length = 7)\nx6\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n\n\n\nPlotting in R\n\nR provides numerous plotting capabilities. For instance, the plot function can create scatter plots and line graphs:\n\nShow the code# Scatter plot\nplot(x5, type = \"p\", main = \"Scatter plot\")\n\n\n\n\n\nShow the code# Line graph\nplot(x = x6, y = x6^2, type = \"l\", main = \"Line graph\")\n\n\n\n\n\nCharacter Vectors Apart from numeric values, R also allows for character vectors. For example, we can create a sex variable coded as females, males and other.\n\n\nShow the code# Character vector\nsex <- c(\"females\", \"males\", \"other\")\nsex\n#> [1] \"females\" \"males\"   \"other\"\n\n\nTo determine a variable’s type, use the mode function:\n\nShow the code# Check data type\nmode(sex)\n#> [1] \"character\"\n\n\nPackage Management\nPackages in R are collections of functions and datasets developed by the community. They enhance the capability of R by adding new functions for data analysis, visualization, data import, and more. Understanding how to install and load packages is essential for effective R programming.\n\nInstalling Packages from CRAN\n\nThe CRAN is a major source of R packages. You can install them directly from within R using the install.packages() function.\n\nShow the code# Installing the 'ggplot2' package\ninstall.packages(\"ggplot2\")\n\n\n\nLoading a Package\n\nAfter a package is installed, it must be loaded to use its functions. This is done with the library() function.\n\nShow the code# Loading the 'ggplot2' package\nlibrary(ggplot2)\n\n\nYou only need to install a package once, but you’ll need to load it every time you start a new R session and want to use its functions.\n\nUpdating Packages\n\nR packages are frequently updated. To ensure you have the latest version of a package, use the update.packages() function.\n\nShow the code# Updating all installed packages\n# could be time consuming!\nupdate.packages(ask = FALSE)  \n# 'ask = FALSE' updates all without asking for confirmation\n\n\n\nListing Installed Packages\n\nYou can view all the installed packages on your R setup using the installed.packages() function.\n\nShow the code# Listing installed packages\ninstalled.packages()[, \"Package\"]\n\n\n\nRemoving a Package\n\nIf you no longer need a package, it can be removed using the remove.packages() function.\n\nShow the code# Removing the 'ggplot2' package\nremove.packages(\"ggplot2\")\n\n\n\nInstalling Packages from Other Sources\n\nWhile CRAN is the primary source, sometimes you might need to install packages from GitHub or other repositories. The devtools package provides a function for this.\n\nShow the code# Installing devtools first\ninstall.packages(\"devtools\")\n# Loading devtools\nlibrary(devtools)\n# Install a package from GitHub\n# https://github.com/ehsanx/simMSM\ninstall_github(\"ehsanx/simMSM\")\n\n\nWhen you are working on a project, it’s a good practice to list and install required packages at the beginning of your R script.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling1b.html",
    "href": "wrangling1b.html",
    "title": "Data types",
    "section": "",
    "text": "Matrix\n\n\n\n\n\n\nTip\n\n\n\nIn R, matrices are two-dimensional rectangular data sets, which can be created using the matrix() function. It’s essential to remember that all the elements of a matrix must be of the same type, such as all numeric or all character.\n\n\nTo construct a matrix, we often start with a vector and specify how we want to reshape it. For instance:\n\nShow the code# Matrix 1\nx <- 1:10\nmatrix1 <- matrix(x, nrow = 5, ncol = 2, byrow = TRUE)\nmatrix1\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nHere, the vector x contains numbers from 1 to 10. We reshape it into a matrix with 5 rows and 2 columns. The byrow = TRUE argument means the matrix will be filled row-wise, with numbers from the vector.\nConversely, if you want the matrix to be filled column-wise, you’d set byrow = FALSE:\n\nShow the code# matrix 2\nmatrix2 <- matrix(x, nrow = 5, ncol = 2, byrow = FALSE)\nmatrix2\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nYou can also combine or concatenate matrices. cbind() joins matrices by columns while rbind() joins them by rows.\n\nShow the code# Merging 2 matrices\ncbind(matrix1, matrix2)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    1    6\n#> [2,]    3    4    2    7\n#> [3,]    5    6    3    8\n#> [4,]    7    8    4    9\n#> [5,]    9   10    5   10\n\n\n\nShow the code# Appending 2 matrices\nrbind(matrix1, matrix2)\n#>       [,1] [,2]\n#>  [1,]    1    2\n#>  [2,]    3    4\n#>  [3,]    5    6\n#>  [4,]    7    8\n#>  [5,]    9   10\n#>  [6,]    1    6\n#>  [7,]    2    7\n#>  [8,]    3    8\n#>  [9,]    4    9\n#> [10,]    5   10\n\n\nList\n\n\n\n\n\n\nTip\n\n\n\nIn R, lists can be seen as a collection where you can store a variety of different objects under a single name. This includes vectors, matrices, or even other lists. It’s very versatile because its components can be of any type of R object.\n\n\nFor instance:\n\nShow the code# List of 2 matrices\nlist1 <- list(matrix1, matrix2)\nlist1\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nLists can also be expanded to include multiple items:\n\nShow the codex6 <- seq(10, 30, length = 7)\nsex <- c(\"females\", \"males\", \"other\")\n# Expanding list to include more items\nlist2 <- list(list1, x6, sex, matrix1)\nlist2 \n#> [[1]]\n#> [[1]][[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[1]][[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n#> \n#> \n#> [[2]]\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n#> \n#> [[3]]\n#> [1] \"females\" \"males\"   \"other\"  \n#> \n#> [[4]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nCombining different types of data into a single matrix converts everything to a character type:\n\nShow the code# A matrix with numeric and character variables\nid <- c(1, 2)\nscore <- c(85, 85)\nsex <- c(\"M\", \"F\")\nnew.matrix <- cbind(id, score, sex)\nnew.matrix\n#>      id  score sex\n#> [1,] \"1\" \"85\"  \"M\"\n#> [2,] \"2\" \"85\"  \"F\"\n\n\nTo check the type of data in your matrix:\n\nShow the codemode(new.matrix)\n#> [1] \"character\"\n\n\nData frame\n\n\n\n\n\n\nTip\n\n\n\nAs we can see combining both numeric and character variables into a matrix ended up with a matrix of character values. To keep the numeric variables as numeric and character variables as character, we can use the data.frame function.\n\n\n\nCreating a data frame\n\n\n\nA data frame is similar to a matrix but allows for columns of different types (numeric, character, factor, etc.). It’s a standard format for storing data sets in R.\n\nShow the codedf <- data.frame(id, score, sex)\ndf\n\n\n\n  \n\n\n\nTo check the mode or type of your data frame:\n\nShow the codemode(df)\n#> [1] \"list\"\n\n\n\nExtract elements\n\nData frames allow easy extraction and modification of specific elements. For example, we can extract the values on the first row and first column as follow:\n\nShow the codedf[1,1]\n#> [1] 1\n\n\nSimilarly, the first column can be extracted as follows:\n\nShow the codedf[,1]\n#> [1] 1 2\n\n\nThe first row can be extracted as follows:\n\nShow the codedf[1,]\n\n\n\n  \n\n\n\n\nModifying values\n\nWe can edit the values in the data frame as well. For example, we can change the score from 85 to 90 for the id 1:\n\nShow the codedf$score[df$id == 1] <- 90\ndf\n\n\n\n  \n\n\n\nWe can also change the name of the variables/columns:\n\nShow the codecolnames(df) <- c(\"Studyid\", \"Grade\", \"Sex\")\ndf\n\n\n\n  \n\n\n\n\nCombining data frames\n\nWe can also merge another data frame with the same variables using the rbind function:\n\nShow the code# Create a new dataset\ndf2 <- data.frame(Studyid = c(10, 15, 50), Grade = c(75, 90, 65), Sex = c(\"F\", \"M\", \"M\"))\n\n# Combining two data frames\ndf.new <- rbind(df, df2)\n\n# Print the first 6 rows\nhead(df.new)\n\n\n\n  \n\n\n\n\nChecking the dimensions\n\nTo see the dimension of the data frame (i.e., number of rows and columns), we can use the dim function:\n\nShow the codedim(df.new)\n#> [1] 5 3\n\n\nAs we can see, we have 5 rows and 3 columns. We can use the nrow and ncol functions respectively for the same output:\n\nShow the codenrow(df.new)\n#> [1] 5\nncol(df.new)\n#> [1] 3"
  },
  {
    "objectID": "wrangling1c.html",
    "href": "wrangling1c.html",
    "title": "Automating tasks",
    "section": "",
    "text": "Repeating a task\n\n\n\n\n\n\nTip\n\n\n\nThe for loop is a control flow statement in R that lets you repeat a particular task multiple times. This repetition is based on a sequence of numbers or values in a vector.\n\n\nConsider a simple real-life analogy: Imagine you are filling water in 10 bottles, one by one. Instead of doing it manually 10 times, you can set a machine to do it in a loop until all 10 bottles are filled.\n\nExample 1\n\n\nShow the code# Looping and adding\nk <- 0\nfor (i in 1:10){\n  k <- k + 5\n  print(k)\n}\n#> [1] 5\n#> [1] 10\n#> [1] 15\n#> [1] 20\n#> [1] 25\n#> [1] 30\n#> [1] 35\n#> [1] 40\n#> [1] 45\n#> [1] 50\n\n\nHere, you’re initiating a counter k at 0. With each iteration of the loop (i.e., every time it “runs”), 5 is added to k. After 10 cycles, the loop will stop, but not before printing k in each cycle.\n\nExample 2\n\nWe create a variable x5 containing the values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. Let us print the first 5 values using the for loop function:\n\nShow the codex5 <- seq(from = 0, to = 100, by = 10)\n# Looping through a vector\nk <- 1:5\nfor (ii in k){\n  print(x5[ii])\n}\n#> [1] 0\n#> [1] 10\n#> [1] 20\n#> [1] 30\n#> [1] 40\n\n\nThis loop cycles through the first five values of a previously created variable x5 and prints them. Each value printed corresponds to the positions 1 to 5 in x5.\n\nExample 3\n\nLet us use the for loop in a more complicated scenario. First, we create a vector of numeric values and square it:\n\nShow the code# Create a vector\nk <- c(1, 3, 6, 2, 0)\nk^2\n#> [1]  1  9 36  4  0\n\n\nThis is just squaring each value in the vector k.\n\nExample 4\n\nWe create the same vector of square values using the for loop function. To do so, (i) we create a null object, (ii) use the loop for each of the elements in the vector (k), (iii) square each of the elements, and (iv) store each of the elements of the new vector. In the example below, the length of k is 5, and the loop will run from the first to the fifth element of k. Also, k.sq[1] is the first stored value for squared-k, and k.sq[2] is the second stored value for squared-k, and so on.\n\nShow the code# Looping through a vector with function\nk.sq <- NULL\nfor (i in 1:length(k)){\n  k.sq[i] <- k[i]^2\n}\n\n# Print the values\nk.sq\n#> [1]  1  9 36  4  0\n\n\nHere, we achieve the same result as the third example but use a for loop. We prepare an empty object k.sq and then use the loop to square each value in k, storing the result in k.sq.\n\nExample 5\n\n\nShow the codedf.new <- data.frame(\n  Studyid = c(1, 2, 10, 15, 50),\n  Grade = c(90, 85, 75, 90, 65),\n  Sex = c('M', 'F', 'F', 'M', 'M')\n)\n# Looping through a data frame\nfor (i in 1:nrow(df.new)){\n  print(df.new[i,\"Sex\"])\n}\n#> [1] \"M\"\n#> [1] \"F\"\n#> [1] \"F\"\n#> [1] \"M\"\n#> [1] \"M\"\n\n\nThis loop prints the “Sex” column value for each row in the df.new data frame.\nFunctions\n\n\n\n\n\n\nTip\n\n\n\nA function in R is a piece of code that can take inputs, process them, and return an output. There are functions built into R, like mean(), which calculates the average of a set of numbers.\n\n\n\nBuilt-in function\n\n\nShow the code# Calculating a mean from a vector\nVector <- 1:100\nmean(Vector)\n#> [1] 50.5\n\n\nHere, we’re using the built-in mean() function to find the average of numbers from 1 to 100.\n\nCustom-made function\n\nTo understand how functions work, sometimes it’s helpful to build our own. Now we will create our own function to calculate the mean, where we will use the following equation to calculate it:\n\\(\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n},\\)\nwhere \\(x_1\\), \\(x_2\\),…, \\(x_n\\) are the values in the vector and \\(n\\) is the sample size. Let us create the function for calculation the mean:\nThis function, mean.own, calculates the average. We add up all the numbers in a vector (Sum <- sum(x)) and divide by the number of items in that vector (n <- length(x)). The result is then returned.\n\nShow the codemean.own <- function(x){\n  Sum <- sum(x)\n  n <- length(x)\n  return(Sum/n)\n}\n\n\nBy using our custom-made function, we calculate the mean of numbers from 1 to 100, getting the same result as the built-in mean() function.\n\nShow the codemean.own(Vector)\n#> [1] 50.5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling2.html",
    "href": "wrangling2.html",
    "title": "Importing dataset",
    "section": "",
    "text": "Introduction to Data Importing\nBefore analyzing data in R, one of the first steps you’ll typically undertake is importing your dataset. R provides numerous methods to do this, depending on the format of your dataset.\nDatasets come in a variety of file formats, with .csv (Comma-Separated Values) and .txt (Text file) being among the most common. While R’s interface offers manual ways to load these datasets, knowing how to code this step ensures better reproducibility and automation.\nImporting .txt files\nA .txt data file can be imported using the read.table function. As an example, consider you have a dataset named grade in the specified path.\nLet’s briefly glance at the file without concerning ourselves with its formatting.\n\nShow the code# Read and print the content of the TXT file\ncontent <- readLines(\"Data/wrangling/grade.txt\")\ncat(content, sep = \"\\n\")\n#> Studyid Grade Sex\n#> 1    90   M\n#> 2    85   F\n#> 10    75   F\n#> 15    90   M\n#> 50    65   M\n\n\nUsing the read.table function, you can load this dataset in R properly. It’s important to specify header = TRUE if the first row of your dataset contains variable names.\n\nTip: Always ensure the header argument matches the structure of your dataset. If your dataset contains variable names, set header = TRUE.\n\n\nShow the code## Read a text dataset\ngrade <- read.table(\"Data/wrangling/grade.txt\", header = TRUE, sep = \"\\t\", quote = \"\\\"\")\n# Display the first few rows of the dataset\nhead(grade)\n\n\n\n  \n\n\n\nImporting .csv files\nSimilarly, .csv files can be loaded using the read.csv function. Here’s how you can load a .csv dataset named mpg:\n\nShow the code## Read a csv dataset\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n# Display the first few rows of the dataset\nhead(mpg)\n\n\n\n  \n\n\n\nWhile we’ve discussed two popular data formats, R can handle a plethora of other formats. For further details, refer to Quick-R (2023). Notably, some datasets come built-in with R packages, like the mpg dataset in the ggplot2 package. To load such a dataset:\n\nShow the codedata(mpg, package = \"ggplot2\")\nhead(mpg)\n\n\n\n  \n\n\n\nTo understand more about the variables and the dataset’s structure, you can consult the documentation:\n\nShow the code?mpg\n\n\nData Screening and Understanding Your Dataset\ndim(), nrow(), ncol(), and str() are incredibly handy functions when initially exploring your dataset.\nOnce your data is in R, the next logical step is to get familiar with it. Knowing the dimensions of your dataset, types of variables, and the first few entries can give you a quick sense of what you’re dealing with.\nFor instance, str (short for structure) is a concise way to display information about your data. It reveals the type of each variable, the first few entries, and the total number of observations:\n\nShow the codestr(mpg)\n#> tibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n#>  $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n#>  $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n#>  $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n#>  $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n#>  $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n#>  $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n#>  $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n#>  $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n#>  $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n#>  $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n#>  $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nIn summary, becoming proficient in data importing and initial screening is a fundamental step in any data analysis process in R. It ensures that subsequent stages of data manipulation and analysis are based on a clear understanding of the dataset at hand.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\nReferences\n\n\n\n\nQuick-R. 2023. “Importing Data.” https://www.statmethods.net/input/importingdata.html."
  },
  {
    "objectID": "wrangling3.html",
    "href": "wrangling3.html",
    "title": "Data manipulation",
    "section": "",
    "text": "Data manipulation is a foundational skill for data analysis. This guide introduces common methods for subsetting datasets, handling variable types, creating summary tables, and dealing with missing values using R.\nLoad dataset\nUnderstanding the dataset’s structure is the first step in data manipulation. Here, we’re using the mpg dataset, which provides information on various car models:\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\n\nSubset\nOften, you’ll need to subset your data for analysis. Here, we’ll explore different methods to both drop unwanted variables and keep desired observations.\nDrop variables\nSometimes, only part of the variables will be used in your analysis. Therefore, you may want to drop the variables you do not need. There are multiple ways to drop variables from a dataset. Below are two examples without using any package and using the dplyr package.\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[, c(the columns you want to KEEP)]\n\n\nSay, we want to keep only three variables in the mpg dataset: manufacturer, model and cyl. For Option 1 (without package), we can use the following R codes to keep these three variables:\n\nShow the codempg1 <- mpg[, c(\"manufacturer\", \"model\", \"cyl\")]\nhead(mpg1)\n\n\n\n  \n\n\n\nHere mpg1 is a new dataset containing only three variables (manufacturer, model and cyl).\n\n\n\n\n\n\nTip\n\n\n\nOption 2: use select in dplyr\nselect(dataset.name, …(columns names you want to KEEP))\n\n\nFor Option 2, the dplyr package offers the select function, which provides a more intuitive way to subset data.\n\nShow the codempg2 <- select(mpg, c(\"manufacturer\", \"model\", \"cyl\"))\nhead(mpg2)\n\n\n\n  \n\n\n\nWe can also exclude any variables from the dataset by using the minus (-) sign with the select function. For example, we we want to drop trans, drv, and cty from the mpg dataset, we can use the following codes:\n\nShow the codempg3 <- select(mpg, -c(\"trans\", \"drv\", \"cty\"))\nhead(mpg3)\n\n\n\n  \n\n\n\nThis mpg3 is a new dataset from mpg after dropping three variables (trans, drv, and cty).\nKeep observations\nIt often happens that we only want to investigate a subset of a population which only requires a subset of our dataset. In this case, we need to subset the dataset to meet certain requirements. Again, there are multiple ways to do this task. Below is an example without a package and with the dplyr package:\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[the rows you want to KEEP, ]\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 2: No package needed\nsubset(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 3: use select in dplyr\nfilter(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nCommon logical tests are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n X <(=) Y \n    Smaller (equal) than \n  \n\n X >(=) Y \n    Larger (equal) than \n  \n\n X == Y \n    Equal to \n  \n\n X != Y \n    Not equal to \n  \n\n is.na(X) \n    is NA/missing? \n  \n\n\n\n\n\n\nSay, we want to keep the observations for which cars are manufactured in 2008. We can use the following R codes to do it:\n\nShow the codempg4 <- mpg[mpg$year == \"2008\",] # Option 1\nhead(mpg4)\n\n\n\n  \n\n\n\nThe following codes with the subset and filter function will do the same:\n\nShow the codempg5 <- subset(mpg, year == \"2008\") # Option 1\nhead(mpg5)\n\n\n\n  \n\n\n\n\nShow the codempg6 <- filter(mpg, year == \"2008\") # Option 3\nhead(mpg6)\n\n\n\n  \n\n\n\nThe filter function can also work when you have multiple criteria (i.e., multiple logical tests) to satisfy. Here, we need Boolean operators to connect different logical tests.\n\n\n\n\n\n\nTip\n\n\n\nCommon boolean operators are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n & \n    and \n  \n\n | \n    or \n  \n\n ! \n    not \n  \n\n == \n    equals to \n  \n\n != \n    not equal to \n  \n\n > \n    greater than \n  \n\n < \n    less than \n  \n\n >= \n    greater than or equal to \n  \n\n <= \n    less than or equal to \n  \n\n\n\n\n\n\nSay, we want to keep the observations for 6 and 8 cylinders (cyl) and engine displacement (displ) greater than or equal to 4 litres. We can use the following codes to do the task:\n\nShow the codempg7 <- filter(mpg, cyl %in% c(\"6\",\"8\") & displ >= 4)\nhead(mpg7)\n\n\n\n  \n\n\n\n\n\nThe %in% operator is used to determine whether the values of the first argument are present in the second argument.\nHandling Variable Types\n\n\n\n\n\n\nTip\n\n\n\nMost common types of variable in R are\n\nnumbers,\nfactors and\nstrings(or character).\n\nUnderstanding and manipulating these types are crucial for data analysis.\n\n\n\nIdentifying Variable Type\n\nWhen we analyze the data, we usually just deal with numbers and factors. If there are variables are strings, we could convert them to factors using as.factors(variable.name)\n\nShow the codemode(mpg$trans)\n#> [1] \"character\"\n\n\n\nShow the codestr(mpg$trans)\n#>  chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" \"auto(l5)\" ...\n\n\n\nConverting Characters to Factors\n\nSometimes, it’s necessary to treat text data as categorical by converting them into factors. as.numeric() converts other types of variables to numbers. For a factor variable, we usually we want to access the categories (or levels) it has. We can use a build-in function to explore: levels(variable.name)\n\nShow the code# no levels for character\nlevels(mpg$trans)\n#> NULL\n\n\n\nShow the code## Ex check how many different trans the dataset has\nmpg$trans <- as.factor(mpg$trans)\nlevels(mpg$trans)\n#>  [1] \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"   \"auto(l6)\"  \n#>  [6] \"auto(s4)\"   \"auto(s5)\"   \"auto(s6)\"   \"manual(m5)\" \"manual(m6)\"\n\n\nThe levels usually will be ordered alphabetically. The first level is called “baseline”. However, the users may/may not want to keep this baseline and want to relevel/change the reference group. We can do it using the relevel function:\nrelevel(variable.name, ref=)\n\nShow the codempg$trans <- relevel(mpg$trans, ref = \"auto(s6)\")\nlevels(mpg$trans)\n#>  [1] \"auto(s6)\"   \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"  \n#>  [6] \"auto(l6)\"   \"auto(s4)\"   \"auto(s5)\"   \"manual(m5)\" \"manual(m6)\"\nnlevels(mpg$trans)\n#> [1] 10\n\n\nfactor function can be also used to combine factors. If the user want to combine multiple factors to one factors\n\nShow the code## EX re-group trans to \"auto\" and \"manual\"\nlevels(mpg$trans) <- list(auto = c(\"auto(av)\", \"auto(l3)\", \"auto(l4)\", \"auto(l5)\", \"auto(l6)\", \n                                   \"auto(s4)\", \"auto(s5)\", \"auto(s6)\"), \n                          manual = c(\"manual(m5)\", \"manual(m6)\"))\nlevels(mpg$trans)\n#> [1] \"auto\"   \"manual\"\n\n\nYou can also change the order of all factors using the following code: factor(variable.name, levels = c(“new order”))\n\nShow the code## EX. Change the order of trans to manual\nmpg$trans <- factor(mpg$trans, levels = c(\"manual\", \"auto\"))\nlevels(mpg$trans)\n#> [1] \"manual\" \"auto\"\n\n\n\n\nIn R, the use of factors with multiple levels is primarily a memory optimization strategy. While users may not directly see this, R assigns internal numerical identifiers to each level, which is a more memory-efficient way of handling such data. Unlike some other software packages that generate multiple dummy variables to represent a single variable, R’s approach is generally more resource-efficient.\nConvert continuous variables to categorical variables\n\n\n\n\n\n\nTip\n\n\n\nifelse, cut, recode all are helpful functions to convert numerical variables to categorical variables.\n\n\nLet’s see the summary of the cty variable first.\n\nShow the codesummary(mpg$cty)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    9.00   14.00   17.00   16.86   19.00   35.00\n\n\nsay, we may want to change continuous ‘cty’ into groups 0-14, 15-18, and 18-40. Below is an example with the cut function.\n\nShow the code## EX. change the cty into two categories (0,14], (14,18] and (18,40]\nmpg$cty.num <- cut(mpg$cty, c(0, 14, 18, 40), right = TRUE)\ntable(mpg$cty.num)\n#> \n#>  (0,14] (14,18] (18,40] \n#>      73      85      76\n\n\n\nShow the code## Try this: do you see a difference?: [0,14), [14,18) and [18,40)\nmpg$cty.num2 <- cut(mpg$cty, c(0, 14, 18, 40), right = FALSE)\ntable(mpg$cty.num2)\n#> \n#>  [0,14) [14,18) [18,40) \n#>      54      78     102\n\n\n\n\n] stands for closed interval, i.e., right = TRUE. On the other hand, ) means open interval. Hence, there will be a huge difference when setting right = TRUE vs. right = FALSE\nMissing value\n\n\n\n\n\n\nTip\n\n\n\nIncomplete datasets can distort analysis. Identifying and managing these missing values is thus crucial.\n\n\nWe can check how many missing values we have by: table(is.na(variable.name))\nLet’s us check whether the cty variable contains any missing values:\n\nShow the codetable(is.na(mpg$cty))\n#> \n#> FALSE \n#>   234\n\n\nIf you want to return all non-missing values, i.e., complete case values: na.omit(variable.name). For more extensive methods on handling missing values, see subsequent tutorials."
  },
  {
    "objectID": "wrangling4.html",
    "href": "wrangling4.html",
    "title": "Import external data",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(dplyr)\nrequire(Hmisc)\n\n\nWhen dealing with data analysis in R, it’s common to need to import external data. This tutorial will walk you through importing data in different formats.\nCSV format data\nCSV stands for “Comma-Separated Values” and it’s a widely used format for data. We’ll be looking at the “Employee Salaries - 2017” dataset, which contains salary information for permanent employees of Montgomery County in 2017.\n\n\nEmployee Salaries - 2017 data\n\n\n\n\n\n\nTip\n\n\n\nWe’ll be loading the Employee_Salaries_-_2017.csv dataset into R from its saved location at Data/wrangling/. Do note, the directory path might vary for you based on where you’ve stored the downloaded data.\n\n\n\nShow the codedata.download <- read.csv(\"Data/wrangling/Employee_Salaries_-_2017.csv\")\n\n\nHere, the read.csv function reads the data from the CSV file and stores it in a variable called data.download.\nTo understand the structure of our dataset, We can see the number of rows and columns and the names of the columns/variables as follows:\n\nShow the codedim(data.download) # check dimension / row / column numbers\n#> [1] 9398   12\nnrow(data.download) # check row numbers\n#> [1] 9398\nnames(data.download) # check column names\n#>  [1] \"Full.Name\"                \"Gender\"                  \n#>  [3] \"Current.Annual.Salary\"    \"X2017.Gross.Pay.Received\"\n#>  [5] \"X2017.Overtime.Pay\"       \"Department\"              \n#>  [7] \"Department.Name\"          \"Division\"                \n#>  [9] \"Assignment.Category\"      \"Employee.Position.Title\" \n#> [11] \"Position.Under.Filled\"    \"Date.First.Hired\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nhead shows the first 6 elements of an object, giving you a sneak peek into the data you’re dealing with, while tail shows the last 6 elements.\n\n\nWe can see the first see six rows of the dataset as follows:\n\nShow the codehead(data.download)\n\n\n\n  \n\n\n\nNext, for learning purposes, let’s artificially assign all male genders in our dataset as missing:\n\nShow the code# Assigning male gender as missing\ndata.download$Gender[data.download$Gender == \"M\"] <- NA\nhead(data.download)\n\n\n\n  \n\n\n\nThis chunk sets the Gender column’s value to NA (missing) wherever the gender is “M”. This is a form of data manipulation, sometimes used to handle missing or incorrect data. If you want to work with datasets that exclude any missing values:\n\n\n\n\n\n\nTip\n\n\n\nna.omit and complete.cases are useful functions to to create datasets with non-NA values\n\n\n\nShow the code# deleting/dropping missing components\ndata.download2 <- na.omit(data.download)\nhead(data.download2)\n\n\n\n  \n\n\nShow the codedim(data.download2)\n#> [1] 3806   12\n\n\nHere, na.omit is used to remove rows with any missing values. This can be essential when preparing data for certain analyses.\nAlternatively, we could have selected only females to drop all males:\n\nShow the codedata.download3 <- filter(data.download, Gender != \"M\")\nhead(data.download3)\n\n\n\n  \n\n\n\nAnd to check the size of this new dataset:\n\nShow the code# new dimension / row / column numbers\ndim(data.download3)\n#> [1] 3806   12\n\n\nSAS format data\n\n\n\n\n\n\nTip\n\n\n\nSAS is another data format, commonly used in professional statistics and analytics.\n\n\nLet’s explore importing a SAS dataset. We download a SAS formatted dataset from the CDC website.\n\nShow the codeNHANES1516data <- sasxport.get(\"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\")\n#> Processing SAS dataset DEMO_I     ..\ndim(NHANES1516data) # check dimension / row / column numbers\n#> [1] 9971   47\nnrow(NHANES1516data) # check row numbers\n#> [1] 9971\nnames(NHANES1516data)[1:10] # check first 10 names\n#>  [1] \"seqn\"     \"sddsrvyr\" \"ridstatr\" \"riagendr\" \"ridageyr\" \"ridagemn\"\n#>  [7] \"ridreth1\" \"ridreth3\" \"ridexmon\" \"ridexagm\"\n\n\nThe sasxport.get function retrieves the SAS dataset. The following lines, just like before, help understand its structure.\nTo analyze some of the data:\n\nShow the codetable(NHANES1516data$riagendr) # tabulating gender variable\n#> \n#>    1    2 \n#> 4892 5079\n\n\n\n\nVerify these numbers from CDC website\nThis code creates a frequency table of the riagendr variable, which represents gender.\nSaving working dataset\n\n\n\n\n\n\nTip\n\n\n\nOnce you’ve made modifications or conducted some preliminary analysis, it’s important to save your dataset. We can save the dataset in a different format, e.g., CSV, txt, or even R, SAS or other formats.\n\n\nWe can save our working dataset in different formats. Say, we want to save our NHANES1516data dataset in csv format. We can use the write.csv() command:\n\nShow the codewrite.csv(NHANES1516data, \"Data/wrangling/NHANES1516.csv\", row.names = FALSE)\n\n\nWe can also save the dataset in R format:\n\nShow the codesave(NHANES1516data, file = \"Data/wrangling/NHANES1516.RData\")"
  },
  {
    "objectID": "wrangling5.html",
    "href": "wrangling5.html",
    "title": "Summary tables",
    "section": "",
    "text": "Medical research and epidemiology often involve large, complex datasets. Data summarization is a vital step that transforms these vast datasets into concise, understandable insights. In medical contexts, these summaries can highlight patterns, indicate data inconsistencies, and guide further research. This tutorial will teach you how to use R to efficiently summarize medical data.\nIn epidemiology and medical research, “Table 1” typically refers to the first table in a research paper or report that provides descriptive statistics of the study population. It offers a snapshot of the baseline characteristics of the study groups, whether in a cohort study, clinical trial, or any other study design.\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n## Ex create a summary table between manufacturer and drv\ntable(mpg$drv, mpg$manufacturer)\n#>    \n#>     audi chevrolet dodge ford honda hyundai jeep land rover lincoln mercury\n#>   4   11         4    26   13     0       0    8          4       0       4\n#>   f    7         5    11    0     9      14    0          0       0       0\n#>   r    0        10     0   12     0       0    0          0       3       0\n#>    \n#>     nissan pontiac subaru toyota volkswagen\n#>   4      4       0     14     15          0\n#>   f      9       5      0     19         27\n#>   r      0       0      0      0          0\n\n\nThe first line reads a CSV file. It uses the table() function to generate a contingency table (cross-tabulation) between two categorical variables: drv (drive) and manufacturer. It essentially counts how many times each combination of drv and manufacturer appears in the dataset.\n\nShow the code## Get the percentage summary using prop.table\nprop.table(table(mpg$drv, mpg$manufacturer), margin = 2)\n#>    \n#>          audi chevrolet     dodge      ford     honda   hyundai      jeep\n#>   4 0.6111111 0.2105263 0.7027027 0.5200000 0.0000000 0.0000000 1.0000000\n#>   f 0.3888889 0.2631579 0.2972973 0.0000000 1.0000000 1.0000000 0.0000000\n#>   r 0.0000000 0.5263158 0.0000000 0.4800000 0.0000000 0.0000000 0.0000000\n#>    \n#>     land rover   lincoln   mercury    nissan   pontiac    subaru    toyota\n#>   4  1.0000000 0.0000000 1.0000000 0.3076923 0.0000000 1.0000000 0.4411765\n#>   f  0.0000000 0.0000000 0.0000000 0.6923077 1.0000000 0.0000000 0.5588235\n#>   r  0.0000000 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>    \n#>     volkswagen\n#>   4  0.0000000\n#>   f  1.0000000\n#>   r  0.0000000\n## margin = 1 sum across row, 2 across col\n\n\nThis code calculates the column-wise proportion (as percentages) for each combination of drv and manufacturer. The prop.table() function is used to compute the proportions. The margin = 2 argument indicates that the proportions are to be computed across columns (margin = 1 would compute them across rows).\ntableone package\n\n\n\n\n\n\nTip\n\n\n\nCreateTableOne function from tableone package could be a very useful function to see the summary table. Type ?tableone::CreateTableOne to see for more details.\n\n\nThis section introduces the tableone package, which offers the CreateTableOne function. This function helps in creating “Table 1” type summary tables, commonly used in epidemiological studies.\n\nShow the coderequire(tableone)\n#> Loading required package: tableone\nCreateTableOne(vars = c(\"cyl\", \"drv\", \"hwy\", \"cty\"), data = mpg, \n               strata = \"trans\", includeNA = TRUE, test = FALSE)\n#>                  Stratified by trans\n#>                   auto(av)       auto(l3)       auto(l4)      auto(l5)     \n#>   n                   5              2             83            39        \n#>   cyl (mean (SD))  5.20 (1.10)    4.00 (0.00)    6.14 (1.62)   6.56 (1.45) \n#>   drv (%)                                                                  \n#>      4                0 (  0.0)      0 (  0.0)     34 (41.0)     29 (74.4) \n#>      f                5 (100.0)      2 (100.0)     37 (44.6)      8 (20.5) \n#>      r                0 (  0.0)      0 (  0.0)     12 (14.5)      2 ( 5.1) \n#>   hwy (mean (SD)) 27.80 (2.59)   27.00 (4.24)   21.96 (5.64)  20.72 (6.04) \n#>   cty (mean (SD)) 20.00 (2.00)   21.00 (4.24)   15.94 (3.98)  14.72 (3.49) \n#>                  Stratified by trans\n#>                   auto(l6)      auto(s4)      auto(s5)      auto(s6)     \n#>   n                   6             3             3            16        \n#>   cyl (mean (SD))  7.33 (1.03)   5.33 (2.31)   6.00 (2.00)   6.00 (1.59) \n#>   drv (%)                                                                \n#>      4                2 (33.3)      2 (66.7)      1 (33.3)      7 (43.8) \n#>      f                2 (33.3)      1 (33.3)      2 (66.7)      8 (50.0) \n#>      r                2 (33.3)      0 ( 0.0)      0 ( 0.0)      1 ( 6.2) \n#>   hwy (mean (SD)) 20.00 (2.37)  25.67 (1.15)  25.33 (6.66)  25.19 (3.99) \n#>   cty (mean (SD)) 13.67 (1.86)  18.67 (2.31)  17.33 (5.03)  17.38 (3.22) \n#>                  Stratified by trans\n#>                   manual(m5)    manual(m6)   \n#>   n                  58            19        \n#>   cyl (mean (SD))  5.00 (1.30)   6.00 (1.76) \n#>   drv (%)                                    \n#>      4               21 (36.2)      7 (36.8) \n#>      f               33 (56.9)      8 (42.1) \n#>      r                4 ( 6.9)      4 (21.1) \n#>   hwy (mean (SD)) 26.29 (5.99)  24.21 (5.75) \n#>   cty (mean (SD)) 19.26 (4.56)  16.89 (3.83)\n\n\nThe CreateTableOne function is used to create a summary table for the variables cyl, drv, hwy, and cty from the mpg dataset. The strata = trans argument means that the summary is stratified by the trans variable. The includeNA = TRUE argument means that missing values (NAs) are included in the summary. The test = FALSE argument indicates that no statistical tests should be applied to the data (often tests are used to compare groups in the table).\ntable1 package\nThis section introduces another package, table1, which can also be used to create “Table 1” type summary tables.\n\nShow the coderequire(table1)\n#> Loading required package: table1\n#> \n#> Attaching package: 'table1'\n#> The following objects are masked from 'package:base':\n#> \n#>     units, units<-\ntable1(~ cyl + drv + hwy + cty | trans, data=mpg)\n\n\n\n\n\nauto(av)(N=5)\nauto(l3)(N=2)\nauto(l4)(N=83)\nauto(l5)(N=39)\nauto(l6)(N=6)\nauto(s4)(N=3)\nauto(s5)(N=3)\nauto(s6)(N=16)\nmanual(m5)(N=58)\nmanual(m6)(N=19)\nOverall(N=234)\n\n\n\ncyl\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n5.20 (1.10)\n4.00 (0)\n6.14 (1.62)\n6.56 (1.45)\n7.33 (1.03)\n5.33 (2.31)\n6.00 (2.00)\n6.00 (1.59)\n5.00 (1.30)\n6.00 (1.76)\n5.89 (1.61)\n\n\nMedian [Min, Max]\n6.00 [4.00, 6.00]\n4.00 [4.00, 4.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n8.00 [6.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n\n\ndrv\n\n\n\n\n\n\n\n\n\n\n\n\n\nf\n5 (100%)\n2 (100%)\n37 (44.6%)\n8 (20.5%)\n2 (33.3%)\n1 (33.3%)\n2 (66.7%)\n8 (50.0%)\n33 (56.9%)\n8 (42.1%)\n106 (45.3%)\n\n\n4\n0 (0%)\n0 (0%)\n34 (41.0%)\n29 (74.4%)\n2 (33.3%)\n2 (66.7%)\n1 (33.3%)\n7 (43.8%)\n21 (36.2%)\n7 (36.8%)\n103 (44.0%)\n\n\nr\n0 (0%)\n0 (0%)\n12 (14.5%)\n2 (5.1%)\n2 (33.3%)\n0 (0%)\n0 (0%)\n1 (6.3%)\n4 (6.9%)\n4 (21.1%)\n25 (10.7%)\n\n\nhwy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n27.8 (2.59)\n27.0 (4.24)\n22.0 (5.64)\n20.7 (6.04)\n20.0 (2.37)\n25.7 (1.15)\n25.3 (6.66)\n25.2 (3.99)\n26.3 (5.99)\n24.2 (5.75)\n23.4 (5.95)\n\n\nMedian [Min, Max]\n27.0 [25.0, 31.0]\n27.0 [24.0, 30.0]\n22.0 [14.0, 41.0]\n19.0 [12.0, 36.0]\n19.0 [18.0, 23.0]\n25.0 [25.0, 27.0]\n27.0 [18.0, 31.0]\n26.0 [18.0, 29.0]\n26.0 [16.0, 44.0]\n26.0 [12.0, 32.0]\n24.0 [12.0, 44.0]\n\n\ncty\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n20.0 (2.00)\n21.0 (4.24)\n15.9 (3.98)\n14.7 (3.49)\n13.7 (1.86)\n18.7 (2.31)\n17.3 (5.03)\n17.4 (3.22)\n19.3 (4.56)\n16.9 (3.83)\n16.9 (4.26)\n\n\nMedian [Min, Max]\n19.0 [18.0, 23.0]\n21.0 [18.0, 24.0]\n16.0 [11.0, 29.0]\n14.0 [9.00, 25.0]\n13.0 [12.0, 16.0]\n20.0 [16.0, 20.0]\n18.0 [12.0, 22.0]\n17.0 [12.0, 22.0]\n19.0 [11.0, 35.0]\n16.0 [9.00, 23.0]\n17.0 [9.00, 35.0]\n\n\n\n\n\nThe table1() function is used to generate a summary table for the specified variables. The formula-like syntax (~ cyl + drv + hwy + cty | trans) indicates that the summary should be stratified by the trans variable."
  },
  {
    "objectID": "wranglingF.html",
    "href": "wranglingF.html",
    "title": "Functions for wrangling",
    "section": "",
    "text": "This review page provides an extensive list of R functions tailored for data wrangling tasks that we have used in this chapter. Each function is systematically described, highlighting its primary package source and its specific utility.\nTo learn more about these functions, readers can:\n\nUse R’s Built-in Help System: For each function, access its documentation by prefixing the function name with a question mark in the R console, e.g., ?as.factor. This displays the function’s manual page with descriptions, usage, and examples.\nSearch Websites: Simply Google, or visit the CRAN website to search for specific function documentation. Websites like Stack Overflow and RStudio Community often have discussions related to R functions.\nTutorials and Online Courses: Platforms like DataCamp, Coursera, and edX offer R courses that cover many functions in depth. Also there are examples of dedicated R tutorial websites that you might find useful. One example is “Introduction to R for health data analysis” by Ehsan Karim, An Hoang and Qu.\nBooks: There are numerous R programming books, such as “R for Data Science” by Hadley Wickham and “The Art of R Programming” by Norman Matloff.\nWorkshops and Webinars: Institutions and organizations occasionally offer R programming workshops or webinars.\n\nWhenever in doubt, exploring existing resources can be highly beneficial.\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.factor \n    base \n    Converts a variable to factors. `as.factor` is a wrapper for the `factor` function. \n  \n\n cbind \n    base \n    Merges matrices. \n  \n\n CreateTableOne \n    tableone \n    Creates a frequency table. \n  \n\n data.frame \n    base \n    Creates a dataset with both numeric and character variables. Requires unique column names and equal length for all variables. \n  \n\n dim \n    base \n    Returns the dimensions of a data frame (rows x columns). \n  \n\n filter \n    dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n function \n    base \n    Used to define custom functions, e.g., for calculating standard deviation. \n  \n\n head \n    base \n    Displays the first six elements of an object (e.g., a dataset). `tail` displays the last six. \n  \n\n is.na \n    base \n    Checks for missing values in a variable. \n  \n\n levels \n    base \n    Displays the levels of a factor variable. \n  \n\n list \n    base \n    Stores vectors, matrices, or lists of differing types. \n  \n\n mode \n    base \n    Determines the type of a variable. \n  \n\n na.omit \n    base/stats \n    Removes all rows with missing values from a dataset. \n  \n\n names \n    base \n    Displays names of objects, e.g., variable names of a data frame. \n  \n\n nlevels \n    base \n    Shows the number of levels in a factor variable. \n  \n\n nrow \n    base \n    Returns the dimensions of a data frame. `nrow` gives row count and `ncol` gives column count. \n  \n\n plot \n    base/graphics \n    Draws scatter plots or line graphs. \n  \n\n print \n    base \n    Prints the output to console. \n  \n\n prop.table \n    base \n    Displays percentage summary for a table. \n  \n\n rbind \n    base \n    Appends matrices row-wise. \n  \n\n read.csv \n    base/utils \n    Reads data from a CSV file. \n  \n\n relevel \n    base/stats \n    Changes the reference group of a factor variable. \n  \n\n sasxport.get \n    Hmisc \n    Loads data in the SAS format. \n  \n\n save \n    base \n    Saves R objects, such as datasets. \n  \n\n select \n    dplyr \n    Selects specified variables from a dataset. \n  \n\n set.seed \n    base \n    Sets a seed for random number generation ensuring reproducibility. \n  \n\n str \n    base/utils \n    Displays the structure of a dataset, including data type of variables. \n  \n\n subset \n    base, dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n summary \n    base \n    Provides a summary of an object, like variable statistics. \n  \n\n table \n    base \n    Displays frequency counts for a variable. \n  \n\n write.csv \n    base/utils \n    Saves a data frame to a CSV file in a specified directory."
  },
  {
    "objectID": "wranglingQ.html",
    "href": "wranglingQ.html",
    "title": "Quiz on wrangling",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes on data wrangling.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "wranglingE.html#problem",
    "href": "wranglingE.html#problem",
    "title": "Exercise on wrangling",
    "section": "Problem",
    "text": "Problem\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nA paper you can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\n\nShow the code# Load required packages\nlibrary(dplyr)\nlibrary(tableone)\n\n\n\nShow the code# Data import: name it rhc\n#rhc <- ...(\"Data/wrangling/rhc.csv\", ...)\n\n\nPart (a) Basic Manipulation [60%]\n\nContinuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\n\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\n\n\n\nSet reference: Change the reference category for gender to Male\n\n\n\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\n\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\n\n\n\ncomorbidities:\n\n\ncreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nreport maximim and minimum values of numcom:\n\n\n\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\n“age”, “sex”, “race”,“cat1”, “ca”, “dnr1”, “aps1”, “surv2md1”, “numcom”, “adld3p”, “das2d3pc”, “temp1”, “hrt1”, “meanbp1”, “resp1”, “wblc1”, “pafi1”, “paco21”, “ph1”, “crea1”, “alb1”, “scoma1”, “swang1”, and\nname it rhc2.\n\n\n\n\nPart (b) Table 1 [20%]\n\nRe-produce the sample table from the rhc2 data (see the Table that was provided with this assignment). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\n\n\nTable 1 for subset\n\nProduce a similar table as part (b) but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]\n\n\n\nPart (c) Considering eligibility criteria [20%]\nProduce a similar table as part (b.i) but only for the subjects who meet all of the following eligibility criteria: (i) age is equal to or above 50, (ii) age is below 80 (iii) Glasgow Coma Score is below 61 and (iv) Primary disease categories are either ARF or MOSF. [Hint: droplevels.data.frame can be a useful function]\n\n\n\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\n\n\n\nCount how many NAs does that variable have?\n\n\n\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\n\n\n\n\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nShow the codeVector <- 1:100\n\n#variance.est <- function(?){?}\n\n#variance.est(Vector)\n\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nShow the codevar(Vector)\n#> [1] 841.6667"
  },
  {
    "objectID": "accessing.html#survey-data-sources",
    "href": "accessing.html#survey-data-sources",
    "title": "Data accessing",
    "section": "Survey data sources",
    "text": "Survey data sources\nThe tutorial lists primary complex survey data sources, including the Canadian Community Health Survey and National Health and Nutrition Examination Survey, with several offering dedicated R packages for data access."
  },
  {
    "objectID": "accessing.html#importing-cchs-to-r",
    "href": "accessing.html#importing-cchs-to-r",
    "title": "Data accessing",
    "section": "Importing CCHS to R",
    "text": "Importing CCHS to R\nThe section provides detailed steps for importing the Canadian Community Health Survey dataset from the UBC library into RStudio, with processing options using SAS, the free software PSPP, and directly in R."
  },
  {
    "objectID": "accessing.html#importing-nhanes-to-r",
    "href": "accessing.html#importing-nhanes-to-r",
    "title": "Data accessing",
    "section": "Importing NHANES to R",
    "text": "Importing NHANES to R\nThe tutorial guides users on how to access and import the NHANES dataset from the CDC website into RStudio, detailing the dataset’s structure and providing methods both manually and using an R package."
  },
  {
    "objectID": "accessing.html#reproducing-results",
    "href": "accessing.html#reproducing-results",
    "title": "Data accessing",
    "section": "Reproducing results",
    "text": "Reproducing results\nThe tutorial guides users through accessing, processing, and analyzing NHANES data to reproduce the results from a referenced article using R code.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "accessing1.html",
    "href": "accessing1.html",
    "title": "Survey data sources",
    "section": "",
    "text": "The tutorial discusses complex survey data and highlights potential data sources. Key datasets with survey features include the Canadian Community Health Survey (CCHS), the National Health and Nutrition Examination Survey (NHANES), and the European Social Survey (ESS), among others. Many of these sources, like NHANES and ESS, have specific R packages for data retrieval. In addition, there are other data sources such as the Vanderbilt Biostatistics Datasets and the World Bank Open Data, with the latter also offering dedicated R packages for data access.\n\nDataset with survey features\n\nCanadian Community Health Survey - Annual Component CCHS\n\nDownload link UBC library\n\nNational Health and Nutrition Examination Survey NHANES\n\nR packages to download data: nhanesA, RNHANES\n\nNational Longitudinal Study of Adolescent to Adult Health [Add Health], 1994-2008 ICPSR 21600\nEuropean Social Survey ESS\n\nR package to download data: essurvey\n\nBehavioral Risk Factor Surveillance System BRFSS\nBureau of Economic Analysis BEA\nUS National Vital Statistics System NVSS\nDemographic and Health Surveys DHS\n\n\n\nOthers\n\nVanderbilt Biostatistics Datasets link\nWorld Bank Open Data WBOD\n\nR packages to download data: wbstats, WDI"
  },
  {
    "objectID": "accessing2.html",
    "href": "accessing2.html",
    "title": "Importing CCHS to R",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(knitr)\n\n\nThis section provides comprehensive instructions on how to import the Canadian Community Health Survey (CCHS) dataset from the UBC library site to the RStudio environment. The process starts with downloading the CCHS data from the UBC library site and includes step-by-step visual guides for each stage. Three primary options are provided to process and format the data:\n\nUsing the commercial software SAS.\nUtilizing the free software PSPP, an alternative to SPSS.\nDirectly processing the data in R.\n\nFor each option, users are guided on how to download, install, access, read, save, and check the dataset. The objective is to help users acquire, visualize, and manipulate the CCHS dataset seamlessly using various software applications.\nDownloading CCHS data from UBC\n\n\nStep 1: Go to dvn.library.ubc.ca, and press ‘log-in’\n\n\n\n\n\n\n\n\nStep 2: Select ‘UBC’ from the dropdown menu\n\n\n\n\n\n\n\n\nStep 3: Enter your CWL or UBC library authentication information\n\n\n\n\n\n\n\n\nStep 4: Once you log-in, search the term ‘cchs’ in the search-box\n\n\n\n\n\n\n\n\nStep 5: For illustrative purposes, let us work with the Cycle 3.1 of the CCHS dataset from the list of results. In that case, type ‘cchs 3.1’\n\n\n\n\n\n\n\n\nStep 6: CCHS Cycle 3.1 information\n\n\n\n\n\n\n\n\nStep 7: Choose the ‘Data: CD’ from the menu\n\n\n\n\n\n\n\n\nStep 8: Download the entire data (about 159 MB) as a zip file\n\n\n\n\n\n\n\n\nStep 9: Accept the ‘terms of use’\n\n\n\n\n\n\n\n\nStep 10: Select a directory to download the zip file. The path of the download directory is important (we need to use this path exactly later). For example, below we are in \"C:\\CCHS\\\" folder, but we will create a “Data” folder there, so that the download path is \"C:\\CCHS\\Data\\\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 11: Extract the zip file\n\n\n\n\n\n\n\n\nStep 12: Be patient with the extraction\n\n\n\n\n\n\n\n\nStep 13: Once extraction is complete, take a look at the folders inside. You will see that there is a folder named ‘SAS_SPSS’\n\n\n\n\n\n\nReading and Formatting the data\nOption 1: Processing data using SAS\nSAS is a commercial software. You may be able to get access to educational version. In case you don’t have access to it, later we outline how to use free packages to read these datasets.\n\n\nStep 1: Inside that ‘SAS_SPSS’ folder, find the file hs_pfe.sas. It is a long file, but we are going to work on part of it. First thing we want to do it to change all the directory names to where you have unzipped the downloaded file (for example, here the zip file was extracted to C:/CCHS/Data/cchs_cycle3-1CD/). We only need the first part of the code (as shown below; only related to data ‘hs’). Delete the rest of the codes for now. The resulting code should like like this:\n\n\nShow the code%include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_pfe.sas\";\n\ndata hs;\n        %let datafid=\"C:\\CCHS\\Data\\cchs_cycle3-1CD\\Data\\hs.txt\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_fmt.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_lbe.sas\";\nrun;\n\n\nOnce the modifications are done, submit the codes in SAS. Note that, the name of the data is ‘hs’.\n\n\n\n\n\n\n\nStep 2: Once you submit the code, you can check the log window in SAS to see how the code submission went. It should tell you how many observations and variables were read.\n\n\n\n\n\n\n\n\nStep 3: If you one to view the dataset, you can go to ‘Explorer’ window within SAS.\n\n\n\n\n\n\n\n\nStep 4: Generally, if you haven’t specified where to load the files, SAS will by default save the data into a library called ‘Work’\n\n\n\n\n\n\n\n\nStep 5: Open that folder, and you will be able to find the dataset ‘Hs’.\n\n\n\n\n\n\n\n\nStep 6: Right click on the data, and click ‘open’ to view the datafile.\n\n\n\n\n\n\n\n\nStep 7: To export the data into a CSV format data (so that we can read this data into other software packages), ckick ‘Menu’.\n\n\n\n\n\n\n\n\nStep 8: then press ‘Export Data’.\n\n\n\n\n\n\n\n\nStep 9: choose the library and the data.\n\n\n\n\n\n\n\n\nStep 10: choose the format in which you may want to save the existing data.\n\n\n\n\n\n\n\n\nStep 11: also specify where you want to save the csv file and the name of that file (e.g., cchs3.csv).\n\n\n\n\n\n\n\n\nStep 12: go to that directory to see the file cchs3.csv\n\n\n\n\n\n\n\n\nStep 13: If you want to save the file in SAS format, you can do so by writing the following sas code into the ‘Editor’ window. Here we are saving the data Hs within the Work library in to a data called cchs3 within the SASLib library. Note that, the directory name has to be where you want to save the output file.\n\n\nShow the codeLIBNAME SASLib \"C:\\CCHS\\Data\";\nDATA SASLib.cchs3;\n    set Work.Hs;\nrun;\n\n\nSubmit these codes into SAS:\n\n\n\n\n\n\n\nStep 13: go to that directory to see the file cchs3.sas7dbat\n\n\n\n\n\n\nOption 2: Processing data using PSPP (Free)\nPSPP is a free package; alternative to commercial software SPSS. We can use the same SPSS codes to read the datafile into PSPP, and save.\n\n\nStep 1: Get the free PSPP software from the website: www.gnu.org/software/pspp/\n\n\nPSPP is available for GNU/Hurd, GNU/Linux, Darwin (Mac OS X), OpenBSD, NetBSD, FreeBSD, and Windows\n\n\n\n\n\nFor windows, download appropriate version.\n\n\n\n\n\nDownload the file\n\n\n\n\n\nInstall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the icon shorcut after installing\n\n\n\n\n\n\n\nStep 2: Open PSPP\n\n\n\n\n\n\n\n\nStep 3: Go to ‘file’ menu and click ‘open’\n\n\n\n\n\n\n\n\nStep 4: Specify the readfile.sps file from the ‘SAS_SPSS’ folder.\n\n\n\n\n\n\nYou will see the following file:\n\n\n\n\n\n\n\nStep 5: Similar to before, change the directories as appropriate. Get rid of the extra lines of codes. Resulting codes are as follows (you can copy and replace the code in the file with the following codes):\n\n\nShow the codefile handle infile/name = 'C:\\CCHS\\Data\\cchs_cycle3-1CD\\DATA\\hs.txt'.\ndata list file = infile notable/.\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvale.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvare.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsmiss.sps\".\nexecute.\n\n\n\n\n\n\n\nFor Mac users, it should be as follows (e.g., username should be your user name, if you are saving under the path \"/Users/username/CCHS/Data/\"):\n\nShow the codefile handle infile/name =\"/Users/username/CCHS/Data/cchs_cycle3-1CD/Data/hs.txt\".\ndata list file = infile notable/.\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hs_i.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvale.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvare.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsmiss.sps\".\n\nexecute.\n\n\n\n\nStep 6: Run the codes.\n\n\n\n\n\n\n\n\nStep 7: This is a large data, and will take some time to load the data into the PSPP data editor. Be patient.\n\n\n\n\n\n\nOnce loading is complete, it will show the ‘output’ and ‘data view’.\n\n\n\n\n\n\n\n\n\n\nNote that, you will get error message, if your files were not in the correct path. In our example, the path was \"C:\\CCHS\\Data\\\" for the zip file content (see the previous steps).\n\n\nStep 7: You can also check the ‘variable view’.\n\n\n\n\n\n\n\n\nStep 8: Save the data by clicking ‘File’ and then ‘save as …’\n\n\n\n\n\n\n\n\nStep 9: Specify the name of the datafile and the location / folder to save the data file.\n\n\n\n\n\n\n\n\nStep 10: See the SAV file saved in the directory.\n\n\n\n\n\n\n\n\nStep 11: To save CSV format data, use the following syntax.\n\n\nShow the codeSAVE TRANSLATE\n  /OUTFILE=\"C:/CCHS/Data/cchs3b.csv\"  \n  /TYPE=CSV\n  /FIELDNAMES      \n  /CELLS=VALUES.\n\n\nNote that, for categorical data, you can either save values or labels. For our purpose, we prefer values, and hence saved with values here.\n\n\n\n\n\n\n\nStep 12: See the CSV file saved in the directory extracted from PSPP.\n\n\n\n\n\n\nOption 3: Processing data using SPSS\nLog into ubc.onthehub.com to download SPSS. With your CWL account, UBC students should be able to download it. UBC IT website for SPSS says:\nThe SPSS software license with UBC specifies that SPSS must only be used by UBC Faculty, Students, and Research Staff and only for Teaching and non-commercial Research purposes related to UBC.\nBoth network (for UBC owened devices) or standalone / home versions (for non-UBC owened devices) should be available. Once downloaded, same process of importing CCHS data in PSPP can also be applied on SPSS (same syntax files should work). Let me know if that is not the case.\nProcessing data in R\nDownload software\n\n\nStep 1: Download either ‘R’ from CRAN www.r-project.org or ‘R open’ from Microsoft mran.microsoft.com/open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download RStudio from www.rstudio.com/\n\n\n\n\n\n\n\n\n\nStep 3: Open RStudio\n\n\n\n\n\n\nImport, export and load data into R\n\n\nStep 1: Set working directory\n\n\nShow the codesetwd(\"C:/CCHS/Data/\") # or something appropriate\n\n\n\n\nStep 2: Read the dataset created from PSPP with cell values. We can also do a small check to see if the cell values are visible. For example, we choose a variable ‘CCCE_05A’, and tabulate it.\n\n\nShow the codeHs <- read.csv(\"cchs3b.csv\", header = TRUE)\ntable(Hs$CCCE_05A)\n\n\n\n\n\n\n\n\n\nStep 3: Save the RData file from R into a folder SurveyData:\n\n\nShow the codesave(Hs, file = \"SurveyData/cchs3.RData\")\n\n\n\n\nStep 4: See the RData file saved in the directory extracted from R.\n\n\n\n\n\n\n\n\nStep 5: Close R / RStudio and restart it. Environment window within RStudio should be empty.\n\n\n\n\n\n\n\n\nStep 6: Load the saved RData into R. Environment window within RStudio should have ‘Hs’ dataset.\n\n\nShow the codeload(\"SurveyData/cchs3.RData\")"
  },
  {
    "objectID": "accessing3.html",
    "href": "accessing3.html",
    "title": "Importing NHANES to R",
    "section": "",
    "text": "This tutorial provides comprehensive instructions on accessing the National Health and Nutrition Examination Survey (NHANES) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment. It covers:\n\nIntroduction to the NHANES dataset, highlighting its significance in evaluating the health and nutritional status of U.S. adults and children.\nSampling Procedure details, explaining the multi-stage sampling strategy and emphasizing the importance of using survey features like weights, strata, and primary sampling units for population-level estimates.\nSurvey History with a visualization representing different NHANES survey cycles.\nNHANES Data Files and Documents:\n\n\nExplains the data’s file format, mostly in SAS transport file format (.xpt).\nBreaks down the NHANES components, which include demographics, dietary, examination, laboratory, and questionnaire data.\nProvides guidelines on combining data from different cycles and handling missing data or outliers.\n\n\nAccessing NHANES Data:\n\n\nDirectly from the CDC website: A step-by-step guide with accompanying images, illustrating how to navigate the CDC website, download the data, and interpret the accompanying codebook.\nUsing R packages, specifically the nhanesA package: A concise guide on how to download and get summaries of the NHANES data using this R package.\n\n\nShow the code# Load required packages\n#devtools::install_github(\"warnes/SASxport\")\nlibrary(SASxport)\nlibrary(foreign)\nlibrary(nhanesA)\nlibrary(knitr)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nuse.saved.chche <- TRUE\n\n\n\n\nBefore installing a package from GitHub, it’s better to check whether you installed the right version of Rtools\nOverview\nNational Center for Health Statistics (NCHS) conducts National Health and Nutrition Examination Survey (NHANES) (CDC,NCHS 2023). These surveys are designed to evaluate the health and nutritional status of U.S. adults and children. These surveys are being administered in two-year cycles or intervals starting from 1999-2000. Prior to 1999, a number of surveys were conducted (e.g., NHANES III), but in our discussion, we will mostly restrict our discussions to continuous NHANES (e.g., NHANES 1999-2000 to NHANES 2017-2018).\n\n\nCDC,NCHS (2023)\nSampling Procedure:\nIt is a probabilistic sample (we know probability of getting selected for all individuals). This sample is unlikely to be representative of the entire population, as some under/oversampling occurs (unlike SRS), and samples may be dependent (due to proximity of some samples). For example, household with the following characteristics may be oversampled in NHANES, e.g., African Americans, Mexican Americans, Low income White Americans, Persons age 60+ years.\n\n\nSampling Procedure:\n\nnot obtained via simple random sample\nmultistage sample designs\nA sample weight is assigned to each sample person where weight = the number of people in the target population represented by that sample person in NHANES\n\nNHANES used multistage sample designs:\n\nStage 1: PSU/clusters = geographically contiguous counties. 50 states - divided into ~3100 counties. Each PSU is assigned to a strata (e.g., urban/rural or PSU size etc.). The counties are randomly/PPS selected using a 2-per-stratum design. Complex sample variance estimation requires PSU + strata (masking involved).\nStage 2: each selected county is broken into segments (with at least ~50-100 housing units). Segments are randomly/PPS selected.\nStage 3: each selected segment is divided into households. Households are randomly selected.\nStage 4: Within each sampled household, an individual is randomly selected.\n\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nSurvey history\nOverall NHANES survey history\n\n\n\n\n\n\n\n\nNHANES datafile and documents\nFile format\nThe Continuous NHANES files are stored in the NHANES website as SAS transport file formats (.xpt). You can import this data in any statistical package that supports this file format.\nContinuous NHANES Components\nContinuous NHANES components separated to reduce the amount of time to download and documentation size:\n\n\nNHANES Tutorials\n\n\n\n\n\n\n\n\n\n\nBroadly, continuous NHANES data are available in 5 categories:\n\nDemographics\nDietary\nExamination\nLaboratory\nQuestionnaire\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHANES. However, NHANES is a cross-sectional data, and identification of the same person accross different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\n\n\nThe following data have not been released on the NHANES website as public release files due to confidentiality concerns:\n\nadolescent data on alcohol use\nsmoking\nsexual behavior\nreproductive health and drug use\n\nWithin the same cycle\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\nMissing data and outliers\nCDC (2023) recommends:\n\n\nCDC (2023)\n\n\n“As a general rule, if 10% or less of your data for a variable are missing from your analytic dataset, it is usually acceptable to continue your analysis without further evaluation or adjustment. However, if more than 10% of the data for a variable are missing, you may need to determine whether the missing values are distributed equally across socio-demographic characteristics, and decide whether further imputation of missing values or use of adjusted weights are necessary.”\n\n\n\n\n“If you fail to identify ‘refusal’ or ‘do not know’ as types of missing data, and treat the assigned values for ‘refused’ or ‘do not know’ as real values, you will get distorted results in your statistical analyses. Therefore, it is important to recode ‘refused’ or ‘don’t know’ responses as missing values (either as a period (.) for numeric variables or as a blank for character variables).”\n\n\n\n\n“Outliers with extremely large weights could have an influential impact on your estimates. You will have to decide whether to keep these influential outliers in your analysis or not. It is up to the analysts to make that decision.”\n\n\nNHANES documents\n\n\n\n\n\n\n\n\n\n\nThe following websites could be helpful: - For more information about NHANES design.\n\nVisit US CDC website and do a variable keyword search based on your research interest (e.g., arthritis).\n\nAccessing NHANES Data Directly from the CDC website\nIn the following example, we will see how to download ‘Demographics’ data, and check associated variable in that dataset.\n\n\n\n\n\n\n\nNHANES 1999-2000 and onward survey datasets are publicly available at wwwn.cdc.gov/nchs/nhanes/\n\n\nStep 1: Say, for example, we are interested about the NHANES 2015-2016 survey. Clicking the associated link in the above Figure gets us to the page for the corresponding cycle (see below).\n\n\n\n\n\n\n\n\nStep 2: There are various types of data available for this survey. Let’s explore the demographic information from this cycle. These data are mostly available in the form of SAS XPT format (see below).\n\n\n\n\n\n\n\n\nStep 3: We can download the XPT data in the local PC folder and read the data into R as as follows:\n\n\nShow the codeDEMO <- read.xport(\"Data/accessing/DEMO_I.XPT\")\n\n\n\n\n\n\n\nStep 4: Once data is imported in RStudio, we will see the DEMO object listed under data window (see below):\n\n\n\n\n\n\n\n\nStep 5: We can also check the variable names in this DEMO dataset as follows:\n\n\nShow the codenames(DEMO)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\n\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the DEMO data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\nStep 7: There is a column name associated with each column, e.g., DMDHSEDU in the first column in the above Figure. To understand what the column names mean in this Figure, we need to take a look at the codebook. To access codebook, click the 'DEMO|Doc' link (in step 2). This will show the data documentation and associated codebook (see the next Figure).\n\n\n\n\n\n\n\n\nStep 8: We can see a link for the column or variable DMDHSEDU in the table of content (in the above Figure). Clicking that link will provide us further information about what this variable means (see the next Figure).\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count and cumulative (from the above Figure) matches with what we get from the DEMO data we just imported (particularly, for the DMDHSEDU variable):\n\n\nShow the codetable(DEMO$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(DEMO$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(DEMO$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nAccessing NHANES Data Using R Packages\nnhanesA package\n\nShow the codelibrary(nhanesA)\n\n\n\n\n\n\n\n\nTip\n\n\n\nR package nhanesA provides a convenient way to download and analyze NHANES survey data.\n\n\n\n\nRNHANES (Susmann 2016) is another packages for downloading the NHANES data easily.\n\n\nStep 1: Witin the CDC website, NHANES data are available in 5 categories\n\nDemographics (DEMO)\nDietary (DIET)\nExamination (EXAM)\nLaboratory (LAB)\nQuestionnaire (Q)\n\n\n\nTo get a list of available variables within a data file, we run the following command (e.g., we check variable names within DEMO data):\n\nShow the codenhanesTables(data_group='DEMO', year=2015)\n\n\n\n  \n\n\n\n\n\nStep 2: We can obtain the summaries of the downloaded data as follows (see below):\n\n\nShow the codedemo <- nhanes('DEMO_I')\nnames(demo)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\ntable(demo$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(demo$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(demo$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nReferences\n\n\n\n\nCDC. 2023. “NHANES Web Tutorial Frequently Asked Questions (FAQs).” https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/faq.aspx.\n\n\nCDC,NCHS. 2023. “National Health and Nutrition Examination Survey Data.” https://wwwn.cdc.gov/nchs/nhanes/.\n\n\nSusmann, Herb. 2016. RNHANES: Facilitates Analysis of CDC NHANES Data. https://CRAN.R-project.org/package=RNHANES."
  },
  {
    "objectID": "accessing4.html#references",
    "href": "accessing4.html#references",
    "title": "Reproducing results",
    "section": "References",
    "text": "References\n\n\n\n\nDhana, A. 2023. “R & Python for Data Science.” https://datascienceplus.com/.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91."
  },
  {
    "objectID": "accessingF.html",
    "href": "accessingF.html",
    "title": "Functions for data accessing",
    "section": "",
    "text": "The section introduces a set of R functions useful for accessing and processing complex survey data, providing their descriptions and the packages they belong to.\n\n\n\n\n\n Function_name \n    Package_name \n    Description \n  \n\n\n apply \n    base \n    Applies a function over an array or matrix. \n  \n\n cut \n    base \n    Converts a numeric variable to a factor variable. \n  \n\n merge \n    base/data.table \n    Merges multiple datasets. \n  \n\n names \n    base \n    Retrieves the names of an object. \n  \n\n nhanes \n    nhanesA \n    Downloads a NHANES datafile. \n  \n\n nhanesTables \n    nhanesA \n    Lists available variables within a datafile. \n  \n\n nhanesTranslate \n    nhanesA \n    Encodes categorical variables to match with certain standards, e.g., CDC website. \n  \n\n recode \n    car \n    Recodes a variable. \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "accessingQ.html",
    "href": "accessingQ.html",
    "title": "Quiz on accessing",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes on data wrangling.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "researchquestion.html#predictive-question",
    "href": "researchquestion.html#predictive-question",
    "title": "Research question",
    "section": "Predictive question",
    "text": "Predictive question\nThe first tutorial serves to educate the user on how to utilize the RHC dataset to answer a predictive research question: developing a prediction model for the length of stay. The tutorial equips users with the skills to clean and process raw data, transforming it into an analyzable format, and introduces concepts that will be foundational for subsequent analysis.\nThe second tutorial provides an in-depth guide on how to build a predictive model for Diastolic blood pressure using the NHANES dataset for the years 2013-14."
  },
  {
    "objectID": "researchquestion.html#causal-question",
    "href": "researchquestion.html#causal-question",
    "title": "Research question",
    "section": "Causal question",
    "text": "Causal question\nThe third tutorial aims to guide a study on the relationship between Osteoarthritis (OA) and cardiovascular diseases (CVD) among Canadian adults from 2001-2005. Utilizing the Canadian Community Health Survey (CCHS) cycle 1.1-3.1, the study intends to explore whether OA increases (more accurately, whether associated with) the risk of developing CVD.\nThe NHANES dataset was analyzed in this forth tutorial to explore the relationship between health predictors and cholesterol levels (association/causal). After refining the survey design and handling missing data, regression models were built using varying predictors. Standard error computations and p-values were derived, adjusting for the survey’s unique structure.\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\nReference\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nThabane, Lehana, Tara Thomas, Chenglin Ye, and James Paul. 2009. “Posing the Research Question: Not so Simple.” Canadian Journal of Anesthesia/Journal Canadien d’anesthésie 56 (1): 71–79."
  },
  {
    "objectID": "researchquestion1.html",
    "href": "researchquestion1.html",
    "title": "Predictive question (1)",
    "section": "",
    "text": "Show the code# Load required packages\nrequire(tableone)\nrequire(Publish)\nrequire(MatchIt)\nrequire(cobalt)\nrequire(ggplot2)\n\n\nWorking with a Predictive question using RHC\nThis tutorial delves into processing and understanding the RHC dataset, which pertains to patients in the intensive care unit. The dataset is particularly centered around the implications of using right heart catheterization (RHC) in the early phases of care, with a focus on comparing two patient groups: those who received the RHC procedure and those who did not. The key outcome being analyzed is the 30-day survival rate. We will use this as an example to explain how to work with a predictive research question to build the analytic data.\n\n\nLink for the RHC dataset\n(Connors et al. 1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as\n\n\nlength of stay (hospital stay; measured continuously)\n\ndeath within certain period (death at any time up to 180 Days; measured as a binary variable)\n\nThe original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above.\nBut we will use this data as a case study for our prediction modelling. Traditional PICOT framework is designed primarily for clinical questions related to interventions, so when applying it to other areas like predictive modeling, some creative adaptation is needed.\n\n\n\n\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nNot applicable, as we are dealing with a prediction model here\n\n\nC\nNot applicable, as we are dealing with a prediction model here\n\n\nO\nin-hospital mortality\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\n\n\n\nWe are interested in developing a prediction model for the length of stay.\nData download\nData is freely available from Vanderbilt Biostatistics, variable list is available here, and the article is freely available from researchgate.\n\n\nRHC Data amd search for right heart catheterization dataset\n\nVariable list\n\nArticle\n\n\nLet us download the dataset and save it for later use.\n\nShow the code# Load the dataset\nObsData <- read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE)\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhc.RDS\")\n\n\nCreating analytic data\nNow, we show the process of preparing analytic data, so that the variables generally match with the way the authors were coded in the original article. Below we show the process of creating the analytic data.\nAdd column for outcome: length of stay\n\nShow the code# Length.of.Stay = date of discharge - study admission date\nObsData$Length.of.Stay <- ObsData$dschdte - ObsData$sadmdte\n\n# Length.of.Stay = date of death - study admission date if date of discharge not available\nObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] <- \n  ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - \n  ObsData$sadmdte[is.na(ObsData$Length.of.Stay)]\n\n\nRecoding column for outcome: death\n\n\n\n\n\n\nTip\n\n\n\nHere we use the ifelse function to create a categorical variable. Other related functions are cut, car.\n\n\nLet us recode our outcome variable as a binary variable:\n\nShow the codeObsData$death <- ifelse(ObsData$death == \"Yes\", 1, 0)\n\n\nRemove unnecessary outcomes\nOur next task is to remove unnecessary outcomes:\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to drop variables from a dataset. E.g., without using any package and using the select function from the dplyr package.\n\n\n\nShow the codeObsData <- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, \n                            t3d30, dth30, surv2md1))\n\n\nRemove unnecessary and problematic variables\nNow we will drop unnecessary and problematic variables:\n\nShow the codeObsData <- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, urin1, cat2))\n\n\nBasic data cleanup\nNow we will do some basic cleanup.\n\n\n\n\n\n\nTip\n\n\n\nWe an use the lapply function to convert all categorical variables to factors at once. Not that a similar function to lapply is sapply. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\n\nShow the code# convert all categorical variables to factors\nfactors <- c(\"cat1\", \"ca\", \"death\", \"cardiohx\", \"chfhx\", \n             \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n             \"transhx\", \"amihx\", \"sex\", \"dnr1\", \"ninsclas\", \n             \"resp\", \"card\", \"neuro\", \"gastr\", \"renal\", \"meta\", \n             \"hema\", \"seps\", \"trauma\", \"ortho\", \"race\", \n             \"income\")\nObsData[factors] <- lapply(ObsData[factors], as.factor)\n\n# convert RHC.use (RHC vs. No RHC) to a binary variable\nObsData$RHC.use <- ifelse(ObsData$swang1 == \"RHC\", 1, 0)\nObsData <- dplyr::select(ObsData, !swang1)\n\n# Categorize the variables to match with the original paper\nObsData$age <- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf),\n                   right=FALSE)\nObsData$race <- factor(ObsData$race, levels=c(\"white\",\"black\",\"other\"))\nObsData$sex <- as.factor(ObsData$sex)\nObsData$sex <- relevel(ObsData$sex, ref = \"Male\")\nObsData$cat1 <- as.factor(ObsData$cat1)\nlevels(ObsData$cat1) <- c(\"ARF\",\"CHF\",\"Other\",\"Other\",\"Other\",\n                          \"Other\",\"Other\",\"MOSF\",\"MOSF\")\nObsData$ca <- as.factor(ObsData$ca)\nlevels(ObsData$ca) <- c(\"Metastatic\",\"None\",\"Localized (Yes)\")\nObsData$ca <- factor(ObsData$ca, levels=c(\"None\", \"Localized (Yes)\",\n                                          \"Metastatic\"))\n\n\nRename variables\n\nShow the code# Rename the variables\nnames(ObsData) <- c(\"Disease.category\", \"Cancer\", \"Death\", \"Cardiovascular\", \n                    \"Congestive.HF\", \"Dementia\", \"Psychiatric\", \"Pulmonary\", \n                    \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\", \n                    \"Immunosupperssion\", \"Transfer.hx\", \"MI\", \"age\", \"sex\", \n                    \"edu\", \"DASIndex\", \"APACHE.score\", \"Glasgow.Coma.Score\", \n                    \"blood.pressure\", \"WBC\", \"Heart.rate\", \"Respiratory.rate\", \n                    \"Temperature\", \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\", \n                    \"Bilirubin\", \"Creatinine\", \"Sodium\", \"Potassium\", \"PaCo2\", \n                    \"PH\", \"Weight\", \"DNR.status\", \"Medical.insurance\", \n                    \"Respiratory.Diag\", \"Cardiovascular.Diag\", \n                    \"Neurological.Diag\", \"Gastrointestinal.Diag\", \"Renal.Diag\",\n                    \"Metabolic.Diag\", \"Hematologic.Diag\", \"Sepsis.Diag\", \n                    \"Trauma.Diag\", \"Orthopedic.Diag\", \"race\", \"income\", \n                    \"Length.of.Stay\", \"RHC.use\")\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhcAnalytic.RDS\")\n\n\nNotations\nlet us introduce with some notations:\n\n\nNotations\nExample in RHC study\n\n\n\n\n\\(Y_1\\): Observed outcome\nlength of stay\n\n\n\n\\(Y_2\\): Observed outcome\ndeath within 3 months\n\n\n\n\\(L\\): Covariates\nSee below\n\n\nBasic data exploration\nDimension\nLet us the how many rows and columns we have:\n\nShow the codedim(ObsData)\n#> [1] 5735   52\n\n\nComprehensive summary\nLet us see the summary statistics of the variables:\n\n\n\n\n\n\nTip\n\n\n\nTo see the comprehensive summary of the variables, we can use the skim function form skimr package or describe function from rms package\n\n\n\nShow the coderequire(skimr)\n#> Loading required package: skimr\n#> Warning: package 'skimr' was built under R version 4.3.1\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nDeath\n0\n1\nFALSE\n2\n1: 3722, 0: 2013\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, > $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\n\n\nWatch the video describing this chapter \nPredictive vs. causal models\nThe focus of current document is predictive models (e.g., predicting a health outcome).\n\n\n\n\n\nThe original article by Connors et al. (1996) focused on the association of\n\n\nConnors et al. (1996)\n\nright heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and\nthe health-outcomes (such as length of stay).\n\n\n\n\n\n\nThen the PICOT table changes as follows:\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nReceiving a right heart catheterization (RHC)\n\n\nC\nNot receiving a right heart catheterization (RHC)\n\n\nO\nlength of stay\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996."
  },
  {
    "objectID": "researchquestion2a.html#saving-data-for-later-use",
    "href": "researchquestion2a.html#saving-data-for-later-use",
    "title": "Predictive question (2a)",
    "section": "Saving data for later use",
    "text": "Saving data for later use\nIt’s a good practice to save your data for future reference.\n\nShow the codesave(analytic.data, file=\"Data/researchquestion/Analytic2013.RData\")"
  },
  {
    "objectID": "researchquestion2a.html#exercise-try-yourself",
    "href": "researchquestion2a.html#exercise-try-yourself",
    "title": "Predictive question (2a)",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nFollow the steps in the exercise section to deepen your understanding and broaden the analysis.\n\nThe following variables were not included in the above analysis, that were included in this paper: try including them and then create the new analytic data:\n\n\neducation level\npoverty income ratio\nSodium intake (mg)\nPotassium intake (mg)\n\n\nDownload the NHANES 2015-2016 and append with the NHANES 2013-2014 analytic data with same variables."
  },
  {
    "objectID": "researchquestion2a.html#references",
    "href": "researchquestion2a.html#references",
    "title": "Predictive question (2a)",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion2b.html#saving-for-further-use",
    "href": "researchquestion2b.html#saving-for-further-use",
    "title": "Predictive question (2b)",
    "section": "Saving for further use",
    "text": "Saving for further use\n\nShow the codesave(analytic.data1, file = \"NHANESanalytic.Rdata\")"
  },
  {
    "objectID": "researchquestion2b.html#regression-summary-optional",
    "href": "researchquestion2b.html#regression-summary-optional",
    "title": "Predictive question (2b)",
    "section": "Regression summary (Optional)",
    "text": "Regression summary (Optional)\n\n\nThis is optional content for this chapter. Later in confounding and predictive factor chapters, we will learn more about adjustment.\nDifferent General Linear Models (GLMs) are fit for diastolic blood pressure using variables like gender, marital status, etc.\nBivariate Regression summary (missing values included)\n\nShow the codefit1g <- glm(diastolic ~ gender, data=analytic.data1)\nsummary(fit1g)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -67.579   -7.091    0.421    6.909   50.421  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   71.5789     0.2352 304.299  < 2e-16 ***\n#> genderFemale  -2.4880     0.3278  -7.591 3.76e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 136.3911)\n#> \n#>     Null deviance: 700862  on 5082  degrees of freedom\n#> Residual deviance: 693003  on 5081  degrees of freedom\n#>   (686 observations deleted due to missingness)\n#> AIC: 39415\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codefit1m <- glm(diastolic ~ marital, data=analytic.data1)\nsummary(fit1m)\n#> \n#> Call:\n#> glm(formula = diastolic ~ marital, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -66.750   -6.838    1.162    7.250   51.250  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                70.7500     0.2138 330.901  < 2e-16 ***\n#> maritalNever married       -1.9116     0.4316  -4.429 9.69e-06 ***\n#> maritalPreviously married  -0.3953     0.4140  -0.955     0.34    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 137.5101)\n#> \n#>     Null deviance: 700840  on 5079  degrees of freedom\n#> Residual deviance: 698139  on 5077  degrees of freedom\n#>   (689 observations deleted due to missingness)\n#> AIC: 39434\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codestr(analytic.data1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 NA 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 NA 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 NA NA 1 1 NA 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 NA 3 NA 3 1 1 NA ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nfit13 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data1)\nsummary(fit13)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender + age.centred + race + marital + \n#>     systolic + smoke + alcohol, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -75.142   -6.090    0.811    7.074   33.512  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               30.92372    2.44895  12.627  < 2e-16 ***\n#> genderFemale              -0.34850    0.59830  -0.582 0.560325    \n#> age.centred               -0.13638    0.02142  -6.367 2.56e-10 ***\n#> raceNon-Hispanic Black     1.44736    1.11246   1.301 0.193443    \n#> raceNon-Hispanic White     0.59565    0.96117   0.620 0.535540    \n#> raceOther Hispanic         1.07369    1.29793   0.827 0.408234    \n#> raceOther race             2.02908    1.22998   1.650 0.099216 .  \n#> maritalNever married      -2.92801    0.79123  -3.701 0.000223 ***\n#> maritalPreviously married  0.44754    0.71911   0.622 0.533804    \n#> systolic                   0.31071    0.01763  17.624  < 2e-16 ***\n#> smokeSome days            -0.42177    0.97853  -0.431 0.666513    \n#> smokeNot at all            0.01796    0.65159   0.028 0.978008    \n#> alcohol                    0.17287    0.10994   1.572 0.116060    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 117.7142)\n#> \n#>     Null deviance: 219477  on 1515  degrees of freedom\n#> Residual deviance: 176924  on 1503  degrees of freedom\n#>   (4253 observations deleted due to missingness)\n#> AIC: 11546\n#> \n#> Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "researchquestion2b.html#check-missingness-optional",
    "href": "researchquestion2b.html#check-missingness-optional",
    "title": "Predictive question (2b)",
    "section": "Check missingness (optional)",
    "text": "Check missingness (optional)\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nThe plot_missing() function from the DataExplorer package is used to plot missing data.\n\nShow the coderequire(DataExplorer)\nplot_missing(analytic.data1)\n\n\n\n\n\nShow the coderequire(\"tableone\")\nvars = c(\"systolic\", \"smoke\", \"diastolic\", \"race\", \n                       \"age.centred\", \"gender\", \"marital\", \"alcohol\")\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nSetting correct variable types\nThe variables are explicitly set to either numeric or factor types.\nNote: In case any of the variables types are wrong, your table 1 output will be wrong. Better to be sure about what type of variable you want them to be (numeric or factor). For example, systolic should be numeric. Is it defined that way?\n\nShow the codemode(analytic.data1$systolic)\n#> [1] \"numeric\"\n\n\nIn case it wasn’t (often they can get converted to character), then here is the solution:\n\nShow the code# solution 1: one-by-one\nanalytic.data1$systolic <- as.numeric(as.character(analytic.data1$systolic))\nsummary(analytic.data1$systolic)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   110.0   120.0   123.2   134.0   228.0     658\n\n\n\nShow the code# solution 2: fixing all variable types at once\nnumeric.names <- c(\"systolic\", \"diastolic\", \"age.centred\", \"alcohol\")\nfactor.names <- vars[!vars %in% numeric.names]\nfactor.names\n#> [1] \"smoke\"   \"race\"    \"gender\"  \"marital\"\nanalytic.data1[,factor.names] <- lapply(analytic.data1[,factor.names] , factor)\nanalytic.data1[numeric.names] <- apply(X = analytic.data1[numeric.names],\n                                       MARGIN = 2, FUN =function (x) \n                                         as.numeric(as.character(x)))\nlevels(analytic.data1$marital)\n#> [1] \"Married\"            \"Never married\"      \"Previously married\"\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nComplete case analysis\nRemoves all rows containing NA.\n\nShow the codedim(analytic.data1)\n#> [1] 5769   14\nanalytic.data2 <- as.data.frame(na.omit(analytic.data1))\ndim(analytic.data2)\n#> [1] 1516   14\nplot_missing(analytic.data2)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data2, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         1516        \n#>   systolic (mean (SD))    123.29 (17.58)\n#>   smoke (%)                             \n#>      Every day               590 (38.9) \n#>      Some days               159 (10.5) \n#>      Not at all              767 (50.6) \n#>   diastolic (mean (SD))    70.11 (12.04)\n#>   race (%)                              \n#>      Mexican American        162 (10.7) \n#>      Non-Hispanic Black      292 (19.3) \n#>      Non-Hispanic White      778 (51.3) \n#>      Other Hispanic          126 ( 8.3) \n#>      Other race              158 (10.4) \n#>   age.centred (mean (SD))  -0.76 (16.71)\n#>   gender = Female (%)        626 (41.3) \n#>   marital (%)                           \n#>      Married                 858 (56.6) \n#>      Never married           300 (19.8) \n#>      Previously married      358 (23.6) \n#>   alcohol (mean (SD))       3.15 (2.76)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data2)\nrequire(Publish)\npublish(fit23)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          30.92 [26.12;35.72]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -0.35  [-1.52;0.82]   0.5603254 \n#>  age.centred                          -0.14 [-0.18;-0.09]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        1.45  [-0.73;3.63]   0.1934428 \n#>              Non-Hispanic White        0.60  [-1.29;2.48]   0.5355396 \n#>                  Other Hispanic        1.07  [-1.47;3.62]   0.4082336 \n#>                      Other race        2.03  [-0.38;4.44]   0.0992165 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.93 [-4.48;-1.38]   0.0002229 \n#>              Previously married        0.45  [-0.96;1.86]   0.5338035 \n#>     systolic                           0.31   [0.28;0.35]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.42  [-2.34;1.50]   0.6665127 \n#>                      Not at all        0.02  [-1.26;1.30]   0.9780080 \n#>      alcohol                           0.17  [-0.04;0.39]   0.1160603\n\n\nImputed data\nWe will learn about proper missing data analysis at a latter class. Currently, we will do a simple (but rather controversial) single imputation. In here we are simply using a random sampling to impute (probably the worst method, but we are just filling in some gaps for now).\n\nShow the coderequire(mice)\nimputation1 <- mice(analytic.data1,\n                   method = \"sample\",  \n                   m = 1, # Number of multiple imputations. \n                   maxit = 1 # Number of iteration; mostly useful for convergence\n                   )\n#> \n#>  iter imp variable\n#>   1   1  systolic  diastolic  marital  alcohol  smoke\n#> Warning: Number of logged events: 5\nanalytic.data.imputation1 <- complete(imputation1)\ndim(analytic.data.imputation1)\n#> [1] 5769   14\nstr(analytic.data.imputation1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 100 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 66 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 1 2 1 1 4 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 3 3 3 3 1 1 3 ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nplot_missing(analytic.data.imputation1)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data.imputation1, includeNA = TRUE,\n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    122.98 (18.06)\n#>   smoke (%)                             \n#>      Every day              2187 (37.9) \n#>      Some days               517 ( 9.0) \n#>      Not at all             3065 (53.1) \n#>   diastolic (mean (SD))    70.25 (11.80)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3383 (58.6) \n#>      Never married          1113 (19.3) \n#>      Previously married     1273 (22.1) \n#>   alcohol (mean (SD))       2.62 (2.28)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23i <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data.imputation1)\npublish(fit23i)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          38.74 [36.37;41.11]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -1.30 [-1.88;-0.72]     < 1e-04 \n#>  age.centred                          -0.12 [-0.14;-0.10]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        0.97  [-0.04;1.98]   0.0606987 \n#>              Non-Hispanic White        0.68  [-0.21;1.57]   0.1337626 \n#>                  Other Hispanic        0.91  [-0.32;2.13]   0.1471629 \n#>                      Other race        2.00   [0.93;3.08]   0.0002535 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.43 [-3.24;-1.63]     < 1e-04 \n#>              Previously married       -0.12  [-0.87;0.62]   0.7450923 \n#>     systolic                           0.26   [0.24;0.28]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.17  [-1.21;0.88]   0.7572027 \n#>                      Not at all       -0.19  [-0.80;0.42]   0.5387956 \n#>      alcohol                          -0.02  [-0.15;0.10]   0.7070437\n\n\nWe see some changes in the estimates. After imputing compared to complete case analysis, any changes dramatic (e.g., changing conclusion)?\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\n\nShow the coderequire(jtools)\nrequire(ggstance)\nrequire(broom.mixed)\nrequire(huxtable)\nexport_summs(fit23, fit23i)\n\n\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n(Intercept)\n30.92 ***\n38.74 ***\n\n\n\n(2.45)   \n(1.21)   \n\n\ngenderFemale\n-0.35    \n-1.30 ***\n\n\n\n(0.60)   \n(0.30)   \n\n\nage.centred\n-0.14 ***\n-0.12 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nraceNon-Hispanic Black\n1.45    \n0.97    \n\n\n\n(1.11)   \n(0.52)   \n\n\nraceNon-Hispanic White\n0.60    \n0.68    \n\n\n\n(0.96)   \n(0.45)   \n\n\nraceOther Hispanic\n1.07    \n0.91    \n\n\n\n(1.30)   \n(0.63)   \n\n\nraceOther race\n2.03    \n2.00 ***\n\n\n\n(1.23)   \n(0.55)   \n\n\nmaritalNever married\n-2.93 ***\n-2.43 ***\n\n\n\n(0.79)   \n(0.41)   \n\n\nmaritalPreviously married\n0.45    \n-0.12    \n\n\n\n(0.72)   \n(0.38)   \n\n\nsystolic\n0.31 ***\n0.26 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nsmokeSome days\n-0.42    \n-0.17    \n\n\n\n(0.98)   \n(0.53)   \n\n\nsmokeNot at all\n0.02    \n-0.19    \n\n\n\n(0.65)   \n(0.31)   \n\n\nalcohol\n0.17    \n-0.02    \n\n\n\n(0.11)   \n(0.07)   \n\n\nN\n1516       \n5769       \n\n\nAIC\n11545.85    \n43965.77    \n\n\nBIC\n11620.38    \n44059.01    \n\n\nPseudo R2\n0.19    \n0.15    \n\n *** p < 0.001;  ** p < 0.01;  * p < 0.05.\n\n\nShow the codeplot_summs(fit23, fit23i)\n\n\n\nShow the code# plot_summs(fit23, fit23i, plot.distributions = TRUE)"
  },
  {
    "objectID": "researchquestion2b.html#exercise-try-yourself",
    "href": "researchquestion2b.html#exercise-try-yourself",
    "title": "Predictive question (2b)",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nIn this lab, we have done multiple steps that could be improved. One of them was single imputation by random sampling. What other ad hoc method you could use to impute the factor variables?"
  },
  {
    "objectID": "researchquestion2b.html#references",
    "href": "researchquestion2b.html#references",
    "title": "Predictive question (2b)",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "href": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "title": "Causal question (1)",
    "section": "Naive Analysis of combined 3 cycles",
    "text": "Naive Analysis of combined 3 cycles\nIn the current analysis, we will simply consider all of the variables under consideration as ‘confounders’, and include in our analysis. Later we will perform a refined analysis.\nSummary of the analytic data\nIncluding missing values\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic <- c123sub3\ndim(analytic)\n#> [1] 241380     17\n\nrequire(\"tableone\")\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, includeNA = TRUE)\n#>                       \n#>                        Overall       \n#>   n                    241380        \n#>   CVD = event (%)        7044 ( 2.9) \n#>   age (%)                            \n#>      20-39 years       108161 (44.8) \n#>      40-49 years        59690 (24.7) \n#>      50-59 years        52685 (21.8) \n#>      60-64 years        20844 ( 8.6) \n#>   sex = Male (%)       114104 (47.3) \n#>   income (%)                         \n#>      $29,999 or less    48005 (19.9) \n#>      $30,000-$49,999    49496 (20.5) \n#>      $50,000-$79,999    61093 (25.3) \n#>      $80,000 or more    57056 (23.6) \n#>      NA                 25730 (10.7) \n#>   race (%)                           \n#>      Non-white          25840 (10.7) \n#>      White             210307 (87.1) \n#>      NA                  5233 ( 2.2) \n#>   bmicat (%)                         \n#>      Normal            103378 (42.8) \n#>      Overweight        120423 (49.9) \n#>      Underweight         8964 ( 3.7) \n#>      NA                  8615 ( 3.6) \n#>   phyact (%)                         \n#>      Active             57033 (23.6) \n#>      Inactive          117516 (48.7) \n#>      Moderate           60164 (24.9) \n#>      NA                  6667 ( 2.8) \n#>   smoke (%)                          \n#>      Current smoker     71321 (29.5) \n#>      Former smoker      97845 (40.5) \n#>      Never smoker       71397 (29.6) \n#>      NA                   817 ( 0.3) \n#>   fruit (%)                          \n#>      0-3 daily serving  56256 (23.3) \n#>      4-6 daily serving  96177 (39.8) \n#>      6+ daily serving   45861 (19.0) \n#>      NA                 43086 (17.8) \n#>   painmed (%)                        \n#>      No                 11141 ( 4.6) \n#>      Yes                25743 (10.7) \n#>      NA                204496 (84.7) \n#>   ht (%)                             \n#>      No                213432 (88.4) \n#>      Yes                27592 (11.4) \n#>      NA                   356 ( 0.1) \n#>   copd (%)                           \n#>      No                192608 (79.8) \n#>      Yes                 1353 ( 0.6) \n#>      NA                 47419 (19.6) \n#>   diab (%)                           \n#>      No                232486 (96.3) \n#>      Yes                 8811 ( 3.7) \n#>      NA                    83 ( 0.0) \n#>   edu (%)                            \n#>      < 2ndary           37775 (15.6) \n#>      2nd grad.          44376 (18.4) \n#>      Other 2nd grad.    19273 ( 8.0) \n#>      Post-2nd grad.    136031 (56.4) \n#>      NA                  3925 ( 1.6)\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control        OA            p      test\n#>   n                    221029         20351                    \n#>   CVD = event (%)        5429 ( 2.5)   1615 ( 7.9)  <0.001     \n#>   age (%)                                           <0.001     \n#>      20-39 years       106003 (48.0)   2158 (10.6)             \n#>      40-49 years        55569 (25.1)   4121 (20.2)             \n#>      50-59 years        43706 (19.8)   8979 (44.1)             \n#>      60-64 years        15751 ( 7.1)   5093 (25.0)             \n#>   sex = Male (%)       107729 (48.7)   6375 (31.3)  <0.001     \n#>   income (%)                                        <0.001     \n#>      $29,999 or less    42019 (19.0)   5986 (29.4)             \n#>      $30,000-$49,999    45090 (20.4)   4406 (21.7)             \n#>      $50,000-$79,999    56754 (25.7)   4339 (21.3)             \n#>      $80,000 or more    53637 (24.3)   3419 (16.8)             \n#>      NA                 23529 (10.6)   2201 (10.8)             \n#>   race (%)                                          <0.001     \n#>      Non-white          24681 (11.2)   1159 ( 5.7)             \n#>      White             191513 (86.6)  18794 (92.3)             \n#>      NA                  4835 ( 2.2)    398 ( 2.0)             \n#>   bmicat (%)                                        <0.001     \n#>      Normal             96697 (43.7)   6681 (32.8)             \n#>      Overweight        107871 (48.8)  12552 (61.7)             \n#>      Underweight         8490 ( 3.8)    474 ( 2.3)             \n#>      NA                  7971 ( 3.6)    644 ( 3.2)             \n#>   phyact (%)                                        <0.001     \n#>      Active             52942 (24.0)   4091 (20.1)             \n#>      Inactive          106580 (48.2)  10936 (53.7)             \n#>      Moderate           55222 (25.0)   4942 (24.3)             \n#>      NA                  6285 ( 2.8)    382 ( 1.9)             \n#>   smoke (%)                                         <0.001     \n#>      Current smoker     65398 (29.6)   5923 (29.1)             \n#>      Former smoker      88210 (39.9)   9635 (47.3)             \n#>      Never smoker       66663 (30.2)   4734 (23.3)             \n#>      NA                   758 ( 0.3)     59 ( 0.3)             \n#>   fruit (%)                                         <0.001     \n#>      0-3 daily serving  52140 (23.6)   4116 (20.2)             \n#>      4-6 daily serving  87951 (39.8)   8226 (40.4)             \n#>      6+ daily serving   41606 (18.8)   4255 (20.9)             \n#>      NA                 39332 (17.8)   3754 (18.4)             \n#>   painmed (%)                                       <0.001     \n#>      No                 10624 ( 4.8)    517 ( 2.5)             \n#>      Yes                23084 (10.4)   2659 (13.1)             \n#>      NA                187321 (84.7)  17175 (84.4)             \n#>   ht (%)                                            <0.001     \n#>      No                198550 (89.8)  14882 (73.1)             \n#>      Yes                22142 (10.0)   5450 (26.8)             \n#>      NA                   337 ( 0.2)     19 ( 0.1)             \n#>   copd (%)                                          <0.001     \n#>      No                173224 (78.4)  19384 (95.2)             \n#>      Yes                  938 ( 0.4)    415 ( 2.0)             \n#>      NA                 46867 (21.2)    552 ( 2.7)             \n#>   diab (%)                                          <0.001     \n#>      No                213910 (96.8)  18576 (91.3)             \n#>      Yes                 7046 ( 3.2)   1765 ( 8.7)             \n#>      NA                    73 ( 0.0)     10 ( 0.0)             \n#>   edu (%)                                           <0.001     \n#>      < 2ndary           32884 (14.9)   4891 (24.0)             \n#>      2nd grad.          40950 (18.5)   3426 (16.8)             \n#>      Other 2nd grad.    17808 ( 8.1)   1465 ( 7.2)             \n#>      Post-2nd grad.    125772 (56.9)  10259 (50.4)             \n#>      NA                  3615 ( 1.6)    310 ( 1.5)\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLet us investigate why pain medication has so much missing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional content respondent (cycle 3.1):\n\n\n\n\n\nIn cycle 2.1, only 21,755 out of 134,072 responded to optional medication component.\nComplete case analysis\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic2 <- as.data.frame(na.omit(c123sub3))\ndim(analytic2)\n#> [1] 21623    17\n\n\ntab1 <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, includeNA = TRUE)\nprint(tab1, showAllLevels = TRUE)\n#>              \n#>               level             Overall      \n#>   n                             21623        \n#>   CVD (%)     0 event           20917 (96.7) \n#>               event               706 ( 3.3) \n#>   age (%)     20-39 years        7119 (32.9) \n#>               40-49 years        7024 (32.5) \n#>               50-59 years        5457 (25.2) \n#>               60-64 years        2023 ( 9.4) \n#>   sex (%)     Female            10982 (50.8) \n#>               Male              10641 (49.2) \n#>   income (%)  $29,999 or less    4054 (18.7) \n#>               $30,000-$49,999    4461 (20.6) \n#>               $50,000-$79,999    6600 (30.5) \n#>               $80,000 or more    6508 (30.1) \n#>   race (%)    Non-white          2488 (11.5) \n#>               White             19135 (88.5) \n#>   bmicat (%)  Normal             8993 (41.6) \n#>               Overweight        11739 (54.3) \n#>               Underweight         891 ( 4.1) \n#>   phyact (%)  Active             5502 (25.4) \n#>               Inactive          10495 (48.5) \n#>               Moderate           5626 (26.0) \n#>   smoke (%)   Current smoker     5887 (27.2) \n#>               Former smoker      9368 (43.3) \n#>               Never smoker       6368 (29.5) \n#>   fruit (%)   0-3 daily serving  5806 (26.9) \n#>               4-6 daily serving 10730 (49.6) \n#>               6+ daily serving   5087 (23.5) \n#>   painmed (%) No                 6197 (28.7) \n#>               Yes               15426 (71.3) \n#>   ht (%)      No                19014 (87.9) \n#>               Yes                2609 (12.1) \n#>   copd (%)    No                21475 (99.3) \n#>               Yes                 148 ( 0.7) \n#>   diab (%)    No                20760 (96.0) \n#>               Yes                 863 ( 4.0) \n#>   edu (%)     < 2ndary           2998 (13.9) \n#>               2nd grad.          4605 (21.3) \n#>               Other 2nd grad.    1509 ( 7.0) \n#>               Post-2nd grad.    12511 (57.9)\ntab1b <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, strata = \"OA\", includeNA = TRUE)\nprint(tab1b, showAllLevels = TRUE)\n#>              Stratified by OA\n#>               level             Control       OA           p      test\n#>   n                             19459         2164                    \n#>   CVD (%)     0 event           18917 (97.2)  2000 (92.4)  <0.001     \n#>               event               542 ( 2.8)   164 ( 7.6)             \n#>   age (%)     20-39 years        6915 (35.5)   204 ( 9.4)  <0.001     \n#>               40-49 years        6515 (33.5)   509 (23.5)             \n#>               50-59 years        4504 (23.1)   953 (44.0)             \n#>               60-64 years        1525 ( 7.8)   498 (23.0)             \n#>   sex (%)     Female             9521 (48.9)  1461 (67.5)  <0.001     \n#>               Male               9938 (51.1)   703 (32.5)             \n#>   income (%)  $29,999 or less    3413 (17.5)   641 (29.6)  <0.001     \n#>               $30,000-$49,999    3968 (20.4)   493 (22.8)             \n#>               $50,000-$79,999    6023 (31.0)   577 (26.7)             \n#>               $80,000 or more    6055 (31.1)   453 (20.9)             \n#>   race (%)    Non-white          2370 (12.2)   118 ( 5.5)  <0.001     \n#>               White             17089 (87.8)  2046 (94.5)             \n#>   bmicat (%)  Normal             8277 (42.5)   716 (33.1)  <0.001     \n#>               Overweight        10356 (53.2)  1383 (63.9)             \n#>               Underweight         826 ( 4.2)    65 ( 3.0)             \n#>   phyact (%)  Active             4986 (25.6)   516 (23.8)   0.190     \n#>               Inactive           9417 (48.4)  1078 (49.8)             \n#>               Moderate           5056 (26.0)   570 (26.3)             \n#>   smoke (%)   Current smoker     5247 (27.0)   640 (29.6)  <0.001     \n#>               Former smoker      8363 (43.0)  1005 (46.4)             \n#>               Never smoker       5849 (30.1)   519 (24.0)             \n#>   fruit (%)   0-3 daily serving  5290 (27.2)   516 (23.8)  <0.001     \n#>               4-6 daily serving  9686 (49.8)  1044 (48.2)             \n#>               6+ daily serving   4483 (23.0)   604 (27.9)             \n#>   painmed (%) No                 5859 (30.1)   338 (15.6)  <0.001     \n#>               Yes               13600 (69.9)  1826 (84.4)             \n#>   ht (%)      No                17356 (89.2)  1658 (76.6)  <0.001     \n#>               Yes                2103 (10.8)   506 (23.4)             \n#>   copd (%)    No                19359 (99.5)  2116 (97.8)  <0.001     \n#>               Yes                 100 ( 0.5)    48 ( 2.2)             \n#>   diab (%)    No                18751 (96.4)  2009 (92.8)  <0.001     \n#>               Yes                 708 ( 3.6)   155 ( 7.2)             \n#>   edu (%)     < 2ndary           2527 (13.0)   471 (21.8)  <0.001     \n#>               2nd grad.          4173 (21.4)   432 (20.0)             \n#>               Other 2nd grad.    1364 ( 7.0)   145 ( 6.7)             \n#>               Post-2nd grad.    11395 (58.6)  1116 (51.6)"
  },
  {
    "objectID": "researchquestion3.html#save-data-for-later",
    "href": "researchquestion3.html#save-data-for-later",
    "title": "Causal question (1)",
    "section": "Save data for later",
    "text": "Save data for later\n\nShow the codesave(analytic, analytic2, cc123a, file = \"Data/researchquestion/OA123CVD.RData\")\n\n\n\n\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "researchquestion4.html",
    "href": "researchquestion4.html",
    "title": "Causal question (2)",
    "section": "",
    "text": "Working with a causal question using NHANES\nWe are interested in exploring the relationship between diabetes (binary exposure variable defined as whether the doctor ever told the participant has diabetes) and cholesterol (binary outcome variable defined as whether total cholesterol is more than 200 mg/dL). Below is the PICOT:\n\n\nPICOT element\nDescription\n\n\n\nP\nUS adults\n\n\nI\nDiabetes\n\n\nC\nNo diabetes\n\n\nO\nTotal cholesterol > 200 mg/dL\n\n\nT\n2017–2018\n\n\n\nFirst, we will prepare the analytic dataset from NHANES 2017–2018.\nSecond, we will work with subset of data to assess the association between diabetes and cholesterol, and to get proper SE and 95% CI for the estimate. We emphasize the correct usage of the survey’s design features (correct handling of survey design elements, such as stratification, clustering, and weighting) to obtain accurate population-level estimates.\n\nShow the code# Load required packages\nrequire(SASxport)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(nhanesA)\nrequire(survey)\nrequire(Publish)\nrequire(jtools)\n\n\nSteps for creating analytic dataset\nWe will combine multiple components (e.g., demographic, blood pressure) using the unique identifier to create our analytic dataset.\n\n\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\n\n\n\nDownload and Subsetting to retain only the useful variables\nSearch literature for the relevant variables, and then see if some of them are available in the NHANES data.\n\n\nPeters, Fabian, and Levy (2014)\nAn an example, let us assume that variables listed in the following figures are known to be useful. Then we will try to indentify, in which NHANES component we have these variables.\n\n\nRefer to the earlier chapter to get a more detailed understanding of how we search for variables within NHANES.\n\n\n\n\n\n\n\nNHANES Data Components:\n\nDemographic (variables like age, gender, income, etc.)\nBlood Pressure (Diastolic and Systolic pressure)\nBody Measures (BMI, Waist Circumference, etc.)\nSmoking Status (Current smoker or not)\nCholesterol (Total cholesterol in different units)\nBiochemistry Profile (Triglycerides, Uric acid, etc.)\nPhysical Activity (Vigorous work and recreational activities)\nDiabetes (Whether the respondent has been told by a doctor that they have diabetes)\n\nDemographic component:\n\nShow the codedemo <- nhanes('DEMO_J') # Both males and females 0 YEARS - 150 YEARS\ndemo <- demo[c(\"SEQN\", # Respondent sequence number\n                 \"RIAGENDR\", # gender\n                 \"RIDAGEYR\", # Age in years at screening\n                 \"DMDBORN4\", # Country of birth\n                 \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                 \"DMDEDUC3\", # Education level - Children/Youth 6-19\n                 \"DMDEDUC2\", # Education level - Adults 20+\n                 \"DMDMARTL\", # Marital status: 20 YEARS - 150 YEARS\n                 \"INDHHIN2\", # Total household income\n                 \"WTMEC2YR\", \"SDMVPSU\", \"SDMVSTRA\")]\ndemo_vars <- names(demo) # nhanesTableVars('DEMO', 'DEMO_J', namesonly=TRUE)\ndemo1 <- nhanesTranslate('DEMO_J', demo_vars, data=demo)\n#> No translation table is available for SEQN\n#> Translated columns: RIAGENDR DMDBORN4 RIDRETH3 DMDEDUC3 DMDEDUC2 DMDMARTL INDHHIN2\n\n\nBlood pressure component:\n\nShow the codebpx <- nhanes('BPX_J')\nbpx <- bpx[c(\"SEQN\", # Respondent sequence number\n             \"BPXDI1\", #Diastolic: Blood pres (1st rdg) mm Hg\n             \"BPXSY1\" # Systolic: Blood pres (1st rdg) mm Hg\n             )]\nbpx_vars <- names(bpx) \nbpx1 <- nhanesTranslate('BPX_J', bpx_vars, data=bpx)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx): No columns were\n#> translated\n\n\nBody measure component:\n\nShow the codebmi <- nhanes('BMX_J')\nbmi <- bmi[c(\"SEQN\", # Respondent sequence number\n               \"BMXWT\", # Weight (kg) \n               \"BMXHT\", # Standing Height (cm)\n               \"BMXBMI\", # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\n               #\"BMDBMIC\", # BMI Category - Children/Youth # 2 YEARS - 19 YEARS\n               \"BMXWAIST\" # Waist Circumference (cm): 2 YEARS - 150 YEARS\n               )]\nbmi_vars <- names(bmi) \nbmi1 <- nhanesTranslate('BMX_J', bmi_vars, data=bmi)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BMX_J\", bmi_vars, data = bmi): No columns were\n#> translated\n\n\nSmoking component:\n\nShow the codesmq <- nhanes('SMQ_J')\nsmq <- smq[c(\"SEQN\", # Respondent sequence number\n               \"SMQ040\" # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\n               )]\nsmq_vars <- names(smq) \nsmq1 <- nhanesTranslate('SMQ_J', smq_vars, data=smq)\n#> No translation table is available for SEQN\n#> Translated columns: SMQ040\n\n\n\nShow the code# alq <- nhanes('ALQ_J')\n# alq <- alq[c(\"SEQN\", # Respondent sequence number\n#                \"ALQ130\" # Avg # alcoholic drinks/day - past 12 mos\n#                # 18 YEARS - 150 YEARS\n#                )]\n# alq_vars <- names(alq) \n# alq1 <- nhanesTranslate('ALQ_J', alq_vars, data=alq)\n\n\nCholesterol component:\n\nShow the codechl <- nhanes('TCHOL_J') # 6 YEARS - 150 YEARS\nchl <- chl[c(\"SEQN\", # Respondent sequence number\n               \"LBXTC\", # Total Cholesterol (mg/dL)\n               \"LBDTCSI\" # Total Cholesterol (mmol/L)\n               )]\nchl_vars <- names(chl) \nchl1 <- nhanesTranslate('TCHOL_J', chl_vars, data=chl)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"TCHOL_J\", chl_vars, data = chl): No columns were\n#> translated\n\n\nBiochemistry Profile component:\n\nShow the codetri <- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\ntri <- tri[c(\"SEQN\", # Respondent sequence number\n               \"LBXSTR\", # Triglycerides, refrig serum (mg/dL)\n               \"LBXSUA\", # Uric acid\n               \"LBXSTP\", # total Protein (g/dL)\n               \"LBXSTB\", # Total Bilirubin (mg/dL)\n               \"LBXSPH\", # Phosphorus (mg/dL)\n               \"LBXSNASI\", # Sodium (mmol/L)\n               \"LBXSKSI\", # Potassium (mmol/L)\n               \"LBXSGB\", # Globulin (g/dL)\n               \"LBXSCA\" # Total Calcium (mg/dL)\n               )]\ntri_vars <- names(tri) \ntri1 <- nhanesTranslate('BIOPRO_J', tri_vars, data=tri)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BIOPRO_J\", tri_vars, data = tri): No columns were\n#> translated\n\n\nPhysical activity component:\n\nShow the codepaq <- nhanes('PAQ_J')\npaq <- paq[c(\"SEQN\", # Respondent sequence number\n               \"PAQ605\", # Vigorous work activity \n               \"PAQ650\" # Vigorous recreational activities\n               )]\npaq_vars <- names(paq) \npaq1 <- nhanesTranslate('PAQ_J', paq_vars, data=paq)\n#> No translation table is available for SEQN\n#> Translated columns: PAQ605 PAQ650\n\n\nDiabetes component:\n\nShow the codediq <- nhanes('DIQ_J')\ndiq <- diq[c(\"SEQN\", # Respondent sequence number\n               \"DIQ010\" # Doctor told you have diabetes\n               )]\ndiq_vars <- names(diq) \ndiq1 <- nhanesTranslate('DIQ_J', diq_vars, data=diq)\n#> No translation table is available for SEQN\n#> Translated columns: DIQ010\n\n\nMerging all the datasets\n\n\n\n\n\n\nTip\n\n\n\nWe can use the merge or Reduce function to combine the datasets\n\n\n\nShow the codeanalytic.data7 <- Reduce(function(x,y) merge(x,y,by=\"SEQN\",all=TRUE) ,\n       list(demo1,bpx1,bmi1,smq1,chl1,tri1,paq1,diq1))\ndim(analytic.data7)\n#> [1] 9254   33\n\n\n\n\nAll these datasets are merged into one analytic dataset using the SEQN as the key. This can be done either all at once using the Reduce function or one by one (using merge once at a time).\n\nShow the code# Merging one by one\n# analytic.data0 <- merge(demo1, bpx1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data1 <- merge(analytic.data0, bmi1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data2 <- merge(analytic.data1, smq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data3 <- merge(analytic.data2, alq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data4 <- merge(analytic.data3, chl1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data5 <- merge(analytic.data4, tri1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data6 <- merge(analytic.data5, paq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data7 <- merge(analytic.data6, diq1, by = c(\"SEQN\"), all=TRUE)\n# dim(analytic.data7)\n\n\nCheck Target population and avoid zero-cell cross-tabulation\n\n\nThe dataset is then filtered to only include adults (20 years and older) and avoid zero-cell cross-tabulation.\nSee that marital status variable was restricted to 20 YEARS - 150 YEARS.\n\nShow the codestr(analytic.data7)\n#> 'data.frame':    9254 obs. of  33 variables:\n#>  $ SEQN    : num  93703 93704 93705 93706 93707 ...\n#>  $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 2 1 2 1 1 2 2 2 1 1 ...\n#>  $ RIDAGEYR: num  2 2 66 18 13 66 75 0 56 18 ...\n#>  $ DMDBORN4: Factor w/ 4 levels \"Born in 50 US states or Washingt\",..: 1 1 1 1 1 2 1 1 2 2 ...\n#>  $ RIDRETH3: Factor w/ 6 levels \"Mexican American\",..: 5 3 4 5 6 5 4 3 5 1 ...\n#>  $ DMDEDUC3: Factor w/ 17 levels \"Never attended / kindergarten on\",..: NA NA NA 16 7 NA NA NA NA 13 ...\n#>  $ DMDEDUC2: Factor w/ 7 levels \"Less than 9th grade\",..: NA NA 2 NA NA 1 4 NA 5 NA ...\n#>  $ DMDMARTL: Factor w/ 7 levels \"Married\",\"Widowed\",..: NA NA 3 NA NA 1 2 NA 1 NA ...\n#>  $ INDHHIN2: Factor w/ 16 levels \"$ 0 to $ 4,999\",..: 14 14 3 NA 10 6 2 14 14 4 ...\n#>  $ WTMEC2YR: num  8540 42567 8338 8723 7065 ...\n#>  $ SDMVPSU : num  2 1 2 2 1 2 1 1 2 2 ...\n#>  $ SDMVSTRA: num  145 143 145 134 138 138 136 134 134 147 ...\n#>  $ BPXDI1  : num  NA NA NA 74 38 NA 66 NA 68 68 ...\n#>  $ BPXSY1  : num  NA NA NA 112 128 NA 120 NA 108 112 ...\n#>  $ BMXWT   : num  13.7 13.9 79.5 66.3 45.4 53.5 88.8 10.2 62.1 58.9 ...\n#>  $ BMXHT   : num  88.6 94.2 158.3 175.7 158.4 ...\n#>  $ BMXBMI  : num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>  $ BMXWAIST: num  48.2 50 101.8 79.3 64.1 ...\n#>  $ SMQ040  : Factor w/ 3 levels \"Every day\",\"Some days\",..: NA NA 3 NA NA NA 1 NA NA 2 ...\n#>  $ LBXTC   : num  NA NA 157 148 189 209 176 NA 238 182 ...\n#>  $ LBDTCSI : num  NA NA 4.06 3.83 4.89 5.4 4.55 NA 6.15 4.71 ...\n#>  $ LBXSTR  : num  NA NA 95 92 110 72 132 NA 59 124 ...\n#>  $ LBXSUA  : num  NA NA 5.8 8 5.5 4.5 6.2 NA 4.2 5.8 ...\n#>  $ LBXSTP  : num  NA NA 7.3 7.1 8 7.1 7 NA 7.1 8.1 ...\n#>  $ LBXSTB  : num  NA NA 0.6 0.7 0.7 0.5 0.3 NA 0.3 0.8 ...\n#>  $ LBXSPH  : num  NA NA 4 4 4.3 3.3 3.5 NA 3.4 5.1 ...\n#>  $ LBXSNASI: num  NA NA 141 144 137 144 141 NA 140 141 ...\n#>  $ LBXSKSI : num  NA NA 4 4.4 3.3 4.4 4.1 NA 4.9 4.3 ...\n#>  $ LBXSGB  : num  NA NA 2.9 2.7 2.8 3.2 3.3 NA 3.1 3.3 ...\n#>  $ LBXSCA  : num  NA NA 9.2 9.6 10.1 9.5 9.9 NA 9.4 9.6 ...\n#>  $ PAQ605  : Factor w/ 3 levels \"Yes\",\"No\",\"Don't know\": NA NA 2 2 NA 2 2 NA 2 1 ...\n#>  $ PAQ650  : Factor w/ 2 levels \"Yes\",\"No\": NA NA 2 2 NA 2 2 NA 1 1 ...\n#>  $ DIQ010  : Factor w/ 4 levels \"Yes\",\"No\",\"Borderline\",..: 2 2 2 2 2 3 2 NA 2 2 ...\nhead(analytic.data7)\n\n\n\n  \n\n\nShow the codesummary(analytic.data7$RIDAGEYR)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   11.00   31.00   34.33   58.00   80.00\n\n\n\nShow the codedim(analytic.data7)\n#> [1] 9254   33\nanalytic.data8 <- analytic.data7\nanalytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR < 20] <- NA\n#analytic.data8 <- subset(analytic.data7, RIDAGEYR >= 20)\ndim(analytic.data8)\n#> [1] 9254   33\n\n\nGet rid of variables where target was less than 20 years of age accordingly.\n\nShow the codeanalytic.data8$DMDEDUC3 <- NULL # not relevant for adults\n#analytic.data8$BMDBMIC <- NULL # not relevant for adults\n\n\nGet rid of invalid responses\n\n\nVariables that have “Don’t Know” or “Refused” as responses are set to NA, effectively getting rid of invalid responses.\n\nShow the codefactor.names <- c(\"RIAGENDR\",\"DMDBORN4\",\"RIDRETH3\",\n                  \"DMDEDUC2\",\"DMDMARTL\",\"INDHHIN2\", \n                  \"SMQ040\", \"PAQ605\", \"PAQ650\", \"DIQ010\")\nnumeric.names <- c(\"SEQN\",\"RIDAGEYR\",\"WTMEC2YR\",\n                   \"SDMVPSU\", \"SDMVSTRA\",\n                   \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"BMXHT\",\n                   \"BMXBMI\", \"BMXWAIST\",\n                   \"ALQ130\", \"LBXTC\", \"LBDTCSI\", \n                   \"LBXSTR\", \"LBXSUA\", \"LBXSTP\", \"LBXSTB\", \n                   \"LBXSPH\", \"LBXSNASI\", \"LBXSKSI\",\n                   \"LBXSGB\",\"LBXSCA\")\nanalytic.data8[factor.names] <- apply(X = analytic.data8[factor.names], \n                                      MARGIN = 2, FUN = as.factor)\n# analytic.data8[numeric.names] <- apply(X = analytic.data8[numeric.names], \n#                                        MARGIN = 2, FUN = \n#                                          function (x) as.numeric(as.character(x)))\n\n\n\nShow the codeanalytic.data9 <- analytic.data8\nanalytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == \"Don't Know\"] <- NA\n#analytic.data9 <- subset(analytic.data8, DMDBORN4 != \"Don't Know\")\ndim(analytic.data9)\n#> [1] 9254   32\n\nanalytic.data10 <- analytic.data9\nanalytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == \"Don't Know\"] <- NA\n#analytic.data10 <- subset(analytic.data9, DMDEDUC2 != \"Don't Know\")\ndim(analytic.data10)\n#> [1] 9254   32\n\nanalytic.data11 <- analytic.data10\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Don't Know\"] <- NA\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Refused\"] <- NA\n# analytic.data11 <- subset(analytic.data10, DMDMARTL != \"Don't Know\" & DMDMARTL != \"Refused\")\ndim(analytic.data11)\n#> [1] 9254   32\n\n\nanalytic.data12 <- analytic.data11\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Don't Know\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Refused\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Under $20,000\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"$20,000 and Over\"] <- NA\n# analytic.data12 <- subset(analytic.data11, INDHHIN2 != \"Don't know\" & INDHHIN2 !=  \"Refused\" & INDHHIN2 != \"Under $20,000\" & INDHHIN2 != \"$20,000 and Over\" )\ndim(analytic.data12)\n#> [1] 9254   32\n\n#analytic.data11 <- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )\n#dim(analytic.data11) # this are listed as NA anyway\n\nanalytic.data13 <- analytic.data12\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Don't know\"] <- NA\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Refused\"] <- NA\n# analytic.data13 <- subset(analytic.data12, PAQ605 != \"Don't know\" & PAQ605 != \"Refused\")\ndim(analytic.data13)\n#> [1] 9254   32\n\nanalytic.data14 <- analytic.data13\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Don't know\"] <- NA\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Refused\"] <- NA\n# analytic.data14 <- subset(analytic.data13, PAQ650 != \"Don't Know\" & PAQ650 != \"Refused\")\ndim(analytic.data14)\n#> [1] 9254   32\n\nanalytic.data15 <- analytic.data14\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Don't know\"] <- NA\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Refused\"] <- NA\n# analytic.data15 <- subset(analytic.data14, DIQ010 != \"Don't Know\" & DIQ010 != \"Refused\")\ndim(analytic.data15)\n#> [1] 9254   32\n\n\n# analytic.data15$ALQ130[analytic.data15$ALQ130 > 100] <- NA\n# summary(analytic.data15$ALQ130)\ntable(analytic.data15$SMQ040,useNA = \"always\")\n#> \n#>  Every day Not at all  Some days       <NA> \n#>        805       1338        216       6895\ntable(analytic.data15$PAQ605,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4461 1389 3404\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\n\n\nRecode values\nLet us recode the variables using the recode function:\n\nShow the coderequire(car)\n#> Loading required package: car\n#> Loading required package: carData\nanalytic.data15$RIDRETH3 <- recode(analytic.data15$RIDRETH3, \n                            \"c('Mexican American','Other Hispanic')='Hispanic'; \n                            'Non-Hispanic White'='White'; \n                            'Non-Hispanic Black'='Black';\n                            c('Non-Hispanic Asian',\n                               'Other Race - Including Multi-Rac')='Other';\n                               else=NA\")\nanalytic.data15$DMDEDUC2 <- recode(analytic.data15$DMDEDUC2, \n                            \"c('Some college or AA degree',\n                             'College graduate or above')='College'; \n                            c('9-11th grade (Includes 12th grad', \n                              'High school graduate/GED or equi')\n                               ='High.School'; \n                            'Less than 9th grade'='School';\n                               else=NA\")\nanalytic.data15$DMDMARTL <- recode(analytic.data15$DMDMARTL, \n                            \"c('Divorced','Separated','Widowed')\n                                ='Previously.married'; \n                            c('Living with partner', 'Married')\n                                ='Married'; \n                            'Never married'='Never.married';\n                               else=NA\")\nanalytic.data15$INDHHIN2 <- recode(analytic.data15$INDHHIN2, \n                            \"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999', \n                                 '$10,000 to $14,999', '$15,000 to $19,999', \n                                 '$20,000 to $24,999')='<25k';\n                            c('$25,000 to $34,999', '$35,000 to $44,999', \n                                 '$45,000 to $54,999') = 'Between.25kto54k';\n                            c('$55,000 to $64,999', '$65,000 to $74,999',\n                                 '$75,000 to $99,999')='Between.55kto99k';\n                            '$100,000 and Over'= 'Over100k';\n                               else=NA\")\nanalytic.data15$SMQ040 <- recode(analytic.data15$SMQ040, \n                            \"'Every day'='Every.day';\n                            'Not at all'='Not.at.all';\n                            'Some days'='Some.days';\n                               else=NA\")\nanalytic.data15$DIQ010 <- recode(analytic.data15$DIQ010, \n                            \"'No'='No';\n                            c('Yes', 'Borderline')='Yes';\n                               else=NA\")\n\n\n\n\nData types for various variables are set correctly; for instance, factor variables are converted to factor data types, and numeric variables to numeric data types.\nCheck missingness\n\n\n\n\n\n\nTip\n\n\n\nWe can use the plot_missing function to plot the profile of missing values, e.g., the percentage of missing per variable\n\n\n\nShow the coderequire(DataExplorer)\n#> Loading required package: DataExplorer\nplot_missing(analytic.data15)\n\n\n\n\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nCheck data summaries\n\nShow the codenames(analytic.data15)\n#>  [1] \"SEQN\"     \"RIAGENDR\" \"RIDAGEYR\" \"DMDBORN4\" \"RIDRETH3\" \"DMDEDUC2\"\n#>  [7] \"DMDMARTL\" \"INDHHIN2\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BPXDI1\"  \n#> [13] \"BPXSY1\"   \"BMXWT\"    \"BMXHT\"    \"BMXBMI\"   \"BMXWAIST\" \"SMQ040\"  \n#> [19] \"LBXTC\"    \"LBDTCSI\"  \"LBXSTR\"   \"LBXSUA\"   \"LBXSTP\"   \"LBXSTB\"  \n#> [25] \"LBXSPH\"   \"LBXSNASI\" \"LBXSKSI\"  \"LBXSGB\"   \"LBXSCA\"   \"PAQ605\"  \n#> [31] \"PAQ650\"   \"DIQ010\"\nnames(analytic.data15) <- c(\"ID\", \"gender\", \"age\", \"born\", \"race\", \"education\", \n\"married\", \"income\", \"weight\", \"psu\", \"strata\", \"diastolicBP\", \n\"systolicBP\", \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \n\"cholesterol\", \"cholesterolM2\", \"triglycerides\", \n\"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \n\"potassium\", \"globulin\", \"calcium\", \"physical.work\", \n\"physical.recreational\",\"diabetes\")\nrequire(\"tableone\")\n#> Loading required package: tableone\nCreateTableOne(data = analytic.data15, includeNA = TRUE)\n#>                                      \n#>                                       Overall            \n#>   n                                       9254           \n#>   ID (mean (SD))                      98329.50 (2671.54) \n#>   gender = Male (%)                       4557 (49.2)    \n#>   age (mean (SD))                        51.50 (17.81)   \n#>   born (%)                                               \n#>      Born in 50 US states or Washingt     7303 (78.9)    \n#>      Others                               1948 (21.1)    \n#>      Refused                                 2 ( 0.0)    \n#>      NA                                      1 ( 0.0)    \n#>   race (%)                                               \n#>      Black                                2115 (22.9)    \n#>      Hispanic                             2187 (23.6)    \n#>      Other                                1802 (19.5)    \n#>      White                                3150 (34.0)    \n#>   education (%)                                          \n#>      College                              3114 (33.7)    \n#>      High.School                          1963 (21.2)    \n#>      School                                479 ( 5.2)    \n#>      NA                                   3698 (40.0)    \n#>   married (%)                                            \n#>      Married                              3252 (35.1)    \n#>      Never.married                        1006 (10.9)    \n#>      Previously.married                   1305 (14.1)    \n#>      NA                                   3691 (39.9)    \n#>   income (%)                                             \n#>      <25k                                 1998 (21.6)    \n#>      Between.25kto54k                     2460 (26.6)    \n#>      Between.55kto99k                     1843 (19.9)    \n#>      Over100k                             1624 (17.5)    \n#>      NA                                   1329 (14.4)    \n#>   weight (mean (SD))                  34670.71 (43344.00)\n#>   psu (mean (SD))                         1.52 (0.50)    \n#>   strata (mean (SD))                    140.97 (4.20)    \n#>   diastolicBP (mean (SD))                67.84 (16.36)   \n#>   systolicBP (mean (SD))                121.33 (19.98)   \n#>   bodyweight (mean (SD))                 65.14 (32.89)   \n#>   bodyheight (mean (SD))                156.59 (22.26)   \n#>   bmi (mean (SD))                        26.58 (8.26)    \n#>   waist (mean (SD))                      89.93 (22.81)   \n#>   smoke (%)                                              \n#>      Every.day                             805 ( 8.7)    \n#>      Not.at.all                           1338 (14.5)    \n#>      Some.days                             216 ( 2.3)    \n#>      NA                                   6895 (74.5)    \n#>   cholesterol (mean (SD))               179.89 (40.60)   \n#>   cholesterolM2 (mean (SD))               4.65 (1.05)    \n#>   triglycerides (mean (SD))             137.44 (109.13)  \n#>   uric.acid (mean (SD))                   5.40 (1.48)    \n#>   protein (mean (SD))                     7.17 (0.44)    \n#>   bilirubin (mean (SD))                   0.46 (0.28)    \n#>   phosphorus (mean (SD))                  3.66 (0.59)    \n#>   sodium (mean (SD))                    140.32 (2.75)    \n#>   potassium (mean (SD))                   4.09 (0.36)    \n#>   globulin (mean (SD))                    3.09 (0.43)    \n#>   calcium (mean (SD))                     9.32 (0.37)    \n#>   physical.work (%)                                      \n#>      No                                   4461 (48.2)    \n#>      Yes                                  1389 (15.0)    \n#>      NA                                   3404 (36.8)    \n#>   physical.recreational (%)                              \n#>      No                                   4422 (47.8)    \n#>      Yes                                  1434 (15.5)    \n#>      NA                                   3398 (36.7)    \n#>   diabetes (%)                                           \n#>      No                                   7816 (84.5)    \n#>      Yes                                  1077 (11.6)    \n#>      NA                                    361 ( 3.9)\n\n\nCreate complete case data (for now)\n\nShow the codeanalytic.with.miss <- analytic.data15\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, 1,0)\nanalytic <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic)\n#> [1] 1562   33\n\n\nCreating Table 1 from the complete case data\n\nShow the coderequire(\"tableone\")\nCreateTableOne(data = analytic, includeNA = TRUE)\n#>                                  \n#>                                   Overall            \n#>   n                                   1562           \n#>   ID (mean (SD))                  98344.21 (2697.76) \n#>   gender = Male (%)                    959 (61.4)    \n#>   age (mean (SD))                    53.18 (17.18)   \n#>   born = Others (%)                    299 (19.1)    \n#>   race (%)                                           \n#>      Black                             324 (20.7)    \n#>      Hispanic                          284 (18.2)    \n#>      Other                             228 (14.6)    \n#>      White                             726 (46.5)    \n#>   education (%)                                      \n#>      College                           806 (51.6)    \n#>      High.School                       658 (42.1)    \n#>      School                             98 ( 6.3)    \n#>   married (%)                                        \n#>      Married                           921 (59.0)    \n#>      Never.married                     228 (14.6)    \n#>      Previously.married                413 (26.4)    \n#>   income (%)                                         \n#>      <25k                              484 (31.0)    \n#>      Between.25kto54k                  520 (33.3)    \n#>      Between.55kto99k                  331 (21.2)    \n#>      Over100k                          227 (14.5)    \n#>   weight (mean (SD))              48538.53 (54106.24)\n#>   psu (mean (SD))                     1.48 (0.50)    \n#>   strata (mean (SD))                141.18 (4.07)    \n#>   diastolicBP (mean (SD))            72.06 (14.17)   \n#>   systolicBP (mean (SD))            127.06 (19.11)   \n#>   bodyweight (mean (SD))             85.66 (22.41)   \n#>   bodyheight (mean (SD))            168.96 (9.30)    \n#>   bmi (mean (SD))                    29.96 (7.33)    \n#>   waist (mean (SD))                 102.98 (17.15)   \n#>   smoke (%)                                          \n#>      Every.day                         530 (33.9)    \n#>      Not.at.all                        903 (57.8)    \n#>      Some.days                         129 ( 8.3)    \n#>   cholesterol (mean (SD))           188.77 (43.51)   \n#>   cholesterolM2 (mean (SD))           4.88 (1.13)    \n#>   triglycerides (mean (SD))         154.71 (123.00)  \n#>   uric.acid (mean (SD))               5.62 (1.53)    \n#>   protein (mean (SD))                 7.09 (0.43)    \n#>   bilirubin (mean (SD))               0.46 (0.27)    \n#>   phosphorus (mean (SD))              3.53 (0.54)    \n#>   sodium (mean (SD))                140.14 (2.83)    \n#>   potassium (mean (SD))               4.10 (0.38)    \n#>   globulin (mean (SD))                3.03 (0.44)    \n#>   calcium (mean (SD))                 9.29 (0.37)    \n#>   physical.work = Yes (%)              476 (30.5)    \n#>   physical.recreational = Yes (%)      290 (18.6)    \n#>   diabetes = Yes (%)                   330 (21.1)    \n#>   cholesterol.bin (mean (SD))         0.63 (0.48)\n\n\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\nSaving data\n\nShow the code# getwd()\nsave(analytic.with.miss, analytic, file=\"Data/researchquestion/NHANES17.RData\")\n\n\nReferences\n\n\n\n\nPeters, Junenette L, M Patricia Fabian, and Jonathan I Levy. 2014. “Combined Impact of Lead, Cadmium, Polychlorinated Biphenyls and Non-Chemical Risk Factors on Blood Pressure in NHANES.” Environmental Research 132: 93–99."
  },
  {
    "objectID": "researchquestionF.html",
    "href": "researchquestionF.html",
    "title": "Functions for research question",
    "section": "",
    "text": "The list of new R functions introduced in this Research question lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.data.frame \n    base \n    To force an object to a data frame \n  \n\n as.formula \n    base/stats \n    To specify a model formula, e.g., formula for an outcome model \n  \n\n confint \n    base/stats \n    To estimate the confidence interval for model parameters \n  \n\n degf \n    survey \n    To see the degrees of freedom for a survey design object \n  \n\n describe \n    DescTools \n    To see the summary statistics of variables \n  \n\n exp \n    base \n    Exponentials \n  \n\n lapply \n    base \n    To apply a function over a list, e.g., to see the summary of a list of variables or to convert a list of categorical variables to factor variables. A similar function is `sapply`. lapply and sapply have the same functionality. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list. \n  \n\n length \n    base \n    To see the length of an object, e.g., number of elements/observations of a variable \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n publish \n    Publish \n    To show/publish regression tables \n  \n\n Reduce \n    base \n    To combine multiple objects, e.g., datasets \n  \n\n round \n    base \n    To round numeric values \n  \n\n saveRDS \n    base \n    To save a single R object. Similarly, readDRS will read an R object \n  \n\n skim \n    skimr \n    To see the summary statistics of variables \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n unique \n    base \n    To see the number of unique elements \n  \n\n weights \n    base/stats \n    To extract model weights, e.g., see the weights from a pre-specified survey design \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "researchquestionQ.html",
    "href": "researchquestionQ.html",
    "title": "Quiz on research question",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes on data wrangling.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "confounding.html#understanding-and-adjustment-of-confounding",
    "href": "confounding.html#understanding-and-adjustment-of-confounding",
    "title": "Confounding and bias",
    "section": "Understanding and adjustment of confounding",
    "text": "Understanding and adjustment of confounding\nThe first tutorial provides a thorough exploration of confounding, with a particular focus on its impact on treatment effect estimates in large datasets. It emphasizes the importance of properly adjusting for confounders to arrive at accurate estimates."
  },
  {
    "objectID": "confounding.html#mediator",
    "href": "confounding.html#mediator",
    "title": "Confounding and bias",
    "section": "Mediator",
    "text": "Mediator\nThis tutorial focuses on the role of mediator variables in estimating treatment effects. It assesses how adjusting for the mediator influences the estimated treatment effect, exploring both scenarios where the true treatment effect is either non-null or null."
  },
  {
    "objectID": "confounding.html#collider",
    "href": "confounding.html#collider",
    "title": "Confounding and bias",
    "section": "Collider",
    "text": "Collider\nThis tutorial serves as a practical guide for understanding how the inclusion of colliders can affect the estimation of treatment effects in causal models."
  },
  {
    "objectID": "confounding.html#z-bias",
    "href": "confounding.html#z-bias",
    "title": "Confounding and bias",
    "section": "Z-bias",
    "text": "Z-bias\nThis tutorial explores the concept of Z-bias, a phenomenon that can lead to misleading estimates of treatment effects in observational studies. It demonstrates how failing to properly adjust or not adjust for instrumental variables can result in biased estimates and compares these with the true treatment effect."
  },
  {
    "objectID": "confounding.html#collapsibility",
    "href": "confounding.html#collapsibility",
    "title": "Confounding and bias",
    "section": "Collapsibility",
    "text": "Collapsibility\nThis tutorial provides a detailed guide on calculating marginal probabilities and measures of association, including Risk Difference (RD), Risk Ratio (RR), and Odds Ratio (OR). It examines the impact of adjusting for various covariates on these measures, highlighting the concept of “collapsibility.”"
  },
  {
    "objectID": "confounding.html#change-in-estimate",
    "href": "confounding.html#change-in-estimate",
    "title": "Confounding and bias",
    "section": "Change-in-estimate",
    "text": "Change-in-estimate\nThis tutorial focuses on the “Change-in-estimate” concept to understand the impact of various variables on measures of effect. For both continuous and binary outcomes, the tutorial reveals that adding a confounder to the model alters the true treatment effect estimate. Conversely, including a variable that is not a confounder but is a pure risk factor can either change or not change the effect estimate, depending on the type of outcome involved. This nuanced approach aids in understanding how different roles of variables can influence results and interpretations in causal inference.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "confounding1.html",
    "href": "confounding1.html",
    "title": "Confounding",
    "section": "",
    "text": "Show the code# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n\n\nBig data: What if we had 1,000,000 (one million) observations? Would that give us true result? Let’s try to answer that using Directed acyclic graphs (DAGs).\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        1.75\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         0.5\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        0.45\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         0.0         0.5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confounding2.html",
    "href": "confounding2.html",
    "title": "Mediator",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nLet us consider\n\nM is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        1.69\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        0.39\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         0.0         0.5"
  },
  {
    "objectID": "confounding3.html",
    "href": "confounding3.html",
    "title": "Collider",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 1.3 * A, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00        1.29\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00        0.58        0.05\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00       -0.01\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00       -0.07        0.05\n\n\nEven 1,000,000 observations were not enough to recover true treatment effect! But we are close enough."
  },
  {
    "objectID": "confounding4.html",
    "href": "confounding4.html",
    "title": "Z-bias",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nYou could watch the video describing Z-bias and Bias amplification.\nContinuous Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"age\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"gender\", distr = \"rbern\", prob = plogis(0.25)) +\n  node(\"education\", distr = \"rbern\", prob = plogis(3 + 5* age)) +\n  node(\"diet\", distr = \"rbern\", prob = plogis(13 + 7 * education)) +\n  node(\"income\", distr = \"rbern\", prob = plogis(2 + 1.4 * education + 2 * age)) +\n  node(\"smoking\", distr = \"rbern\", prob = plogis(1 + 1.2 * gender + 2 * age)) +\n  node(\"hypertension\", distr = \"rnorm\", mean = 3 * diet + 1.3 * age + 2 * smoking + 0.5 * gender, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the codeObs.Data$income <- as.factor(Obs.Data$income)\n# True data generating mechanism \n# (unattainable as U is unmeasured)\nfit0 <- glm(hypertension ~ diet + age + smoking + gender, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)        diet         age     smoking      gender \n#>  -2024169.9   2024172.9         1.3         2.0         0.5\n\nrequire(Publish)\nfit1 <- glm(hypertension ~ diet + age + smoking*income + gender, family=\"gaussian\", data=Obs.Data)\npublish(fit1)\n#>            Variable Units Coefficient                     CI.95  p-value \n#>         (Intercept)       -2024807.39 [-13733931.24;9684316.46]   0.7347 \n#>                diet        2024810.39 [-9684313.46;13733934.24]   0.7347 \n#>                 age              1.30               [1.30;1.30]   <1e-04 \n#>              gender              0.50               [0.50;0.50]   <1e-04 \n#>  smoking: income(0)              2.00               [1.99;2.00]   <1e-04 \n#>  smoking: income(1)              2.00               [2.00;2.00]   <1e-04\n\n\nBinary Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is binary outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rbern\", prob = plogis(-1 + 3 * U + 1.3 * A))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# True data generating mechanism (unattainable as U is unmeasured)\nfit0 <- glm(Y ~ A + U, family=\"binomial\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A           U \n#>       -0.99        1.30        3.01\n\n# Unadjusted effect (Z not controlled)\nfit1 <- glm(Y ~ A, family=\"binomial\", data=Obs.Data)\nround(coef(fit1),2)\n#> (Intercept)           A \n#>        0.40        3.02\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#>        A \n#> 1.716482\n\n# Adjusted effect (Z  controlled)\nfit2 <- glm(Y ~ A + Z, family=\"binomial\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           Z \n#>        0.51        3.29       -0.18\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#>        A \n#> 1.991396"
  },
  {
    "objectID": "confounding5.html",
    "href": "confounding5.html",
    "title": "Collapsibility",
    "section": "",
    "text": "Explanation of collapsibility property of an estimate (RD, RR and OR: conditional or marginal) in absence of confounding\n\nShow the code# Load required packages\nlibrary(simcausal)\nlibrary(tableone)\nlibrary(Publish)\nlibrary(lawstat)\n\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"gender\", distr = \"rbern\", \n       prob = 0.7) +\n  node(\"age\", distr = \"rnorm\", \n       mean = 2, sd = 4) +\n  node(\"smoking\", distr = \"rbern\", \n       prob = plogis(.1)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3.5) * smoking \n                     + log(.1) * gender  \n                       + log(7) * age))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the codeObs.Data <- sim(DAG = Dset, n = 100000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nBalance check\n\nShow the coderequire(tableone)\nCreateTableOne(data = Obs.Data, \n               strata = \"smoking\", \n               vars = c(\"gender\", \"age\"))\n#>                     Stratified by smoking\n#>                      0            1            p      test\n#>   n                  47720        52280                   \n#>   gender (mean (SD))  0.70 (0.46)  0.70 (0.46)  0.403     \n#>   age (mean (SD))     2.02 (4.02)  2.01 (4.00)  0.690\n\n\nConditional and crude RD\nFull list of risk factors for outcome (2 variables)\n\nShow the code## RD\nrequire(Publish)\nfitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator …robust variance estimator (or bootstrap) should be used to obtain valid standard errors.”)\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"],\n             coef(fitx3)[\"smoking\"],\n             coef(fitx4)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nConditional and crude RR\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk ratio, one may use a GLM with a Poisson distribution and log link function …. one should use the robust (or sandwich) variance estimator to obtain valid standard errors (the bootstrap can also be used)”).\n\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 1.156387\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the codefitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.077402\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nConditional and crude OR\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 2.180804\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\",\n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.290429\n\n\nMantel-Haenszel adjusted ORs with 1 variable\n\nShow the codetabx <- xtabs( ~ hypertension + smoking + gender, data = Obs.Data)\nftable(tabx)    \n#>                      gender     0     1\n#> hypertension smoking                   \n#> 0            0               3788 12401\n#>              1               3400 11504\n#> 1            0              10547 20984\n#>              1              12178 25198\n# require(samplesizeCMH)\n# apply(tabx, 3, odds.ratio)\n\nlibrary(lawstat)\ncmh.test(tabx)\n#> \n#>  Cochran-Mantel-Haenszel Chi-square Test\n#> \n#> data:  tabx\n#> CMH statistic = NA, df = 1.0000, p-value = NA, MH Estimate = 1.2924,\n#> Pooled Odd Ratio = 1.2876, Odd Ratio of level 1 = 1.2864, Odd Ratio of\n#> level 2 = 1.2945\n# mantelhaen.test(tabx, exact = TRUE)\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nMarginal RD, RR and OR\nBelow we show a procedure for calculating marginal probabilities \\(p_1\\) (for treated) and \\(p_0\\) (for untreated).\nAdjustment of 2 variables\n\nShow the codefitx3 <- glm(hypertension ~ smoking + gender + age, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx3)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  3.37 \n#> RR (ZY)=  1.08\n\n\nAdjustment of 1 variable\n\nShow the codefitx2 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx2)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nNo adjustment\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx1)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nBootstrap could be used to estimate confidence intervals.\nRef:\n\n\n(Kleinman and Norton 2009) (“this paper demonstrates how to move from a nonlinear model to estimates of marginal effects that are quantified as the adjusted risk ratio or adjusted risk difference”)\n\n(Austin 2010) (“clinically meaningful measures of treatment effect using logistic regression model”)\n\n(Luijken et al. 2022) (“marginal odds ratio”)\n\n(Muller and MacLehose 2014) (“marginal standardization”)\n\n(Greenland 2004) (“standardized / population-averaged”)\n\n(Bieler et al. 2010) (“standardized /population-averaged risk from the logistic model”)\nSummary\nHere are the summary of the results based on a scenario where confounding was absent:\n\n\n\n\n\n\n\n\nModelling strategy\nRD (conditional)\nRR (conditional)\nOR (conditional)\n\n\n\nage + gender in regression\n0.06 [0.05;0.06]\n1.08 [1.08;1.09]\n3.37 [3.17;3.58]\n\n\nstratified by age and gender (mean)\n0.05 (0.11, 0.1,0.01,0)\n1.16 (1.41, 1.21, 1.01,1)\n2.18 (unweighted; 1.65, 1.49, 3.45, 2.14)\n\n\ngender in regression\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.26;1.33]\n\n\nstratified by gender (mean)\n0.05 (0.6,0.5)\n1.08 (1.09, 1.06)\n1.29 (1.29, 1.29; M-H 1.29)\n\n\nMarginal estimates\n\n\n\n\n\ncrude\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.25;1.32]\n\n\nBased on marginal probabilities (any variable combination)\n0.05\n1.08\n1.29\n\n\n\nLet us assume we have a regression of hypertension (\\(Y\\)), smoking (\\(A\\)) and a risk factor for outcome, gender (\\(L\\)). Then let us set up 2 regression models:\n\n1st regression model is \\(Y \\sim \\beta \\times A + \\alpha \\times L\\). Here we are conditioning on gender (\\(L\\)).\n2nd regression model is \\(Y \\sim \\beta' \\times A\\)\n\n\nThen regression is collapsible for \\(\\beta\\) over \\(L\\) if \\(\\beta = \\beta'\\) from the 2nd regression omitting \\(L\\). \\(\\beta \\ne \\beta'\\) would mean non-collapsibility. A measure of association (say, risk difference) is collapsible if the marginal measure of association is equal to a weighted average of the stratum-specific measures of association. Non-collapsibility is also knows as Simpson’s Paradox (in absence of confoinding of course): a statistical phenomenon where an association between two factors (say, hypertension and smoking) in a population (we are talking about marginal estimate here) is different than the associations of same relationship in subpopulations (conditional on some other factor, say, age; hence talking about conditional estimates).\nOdds ratio can be non-collapsible. It can produce different treatment effect estimate for different covariate adjustment sets (see our above example of when adjusting form age and sex vs. when adjusting none). This is true even in the absence of confounding. However, according to our definition here, OR is collapsible when we consider gender in the adjustment set.\nNote that, OR non-collapsibility is a consequence of the fact that it is estimated via a logit link function (nonlinearity of the logistic transformation).\nRef:\n\n(Greenland, Pearl, and Robins 1999)\n(Mansournia and Greenland 2015)\nReferences\n\n\n\n\nAustin, Peter C. 2010. “Absolute Risk Reductions, Relative Risks, Relative Risk Reductions, and Numbers Needed to Treat Can Be Obtained from a Logistic Regression Model.” Journal of Clinical Epidemiology 63 (1): 2–6.\n\n\nBieler, Gayle S, G Gordon Brown, Rick L Williams, and Donna J Brogan. 2010. “Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios from Complex Survey Data.” American Journal of Epidemiology 171 (5): 618–23.\n\n\nGreenland, Sander. 2004. “Model-Based Estimation of Relative Risks and Other Epidemiologic Measures in Studies of Common Outcomes and in Case-Control Studies.” American Journal of Epidemiology 160 (4): 301–5.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Confounding and Collapsibility in Causal Inference.” Statistical Science 14 (1): 29–46.\n\n\nKleinman, Lawrence C, and Edward C Norton. 2009. “What’s the Risk? A Simple Approach for Estimating Adjusted Risk Measures from Nonlinear Models Including Logistic Regression.” Health Services Research 44 (1): 288–302.\n\n\nLuijken, Kim, Rolf HH Groenwold, Maarten van Smeden, Susanne Strohmaier, and Georg Heinze. 2022. “A Comparison of Full Model Specification and Backward Elimination of Potential Confounders When Estimating Marginal and Conditional Causal Effects on Binary Outcomes from Observational Data.” Biometrical Journal.\n\n\nMansournia, Mohammad Ali, and Sander Greenland. 2015. “The Relation of Collapsibility and Confounding to Faithfulness and Stability.” Epidemiology 26 (4): 466–72.\n\n\nMuller, Clemma J, and Richard F MacLehose. 2014. “Estimating Predicted Probabilities from Logistic Regression: Different Methods Correspond to Different Target Populations.” International Journal of Epidemiology 43 (3): 962–70.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is a confounder",
    "text": "Adjusting for a variable that is a confounder\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.85\n\nfit2 <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis( 1.1 * L + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node L, order:1\n#> node A, order:2\n#> node P, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.68\n\nfit2 <- glm(Y ~ A + L, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (simplified)",
    "text": "Adjusting for a variable that is not a confounder (simplified)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>         0.0         1.3\n\nfit2 <- glm(Y ~ A + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.06\n\nfit2 <- glm(Y ~ A + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (Complex)",
    "text": "Adjusting for a variable that is not a confounder (Complex)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\nfit2 <- glm(Y ~ A + M + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>         0.0         1.3         0.5         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>        0.00        1.06        0.41\n\nfit2 <- glm(Y ~ A + M + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>        0.00        1.29        0.50        1.10\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confoundingF.html",
    "href": "confoundingF.html",
    "title": "R for confounding",
    "section": "",
    "text": "The list of new R functions introduced in this Confounding and bias lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n cmh.test \n    lawstat \n    To conduct the Mantel-Haenszel Chi-square test \n  \n\n DAG.empty \n    simcausal \n    To initialize an empty DAG \n  \n\n ftable \n    base/stats \n    To create a flat contingency table \n  \n\n plotDAG \n    simcausal \n    To visualize a DAG \n  \n\n set.DAG \n    simcausal \n    To create a DAG \n  \n\n sim \n    simcausal \n    To simulate data using a DAG"
  },
  {
    "objectID": "predictivefactors.html",
    "href": "predictivefactors.html",
    "title": "Predictive factors",
    "section": "",
    "text": "This chapter is about:\n\npredictive factors with continuous and a binary outcome variable\ndiagnostics, overfitting and optimism"
  },
  {
    "objectID": "predictivefactors1.html",
    "href": "predictivefactors1.html",
    "title": "Collinear predictors",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\n\n\nLoad data\nLet us load the dataset and see structure of the variables:\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n#head(analytic)\nstr(analytic)\n#> 'data.frame':    1267 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83741 83747 83750 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n#>  $ age                  : num  62 53 22 46 45 30 60 69 24 70 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Others\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"Black\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"College\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Never.married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"Between.25kto54k\" \"<25k\" ...\n#>  $ weight               : num  135630 25282 39353 35674 97002 ...\n#>  $ psu                  : num  1 1 2 1 1 1 1 2 1 2 ...\n#>  $ strata               : num  125 125 128 121 125 124 128 120 130 132 ...\n#>  $ diastolicBP          : num  70 88 70 94 70 50 74 70 72 54 ...\n#>  $ systolicBP           : num  128 146 110 144 116 104 142 146 126 144 ...\n#>  $ bodyweight           : num  94.8 90.4 76.6 86.2 76.2 71.2 75.6 84 89.2 81.7 ...\n#>  $ bodyheight           : num  184 171 165 177 178 ...\n#>  $ bmi                  : num  27.8 30.8 28 27.6 24.1 26.6 35.9 31 26.9 27 ...\n#>  $ waist                : num  101.1 107.9 86.6 104.3 90.1 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Some.days\" \"Every.day\" ...\n#>  $ alcohol              : num  1 6 8 1 3 2 1 1 2 2 ...\n#>  $ cholesterol          : num  173 265 164 242 181 184 205 287 126 192 ...\n#>  $ cholesterolM2        : num  4.47 6.85 4.24 6.26 4.68 4.76 5.3 7.42 3.26 4.97 ...\n#>  $ triglycerides        : num  158 170 77 497 63 62 169 245 95 64 ...\n#>  $ uric.acid            : num  4.2 7 6 6.5 5.4 5.5 5.1 4.3 7.6 7.1 ...\n#>  $ protein              : num  7.5 7.4 7.4 6.8 7.4 6.7 7.4 6.8 7.3 7.2 ...\n#>  $ bilirubin            : num  0.5 0.6 0.2 0.5 0.7 0.8 0.4 0.6 1.2 1.2 ...\n#>  $ phosphorus           : num  4.7 4.4 5.3 3.6 3.9 3.4 3.9 4.4 3.2 3 ...\n#>  $ sodium               : num  136 140 139 138 138 136 139 140 140 139 ...\n#>  $ potassium            : num  4.3 4.55 4.16 4.27 3.91 3.97 3.99 4.25 3.8 4.63 ...\n#>  $ globulin             : num  2.9 2.9 3 2.6 2.8 2.5 3.2 2.3 2.7 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.3 9.3 9.3 9.4 9.6 9.6 9.6 9.6 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"Yes\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:3739] 3 4 5 6 8 9 13 14 15 16 ...\n#>   ..- attr(*, \"names\")= chr [1:3739] \"3\" \"4\" \"5\" \"6\" ...\n\n\nDescribe the data\n\nShow the coderequire(rms)\ndescribe(analytic) \n#> analytic \n#> \n#>  33  Variables      1267  Observations\n#> --------------------------------------------------------------------------------\n#> ID \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1267        1    88660     3366    84250    84687 \n#>      .25      .50      .75      .90      .95 \n#>    86019    88692    91252    92670    93089 \n#> \n#> lowest : 83732 83733 83741 83747 83750, highest: 93617 93633 93643 93659 93685\n#> --------------------------------------------------------------------------------\n#> gender \n#>        n  missing distinct \n#>     1267        0        2 \n#>                         \n#> Value      Female   Male\n#> Frequency     496    771\n#> Proportion  0.391  0.609\n#> --------------------------------------------------------------------------------\n#> age \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       61        1    49.91    19.18       24       27 \n#>      .25      .50      .75      .90      .95 \n#>       36       51       63       72       78 \n#> \n#> lowest : 20 21 22 23 24, highest: 76 77 78 79 80\n#> --------------------------------------------------------------------------------\n#> born \n#>        n  missing distinct \n#>     1267        0        2 \n#>                                                                             \n#> Value      Born in 50 US states or Washingt                           Others\n#> Frequency                               991                              276\n#> Proportion                            0.782                            0.218\n#> --------------------------------------------------------------------------------\n#> race \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                               \n#> Value         Black Hispanic    Other    White\n#> Frequency       246      337      132      552\n#> Proportion    0.194    0.266    0.104    0.436\n#> --------------------------------------------------------------------------------\n#> education \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                               \n#> Value          College High.School      School\n#> Frequency          648         523          96\n#> Proportion       0.511       0.413       0.076\n#> --------------------------------------------------------------------------------\n#> married \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                                                    \n#> Value                 Married      Never.married Previously.married\n#> Frequency                 751                226                290\n#> Proportion              0.593              0.178              0.229\n#> --------------------------------------------------------------------------------\n#> income \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                                                               \n#> Value                  <25k Between.25kto54k Between.55kto99k         Over100k\n#> Frequency               344              435              297              191\n#> Proportion            0.272            0.343            0.234            0.151\n#> --------------------------------------------------------------------------------\n#> weight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1184        1    48904    44337     9158    11549 \n#>      .25      .50      .75      .90      .95 \n#>    19540    30335    63822   121803   151546 \n#> \n#> lowest :   5470.041   5948.955   6197.660   6480.947   6703.837\n#> highest: 203562.855 207197.232 213611.345 218138.797 224891.623\n#> --------------------------------------------------------------------------------\n#> psu \n#>        n  missing distinct     Info     Mean      Gmd \n#>     1267        0        2     0.75    1.493   0.5003 \n#>                       \n#> Value          1     2\n#> Frequency    642   625\n#> Proportion 0.507 0.493\n#> --------------------------------------------------------------------------------\n#> strata \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       15    0.994    126.3    4.792      120      121 \n#>      .25      .50      .75      .90      .95 \n#>      123      126      130      132      133 \n#> \n#> lowest : 119 120 121 122 123, highest: 129 130 131 132 133\n#>                                                                             \n#> Value        119   120   121   122   123   124   125   126   127   128   129\n#> Frequency     47    74   118    63    77    66   114   104   107    65    53\n#> Proportion 0.037 0.058 0.093 0.050 0.061 0.052 0.090 0.082 0.084 0.051 0.042\n#>                                   \n#> Value        130   131   132   133\n#> Frequency     99   120    95    65\n#> Proportion 0.078 0.095 0.075 0.051\n#> --------------------------------------------------------------------------------\n#> diastolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       41    0.997    70.37    13.99       52       54 \n#>      .25      .50      .75      .90      .95 \n#>       62       70       78       86       92 \n#> \n#> lowest :   0  26  34  38  40, highest: 104 106 108 110 112\n#> --------------------------------------------------------------------------------\n#> systolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       56    0.998    126.5     19.3    102.0    106.0 \n#>      .25      .50      .75      .90      .95 \n#>    114.0    124.0    136.0    148.8    160.0 \n#> \n#> lowest :  84  88  90  92  94, highest: 194 196 206 218 236\n#> --------------------------------------------------------------------------------\n#> bodyweight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      615        1    84.95    23.56    56.29    61.10 \n#>      .25      .50      .75      .90      .95 \n#>    69.70    81.40    97.00   113.44   127.47 \n#> \n#> lowest :  39.7  39.8  40.7  42.6  42.7, highest: 161.9 166.3 175.7 175.9 178.4\n#> --------------------------------------------------------------------------------\n#> bodyheight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      376        1    169.2    10.66    153.8    157.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.6    169.3    176.2    181.1    184.2 \n#> \n#> lowest : 143.8 144.2 145.2 145.9 146.2, highest: 194.6 195.1 195.6 198.4 201.0\n#> --------------------------------------------------------------------------------\n#> bmi \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      284        1    29.58    7.403    20.60    22.06 \n#>      .25      .50      .75      .90      .95 \n#>    24.80    28.60    33.30    38.24    42.00 \n#> \n#> lowest : 16.3 17.5 17.6 17.7 17.9, highest: 57.2 57.6 59.4 60.7 64.5\n#> --------------------------------------------------------------------------------\n#> waist \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      544        1    101.8    18.47     77.1     81.4 \n#>      .25      .50      .75      .90      .95 \n#>     90.5    100.3    111.2    122.8    132.5 \n#> \n#> lowest :  65.0  65.5  66.5  68.2  68.7, highest: 159.2 159.8 160.2 160.5 161.5\n#> --------------------------------------------------------------------------------\n#> smoke \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                            \n#> Value       Every.day Not.at.all  Some.days\n#> Frequency         448        665        154\n#> Proportion      0.354      0.525      0.122\n#> --------------------------------------------------------------------------------\n#> alcohol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       14    0.952    3.109    2.419        1        1 \n#>      .25      .50      .75      .90      .95 \n#>        1        2        4        6        8 \n#> \n#> lowest :  1  2  3  4  5, highest: 10 11 12 14 15\n#>                                                                             \n#> Value          1     2     3     4     5     6     7     8     9    10    11\n#> Frequency    336   371   189   106    79    95    10    26     4    20     1\n#> Proportion 0.265 0.293 0.149 0.084 0.062 0.075 0.008 0.021 0.003 0.016 0.001\n#>                             \n#> Value         12    14    15\n#> Frequency     23     1     6\n#> Proportion 0.018 0.001 0.005\n#> --------------------------------------------------------------------------------\n#> cholesterol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    193.1    47.47    132.0    142.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.5    191.0    217.0    248.0    268.0 \n#> \n#> lowest :  81  93  97 100 101, highest: 345 348 349 358 545\n#> --------------------------------------------------------------------------------\n#> cholesterolM2 \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    4.994    1.228    3.410    3.670 \n#>      .25      .50      .75      .90      .95 \n#>    4.205    4.940    5.610    6.410    6.930 \n#> \n#> lowest :  2.09  2.40  2.51  2.59  2.61, highest:  8.92  9.00  9.03  9.26 14.09\n#> --------------------------------------------------------------------------------\n#> triglycerides \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      361        1    165.8    124.1     48.0     59.0 \n#>      .25      .50      .75      .90      .95 \n#>     84.0    127.0    201.5    309.0    396.6 \n#> \n#> lowest :   18   21   24   25   31, highest:  964 1020 1157 1253 3061\n#> --------------------------------------------------------------------------------\n#> uric.acid \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       84        1    5.598    1.626     3.43     3.80 \n#>      .25      .50      .75      .90      .95 \n#>     4.60     5.50     6.50     7.40     8.00 \n#> \n#> lowest :  1.6  2.2  2.3  2.4  2.5, highest: 10.2 10.3 11.7 12.2 18.0\n#> --------------------------------------------------------------------------------\n#> protein \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       32    0.995    7.126   0.5095      6.4      6.6 \n#>      .25      .50      .75      .90      .95 \n#>      6.8      7.1      7.4      7.7      7.9 \n#> \n#> lowest : 5.7 5.8 5.9 6.0 6.1, highest: 8.4 8.5 8.6 8.8 9.0\n#> --------------------------------------------------------------------------------\n#> bilirubin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       31    0.984   0.5467   0.2949      0.2      0.2 \n#>      .25      .50      .75      .90      .95 \n#>      0.4      0.5      0.7      0.9      1.0 \n#> \n#> lowest : 0.00 0.01 0.02 0.03 0.04, highest: 1.80 2.00 2.10 2.60 3.30\n#> --------------------------------------------------------------------------------\n#> phosphorus \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       37    0.996    3.642    0.593      2.8      3.0 \n#>      .25      .50      .75      .90      .95 \n#>      3.3      3.6      4.0      4.3      4.5 \n#> \n#> lowest : 1.8 2.0 2.2 2.3 2.4, highest: 5.2 5.3 5.4 5.6 6.1\n#> --------------------------------------------------------------------------------\n#> sodium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       20    0.977    138.5    2.383      135      136 \n#>      .25      .50      .75      .90      .95 \n#>      137      139      140      141      142 \n#> \n#> lowest : 124 126 129 130 131, highest: 142 143 144 146 148\n#>                                                                             \n#> Value        124   126   129   130   131   132   133   134   135   136   137\n#> Frequency      1     1     1     1     5     4    11    23    46    93   176\n#> Proportion 0.001 0.001 0.001 0.001 0.004 0.003 0.009 0.018 0.036 0.073 0.139\n#>                                                                 \n#> Value        138   139   140   141   142   143   144   146   148\n#> Frequency    235   260   206   112    55    29     6     1     1\n#> Proportion 0.185 0.205 0.163 0.088 0.043 0.023 0.005 0.001 0.001\n#> --------------------------------------------------------------------------------\n#> potassium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      175        1    3.985   0.3725     3.45     3.57 \n#>      .25      .50      .75      .90      .95 \n#>     3.78     3.98     4.19     4.40     4.54 \n#> \n#> lowest : 2.60 2.92 2.96 3.07 3.09, highest: 5.15 5.21 5.36 5.37 5.51\n#> --------------------------------------------------------------------------------\n#> globulin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       29    0.994    2.799   0.4536      2.2      2.3 \n#>      .25      .50      .75      .90      .95 \n#>      2.5      2.8      3.0      3.3      3.5 \n#> \n#> lowest : 1.6 1.8 1.9 2.0 2.1, highest: 4.1 4.2 4.3 4.5 5.5\n#> --------------------------------------------------------------------------------\n#> calcium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       25    0.991    9.335   0.3786      8.8      8.9 \n#>      .25      .50      .75      .90      .95 \n#>      9.1      9.3      9.6      9.7      9.9 \n#> \n#> lowest :  8.4  8.5  8.6  8.7  8.8, highest: 10.4 10.5 10.7 11.0 11.1\n#> --------------------------------------------------------------------------------\n#> physical.work \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency    895   372\n#> Proportion 0.706 0.294\n#> --------------------------------------------------------------------------------\n#> physical.recreational \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency   1002   265\n#> Proportion 0.791 0.209\n#> --------------------------------------------------------------------------------\n#> diabetes \n#>        n  missing distinct \n#>     1267        0        2 \n#>                     \n#> Value        No  Yes\n#> Frequency  1064  203\n#> Proportion 0.84 0.16\n#> --------------------------------------------------------------------------------\n\n\nIdentify collinear predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can use hclust and varclus or variable clustering, i.e., to identify collinear predictors\n\n\n\n\nhclust is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.\n\nShow the coderequire(Hmisc)\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.cluster <- varclus(~., data = analytic[sel.names])\n# var.cluster\nplot(var.cluster)"
  },
  {
    "objectID": "predictivefactors2.html",
    "href": "predictivefactors2.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(car)\nlibrary(corrplot)\nlibrary(olsrr)\n\n\nExplore relationships for continuous outcome variable\nLoad data\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n\n\nCorrelation plot\n\n\n\n\n\n\nTip\n\n\n\nWe can use the cor function to see the correlation between numeric variables and then use the corrplot function to plot the cor object.\n\n\n\nShow the coderequire(corrplot)\nnumeric.names <- c(\"age\", \"diastolicBP\", \"systolicBP\", \n                   \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"alcohol\", \n                   \"cholesterol\", \"triglycerides\", \"uric.acid\", \n                   \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n                   \"globulin\", \"calcium\")\ncorrelationMatrix <- cor(analytic[numeric.names])\nmat.num <- round(correlationMatrix,2)\nmat.num[mat.num>0.8 & mat.num < 1]\n#> [1] 0.89 0.90 0.89 0.91 0.90 0.91\ncorrplot(correlationMatrix, method=\"number\", type=\"upper\")\n\n\n\n\nExamine descriptive associations\nLet us examine the descriptive associations with the dependent variable by stratifying separately by key predictors\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to examine the descriptive associations by strata/groups, e.g., summarize, aggregate, describeBy, tapply, summary\n\n\n\nShow the codemean(analytic$cholesterol)\n#> [1] 193.1002\n\n# Process 1\nmean(analytic$cholesterol[analytic$gender == \"Male\"])\n#> [1] 190.7626\nmean(analytic$cholesterol[analytic$gender == \"Female\"])\n#> [1] 196.7339\n\n# Process 2\nlibrary(dplyr)\nanalytic %>%\n  group_by(gender) %>%\n  summarize(mean.ch=mean(cholesterol), .groups = 'drop') \n\n\n\n  \n\n\nShow the code\n# process 3\nwith(analytic, aggregate( analytic$cholesterol, by=list(gender) , FUN=summary))\n\n\n\n  \n\n\nShow the code\n# process 4\npsych::describeBy(analytic$cholesterol, analytic$gender)\n#> \n#>  Descriptive statistics by group \n#> group: Female\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 496 196.73 43.26  194.5  194.44 40.77 100 358   258 0.57     0.56 1.94\n#> ------------------------------------------------------------ \n#> group: Male\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 771 190.76 43.06    188  188.54 40.03  81 545   464  1.1     5.76 1.55\n\n# process 5\ntapply(analytic$cholesterol, analytic$gender, summary)\n#> $Female\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   100.0   166.8   194.5   196.7   220.2   358.0 \n#> \n#> $Male\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   161.0   188.0   190.8   215.0   545.0\n\n# A general process\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.summ <- summary(cholesterol~ ., data = analytic[sel.names])\nvar.summ\n#> cholesterol      N= 1267  \n#> \n#> +---------------------+--------------------------------+----+-----------+\n#> |                     |                                |   N|cholesterol|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               gender|                          Female| 496|   196.7339|\n#> |                     |                            Male| 771|   190.7626|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  age|                         [20,37)| 342|   182.4854|\n#> |                     |                         [37,52)| 313|   200.1661|\n#> |                     |                         [52,64)| 315|   199.7873|\n#> |                     |                         [64,80]| 297|   190.7845|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 born|Born in 50 US states or Washingt| 991|   190.9253|\n#> |                     |                          Others| 276|   200.9094|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 race|                           Black| 246|   187.3740|\n#> |                     |                        Hispanic| 337|   193.5490|\n#> |                     |                           Other| 132|   191.8561|\n#> |                     |                           White| 552|   195.6757|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            education|                         College| 648|   192.5478|\n#> |                     |                     High.School| 523|   193.4532|\n#> |                     |                          School|  96|   194.9062|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              married|                         Married| 751|   194.0306|\n#> |                     |                   Never.married| 226|   182.8761|\n#> |                     |              Previously.married| 290|   198.6586|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               income|                            <25k| 344|   191.9564|\n#> |                     |                Between.25kto54k| 435|   191.9310|\n#> |                     |                Between.55kto99k| 297|   195.7508|\n#> |                     |                        Over100k| 191|   193.7016|\n#> +---------------------+--------------------------------+----+-----------+\n#> |          diastolicBP|                        [ 0, 64)| 336|   186.7649|\n#> |                     |                        [64, 72)| 321|   189.3458|\n#> |                     |                        [72, 80)| 319|   195.7085|\n#> |                     |                        [80,112]| 291|   201.6976|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           systolicBP|                       [ 84,116)| 340|   186.2765|\n#> |                     |                       [116,126)| 317|   190.6372|\n#> |                     |                       [126,138)| 335|   196.9881|\n#> |                     |                       [138,236]| 275|   199.6400|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyweight|                    [39.7, 69.8)| 319|   193.8903|\n#> |                     |                    [69.8, 81.5)| 316|   197.1424|\n#> |                     |                    [81.5, 97.2)| 317|   192.4984|\n#> |                     |                    [97.2,178.4]| 315|   188.8508|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyheight|                       [144,163)| 317|   198.7003|\n#> |                     |                       [163,169)| 320|   193.7750|\n#> |                     |                       [169,176)| 314|   189.8790|\n#> |                     |                       [176,201]| 316|   190.0000|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  bmi|                     [16.3,24.9)| 322|   188.8043|\n#> |                     |                     [24.9,28.7)| 315|   198.5016|\n#> |                     |                     [28.7,33.4)| 317|   197.5016|\n#> |                     |                     [33.4,64.5]| 313|   187.6262|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                waist|                   [ 65.0, 90.7)| 320|   188.9688|\n#> |                     |                   [ 90.7,100.4)| 315|   199.6413|\n#> |                     |                   [100.4,111.3)| 316|   197.3892|\n#> |                     |                   [111.3,161.5]| 316|   186.4747|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                smoke|                       Every.day| 448|   191.5938|\n#> |                     |                      Not.at.all| 665|   194.6451|\n#> |                     |                       Some.days| 154|   190.8117|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              alcohol|                               1| 336|   191.0387|\n#> |                     |                               2| 371|   192.0809|\n#> |                     |                          [3, 5)| 295|   195.9356|\n#> |                     |                          [5,15]| 265|   193.9849|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        triglycerides|                      [ 18,  85)| 320|   172.2344|\n#> |                     |                      [ 85, 128)| 319|   185.6834|\n#> |                     |                      [128, 203)| 314|   199.4140|\n#> |                     |                      [203,3061]| 314|   215.5860|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            uric.acid|                      [1.6, 4.7)| 348|   188.9310|\n#> |                     |                      [4.7, 5.6)| 305|   191.8033|\n#> |                     |                      [5.6, 6.6)| 307|   195.7720|\n#> |                     |                      [6.6,18.0]| 307|   196.4430|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              protein|                       [5.7,6.9)| 336|   189.8631|\n#> |                     |                       [6.9,7.2)| 328|   192.3201|\n#> |                     |                       [7.2,7.5)| 310|   193.4258|\n#> |                     |                       [7.5,9.0]| 293|   197.3413|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            bilirubin|                       [0.0,0.5)| 506|   195.7391|\n#> |                     |                             0.5| 212|   192.2264|\n#> |                     |                       [0.6,0.8)| 310|   192.0645|\n#> |                     |                       [0.8,3.3]| 239|   189.6318|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           phosphorus|                       [1.8,3.4)| 362|   188.0387|\n#> |                     |                       [3.4,3.7)| 309|   192.5405|\n#> |                     |                       [3.7,4.1)| 323|   195.5542|\n#> |                     |                       [4.1,6.1]| 273|   197.5421|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               sodium|                       [124,138)| 362|   191.9420|\n#> |                     |                       [138,140)| 495|   194.2929|\n#> |                     |                             140| 206|   191.7864|\n#> |                     |                       [141,148]| 204|   193.5882|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            potassium|                     [2.60,3.79)| 320|   191.5375|\n#> |                     |                     [3.79,3.99)| 328|   192.3628|\n#> |                     |                     [3.99,4.20)| 308|   196.9643|\n#> |                     |                     [4.20,5.51]| 311|   191.6592|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             globulin|                       [1.6,2.6)| 350|   189.9429|\n#> |                     |                       [2.6,2.9)| 388|   199.0052|\n#> |                     |                       [2.9,3.1)| 230|   193.3783|\n#> |                     |                       [3.1,5.5]| 299|   188.9197|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              calcium|                      [8.4, 9.2)| 371|   186.0323|\n#> |                     |                      [9.2, 9.4)| 294|   188.3605|\n#> |                     |                      [9.4, 9.7)| 395|   197.4430|\n#> |                     |                      [9.7,11.1]| 207|   204.2126|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        physical.work|                              No| 895|   194.0078|\n#> |                     |                             Yes| 372|   190.9167|\n#> +---------------------+--------------------------------+----+-----------+\n#> |physical.recreational|                              No|1002|   193.5359|\n#> |                     |                             Yes| 265|   191.4528|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             diabetes|                              No|1064|   194.8036|\n#> |                     |                             Yes| 203|   184.1724|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              Overall|                                |1267|   193.1002|\n#> +---------------------+--------------------------------+----+-----------+\nplot(var.summ)\n\n\n\nShow the code\nsummary(analytic$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   62.00   70.00   70.37   78.00  112.00\n\nanalytic$diastolicBP[analytic$diastolicBP == 0] <- NA\n\n# Bivariate Summaries Computed Separately by a Series of Predictors\nvar.summ2 <- spearman2(cholesterol~ ., data = analytic[sel.names])\nplot(var.summ2)\n\n\n\n\nRegression: Linear regression\nWe can also use regression analysis to examine the association:\n\n\n\n\n\n\nTip\n\n\n\nWe use lm function to fit the linear regression\n\n\n\nShow the code# set up formula with just 1 variable\nformula0 <- as.formula(\"cholesterol~triglycerides\")\n\n# fitting regression on the analytic2 data\nfit0 <- lm(formula0,data = analytic2)\n\n# extract results\nsummary(fit0)\n#> \n#> Call:\n#> lm(formula = formula0, data = analytic2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -111.651  -26.157   -2.661   22.549  166.752 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.716e+02  1.127e+00  152.23   <2e-16 ***\n#> triglycerides 1.275e-01  5.456e-03   23.37   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 37.38 on 2632 degrees of freedom\n#> Multiple R-squared:  0.1718, Adjusted R-squared:  0.1715 \n#> F-statistic:   546 on 1 and 2632 DF,  p-value: < 2.2e-16\n\n# extract just the coefficients/estimates\ncoef(fit0)\n#>   (Intercept) triglycerides \n#>   171.6147531     0.1274909\n\n# extract confidence intervals\nconfint(fit0)\n#>                     2.5 %      97.5 %\n#> (Intercept)   169.4042284 173.8252779\n#> triglycerides   0.1167919   0.1381899\n\n# residual plots\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\nDiagnosis\nIdentifying problematic data\n\nShow the coderequire(olsrr)\n# Outlier\nplot(cholesterol ~ triglycerides, data = analytic2)\n\n\n\nShow the codesubset(analytic2, triglycerides > 1500)\n\n\n\n  \n\n\nShow the code\n# leverage\nols_plot_resid_lev(fit0)\n\n\n\nShow the codeanalytic2$lev <- hat(model.matrix(fit0))\nplot(analytic2$lev)\n\n\n\nShow the codesummary(analytic2$lev)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.0003796 0.0004062 0.0004773 0.0007593 0.0005831 0.1800021\nwhich(analytic2$lev > 0.05)\n#> [1] 1102\nsubset(analytic2, lev > 0.05)\n\n\n\n  \n\n\nShow the code\n# Residual\nanalytic2$rstudent.values <- rstudent(fit0)\nplot(analytic2$rstudent.values)\n\n\n\nShow the codewhich(analytic2$rstudent.values < -5)\n#> integer(0)\n# Heteroskedasticity: Test for constant variance\n#ols_test_breusch_pagan(fit0, rhs = TRUE)\n\n\nDeleting suspicious data\n\nShow the code# condition 1: triglycerides above 1500 needs deleting\nanalytic2b <- subset(analytic2, triglycerides < 1500)\ndim(analytic2b)\n#> [1] 2632   34\n\n# condition 2: leverage above 0.05 needs deleting\nanalytic3 <- subset(analytic2b, lev < 0.05)\ndim(analytic3)\n#> [1] 2632   34\n\n# Check how many observations are deleted\nnrow(analytic2)-nrow(analytic3)\n#> [1] 2\n\n\nRefitting in cleaned data\n\nShow the code### Re-fit in data analytic3 (without problematic data)\nformula0\n#> cholesterol ~ triglycerides\nfit0 <- lm(formula0,data = analytic3)\n\nrequire(Publish)\npublish(fit0)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\nShow the code\nrequire(car)\n# component+residual plot or partial-residual plot\ncrPlots(fit0)\n\n\n\n\npolynomial order 2\n\nShow the codeformula1 <- as.formula(\"cholesterol~poly(triglycerides,2)\")\nformula1 <- as.formula(\"cholesterol~triglycerides^2\")\nfit1 <- lm(formula1,data = analytic3)\npublish(fit1)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit1)\n\n\n\nShow the code\n# compare fit0 and fit1 models\nanova(fit0,fit1)\n\n\n\n  \n\n\n\npolynomial order 3\n\nShow the code# Fit a polynomial of order 3\nformula2 <- as.formula(\"cholesterol~poly(triglycerides,3)\")\nformula2 <- as.formula(\"cholesterol~triglycerides^3\")\nfit2 <- lm(formula2,data = analytic3)\npublish(fit2)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit2)\n\n\n\nShow the code\n# compare fit1 and fit2 models\nanova(fit1,fit2)\n\n\n\n  \n\n\n\nMultiple covariates\n\nShow the code# include everything!\nformula3 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit3 <- lm(formula3, data = analytic3)\npublish(fit3)\n#>               Variable                            Units Coefficient           CI.95     p-value \n#>            (Intercept)                                       280.02 [133.42;426.62]   0.0001852 \n#>                 gender                           Female         Ref                             \n#>                                                    Male      -11.99  [-16.41;-7.57]     < 1e-04 \n#>                    age                                         0.35     [0.23;0.47]     < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                             \n#>                                                  Others        7.52    [3.68;11.36]   0.0001270 \n#>                   race                            Black         Ref                             \n#>                                                Hispanic       -6.15  [-10.87;-1.44]   0.0106253 \n#>                                                   Other       -5.37   [-10.92;0.18]   0.0579281 \n#>                                                   White       -0.95    [-5.21;3.30]   0.6603698 \n#>              education                          College         Ref                             \n#>                                             High.School        2.90    [-0.28;6.08]   0.0743132 \n#>                                                  School       -2.54    [-8.61;3.54]   0.4134016 \n#>                married                          Married         Ref                             \n#>                                           Never.married       -5.72   [-9.63;-1.81]   0.0041887 \n#>                                      Previously.married        0.31    [-3.54;4.17]   0.8730460 \n#>                 income                             <25k         Ref                             \n#>                                        Between.25kto54k       -0.97    [-4.87;2.93]   0.6261315 \n#>                                        Between.55kto99k        2.29    [-1.98;6.56]   0.2928564 \n#>                                                Over100k        2.44    [-2.27;7.14]   0.3099380 \n#>            diastolicBP                                         0.38     [0.25;0.50]     < 1e-04 \n#>             systolicBP                                         0.02    [-0.08;0.12]   0.6668119 \n#>                    bmi                                        -2.55   [-4.29;-0.81]   0.0041392 \n#>             bodyweight                                         0.82     [0.19;1.45]   0.0105518 \n#>             bodyheight                                        -0.89   [-1.55;-0.24]   0.0074286 \n#>                  waist                                        -0.02    [-0.29;0.26]   0.9020424 \n#>          triglycerides                                         0.12     [0.11;0.14]     < 1e-04 \n#>              uric.acid                                         1.27     [0.08;2.47]   0.0369190 \n#>                protein                                         4.99   [-0.77;10.74]   0.0897748 \n#>              bilirubin                                        -5.43  [-10.53;-0.33]   0.0370512 \n#>             phosphorus                                        -0.18    [-2.81;2.45]   0.8939361 \n#>                 sodium                                        -0.97   [-1.66;-0.29]   0.0052516 \n#>              potassium                                         1.04    [-3.44;5.52]   0.6487979 \n#>               globulin                                        -2.25    [-8.22;3.71]   0.4591138 \n#>                calcium                                        12.02    [6.98;17.07]     < 1e-04 \n#>          physical.work                               No         Ref                             \n#>                                                     Yes       -0.45    [-3.68;2.79]   0.7858787 \n#>  physical.recreational                               No         Ref                             \n#>                                                     Yes        1.35    [-1.94;4.65]   0.4210703 \n#>               diabetes                               No         Ref                             \n#>                                                     Yes      -19.11 [-23.37;-14.85]     < 1e-04\n\n\nColinearity\n\n\nRule of thumb: variables with VIF > 4 needs further investigation\n\nShow the codecar::vif(fit3)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.694171  1        1.641393\n#> age                     2.164388  1        1.471186\n#> born                    1.611478  1        1.269440\n#> race                    2.463445  3        1.162137\n#> education               1.435876  2        1.094660\n#> married                 1.481141  2        1.103187\n#> income                  1.402249  3        1.057964\n#> diastolicBP             1.271126  1        1.127442\n#> systolicBP              1.594986  1        1.262928\n#> bmi                    81.811969  1        9.044997\n#> bodyweight            101.102349  1       10.054966\n#> bodyheight             21.863188  1        4.675809\n#> waist                  11.913719  1        3.451626\n#> triglycerides           1.219331  1        1.104233\n#> uric.acid               1.603290  1        1.266211\n#> protein                 3.622385  1        1.903256\n#> bilirubin               1.185035  1        1.088593\n#> phosphorus              1.116982  1        1.056874\n#> sodium                  1.120920  1        1.058735\n#> potassium               1.178381  1        1.085533\n#> globulin                3.371211  1        1.836086\n#> calcium                 1.591677  1        1.261617\n#> physical.work           1.087315  1        1.042744\n#> physical.recreational   1.226830  1        1.107624\n#> diabetes                1.210715  1        1.100325\ncollinearity <- ols_vif_tol(fit3)\ncollinearity\n\n\n\n  \n\n\nShow the code\n# VIF > 4\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + # bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit4 <- lm(formula4, data = analytic3)\npublish(fit4)\n#>               Variable                            Units Coefficient           CI.95    p-value \n#>            (Intercept)                                       136.87  [34.96;238.79]   0.008533 \n#>                 gender                           Female         Ref                            \n#>                                                    Male      -13.06  [-16.60;-9.53]    < 1e-04 \n#>                    age                                         0.35     [0.24;0.46]    < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                            \n#>                                                  Others        7.88    [4.06;11.69]    < 1e-04 \n#>                   race                            Black         Ref                            \n#>                                                Hispanic       -5.79  [-10.34;-1.24]   0.012740 \n#>                                                   Other       -4.88   [-10.33;0.57]   0.079497 \n#>                                                   White       -0.85    [-5.02;3.33]   0.690720 \n#>              education                          College         Ref                            \n#>                                             High.School        2.85    [-0.32;6.02]   0.078008 \n#>                                                  School       -2.45    [-8.49;3.60]   0.427694 \n#>                married                          Married         Ref                            \n#>                                           Never.married       -5.74   [-9.65;-1.83]   0.004088 \n#>                                      Previously.married        0.34    [-3.52;4.20]   0.861981 \n#>                 income                             <25k         Ref                            \n#>                                        Between.25kto54k       -0.87    [-4.77;3.03]   0.663123 \n#>                                        Between.55kto99k        2.46    [-1.79;6.71]   0.256585 \n#>                                                Over100k        2.63    [-2.07;7.32]   0.272886 \n#>            diastolicBP                                         0.37     [0.25;0.50]    < 1e-04 \n#>             systolicBP                                         0.03    [-0.07;0.13]   0.544971 \n#>                    bmi                                        -0.31   [-0.54;-0.08]   0.009302 \n#>          triglycerides                                         0.12     [0.11;0.14]    < 1e-04 \n#>              uric.acid                                         1.36     [0.16;2.55]   0.025926 \n#>                protein                                         4.77   [-0.98;10.51]   0.104059 \n#>              bilirubin                                        -6.06  [-11.14;-0.98]   0.019519 \n#>             phosphorus                                        -0.08    [-2.71;2.55]   0.954561 \n#>                 sodium                                        -1.03   [-1.71;-0.35]   0.003175 \n#>              potassium                                         0.89    [-3.58;5.37]   0.695615 \n#>               globulin                                        -2.20    [-8.15;3.75]   0.469150 \n#>                calcium                                        12.20    [7.16;17.25]    < 1e-04 \n#>          physical.work                               No         Ref                            \n#>                                                     Yes       -0.44    [-3.68;2.80]   0.790297 \n#>  physical.recreational                               No         Ref                            \n#>                                                     Yes        1.24    [-2.03;4.51]   0.457666 \n#>               diabetes                               No         Ref                            \n#>                                                     Yes      -19.03 [-23.26;-14.80]    < 1e-04\n\n# check if there is still any problematic variable\n# with high collinearity problem\ncollinearity <- ols_vif_tol(fit4)\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")"
  },
  {
    "objectID": "predictivefactors3.html",
    "href": "predictivefactors3.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Explore relationships for binary outcome variable\nLoad data\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")\n\n\nCreating binary variable\nLet us create a binary variable using the ifelse function:\n\nShow the code# Binary variable\nanalytic3$cholesterol.bin <- ifelse(analytic3$cholesterol < 200, \"healthy\", \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>      1586      1046\n\n# Changing the reference category\nanalytic3$cholesterol.bin <- as.factor(analytic3$cholesterol.bin)\nanalytic3$cholesterol.bin <- relevel(analytic3$cholesterol.bin, ref = \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#> unhealthy   healthy \n#>      1046      1586\n\n\nModelling data\n\n\n\n\n\n\nTip\n\n\n\nWe use the glm function to run generalized linear models. The default family is gaussian with identity link. Setting binomial family with logit link (logit link is default for binomial family) means fitting logistic regression.\n\n\n\nShow the code# Regression model\nformula5x <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\n\n# Summary\nfit5x <- glm(formula5x, family = binomial(), data = analytic3)\npublish(fit5x)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.68 [1.27;2.23]   0.000313 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.69 [0.54;0.88]   0.002636 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.29 [0.95;1.75]   0.107104 \n#>                                                   Other      1.24 [0.87;1.79]   0.234062 \n#>                                                   White      1.10 [0.83;1.44]   0.505147 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.082168 \n#>                                                  School      1.23 [0.84;1.82]   0.292070 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.67]   0.052969 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.345688 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.01 [0.78;1.29]   0.957537 \n#>                                        Between.55kto99k      0.90 [0.69;1.19]   0.462854 \n#>                                                Over100k      0.90 [0.66;1.21]   0.472137 \n#>            diastolicBP                                       0.98 [0.97;0.98]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.029513 \n#>                    bmi                                       1.11 [0.99;1.24]   0.065627 \n#>             bodyweight                                       0.96 [0.92;1.00]   0.045338 \n#>             bodyheight                                       1.05 [1.00;1.09]   0.030995 \n#>                  waist                                       1.01 [0.99;1.02]   0.464825 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.273792 \n#>                protein                                       0.61 [0.42;0.89]   0.009192 \n#>              bilirubin                                       1.19 [0.86;1.66]   0.292632 \n#>             phosphorus                                       0.96 [0.81;1.13]   0.610931 \n#>                 sodium                                       1.06 [1.02;1.11]   0.007980 \n#>              potassium                                       0.95 [0.71;1.26]   0.729218 \n#>               globulin                                       1.38 [0.94;2.01]   0.101667 \n#>                calcium                                       0.64 [0.47;0.89]   0.007026 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.392539 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.05 [0.85;1.29]   0.681388 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.68 [2.02;3.56]    < 1e-04\n\n# VIF\ncar::vif(fit5x)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.735258  1        1.653862\n#> age                     2.121098  1        1.456399\n#> born                    1.664094  1        1.289998\n#> race                    2.585539  3        1.171544\n#> education               1.458430  2        1.098933\n#> married                 1.432595  2        1.094034\n#> income                  1.426911  3        1.061043\n#> diastolicBP             1.297308  1        1.138994\n#> systolicBP              1.614374  1        1.270580\n#> bmi                    81.928815  1        9.051454\n#> bodyweight            103.125772  1       10.155086\n#> bodyheight             22.647853  1        4.758976\n#> waist                  11.493710  1        3.390237\n#> triglycerides           1.258340  1        1.121758\n#> uric.acid               1.636512  1        1.279262\n#> protein                 3.684816  1        1.919587\n#> bilirubin               1.186181  1        1.089119\n#> phosphorus              1.117915  1        1.057315\n#> sodium                  1.123193  1        1.059808\n#> potassium               1.181358  1        1.086903\n#> globulin                3.427401  1        1.851324\n#> calcium                 1.543019  1        1.242183\n#> physical.work           1.090958  1        1.044490\n#> physical.recreational   1.218558  1        1.103883\n#> diabetes                1.212365  1        1.101074\n\n\nAUC\nLet us measure the accuracy for classification models fit5x.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the roc function to build a ROC curve and auc function to calculate the AUC (are under the ROC curve) value.\n\n\n\nShow the coderequire(pROC)\npred.y <- predict(fit5x, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7411\n\nauc(rocobj)\n#> Area under the curve: 0.7411\n\n\nRe-modelling\nLet us re-fit the model and measure the AUC:\n\nShow the codeformula5 <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\npublish(fit5)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.86 [1.48;2.33]    < 1e-04 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.67 [0.52;0.85]   0.001233 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.26 [0.94;1.69]   0.128165 \n#>                                                   Other      1.21 [0.85;1.73]   0.284083 \n#>                                                   White      1.11 [0.85;1.45]   0.460652 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.083806 \n#>                                                  School      1.22 [0.83;1.80]   0.305514 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.68]   0.049983 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.339401 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.00 [0.78;1.28]   0.999445 \n#>                                        Between.55kto99k      0.90 [0.68;1.17]   0.425427 \n#>                                                Over100k      0.89 [0.66;1.20]   0.447012 \n#>            diastolicBP                                       0.98 [0.97;0.99]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.042769 \n#>                    bmi                                       1.01 [0.99;1.02]   0.496430 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.242942 \n#>                protein                                       0.62 [0.43;0.89]   0.010343 \n#>              bilirubin                                       1.24 [0.89;1.72]   0.203993 \n#>             phosphorus                                       0.95 [0.80;1.12]   0.539847 \n#>                 sodium                                       1.06 [1.02;1.11]   0.006777 \n#>              potassium                                       0.96 [0.72;1.28]   0.790080 \n#>               globulin                                       1.37 [0.94;2.00]   0.102430 \n#>                calcium                                       0.64 [0.46;0.88]   0.005772 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.382281 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.04 [0.85;1.29]   0.682962 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.69 [2.03;3.57]    < 1e-04\n\n# VIF\ncar::vif(fit5)\n#>                           GVIF Df GVIF^(1/(2*Df))\n#> gender                1.749947  1        1.322856\n#> age                   1.850160  1        1.360206\n#> born                  1.640947  1        1.280994\n#> race                  2.345460  3        1.152669\n#> education             1.430721  2        1.093676\n#> married               1.432015  2        1.093923\n#> income                1.409064  3        1.058819\n#> diastolicBP           1.289411  1        1.135523\n#> systolicBP            1.605248  1        1.266984\n#> bmi                   1.477795  1        1.215646\n#> triglycerides         1.246395  1        1.116421\n#> uric.acid             1.624039  1        1.274378\n#> protein               3.648367  1        1.910070\n#> bilirubin             1.177643  1        1.085193\n#> phosphorus            1.114298  1        1.055603\n#> sodium                1.117463  1        1.057101\n#> potassium             1.176914  1        1.084857\n#> globulin              3.395946  1        1.842809\n#> calcium               1.542486  1        1.241969\n#> physical.work         1.089742  1        1.043907\n#> physical.recreational 1.197719  1        1.094404\n#> diabetes              1.200402  1        1.095629\n\n\n\nShow the code#### AUC\npred.y <- predict(fit5, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7406\nauc(rocobj)\n#> Area under the curve: 0.7406\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nReferences"
  },
  {
    "objectID": "predictivefactors4.html",
    "href": "predictivefactors4.html",
    "title": "Overfitting and performance",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nNow we will fit the final model that we decided at the end of previous part of the lab.\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4 <- lm(formula4, data = analytic3)\nsummary(fit4)\n#> \n#> Call:\n#> lm(formula = formula4, data = analytic3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -115.465  -23.695   -2.598   20.017  177.264 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               136.871606  51.998527   2.632  0.00853 ** \n#> genderMale                -13.064857   1.802099  -7.250 5.48e-13 ***\n#> age                         0.351838   0.056116   6.270 4.22e-10 ***\n#> bornOthers                  7.877420   1.947498   4.045 5.39e-05 ***\n#> raceHispanic               -5.790547   2.323010  -2.493  0.01274 *  \n#> raceOther                  -4.879882   2.781673  -1.754  0.07950 .  \n#> raceWhite                  -0.847635   2.130149  -0.398  0.69072    \n#> educationHigh.School        2.851633   1.617435   1.763  0.07801 .  \n#> educationSchool            -2.446765   3.084409  -0.793  0.42769    \n#> marriedNever.married       -5.739509   1.997152  -2.874  0.00409 ** \n#> marriedPreviously.married   0.342206   1.968165   0.174  0.86198    \n#> incomeBetween.25kto54k     -0.867063   1.990253  -0.436  0.66312    \n#> incomeBetween.55kto99k      2.462130   2.169757   1.135  0.25658    \n#> incomeOver100k              2.626046   2.394560   1.097  0.27289    \n#> diastolicBP                 0.374971   0.062238   6.025 1.93e-09 ***\n#> systolicBP                  0.029976   0.049515   0.605  0.54497    \n#> bmi                        -0.309530   0.118927  -2.603  0.00930 ** \n#> triglycerides               0.124806   0.006427  19.419  < 2e-16 ***\n#> uric.acid                   1.357242   0.609012   2.229  0.02593 *  \n#> protein                     4.767008   2.931636   1.626  0.10406    \n#> bilirubin                  -6.060791   2.593508  -2.337  0.01952 *  \n#> phosphorus                 -0.076472   1.341957  -0.057  0.95456    \n#> sodium                     -1.026686   0.347679  -2.953  0.00318 ** \n#> potassium                   0.893507   2.283488   0.391  0.69561    \n#> globulin                   -2.198037   3.036091  -0.724  0.46915    \n#> calcium                    12.202366   2.574400   4.740 2.25e-06 ***\n#> physical.workYes           -0.439108   1.651078  -0.266  0.79030    \n#> physical.recreationalYes    1.238756   1.667670   0.743  0.45767    \n#> diabetesYes               -19.032748   2.158825  -8.816  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 35.22 on 2603 degrees of freedom\n#> Multiple R-squared:  0.2415, Adjusted R-squared:  0.2334 \n#> F-statistic: 29.61 on 28 and 2603 DF,  p-value: < 2.2e-16\n\n\nDesign Matrix\nExpands factors to a set of dummy variables.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the model.matrix function to construct a design/model matrix, such as expand factor variables to a matrix of dummy variable\n\n\n\nShow the codehead(model.matrix(fit4))\n#>    (Intercept) genderMale age bornOthers raceHispanic raceOther raceWhite\n#> 1            1          1  62          0            0         0         1\n#> 2            1          1  53          1            0         0         1\n#> 4            1          0  56          0            0         0         1\n#> 5            1          0  42          0            0         0         0\n#> 10           1          1  22          0            0         0         0\n#> 11           1          0  32          1            1         0         0\n#>    educationHigh.School educationSchool marriedNever.married\n#> 1                     0               0                    0\n#> 2                     1               0                    0\n#> 4                     0               0                    0\n#> 5                     0               0                    0\n#> 10                    0               0                    1\n#> 11                    0               0                    0\n#>    marriedPreviously.married incomeBetween.25kto54k incomeBetween.55kto99k\n#> 1                          0                      0                      1\n#> 2                          1                      0                      0\n#> 4                          0                      0                      1\n#> 5                          1                      1                      0\n#> 10                         0                      1                      0\n#> 11                         0                      1                      0\n#>    incomeOver100k diastolicBP systolicBP  bmi triglycerides uric.acid protein\n#> 1               0          70        128 27.8           158       4.2     7.5\n#> 2               0          88        146 30.8           170       7.0     7.4\n#> 4               0          72        132 42.4            93       5.4     6.1\n#> 5               0          70        100 20.3            52       3.3     7.7\n#> 10              0          70        110 28.0            77       6.0     7.4\n#> 11              0          70        120 28.2           295       5.2     7.4\n#>    bilirubin phosphorus sodium potassium globulin calcium physical.workYes\n#> 1        0.5        4.7    136      4.30      2.9     9.8                0\n#> 2        0.6        4.4    140      4.55      2.9     9.8                0\n#> 4        0.3        3.8    141      4.08      2.3     8.9                0\n#> 5        0.3        3.2    136      3.50      3.4     9.3                0\n#> 10       0.2        5.3    139      4.16      3.0     9.3                0\n#> 11       0.4        3.1    138      4.31      2.9    10.3                0\n#>    physical.recreationalYes diabetesYes\n#> 1                         0           1\n#> 2                         0           0\n#> 4                         0           0\n#> 5                         0           0\n#> 10                        1           0\n#> 11                        0           0\n\n# Dimension of the model matrix\ndim(model.matrix(fit4))\n#> [1] 2632   29\n\n# Number of parameters = intercept + slopes\np <- dim(model.matrix(fit4))[2] \np\n#> [1] 29\n\n\nCheck prediction\n\nShow the codeobs.y <- analytic3$cholesterol\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   163.0   189.0   191.5   216.0   362.0\n\n# Predict the above fit on analytic3 data\npred.y <- predict(fit4, analytic3)\nsummary(pred.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   136.3   178.2   189.4   191.5   202.4   337.6\nn <- length(pred.y)\nn\n#> [1] 2632\nplot(obs.y,pred.y)\nlines(lowess(obs.y,pred.y), col = \"red\")\n\n\n\nShow the code\n# Prediction on a new data: fictitious.data\nstr(fictitious.data)\n#> 'data.frame':    4121 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83734 83735 83736 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n#>  $ age                  : num  62 53 78 56 42 72 22 32 56 46 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"High.School\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"<25k\" \"Between.55kto99k\" ...\n#>  $ weight               : num  135630 25282 12576 102079 18235 ...\n#>  $ psu                  : num  1 1 1 1 2 1 2 1 2 1 ...\n#>  $ strata               : num  125 125 131 131 126 128 128 125 126 121 ...\n#>  $ diastolicBP          : num  70 88 46 72 70 58 70 70 116 94 ...\n#>  $ systolicBP           : num  128 146 138 132 100 116 110 120 178 144 ...\n#>  $ bodyweight           : num  94.8 90.4 83.4 109.8 55.2 ...\n#>  $ bodyheight           : num  184 171 170 161 165 ...\n#>  $ bmi                  : num  27.8 30.8 28.8 42.4 20.3 28.6 28 28.2 33.6 27.6 ...\n#>  $ waist                : num  101.1 107.9 116.5 110.1 80.4 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Not.at.all\" \"Not.at.all\" ...\n#>  $ alcohol              : num  1 6 0 1 1 0 8 1 0 1 ...\n#>  $ cholesterol          : num  173 265 229 174 204 190 164 190 145 242 ...\n#>  $ cholesterolM2        : num  4.47 6.85 5.92 4.5 5.28 4.91 4.24 4.91 3.75 6.26 ...\n#>  $ triglycerides        : num  158 170 299 93 52 52 77 295 121 497 ...\n#>  $ uric.acid            : num  4.2 7 7.3 5.4 3.3 4.9 6 5.2 4.8 6.5 ...\n#>  $ protein              : num  7.5 7.4 7.3 6.1 7.7 7.1 7.4 7.4 6.9 6.8 ...\n#>  $ bilirubin            : num  0.5 0.6 0.5 0.3 0.3 0.5 0.2 0.4 0.4 0.5 ...\n#>  $ phosphorus           : num  4.7 4.4 3.6 3.8 3.2 3.7 5.3 3.1 4.1 3.6 ...\n#>  $ sodium               : num  136 140 140 141 136 140 139 138 140 138 ...\n#>  $ potassium            : num  4.3 4.55 4.7 4.08 3.5 4.2 4.16 4.31 4.5 4.27 ...\n#>  $ globulin             : num  2.9 2.9 2.8 2.3 3.4 3 3 2.9 2.9 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.7 8.9 9.3 9.3 9.3 10.3 9.5 9.3 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:885] 16 30 39 48 50 58 61 65 67 68 ...\n#>   ..- attr(*, \"names\")= chr [1:885] \"27\" \"68\" \"90\" \"112\" ...\npred.y.new1 <- predict(fit4, fictitious.data)\nsummary(pred.y.new1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   128.7   178.9   190.6   192.5   203.3   557.4\n\n\nMeasuring prediction error\nContinuous outcomes\nR2\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find SSE\nSSE <- sum( (obs.y - pred.y)^2 )\nSSE\n#> [1] 3228460\n\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 4256586\n\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.2415378\n\nrequire(caret)\nR2(pred.y, obs.y)\n#> [1] 0.2415378\n\n\nRMSE\n\n\nSee Wikipedia (2023b)\n\nShow the code# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 35.21767\n\nRMSE(pred.y, obs.y)\n#> [1] 35.02311\n\n\nAdj R2\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p))\nadjR2\n#> [1] 0.2333791\n\n\nWriting function\nSyntax for Writing Functions\n\nShow the codefunc_name <- function (argument) {\n  A statement or multiple lines of statements\n  return(output)\n}\n\n\nExample of a simple function\n\nShow the codef1 <- function(a,b){\n  result <- a + b\n  return(result)\n}\nf1(a=1,b=3)\n#> [1] 4\nf1(a=1,b=6)\n#> [1] 7\n# setting default values\nf1 <- function(a=1,b=1){\n  result <- a + b\n  return(result)\n}\nf1()\n#> [1] 2\nf1(b = 10)\n#> [1] 11\n\n\nA bit more complicated\n\nShow the code# one argument\nmodel.fit <- function(data.for.fitting){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#> 184.3131838  -7.8095595   0.2225745  11.1557140\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#> 176.1286576  -4.8256829   0.3375009   7.7186190\n\n\n\nShow the code# adding one more argument: digits\nmodel.fit <- function(data.for.fitting, digits=2){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  result <- round(result,digits)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#>      184.31       -7.81        0.22       11.16\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#>      176.13       -4.83        0.34        7.72\n\n\nFunction that gives performance measures\nlet us create a function that will give us the performance measures:\n\nShow the codeperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  \n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  \n  # sample size\n  n <- length(pred.y)\n  \n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  \n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  \n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  \n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  \n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  \n  # returning object\n  return(res)\n}\nperform(new.data = analytic3, y.name = \"cholesterol\", model.fit = fit4)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 2632 29 0.242 0.233 35.023\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "predictivefactors5.html",
    "href": "predictivefactors5.html",
    "title": "Data spliting",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data anf files\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nData spliting to avoid model overfitting\n\n\nKDnuggets (2023)\n\nKuhn (2023a)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use the createDataPartition function to split a dataset into training and testing datasets\n\n\n\nShow the code# Using a seed to randomize in a reproducible way \nset.seed(123)\nsplit <- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:1844, 1] 3 4 5 8 9 13 14 16 20 21 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 1844    1\n\n# Approximate train data\ndim(analytic3)*.7 \n#> [1] 1842.4   24.5\n\n# Approximate test data\ndim(analytic3)*(1-.7) \n#> [1] 789.6  10.5\n\n\nSplit the data\nNow let us split the dataset into training and testing:\n\nShow the code# Create train data\ntrain.data <- analytic3[split,]\ndim(train.data)\n#> [1] 1844   35\n\n# Create test data\ntest.data <- analytic3[-split,]\ndim(test.data)\n#> [1] 788  35\n\n\nOur next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.\nTrain the model\n\nShow the codeformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4.train1 <- lm(formula4, data = train.data)\nsummary(fit4.train1)\n#> \n#> Call:\n#> lm(formula = formula4, data = train.data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -91.973 -23.719  -1.563  20.586 178.542 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                72.716792  59.916086   1.214  0.22504    \n#> genderMale                -11.293629   2.136545  -5.286 1.40e-07 ***\n#> age                         0.306235   0.066376   4.614 4.23e-06 ***\n#> bornOthers                  7.220858   2.300658   3.139  0.00172 ** \n#> raceHispanic               -6.727473   2.709718  -2.483  0.01313 *  \n#> raceOther                  -4.865771   3.237066  -1.503  0.13298    \n#> raceWhite                  -1.468522   2.494981  -0.589  0.55621    \n#> educationHigh.School        1.626097   1.920289   0.847  0.39722    \n#> educationSchool            -4.853095   3.585185  -1.354  0.17602    \n#> marriedNever.married       -5.298265   2.332033  -2.272  0.02321 *  \n#> marriedPreviously.married   1.202448   2.305191   0.522  0.60199    \n#> incomeBetween.25kto54k     -1.736495   2.360385  -0.736  0.46202    \n#> incomeBetween.55kto99k      0.170505   2.565896   0.066  0.94703    \n#> incomeOver100k              1.712359   2.860226   0.599  0.54946    \n#> diastolicBP                 0.355813   0.074380   4.784 1.86e-06 ***\n#> systolicBP                  0.037464   0.059848   0.626  0.53140    \n#> bmi                        -0.282881   0.139160  -2.033  0.04222 *  \n#> triglycerides               0.123797   0.007613  16.261  < 2e-16 ***\n#> uric.acid                   1.006499   0.712871   1.412  0.15815    \n#> protein                     1.721623   3.468969   0.496  0.61975    \n#> bilirubin                  -6.143411   3.006858  -2.043  0.04118 *  \n#> phosphorus                  0.093824   1.575489   0.060  0.95252    \n#> sodium                     -0.604286   0.400694  -1.508  0.13170    \n#> potassium                  -0.583525   2.715189  -0.215  0.82986    \n#> globulin                   -0.278970   3.614404  -0.077  0.93849    \n#> calcium                    15.679677   3.054968   5.133 3.17e-07 ***\n#> physical.workYes           -1.099540   1.960321  -0.561  0.57494    \n#> physical.recreationalYes    0.834737   1.953960   0.427  0.66928    \n#> diabetesYes               -19.932101   2.580138  -7.725 1.83e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 34.68 on 1815 degrees of freedom\n#> Multiple R-squared:  0.2433, Adjusted R-squared:  0.2316 \n#> F-statistic: 20.84 on 28 and 1815 DF,  p-value: < 2.2e-16\n\n\nExtract performance measures\n\n\n\n\n\n\nTip\n\n\n\nBelow we use the perform function that we saved to evaluate the model performances\n\n\n\nShow the codeperform(new.data = train.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma   logLik      AIC\n#> [1,] 1844 29        1815 2182509 2884109 0.243 0.232 34.677 -9140.98 18341.96\n#>           BIC\n#> [1,] 18507.55\nperform(new.data = test.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>        n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 788 29         759 1057454 1372214 0.229 0.201 37.326 -3955.936 7971.873\n#>           BIC\n#> [1,] 8111.958\nperform(new.data = analytic3,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2 sigma    logLik      AIC\n#> [1,] 2632 29        2603 3239962 4256586 0.239 0.231 35.28 -13098.82 26257.64\n#>           BIC\n#> [1,] 26433.91\nperform(new.data = fictitious.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 4121 29        4092 5306559 6912485 0.232 0.227 36.011 -20601.92 41263.84\n#>           BIC\n#> [1,] 41453.55\n\n\n\n\nFor more on model training and tuning, see Kuhn (2023b)\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023a. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.\n\n\n———. 2023b. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html."
  },
  {
    "objectID": "predictivefactors6.html",
    "href": "predictivefactors6.html",
    "title": "Cross-vaildation",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nk-fold cross-vaildation\n\n\nSee Wikipedia (2023)\n\nShow the codek = 5\ndim(analytic3)\n#> [1] 2632   35\nset.seed(567)\n\n# Create folds (based on the outcome)\nfolds <- createFolds(analytic3$cholesterol, k = k, list = TRUE, \n                     returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\n\n# Approximate training data size\ndim(analytic3)*4/5\n#> [1] 2105.6   28.0\n\n# Approximate test data size\ndim(analytic3)/5  \n#> [1] 526.4   7.0\n\nlength(folds[[1]])\n#> [1] 2105\nlength(folds[[2]])\n#> [1] 2107\nlength(folds[[3]])\n#> [1] 2106\nlength(folds[[4]])\n#> [1] 2105\nlength(folds[[5]])\n#> [1] 2105\n\nstr(folds[[1]])\n#>  int [1:2105] 1 3 5 6 8 10 11 12 13 14 ...\nstr(folds[[2]])\n#>  int [1:2107] 1 2 3 4 5 6 7 8 9 12 ...\nstr(folds[[3]])\n#>  int [1:2106] 2 4 5 7 8 9 10 11 12 14 ...\nstr(folds[[4]])\n#>  int [1:2105] 1 2 3 4 6 7 8 9 10 11 ...\nstr(folds[[5]])\n#>  int [1:2105] 1 2 3 4 5 6 7 9 10 11 ...\n\n\nCalculation for Fold 1\n\nShow the codefold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1]  1  3  5  6  8 10\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\nmodel.fit <- lm(formula4, data = fold1.train)\npredictions <- predict(model.fit, newdata = fold1.test)\n\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual      SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 527 29         498 637317.5 830983.2 0.233  0.19 35.774 -2618.471 5296.942\n#>           BIC\n#> [1,] 5424.958\n\n\nCalculation for Fold 2\n\nShow the codefold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 3 4 5 6\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\n\nmodel.fit <- lm(formula4, data = fold1.train)\n\npredictions <- predict(model.fit, newdata = fold1.test)\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual    SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 525 29         496 615243 785326.6 0.217 0.172 35.219 -2600.282 5260.564\n#>           BIC\n#> [1,] 5388.466\n\n\nUsing caret package to automate\n\n\nSee Kuhn (2023)\n\nShow the code# Using Caret package\nset.seed(567)\n\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n\n# fit the model with formula = formula4\n# use training method lm\nfit4.cv<-train(formula4, trControl = ctrl,\n               data = analytic3, method = \"lm\")\nfit4.cv\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2105, 2106, 2105, 2106 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.62758  0.2194187  27.85731\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# extract results from each test data \nsummary.res <- fit4.cv$resample\nsummary.res\n\n\n\n  \n\n\nShow the codemean(fit4.cv$resample$Rsquared)\n#> [1] 0.2194187\nsd(fit4.cv$resample$Rsquared)\n#> [1] 0.02755561\n\n# # extract adj R2\n# k <- 5\n# p <- 2\n# n <- round(nrow(analytic3)/k)\n# summary.res$adjR2 <- 1-(1-fit4.cv$resample$Rsquared)*((n-1)/(n-p))\n# summary.res\n\n\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "predictivefactors7.html",
    "href": "predictivefactors7.html",
    "title": "Bootstrap",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nResampling a vector\n\nShow the codefake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n\n\n\nShow the coderesampled.fake.data <- sample(fake.data, size = length(fake.data), replace = TRUE)\nresampled.fake.data\n#> [1] 4 1 3 3 1\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 4 1 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 2 5\n\n\nThe samples not selected are known as the out-of-bag samples\n\nShow the codeB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 1 4 4 1 4\n#> [1] 5 3 1 3 3\n#> [1] 2 5 4 5 3\n#> [1] 4 4 3 5 4\n#> [1] 4 5 3 5 5\n#> [1] 3 5 3 3 2\n#> [1] 4 4 5 3 2\n#> [1] 1 4 2 4 3\n#> [1] 2 5 5 4 4\n#> [1] 2 3 3 5 4\n\n\nCalculating SD of a statistics\nIdea:\n\nNot sure about what distribution is appropriate to make inference?\nIf that is the case, calculating CI is hard.\nresample and get a new bootstrap sample\ncalculate a statistic (say, mean) from that sample\nfind SD of those statistic (say, means)\nUse those SD to calculate CI\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 4 4 2 3 5\n#>  $ : int [1:5] 4 2 2 1 3\n#>  $ : int [1:5] 2 2 1 2 2\n#>  $ : int [1:5] 4 4 3 1 4\n#>  $ : int [1:5] 3 2 4 4 2\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 3.6 2.4 1.8 3.2 3.0\nmean(B.means)\n#> [1] 2.8\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.7071068\n\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 3.018\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6366337\nmean(B.medians)\n#> [1] 3.05\nhist(B.means)\n\n\n\nShow the code\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 0.996224\nhist(B.medians)\n\n\n\n\nResampling a data or matrix\n\nShow the codeanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n1\n83732\nMale\n62\n\n\n2\n83733\nMale\n53\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n21\n83752\nFemale\n30\n\n\n\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n1\n83732\nMale\n62\n\n\n21.1\n83752\nFemale\n30\n\n\n1.1\n83732\nMale\n62\n\n\n1.2\n83732\nMale\n62\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83732\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83741 83747 83750\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n21.1\n83752\nFemale\n30\n\n\n21.2\n83752\nFemale\n30\n\n\n21.3\n83752\nFemale\n30\n\n\n19\n83750\nMale\n45\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83750\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732 83741 83747\n\n\nThe caret package / boot\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time).\n\nShow the codeset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n\n\nMethod boot632\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n\n\nMethod boot632 for stepwise\nA stable model\n\n\nSee Kuhn (2023)\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\nShow the codectrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n\n\nAn unstable model\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n\n\nNote that SD should be higher for larger B.\nOptimism corrected bootstrap\n\n\nSee Bondarenko and Consulting (2023)\nSteps:\n\nFit a model M to entire data D and estimate predictive ability R2.\nIterate from b=1 to B:\n\nTake a resample from the original data, and name it D.star\nFit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\nUse the bootstrap model M.star to get predictive ability on D, R2.fullData\n\n\nOptimism Opt is calculated as mean(R2.boot - R2.fullData)\nCalculate optimism corrected performance as R2-Opt.\n\n\nShow the codeR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n\n\nBinary outcome\nAUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\nAUC = 1 (perfect classification) AUC = 0.5 (random classification)\n\nShow the codeset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n\n\n\n  \n\n\nShow the codemean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n\n\n\nShow the coderequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n\n\nYouTube Videos\nYou could watch the videos describing Cross-validation, Bootstrapping, Change-in-estimate and Collapsibility.\nReferences\n\n\n\n\nBondarenko, Vadim, and FI Consulting. 2023. “The Bootstrap Approach to Managing Model Uncertainty.” https://rstudio-pubs-static.s3.amazonaws.com/90467_c70206f3dc864d53bf36072207ee011d.html.\n\n\nKuhn, Max. 2023. “Available Models.” https://topepo.github.io/caret/available-models.html."
  },
  {
    "objectID": "predictivefactorsF.html",
    "href": "predictivefactorsF.html",
    "title": "R for predictive factors",
    "section": "",
    "text": "The list of new R functions introduced in this Predictive factors lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggregate \n    base/stats \n    To see summary by groups, e.g., by gender \n  \n\n anova \n    base/stats \n    To compare models \n  \n\n auc \n    pROC \n    To compute the AUC (area under the ROC curve) value \n  \n\n BrierScore \n    DescTools \n    To calculate the Brier score \n  \n\n coef \n    base/stats \n    To see the coefficients of a fitted model \n  \n\n cor \n    base/stats \n    To see the correlation between numeric variables \n  \n\n corrplot \n    corrplot \n    To visualize a correlation matrix \n  \n\n createDataPartition \n    caret \n    To split a dataset into training and testing sets \n  \n\n createFolds \n    caret \n    To create k folds based on the outcome variable \n  \n\n crPlots \n    car \n    To see partial residual plot \n  \n\n describeBy \n    psych \n    To see summary by groups, e.g., by gender \n  \n\n glm \n    base/stats \n    To run generalized linear models \n  \n\n group_by \n    dplyr \n    To group by variables \n  \n\n hat \n    base/stats \n    To return a hat matrix \n  \n\n ifelse \n    base \n    To set an condition, e.g., creating a categorical variable from a numerical variable based on a condition \n  \n\n kable \n    knitr \n    To create a nice table \n  \n\n layout \n    base/graphics \n    To specify plot arrangement \n  \n\n lines \n    base/graphics \n    To draw a line graph \n  \n\n lm \n    base/stats \n    To fit a linear regression \n  \n\n lowess \n    base/stats \n    To smooth a scatter plot \n  \n\n model.matrix \n    base/stats \n    To construct a design/model matrix, e.g., a matrix with covariate values \n  \n\n ols_plot_resid_lev \n    olsrr \n    To visualize the residuals vs leverage plot \n  \n\n ols_vif_tol \n    olsrr \n    To calculate tolerance and variance inflation factor \n  \n\n predict \n    base/stats \n    `predict` is a generic function that is used for prediction, e.g., predicting probability of an event from a model \n  \n\n R2 \n    caret \n    To calculate the R-squared value \n  \n\n RMSE \n    caret \n    To calculate the RMSE value \n  \n\n roc \n    pROC \n    To build a ROC curve \n  \n\n sample \n    base \n    To take/draw random samples with or without replacement \n  \n\n save.image \n    base \n    To save an R object \n  \n\n spearman2 \n    Hmisc \n    To compute the square of Spearman's rank correlation \n  \n\n summarize \n    dplyr \n    To see summary \n  \n\n tapply \n    base \n    To apply a function over an array, e.g., to see the summary of a variable by gender \n  \n\n train \n    caret \n    To fit the model with tuning hyperparameters \n  \n\n trainControl \n    caret \n    To tune the hyperparameters, i.e., controlling the parameters to train the model \n  \n\n varclus \n    Hmisc \n    We use the `varclus` function to identify collinear predictors with cluster analysis \n  \n\n vif \n    car \n    To calculate variance inflation factor \n  \n\n which \n    base \n    To see which indices are TRUE"
  },
  {
    "objectID": "surveydata.html",
    "href": "surveydata.html",
    "title": "Survey data analysis",
    "section": "",
    "text": "This chapter is about:\n\ncreating the analytic survey dataset\nchecking the analytic survey dataset\nbivariate analysis of complex survey dataset\nlogistic regression in a complex survey dataset\nmodel performance of logistic regression in a complex survey dataset\nanalyzing the analytic dataset from NHANES\nsubsetting in complex survey data"
  },
  {
    "objectID": "surveydataF.html",
    "href": "surveydataF.html",
    "title": "R for survey data analysis",
    "section": "",
    "text": "The list of new R functions introduced in this Survey data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n AIC \n    base/stats \n    To extract the AIC value of a model \n  \n\n as.character \n    base \n    To create a character vector \n  \n\n as.numeric \n    base \n    To create a numeric vector \n  \n\n eval \n    base \n    To evaluate an expression \n  \n\n fitted \n    base/stats \n    To extract fitted values of a model \n  \n\n ls \n    base \n    To see the list of objects \n  \n\n psrsq \n    survey \n    To compute the Nagelkerke and Cox-Snell pseudo R-squared statistics for survey data \n  \n\n regTermTest \n    survey \n    To test for an additional variable in a regression model \n  \n\n residuals \n    base/stats \n    To extract residuals of a model \n  \n\n stepAIC \n    MASS \n    To choose a model by stepwise AIC \n  \n\n step \n    base/stats \n    To choose a model by stepwise AIC but it can keep the pre-specified variables in the model \n  \n\n summ \n    jtools \n    To show/publish regression tables \n  \n\n svyboxplot \n    survey \n    To produce a box plot for survey data \n  \n\n svyby \n    survey \n    To see the summary statistics for a survey design \n  \n\n svychisq \n    survey \n    To test the bivariate assocaition between two categorical variables for survey data \n  \n\n svyCreateTableOne \n    tableone \n    To create a frequency table with a survey design \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n update \n    base/stats \n    To update and re-fit a regression model"
  },
  {
    "objectID": "missingdata.html",
    "href": "missingdata.html",
    "title": "Missing data analysis",
    "section": "",
    "text": "This chapter is about:\n\nmissing data and imputation\nmultiple imputation in complex survey data\nmultiple imputation then deletion (MID)\nestimating model performance from multiple imputed datasets\ndealing with subpopulations with missing observations\ntesting MCAR assumption empirically in the data\neffect modification within multiple imputation"
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R for missing data",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggr \n    VIM \n    To calculate/plot the missing values in the variables \n  \n\n boxplot \n    base/graphics \n    To produce a box plot \n  \n\n bwplot \n    mice \n    To produce box plot to compare the imputed and observed values \n  \n\n colMeans \n    base \n    To compute the column-wise mean, i.e., mean for each variable/column \n  \n\n complete \n    mice \n    To extract the imputed dataset \n  \n\n complete.cases \n    base/stats \n    To select the complete cases, i.e., observations without missing values \n  \n\n D1 \n    mice \n    To conduct the multivariate Wald test with D1-statistic \n  \n\n densityplot \n    mice \n    To produce desnsity plots \n  \n\n expression \n    base \n    To set/create an expression \n  \n\n imputationList \n    mice \n    To combine multiple imputed datasets \n  \n\n marginplot \n    VIM \n    To draw a scatterplot with additional information when there are missing values \n  \n\n mcar_test \n    naniar \n    To conduct Little's MCAR test \n  \n\n md.pattern \n    mice \n    To see the pattern of the missing data \n  \n\n mice \n    mice \n    To impute missing data where the argument m represents the number of multiple imputation \n  \n\n MIcombine \n    mitools \n    To combine/pool the results using Rubin's rule \n  \n\n MIextract \n    mitools \n    To extract parameters from a list of outputs \n  \n\n na.test \n    misty \n    To conduct Little's MCAR test \n  \n\n parlmice \n    mice \n    To run `mice` function in parallel, i.e., parallel computing of mice \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n pool \n    mice \n    To pool the results using Rubin's rule \n  \n\n pool.compare \n    mice \n    To compare two nested models \n  \n\n pool_mi \n    miceadds \n    To combine/pool the results using Rubin's rule \n  \n\n quickpred \n    mice \n    To set imputation model based on the correlation \n  \n\n sim_slopes \n    interactions \n    To perform simple slope analyses \n  \n\n TestMCARNormality \n    MissMech \n    To test multivariate normality and homoscedasticity in the context of missing data \n  \n\n unlist \n    base \n    To convert a list to a vector"
  },
  {
    "objectID": "propensityscore.html",
    "href": "propensityscore.html",
    "title": "Propensity score",
    "section": "",
    "text": "This chapter is about:\n\nCovariate matching using CCHS\nPropensity score matching using CCHS\nPropensity score matching using NHANES\nPropensity score matching using NHANES when some variables have missing observations"
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R for propensity score",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n bal.plot \n    cobalt \n    To produce a overalp/balance plot for propensity scoes \n  \n\n bal.tab \n    cobalt \n    To check the balance at each category of covariates \n  \n\n CreateCatTable \n    tableone \n    To create a frequency table with categorical variables only \n  \n\n do.call \n    base \n    To execute a function call \n  \n\n love.plot \n    cobalt \n    To plot the standardized mean differences at each category of covariates \n  \n\n match.data \n    MatchIt \n    To extract the matched dataste from a matchit object \n  \n\n matchit \n    MatchIt \n    To match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m. \n  \n\n rownames \n    base \n    Names of the rows"
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "This chapter is about:\n\nanalysis reporting guidelines"
  },
  {
    "objectID": "reportingF.html",
    "href": "reportingF.html",
    "title": "R for reporting",
    "section": "",
    "text": "The list of new R functions introduced in this reporting lab component are below:"
  },
  {
    "objectID": "machinelearning.html#key-references",
    "href": "machinelearning.html#key-references",
    "title": "Machine learning",
    "section": "Key References",
    "text": "Key References\n\nWatch the reference video \n(Bi et al. 2019)\n(Liu et al. 2019)\n(Kuhn et al. 2013)"
  },
  {
    "objectID": "machinelearning.html#additional-useful-references",
    "href": "machinelearning.html#additional-useful-references",
    "title": "Machine learning",
    "section": "Additional useful references",
    "text": "Additional useful references\n\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)\n\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. “What Is Machine Learning? A Primer for the Epidemiologist.” American Journal of Epidemiology 188 (12): 2222–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. “Over-Fitting and Model Tuning.” Applied Predictive Modeling, 61–92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. “How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature.” Jama 322 (18): 1806–16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. “Predictor Selection.” Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395–429."
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R for machine learning",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n fancyRpartPlot \n    rattle \n    To plot an rpart object \n  \n\n fviz_nbclust \n    factoextra \n    To visualize the optimal number of clusters \n  \n\n kmeans \n    base/stats \n    To conduct K-Means cluster analysis \n  \n\n lowess \n    base/stats \n    To perform scatter plot smoothing aka lowess smoothing \n  \n\n rpart \n    rpart \n    To fit a classification tree (CART) \n  \n\n terms \n    base/stats \n    To extarct terms objects \n  \n\n varImp \n    caret \n    To calculate the variable importance measure"
  },
  {
    "objectID": "machinelearningCausal.html",
    "href": "machinelearningCausal.html",
    "title": "ML in causal inference",
    "section": "",
    "text": "This chapter is about:\n\nMachine learning and their use in causal inference such as propensity score modelling\nTMLE in medical research"
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R for ML in causal inference",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\n Function.name \n    Package.name \n    Use \n  \n\n\n ExtractSmd \n    tableone \n    To extract the standardized mean differences of a tableone object \n  \n\n listWrappers \n    SuperLearner \n    To see the list of wrapper functions, i.e., list of learners, in SuperLearner \n  \n\n Match \n    Matching \n    To match an exposed/treated to M unexposed/controls"
  }
]