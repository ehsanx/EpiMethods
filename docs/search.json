[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "",
    "text": "The Project\nWelcome to a place crafted to bridge a unique gap in the health research world. This website offers valuable resources for those who are taking their first steps into health research and advanced statistics. Even if you’re familiar with health research, but advanced statistical methods seem daunting, you’re in the right place. Here, we offer:\nThis hub is a part of an open educational initiative, meaning it’s available to everyone. We hope to uplift the standard of health research methodology through this endeavor."
  },
  {
    "objectID": "index.html#what-we-aim-to-achieve",
    "href": "index.html#what-we-aim-to-achieve",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "What We Aim to Achieve",
    "text": "What We Aim to Achieve\nWe’re on a mission to:\n\nEquip public health learners with hands-on experience.\nTeach the nuances of applying advanced epidemiological methods using real data.\nOffer a unique open textbook that’s enriched with interactive tools and quizzes for a self-paced learning experience."
  },
  {
    "objectID": "index.html#dive-into-our-modules",
    "href": "index.html#dive-into-our-modules",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Dive into Our Modules",
    "text": "Dive into Our Modules\nEmbark on a journey through 10 core learning modules, and one introductory module about R (module 0). Letters in the parentheses (W, A, Q, R, P, D, M, S, G, M, C) are the chapter indicators; these are indicated along with quizzes, R functions, and exercises associated with the corresponding chapters. Only key chapters have exercises.\n\n\n\n\n\n Module \n    Topics.Indicators \n    Descriptions \n  \n\n\n 0 \n    R for Data Wrangling (W) \n    Get to know R. \n  \n\n 1 \n    Accessing (A) Survey Data Resources \n    Understand and source reliable national survey data. \n  \n\n 2 \n    Crafting Analytic Data for Research Questions (Q) \n    Customize data to your research query. \n  \n\n 3 \n    Role (R) of Variables \n    Delve into the concept of confounding and its implications. \n  \n\n 4 \n    Predictive (P) Modelling \n    Introduction to key concepts of prediction modelling. \n  \n\n 5 \n    Complex Survey Data (D) Analysis \n    Handle data sets obtained from complex survey designs. \n  \n\n 6 \n    Missing (M) Data Analysis \n    Understand and tackle missingness in your data. \n  \n\n 7 \n    Propensity Score (S) Analysis \n    Dive deeper into advanced observational data analyses. \n  \n\n 8 \n    Reporting Guideline (G) \n    Guidelines to share your findings. \n  \n\n 9 \n    Machine (M) Learning \n    Introduction to machine learning algorithms, and applications. \n  \n\n 10 \n    Intergrating Machine Learners in Causal (C) Inference \n    Discusses the potential pitfalls and challenges in merging machine learning with causal inference, and a way forward. \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tutorial is designed with a consistent structure across all chapters to provide a cohesive and thorough learning experience. Here’s what you can expect in each chapter:\n\nOverview: The first page of each chapter offers a concise summary that outlines the key learning objectives, topics covered, and what you can expect to gain from the chapter. The overview page will also feature links to the data sources used in the tutorials as well as a form where you can report any bugs or issues you encounter. This helps you quickly grasp the chapter’s essence and set learning expectations.\nTutorial Topics: Immediately following the overview, you’ll find in-depth tutorials that cover each topic in detail. These are designed to provide comprehensive insights and are spread across multiple pages for easier navigation and understanding.\nSummary of R Functions: Each chapter includes a succinct summary of the R functions used in the tutorials. This serves as a quick reference guide for learners to understand the tools they will be applying.\nChapter-Specific Quiz: For those interested in self-assessment, each chapter concludes with an optional quiz. This is a self-paced learning tool to help reinforce the chapter’s key concepts.\nPractice Exercises: Finally, practice exercises are available for selected chapters to help you apply what you’ve learned in a hands-on manner. These exercises are designed to reinforce your understanding and give you practical experience with the chapter’s topics."
  },
  {
    "objectID": "index.html#how-our-content-is-presented",
    "href": "index.html#how-our-content-is-presented",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "How Our Content is Presented",
    "text": "How Our Content is Presented\nAll our resources are hosted on an easy-to-access GitHub page. The format? Engaging text, reproducible software codes, clear analysis outputs, and crisp videos that distill complex topics. And don’t miss our quiz section at the end of each module for a quick self-check on what you’ve learned. This document is created using quarto and R."
  },
  {
    "objectID": "index.html#open-copyright-license",
    "href": "index.html#open-copyright-license",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Open Copyright License",
    "text": "Open Copyright License\nCC-BY"
  },
  {
    "objectID": "index.html#contributor-list",
    "href": "index.html#contributor-list",
    "title": "OER: Advanced Epidemiological Methods",
    "section": "Contributor list",
    "text": "Contributor list\nDive into this captivating content, brought to life with the generous support of the UBC OER Fund Implementation Grant and further supported by UBC SPPH. The foundation of this content traces back to the PI’s work over five years while instructing SPPH 604 (2018-2022). That knowledge have now transformed into an open educational resource, thanks to this grant. Meet the innovative minds behind the grant proposal below.\n\n\n\n\n\n Role \n    Team_Member \n    Affiliation \n  \n\n\n Principal Applicant (PI) \n    Dr. M Ehsan Karim \n    UBC School of Population and Public Health \n  \n\n Co-applicant (Co-I) \n    Dr. Suborna Ahmed \n    UBC Department of Forest Resources Management \n  \n\n Trainee co-applicants \n    Md Belal Hossain \n    UBC School of Population and Public Health \n  \n\n  \n    Fardowsa Yusuf \n    UBC School of Population and Public Health \n  \n\n  \n    Hanna Frank \n    UBC School of Population and Public Health \n  \n\n  \n    Dr. Michael Asamoah-Boaheng \n    UBC Department of Emergency Medicine \n  \n\n  \n    Chuyi (Astra) Zheng \n    UBC Faculty of Arts \n  \n\n\n\n\nAdditional earlier contributors to the course material development, who were not part of this current OER grant, include Derek Ouyang, Kate McLeod (both from UBC School of Population and Public Health), and Mohammad Atiquzzaman (UBC Pharmaceutical Sciences). Numerous pieces of student feedback were also incorporated in order to update the content."
  },
  {
    "objectID": "wrangling.html#r-basics",
    "href": "wrangling.html#r-basics",
    "title": "Data wrangling",
    "section": "R Basics",
    "text": "R Basics\nThis tutorial introduces the basics of R programming. It covers topics such as setting up R and RStudio, using R as a calculator, creating variables, working with vectors, plotting data, and accessing help resources."
  },
  {
    "objectID": "wrangling.html#r-data-types",
    "href": "wrangling.html#r-data-types",
    "title": "Data wrangling",
    "section": "R Data Types",
    "text": "R Data Types\nThis tutorial covers three primary data structures in R: matrices, lists, and data frames. Matrices are two-dimensional arrays with elements of the same type, and their manipulation includes reshaping and combining. Lists in R are versatile collections that can store various R objects, including matrices. Data frames, on the other hand, are akin to matrices but permit columns of diverse data types. The tutorial offers guidance on creating, modifying, and merging data frames and checking their dimensions."
  },
  {
    "objectID": "wrangling.html#automating-tasks",
    "href": "wrangling.html#automating-tasks",
    "title": "Data wrangling",
    "section": "Automating Tasks",
    "text": "Automating Tasks\nMedical data analysis often grapples with vast and intricate data sets. Manual handling isn’t just tedious; it’s error-prone, especially given the critical decisions hinging on the results. This tutorial introduces automation techniques in R, a leading language for statistical analysis. By learning to use loops and functions, you can automate repetitive tasks, minimize errors, and conduct analyses more efficiently. Dive in to enhance your data handling skills."
  },
  {
    "objectID": "wrangling.html#importing-dataset",
    "href": "wrangling.html#importing-dataset",
    "title": "Data wrangling",
    "section": "Importing Dataset",
    "text": "Importing Dataset\nThis tutorial focuses on importing data into R. It demonstrates how to import data from CSV and SAS formats using functions like read.csv and sasxport.get. It also includes examples of loading specific variables, dropping variables, subsetting observations based on certain criteria, and handling missing values."
  },
  {
    "objectID": "wrangling.html#data-manipulation",
    "href": "wrangling.html#data-manipulation",
    "title": "Data wrangling",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nThis tutorial explores various data manipulation techniques in R. It covers topics such as dropping variables from a dataset, keeping specific variables, subsetting observations based on specific criteria, converting variable types (e.g., factors, strings), and handling missing values."
  },
  {
    "objectID": "wrangling.html#import-external-data",
    "href": "wrangling.html#import-external-data",
    "title": "Data wrangling",
    "section": "Import External Data",
    "text": "Import External Data\nThis tutorial provides examples of importing external data into R. It includes specific examples of importing a CSV file (Employee Salaries - 2017 data) and a SAS file (NHANES 2015-2016 data). It also demonstrates how to save a working dataset in different formats, such as CSV and RData."
  },
  {
    "objectID": "wrangling.html#summary-tables",
    "href": "wrangling.html#summary-tables",
    "title": "Data wrangling",
    "section": "Summary Tables",
    "text": "Summary Tables\nThis tutorial emphasizes the importance of data summarization in medical research and epidemiology, specifically how to summarize medical data using R. It demonstrates creating “Table 1”, a typical descriptive statistics table in research papers, with examples that use the built-in R functions and specialized packages to efficiently summarize and stratify data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "wrangling1a.html",
    "href": "wrangling1a.html",
    "title": "R basics",
    "section": "",
    "text": "Important\n\n\n\nShow/hide code:\nOn every page, at the top, you’ll find a </> button. Click it to toggle the visibility of all R code on the page at once. Alternatively, you can click ‘Show the code’ within individual code chunks to view code on a case-by-case basis.\n\n\nStart using R\nTo get started with R, follow these steps:\n\nDownload and Install R: Grab the newest version from the official R website. > Tip: Download from a Comprehensive R Archive Network (CRAN) server near your geographic location.\nDownload and Install RStudio: You can get it from this link. > Note: RStudio serves as an Integrated Development Environment (IDE) offering a user-friendly interface. It facilitates operations such as executing R commands, preserving scripts, inspecting results, managing data, and more.\nBegin with RStudio: Once you open RStudio, delve into using R. For starters, employ the R syntax for script preservation, allowing future code adjustments and additions.\nBasic syntax\n\n\n\n\n\n\nTip\n\n\n\nR, a versatile programming language for statistics and data analysis, can execute numerous tasks. Let’s break down some of the fundamental aspects of R’s syntax.\n\n\n\nUsing R as a Calculator\n\nSimilar to how you’d use a traditional calculator for basic arithmetic operations, R can perform these functions with ease. For instance:\n\nShow the code# Simple arithmetic\n1 + 1\n#> [1] 2\n\n\nThis is a basic addition, resulting in 2.\nA more intricate calculation:\n\nShow the code# Complex calculation involving \n# multiplication, subtraction, division, powers, and square root\n20 * 5 - 10 * (3/4) * (2^3) + sqrt(25)\n#> [1] 45\n\n\nThis demonstrates R’s capability to handle complex arithmetic operations.\n\nVariable Assignment in R\n\nR allows you to store values in variables, acting like labeled containers that can be recalled and manipulated later. For example,\n\nShow the code# Assigning a value of 2 to variable x1\nx1 <- 2\nprint(x1)\n#> [1] 2\n\n\nSimilarly:\n\nShow the codex2 <- 9\nx2\n#> [1] 9\n\n\n\nCreating New Variables Using Existing Ones\n\nYou can combine and manipulate previously assigned variables to create new ones.\n\nShow the code# Using variable x1 \n# to compute its square and assign to y1\ny1 <- x1^2\ny1\n#> [1] 4\n\n\nYou can also use multiple variables in a single expression:\n\nShow the codey2 <- 310 - x1 + 2*x2 - 5*y1^3\ny2\n#> [1] 6\n\n\n\nCreating Functions\n\nFunctions act as reusable blocks of code. Once defined, they can be called multiple times with different arguments. Here’s how to define a function that squares a number:\n\nShow the codez <- function(x) {x^2}\n\n\nR also comes with a plethora of built-in functions. Examples include exp (exponential function) and rnorm (random number generation from a normal distribution).\n\nUtilizing Built-In Functions\n\nFor instance, using the exponential function:\n\nShow the code# Calling functions\nexp(x1)\n#> [1] 7.389056\nlog(exp(x1))\n#> [1] 2\n\n\nThe rnorm function can generate random samples from a normal distribution: below we are generating 10 random sampling from the normal distribution with mean 0 and standard deviation 1:\n\nShow the codernorm(n = 10, mean = 0, sd = 1)\n#>  [1] -1.35075652 -0.50412761 -0.19264235  0.26622497  1.13099668  0.07959930\n#>  [7]  1.00311899  1.21638989 -0.20415432  0.05776501\n\n\nAs random number generation relies on algorithms, results will differ with each execution.\n\nShow the code# Random sampling (again)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.22434788  0.17886145 -1.91964887 -0.10497858 -1.83003488  0.40758183\n#>  [7]  0.31717725 -0.77952024  0.05435223  0.13119455\n\n\nHowever, by setting a seed, we can reproduce identical random results:\n\nShow the code# Random sampling (again, but with a seed)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\n\nShow the code# random sampling (reproducing the same numbers)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#>  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#>  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\nAs we can see, when we set the same seed, we get exactly the same random number. This is very important for reproducing the same results. There are many other pre-exiting functions in R.\n\nSeeking Help in R\n\n\n\n\n\n\n\nTip\n\n\n\nR’s help function, invoked with ?function_name, provides detailed documentation on functions, assisting users with unclear or forgotten arguments:\n\n\n\nShow the code# Searching for help if you know \n# the exact name of the function with a question mark\n?curve\n\n\nBelow is an example of using the pre-exiting function for plotting a curve ranging from -10 to 10.\n\nShow the code# Plotting a function\ncurve(z, from = -10, to = 10, xlab = \"x\", ylab = \"Squared x\")\n\n\n\n\nIf some of the arguments are difficult to remember or what else could be done with that function, we could use the help function. For example, we can simply type help(curve) or ?curve to get help on the curve function:\n\n\n\n\n\n\nTip\n\n\n\nIf you’re uncertain about a function’s precise name, two question marks can assist in the search:\n\n\n\nShow the code# Searching for help if don't know \n# the exact name of the function\n??boxplot\n\n\n\nCreating Vectors\n\nVectors are sequences of data elements of the same basic type. Here are some methods to create them:\n\nShow the code# Creating vectors in different ways\nx3 <- c(1, 2, 3, 4, 5)\nprint(x3)\n#> [1] 1 2 3 4 5\n\nx4 <- 1:7\nprint(x4)\n#> [1] 1 2 3 4 5 6 7\n\nx5 <- seq(from = 0, to = 100, by = 10)\nprint(x5)\n#>  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nx6 <- seq(10, 30, length = 7)\nx6\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n\n\n\nPlotting in R\n\nR provides numerous plotting capabilities. For instance, the plot function can create scatter plots and line graphs:\n\nShow the code# Scatter plot\nplot(x5, type = \"p\", main = \"Scatter plot\")\n\n\n\n\n\nShow the code# Line graph\nplot(x = x6, y = x6^2, type = \"l\", main = \"Line graph\")\n\n\n\n\n\nCharacter Vectors Apart from numeric values, R also allows for character vectors. For example, we can create a sex variable coded as females, males and other.\n\n\nShow the code# Character vector\nsex <- c(\"females\", \"males\", \"other\")\nsex\n#> [1] \"females\" \"males\"   \"other\"\n\n\nTo determine a variable’s type, use the mode function:\n\nShow the code# Check data type\nmode(sex)\n#> [1] \"character\"\n\n\nPackage Management\nPackages in R are collections of functions and datasets developed by the community. They enhance the capability of R by adding new functions for data analysis, visualization, data import, and more. Understanding how to install and load packages is essential for effective R programming.\n\nInstalling Packages from CRAN\n\nThe CRAN is a major source of R packages. You can install them directly from within R using the install.packages() function.\n\nShow the code# Installing the 'ggplot2' package\ninstall.packages(\"ggplot2\")\n\n\n\nLoading a Package\n\nAfter a package is installed, it must be loaded to use its functions. This is done with the library() function.\n\nShow the code# Loading the 'ggplot2' package\nlibrary(ggplot2)\n\n\nYou only need to install a package once, but you’ll need to load it every time you start a new R session and want to use its functions.\n\nUpdating Packages\n\nR packages are frequently updated. To ensure you have the latest version of a package, use the update.packages() function.\n\nShow the code# Updating all installed packages\n# could be time consuming!\nupdate.packages(ask = FALSE)  \n# 'ask = FALSE' updates all without asking for confirmation\n\n\n\nListing Installed Packages\n\nYou can view all the installed packages on your R setup using the installed.packages() function.\n\nShow the code# Listing installed packages\ninstalled.packages()[, \"Package\"]\n\n\n\nRemoving a Package\n\nIf you no longer need a package, it can be removed using the remove.packages() function.\n\nShow the code# Removing the 'ggplot2' package\nremove.packages(\"ggplot2\")\n\n\n\nInstalling Packages from Other Sources\n\nWhile CRAN is the primary source, sometimes you might need to install packages from GitHub or other repositories. The devtools package provides a function for this.\n\nShow the code# Installing devtools first\ninstall.packages(\"devtools\")\n# Loading devtools\nlibrary(devtools)\n# Install a package from GitHub\n# https://github.com/ehsanx/simMSM\ninstall_github(\"ehsanx/simMSM\")\n\n\nWhen you are working on a project, it’s a good practice to list and install required packages at the beginning of your R script.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling1b.html",
    "href": "wrangling1b.html",
    "title": "Data types",
    "section": "",
    "text": "Matrix\n\n\n\n\n\n\nTip\n\n\n\nIn R, matrices are two-dimensional rectangular data sets, which can be created using the matrix() function. It’s essential to remember that all the elements of a matrix must be of the same type, such as all numeric or all character.\n\n\nTo construct a matrix, we often start with a vector and specify how we want to reshape it. For instance:\n\nShow the code# Matrix 1\nx <- 1:10\nmatrix1 <- matrix(x, nrow = 5, ncol = 2, byrow = TRUE)\nmatrix1\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nHere, the vector x contains numbers from 1 to 10. We reshape it into a matrix with 5 rows and 2 columns. The byrow = TRUE argument means the matrix will be filled row-wise, with numbers from the vector.\nConversely, if you want the matrix to be filled column-wise, you’d set byrow = FALSE:\n\nShow the code# matrix 2\nmatrix2 <- matrix(x, nrow = 5, ncol = 2, byrow = FALSE)\nmatrix2\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nYou can also combine or concatenate matrices. cbind() joins matrices by columns while rbind() joins them by rows.\n\nShow the code# Merging 2 matrices\ncbind(matrix1, matrix2)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    1    6\n#> [2,]    3    4    2    7\n#> [3,]    5    6    3    8\n#> [4,]    7    8    4    9\n#> [5,]    9   10    5   10\n\n\n\nShow the code# Appending 2 matrices\nrbind(matrix1, matrix2)\n#>       [,1] [,2]\n#>  [1,]    1    2\n#>  [2,]    3    4\n#>  [3,]    5    6\n#>  [4,]    7    8\n#>  [5,]    9   10\n#>  [6,]    1    6\n#>  [7,]    2    7\n#>  [8,]    3    8\n#>  [9,]    4    9\n#> [10,]    5   10\n\n\nList\n\n\n\n\n\n\nTip\n\n\n\nIn R, lists can be seen as a collection where you can store a variety of different objects under a single name. This includes vectors, matrices, or even other lists. It’s very versatile because its components can be of any type of R object.\n\n\nFor instance:\n\nShow the code# List of 2 matrices\nlist1 <- list(matrix1, matrix2)\nlist1\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n\n\nLists can also be expanded to include multiple items:\n\nShow the codex6 <- seq(10, 30, length = 7)\nsex <- c(\"females\", \"males\", \"other\")\n# Expanding list to include more items\nlist2 <- list(list1, x6, sex, matrix1)\nlist2 \n#> [[1]]\n#> [[1]][[1]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n#> \n#> [[1]][[2]]\n#>      [,1] [,2]\n#> [1,]    1    6\n#> [2,]    2    7\n#> [3,]    3    8\n#> [4,]    4    9\n#> [5,]    5   10\n#> \n#> \n#> [[2]]\n#> [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n#> \n#> [[3]]\n#> [1] \"females\" \"males\"   \"other\"  \n#> \n#> [[4]]\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n#> [3,]    5    6\n#> [4,]    7    8\n#> [5,]    9   10\n\n\nCombining different types of data into a single matrix converts everything to a character type:\n\nShow the code# A matrix with numeric and character variables\nid <- c(1, 2)\nscore <- c(85, 85)\nsex <- c(\"M\", \"F\")\nnew.matrix <- cbind(id, score, sex)\nnew.matrix\n#>      id  score sex\n#> [1,] \"1\" \"85\"  \"M\"\n#> [2,] \"2\" \"85\"  \"F\"\n\n\nTo check the type of data in your matrix:\n\nShow the codemode(new.matrix)\n#> [1] \"character\"\n\n\nData frame\n\n\n\n\n\n\nTip\n\n\n\nAs we can see combining both numeric and character variables into a matrix ended up with a matrix of character values. To keep the numeric variables as numeric and character variables as character, we can use the data.frame function.\n\n\n\nCreating a data frame\n\n\n\nA data frame is similar to a matrix but allows for columns of different types (numeric, character, factor, etc.). It’s a standard format for storing data sets in R.\n\nShow the codedf <- data.frame(id, score, sex)\ndf\n\n\n\n  \n\n\n\nTo check the mode or type of your data frame:\n\nShow the codemode(df)\n#> [1] \"list\"\n\n\n\nExtract elements\n\nData frames allow easy extraction and modification of specific elements. For example, we can extract the values on the first row and first column as follow:\n\nShow the codedf[1,1]\n#> [1] 1\n\n\nSimilarly, the first column can be extracted as follows:\n\nShow the codedf[,1]\n#> [1] 1 2\n\n\nThe first row can be extracted as follows:\n\nShow the codedf[1,]\n\n\n\n  \n\n\n\n\nModifying values\n\nWe can edit the values in the data frame as well. For example, we can change the score from 85 to 90 for the id 1:\n\nShow the codedf$score[df$id == 1] <- 90\ndf\n\n\n\n  \n\n\n\nWe can also change the name of the variables/columns:\n\nShow the codecolnames(df) <- c(\"Studyid\", \"Grade\", \"Sex\")\ndf\n\n\n\n  \n\n\n\n\nCombining data frames\n\nWe can also merge another data frame with the same variables using the rbind function:\n\nShow the code# Create a new dataset\ndf2 <- data.frame(Studyid = c(10, 15, 50), Grade = c(75, 90, 65), Sex = c(\"F\", \"M\", \"M\"))\n\n# Combining two data frames\ndf.new <- rbind(df, df2)\n\n# Print the first 6 rows\nhead(df.new)\n\n\n\n  \n\n\n\n\nChecking the dimensions\n\nTo see the dimension of the data frame (i.e., number of rows and columns), we can use the dim function:\n\nShow the codedim(df.new)\n#> [1] 5 3\n\n\nAs we can see, we have 5 rows and 3 columns. We can use the nrow and ncol functions respectively for the same output:\n\nShow the codenrow(df.new)\n#> [1] 5\nncol(df.new)\n#> [1] 3"
  },
  {
    "objectID": "wrangling1c.html",
    "href": "wrangling1c.html",
    "title": "Automating tasks",
    "section": "",
    "text": "Repeating a task\n\n\n\n\n\n\nTip\n\n\n\nThe for loop is a control flow statement in R that lets you repeat a particular task multiple times. This repetition is based on a sequence of numbers or values in a vector.\n\n\nConsider a simple real-life analogy: Imagine you are filling water in 10 bottles, one by one. Instead of doing it manually 10 times, you can set a machine to do it in a loop until all 10 bottles are filled.\n\nExample 1\n\n\nShow the code# Looping and adding\nk <- 0\nfor (i in 1:10){\n  k <- k + 5\n  print(k)\n}\n#> [1] 5\n#> [1] 10\n#> [1] 15\n#> [1] 20\n#> [1] 25\n#> [1] 30\n#> [1] 35\n#> [1] 40\n#> [1] 45\n#> [1] 50\n\n\nHere, you’re initiating a counter k at 0. With each iteration of the loop (i.e., every time it “runs”), 5 is added to k. After 10 cycles, the loop will stop, but not before printing k in each cycle.\n\nExample 2\n\nWe create a variable x5 containing the values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. Let us print the first 5 values using the for loop function:\n\nShow the codex5 <- seq(from = 0, to = 100, by = 10)\n# Looping through a vector\nk <- 1:5\nfor (ii in k){\n  print(x5[ii])\n}\n#> [1] 0\n#> [1] 10\n#> [1] 20\n#> [1] 30\n#> [1] 40\n\n\nThis loop cycles through the first five values of a previously created variable x5 and prints them. Each value printed corresponds to the positions 1 to 5 in x5.\n\nExample 3\n\nLet us use the for loop in a more complicated scenario. First, we create a vector of numeric values and square it:\n\nShow the code# Create a vector\nk <- c(1, 3, 6, 2, 0)\nk^2\n#> [1]  1  9 36  4  0\n\n\nThis is just squaring each value in the vector k.\n\nExample 4\n\nWe create the same vector of square values using the for loop function. To do so, (i) we create a null object, (ii) use the loop for each of the elements in the vector (k), (iii) square each of the elements, and (iv) store each of the elements of the new vector. In the example below, the length of k is 5, and the loop will run from the first to the fifth element of k. Also, k.sq[1] is the first stored value for squared-k, and k.sq[2] is the second stored value for squared-k, and so on.\n\nShow the code# Looping through a vector with function\nk.sq <- NULL\nfor (i in 1:length(k)){\n  k.sq[i] <- k[i]^2\n}\n\n# Print the values\nk.sq\n#> [1]  1  9 36  4  0\n\n\nHere, we achieve the same result as the third example but use a for loop. We prepare an empty object k.sq and then use the loop to square each value in k, storing the result in k.sq.\n\nExample 5\n\n\nShow the codedf.new <- data.frame(\n  Studyid = c(1, 2, 10, 15, 50),\n  Grade = c(90, 85, 75, 90, 65),\n  Sex = c('M', 'F', 'F', 'M', 'M')\n)\n# Looping through a data frame\nfor (i in 1:nrow(df.new)){\n  print(df.new[i,\"Sex\"])\n}\n#> [1] \"M\"\n#> [1] \"F\"\n#> [1] \"F\"\n#> [1] \"M\"\n#> [1] \"M\"\n\n\nThis loop prints the “Sex” column value for each row in the df.new data frame.\nFunctions\n\n\n\n\n\n\nTip\n\n\n\nA function in R is a piece of code that can take inputs, process them, and return an output. There are functions built into R, like mean(), which calculates the average of a set of numbers.\n\n\n\nBuilt-in function\n\n\nShow the code# Calculating a mean from a vector\nVector <- 1:100\nmean(Vector)\n#> [1] 50.5\n\n\nHere, we’re using the built-in mean() function to find the average of numbers from 1 to 100.\n\nCustom-made function\n\nTo understand how functions work, sometimes it’s helpful to build our own. Now we will create our own function to calculate the mean, where we will use the following equation to calculate it:\n\\(\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n},\\)\nwhere \\(x_1\\), \\(x_2\\),…, \\(x_n\\) are the values in the vector and \\(n\\) is the sample size. Let us create the function for calculation the mean:\nThis function, mean.own, calculates the average. We add up all the numbers in a vector (Sum <- sum(x)) and divide by the number of items in that vector (n <- length(x)). The result is then returned.\n\nShow the codemean.own <- function(x){\n  Sum <- sum(x)\n  n <- length(x)\n  return(Sum/n)\n}\n\n\nBy using our custom-made function, we calculate the mean of numbers from 1 to 100, getting the same result as the built-in mean() function.\n\nShow the codemean.own(Vector)\n#> [1] 50.5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "wrangling2.html",
    "href": "wrangling2.html",
    "title": "Importing dataset",
    "section": "",
    "text": "Introduction to Data Importing\nBefore analyzing data in R, one of the first steps you’ll typically undertake is importing your dataset. R provides numerous methods to do this, depending on the format of your dataset.\nDatasets come in a variety of file formats, with .csv (Comma-Separated Values) and .txt (Text file) being among the most common. While R’s interface offers manual ways to load these datasets, knowing how to code this step ensures better reproducibility and automation.\nImporting .txt files\nA .txt data file can be imported using the read.table function. As an example, consider you have a dataset named grade in the specified path.\nLet’s briefly glance at the file without concerning ourselves with its formatting.\n\nShow the code# Read and print the content of the TXT file\ncontent <- readLines(\"Data/wrangling/grade.txt\")\ncat(content, sep = \"\\n\")\n#> Studyid Grade Sex\n#> 1    90   M\n#> 2    85   F\n#> 10    75   F\n#> 15    90   M\n#> 50    65   M\n\n\nUsing the read.table function, you can load this dataset in R properly. It’s important to specify header = TRUE if the first row of your dataset contains variable names.\n\nTip: Always ensure the header argument matches the structure of your dataset. If your dataset contains variable names, set header = TRUE.\n\n\nShow the code## Read a text dataset\ngrade <- read.table(\"Data/wrangling/grade.txt\", header = TRUE, sep = \"\\t\", quote = \"\\\"\")\n# Display the first few rows of the dataset\nhead(grade)\n\n\n\n  \n\n\n\nImporting .csv files\nSimilarly, .csv files can be loaded using the read.csv function. Here’s how you can load a .csv dataset named mpg:\n\nShow the code## Read a csv dataset\nmpg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n# Display the first few rows of the dataset\nhead(mpg)\n\n\n\n  \n\n\n\nWhile we’ve discussed two popular data formats, R can handle a plethora of other formats. For further details, refer to Quick-R (2023). Notably, some datasets come built-in with R packages, like the mpg dataset in the ggplot2 package. To load such a dataset:\n\nShow the codedata(mpg, package = \"ggplot2\")\nhead(mpg)\n\n\n\n  \n\n\n\nTo understand more about the variables and the dataset’s structure, you can consult the documentation:\n\nShow the code?mpg\n\n\nData Screening and Understanding Your Dataset\ndim(), nrow(), ncol(), and str() are incredibly handy functions when initially exploring your dataset.\nOnce your data is in R, the next logical step is to get familiar with it. Knowing the dimensions of your dataset, types of variables, and the first few entries can give you a quick sense of what you’re dealing with.\nFor instance, str (short for structure) is a concise way to display information about your data. It reveals the type of each variable, the first few entries, and the total number of observations:\n\nShow the codestr(mpg)\n#> tibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n#>  $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n#>  $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n#>  $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n#>  $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n#>  $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n#>  $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n#>  $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n#>  $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n#>  $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n#>  $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n#>  $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nIn summary, becoming proficient in data importing and initial screening is a fundamental step in any data analysis process in R. It ensures that subsequent stages of data manipulation and analysis are based on a clear understanding of the dataset at hand.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\nReferences\n\n\n\n\nQuick-R. 2023. “Importing Data.” https://www.statmethods.net/input/importingdata.html."
  },
  {
    "objectID": "wrangling3.html",
    "href": "wrangling3.html",
    "title": "Data manipulation",
    "section": "",
    "text": "Data manipulation is a foundational skill for data analysis. This guide introduces common methods for subsetting datasets, handling variable types, creating summary tables, and dealing with missing values using R.\nLoad dataset\nUnderstanding the dataset’s structure is the first step in data manipulation. Here, we’re using the mpg dataset, which provides information on various car models:\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\n\nSubset\nOften, you’ll need to subset your data for analysis. Here, we’ll explore different methods to both drop unwanted variables and keep desired observations.\nDrop variables\nSometimes, only part of the variables will be used in your analysis. Therefore, you may want to drop the variables you do not need. There are multiple ways to drop variables from a dataset. Below are two examples without using any package and using the dplyr package.\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[, c(the columns you want to KEEP)]\n\n\nSay, we want to keep only three variables in the mpg dataset: manufacturer, model and cyl. For Option 1 (without package), we can use the following R codes to keep these three variables:\n\nShow the codempg1 <- mpg[, c(\"manufacturer\", \"model\", \"cyl\")]\nhead(mpg1)\n\n\n\n  \n\n\n\nHere mpg1 is a new dataset containing only three variables (manufacturer, model and cyl).\n\n\n\n\n\n\nTip\n\n\n\nOption 2: use select in dplyr\nselect(dataset.name, …(columns names you want to KEEP))\n\n\nFor Option 2, the dplyr package offers the select function, which provides a more intuitive way to subset data.\n\nShow the codempg2 <- select(mpg, c(\"manufacturer\", \"model\", \"cyl\"))\nhead(mpg2)\n\n\n\n  \n\n\n\nWe can also exclude any variables from the dataset by using the minus (-) sign with the select function. For example, we we want to drop trans, drv, and cty from the mpg dataset, we can use the following codes:\n\nShow the codempg3 <- select(mpg, -c(\"trans\", \"drv\", \"cty\"))\nhead(mpg3)\n\n\n\n  \n\n\n\nThis mpg3 is a new dataset from mpg after dropping three variables (trans, drv, and cty).\nKeep observations\nIt often happens that we only want to investigate a subset of a population which only requires a subset of our dataset. In this case, we need to subset the dataset to meet certain requirements. Again, there are multiple ways to do this task. Below is an example without a package and with the dplyr package:\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[the rows you want to KEEP, ]\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 2: No package needed\nsubset(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 3: use select in dplyr\nfilter(dataset.name, …(logical tests))\n\n\n\n\n\n\n\n\nTip\n\n\n\nCommon logical tests are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n X <(=) Y \n    Smaller (equal) than \n  \n\n X >(=) Y \n    Larger (equal) than \n  \n\n X == Y \n    Equal to \n  \n\n X != Y \n    Not equal to \n  \n\n is.na(X) \n    is NA/missing? \n  \n\n\n\n\n\n\nSay, we want to keep the observations for which cars are manufactured in 2008. We can use the following R codes to do it:\n\nShow the codempg4 <- mpg[mpg$year == \"2008\",] # Option 1\nhead(mpg4)\n\n\n\n  \n\n\n\nThe following codes with the subset and filter function will do the same:\n\nShow the codempg5 <- subset(mpg, year == \"2008\") # Option 1\nhead(mpg5)\n\n\n\n  \n\n\n\n\nShow the codempg6 <- filter(mpg, year == \"2008\") # Option 3\nhead(mpg6)\n\n\n\n  \n\n\n\nThe filter function can also work when you have multiple criteria (i.e., multiple logical tests) to satisfy. Here, we need Boolean operators to connect different logical tests.\n\n\n\n\n\n\nTip\n\n\n\nCommon boolean operators are:\n\n\n\n\n\n Syntax \n    Meaning \n  \n\n\n & \n    and \n  \n\n | \n    or \n  \n\n ! \n    not \n  \n\n == \n    equals to \n  \n\n != \n    not equal to \n  \n\n > \n    greater than \n  \n\n < \n    less than \n  \n\n >= \n    greater than or equal to \n  \n\n <= \n    less than or equal to \n  \n\n\n\n\n\n\nSay, we want to keep the observations for 6 and 8 cylinders (cyl) and engine displacement (displ) greater than or equal to 4 litres. We can use the following codes to do the task:\n\nShow the codempg7 <- filter(mpg, cyl %in% c(\"6\",\"8\") & displ >= 4)\nhead(mpg7)\n\n\n\n  \n\n\n\n\n\nThe %in% operator is used to determine whether the values of the first argument are present in the second argument.\nHandling Variable Types\n\n\n\n\n\n\nTip\n\n\n\nMost common types of variable in R are\n\nnumbers,\nfactors and\nstrings(or character).\n\nUnderstanding and manipulating these types are crucial for data analysis.\n\n\n\nIdentifying Variable Type\n\nWhen we analyze the data, we usually just deal with numbers and factors. If there are variables are strings, we could convert them to factors using as.factors(variable.name)\n\nShow the codemode(mpg$trans)\n#> [1] \"character\"\n\n\n\nShow the codestr(mpg$trans)\n#>  chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" \"auto(l5)\" ...\n\n\n\nConverting Characters to Factors\n\nSometimes, it’s necessary to treat text data as categorical by converting them into factors. as.numeric() converts other types of variables to numbers. For a factor variable, we usually we want to access the categories (or levels) it has. We can use a build-in function to explore: levels(variable.name)\n\nShow the code# no levels for character\nlevels(mpg$trans)\n#> NULL\n\n\n\nShow the code## Ex check how many different trans the dataset has\nmpg$trans <- as.factor(mpg$trans)\nlevels(mpg$trans)\n#>  [1] \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"   \"auto(l6)\"  \n#>  [6] \"auto(s4)\"   \"auto(s5)\"   \"auto(s6)\"   \"manual(m5)\" \"manual(m6)\"\n\n\nThe levels usually will be ordered alphabetically. The first level is called “baseline”. However, the users may/may not want to keep this baseline and want to relevel/change the reference group. We can do it using the relevel function:\nrelevel(variable.name, ref=)\n\nShow the codempg$trans <- relevel(mpg$trans, ref = \"auto(s6)\")\nlevels(mpg$trans)\n#>  [1] \"auto(s6)\"   \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"  \n#>  [6] \"auto(l6)\"   \"auto(s4)\"   \"auto(s5)\"   \"manual(m5)\" \"manual(m6)\"\nnlevels(mpg$trans)\n#> [1] 10\n\n\nfactor function can be also used to combine factors. If the user want to combine multiple factors to one factors\n\nShow the code## EX re-group trans to \"auto\" and \"manual\"\nlevels(mpg$trans) <- list(auto = c(\"auto(av)\", \"auto(l3)\", \"auto(l4)\", \"auto(l5)\", \"auto(l6)\", \n                                   \"auto(s4)\", \"auto(s5)\", \"auto(s6)\"), \n                          manual = c(\"manual(m5)\", \"manual(m6)\"))\nlevels(mpg$trans)\n#> [1] \"auto\"   \"manual\"\n\n\nYou can also change the order of all factors using the following code: factor(variable.name, levels = c(“new order”))\n\nShow the code## EX. Change the order of trans to manual\nmpg$trans <- factor(mpg$trans, levels = c(\"manual\", \"auto\"))\nlevels(mpg$trans)\n#> [1] \"manual\" \"auto\"\n\n\n\n\nIn R, the use of factors with multiple levels is primarily a memory optimization strategy. While users may not directly see this, R assigns internal numerical identifiers to each level, which is a more memory-efficient way of handling such data. Unlike some other software packages that generate multiple dummy variables to represent a single variable, R’s approach is generally more resource-efficient.\nConvert continuous variables to categorical variables\n\n\n\n\n\n\nTip\n\n\n\nifelse, cut, recode all are helpful functions to convert numerical variables to categorical variables.\n\n\nLet’s see the summary of the cty variable first.\n\nShow the codesummary(mpg$cty)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    9.00   14.00   17.00   16.86   19.00   35.00\n\n\nsay, we may want to change continuous ‘cty’ into groups 0-14, 15-18, and 18-40. Below is an example with the cut function.\n\nShow the code## EX. change the cty into two categories (0,14], (14,18] and (18,40]\nmpg$cty.num <- cut(mpg$cty, c(0, 14, 18, 40), right = TRUE)\ntable(mpg$cty.num)\n#> \n#>  (0,14] (14,18] (18,40] \n#>      73      85      76\n\n\n\nShow the code## Try this: do you see a difference?: [0,14), [14,18) and [18,40)\nmpg$cty.num2 <- cut(mpg$cty, c(0, 14, 18, 40), right = FALSE)\ntable(mpg$cty.num2)\n#> \n#>  [0,14) [14,18) [18,40) \n#>      54      78     102\n\n\n\n\n] stands for closed interval, i.e., right = TRUE. On the other hand, ) means open interval. Hence, there will be a huge difference when setting right = TRUE vs. right = FALSE\nMissing value\n\n\n\n\n\n\nTip\n\n\n\nIncomplete datasets can distort analysis. Identifying and managing these missing values is thus crucial.\n\n\nWe can check how many missing values we have by: table(is.na(variable.name))\nLet’s us check whether the cty variable contains any missing values:\n\nShow the codetable(is.na(mpg$cty))\n#> \n#> FALSE \n#>   234\n\n\nIf you want to return all non-missing values, i.e., complete case values: na.omit(variable.name). For more extensive methods on handling missing values, see subsequent tutorials."
  },
  {
    "objectID": "wrangling4.html",
    "href": "wrangling4.html",
    "title": "Import external data",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(dplyr)\nrequire(Hmisc)\n\n\nWhen dealing with data analysis in R, it’s common to need to import external data. This tutorial will walk you through importing data in different formats.\nCSV format data\nCSV stands for “Comma-Separated Values” and it’s a widely used format for data. We’ll be looking at the “Employee Salaries - 2017” dataset, which contains salary information for permanent employees of Montgomery County in 2017.\n\n\nEmployee Salaries - 2017 data\n\n\n\n\n\n\nTip\n\n\n\nWe’ll be loading the Employee_Salaries_-_2017.csv dataset into R from its saved location at Data/wrangling/. Do note, the directory path might vary for you based on where you’ve stored the downloaded data.\n\n\n\nShow the codedata.download <- read.csv(\"Data/wrangling/Employee_Salaries_-_2017.csv\")\n\n\nHere, the read.csv function reads the data from the CSV file and stores it in a variable called data.download.\nTo understand the structure of our dataset, We can see the number of rows and columns and the names of the columns/variables as follows:\n\nShow the codedim(data.download) # check dimension / row / column numbers\n#> [1] 9398   12\nnrow(data.download) # check row numbers\n#> [1] 9398\nnames(data.download) # check column names\n#>  [1] \"Full.Name\"                \"Gender\"                  \n#>  [3] \"Current.Annual.Salary\"    \"X2017.Gross.Pay.Received\"\n#>  [5] \"X2017.Overtime.Pay\"       \"Department\"              \n#>  [7] \"Department.Name\"          \"Division\"                \n#>  [9] \"Assignment.Category\"      \"Employee.Position.Title\" \n#> [11] \"Position.Under.Filled\"    \"Date.First.Hired\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nhead shows the first 6 elements of an object, giving you a sneak peek into the data you’re dealing with, while tail shows the last 6 elements.\n\n\nWe can see the first see six rows of the dataset as follows:\n\nShow the codehead(data.download)\n\n\n\n  \n\n\n\nNext, for learning purposes, let’s artificially assign all male genders in our dataset as missing:\n\nShow the code# Assigning male gender as missing\ndata.download$Gender[data.download$Gender == \"M\"] <- NA\nhead(data.download)\n\n\n\n  \n\n\n\nThis chunk sets the Gender column’s value to NA (missing) wherever the gender is “M”. This is a form of data manipulation, sometimes used to handle missing or incorrect data. If you want to work with datasets that exclude any missing values:\n\n\n\n\n\n\nTip\n\n\n\nna.omit and complete.cases are useful functions to to create datasets with non-NA values\n\n\n\nShow the code# deleting/dropping missing components\ndata.download2 <- na.omit(data.download)\nhead(data.download2)\n\n\n\n  \n\n\nShow the codedim(data.download2)\n#> [1] 3806   12\n\n\nHere, na.omit is used to remove rows with any missing values. This can be essential when preparing data for certain analyses.\nAlternatively, we could have selected only females to drop all males:\n\nShow the codedata.download3 <- filter(data.download, Gender != \"M\")\nhead(data.download3)\n\n\n\n  \n\n\n\nAnd to check the size of this new dataset:\n\nShow the code# new dimension / row / column numbers\ndim(data.download3)\n#> [1] 3806   12\n\n\nSAS format data\n\n\n\n\n\n\nTip\n\n\n\nSAS is another data format, commonly used in professional statistics and analytics.\n\n\nLet’s explore importing a SAS dataset. We download a SAS formatted dataset from the CDC website.\n\nShow the codeNHANES1516data <- sasxport.get(\"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\")\n#> Processing SAS dataset DEMO_I     ..\ndim(NHANES1516data) # check dimension / row / column numbers\n#> [1] 9971   47\nnrow(NHANES1516data) # check row numbers\n#> [1] 9971\nnames(NHANES1516data)[1:10] # check first 10 names\n#>  [1] \"seqn\"     \"sddsrvyr\" \"ridstatr\" \"riagendr\" \"ridageyr\" \"ridagemn\"\n#>  [7] \"ridreth1\" \"ridreth3\" \"ridexmon\" \"ridexagm\"\n\n\nThe sasxport.get function retrieves the SAS dataset. The following lines, just like before, help understand its structure.\nTo analyze some of the data:\n\nShow the codetable(NHANES1516data$riagendr) # tabulating gender variable\n#> \n#>    1    2 \n#> 4892 5079\n\n\n\n\nVerify these numbers from CDC website\nThis code creates a frequency table of the riagendr variable, which represents gender.\nSaving working dataset\n\n\n\n\n\n\nTip\n\n\n\nOnce you’ve made modifications or conducted some preliminary analysis, it’s important to save your dataset. We can save the dataset in a different format, e.g., CSV, txt, or even R, SAS or other formats.\n\n\nWe can save our working dataset in different formats. Say, we want to save our NHANES1516data dataset in csv format. We can use the write.csv() command:\n\nShow the codewrite.csv(NHANES1516data, \"Data/wrangling/NHANES1516.csv\", row.names = FALSE)\n\n\nWe can also save the dataset in R format:\n\nShow the codesave(NHANES1516data, file = \"Data/wrangling/NHANES1516.RData\")"
  },
  {
    "objectID": "wrangling5.html",
    "href": "wrangling5.html",
    "title": "Summary tables",
    "section": "",
    "text": "Medical research and epidemiology often involve large, complex datasets. Data summarization is a vital step that transforms these vast datasets into concise, understandable insights. In medical contexts, these summaries can highlight patterns, indicate data inconsistencies, and guide further research. This tutorial will teach you how to use R to efficiently summarize medical data.\nIn epidemiology and medical research, “Table 1” typically refers to the first table in a research paper or report that provides descriptive statistics of the study population. It offers a snapshot of the baseline characteristics of the study groups, whether in a cohort study, clinical trial, or any other study design.\n\nShow the codempg <- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n## Ex create a summary table between manufacturer and drv\ntable(mpg$drv, mpg$manufacturer)\n#>    \n#>     audi chevrolet dodge ford honda hyundai jeep land rover lincoln mercury\n#>   4   11         4    26   13     0       0    8          4       0       4\n#>   f    7         5    11    0     9      14    0          0       0       0\n#>   r    0        10     0   12     0       0    0          0       3       0\n#>    \n#>     nissan pontiac subaru toyota volkswagen\n#>   4      4       0     14     15          0\n#>   f      9       5      0     19         27\n#>   r      0       0      0      0          0\n\n\nThe first line reads a CSV file. It uses the table() function to generate a contingency table (cross-tabulation) between two categorical variables: drv (drive) and manufacturer. It essentially counts how many times each combination of drv and manufacturer appears in the dataset.\n\nShow the code## Get the percentage summary using prop.table\nprop.table(table(mpg$drv, mpg$manufacturer), margin = 2)\n#>    \n#>          audi chevrolet     dodge      ford     honda   hyundai      jeep\n#>   4 0.6111111 0.2105263 0.7027027 0.5200000 0.0000000 0.0000000 1.0000000\n#>   f 0.3888889 0.2631579 0.2972973 0.0000000 1.0000000 1.0000000 0.0000000\n#>   r 0.0000000 0.5263158 0.0000000 0.4800000 0.0000000 0.0000000 0.0000000\n#>    \n#>     land rover   lincoln   mercury    nissan   pontiac    subaru    toyota\n#>   4  1.0000000 0.0000000 1.0000000 0.3076923 0.0000000 1.0000000 0.4411765\n#>   f  0.0000000 0.0000000 0.0000000 0.6923077 1.0000000 0.0000000 0.5588235\n#>   r  0.0000000 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>    \n#>     volkswagen\n#>   4  0.0000000\n#>   f  1.0000000\n#>   r  0.0000000\n## margin = 1 sum across row, 2 across col\n\n\nThis code calculates the column-wise proportion (as percentages) for each combination of drv and manufacturer. The prop.table() function is used to compute the proportions. The margin = 2 argument indicates that the proportions are to be computed across columns (margin = 1 would compute them across rows).\ntableone package\n\n\n\n\n\n\nTip\n\n\n\nCreateTableOne function from tableone package could be a very useful function to see the summary table. Type ?tableone::CreateTableOne to see for more details.\n\n\nThis section introduces the tableone package, which offers the CreateTableOne function. This function helps in creating “Table 1” type summary tables, commonly used in epidemiological studies.\n\nShow the coderequire(tableone)\n#> Loading required package: tableone\nCreateTableOne(vars = c(\"cyl\", \"drv\", \"hwy\", \"cty\"), data = mpg, \n               strata = \"trans\", includeNA = TRUE, test = FALSE)\n#>                  Stratified by trans\n#>                   auto(av)       auto(l3)       auto(l4)      auto(l5)     \n#>   n                   5              2             83            39        \n#>   cyl (mean (SD))  5.20 (1.10)    4.00 (0.00)    6.14 (1.62)   6.56 (1.45) \n#>   drv (%)                                                                  \n#>      4                0 (  0.0)      0 (  0.0)     34 (41.0)     29 (74.4) \n#>      f                5 (100.0)      2 (100.0)     37 (44.6)      8 (20.5) \n#>      r                0 (  0.0)      0 (  0.0)     12 (14.5)      2 ( 5.1) \n#>   hwy (mean (SD)) 27.80 (2.59)   27.00 (4.24)   21.96 (5.64)  20.72 (6.04) \n#>   cty (mean (SD)) 20.00 (2.00)   21.00 (4.24)   15.94 (3.98)  14.72 (3.49) \n#>                  Stratified by trans\n#>                   auto(l6)      auto(s4)      auto(s5)      auto(s6)     \n#>   n                   6             3             3            16        \n#>   cyl (mean (SD))  7.33 (1.03)   5.33 (2.31)   6.00 (2.00)   6.00 (1.59) \n#>   drv (%)                                                                \n#>      4                2 (33.3)      2 (66.7)      1 (33.3)      7 (43.8) \n#>      f                2 (33.3)      1 (33.3)      2 (66.7)      8 (50.0) \n#>      r                2 (33.3)      0 ( 0.0)      0 ( 0.0)      1 ( 6.2) \n#>   hwy (mean (SD)) 20.00 (2.37)  25.67 (1.15)  25.33 (6.66)  25.19 (3.99) \n#>   cty (mean (SD)) 13.67 (1.86)  18.67 (2.31)  17.33 (5.03)  17.38 (3.22) \n#>                  Stratified by trans\n#>                   manual(m5)    manual(m6)   \n#>   n                  58            19        \n#>   cyl (mean (SD))  5.00 (1.30)   6.00 (1.76) \n#>   drv (%)                                    \n#>      4               21 (36.2)      7 (36.8) \n#>      f               33 (56.9)      8 (42.1) \n#>      r                4 ( 6.9)      4 (21.1) \n#>   hwy (mean (SD)) 26.29 (5.99)  24.21 (5.75) \n#>   cty (mean (SD)) 19.26 (4.56)  16.89 (3.83)\n\n\nThe CreateTableOne function is used to create a summary table for the variables cyl, drv, hwy, and cty from the mpg dataset. The strata = trans argument means that the summary is stratified by the trans variable. The includeNA = TRUE argument means that missing values (NAs) are included in the summary. The test = FALSE argument indicates that no statistical tests should be applied to the data (often tests are used to compare groups in the table).\ntable1 package\nThis section introduces another package, table1, which can also be used to create “Table 1” type summary tables.\n\nShow the coderequire(table1)\n#> Loading required package: table1\n#> \n#> Attaching package: 'table1'\n#> The following objects are masked from 'package:base':\n#> \n#>     units, units<-\ntable1(~ cyl + drv + hwy + cty | trans, data=mpg)\n\n\n\n\n\nauto(av)(N=5)\nauto(l3)(N=2)\nauto(l4)(N=83)\nauto(l5)(N=39)\nauto(l6)(N=6)\nauto(s4)(N=3)\nauto(s5)(N=3)\nauto(s6)(N=16)\nmanual(m5)(N=58)\nmanual(m6)(N=19)\nOverall(N=234)\n\n\n\ncyl\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n5.20 (1.10)\n4.00 (0)\n6.14 (1.62)\n6.56 (1.45)\n7.33 (1.03)\n5.33 (2.31)\n6.00 (2.00)\n6.00 (1.59)\n5.00 (1.30)\n6.00 (1.76)\n5.89 (1.61)\n\n\nMedian [Min, Max]\n6.00 [4.00, 6.00]\n4.00 [4.00, 4.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n8.00 [6.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n\n\ndrv\n\n\n\n\n\n\n\n\n\n\n\n\n\nf\n5 (100%)\n2 (100%)\n37 (44.6%)\n8 (20.5%)\n2 (33.3%)\n1 (33.3%)\n2 (66.7%)\n8 (50.0%)\n33 (56.9%)\n8 (42.1%)\n106 (45.3%)\n\n\n4\n0 (0%)\n0 (0%)\n34 (41.0%)\n29 (74.4%)\n2 (33.3%)\n2 (66.7%)\n1 (33.3%)\n7 (43.8%)\n21 (36.2%)\n7 (36.8%)\n103 (44.0%)\n\n\nr\n0 (0%)\n0 (0%)\n12 (14.5%)\n2 (5.1%)\n2 (33.3%)\n0 (0%)\n0 (0%)\n1 (6.3%)\n4 (6.9%)\n4 (21.1%)\n25 (10.7%)\n\n\nhwy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n27.8 (2.59)\n27.0 (4.24)\n22.0 (5.64)\n20.7 (6.04)\n20.0 (2.37)\n25.7 (1.15)\n25.3 (6.66)\n25.2 (3.99)\n26.3 (5.99)\n24.2 (5.75)\n23.4 (5.95)\n\n\nMedian [Min, Max]\n27.0 [25.0, 31.0]\n27.0 [24.0, 30.0]\n22.0 [14.0, 41.0]\n19.0 [12.0, 36.0]\n19.0 [18.0, 23.0]\n25.0 [25.0, 27.0]\n27.0 [18.0, 31.0]\n26.0 [18.0, 29.0]\n26.0 [16.0, 44.0]\n26.0 [12.0, 32.0]\n24.0 [12.0, 44.0]\n\n\ncty\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n20.0 (2.00)\n21.0 (4.24)\n15.9 (3.98)\n14.7 (3.49)\n13.7 (1.86)\n18.7 (2.31)\n17.3 (5.03)\n17.4 (3.22)\n19.3 (4.56)\n16.9 (3.83)\n16.9 (4.26)\n\n\nMedian [Min, Max]\n19.0 [18.0, 23.0]\n21.0 [18.0, 24.0]\n16.0 [11.0, 29.0]\n14.0 [9.00, 25.0]\n13.0 [12.0, 16.0]\n20.0 [16.0, 20.0]\n18.0 [12.0, 22.0]\n17.0 [12.0, 22.0]\n19.0 [11.0, 35.0]\n16.0 [9.00, 23.0]\n17.0 [9.00, 35.0]\n\n\n\n\n\nThe table1() function is used to generate a summary table for the specified variables. The formula-like syntax (~ cyl + drv + hwy + cty | trans) indicates that the summary should be stratified by the trans variable."
  },
  {
    "objectID": "wranglingF.html",
    "href": "wranglingF.html",
    "title": "R Functions (W)",
    "section": "",
    "text": "This review page provides an extensive list of R functions tailored for data wrangling tasks that we have used in this chapter. Each function is systematically described, highlighting its primary package source and its specific utility.\nTo learn more about these functions, readers can:\n\nUse R’s Built-in Help System: For each function, access its documentation by prefixing the function name with a question mark in the R console, e.g., ?as.factor. This displays the function’s manual page with descriptions, usage, and examples.\nSearch Websites: Simply Google, or visit the CRAN website to search for specific function documentation. Websites like Stack Overflow and RStudio Community often have discussions related to R functions.\nTutorials and Online Courses: Platforms like DataCamp, Coursera, and edX offer R courses that cover many functions in depth. Also there are examples of dedicated R tutorial websites that you might find useful. One example is “Introduction to R for health data analysis” by Ehsan Karim, An Hoang and Qu.\nBooks: There are numerous R programming books, such as “R for Data Science” by Hadley Wickham and “The Art of R Programming” by Norman Matloff.\nWorkshops and Webinars: Institutions and organizations occasionally offer R programming workshops or webinars.\n\nWhenever in doubt, exploring existing resources can be highly beneficial.\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.factor \n    base \n    Converts a variable to factors. `as.factor` is a wrapper for the `factor` function. \n  \n\n cbind \n    base \n    Merges matrices. \n  \n\n CreateTableOne \n    tableone \n    Creates a frequency table. \n  \n\n data.frame \n    base \n    Creates a dataset with both numeric and character variables. Requires unique column names and equal length for all variables. \n  \n\n dim \n    base \n    Returns the dimensions of a data frame (rows x columns). \n  \n\n filter \n    dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n function \n    base \n    Used to define custom functions, e.g., for calculating standard deviation. \n  \n\n head \n    base \n    Displays the first six elements of an object (e.g., a dataset). `tail` displays the last six. \n  \n\n is.na \n    base \n    Checks for missing values in a variable. \n  \n\n levels \n    base \n    Displays the levels of a factor variable. \n  \n\n list \n    base \n    Stores vectors, matrices, or lists of differing types. \n  \n\n mode \n    base \n    Determines the type of a variable. \n  \n\n na.omit \n    base/stats \n    Removes all rows with missing values from a dataset. \n  \n\n names \n    base \n    Displays names of objects, e.g., variable names of a data frame. \n  \n\n nlevels \n    base \n    Shows the number of levels in a factor variable. \n  \n\n nrow \n    base \n    Returns the dimensions of a data frame. `nrow` gives row count and `ncol` gives column count. \n  \n\n plot \n    base/graphics \n    Draws scatter plots or line graphs. \n  \n\n print \n    base \n    Prints the output to console. \n  \n\n prop.table \n    base \n    Displays percentage summary for a table. \n  \n\n rbind \n    base \n    Appends matrices row-wise. \n  \n\n read.csv \n    base/utils \n    Reads data from a CSV file. \n  \n\n relevel \n    base/stats \n    Changes the reference group of a factor variable. \n  \n\n sasxport.get \n    Hmisc \n    Loads data in the SAS format. \n  \n\n save \n    base \n    Saves R objects, such as datasets. \n  \n\n select \n    dplyr \n    Selects specified variables from a dataset. \n  \n\n set.seed \n    base \n    Sets a seed for random number generation ensuring reproducibility. \n  \n\n str \n    base/utils \n    Displays the structure of a dataset, including data type of variables. \n  \n\n subset \n    base, dplyr \n    Subsets a dataset by selecting a sub-population. \n  \n\n summary \n    base \n    Provides a summary of an object, like variable statistics. \n  \n\n table \n    base \n    Displays frequency counts for a variable. \n  \n\n write.csv \n    base/utils \n    Saves a data frame to a CSV file in a specified directory."
  },
  {
    "objectID": "wranglingQ.html",
    "href": "wranglingQ.html",
    "title": "Quiz (W)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "wranglingE.html#problem-statement",
    "href": "wranglingE.html#problem-statement",
    "title": "Exercise (W)",
    "section": "Problem Statement",
    "text": "Problem Statement\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nA paper you can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\n\nShow the code# Load required packages\nlibrary(dplyr)\nlibrary(tableone)\n\n\n\nShow the code# Data import: name it rhc\n#rhc <- ...(\"Data/wrangling/rhc.csv\", ...)\n\n\nPart (a) Basic Manipulation [60%]\n\nContinuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\n\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\n\n\n\nSet reference: Change the reference category for gender to Male\n\n\n\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\n\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\n\n\n\ncomorbidities:\n\n\ncreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nreport maximim and minimum values of numcom:\n\n\n\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\n“age”, “sex”, “race”,“cat1”, “ca”, “dnr1”, “aps1”, “surv2md1”, “numcom”, “adld3p”, “das2d3pc”, “temp1”, “hrt1”, “meanbp1”, “resp1”, “wblc1”, “pafi1”, “paco21”, “ph1”, “crea1”, “alb1”, “scoma1”, “swang1”, and\nname it rhc2.\n\n\n\n\nPart (b) Table 1 [20%]\n\nRe-produce the sample table from the rhc2 data (see the Table that was provided with this assignment). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\n\n\nTable 1 for subset\n\nProduce a similar table as part (b) but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]\n\n\n\nPart (c) Considering eligibility criteria [20%]\nProduce a similar table as part (b.i) but only for the subjects who meet all of the following eligibility criteria: (i) age is equal to or above 50, (ii) age is below 80 (iii) Glasgow Coma Score is below 61 and (iv) Primary disease categories are either ARF or MOSF. [Hint: droplevels.data.frame can be a useful function]\n\n\n\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\n\n\n\nCount how many NAs does that variable have?\n\n\n\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\n\n\n\n\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nShow the codeVector <- 1:100\n\n#variance.est <- function(?){?}\n\n#variance.est(Vector)\n\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nShow the codevar(Vector)\n#> [1] 841.6667"
  },
  {
    "objectID": "accessing.html#survey-data-sources",
    "href": "accessing.html#survey-data-sources",
    "title": "Data accessing",
    "section": "Survey data sources",
    "text": "Survey data sources\nThe tutorial lists primary complex survey data sources, including the Canadian Community Health Survey and National Health and Nutrition Examination Survey, with several offering dedicated R packages for data access."
  },
  {
    "objectID": "accessing.html#importing-cchs-to-r",
    "href": "accessing.html#importing-cchs-to-r",
    "title": "Data accessing",
    "section": "Importing CCHS to R",
    "text": "Importing CCHS to R\nThe section provides detailed steps for importing the Canadian Community Health Survey dataset from the UBC library into RStudio, with processing options using SAS, the free software PSPP, and directly in R."
  },
  {
    "objectID": "accessing.html#importing-nhanes-to-r",
    "href": "accessing.html#importing-nhanes-to-r",
    "title": "Data accessing",
    "section": "Importing NHANES to R",
    "text": "Importing NHANES to R\nThe tutorial guides users on how to access and import the NHANES dataset from the CDC website into RStudio, detailing the dataset’s structure and providing methods both manually and using an R package."
  },
  {
    "objectID": "accessing.html#reproducing-results",
    "href": "accessing.html#reproducing-results",
    "title": "Data accessing",
    "section": "Reproducing results",
    "text": "Reproducing results\nThe tutorial guides users through accessing, processing, and analyzing NHANES data to reproduce the results from a referenced article using R code.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThe subsequent chapter on Research Questions serves as a valuable guide for constructing an analytics-driven data set tailored to your specific research queries. It will cover crucial aspects such as the types of variables to collect and how to set eligibility criteria, followed by approaches to data analysis based on your research questions. It’s important to note that research questions can fall into two main categories: predictive or causal. For a deeper understanding of variable selection and analytical tools suited to these types of questions, the chapters on the Roles of Variables and Predictive Models offer insightful guidance.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "accessing1.html",
    "href": "accessing1.html",
    "title": "Survey data sources",
    "section": "",
    "text": "The tutorial discusses complex survey data and highlights potential data sources. Key datasets with survey features include the Canadian Community Health Survey (CCHS), the National Health and Nutrition Examination Survey (NHANES), and the European Social Survey (ESS), among others. Many of these sources, like NHANES and ESS, have specific R packages for data retrieval. In addition, there are other data sources such as the Vanderbilt Biostatistics Datasets and the World Bank Open Data, with the latter also offering dedicated R packages for data access.\n\nDataset with survey features\n\nCanadian Community Health Survey - Annual Component CCHS\n\nDownload link UBC library\n\nNational Health and Nutrition Examination Survey NHANES\n\nR packages to download data: nhanesA, RNHANES\n\nNational Longitudinal Study of Adolescent to Adult Health [Add Health], 1994-2008 ICPSR 21600\nEuropean Social Survey ESS\n\nR package to download data: essurvey\n\nBehavioral Risk Factor Surveillance System BRFSS\nBureau of Economic Analysis BEA\nUS National Vital Statistics System NVSS\nDemographic and Health Surveys DHS\n\n\n\nOthers\n\nVanderbilt Biostatistics Datasets link\nWorld Bank Open Data WBOD\n\nR packages to download data: wbstats, WDI"
  },
  {
    "objectID": "accessing2.html",
    "href": "accessing2.html",
    "title": "Importing CCHS to R",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(knitr)\n\n\nThis section provides comprehensive instructions on how to import the Canadian Community Health Survey (CCHS) dataset from the UBC library site to the RStudio environment. The process starts with downloading the CCHS data from the UBC library site and includes step-by-step visual guides for each stage. Three primary options are provided to process and format the data:\n\nUsing the commercial software SAS.\nUtilizing the free software PSPP, an alternative to SPSS.\nDirectly processing the data in R.\n\nFor each option, users are guided on how to download, install, access, read, save, and check the dataset. The objective is to help users acquire, visualize, and manipulate the CCHS dataset seamlessly using various software applications.\nDownloading CCHS data from UBC\n\n\nStep 1: Go to dvn.library.ubc.ca, and press ‘log-in’\n\n\n\n\n\n\n\n\nStep 2: Select ‘UBC’ from the dropdown menu\n\n\n\n\n\n\n\n\nStep 3: Enter your CWL or UBC library authentication information\n\n\n\n\n\n\n\n\nStep 4: Once you log-in, search the term ‘cchs’ in the search-box\n\n\n\n\n\n\n\n\nStep 5: For illustrative purposes, let us work with the Cycle 3.1 of the CCHS dataset from the list of results. In that case, type ‘cchs 3.1’\n\n\n\n\n\n\n\n\nStep 6: CCHS Cycle 3.1 information\n\n\n\n\n\n\n\n\nStep 7: Choose the ‘Data: CD’ from the menu\n\n\n\n\n\n\n\n\nStep 8: Download the entire data (about 159 MB) as a zip file\n\n\n\n\n\n\n\n\nStep 9: Accept the ‘terms of use’\n\n\n\n\n\n\n\n\nStep 10: Select a directory to download the zip file. The path of the download directory is important (we need to use this path exactly later). For example, below we are in \"C:\\CCHS\\\" folder, but we will create a “Data” folder there, so that the download path is \"C:\\CCHS\\Data\\\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 11: Extract the zip file\n\n\n\n\n\n\n\n\nStep 12: Be patient with the extraction\n\n\n\n\n\n\n\n\nStep 13: Once extraction is complete, take a look at the folders inside. You will see that there is a folder named ‘SAS_SPSS’\n\n\n\n\n\n\nReading and Formatting the data\nOption 1: Processing data using SAS\nSAS is a commercial software. You may be able to get access to educational version. In case you don’t have access to it, later we outline how to use free packages to read these datasets.\n\n\nStep 1: Inside that ‘SAS_SPSS’ folder, find the file hs_pfe.sas. It is a long file, but we are going to work on part of it. First thing we want to do it to change all the directory names to where you have unzipped the downloaded file (for example, here the zip file was extracted to C:/CCHS/Data/cchs_cycle3-1CD/). We only need the first part of the code (as shown below; only related to data ‘hs’). Delete the rest of the codes for now. The resulting code should like like this:\n\n\nShow the code%include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_pfe.sas\";\n\ndata hs;\n        %let datafid=\"C:\\CCHS\\Data\\cchs_cycle3-1CD\\Data\\hs.txt\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_fmt.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_lbe.sas\";\nrun;\n\n\nOnce the modifications are done, submit the codes in SAS. Note that, the name of the data is ‘hs’.\n\n\n\n\n\n\n\nStep 2: Once you submit the code, you can check the log window in SAS to see how the code submission went. It should tell you how many observations and variables were read.\n\n\n\n\n\n\n\n\nStep 3: If you one to view the dataset, you can go to ‘Explorer’ window within SAS.\n\n\n\n\n\n\n\n\nStep 4: Generally, if you haven’t specified where to load the files, SAS will by default save the data into a library called ‘Work’\n\n\n\n\n\n\n\n\nStep 5: Open that folder, and you will be able to find the dataset ‘Hs’.\n\n\n\n\n\n\n\n\nStep 6: Right click on the data, and click ‘open’ to view the datafile.\n\n\n\n\n\n\n\n\nStep 7: To export the data into a CSV format data (so that we can read this data into other software packages), ckick ‘Menu’.\n\n\n\n\n\n\n\n\nStep 8: then press ‘Export Data’.\n\n\n\n\n\n\n\n\nStep 9: choose the library and the data.\n\n\n\n\n\n\n\n\nStep 10: choose the format in which you may want to save the existing data.\n\n\n\n\n\n\n\n\nStep 11: also specify where you want to save the csv file and the name of that file (e.g., cchs3.csv).\n\n\n\n\n\n\n\n\nStep 12: go to that directory to see the file cchs3.csv\n\n\n\n\n\n\n\n\nStep 13: If you want to save the file in SAS format, you can do so by writing the following sas code into the ‘Editor’ window. Here we are saving the data Hs within the Work library in to a data called cchs3 within the SASLib library. Note that, the directory name has to be where you want to save the output file.\n\n\nShow the codeLIBNAME SASLib \"C:\\CCHS\\Data\";\nDATA SASLib.cchs3;\n    set Work.Hs;\nrun;\n\n\nSubmit these codes into SAS:\n\n\n\n\n\n\n\nStep 13: go to that directory to see the file cchs3.sas7dbat\n\n\n\n\n\n\nOption 2: Processing data using PSPP (Free)\nPSPP is a free package; alternative to commercial software SPSS. We can use the same SPSS codes to read the datafile into PSPP, and save.\n\n\nStep 1: Get the free PSPP software from the website: www.gnu.org/software/pspp/\n\n\nPSPP is available for GNU/Hurd, GNU/Linux, Darwin (Mac OS X), OpenBSD, NetBSD, FreeBSD, and Windows\n\n\n\n\n\nFor windows, download appropriate version.\n\n\n\n\n\nDownload the file\n\n\n\n\n\nInstall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the icon shorcut after installing\n\n\n\n\n\n\n\nStep 2: Open PSPP\n\n\n\n\n\n\n\n\nStep 3: Go to ‘file’ menu and click ‘open’\n\n\n\n\n\n\n\n\nStep 4: Specify the readfile.sps file from the ‘SAS_SPSS’ folder.\n\n\n\n\n\n\nYou will see the following file:\n\n\n\n\n\n\n\nStep 5: Similar to before, change the directories as appropriate. Get rid of the extra lines of codes. Resulting codes are as follows (you can copy and replace the code in the file with the following codes):\n\n\nShow the codefile handle infile/name = 'C:\\CCHS\\Data\\cchs_cycle3-1CD\\DATA\\hs.txt'.\ndata list file = infile notable/.\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvale.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvare.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsmiss.sps\".\nexecute.\n\n\n\n\n\n\n\nFor Mac users, it should be as follows (e.g., username should be your user name, if you are saving under the path \"/Users/username/CCHS/Data/\"):\n\nShow the codefile handle infile/name =\"/Users/username/CCHS/Data/cchs_cycle3-1CD/Data/hs.txt\".\ndata list file = infile notable/.\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hs_i.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvale.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvare.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsmiss.sps\".\n\nexecute.\n\n\n\n\nStep 6: Run the codes.\n\n\n\n\n\n\n\n\nStep 7: This is a large data, and will take some time to load the data into the PSPP data editor. Be patient.\n\n\n\n\n\n\nOnce loading is complete, it will show the ‘output’ and ‘data view’.\n\n\n\n\n\n\n\n\n\n\nNote that, you will get error message, if your files were not in the correct path. In our example, the path was \"C:\\CCHS\\Data\\\" for the zip file content (see the previous steps).\n\n\nStep 7: You can also check the ‘variable view’.\n\n\n\n\n\n\n\n\nStep 8: Save the data by clicking ‘File’ and then ‘save as …’\n\n\n\n\n\n\n\n\nStep 9: Specify the name of the datafile and the location / folder to save the data file.\n\n\n\n\n\n\n\n\nStep 10: See the SAV file saved in the directory.\n\n\n\n\n\n\n\n\nStep 11: To save CSV format data, use the following syntax.\n\n\nShow the codeSAVE TRANSLATE\n  /OUTFILE=\"C:/CCHS/Data/cchs3b.csv\"  \n  /TYPE=CSV\n  /FIELDNAMES      \n  /CELLS=VALUES.\n\n\nNote that, for categorical data, you can either save values or labels. For our purpose, we prefer values, and hence saved with values here.\n\n\n\n\n\n\n\nStep 12: See the CSV file saved in the directory extracted from PSPP.\n\n\n\n\n\n\nOption 3: Processing data using SPSS\nLog into ubc.onthehub.com to download SPSS. With your CWL account, UBC students should be able to download it. UBC IT website for SPSS says:\nThe SPSS software license with UBC specifies that SPSS must only be used by UBC Faculty, Students, and Research Staff and only for Teaching and non-commercial Research purposes related to UBC.\nBoth network (for UBC owened devices) or standalone / home versions (for non-UBC owened devices) should be available. Once downloaded, same process of importing CCHS data in PSPP can also be applied on SPSS (same syntax files should work). Let me know if that is not the case.\nProcessing data in R\nDownload software\n\n\nStep 1: Download either ‘R’ from CRAN www.r-project.org or ‘R open’ from Microsoft mran.microsoft.com/open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download RStudio from www.rstudio.com/\n\n\n\n\n\n\n\n\n\nStep 3: Open RStudio\n\n\n\n\n\n\nImport, export and load data into R\n\n\nStep 1: Set working directory\n\n\nShow the codesetwd(\"C:/CCHS/Data/\") # or something appropriate\n\n\n\n\nStep 2: Read the dataset created from PSPP with cell values. We can also do a small check to see if the cell values are visible. For example, we choose a variable ‘CCCE_05A’, and tabulate it.\n\n\nShow the codeHs <- read.csv(\"cchs3b.csv\", header = TRUE)\ntable(Hs$CCCE_05A)\n\n\n\n\n\n\n\n\n\nStep 3: Save the RData file from R into a folder SurveyData:\n\n\nShow the codesave(Hs, file = \"SurveyData/cchs3.RData\")\n\n\n\n\nStep 4: See the RData file saved in the directory extracted from R.\n\n\n\n\n\n\n\n\nStep 5: Close R / RStudio and restart it. Environment window within RStudio should be empty.\n\n\n\n\n\n\n\n\nStep 6: Load the saved RData into R. Environment window within RStudio should have ‘Hs’ dataset.\n\n\nShow the codeload(\"SurveyData/cchs3.RData\")"
  },
  {
    "objectID": "accessing3.html",
    "href": "accessing3.html",
    "title": "Importing NHANES to R",
    "section": "",
    "text": "This tutorial provides comprehensive instructions on accessing the National Health and Nutrition Examination Survey (NHANES) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment. It covers:\n\nIntroduction to the NHANES dataset, highlighting its significance in evaluating the health and nutritional status of U.S. adults and children.\nSampling Procedure details, explaining the multi-stage sampling strategy and emphasizing the importance of using survey features like weights, strata, and primary sampling units for population-level estimates.\nSurvey History with a visualization representing different NHANES survey cycles.\nNHANES Data Files and Documents:\n\n\nExplains the data’s file format, mostly in SAS transport file format (.xpt).\nBreaks down the NHANES components, which include demographics, dietary, examination, laboratory, and questionnaire data.\nProvides guidelines on combining data from different cycles and handling missing data or outliers.\n\n\nAccessing NHANES Data:\n\n\nDirectly from the CDC website: A step-by-step guide with accompanying images, illustrating how to navigate the CDC website, download the data, and interpret the accompanying codebook.\nUsing R packages, specifically the nhanesA package: A concise guide on how to download and get summaries of the NHANES data using this R package.\n\n\nShow the code# Load required packages\n#devtools::install_github(\"warnes/SASxport\")\nlibrary(SASxport)\nlibrary(foreign)\nlibrary(nhanesA)\nlibrary(knitr)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nuse.saved.chche <- TRUE\n\n\n\n\nBefore installing a package from GitHub, it’s better to check whether you installed the right version of Rtools\nOverview\nNational Center for Health Statistics (NCHS) conducts National Health and Nutrition Examination Survey (NHANES) (CDC,NCHS 2023). These surveys are designed to evaluate the health and nutritional status of U.S. adults and children. These surveys are being administered in two-year cycles or intervals starting from 1999-2000. Prior to 1999, a number of surveys were conducted (e.g., NHANES III), but in our discussion, we will mostly restrict our discussions to continuous NHANES (e.g., NHANES 1999-2000 to NHANES 2017-2018).\n\n\nCDC,NCHS (2023)\nSampling Procedure:\nIt is a probabilistic sample (we know probability of getting selected for all individuals). This sample is unlikely to be representative of the entire population, as some under/oversampling occurs (unlike SRS), and samples may be dependent (due to proximity of some samples). For example, household with the following characteristics may be oversampled in NHANES, e.g., African Americans, Mexican Americans, Low income White Americans, Persons age 60+ years.\n\n\nSampling Procedure:\n\nnot obtained via simple random sample\nmultistage sample designs\nA sample weight is assigned to each sample person where weight = the number of people in the target population represented by that sample person in NHANES\n\nNHANES used multistage sample designs:\n\nStage 1: PSU/clusters = geographically contiguous counties. 50 states - divided into ~3100 counties. Each PSU is assigned to a strata (e.g., urban/rural or PSU size etc.). The counties are randomly/PPS selected using a 2-per-stratum design. Complex sample variance estimation requires PSU + strata (masking involved).\nStage 2: each selected county is broken into segments (with at least ~50-100 housing units). Segments are randomly/PPS selected.\nStage 3: each selected segment is divided into households. Households are randomly selected.\nStage 4: Within each sampled household, an individual is randomly selected.\n\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nSurvey history\nOverall NHANES survey history\n\n\n\n\n\n\n\n\nNHANES datafile and documents\nFile format\nThe Continuous NHANES files are stored in the NHANES website as SAS transport file formats (.xpt). You can import this data in any statistical package that supports this file format.\nContinuous NHANES Components\nContinuous NHANES components separated to reduce the amount of time to download and documentation size:\n\n\nNHANES Tutorials\n\n\n\n\n\n\n\n\n\n\nBroadly, continuous NHANES data are available in 5 categories:\n\nDemographics\nDietary\nExamination\nLaboratory\nQuestionnaire\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHANES. However, NHANES is a cross-sectional data, and identification of the same person accross different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\n\n\nThe following data have not been released on the NHANES website as public release files due to confidentiality concerns:\n\nadolescent data on alcohol use\nsmoking\nsexual behavior\nreproductive health and drug use\n\nWithin the same cycle\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\nMissing data and outliers\nCDC (2023) recommends:\n\n\nCDC (2023)\n\n\n“As a general rule, if 10% or less of your data for a variable are missing from your analytic dataset, it is usually acceptable to continue your analysis without further evaluation or adjustment. However, if more than 10% of the data for a variable are missing, you may need to determine whether the missing values are distributed equally across socio-demographic characteristics, and decide whether further imputation of missing values or use of adjusted weights are necessary.”\n\n\n\n\n“If you fail to identify ‘refusal’ or ‘do not know’ as types of missing data, and treat the assigned values for ‘refused’ or ‘do not know’ as real values, you will get distorted results in your statistical analyses. Therefore, it is important to recode ‘refused’ or ‘don’t know’ responses as missing values (either as a period (.) for numeric variables or as a blank for character variables).”\n\n\n\n\n“Outliers with extremely large weights could have an influential impact on your estimates. You will have to decide whether to keep these influential outliers in your analysis or not. It is up to the analysts to make that decision.”\n\n\nNHANES documents\n\n\n\n\n\n\n\n\n\n\nThe following websites could be helpful: - For more information about NHANES design.\n\nVisit US CDC website and do a variable keyword search based on your research interest (e.g., arthritis).\n\nAccessing NHANES Data Directly from the CDC website\nIn the following example, we will see how to download ‘Demographics’ data, and check associated variable in that dataset.\n\n\n\n\n\n\n\nNHANES 1999-2000 and onward survey datasets are publicly available at wwwn.cdc.gov/nchs/nhanes/\n\n\nStep 1: Say, for example, we are interested about the NHANES 2015-2016 survey. Clicking the associated link in the above Figure gets us to the page for the corresponding cycle (see below).\n\n\n\n\n\n\n\n\nStep 2: There are various types of data available for this survey. Let’s explore the demographic information from this cycle. These data are mostly available in the form of SAS XPT format (see below).\n\n\n\n\n\n\n\n\nStep 3: We can download the XPT data in the local PC folder and read the data into R as as follows:\n\n\nShow the codeDEMO <- read.xport(\"Data/accessing/DEMO_I.XPT\")\n\n\n\n\n\n\n\nStep 4: Once data is imported in RStudio, we will see the DEMO object listed under data window (see below):\n\n\n\n\n\n\n\n\nStep 5: We can also check the variable names in this DEMO dataset as follows:\n\n\nShow the codenames(DEMO)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\n\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the DEMO data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\nStep 7: There is a column name associated with each column, e.g., DMDHSEDU in the first column in the above Figure. To understand what the column names mean in this Figure, we need to take a look at the codebook. To access codebook, click the 'DEMO|Doc' link (in step 2). This will show the data documentation and associated codebook (see the next Figure).\n\n\n\n\n\n\n\n\nStep 8: We can see a link for the column or variable DMDHSEDU in the table of content (in the above Figure). Clicking that link will provide us further information about what this variable means (see the next Figure).\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count and cumulative (from the above Figure) matches with what we get from the DEMO data we just imported (particularly, for the DMDHSEDU variable):\n\n\nShow the codetable(DEMO$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(DEMO$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(DEMO$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nAccessing NHANES Data Using R Packages\nnhanesA package\n\nShow the codelibrary(nhanesA)\n\n\n\n\n\n\n\n\nTip\n\n\n\nR package nhanesA provides a convenient way to download and analyze NHANES survey data.\n\n\n\n\nRNHANES (Susmann 2016) is another packages for downloading the NHANES data easily.\n\n\nStep 1: Witin the CDC website, NHANES data are available in 5 categories\n\nDemographics (DEMO)\nDietary (DIET)\nExamination (EXAM)\nLaboratory (LAB)\nQuestionnaire (Q)\n\n\n\nTo get a list of available variables within a data file, we run the following command (e.g., we check variable names within DEMO data):\n\nShow the codenhanesTables(data_group='DEMO', year=2015)\n\n\n\n  \n\n\n\n\n\nStep 2: We can obtain the summaries of the downloaded data as follows (see below):\n\n\nShow the codedemo <- nhanes('DEMO_I')\nnames(demo)\n#>  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#>  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#> [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#> [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#> [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#> [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#> [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#> [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\ntable(demo$DMDHSEDU) # Frequency table\n#> \n#>    1    2    3    4    5    7    9 \n#>  619  511  980 1462 1629    2   23\ncumsum(table(demo$DMDHSEDU)) # Cumulative frequency table\n#>    1    2    3    4    5    7    9 \n#>  619 1130 2110 3572 5201 5203 5226\nlength(is.na(demo$DMDHSEDU)) # Number of non-NA observations\n#> [1] 9971\n\n\nReferences\n\n\n\n\nCDC. 2023. “NHANES Web Tutorial Frequently Asked Questions (FAQs).” https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/faq.aspx.\n\n\nCDC,NCHS. 2023. “National Health and Nutrition Examination Survey Data.” https://wwwn.cdc.gov/nchs/nhanes/.\n\n\nSusmann, Herb. 2016. RNHANES: Facilitates Analysis of CDC NHANES Data. https://CRAN.R-project.org/package=RNHANES."
  },
  {
    "objectID": "accessing4.html#references",
    "href": "accessing4.html#references",
    "title": "Reproducing results",
    "section": "References",
    "text": "References\n\n\n\n\nDhana, A. 2023. “R & Python for Data Science.” https://datascienceplus.com/.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91."
  },
  {
    "objectID": "accessingF.html",
    "href": "accessingF.html",
    "title": "R Functions (A)",
    "section": "",
    "text": "The section introduces a set of R functions useful for accessing and processing complex survey data, providing their descriptions and the packages they belong to.\n\n\n\n\n\n Function_name \n    Package_name \n    Description \n  \n\n\n apply \n    base \n    Applies a function over an array or matrix. \n  \n\n cut \n    base \n    Converts a numeric variable to a factor variable. \n  \n\n merge \n    base/data.table \n    Merges multiple datasets. \n  \n\n names \n    base \n    Retrieves the names of an object. \n  \n\n nhanes \n    nhanesA \n    Downloads a NHANES datafile. \n  \n\n nhanesTables \n    nhanesA \n    Lists available variables within a datafile. \n  \n\n nhanesTranslate \n    nhanesA \n    Encodes categorical variables to match with certain standards, e.g., CDC website. \n  \n\n recode \n    car \n    Recodes a variable. \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "accessingQ.html",
    "href": "accessingQ.html",
    "title": "Quiz (A)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "accessingE.html#problem-statement",
    "href": "accessingE.html#problem-statement",
    "title": "Exercise (A)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Palis, Marchand, and Oviedo-Joekes (2020), DOI: 10.1080/09638237.2018.1437602.\n\nDownload the CCHS MH topical index\n\nDownload the CCHS MH Data Dictionary"
  },
  {
    "objectID": "accessingE.html#question-i-60-grade",
    "href": "accessingE.html#question-i-60-grade",
    "title": "Exercise (A)",
    "section": "Question I: [60% grade]",
    "text": "Question I: [60% grade]\n1(a) Importing dataset\n\nShow the code# Importing dataset\nload(\"Data/accessing/cchsMH.RData\") \n\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper\n\nIdentify the variable needed for eligibility criteria\nIdentify the outcome variable\nIdentify the explanatory variable\nIdentify the potential confounders\nIdentify the survey weight variable\n\nHint\n\nRead\n\n\nthe first paragraph of Analytic sample (page 2) for the eligibility criteria, and\nfirst and second paragraphs of Study variables for rest of the variables,\nthird paragraph of the Statistical analyses for the survey weights variable.\n\n\neligibility criteria was determined based on one variable. Only work with ‘YES’ category.\nOutcome variable has a category ‘NOT STATED’, but for our analysis, we will omit anyone associated with this category.\nFor explanatory variable, we have categories such as DON’T KNOW, REFUSAL and NOT STATED. We will not use these categories (omit anyone with these categories).\nThere were five potential confounders.\nPotentially useful functions:\n\n\n%in%\nlevels\nrecode\nsubset\nas.factor\nrelevel\n\nor dplyr ways:\n\nfilter\nselect\n\n\n\n\nShow the code# your code here\n\n\n1(c) Retaining necessary variables\nIn the dataset, retain only the variables associated with outcome measure, explanatory variable, potential confounders and survey weight\n\nShow the code# your code here\n\n\n1(d) Creating analytic dataset\n\nAssign missing values for categories such as DON’T KNOW, REFUSAL and NOT STATED.\n‘recode’ the variables as shown in Table 1 (choose any function of your choice). Here is an example (but feel free to use other functions. In R there are many other ways to do this same task):\n\n\nShow the code## your code here\n# levels(your.data.frame$your.age.variable) <- \n#   list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n#        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n#        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n#        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n#        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n#        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n#        \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n\n1(e) Number of columns and variable names\nreport the number of columns in your analytic dataset, and the variable names.\n\nShow the code# your code here"
  },
  {
    "objectID": "accessingE.html#question-ii-20-grade",
    "href": "accessingE.html#question-ii-20-grade",
    "title": "Exercise (A)",
    "section": "Question II: [20% grade]",
    "text": "Question II: [20% grade]\n2(a) Table 1\nReproduce Table 1 presented in the above paper (omit the ‘Main source of income’ variable). If necessary, drop other variables from the analytic dataset that are not presented in Table 1.\nThe table you produce should report numbers as follows:\n\n\n\n\n\n\n\n\n\nSelf-rated Mental Health Variable\nTotal n(%)\nPoor or Fair n(%)\nGood n(%)\nVery good or excellent n(%)\n\n\n\nStudy sample\n2628 (100)\n1002 (38.1)\n885 (33.7)\n741 (28.2)\n\n\nCommunity belonging\n\n\n\n\n\n\n- Very weak\n480 (18.3)\n282 (28.1)\n118 (13.3)a\n80 (10.8)a\n\n\n- Somewhat weak\n857 (32.6)\n358 (35.7)\n309 (34.9)\n190 (25.6)\n\n\n- Somewhat strong\n1005 (38.2)\n288 (28.7)\n362 (40.9)\n355 (47.9)\n\n\n- Very strong\n286 (10.9)\n74 (7.4)a\n96 (10.8)a\n116 (15.7)a\n\n\nSex\n\n\n\n\n\n\n- Females\n1407 (53.5)\n616 (61.5)\n487 (55.0)\n304 (41.0)\n\n\n- Males\n1221 (46.5)\n386 (38.5)\n398 (45.0)\n437 (59.0)\n\n\nAge group\n\n\n\n\n\n\n- 15 to 24 years\n740 (28.2)\n191 (19.1)\n264 (29.8)\n285 (38.5)\n\n\n- 25 to 34 years\n475 (18.1)\n141 (14.1)\n167 (18.9)\n167 (22.5)\n\n\n- 35 to 44 years\n393 (15.0)\n185 (18.5)\n119 (13.4)a\n89 (12.0)a\n\n\n- 45 to 54 years\n438 (16.6)\n220 (22.0)\n139 (15.7)\n79 (10.7)a\n\n\n- 55 to 64 years\n379 (14.4)\n198 (19.7)\n113 (12.8)a\n68 (9.2)a\n\n\n- 65 years or older\n203 (7.7)\n67 (6.6)a\n83 (8.4)a\n53 (7.1)b\n\n\nRace/Ethnicity\n\n\n\n\n\n\n- Non-white\n458 (17.4)\n184 (18.4)\n140 (15.8)\n134 (18.1)\n\n\n- White\n2170 (82.6)\n818 (81.6)\n745 (84.2)\n607 (81.9)\n\n\nMain source of income\n\n\n\n\n\n\n- Employment Income^d\n1054 (40.1)\n289 (28.8)\n386 (43.6)\n379 (51.1)\n\n\n- Worker’s Compensation^e\n160 (6.1)\n91 (9.1)a\n44 (5.0)b\n25 (3.4)c\n\n\n- Senior Benefits^f\n134 (5.1)\n57 (5.7)a\n42 (4.7)b\n35 (4.7)\n\n\n- Other^g\n184 (7.0)\n82 (8.2)a\n60 (6.8)a\n42 (5.7)b\n\n\n- Not applicable^h\n851 (32.4)\n402 (40.1)\n263 (29.7)\n186 (25.1)\n\n\n- Not Stated^i\n245 (9.3)\n81 (8.1)a\n90 (10.2)a\n74 (10.0)\n\n\n\n\\(^a\\) Coefficient of variation between 16.6 and 25.0%. \\(^b\\) Coefficient of variation between 25.1 and 33.3%. \\(^c\\) Coefficient of variation > 33.3%. \\(^d\\) Employment Income: Wages/salaries or self-employment. \\(^e\\) Worker’s compensation: Employment insurance or worker’s compensation or social assistance/welfare. \\(^f\\) Senior Benefits: Benefits from Canada or Quebec Pension Plan or job related retirement pensions, superannuation and annuities or RRSP/RRIF of Old Age Security and Guaranteed Income Supplement. \\(^g\\) Other: Dividends/interest or child tax benefit or child support or alimony or other or no income. \\(^h\\) Not applicable: Respondents who live in a household with only one person. The income variable “main source of personal income” is applicable only to those that live in a household of more than one person. \\(^i\\) Not Stated: Question was not answered (don’t know, refusal, not stated). - Hint - You can produce 1 table with total, and another table stratified by the necessary variable.\n\nShow the code# your code here\nrequire(tableone)"
  },
  {
    "objectID": "accessingE.html#question-iii-20-grade",
    "href": "accessingE.html#question-iii-20-grade",
    "title": "Exercise (A)",
    "section": "Question III: [20% grade]",
    "text": "Question III: [20% grade]\n3(a) Subset\nSubset the dataset excluding ‘Very good or excellent’ responses from the self-rated mental health variable\n\nShow the code# your code here\n\n\n3(b) Recode\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\nShow the code# your code here\n\n\n3(c) Regression\nRun a logistic regression model for finding the relationship between community belonging (Reference: Very weak) and self-rated mental health (Reference: Poor) among respondents with mental or substance use disorders. Adjust the model for three confounders: sex, age, and race/ethnicity.\n\nShow the code# your code here\n\n\n3(d) Reporting odds ratio\nReport the odds ratios and associated confidence intervals (use Publish package).\n\nShow the coderequire(Publish)\n# your code here\n\n\n\n\n\n\nPalis, Heather, Kirsten Marchand, and Eugenia Oviedo-Joekes. 2020. “The Relationship Between Sense of Community Belonging and Self-Rated Mental Health Among Canadians with Mental or Substance Use Disorders.” Journal of Mental Health 29 (2): 168–75."
  },
  {
    "objectID": "researchquestion.html#predictive-questions",
    "href": "researchquestion.html#predictive-questions",
    "title": "Research question",
    "section": "Predictive questions",
    "text": "Predictive questions\nThe first tutorial serves to educate the user on how to utilize the RHC dataset to answer a predictive research question: developing a prediction model for the length of stay. The tutorial equips users with the skills to clean and process raw data, transforming it into an analyzable format, and introduces concepts that will be foundational for subsequent analysis.\nThe second tutorial (part a for downloading and part b for analyzing) provides an in-depth guide on how to build a predictive model for Diastolic blood pressure using the NHANES dataset for the years 2013-14."
  },
  {
    "objectID": "researchquestion.html#causal-questions",
    "href": "researchquestion.html#causal-questions",
    "title": "Research question",
    "section": "Causal questions",
    "text": "Causal questions\nThe third tutorial aims to guide a study on the relationship between Osteoarthritis (OA) and cardiovascular diseases (CVD) among Canadian adults from 2001-2005. Utilizing the Canadian Community Health Survey (CCHS) cycle 1.1-3.1, the study intends to explore whether OA increases (more accurately, whether associated with) the risk of developing CVD.\nThe NHANES dataset was analyzed in this forth tutorial to explore the relationship between health predictors and cholesterol levels (association/causal). After refining the survey design and handling missing data, regression models were built using varying predictors. Standard error computations and p-values were derived, adjusting for the survey’s unique structure.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\nReference\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nThabane, Lehana, Tara Thomas, Chenglin Ye, and James Paul. 2009. “Posing the Research Question: Not so Simple.” Canadian Journal of Anesthesia/Journal Canadien d’anesthésie 56 (1): 71–79."
  },
  {
    "objectID": "researchquestion1.html",
    "href": "researchquestion1.html",
    "title": "Predictive question-1",
    "section": "",
    "text": "Show the code# Load required packages\nrequire(tableone)\nrequire(Publish)\nrequire(MatchIt)\nrequire(cobalt)\nrequire(ggplot2)\n\n\nWorking with a Predictive question using RHC\nThis tutorial delves into processing and understanding the RHC dataset, which pertains to patients in the intensive care unit. The dataset is particularly centered around the implications of using right heart catheterization (RHC) in the early phases of care, with a focus on comparing two patient groups: those who received the RHC procedure and those who did not. The key outcome being analyzed is the 30-day survival rate. We will use this as an example to explain how to work with a predictive research question to build the analytic data.\n\n\nLink for the RHC dataset\n(Connors et al. 1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as\n\n\nlength of stay (hospital stay; measured continuously)\n\ndeath within certain period (death at any time up to 180 Days; measured as a binary variable)\n\nThe original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above.\nBut we will use this data as a case study for our prediction modelling. Traditional PICOT framework is designed primarily for clinical questions related to interventions, so when applying it to other areas like predictive modeling, some creative adaptation is needed.\n\n\n\n\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nNot applicable, as we are dealing with a prediction model here\n\n\nC\nNot applicable, as we are dealing with a prediction model here\n\n\nO\nin-hospital mortality\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\n\n\n\nWe are interested in developing a prediction model for the length of stay.\nData download\nData is freely available from Vanderbilt Biostatistics, variable list is available here, and the article is freely available from researchgate.\n\n\nRHC Data amd search for right heart catheterization dataset\n\nVariable list\n\nArticle\n\n\nLet us download the dataset and save it for later use.\n\nShow the code# Load the dataset\nObsData <- read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE)\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhc.RDS\")\n\n\nCreating analytic data\nNow, we show the process of preparing analytic data, so that the variables generally match with the way the authors were coded in the original article. Below we show the process of creating the analytic data.\nAdd column for outcome: length of stay\n\nShow the code# Length.of.Stay = date of discharge - study admission date\nObsData$Length.of.Stay <- ObsData$dschdte - ObsData$sadmdte\n\n# Length.of.Stay = date of death - study admission date if date of discharge not available\nObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] <- \n  ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - \n  ObsData$sadmdte[is.na(ObsData$Length.of.Stay)]\n\n\nRecoding column for outcome: death\n\n\n\n\n\n\nTip\n\n\n\nHere we use the ifelse function to create a categorical variable. Other related functions are cut, car.\n\n\nLet us recode our outcome variable as a binary variable:\n\nShow the codeObsData$death <- ifelse(ObsData$death == \"Yes\", 1, 0)\n\n\nRemove unnecessary outcomes\nOur next task is to remove unnecessary outcomes:\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to drop variables from a dataset. E.g., without using any package and using the select function from the dplyr package.\n\n\n\nShow the codeObsData <- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, \n                            t3d30, dth30, surv2md1))\n\n\nRemove unnecessary and problematic variables\nNow we will drop unnecessary and problematic variables:\n\nShow the codeObsData <- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, urin1, cat2))\n\n\nBasic data cleanup\nNow we will do some basic cleanup.\n\n\n\n\n\n\nTip\n\n\n\nWe an use the lapply function to convert all categorical variables to factors at once. Not that a similar function to lapply is sapply. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\n\nShow the code# convert all categorical variables to factors\nfactors <- c(\"cat1\", \"ca\", \"death\", \"cardiohx\", \"chfhx\", \n             \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n             \"transhx\", \"amihx\", \"sex\", \"dnr1\", \"ninsclas\", \n             \"resp\", \"card\", \"neuro\", \"gastr\", \"renal\", \"meta\", \n             \"hema\", \"seps\", \"trauma\", \"ortho\", \"race\", \n             \"income\")\nObsData[factors] <- lapply(ObsData[factors], as.factor)\n\n# convert RHC.use (RHC vs. No RHC) to a binary variable\nObsData$RHC.use <- ifelse(ObsData$swang1 == \"RHC\", 1, 0)\nObsData <- dplyr::select(ObsData, !swang1)\n\n# Categorize the variables to match with the original paper\nObsData$age <- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf),\n                   right=FALSE)\nObsData$race <- factor(ObsData$race, levels=c(\"white\",\"black\",\"other\"))\nObsData$sex <- as.factor(ObsData$sex)\nObsData$sex <- relevel(ObsData$sex, ref = \"Male\")\nObsData$cat1 <- as.factor(ObsData$cat1)\nlevels(ObsData$cat1) <- c(\"ARF\",\"CHF\",\"Other\",\"Other\",\"Other\",\n                          \"Other\",\"Other\",\"MOSF\",\"MOSF\")\nObsData$ca <- as.factor(ObsData$ca)\nlevels(ObsData$ca) <- c(\"Metastatic\",\"None\",\"Localized (Yes)\")\nObsData$ca <- factor(ObsData$ca, levels=c(\"None\", \"Localized (Yes)\",\n                                          \"Metastatic\"))\n\n\nRename variables\n\nShow the code# Rename the variables\nnames(ObsData) <- c(\"Disease.category\", \"Cancer\", \"Death\", \"Cardiovascular\", \n                    \"Congestive.HF\", \"Dementia\", \"Psychiatric\", \"Pulmonary\", \n                    \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\", \n                    \"Immunosupperssion\", \"Transfer.hx\", \"MI\", \"age\", \"sex\", \n                    \"edu\", \"DASIndex\", \"APACHE.score\", \"Glasgow.Coma.Score\", \n                    \"blood.pressure\", \"WBC\", \"Heart.rate\", \"Respiratory.rate\", \n                    \"Temperature\", \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\", \n                    \"Bilirubin\", \"Creatinine\", \"Sodium\", \"Potassium\", \"PaCo2\", \n                    \"PH\", \"Weight\", \"DNR.status\", \"Medical.insurance\", \n                    \"Respiratory.Diag\", \"Cardiovascular.Diag\", \n                    \"Neurological.Diag\", \"Gastrointestinal.Diag\", \"Renal.Diag\",\n                    \"Metabolic.Diag\", \"Hematologic.Diag\", \"Sepsis.Diag\", \n                    \"Trauma.Diag\", \"Orthopedic.Diag\", \"race\", \"income\", \n                    \"Length.of.Stay\", \"RHC.use\")\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhcAnalytic.RDS\")\n\n\nNotations\nlet us introduce with some notations:\n\n\nNotations\nExample in RHC study\n\n\n\n\n\\(Y_1\\): Observed outcome\nlength of stay\n\n\n\n\\(Y_2\\): Observed outcome\ndeath within 3 months\n\n\n\n\\(L\\): Covariates\nSee below\n\n\nBasic data exploration\nDimension\nLet us the how many rows and columns we have:\n\nShow the codedim(ObsData)\n#> [1] 5735   52\n\n\nComprehensive summary\nLet us see the summary statistics of the variables:\n\n\n\n\n\n\nTip\n\n\n\nTo see the comprehensive summary of the variables, we can use the skim function form skimr package or describe function from rms package\n\n\n\nShow the coderequire(skimr)\n#> Loading required package: skimr\n#> Warning: package 'skimr' was built under R version 4.3.1\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nDeath\n0\n1\nFALSE\n2\n1: 3722, 0: 2013\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, > $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\nPredictive vs. causal models\nThe focus of current document is predictive models (e.g., predicting a health outcome).\n\n\n\n\n\nThe original article by Connors et al. (1996) focused on the association of\n\n\nConnors et al. (1996)\n\nright heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and\nthe health-outcomes (such as length of stay).\n\n\n\n\n\n\nThen the PICOT table changes as follows:\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nReceiving a right heart catheterization (RHC)\n\n\nC\nNot receiving a right heart catheterization (RHC)\n\n\nO\nlength of stay\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996."
  },
  {
    "objectID": "researchquestion2a.html#saving-data-for-later-use",
    "href": "researchquestion2a.html#saving-data-for-later-use",
    "title": "Predictive question-2a",
    "section": "Saving data for later use",
    "text": "Saving data for later use\nIt’s a good practice to save your data for future reference.\n\nShow the codesave(analytic.data, file=\"Data/researchquestion/Analytic2013.RData\")"
  },
  {
    "objectID": "researchquestion2a.html#exercise-try-yourself",
    "href": "researchquestion2a.html#exercise-try-yourself",
    "title": "Predictive question-2a",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nFollow the steps in the exercise section to deepen your understanding and broaden the analysis.\n\nThe following variables were not included in the above analysis, that were included in this paper: try including them and then create the new analytic data:\n\n\neducation level\npoverty income ratio\nSodium intake (mg)\nPotassium intake (mg)\n\n\nDownload the NHANES 2015-2016 and append with the NHANES 2013-2014 analytic data with same variables."
  },
  {
    "objectID": "researchquestion2a.html#references",
    "href": "researchquestion2a.html#references",
    "title": "Predictive question-2a",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion2b.html#saving-for-further-use",
    "href": "researchquestion2b.html#saving-for-further-use",
    "title": "Predictive question-2b",
    "section": "Saving for further use",
    "text": "Saving for further use\n\nShow the codesave(analytic.data1, file = \"Data/researchquestion/NHANESanalytic.Rdata\")"
  },
  {
    "objectID": "researchquestion2b.html#regression-summary-optional",
    "href": "researchquestion2b.html#regression-summary-optional",
    "title": "Predictive question-2b",
    "section": "Regression summary (Optional)",
    "text": "Regression summary (Optional)\n\n\nThis is optional content for this chapter. Later in confounding and predictive factor chapters, we will learn more about adjustment.\nDifferent General Linear Models (GLMs) are fit for diastolic blood pressure using variables like gender, marital status, etc.\nBivariate Regression summary (missing values included)\n\nShow the codefit1g <- glm(diastolic ~ gender, data=analytic.data1)\nsummary(fit1g)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -67.579   -7.091    0.421    6.909   50.421  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   71.5789     0.2352 304.299  < 2e-16 ***\n#> genderFemale  -2.4880     0.3278  -7.591 3.76e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 136.3911)\n#> \n#>     Null deviance: 700862  on 5082  degrees of freedom\n#> Residual deviance: 693003  on 5081  degrees of freedom\n#>   (686 observations deleted due to missingness)\n#> AIC: 39415\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codefit1m <- glm(diastolic ~ marital, data=analytic.data1)\nsummary(fit1m)\n#> \n#> Call:\n#> glm(formula = diastolic ~ marital, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -66.750   -6.838    1.162    7.250   51.250  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                70.7500     0.2138 330.901  < 2e-16 ***\n#> maritalNever married       -1.9116     0.4316  -4.429 9.69e-06 ***\n#> maritalPreviously married  -0.3953     0.4140  -0.955     0.34    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 137.5101)\n#> \n#>     Null deviance: 700840  on 5079  degrees of freedom\n#> Residual deviance: 698139  on 5077  degrees of freedom\n#>   (689 observations deleted due to missingness)\n#> AIC: 39434\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\nShow the codestr(analytic.data1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 NA 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 NA 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 NA NA 1 1 NA 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 NA 3 NA 3 1 1 NA ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nfit13 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data1)\nsummary(fit13)\n#> \n#> Call:\n#> glm(formula = diastolic ~ gender + age.centred + race + marital + \n#>     systolic + smoke + alcohol, data = analytic.data1)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -75.142   -6.090    0.811    7.074   33.512  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               30.92372    2.44895  12.627  < 2e-16 ***\n#> genderFemale              -0.34850    0.59830  -0.582 0.560325    \n#> age.centred               -0.13638    0.02142  -6.367 2.56e-10 ***\n#> raceNon-Hispanic Black     1.44736    1.11246   1.301 0.193443    \n#> raceNon-Hispanic White     0.59565    0.96117   0.620 0.535540    \n#> raceOther Hispanic         1.07369    1.29793   0.827 0.408234    \n#> raceOther race             2.02908    1.22998   1.650 0.099216 .  \n#> maritalNever married      -2.92801    0.79123  -3.701 0.000223 ***\n#> maritalPreviously married  0.44754    0.71911   0.622 0.533804    \n#> systolic                   0.31071    0.01763  17.624  < 2e-16 ***\n#> smokeSome days            -0.42177    0.97853  -0.431 0.666513    \n#> smokeNot at all            0.01796    0.65159   0.028 0.978008    \n#> alcohol                    0.17287    0.10994   1.572 0.116060    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 117.7142)\n#> \n#>     Null deviance: 219477  on 1515  degrees of freedom\n#> Residual deviance: 176924  on 1503  degrees of freedom\n#>   (4253 observations deleted due to missingness)\n#> AIC: 11546\n#> \n#> Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "researchquestion2b.html#check-missingness-optional",
    "href": "researchquestion2b.html#check-missingness-optional",
    "title": "Predictive question-2b",
    "section": "Check missingness (optional)",
    "text": "Check missingness (optional)\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nThe plot_missing() function from the DataExplorer package is used to plot missing data.\n\nShow the coderequire(DataExplorer)\nplot_missing(analytic.data1)\n\n\n\n\n\nShow the coderequire(\"tableone\")\nvars = c(\"systolic\", \"smoke\", \"diastolic\", \"race\", \n                       \"age.centred\", \"gender\", \"marital\", \"alcohol\")\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nSetting correct variable types\nThe variables are explicitly set to either numeric or factor types.\nNote: In case any of the variables types are wrong, your table 1 output will be wrong. Better to be sure about what type of variable you want them to be (numeric or factor). For example, systolic should be numeric. Is it defined that way?\n\nShow the codemode(analytic.data1$systolic)\n#> [1] \"numeric\"\n\n\nIn case it wasn’t (often they can get converted to character), then here is the solution:\n\nShow the code# solution 1: one-by-one\nanalytic.data1$systolic <- as.numeric(as.character(analytic.data1$systolic))\nsummary(analytic.data1$systolic)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   110.0   120.0   123.2   134.0   228.0     658\n\n\n\nShow the code# solution 2: fixing all variable types at once\nnumeric.names <- c(\"systolic\", \"diastolic\", \"age.centred\", \"alcohol\")\nfactor.names <- vars[!vars %in% numeric.names]\nfactor.names\n#> [1] \"smoke\"   \"race\"    \"gender\"  \"marital\"\nanalytic.data1[,factor.names] <- lapply(analytic.data1[,factor.names] , factor)\nanalytic.data1[numeric.names] <- apply(X = analytic.data1[numeric.names],\n                                       MARGIN = 2, FUN =function (x) \n                                         as.numeric(as.character(x)))\nlevels(analytic.data1$marital)\n#> [1] \"Married\"            \"Never married\"      \"Previously married\"\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    123.16 (18.12)\n#>   smoke (%)                             \n#>      Every day               965 (16.7) \n#>      Some days               229 ( 4.0) \n#>      Not at all             1336 (23.2) \n#>      NA                     3239 (56.1) \n#>   diastolic (mean (SD))    70.30 (11.74)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3382 (58.6) \n#>      Never married          1112 (19.3) \n#>      Previously married     1272 (22.0) \n#>      NA                        3 ( 0.1) \n#>   alcohol (mean (SD))       2.65 (2.34)\n\n\nComplete case analysis\nRemoves all rows containing NA.\n\nShow the codedim(analytic.data1)\n#> [1] 5769   14\nanalytic.data2 <- as.data.frame(na.omit(analytic.data1))\ndim(analytic.data2)\n#> [1] 1516   14\nplot_missing(analytic.data2)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data2, includeNA = TRUE, \n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         1516        \n#>   systolic (mean (SD))    123.29 (17.58)\n#>   smoke (%)                             \n#>      Every day               590 (38.9) \n#>      Some days               159 (10.5) \n#>      Not at all              767 (50.6) \n#>   diastolic (mean (SD))    70.11 (12.04)\n#>   race (%)                              \n#>      Mexican American        162 (10.7) \n#>      Non-Hispanic Black      292 (19.3) \n#>      Non-Hispanic White      778 (51.3) \n#>      Other Hispanic          126 ( 8.3) \n#>      Other race              158 (10.4) \n#>   age.centred (mean (SD))  -0.76 (16.71)\n#>   gender = Female (%)        626 (41.3) \n#>   marital (%)                           \n#>      Married                 858 (56.6) \n#>      Never married           300 (19.8) \n#>      Previously married      358 (23.6) \n#>   alcohol (mean (SD))       3.15 (2.76)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23 <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data2)\nrequire(Publish)\npublish(fit23)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          30.92 [26.12;35.72]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -0.35  [-1.52;0.82]   0.5603254 \n#>  age.centred                          -0.14 [-0.18;-0.09]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        1.45  [-0.73;3.63]   0.1934428 \n#>              Non-Hispanic White        0.60  [-1.29;2.48]   0.5355396 \n#>                  Other Hispanic        1.07  [-1.47;3.62]   0.4082336 \n#>                      Other race        2.03  [-0.38;4.44]   0.0992165 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.93 [-4.48;-1.38]   0.0002229 \n#>              Previously married        0.45  [-0.96;1.86]   0.5338035 \n#>     systolic                           0.31   [0.28;0.35]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.42  [-2.34;1.50]   0.6665127 \n#>                      Not at all        0.02  [-1.26;1.30]   0.9780080 \n#>      alcohol                           0.17  [-0.04;0.39]   0.1160603\n\n\nImputed data\nWe will learn about proper missing data analysis at a latter class. Currently, we will do a simple (but rather controversial) single imputation. In here we are simply using a random sampling to impute (probably the worst method, but we are just filling in some gaps for now).\n\nShow the coderequire(mice)\nimputation1 <- mice(analytic.data1,\n                   method = \"sample\",  \n                   m = 1, # Number of multiple imputations. \n                   maxit = 1 # Number of iteration; mostly useful for convergence\n                   )\n#> \n#>  iter imp variable\n#>   1   1  systolic  diastolic  marital  alcohol  smoke\n#> Warning: Number of logged events: 5\nanalytic.data.imputation1 <- complete(imputation1)\ndim(analytic.data.imputation1)\n#> [1] 5769   14\nstr(analytic.data.imputation1)\n#> 'data.frame':    5769 obs. of  14 variables:\n#>  $ id         : num  73557 73558 73559 73561 73562 ...\n#>  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#>  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#>  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#>  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#>  $ systolic   : num  122 156 140 136 160 118 100 128 140 106 ...\n#>  $ diastolic  : num  72 62 90 86 84 80 66 74 78 60 ...\n#>  $ race       : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#>  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#>  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#>  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#>  $ alcohol    : num  1 4 1 2 1 1 4 1 3 2 ...\n#>  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 3 3 3 3 1 1 3 ...\n#>  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nplot_missing(analytic.data.imputation1)\n\n\n\n\n\nShow the codeCreateTableOne(data = analytic.data.imputation1, includeNA = TRUE,\n               vars = vars)\n#>                          \n#>                           Overall       \n#>   n                         5769        \n#>   systolic (mean (SD))    122.98 (18.06)\n#>   smoke (%)                             \n#>      Every day              2187 (37.9) \n#>      Some days               517 ( 9.0) \n#>      Not at all             3065 (53.1) \n#>   diastolic (mean (SD))    70.25 (11.80)\n#>   race (%)                              \n#>      Mexican American        767 (13.3) \n#>      Non-Hispanic Black     1177 (20.4) \n#>      Non-Hispanic White     2472 (42.8) \n#>      Other Hispanic          508 ( 8.8) \n#>      Other race              845 (14.6) \n#>   age.centred (mean (SD))   0.00 (17.56)\n#>   gender = Female (%)       3011 (52.2) \n#>   marital (%)                           \n#>      Married                3383 (58.6) \n#>      Never married          1113 (19.3) \n#>      Previously married     1273 (22.1) \n#>   alcohol (mean (SD))       2.62 (2.28)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\n\nShow the codefit23i <- glm(diastolic ~ gender+age.centred+race+marital+systolic+smoke+alcohol, data=analytic.data.imputation1)\npublish(fit23i)\n#>     Variable              Units Coefficient         CI.95     p-value \n#>  (Intercept)                          38.74 [36.37;41.11]     < 1e-04 \n#>       gender               Male         Ref                           \n#>                          Female       -1.30 [-1.88;-0.72]     < 1e-04 \n#>  age.centred                          -0.12 [-0.14;-0.10]     < 1e-04 \n#>         race   Mexican American         Ref                           \n#>              Non-Hispanic Black        0.97  [-0.04;1.98]   0.0606987 \n#>              Non-Hispanic White        0.68  [-0.21;1.57]   0.1337626 \n#>                  Other Hispanic        0.91  [-0.32;2.13]   0.1471629 \n#>                      Other race        2.00   [0.93;3.08]   0.0002535 \n#>      marital            Married         Ref                           \n#>                   Never married       -2.43 [-3.24;-1.63]     < 1e-04 \n#>              Previously married       -0.12  [-0.87;0.62]   0.7450923 \n#>     systolic                           0.26   [0.24;0.28]     < 1e-04 \n#>        smoke          Every day         Ref                           \n#>                       Some days       -0.17  [-1.21;0.88]   0.7572027 \n#>                      Not at all       -0.19  [-0.80;0.42]   0.5387956 \n#>      alcohol                          -0.02  [-0.15;0.10]   0.7070437\n\n\nWe see some changes in the estimates. After imputing compared to complete case analysis, any changes dramatic (e.g., changing conclusion)?\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\n\nShow the coderequire(jtools)\nrequire(ggstance)\nrequire(broom.mixed)\nrequire(huxtable)\nexport_summs(fit23, fit23i)\n\n\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n(Intercept)\n30.92 ***\n38.74 ***\n\n\n\n(2.45)   \n(1.21)   \n\n\ngenderFemale\n-0.35    \n-1.30 ***\n\n\n\n(0.60)   \n(0.30)   \n\n\nage.centred\n-0.14 ***\n-0.12 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nraceNon-Hispanic Black\n1.45    \n0.97    \n\n\n\n(1.11)   \n(0.52)   \n\n\nraceNon-Hispanic White\n0.60    \n0.68    \n\n\n\n(0.96)   \n(0.45)   \n\n\nraceOther Hispanic\n1.07    \n0.91    \n\n\n\n(1.30)   \n(0.63)   \n\n\nraceOther race\n2.03    \n2.00 ***\n\n\n\n(1.23)   \n(0.55)   \n\n\nmaritalNever married\n-2.93 ***\n-2.43 ***\n\n\n\n(0.79)   \n(0.41)   \n\n\nmaritalPreviously married\n0.45    \n-0.12    \n\n\n\n(0.72)   \n(0.38)   \n\n\nsystolic\n0.31 ***\n0.26 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nsmokeSome days\n-0.42    \n-0.17    \n\n\n\n(0.98)   \n(0.53)   \n\n\nsmokeNot at all\n0.02    \n-0.19    \n\n\n\n(0.65)   \n(0.31)   \n\n\nalcohol\n0.17    \n-0.02    \n\n\n\n(0.11)   \n(0.07)   \n\n\nN\n1516       \n5769       \n\n\nAIC\n11545.85    \n43965.77    \n\n\nBIC\n11620.38    \n44059.01    \n\n\nPseudo R2\n0.19    \n0.15    \n\n *** p < 0.001;  ** p < 0.01;  * p < 0.05.\n\n\nShow the codeplot_summs(fit23, fit23i)\n\n\n\nShow the code# plot_summs(fit23, fit23i, plot.distributions = TRUE)"
  },
  {
    "objectID": "researchquestion2b.html#exercise-try-yourself",
    "href": "researchquestion2b.html#exercise-try-yourself",
    "title": "Predictive question-2b",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nIn this lab, we have done multiple steps that could be improved. One of them was single imputation by random sampling. What other ad hoc method you could use to impute the factor variables?"
  },
  {
    "objectID": "researchquestion2b.html#references",
    "href": "researchquestion2b.html#references",
    "title": "Predictive question-2b",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54."
  },
  {
    "objectID": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "href": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "title": "Causal question-1",
    "section": "Naive Analysis of combined 3 cycles",
    "text": "Naive Analysis of combined 3 cycles\nIn the current analysis, we will simply consider all of the variables under consideration as ‘confounders’, and include in our analysis. Later we will perform a refined analysis.\nSummary of the analytic data\nIncluding missing values\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic <- c123sub3\ndim(analytic)\n#> [1] 241380     17\n\nrequire(\"tableone\")\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, includeNA = TRUE)\n#>                       \n#>                        Overall       \n#>   n                    241380        \n#>   CVD = event (%)        7044 ( 2.9) \n#>   age (%)                            \n#>      20-39 years       108161 (44.8) \n#>      40-49 years        59690 (24.7) \n#>      50-59 years        52685 (21.8) \n#>      60-64 years        20844 ( 8.6) \n#>   sex = Male (%)       114104 (47.3) \n#>   income (%)                         \n#>      $29,999 or less    48005 (19.9) \n#>      $30,000-$49,999    49496 (20.5) \n#>      $50,000-$79,999    61093 (25.3) \n#>      $80,000 or more    57056 (23.6) \n#>      NA                 25730 (10.7) \n#>   race (%)                           \n#>      Non-white          25840 (10.7) \n#>      White             210307 (87.1) \n#>      NA                  5233 ( 2.2) \n#>   bmicat (%)                         \n#>      Normal            103378 (42.8) \n#>      Overweight        120423 (49.9) \n#>      Underweight         8964 ( 3.7) \n#>      NA                  8615 ( 3.6) \n#>   phyact (%)                         \n#>      Active             57033 (23.6) \n#>      Inactive          117516 (48.7) \n#>      Moderate           60164 (24.9) \n#>      NA                  6667 ( 2.8) \n#>   smoke (%)                          \n#>      Current smoker     71321 (29.5) \n#>      Former smoker      97845 (40.5) \n#>      Never smoker       71397 (29.6) \n#>      NA                   817 ( 0.3) \n#>   fruit (%)                          \n#>      0-3 daily serving  56256 (23.3) \n#>      4-6 daily serving  96177 (39.8) \n#>      6+ daily serving   45861 (19.0) \n#>      NA                 43086 (17.8) \n#>   painmed (%)                        \n#>      No                 11141 ( 4.6) \n#>      Yes                25743 (10.7) \n#>      NA                204496 (84.7) \n#>   ht (%)                             \n#>      No                213432 (88.4) \n#>      Yes                27592 (11.4) \n#>      NA                   356 ( 0.1) \n#>   copd (%)                           \n#>      No                192608 (79.8) \n#>      Yes                 1353 ( 0.6) \n#>      NA                 47419 (19.6) \n#>   diab (%)                           \n#>      No                232486 (96.3) \n#>      Yes                 8811 ( 3.7) \n#>      NA                    83 ( 0.0) \n#>   edu (%)                            \n#>      < 2ndary           37775 (15.6) \n#>      2nd grad.          44376 (18.4) \n#>      Other 2nd grad.    19273 ( 8.0) \n#>      Post-2nd grad.    136031 (56.4) \n#>      NA                  3925 ( 1.6)\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control        OA            p      test\n#>   n                    221029         20351                    \n#>   CVD = event (%)        5429 ( 2.5)   1615 ( 7.9)  <0.001     \n#>   age (%)                                           <0.001     \n#>      20-39 years       106003 (48.0)   2158 (10.6)             \n#>      40-49 years        55569 (25.1)   4121 (20.2)             \n#>      50-59 years        43706 (19.8)   8979 (44.1)             \n#>      60-64 years        15751 ( 7.1)   5093 (25.0)             \n#>   sex = Male (%)       107729 (48.7)   6375 (31.3)  <0.001     \n#>   income (%)                                        <0.001     \n#>      $29,999 or less    42019 (19.0)   5986 (29.4)             \n#>      $30,000-$49,999    45090 (20.4)   4406 (21.7)             \n#>      $50,000-$79,999    56754 (25.7)   4339 (21.3)             \n#>      $80,000 or more    53637 (24.3)   3419 (16.8)             \n#>      NA                 23529 (10.6)   2201 (10.8)             \n#>   race (%)                                          <0.001     \n#>      Non-white          24681 (11.2)   1159 ( 5.7)             \n#>      White             191513 (86.6)  18794 (92.3)             \n#>      NA                  4835 ( 2.2)    398 ( 2.0)             \n#>   bmicat (%)                                        <0.001     \n#>      Normal             96697 (43.7)   6681 (32.8)             \n#>      Overweight        107871 (48.8)  12552 (61.7)             \n#>      Underweight         8490 ( 3.8)    474 ( 2.3)             \n#>      NA                  7971 ( 3.6)    644 ( 3.2)             \n#>   phyact (%)                                        <0.001     \n#>      Active             52942 (24.0)   4091 (20.1)             \n#>      Inactive          106580 (48.2)  10936 (53.7)             \n#>      Moderate           55222 (25.0)   4942 (24.3)             \n#>      NA                  6285 ( 2.8)    382 ( 1.9)             \n#>   smoke (%)                                         <0.001     \n#>      Current smoker     65398 (29.6)   5923 (29.1)             \n#>      Former smoker      88210 (39.9)   9635 (47.3)             \n#>      Never smoker       66663 (30.2)   4734 (23.3)             \n#>      NA                   758 ( 0.3)     59 ( 0.3)             \n#>   fruit (%)                                         <0.001     \n#>      0-3 daily serving  52140 (23.6)   4116 (20.2)             \n#>      4-6 daily serving  87951 (39.8)   8226 (40.4)             \n#>      6+ daily serving   41606 (18.8)   4255 (20.9)             \n#>      NA                 39332 (17.8)   3754 (18.4)             \n#>   painmed (%)                                       <0.001     \n#>      No                 10624 ( 4.8)    517 ( 2.5)             \n#>      Yes                23084 (10.4)   2659 (13.1)             \n#>      NA                187321 (84.7)  17175 (84.4)             \n#>   ht (%)                                            <0.001     \n#>      No                198550 (89.8)  14882 (73.1)             \n#>      Yes                22142 (10.0)   5450 (26.8)             \n#>      NA                   337 ( 0.2)     19 ( 0.1)             \n#>   copd (%)                                          <0.001     \n#>      No                173224 (78.4)  19384 (95.2)             \n#>      Yes                  938 ( 0.4)    415 ( 2.0)             \n#>      NA                 46867 (21.2)    552 ( 2.7)             \n#>   diab (%)                                          <0.001     \n#>      No                213910 (96.8)  18576 (91.3)             \n#>      Yes                 7046 ( 3.2)   1765 ( 8.7)             \n#>      NA                    73 ( 0.0)     10 ( 0.0)             \n#>   edu (%)                                           <0.001     \n#>      < 2ndary           32884 (14.9)   4891 (24.0)             \n#>      2nd grad.          40950 (18.5)   3426 (16.8)             \n#>      Other 2nd grad.    17808 ( 8.1)   1465 ( 7.2)             \n#>      Post-2nd grad.    125772 (56.9)  10259 (50.4)             \n#>      NA                  3615 ( 1.6)    310 ( 1.5)\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLet us investigate why pain medication has so much missing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional content respondent (cycle 3.1):\n\n\n\n\n\nIn cycle 2.1, only 21,755 out of 134,072 responded to optional medication component.\nComplete case analysis\n\nShow the codedim(c123sub3)\n#> [1] 241380     17\nanalytic2 <- as.data.frame(na.omit(c123sub3))\ndim(analytic2)\n#> [1] 21623    17\n\n\ntab1 <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, includeNA = TRUE)\nprint(tab1, showAllLevels = TRUE)\n#>              \n#>               level             Overall      \n#>   n                             21623        \n#>   CVD (%)     0 event           20917 (96.7) \n#>               event               706 ( 3.3) \n#>   age (%)     20-39 years        7119 (32.9) \n#>               40-49 years        7024 (32.5) \n#>               50-59 years        5457 (25.2) \n#>               60-64 years        2023 ( 9.4) \n#>   sex (%)     Female            10982 (50.8) \n#>               Male              10641 (49.2) \n#>   income (%)  $29,999 or less    4054 (18.7) \n#>               $30,000-$49,999    4461 (20.6) \n#>               $50,000-$79,999    6600 (30.5) \n#>               $80,000 or more    6508 (30.1) \n#>   race (%)    Non-white          2488 (11.5) \n#>               White             19135 (88.5) \n#>   bmicat (%)  Normal             8993 (41.6) \n#>               Overweight        11739 (54.3) \n#>               Underweight         891 ( 4.1) \n#>   phyact (%)  Active             5502 (25.4) \n#>               Inactive          10495 (48.5) \n#>               Moderate           5626 (26.0) \n#>   smoke (%)   Current smoker     5887 (27.2) \n#>               Former smoker      9368 (43.3) \n#>               Never smoker       6368 (29.5) \n#>   fruit (%)   0-3 daily serving  5806 (26.9) \n#>               4-6 daily serving 10730 (49.6) \n#>               6+ daily serving   5087 (23.5) \n#>   painmed (%) No                 6197 (28.7) \n#>               Yes               15426 (71.3) \n#>   ht (%)      No                19014 (87.9) \n#>               Yes                2609 (12.1) \n#>   copd (%)    No                21475 (99.3) \n#>               Yes                 148 ( 0.7) \n#>   diab (%)    No                20760 (96.0) \n#>               Yes                 863 ( 4.0) \n#>   edu (%)     < 2ndary           2998 (13.9) \n#>               2nd grad.          4605 (21.3) \n#>               Other 2nd grad.    1509 ( 7.0) \n#>               Post-2nd grad.    12511 (57.9)\ntab1b <- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, strata = \"OA\", includeNA = TRUE)\nprint(tab1b, showAllLevels = TRUE)\n#>              Stratified by OA\n#>               level             Control       OA           p      test\n#>   n                             19459         2164                    \n#>   CVD (%)     0 event           18917 (97.2)  2000 (92.4)  <0.001     \n#>               event               542 ( 2.8)   164 ( 7.6)             \n#>   age (%)     20-39 years        6915 (35.5)   204 ( 9.4)  <0.001     \n#>               40-49 years        6515 (33.5)   509 (23.5)             \n#>               50-59 years        4504 (23.1)   953 (44.0)             \n#>               60-64 years        1525 ( 7.8)   498 (23.0)             \n#>   sex (%)     Female             9521 (48.9)  1461 (67.5)  <0.001     \n#>               Male               9938 (51.1)   703 (32.5)             \n#>   income (%)  $29,999 or less    3413 (17.5)   641 (29.6)  <0.001     \n#>               $30,000-$49,999    3968 (20.4)   493 (22.8)             \n#>               $50,000-$79,999    6023 (31.0)   577 (26.7)             \n#>               $80,000 or more    6055 (31.1)   453 (20.9)             \n#>   race (%)    Non-white          2370 (12.2)   118 ( 5.5)  <0.001     \n#>               White             17089 (87.8)  2046 (94.5)             \n#>   bmicat (%)  Normal             8277 (42.5)   716 (33.1)  <0.001     \n#>               Overweight        10356 (53.2)  1383 (63.9)             \n#>               Underweight         826 ( 4.2)    65 ( 3.0)             \n#>   phyact (%)  Active             4986 (25.6)   516 (23.8)   0.190     \n#>               Inactive           9417 (48.4)  1078 (49.8)             \n#>               Moderate           5056 (26.0)   570 (26.3)             \n#>   smoke (%)   Current smoker     5247 (27.0)   640 (29.6)  <0.001     \n#>               Former smoker      8363 (43.0)  1005 (46.4)             \n#>               Never smoker       5849 (30.1)   519 (24.0)             \n#>   fruit (%)   0-3 daily serving  5290 (27.2)   516 (23.8)  <0.001     \n#>               4-6 daily serving  9686 (49.8)  1044 (48.2)             \n#>               6+ daily serving   4483 (23.0)   604 (27.9)             \n#>   painmed (%) No                 5859 (30.1)   338 (15.6)  <0.001     \n#>               Yes               13600 (69.9)  1826 (84.4)             \n#>   ht (%)      No                17356 (89.2)  1658 (76.6)  <0.001     \n#>               Yes                2103 (10.8)   506 (23.4)             \n#>   copd (%)    No                19359 (99.5)  2116 (97.8)  <0.001     \n#>               Yes                 100 ( 0.5)    48 ( 2.2)             \n#>   diab (%)    No                18751 (96.4)  2009 (92.8)  <0.001     \n#>               Yes                 708 ( 3.6)   155 ( 7.2)             \n#>   edu (%)     < 2ndary           2527 (13.0)   471 (21.8)  <0.001     \n#>               2nd grad.          4173 (21.4)   432 (20.0)             \n#>               Other 2nd grad.    1364 ( 7.0)   145 ( 6.7)             \n#>               Post-2nd grad.    11395 (58.6)  1116 (51.6)"
  },
  {
    "objectID": "researchquestion3.html#save-data-for-later",
    "href": "researchquestion3.html#save-data-for-later",
    "title": "Causal question-1",
    "section": "Save data for later",
    "text": "Save data for later\n\nShow the codesave(analytic, analytic2, cc123a, file = \"Data/researchquestion/OA123CVD.RData\")"
  },
  {
    "objectID": "researchquestion3.html#references",
    "href": "researchquestion3.html#references",
    "title": "Causal question-1",
    "section": "References",
    "text": "References\n\n\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "researchquestion4.html",
    "href": "researchquestion4.html",
    "title": "Causal question-2",
    "section": "",
    "text": "Working with a causal question using NHANES\nWe are interested in exploring the relationship between diabetes (binary exposure variable defined as whether the doctor ever told the participant has diabetes) and cholesterol (binary outcome variable defined as whether total cholesterol is more than 200 mg/dL). Below is the PICOT:\n\n\nPICOT element\nDescription\n\n\n\nP\nUS adults\n\n\nI\nDiabetes\n\n\nC\nNo diabetes\n\n\nO\nTotal cholesterol > 200 mg/dL\n\n\nT\n2017–2018\n\n\n\nFirst, we will prepare the analytic dataset from NHANES 2017–2018.\nSecond, we will work with subset of data to assess the association between diabetes and cholesterol, and to get proper SE and 95% CI for the estimate. We emphasize the correct usage of the survey’s design features (correct handling of survey design elements, such as stratification, clustering, and weighting) to obtain accurate population-level estimates.\n\nShow the code# Load required packages\nrequire(SASxport)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(nhanesA)\nrequire(survey)\nrequire(Publish)\nrequire(jtools)\n\n\nSteps for creating analytic dataset\nWe will combine multiple components (e.g., demographic, blood pressure) using the unique identifier to create our analytic dataset.\n\n\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\n\n\n\nDownload and Subsetting to retain only the useful variables\nSearch literature for the relevant variables, and then see if some of them are available in the NHANES data.\n\n\nPeters, Fabian, and Levy (2014)\nAn an example, let us assume that variables listed in the following figures are known to be useful. Then we will try to indentify, in which NHANES component we have these variables.\n\n\nRefer to the earlier chapter to get a more detailed understanding of how we search for variables within NHANES.\n\n\n\n\n\n\n\nNHANES Data Components:\n\nDemographic (variables like age, gender, income, etc.)\nBlood Pressure (Diastolic and Systolic pressure)\nBody Measures (BMI, Waist Circumference, etc.)\nSmoking Status (Current smoker or not)\nCholesterol (Total cholesterol in different units)\nBiochemistry Profile (Triglycerides, Uric acid, etc.)\nPhysical Activity (Vigorous work and recreational activities)\nDiabetes (Whether the respondent has been told by a doctor that they have diabetes)\n\nDemographic component:\n\nShow the codedemo <- nhanes('DEMO_J') # Both males and females 0 YEARS - 150 YEARS\ndemo <- demo[c(\"SEQN\", # Respondent sequence number\n                 \"RIAGENDR\", # gender\n                 \"RIDAGEYR\", # Age in years at screening\n                 \"DMDBORN4\", # Country of birth\n                 \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                 \"DMDEDUC3\", # Education level - Children/Youth 6-19\n                 \"DMDEDUC2\", # Education level - Adults 20+\n                 \"DMDMARTL\", # Marital status: 20 YEARS - 150 YEARS\n                 \"INDHHIN2\", # Total household income\n                 \"WTMEC2YR\", \"SDMVPSU\", \"SDMVSTRA\")]\ndemo_vars <- names(demo) # nhanesTableVars('DEMO', 'DEMO_J', namesonly=TRUE)\ndemo1 <- nhanesTranslate('DEMO_J', demo_vars, data=demo)\n#> Translated columns: RIAGENDR DMDBORN4 RIDRETH3 DMDEDUC3 DMDEDUC2 DMDMARTL INDHHIN2\n\n\nBlood pressure component:\n\nShow the codebpx <- nhanes('BPX_J')\nbpx <- bpx[c(\"SEQN\", # Respondent sequence number\n             \"BPXDI1\", #Diastolic: Blood pres (1st rdg) mm Hg\n             \"BPXSY1\" # Systolic: Blood pres (1st rdg) mm Hg\n             )]\nbpx_vars <- names(bpx) \nbpx1 <- nhanesTranslate('BPX_J', bpx_vars, data=bpx)\n#> Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx): No columns were\n#> translated\n\n\nBody measure component:\n\nShow the codebmi <- nhanes('BMX_J')\nbmi <- bmi[c(\"SEQN\", # Respondent sequence number\n               \"BMXWT\", # Weight (kg) \n               \"BMXHT\", # Standing Height (cm)\n               \"BMXBMI\", # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\n               #\"BMDBMIC\", # BMI Category - Children/Youth # 2 YEARS - 19 YEARS\n               \"BMXWAIST\" # Waist Circumference (cm): 2 YEARS - 150 YEARS\n               )]\nbmi_vars <- names(bmi) \nbmi1 <- nhanesTranslate('BMX_J', bmi_vars, data=bmi)\n#> Warning in nhanesTranslate(\"BMX_J\", bmi_vars, data = bmi): No columns were\n#> translated\n\n\nSmoking component:\n\nShow the codesmq <- nhanes('SMQ_J')\nsmq <- smq[c(\"SEQN\", # Respondent sequence number\n               \"SMQ040\" # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\n               )]\nsmq_vars <- names(smq) \nsmq1 <- nhanesTranslate('SMQ_J', smq_vars, data=smq)\n#> Translated columns: SMQ040\n\n\n\nShow the code# alq <- nhanes('ALQ_J')\n# alq <- alq[c(\"SEQN\", # Respondent sequence number\n#                \"ALQ130\" # Avg # alcoholic drinks/day - past 12 mos\n#                # 18 YEARS - 150 YEARS\n#                )]\n# alq_vars <- names(alq) \n# alq1 <- nhanesTranslate('ALQ_J', alq_vars, data=alq)\n\n\nCholesterol component:\n\nShow the codechl <- nhanes('TCHOL_J') # 6 YEARS - 150 YEARS\nchl <- chl[c(\"SEQN\", # Respondent sequence number\n               \"LBXTC\", # Total Cholesterol (mg/dL)\n               \"LBDTCSI\" # Total Cholesterol (mmol/L)\n               )]\nchl_vars <- names(chl) \nchl1 <- nhanesTranslate('TCHOL_J', chl_vars, data=chl)\n#> Warning in nhanesTranslate(\"TCHOL_J\", chl_vars, data = chl): No columns were\n#> translated\n\n\nBiochemistry Profile component:\n\nShow the codetri <- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\ntri <- tri[c(\"SEQN\", # Respondent sequence number\n               \"LBXSTR\", # Triglycerides, refrig serum (mg/dL)\n               \"LBXSUA\", # Uric acid\n               \"LBXSTP\", # total Protein (g/dL)\n               \"LBXSTB\", # Total Bilirubin (mg/dL)\n               \"LBXSPH\", # Phosphorus (mg/dL)\n               \"LBXSNASI\", # Sodium (mmol/L)\n               \"LBXSKSI\", # Potassium (mmol/L)\n               \"LBXSGB\", # Globulin (g/dL)\n               \"LBXSCA\" # Total Calcium (mg/dL)\n               )]\ntri_vars <- names(tri) \ntri1 <- nhanesTranslate('BIOPRO_J', tri_vars, data=tri)\n#> Warning in nhanesTranslate(\"BIOPRO_J\", tri_vars, data = tri): No columns were\n#> translated\n\n\nPhysical activity component:\n\nShow the codepaq <- nhanes('PAQ_J')\npaq <- paq[c(\"SEQN\", # Respondent sequence number\n               \"PAQ605\", # Vigorous work activity \n               \"PAQ650\" # Vigorous recreational activities\n               )]\npaq_vars <- names(paq) \npaq1 <- nhanesTranslate('PAQ_J', paq_vars, data=paq)\n#> Translated columns: PAQ605 PAQ650\n\n\nDiabetes component:\n\nShow the codediq <- nhanes('DIQ_J')\ndiq <- diq[c(\"SEQN\", # Respondent sequence number\n               \"DIQ010\" # Doctor told you have diabetes\n               )]\ndiq_vars <- names(diq) \ndiq1 <- nhanesTranslate('DIQ_J', diq_vars, data=diq)\n#> Translated columns: DIQ010\n\n\nMerging all the datasets\n\n\n\n\n\n\nTip\n\n\n\nWe can use the merge or Reduce function to combine the datasets\n\n\n\nShow the codeanalytic.data7 <- Reduce(function(x,y) merge(x,y,by=\"SEQN\",all=TRUE) ,\n       list(demo1,bpx1,bmi1,smq1,chl1,tri1,paq1,diq1))\ndim(analytic.data7)\n#> [1] 9254   33\n\n\n\n\nAll these datasets are merged into one analytic dataset using the SEQN as the key. This can be done either all at once using the Reduce function or one by one (using merge once at a time).\n\nShow the code# Merging one by one\n# analytic.data0 <- merge(demo1, bpx1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data1 <- merge(analytic.data0, bmi1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data2 <- merge(analytic.data1, smq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data3 <- merge(analytic.data2, alq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data4 <- merge(analytic.data3, chl1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data5 <- merge(analytic.data4, tri1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data6 <- merge(analytic.data5, paq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data7 <- merge(analytic.data6, diq1, by = c(\"SEQN\"), all=TRUE)\n# dim(analytic.data7)\n\n\nCheck Target population and avoid zero-cell cross-tabulation\n\n\nThe dataset is then filtered to only include adults (20 years and older) and avoid zero-cell cross-tabulation.\nSee that marital status variable was restricted to 20 YEARS - 150 YEARS.\n\nShow the codestr(analytic.data7)\n#> 'data.frame':    9254 obs. of  33 variables:\n#>  $ SEQN    : num  93703 93704 93705 93706 93707 ...\n#>  $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 2 1 2 1 1 2 2 2 1 1 ...\n#>  $ RIDAGEYR: num  2 2 66 18 13 66 75 0 56 18 ...\n#>  $ DMDBORN4: Factor w/ 4 levels \"Born in 50 US states or Washingt\",..: 1 1 1 1 1 2 1 1 2 2 ...\n#>  $ RIDRETH3: Factor w/ 6 levels \"Mexican American\",..: 5 3 4 5 6 5 4 3 5 1 ...\n#>  $ DMDEDUC3: Factor w/ 17 levels \"Never attended / kindergarten on\",..: NA NA NA 16 7 NA NA NA NA 13 ...\n#>  $ DMDEDUC2: Factor w/ 7 levels \"Less than 9th grade\",..: NA NA 2 NA NA 1 4 NA 5 NA ...\n#>  $ DMDMARTL: Factor w/ 7 levels \"Married\",\"Widowed\",..: NA NA 3 NA NA 1 2 NA 1 NA ...\n#>  $ INDHHIN2: Factor w/ 16 levels \"$ 0 to $ 4,999\",..: 14 14 3 NA 10 6 2 14 14 4 ...\n#>  $ WTMEC2YR: num  8540 42567 8338 8723 7065 ...\n#>  $ SDMVPSU : num  2 1 2 2 1 2 1 1 2 2 ...\n#>  $ SDMVSTRA: num  145 143 145 134 138 138 136 134 134 147 ...\n#>  $ BPXDI1  : num  NA NA NA 74 38 NA 66 NA 68 68 ...\n#>  $ BPXSY1  : num  NA NA NA 112 128 NA 120 NA 108 112 ...\n#>  $ BMXWT   : num  13.7 13.9 79.5 66.3 45.4 53.5 88.8 10.2 62.1 58.9 ...\n#>  $ BMXHT   : num  88.6 94.2 158.3 175.7 158.4 ...\n#>  $ BMXBMI  : num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#>  $ BMXWAIST: num  48.2 50 101.8 79.3 64.1 ...\n#>  $ SMQ040  : Factor w/ 3 levels \"Every day\",\"Some days\",..: NA NA 3 NA NA NA 1 NA NA 2 ...\n#>  $ LBXTC   : num  NA NA 157 148 189 209 176 NA 238 182 ...\n#>  $ LBDTCSI : num  NA NA 4.06 3.83 4.89 5.4 4.55 NA 6.15 4.71 ...\n#>  $ LBXSTR  : num  NA NA 95 92 110 72 132 NA 59 124 ...\n#>  $ LBXSUA  : num  NA NA 5.8 8 5.5 4.5 6.2 NA 4.2 5.8 ...\n#>  $ LBXSTP  : num  NA NA 7.3 7.1 8 7.1 7 NA 7.1 8.1 ...\n#>  $ LBXSTB  : num  NA NA 0.6 0.7 0.7 0.5 0.3 NA 0.3 0.8 ...\n#>  $ LBXSPH  : num  NA NA 4 4 4.3 3.3 3.5 NA 3.4 5.1 ...\n#>  $ LBXSNASI: num  NA NA 141 144 137 144 141 NA 140 141 ...\n#>  $ LBXSKSI : num  NA NA 4 4.4 3.3 4.4 4.1 NA 4.9 4.3 ...\n#>  $ LBXSGB  : num  NA NA 2.9 2.7 2.8 3.2 3.3 NA 3.1 3.3 ...\n#>  $ LBXSCA  : num  NA NA 9.2 9.6 10.1 9.5 9.9 NA 9.4 9.6 ...\n#>  $ PAQ605  : Factor w/ 3 levels \"Yes\",\"No\",\"Don't know\": NA NA 2 2 NA 2 2 NA 2 1 ...\n#>  $ PAQ650  : Factor w/ 2 levels \"Yes\",\"No\": NA NA 2 2 NA 2 2 NA 1 1 ...\n#>  $ DIQ010  : Factor w/ 4 levels \"Yes\",\"No\",\"Borderline\",..: 2 2 2 2 2 3 2 NA 2 2 ...\nhead(analytic.data7)\n\n\n\n  \n\n\nShow the codesummary(analytic.data7$RIDAGEYR)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   11.00   31.00   34.33   58.00   80.00\n\n\n\nShow the codedim(analytic.data7)\n#> [1] 9254   33\nanalytic.data8 <- analytic.data7\nanalytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR < 20] <- NA\n#analytic.data8 <- subset(analytic.data7, RIDAGEYR >= 20)\ndim(analytic.data8)\n#> [1] 9254   33\n\n\nGet rid of variables where target was less than 20 years of age accordingly.\n\nShow the codeanalytic.data8$DMDEDUC3 <- NULL # not relevant for adults\n#analytic.data8$BMDBMIC <- NULL # not relevant for adults\n\n\nGet rid of invalid responses\n\n\nVariables that have “Don’t Know” or “Refused” as responses are set to NA, effectively getting rid of invalid responses.\n\nShow the codefactor.names <- c(\"RIAGENDR\",\"DMDBORN4\",\"RIDRETH3\",\n                  \"DMDEDUC2\",\"DMDMARTL\",\"INDHHIN2\", \n                  \"SMQ040\", \"PAQ605\", \"PAQ650\", \"DIQ010\")\nnumeric.names <- c(\"SEQN\",\"RIDAGEYR\",\"WTMEC2YR\",\n                   \"SDMVPSU\", \"SDMVSTRA\",\n                   \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"BMXHT\",\n                   \"BMXBMI\", \"BMXWAIST\",\n                   \"ALQ130\", \"LBXTC\", \"LBDTCSI\", \n                   \"LBXSTR\", \"LBXSUA\", \"LBXSTP\", \"LBXSTB\", \n                   \"LBXSPH\", \"LBXSNASI\", \"LBXSKSI\",\n                   \"LBXSGB\",\"LBXSCA\")\nanalytic.data8[factor.names] <- apply(X = analytic.data8[factor.names], \n                                      MARGIN = 2, FUN = as.factor)\n# analytic.data8[numeric.names] <- apply(X = analytic.data8[numeric.names], \n#                                        MARGIN = 2, FUN = \n#                                          function (x) as.numeric(as.character(x)))\n\n\n\nShow the codeanalytic.data9 <- analytic.data8\nanalytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == \"Don't Know\"] <- NA\n#analytic.data9 <- subset(analytic.data8, DMDBORN4 != \"Don't Know\")\ndim(analytic.data9)\n#> [1] 9254   32\n\nanalytic.data10 <- analytic.data9\nanalytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == \"Don't Know\"] <- NA\n#analytic.data10 <- subset(analytic.data9, DMDEDUC2 != \"Don't Know\")\ndim(analytic.data10)\n#> [1] 9254   32\n\nanalytic.data11 <- analytic.data10\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Don't Know\"] <- NA\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Refused\"] <- NA\n# analytic.data11 <- subset(analytic.data10, DMDMARTL != \"Don't Know\" & DMDMARTL != \"Refused\")\ndim(analytic.data11)\n#> [1] 9254   32\n\n\nanalytic.data12 <- analytic.data11\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Don't Know\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Refused\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Under $20,000\"] <- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"$20,000 and Over\"] <- NA\n# analytic.data12 <- subset(analytic.data11, INDHHIN2 != \"Don't know\" & INDHHIN2 !=  \"Refused\" & INDHHIN2 != \"Under $20,000\" & INDHHIN2 != \"$20,000 and Over\" )\ndim(analytic.data12)\n#> [1] 9254   32\n\n#analytic.data11 <- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )\n#dim(analytic.data11) # this are listed as NA anyway\n\nanalytic.data13 <- analytic.data12\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Don't know\"] <- NA\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Refused\"] <- NA\n# analytic.data13 <- subset(analytic.data12, PAQ605 != \"Don't know\" & PAQ605 != \"Refused\")\ndim(analytic.data13)\n#> [1] 9254   32\n\nanalytic.data14 <- analytic.data13\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Don't know\"] <- NA\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Refused\"] <- NA\n# analytic.data14 <- subset(analytic.data13, PAQ650 != \"Don't Know\" & PAQ650 != \"Refused\")\ndim(analytic.data14)\n#> [1] 9254   32\n\nanalytic.data15 <- analytic.data14\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Don't know\"] <- NA\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Refused\"] <- NA\n# analytic.data15 <- subset(analytic.data14, DIQ010 != \"Don't Know\" & DIQ010 != \"Refused\")\ndim(analytic.data15)\n#> [1] 9254   32\n\n\n# analytic.data15$ALQ130[analytic.data15$ALQ130 > 100] <- NA\n# summary(analytic.data15$ALQ130)\ntable(analytic.data15$SMQ040,useNA = \"always\")\n#> \n#>  Every day Not at all  Some days       <NA> \n#>        805       1338        216       6895\ntable(analytic.data15$PAQ605,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4461 1389 3404\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4422 1434 3398\n\n\nRecode values\nLet us recode the variables using the recode function:\n\nShow the coderequire(car)\nanalytic.data15$RIDRETH3 <- recode(analytic.data15$RIDRETH3, \n                            \"c('Mexican American','Other Hispanic')='Hispanic'; \n                            'Non-Hispanic White'='White'; \n                            'Non-Hispanic Black'='Black';\n                            c('Non-Hispanic Asian',\n                               'Other Race - Including Multi-Rac')='Other';\n                               else=NA\")\nanalytic.data15$DMDEDUC2 <- recode(analytic.data15$DMDEDUC2, \n                            \"c('Some college or AA degree',\n                             'College graduate or above')='College'; \n                            c('9-11th grade (Includes 12th grad', \n                              'High school graduate/GED or equi')\n                               ='High.School'; \n                            'Less than 9th grade'='School';\n                               else=NA\")\nanalytic.data15$DMDMARTL <- recode(analytic.data15$DMDMARTL, \n                            \"c('Divorced','Separated','Widowed')\n                                ='Previously.married'; \n                            c('Living with partner', 'Married')\n                                ='Married'; \n                            'Never married'='Never.married';\n                               else=NA\")\nanalytic.data15$INDHHIN2 <- recode(analytic.data15$INDHHIN2, \n                            \"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999', \n                                 '$10,000 to $14,999', '$15,000 to $19,999', \n                                 '$20,000 to $24,999')='<25k';\n                            c('$25,000 to $34,999', '$35,000 to $44,999', \n                                 '$45,000 to $54,999') = 'Between.25kto54k';\n                            c('$55,000 to $64,999', '$65,000 to $74,999',\n                                 '$75,000 to $99,999')='Between.55kto99k';\n                            '$100,000 and Over'= 'Over100k';\n                               else=NA\")\nanalytic.data15$SMQ040 <- recode(analytic.data15$SMQ040, \n                            \"'Every day'='Every.day';\n                            'Not at all'='Not.at.all';\n                            'Some days'='Some.days';\n                               else=NA\")\nanalytic.data15$DIQ010 <- recode(analytic.data15$DIQ010, \n                            \"'No'='No';\n                            c('Yes', 'Borderline')='Yes';\n                               else=NA\")\n\n\n\n\nData types for various variables are set correctly; for instance, factor variables are converted to factor data types, and numeric variables to numeric data types.\nCheck missingness\n\n\n\n\n\n\nTip\n\n\n\nWe can use the plot_missing function to plot the profile of missing values, e.g., the percentage of missing per variable\n\n\n\nShow the coderequire(DataExplorer)\nplot_missing(analytic.data15)\n\n\n\n\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nCheck data summaries\n\nShow the codenames(analytic.data15)\n#>  [1] \"SEQN\"     \"RIAGENDR\" \"RIDAGEYR\" \"DMDBORN4\" \"RIDRETH3\" \"DMDEDUC2\"\n#>  [7] \"DMDMARTL\" \"INDHHIN2\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BPXDI1\"  \n#> [13] \"BPXSY1\"   \"BMXWT\"    \"BMXHT\"    \"BMXBMI\"   \"BMXWAIST\" \"SMQ040\"  \n#> [19] \"LBXTC\"    \"LBDTCSI\"  \"LBXSTR\"   \"LBXSUA\"   \"LBXSTP\"   \"LBXSTB\"  \n#> [25] \"LBXSPH\"   \"LBXSNASI\" \"LBXSKSI\"  \"LBXSGB\"   \"LBXSCA\"   \"PAQ605\"  \n#> [31] \"PAQ650\"   \"DIQ010\"\nnames(analytic.data15) <- c(\"ID\", \"gender\", \"age\", \"born\", \"race\", \"education\", \n\"married\", \"income\", \"weight\", \"psu\", \"strata\", \"diastolicBP\", \n\"systolicBP\", \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \n\"cholesterol\", \"cholesterolM2\", \"triglycerides\", \n\"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \n\"potassium\", \"globulin\", \"calcium\", \"physical.work\", \n\"physical.recreational\",\"diabetes\")\nrequire(\"tableone\")\nCreateTableOne(data = analytic.data15, includeNA = TRUE)\n#>                                      \n#>                                       Overall            \n#>   n                                       9254           \n#>   ID (mean (SD))                      98329.50 (2671.54) \n#>   gender = Male (%)                       4557 (49.2)    \n#>   age (mean (SD))                        51.50 (17.81)   \n#>   born (%)                                               \n#>      Born in 50 US states or Washingt     7303 (78.9)    \n#>      Others                               1948 (21.1)    \n#>      Refused                                 2 ( 0.0)    \n#>      NA                                      1 ( 0.0)    \n#>   race (%)                                               \n#>      Black                                2115 (22.9)    \n#>      Hispanic                             2187 (23.6)    \n#>      Other                                1802 (19.5)    \n#>      White                                3150 (34.0)    \n#>   education (%)                                          \n#>      College                              3114 (33.7)    \n#>      High.School                          1963 (21.2)    \n#>      School                                479 ( 5.2)    \n#>      NA                                   3698 (40.0)    \n#>   married (%)                                            \n#>      Married                              3252 (35.1)    \n#>      Never.married                        1006 (10.9)    \n#>      Previously.married                   1305 (14.1)    \n#>      NA                                   3691 (39.9)    \n#>   income (%)                                             \n#>      <25k                                 1998 (21.6)    \n#>      Between.25kto54k                     2460 (26.6)    \n#>      Between.55kto99k                     1843 (19.9)    \n#>      Over100k                             1624 (17.5)    \n#>      NA                                   1329 (14.4)    \n#>   weight (mean (SD))                  34670.71 (43344.00)\n#>   psu (mean (SD))                         1.52 (0.50)    \n#>   strata (mean (SD))                    140.97 (4.20)    \n#>   diastolicBP (mean (SD))                67.84 (16.36)   \n#>   systolicBP (mean (SD))                121.33 (19.98)   \n#>   bodyweight (mean (SD))                 65.14 (32.89)   \n#>   bodyheight (mean (SD))                156.59 (22.26)   \n#>   bmi (mean (SD))                        26.58 (8.26)    \n#>   waist (mean (SD))                      89.93 (22.81)   \n#>   smoke (%)                                              \n#>      Every.day                             805 ( 8.7)    \n#>      Not.at.all                           1338 (14.5)    \n#>      Some.days                             216 ( 2.3)    \n#>      NA                                   6895 (74.5)    \n#>   cholesterol (mean (SD))               179.89 (40.60)   \n#>   cholesterolM2 (mean (SD))               4.65 (1.05)    \n#>   triglycerides (mean (SD))             137.44 (109.13)  \n#>   uric.acid (mean (SD))                   5.40 (1.48)    \n#>   protein (mean (SD))                     7.17 (0.44)    \n#>   bilirubin (mean (SD))                   0.46 (0.28)    \n#>   phosphorus (mean (SD))                  3.66 (0.59)    \n#>   sodium (mean (SD))                    140.32 (2.75)    \n#>   potassium (mean (SD))                   4.09 (0.36)    \n#>   globulin (mean (SD))                    3.09 (0.43)    \n#>   calcium (mean (SD))                     9.32 (0.37)    \n#>   physical.work (%)                                      \n#>      No                                   4461 (48.2)    \n#>      Yes                                  1389 (15.0)    \n#>      NA                                   3404 (36.8)    \n#>   physical.recreational (%)                              \n#>      No                                   4422 (47.8)    \n#>      Yes                                  1434 (15.5)    \n#>      NA                                   3398 (36.7)    \n#>   diabetes (%)                                           \n#>      No                                   7816 (84.5)    \n#>      Yes                                  1077 (11.6)    \n#>      NA                                    361 ( 3.9)\n\n\nCreate complete case data (for now)\n\nShow the codeanalytic.with.miss <- analytic.data15\nanalytic.with.miss$cholesterol.bin <- ifelse(analytic.with.miss$cholesterol <200, 1,0)\nanalytic <- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic)\n#> [1] 1562   33\n\n\nCreating Table 1 from the complete case data\n\nShow the coderequire(\"tableone\")\nCreateTableOne(data = analytic, includeNA = TRUE)\n#>                                  \n#>                                   Overall            \n#>   n                                   1562           \n#>   ID (mean (SD))                  98344.21 (2697.76) \n#>   gender = Male (%)                    959 (61.4)    \n#>   age (mean (SD))                    53.18 (17.18)   \n#>   born = Others (%)                    299 (19.1)    \n#>   race (%)                                           \n#>      Black                             324 (20.7)    \n#>      Hispanic                          284 (18.2)    \n#>      Other                             228 (14.6)    \n#>      White                             726 (46.5)    \n#>   education (%)                                      \n#>      College                           806 (51.6)    \n#>      High.School                       658 (42.1)    \n#>      School                             98 ( 6.3)    \n#>   married (%)                                        \n#>      Married                           921 (59.0)    \n#>      Never.married                     228 (14.6)    \n#>      Previously.married                413 (26.4)    \n#>   income (%)                                         \n#>      <25k                              484 (31.0)    \n#>      Between.25kto54k                  520 (33.3)    \n#>      Between.55kto99k                  331 (21.2)    \n#>      Over100k                          227 (14.5)    \n#>   weight (mean (SD))              48538.53 (54106.24)\n#>   psu (mean (SD))                     1.48 (0.50)    \n#>   strata (mean (SD))                141.18 (4.07)    \n#>   diastolicBP (mean (SD))            72.06 (14.17)   \n#>   systolicBP (mean (SD))            127.06 (19.11)   \n#>   bodyweight (mean (SD))             85.66 (22.41)   \n#>   bodyheight (mean (SD))            168.96 (9.30)    \n#>   bmi (mean (SD))                    29.96 (7.33)    \n#>   waist (mean (SD))                 102.98 (17.15)   \n#>   smoke (%)                                          \n#>      Every.day                         530 (33.9)    \n#>      Not.at.all                        903 (57.8)    \n#>      Some.days                         129 ( 8.3)    \n#>   cholesterol (mean (SD))           188.77 (43.51)   \n#>   cholesterolM2 (mean (SD))           4.88 (1.13)    \n#>   triglycerides (mean (SD))         154.71 (123.00)  \n#>   uric.acid (mean (SD))               5.62 (1.53)    \n#>   protein (mean (SD))                 7.09 (0.43)    \n#>   bilirubin (mean (SD))               0.46 (0.27)    \n#>   phosphorus (mean (SD))              3.53 (0.54)    \n#>   sodium (mean (SD))                140.14 (2.83)    \n#>   potassium (mean (SD))               4.10 (0.38)    \n#>   globulin (mean (SD))                3.03 (0.44)    \n#>   calcium (mean (SD))                 9.29 (0.37)    \n#>   physical.work = Yes (%)              476 (30.5)    \n#>   physical.recreational = Yes (%)      290 (18.6)    \n#>   diabetes = Yes (%)                   330 (21.1)    \n#>   cholesterol.bin (mean (SD))         0.63 (0.48)\n\n\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\nSaving data\n\nShow the code# getwd()\nsave(analytic.with.miss, analytic, file=\"Data/researchquestion/NHANES17.RData\")\n\n\nReferences\n\n\n\n\nPeters, Junenette L, M Patricia Fabian, and Jonathan I Levy. 2014. “Combined Impact of Lead, Cadmium, Polychlorinated Biphenyls and Non-Chemical Risk Factors on Blood Pressure in NHANES.” Environmental Research 132: 93–99."
  },
  {
    "objectID": "researchquestionF.html",
    "href": "researchquestionF.html",
    "title": "R functions (Q)",
    "section": "",
    "text": "The list of new R functions introduced in this Research question lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n as.data.frame \n    base \n    To force an object to a data frame \n  \n\n as.formula \n    base/stats \n    To specify a model formula, e.g., formula for an outcome model \n  \n\n confint \n    base/stats \n    To estimate the confidence interval for model parameters \n  \n\n degf \n    survey \n    To see the degrees of freedom for a survey design object \n  \n\n describe \n    DescTools \n    To see the summary statistics of variables \n  \n\n exp \n    base \n    Exponentials \n  \n\n lapply \n    base \n    To apply a function over a list, e.g., to see the summary of a list of variables or to convert a list of categorical variables to factor variables. A similar function is `sapply`. lapply and sapply have the same functionality. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list. \n  \n\n length \n    base \n    To see the length of an object, e.g., number of elements/observations of a variable \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n publish \n    Publish \n    To show/publish regression tables \n  \n\n Reduce \n    base \n    To combine multiple objects, e.g., datasets \n  \n\n round \n    base \n    To round numeric values \n  \n\n saveRDS \n    base \n    To save a single R object. Similarly, readDRS will read an R object \n  \n\n skim \n    skimr \n    To see the summary statistics of variables \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n unique \n    base \n    To see the number of unique elements \n  \n\n weights \n    base/stats \n    To extract model weights, e.g., see the weights from a pre-specified survey design \n  \n\n\n\n\n\nFor more information, visit the resources mentioned earlier."
  },
  {
    "objectID": "researchquestionQ.html",
    "href": "researchquestionQ.html",
    "title": "Quiz (Q)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "confounding.html#confounding",
    "href": "confounding.html#confounding",
    "title": "Role of variables",
    "section": "Confounding",
    "text": "Confounding\nThe first tutorial provides a thorough exploration of confounding, with a particular focus on its impact on treatment effect estimates in large datasets. It emphasizes the importance of properly adjusting for confounders to arrive at accurate estimates."
  },
  {
    "objectID": "confounding.html#mediator",
    "href": "confounding.html#mediator",
    "title": "Role of variables",
    "section": "Mediator",
    "text": "Mediator\nThis tutorial focuses on the role of mediator variables in estimating treatment effects. It assesses how adjusting for the mediator influences the estimated treatment effect, exploring both scenarios where the true treatment effect is either non-null or null."
  },
  {
    "objectID": "confounding.html#collider",
    "href": "confounding.html#collider",
    "title": "Role of variables",
    "section": "Collider",
    "text": "Collider\nThis tutorial serves as a practical guide for understanding how the inclusion of colliders can affect the estimation of treatment effects in causal models."
  },
  {
    "objectID": "confounding.html#z-bias",
    "href": "confounding.html#z-bias",
    "title": "Role of variables",
    "section": "Z-bias",
    "text": "Z-bias\nThis tutorial explores the concept of Z-bias, a phenomenon that can lead to misleading estimates of treatment effects in observational studies. It demonstrates how failing to properly adjust or not adjust for instrumental variables can result in biased estimates and compares these with the true treatment effect."
  },
  {
    "objectID": "confounding.html#collapsibility",
    "href": "confounding.html#collapsibility",
    "title": "Role of variables",
    "section": "Collapsibility",
    "text": "Collapsibility\nThis tutorial provides a detailed guide on calculating marginal probabilities and measures of association, including Risk Difference (RD), Risk Ratio (RR), and Odds Ratio (OR). It examines the impact of adjusting for various covariates on these measures, highlighting the concept of “collapsibility.”"
  },
  {
    "objectID": "confounding.html#change-in-estimate",
    "href": "confounding.html#change-in-estimate",
    "title": "Role of variables",
    "section": "Change-in-estimate",
    "text": "Change-in-estimate\nThis tutorial focuses on the “Change-in-estimate” concept to understand the impact of various variables on measures of effect. For both continuous and binary outcomes, the tutorial reveals that adding a confounder to the model alters the true treatment effect estimate. Conversely, including a variable that is not a confounder but is a pure risk factor can either change or not change the effect estimate, depending on the type of outcome involved. This nuanced approach aids in understanding how different roles of variables can influence results and interpretations in causal inference.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "confounding1.html",
    "href": "confounding1.html",
    "title": "Confounding",
    "section": "",
    "text": "Show the code# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n\n\nBig data: What if we had 1,000,000 (one million) observations? Would that give us true result? Let’s try to answer that using Directed acyclic graphs (DAGs).\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        1.75\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         0.5\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        4.69        0.45\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>         0.0         0.0         0.5\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confounding2.html",
    "href": "confounding2.html",
    "title": "Mediator",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nLet us consider\n\nM is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        1.69\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for M\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        5.00        0.39\n\n# Adjusted for M\nfit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         0.0         0.5"
  },
  {
    "objectID": "confounding3.html",
    "href": "confounding3.html",
    "title": "Collider",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 1.3 * A, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00        1.29\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00        0.58        0.05\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nShow the codeD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# Not adjusted for L\nfit0 <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A \n#>        0.00       -0.01\n\n# Adjusted for L\nfit <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           L \n#>        0.00       -0.07        0.05\n\n\nEven 1,000,000 observations were not enough to recover true treatment effect! But we are close enough."
  },
  {
    "objectID": "confounding4.html",
    "href": "confounding4.html",
    "title": "Z-bias",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(simcausal)\n\n\nYou could watch the video describing Z-bias and Bias amplification.\nContinuous Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"age\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"gender\", distr = \"rbern\", prob = plogis(0.25)) +\n  node(\"education\", distr = \"rbern\", prob = plogis(3 + 5* age)) +\n  node(\"diet\", distr = \"rbern\", prob = plogis(13 + 7 * education)) +\n  node(\"income\", distr = \"rbern\", prob = plogis(2 + 1.4 * education + 2 * age)) +\n  node(\"smoking\", distr = \"rbern\", prob = plogis(1 + 1.2 * gender + 2 * age)) +\n  node(\"hypertension\", distr = \"rnorm\", mean = 3 * diet + 1.3 * age + 2 * smoking + 0.5 * gender, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the codeObs.Data$income <- as.factor(Obs.Data$income)\n# True data generating mechanism \n# (unattainable as U is unmeasured)\nfit0 <- glm(hypertension ~ diet + age + smoking + gender, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)        diet         age     smoking      gender \n#>  -2024169.9   2024172.9         1.3         2.0         0.5\n\nrequire(Publish)\nfit1 <- glm(hypertension ~ diet + age + smoking*income + gender, family=\"gaussian\", data=Obs.Data)\npublish(fit1)\n#>            Variable Units Coefficient                     CI.95  p-value \n#>         (Intercept)       -2024807.39 [-13733931.24;9684316.46]   0.7347 \n#>                diet        2024810.39 [-9684313.46;13733934.24]   0.7347 \n#>                 age              1.30               [1.30;1.30]   <1e-04 \n#>              gender              0.50               [0.50;0.50]   <1e-04 \n#>  smoking: income(0)              2.00               [1.99;2.00]   <1e-04 \n#>  smoking: income(1)              2.00               [2.00;2.00]   <1e-04\n\n\nBinary Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is binary outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rbern\", prob = plogis(-1 + 3 * U + 1.3 * A))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect\n\nShow the code# True data generating mechanism (unattainable as U is unmeasured)\nfit0 <- glm(Y ~ A + U, family=\"binomial\", data=Obs.Data)\nround(coef(fit0),2)\n#> (Intercept)           A           U \n#>       -0.99        1.30        3.01\n\n# Unadjusted effect (Z not controlled)\nfit1 <- glm(Y ~ A, family=\"binomial\", data=Obs.Data)\nround(coef(fit1),2)\n#> (Intercept)           A \n#>        0.40        3.02\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#>        A \n#> 1.716482\n\n# Adjusted effect (Z  controlled)\nfit2 <- glm(Y ~ A + Z, family=\"binomial\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           Z \n#>        0.51        3.29       -0.18\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#>        A \n#> 1.991396"
  },
  {
    "objectID": "confounding5.html",
    "href": "confounding5.html",
    "title": "Collapsibility",
    "section": "",
    "text": "Explanation of collapsibility property of an estimate (RD, RR and OR: conditional or marginal) in absence of confounding\n\nShow the code# Load required packages\nlibrary(simcausal)\nlibrary(tableone)\nlibrary(Publish)\nlibrary(lawstat)\n\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"gender\", distr = \"rbern\", \n       prob = 0.7) +\n  node(\"age\", distr = \"rnorm\", \n       mean = 2, sd = 4) +\n  node(\"smoking\", distr = \"rbern\", \n       prob = plogis(.1)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3.5) * smoking \n                     + log(.1) * gender  \n                       + log(7) * age))\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the codeObs.Data <- sim(DAG = Dset, n = 100000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nBalance check\n\nShow the coderequire(tableone)\nCreateTableOne(data = Obs.Data, \n               strata = \"smoking\", \n               vars = c(\"gender\", \"age\"))\n#>                     Stratified by smoking\n#>                      0            1            p      test\n#>   n                  47720        52280                   \n#>   gender (mean (SD))  0.70 (0.46)  0.70 (0.46)  0.403     \n#>   age (mean (SD))     2.02 (4.02)  2.01 (4.00)  0.690\n\n\nConditional and crude RD\nFull list of risk factors for outcome (2 variables)\n\nShow the code## RD\nrequire(Publish)\nfitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator …robust variance estimator (or bootstrap) should be used to obtain valid standard errors.”)\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"],\n             coef(fitx3)[\"smoking\"],\n             coef(fitx4)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variables)\n\nShow the coderound(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"])),2)\n#> [1] 0.05\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n\n  \n\n\n\nConditional and crude RR\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk ratio, one may use a GLM with a Poisson distribution and log link function …. one should use the robust (or sandwich) variance estimator to obtain valid standard errors (the bootstrap can also be used)”).\n\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 1.156387\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the codefitx2 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.077402\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nConditional and crude OR\nFull list of risk factors for outcome (2 variables)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender + age, family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age < 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age < 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx3 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age >= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx4 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age >= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#> [1] 2.180804\n\n\nPartial list of risk factors for outcome (1 variable)\n\nShow the codefitx0 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\",\n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\nShow the code\nfitx2 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nShow the codemean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#> [1] 1.290429\n\n\nMantel-Haenszel adjusted ORs with 1 variable\n\nShow the codetabx <- xtabs( ~ hypertension + smoking + gender, data = Obs.Data)\nftable(tabx)    \n#>                      gender     0     1\n#> hypertension smoking                   \n#> 0            0               3788 12401\n#>              1               3400 11504\n#> 1            0              10547 20984\n#>              1              12178 25198\n# require(samplesizeCMH)\n# apply(tabx, 3, odds.ratio)\n\nlibrary(lawstat)\ncmh.test(tabx)\n#> \n#>  Cochran-Mantel-Haenszel Chi-square Test\n#> \n#> data:  tabx\n#> CMH statistic = NA, df = 1.0000, p-value = NA, MH Estimate = 1.2924,\n#> Pooled Odd Ratio = 1.2876, Odd Ratio of level 1 = 1.2864, Odd Ratio of\n#> level 2 = 1.2945\n# mantelhaen.test(tabx, exact = TRUE)\n\n\nCrude (in absence of confounding)\n\nShow the codefitx0 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n\n  \n\n\n\nMarginal RD, RR and OR\nBelow we show a procedure for calculating marginal probabilities \\(p_1\\) (for treated) and \\(p_0\\) (for untreated).\nAdjustment of 2 variables\n\nShow the codefitx3 <- glm(hypertension ~ smoking + gender + age, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx3, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx3)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  3.37 \n#> RR (ZY)=  1.08\n\n\nAdjustment of 1 variable\n\nShow the codefitx2 <- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx2, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx2)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nNo adjustment\n\nShow the codefitx1 <- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx <- Obs.Data\nObs.Data.all.tx$smoking <- 1\np1 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx <- Obs.Data\nObs.Data.all.utx$smoking <- 0\np0 <- mean(predict(fitx0, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm <- p1 - p0\nRRm <- p1 / p0\nORm <- (p1 / (1-p1)) / (p0 / (1-p0))\nORc <- as.numeric(exp(coef(fitx1)[\"smoking\"]))\nRRz <- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#> RD marginal =  0.05 \n#> RR marginal =  1.08 \n#> OR marginal =  1.29 \n#> OR conditional =  1.29 \n#> RR (ZY)=  1.08\n\n\nBootstrap could be used to estimate confidence intervals.\nRef:\n\n\n(Kleinman and Norton 2009) (“this paper demonstrates how to move from a nonlinear model to estimates of marginal effects that are quantified as the adjusted risk ratio or adjusted risk difference”)\n\n(Austin 2010) (“clinically meaningful measures of treatment effect using logistic regression model”)\n\n(Luijken et al. 2022) (“marginal odds ratio”)\n\n(Muller and MacLehose 2014) (“marginal standardization”)\n\n(Greenland 2004) (“standardized / population-averaged”)\n\n(Bieler et al. 2010) (“standardized /population-averaged risk from the logistic model”)\nSummary\nHere are the summary of the results based on a scenario where confounding was absent:\n\n\n\n\n\n\n\n\nModelling strategy\nRD (conditional)\nRR (conditional)\nOR (conditional)\n\n\n\nage + gender in regression\n0.06 [0.05;0.06]\n1.08 [1.08;1.09]\n3.37 [3.17;3.58]\n\n\nstratified by age and gender (mean)\n0.05 (0.11, 0.1,0.01,0)\n1.16 (1.41, 1.21, 1.01,1)\n2.18 (unweighted; 1.65, 1.49, 3.45, 2.14)\n\n\ngender in regression\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.26;1.33]\n\n\nstratified by gender (mean)\n0.05 (0.6,0.5)\n1.08 (1.09, 1.06)\n1.29 (1.29, 1.29; M-H 1.29)\n\n\nMarginal estimates\n\n\n\n\n\ncrude\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.25;1.32]\n\n\nBased on marginal probabilities (any variable combination)\n0.05\n1.08\n1.29\n\n\n\nLet us assume we have a regression of hypertension (\\(Y\\)), smoking (\\(A\\)) and a risk factor for outcome, gender (\\(L\\)). Then let us set up 2 regression models:\n\n1st regression model is \\(Y \\sim \\beta \\times A + \\alpha \\times L\\). Here we are conditioning on gender (\\(L\\)).\n2nd regression model is \\(Y \\sim \\beta' \\times A\\)\n\n\nThen regression is collapsible for \\(\\beta\\) over \\(L\\) if \\(\\beta = \\beta'\\) from the 2nd regression omitting \\(L\\). \\(\\beta \\ne \\beta'\\) would mean non-collapsibility. A measure of association (say, risk difference) is collapsible if the marginal measure of association is equal to a weighted average of the stratum-specific measures of association. Non-collapsibility is also knows as Simpson’s Paradox (in absence of confoinding of course): a statistical phenomenon where an association between two factors (say, hypertension and smoking) in a population (we are talking about marginal estimate here) is different than the associations of same relationship in subpopulations (conditional on some other factor, say, age; hence talking about conditional estimates).\nOdds ratio can be non-collapsible. It can produce different treatment effect estimate for different covariate adjustment sets (see our above example of when adjusting form age and sex vs. when adjusting none). This is true even in the absence of confounding. However, according to our definition here, OR is collapsible when we consider gender in the adjustment set.\nNote that, OR non-collapsibility is a consequence of the fact that it is estimated via a logit link function (nonlinearity of the logistic transformation).\nRef:\n\n(Greenland, Pearl, and Robins 1999)\n(Mansournia and Greenland 2015)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nAustin, Peter C. 2010. “Absolute Risk Reductions, Relative Risks, Relative Risk Reductions, and Numbers Needed to Treat Can Be Obtained from a Logistic Regression Model.” Journal of Clinical Epidemiology 63 (1): 2–6.\n\n\nBieler, Gayle S, G Gordon Brown, Rick L Williams, and Donna J Brogan. 2010. “Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios from Complex Survey Data.” American Journal of Epidemiology 171 (5): 618–23.\n\n\nGreenland, Sander. 2004. “Model-Based Estimation of Relative Risks and Other Epidemiologic Measures in Studies of Common Outcomes and in Case-Control Studies.” American Journal of Epidemiology 160 (4): 301–5.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Confounding and Collapsibility in Causal Inference.” Statistical Science 14 (1): 29–46.\n\n\nKleinman, Lawrence C, and Edward C Norton. 2009. “What’s the Risk? A Simple Approach for Estimating Adjusted Risk Measures from Nonlinear Models Including Logistic Regression.” Health Services Research 44 (1): 288–302.\n\n\nLuijken, Kim, Rolf HH Groenwold, Maarten van Smeden, Susanne Strohmaier, and Georg Heinze. 2022. “A Comparison of Full Model Specification and Backward Elimination of Potential Confounders When Estimating Marginal and Conditional Causal Effects on Binary Outcomes from Observational Data.” Biometrical Journal.\n\n\nMansournia, Mohammad Ali, and Sander Greenland. 2015. “The Relation of Collapsibility and Confounding to Faithfulness and Stability.” Epidemiology 26 (4): 466–72.\n\n\nMuller, Clemma J, and Richard F MacLehose. 2014. “Estimating Predicted Probabilities from Logistic Regression: Different Methods Correspond to Different Target Populations.” International Journal of Epidemiology 43 (3): 962–70.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is a confounder",
    "text": "Adjusting for a variable that is a confounder\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.85\n\nfit2 <- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis( 1.1 * L + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node L, order:1\n#> node A, order:2\n#> node P, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.68\n\nfit2 <- glm(Y ~ A + L, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           L \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (simplified)",
    "text": "Adjusting for a variable that is not a confounder (simplified)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>         0.0         1.3\n\nfit2 <- glm(Y ~ A + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node R, order:3\n#> node Y, order:4\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A \n#>        0.00        1.06\n\nfit2 <- glm(Y ~ A + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           R \n#>         0.0         1.3         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3)."
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (Complex)",
    "text": "Adjusting for a variable that is not a confounder (Complex)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.1 * R + 1.3 * A, sd = .1)\nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nShow the codefit <- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>         0.0         1.3         0.5\n\nfit2 <- glm(Y ~ A + M + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>         0.0         1.3         0.5         1.1\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nShow the coderequire(simcausal)\nD <- DAG.empty()\nD <- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.1 * R + 1.3 * A)) \nDset <- set.DAG(D)\n#> ...automatically assigning order attribute to some nodes...\n#> node A, order:1\n#> node P, order:2\n#> node M, order:3\n#> node R, order:4\n#> node Y, order:5\n\n\nGenerate DAG\n\nShow the codeplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#> using the following vertex attributes:\n#> 120.8NAdarkbluenone0\n#> using the following edge attributes:\n#> 0.50.40.7black1\n\n\n\n\nGenerate Data\n\nShow the coderequire(simcausal)\nObs.Data <- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n\n  \n\n\n\nEstimate effect (OR)\n\nShow the codefit <- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#> (Intercept)           A           M \n#>        0.00        1.06        0.41\n\nfit2 <- glm(Y ~ A + M + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#> (Intercept)           A           M           R \n#>        0.00        1.29        0.50        1.10\n\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "confoundingF.html",
    "href": "confoundingF.html",
    "title": "R functions (R)",
    "section": "",
    "text": "The list of new R functions introduced in this Confounding and bias lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n cmh.test \n    lawstat \n    To conduct the Mantel-Haenszel Chi-square test \n  \n\n DAG.empty \n    simcausal \n    To initialize an empty DAG \n  \n\n ftable \n    base/stats \n    To create a flat contingency table \n  \n\n plotDAG \n    simcausal \n    To visualize a DAG \n  \n\n set.DAG \n    simcausal \n    To create a DAG \n  \n\n sim \n    simcausal \n    To simulate data using a DAG"
  },
  {
    "objectID": "confoundingQ.html",
    "href": "confoundingQ.html",
    "title": "Quiz (R)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "predictivefactors.html#identify-collinear-predictors",
    "href": "predictivefactors.html#identify-collinear-predictors",
    "title": "Prediction models",
    "section": "Identify collinear predictors",
    "text": "Identify collinear predictors\nThis tutorial focuses on identifying collinear predictors in a dataset related to cholesterol levels from the NHANES 2015 collection. The tutorial guides you through summarizing its structure, and applying methods for variable clustering to detect collinear predictors. The tutorial is practical for data analysts aiming to improve model accuracy by identifying and addressing redundant variables."
  },
  {
    "objectID": "predictivefactors.html#explore-relationships-for-continuous-outcome-variable",
    "href": "predictivefactors.html#explore-relationships-for-continuous-outcome-variable",
    "title": "Prediction models",
    "section": "Explore relationships for continuous outcome variable",
    "text": "Explore relationships for continuous outcome variable\nThis comprehensive tutorial walks you through the process of analyzing a dataset on cholesterol levels, focusing on exploring relationships for a continuous outcome variable. It starts by generating a correlation plot. Multiple methods for examining descriptive associations are provided, including stratification by key predictors. The tutorial also covers linear regression modeling, diagnosing data issues like outliers and leverage, and refitting the model after cleaning the data. Additionally, the tutorial delves into more complex modeling techniques like polynomial regression and multiple covariates, and addresses issues of collinearity using Variance Inflation Factors (VIF)."
  },
  {
    "objectID": "predictivefactors.html#explore-relationships-for-binary-outcome-variable",
    "href": "predictivefactors.html#explore-relationships-for-binary-outcome-variable",
    "title": "Prediction models",
    "section": "Explore relationships for binary outcome variable",
    "text": "Explore relationships for binary outcome variable\nA binary outcome variable is created to classify cholesterol levels as ‘healthy’ or ‘unhealthy’. This transformed variable is then modeled using logistic regression. Various predictors including demographic variables, vital statistics, and other health parameters are considered in the model. The performance of the model is evaluated using Variance Inflation Factor (VIF) for multicollinearity and Area Under the Curve (AUC) for classification accuracy. Two models are fitted, and their respective AUCs are calculated to assess the predictive power."
  },
  {
    "objectID": "predictivefactors.html#overfitting-and-performance",
    "href": "predictivefactors.html#overfitting-and-performance",
    "title": "Prediction models",
    "section": "Overfitting and performance",
    "text": "Overfitting and performance\nThe tutorial focus is on addressing overfitting and assessing model performance. A linear regression model is fitted using a comprehensive set of predictors. Various statistical metrics such as the design matrix dimensions, Sum of Squares for Error (SSE), Total Sum of Squares (SST), R-squared (R2), Root Mean Square Error (RMSE), and Adjusted R2 are calculated to evaluate the model’s predictive power and fit. Functions are also created to streamline the calculation of these metrics, allowing for more dynamic and customizable performance assessment. One such function, perform, encapsulates the entire process, outputting key performance indicators including R2, adjusted R2, and RMSE, and it can be applied to new data sets for validation."
  },
  {
    "objectID": "predictivefactors.html#data-spliting",
    "href": "predictivefactors.html#data-spliting",
    "title": "Prediction models",
    "section": "Data spliting",
    "text": "Data spliting\nThe tutorial focuses on splitting data into training and testing sets to prevent model overfitting. We allocate approximately 70% of the data to the training set and the remaining 30% to the test set. The linear regression model is then fitted using the training data. Performance metrics are extracted using the previously defined perform function, which is applied not only to the training and test sets but also to the entire dataset for comprehensive performance evaluation. This data splitting approach allows for more robust model validation by assessing how well the model generalizes to unseen data."
  },
  {
    "objectID": "predictivefactors.html#cross-vaildation",
    "href": "predictivefactors.html#cross-vaildation",
    "title": "Prediction models",
    "section": "Cross-vaildation",
    "text": "Cross-vaildation\nThe tutorial outlines the process of implementing k-fold cross-validation to validate a linear regression model’s performance, aiming to predict cholesterol levels. The dataset is divided into 5 folds, by turn used as training (to fit the model), and test sets (used for prediction and performance evaluation). Performance metrics such as R-squared are calculated for each fold. The process can also be automated , which helps in fitting the model across all folds and summarizing the results, including calculating the mean and standard deviation of the R-squared values to understand the model’s consistency and reliability."
  },
  {
    "objectID": "predictivefactors.html#bootstrap",
    "href": "predictivefactors.html#bootstrap",
    "title": "Prediction models",
    "section": "Bootstrap",
    "text": "Bootstrap\nThe tutorial outlines methods for implementing various bootstrapping techniques in statistical analysis. It demonstrates resampling methods using vectors and matrices. The idea of bootstrapping is emphasized as a useful technique for estimating the standard deviation (SD) of a statistic (e.g., mean), when the distribution of the data is unknown. This SD is then used to calculate confidence intervals. Different variations of bootstrap methods, such as “boot,” “boot632,” and “Optimism corrected bootstrap,” are demonstrated for linear regression and logistic regression models. They are used to obtain performance metrics like R-squared for regression models and the Receiver Operating Characteristic (ROC) curve for classification models. The tutorial also includes an example of calculating the Brier Score. The examples aim to offer various strategies for model evaluation, from the basics of resampling a vector to applying complex methods like ‘Optimism corrected bootstrap’ on real-world data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "predictivefactors1.html",
    "href": "predictivefactors1.html",
    "title": "Collinear predictors",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\n\n\nLoad data\nLet us load the dataset and see structure of the variables:\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n#head(analytic)\nstr(analytic)\n#> 'data.frame':    1267 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83741 83747 83750 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n#>  $ age                  : num  62 53 22 46 45 30 60 69 24 70 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Others\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"Black\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"College\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Never.married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"Between.25kto54k\" \"<25k\" ...\n#>  $ weight               : num  135630 25282 39353 35674 97002 ...\n#>  $ psu                  : num  1 1 2 1 1 1 1 2 1 2 ...\n#>  $ strata               : num  125 125 128 121 125 124 128 120 130 132 ...\n#>  $ diastolicBP          : num  70 88 70 94 70 50 74 70 72 54 ...\n#>  $ systolicBP           : num  128 146 110 144 116 104 142 146 126 144 ...\n#>  $ bodyweight           : num  94.8 90.4 76.6 86.2 76.2 71.2 75.6 84 89.2 81.7 ...\n#>  $ bodyheight           : num  184 171 165 177 178 ...\n#>  $ bmi                  : num  27.8 30.8 28 27.6 24.1 26.6 35.9 31 26.9 27 ...\n#>  $ waist                : num  101.1 107.9 86.6 104.3 90.1 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Some.days\" \"Every.day\" ...\n#>  $ alcohol              : num  1 6 8 1 3 2 1 1 2 2 ...\n#>  $ cholesterol          : num  173 265 164 242 181 184 205 287 126 192 ...\n#>  $ cholesterolM2        : num  4.47 6.85 4.24 6.26 4.68 4.76 5.3 7.42 3.26 4.97 ...\n#>  $ triglycerides        : num  158 170 77 497 63 62 169 245 95 64 ...\n#>  $ uric.acid            : num  4.2 7 6 6.5 5.4 5.5 5.1 4.3 7.6 7.1 ...\n#>  $ protein              : num  7.5 7.4 7.4 6.8 7.4 6.7 7.4 6.8 7.3 7.2 ...\n#>  $ bilirubin            : num  0.5 0.6 0.2 0.5 0.7 0.8 0.4 0.6 1.2 1.2 ...\n#>  $ phosphorus           : num  4.7 4.4 5.3 3.6 3.9 3.4 3.9 4.4 3.2 3 ...\n#>  $ sodium               : num  136 140 139 138 138 136 139 140 140 139 ...\n#>  $ potassium            : num  4.3 4.55 4.16 4.27 3.91 3.97 3.99 4.25 3.8 4.63 ...\n#>  $ globulin             : num  2.9 2.9 3 2.6 2.8 2.5 3.2 2.3 2.7 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.3 9.3 9.3 9.4 9.6 9.6 9.6 9.6 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"Yes\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:3739] 3 4 5 6 8 9 13 14 15 16 ...\n#>   ..- attr(*, \"names\")= chr [1:3739] \"3\" \"4\" \"5\" \"6\" ...\n\n\nDescribe the data\n\nShow the coderequire(rms)\ndescribe(analytic) \n#> analytic \n#> \n#>  33  Variables      1267  Observations\n#> --------------------------------------------------------------------------------\n#> ID \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1267        1    88660     3366    84250    84687 \n#>      .25      .50      .75      .90      .95 \n#>    86019    88692    91252    92670    93089 \n#> \n#> lowest : 83732 83733 83741 83747 83750, highest: 93617 93633 93643 93659 93685\n#> --------------------------------------------------------------------------------\n#> gender \n#>        n  missing distinct \n#>     1267        0        2 \n#>                         \n#> Value      Female   Male\n#> Frequency     496    771\n#> Proportion  0.391  0.609\n#> --------------------------------------------------------------------------------\n#> age \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       61        1    49.91    19.18       24       27 \n#>      .25      .50      .75      .90      .95 \n#>       36       51       63       72       78 \n#> \n#> lowest : 20 21 22 23 24, highest: 76 77 78 79 80\n#> --------------------------------------------------------------------------------\n#> born \n#>        n  missing distinct \n#>     1267        0        2 \n#>                                                                             \n#> Value      Born in 50 US states or Washingt                           Others\n#> Frequency                               991                              276\n#> Proportion                            0.782                            0.218\n#> --------------------------------------------------------------------------------\n#> race \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                               \n#> Value         Black Hispanic    Other    White\n#> Frequency       246      337      132      552\n#> Proportion    0.194    0.266    0.104    0.436\n#> --------------------------------------------------------------------------------\n#> education \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                               \n#> Value          College High.School      School\n#> Frequency          648         523          96\n#> Proportion       0.511       0.413       0.076\n#> --------------------------------------------------------------------------------\n#> married \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                                                    \n#> Value                 Married      Never.married Previously.married\n#> Frequency                 751                226                290\n#> Proportion              0.593              0.178              0.229\n#> --------------------------------------------------------------------------------\n#> income \n#>        n  missing distinct \n#>     1267        0        4 \n#>                                                                               \n#> Value                  <25k Between.25kto54k Between.55kto99k         Over100k\n#> Frequency               344              435              297              191\n#> Proportion            0.272            0.343            0.234            0.151\n#> --------------------------------------------------------------------------------\n#> weight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0     1184        1    48904    44337     9158    11549 \n#>      .25      .50      .75      .90      .95 \n#>    19540    30335    63822   121803   151546 \n#> \n#> lowest :   5470.041   5948.955   6197.660   6480.947   6703.837\n#> highest: 203562.855 207197.232 213611.345 218138.797 224891.623\n#> --------------------------------------------------------------------------------\n#> psu \n#>        n  missing distinct     Info     Mean      Gmd \n#>     1267        0        2     0.75    1.493   0.5003 \n#>                       \n#> Value          1     2\n#> Frequency    642   625\n#> Proportion 0.507 0.493\n#> --------------------------------------------------------------------------------\n#> strata \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       15    0.994    126.3    4.792      120      121 \n#>      .25      .50      .75      .90      .95 \n#>      123      126      130      132      133 \n#> \n#> lowest : 119 120 121 122 123, highest: 129 130 131 132 133\n#>                                                                             \n#> Value        119   120   121   122   123   124   125   126   127   128   129\n#> Frequency     47    74   118    63    77    66   114   104   107    65    53\n#> Proportion 0.037 0.058 0.093 0.050 0.061 0.052 0.090 0.082 0.084 0.051 0.042\n#>                                   \n#> Value        130   131   132   133\n#> Frequency     99   120    95    65\n#> Proportion 0.078 0.095 0.075 0.051\n#> --------------------------------------------------------------------------------\n#> diastolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       41    0.997    70.37    13.99       52       54 \n#>      .25      .50      .75      .90      .95 \n#>       62       70       78       86       92 \n#> \n#> lowest :   0  26  34  38  40, highest: 104 106 108 110 112\n#> --------------------------------------------------------------------------------\n#> systolicBP \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       56    0.998    126.5     19.3    102.0    106.0 \n#>      .25      .50      .75      .90      .95 \n#>    114.0    124.0    136.0    148.8    160.0 \n#> \n#> lowest :  84  88  90  92  94, highest: 194 196 206 218 236\n#> --------------------------------------------------------------------------------\n#> bodyweight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      615        1    84.95    23.56    56.29    61.10 \n#>      .25      .50      .75      .90      .95 \n#>    69.70    81.40    97.00   113.44   127.47 \n#> \n#> lowest :  39.7  39.8  40.7  42.6  42.7, highest: 161.9 166.3 175.7 175.9 178.4\n#> --------------------------------------------------------------------------------\n#> bodyheight \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      376        1    169.2    10.66    153.8    157.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.6    169.3    176.2    181.1    184.2 \n#> \n#> lowest : 143.8 144.2 145.2 145.9 146.2, highest: 194.6 195.1 195.6 198.4 201.0\n#> --------------------------------------------------------------------------------\n#> bmi \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      284        1    29.58    7.403    20.60    22.06 \n#>      .25      .50      .75      .90      .95 \n#>    24.80    28.60    33.30    38.24    42.00 \n#> \n#> lowest : 16.3 17.5 17.6 17.7 17.9, highest: 57.2 57.6 59.4 60.7 64.5\n#> --------------------------------------------------------------------------------\n#> waist \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      544        1    101.8    18.47     77.1     81.4 \n#>      .25      .50      .75      .90      .95 \n#>     90.5    100.3    111.2    122.8    132.5 \n#> \n#> lowest :  65.0  65.5  66.5  68.2  68.7, highest: 159.2 159.8 160.2 160.5 161.5\n#> --------------------------------------------------------------------------------\n#> smoke \n#>        n  missing distinct \n#>     1267        0        3 \n#>                                            \n#> Value       Every.day Not.at.all  Some.days\n#> Frequency         448        665        154\n#> Proportion      0.354      0.525      0.122\n#> --------------------------------------------------------------------------------\n#> alcohol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       14    0.952    3.109    2.419        1        1 \n#>      .25      .50      .75      .90      .95 \n#>        1        2        4        6        8 \n#> \n#> lowest :  1  2  3  4  5, highest: 10 11 12 14 15\n#>                                                                             \n#> Value          1     2     3     4     5     6     7     8     9    10    11\n#> Frequency    336   371   189   106    79    95    10    26     4    20     1\n#> Proportion 0.265 0.293 0.149 0.084 0.062 0.075 0.008 0.021 0.003 0.016 0.001\n#>                             \n#> Value         12    14    15\n#> Frequency     23     1     6\n#> Proportion 0.018 0.001 0.005\n#> --------------------------------------------------------------------------------\n#> cholesterol \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    193.1    47.47    132.0    142.0 \n#>      .25      .50      .75      .90      .95 \n#>    162.5    191.0    217.0    248.0    268.0 \n#> \n#> lowest :  81  93  97 100 101, highest: 345 348 349 358 545\n#> --------------------------------------------------------------------------------\n#> cholesterolM2 \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      203        1    4.994    1.228    3.410    3.670 \n#>      .25      .50      .75      .90      .95 \n#>    4.205    4.940    5.610    6.410    6.930 \n#> \n#> lowest :  2.09  2.40  2.51  2.59  2.61, highest:  8.92  9.00  9.03  9.26 14.09\n#> --------------------------------------------------------------------------------\n#> triglycerides \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      361        1    165.8    124.1     48.0     59.0 \n#>      .25      .50      .75      .90      .95 \n#>     84.0    127.0    201.5    309.0    396.6 \n#> \n#> lowest :   18   21   24   25   31, highest:  964 1020 1157 1253 3061\n#> --------------------------------------------------------------------------------\n#> uric.acid \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       84        1    5.598    1.626     3.43     3.80 \n#>      .25      .50      .75      .90      .95 \n#>     4.60     5.50     6.50     7.40     8.00 \n#> \n#> lowest :  1.6  2.2  2.3  2.4  2.5, highest: 10.2 10.3 11.7 12.2 18.0\n#> --------------------------------------------------------------------------------\n#> protein \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       32    0.995    7.126   0.5095      6.4      6.6 \n#>      .25      .50      .75      .90      .95 \n#>      6.8      7.1      7.4      7.7      7.9 \n#> \n#> lowest : 5.7 5.8 5.9 6.0 6.1, highest: 8.4 8.5 8.6 8.8 9.0\n#> --------------------------------------------------------------------------------\n#> bilirubin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       31    0.984   0.5467   0.2949      0.2      0.2 \n#>      .25      .50      .75      .90      .95 \n#>      0.4      0.5      0.7      0.9      1.0 \n#> \n#> lowest : 0.00 0.01 0.02 0.03 0.04, highest: 1.80 2.00 2.10 2.60 3.30\n#> --------------------------------------------------------------------------------\n#> phosphorus \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       37    0.996    3.642    0.593      2.8      3.0 \n#>      .25      .50      .75      .90      .95 \n#>      3.3      3.6      4.0      4.3      4.5 \n#> \n#> lowest : 1.8 2.0 2.2 2.3 2.4, highest: 5.2 5.3 5.4 5.6 6.1\n#> --------------------------------------------------------------------------------\n#> sodium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       20    0.977    138.5    2.383      135      136 \n#>      .25      .50      .75      .90      .95 \n#>      137      139      140      141      142 \n#> \n#> lowest : 124 126 129 130 131, highest: 142 143 144 146 148\n#>                                                                             \n#> Value        124   126   129   130   131   132   133   134   135   136   137\n#> Frequency      1     1     1     1     5     4    11    23    46    93   176\n#> Proportion 0.001 0.001 0.001 0.001 0.004 0.003 0.009 0.018 0.036 0.073 0.139\n#>                                                                 \n#> Value        138   139   140   141   142   143   144   146   148\n#> Frequency    235   260   206   112    55    29     6     1     1\n#> Proportion 0.185 0.205 0.163 0.088 0.043 0.023 0.005 0.001 0.001\n#> --------------------------------------------------------------------------------\n#> potassium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0      175        1    3.985   0.3725     3.45     3.57 \n#>      .25      .50      .75      .90      .95 \n#>     3.78     3.98     4.19     4.40     4.54 \n#> \n#> lowest : 2.60 2.92 2.96 3.07 3.09, highest: 5.15 5.21 5.36 5.37 5.51\n#> --------------------------------------------------------------------------------\n#> globulin \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       29    0.994    2.799   0.4536      2.2      2.3 \n#>      .25      .50      .75      .90      .95 \n#>      2.5      2.8      3.0      3.3      3.5 \n#> \n#> lowest : 1.6 1.8 1.9 2.0 2.1, highest: 4.1 4.2 4.3 4.5 5.5\n#> --------------------------------------------------------------------------------\n#> calcium \n#>        n  missing distinct     Info     Mean      Gmd      .05      .10 \n#>     1267        0       25    0.991    9.335   0.3786      8.8      8.9 \n#>      .25      .50      .75      .90      .95 \n#>      9.1      9.3      9.6      9.7      9.9 \n#> \n#> lowest :  8.4  8.5  8.6  8.7  8.8, highest: 10.4 10.5 10.7 11.0 11.1\n#> --------------------------------------------------------------------------------\n#> physical.work \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency    895   372\n#> Proportion 0.706 0.294\n#> --------------------------------------------------------------------------------\n#> physical.recreational \n#>        n  missing distinct \n#>     1267        0        2 \n#>                       \n#> Value         No   Yes\n#> Frequency   1002   265\n#> Proportion 0.791 0.209\n#> --------------------------------------------------------------------------------\n#> diabetes \n#>        n  missing distinct \n#>     1267        0        2 \n#>                     \n#> Value        No  Yes\n#> Frequency  1064  203\n#> Proportion 0.84 0.16\n#> --------------------------------------------------------------------------------\n\n\nIdentify collinear predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can use hclust and varclus or variable clustering, i.e., to identify collinear predictors\n\n\n\n\nhclust is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.\n\nShow the coderequire(Hmisc)\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.cluster <- varclus(~., data = analytic[sel.names])\n# var.cluster\nplot(var.cluster)\n\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "predictivefactors2.html",
    "href": "predictivefactors2.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(car)\nlibrary(corrplot)\nlibrary(olsrr)\n\n\nExplore relationships for continuous outcome variable\nLoad data\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n\n\nCorrelation plot\n\n\n\n\n\n\nTip\n\n\n\nWe can use the cor function to see the correlation between numeric variables and then use the corrplot function to plot the cor object.\n\n\n\nShow the coderequire(corrplot)\nnumeric.names <- c(\"age\", \"diastolicBP\", \"systolicBP\", \n                   \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"alcohol\", \n                   \"cholesterol\", \"triglycerides\", \"uric.acid\", \n                   \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n                   \"globulin\", \"calcium\")\ncorrelationMatrix <- cor(analytic[numeric.names])\nmat.num <- round(correlationMatrix,2)\nmat.num[mat.num>0.8 & mat.num < 1]\n#> [1] 0.89 0.90 0.89 0.91 0.90 0.91\ncorrplot(correlationMatrix, method=\"number\", type=\"upper\")\n\n\n\n\nExamine descriptive associations\nLet us examine the descriptive associations with the dependent variable by stratifying separately by key predictors\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to examine the descriptive associations by strata/groups, e.g., summarize, aggregate, describeBy, tapply, summary\n\n\n\nShow the codemean(analytic$cholesterol)\n#> [1] 193.1002\n\n# Process 1\nmean(analytic$cholesterol[analytic$gender == \"Male\"])\n#> [1] 190.7626\nmean(analytic$cholesterol[analytic$gender == \"Female\"])\n#> [1] 196.7339\n\n# Process 2\nlibrary(dplyr)\nanalytic %>%\n  group_by(gender) %>%\n  summarize(mean.ch=mean(cholesterol), .groups = 'drop') \n\n\n\n  \n\n\nShow the code\n# process 3\nwith(analytic, aggregate( analytic$cholesterol, by=list(gender) , FUN=summary))\n\n\n\n  \n\n\nShow the code\n# process 4\npsych::describeBy(analytic$cholesterol, analytic$gender)\n#> \n#>  Descriptive statistics by group \n#> group: Female\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 496 196.73 43.26  194.5  194.44 40.77 100 358   258 0.57     0.56 1.94\n#> ------------------------------------------------------------ \n#> group: Male\n#>    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#> X1    1 771 190.76 43.06    188  188.54 40.03  81 545   464  1.1     5.76 1.55\n\n# process 5\ntapply(analytic$cholesterol, analytic$gender, summary)\n#> $Female\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   100.0   166.8   194.5   196.7   220.2   358.0 \n#> \n#> $Male\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   161.0   188.0   190.8   215.0   545.0\n\n# A general process\nsel.names <- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \"married\", \n               \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \"alcohol\", \n               \"cholesterol\", \"triglycerides\", \"uric.acid\", \n               \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.summ <- summary(cholesterol~ ., data = analytic[sel.names])\nvar.summ\n#> cholesterol      N= 1267  \n#> \n#> +---------------------+--------------------------------+----+-----------+\n#> |                     |                                |   N|cholesterol|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               gender|                          Female| 496|   196.7339|\n#> |                     |                            Male| 771|   190.7626|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  age|                         [20,37)| 342|   182.4854|\n#> |                     |                         [37,52)| 313|   200.1661|\n#> |                     |                         [52,64)| 315|   199.7873|\n#> |                     |                         [64,80]| 297|   190.7845|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 born|Born in 50 US states or Washingt| 991|   190.9253|\n#> |                     |                          Others| 276|   200.9094|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                 race|                           Black| 246|   187.3740|\n#> |                     |                        Hispanic| 337|   193.5490|\n#> |                     |                           Other| 132|   191.8561|\n#> |                     |                           White| 552|   195.6757|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            education|                         College| 648|   192.5478|\n#> |                     |                     High.School| 523|   193.4532|\n#> |                     |                          School|  96|   194.9062|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              married|                         Married| 751|   194.0306|\n#> |                     |                   Never.married| 226|   182.8761|\n#> |                     |              Previously.married| 290|   198.6586|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               income|                            <25k| 344|   191.9564|\n#> |                     |                Between.25kto54k| 435|   191.9310|\n#> |                     |                Between.55kto99k| 297|   195.7508|\n#> |                     |                        Over100k| 191|   193.7016|\n#> +---------------------+--------------------------------+----+-----------+\n#> |          diastolicBP|                        [ 0, 64)| 336|   186.7649|\n#> |                     |                        [64, 72)| 321|   189.3458|\n#> |                     |                        [72, 80)| 319|   195.7085|\n#> |                     |                        [80,112]| 291|   201.6976|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           systolicBP|                       [ 84,116)| 340|   186.2765|\n#> |                     |                       [116,126)| 317|   190.6372|\n#> |                     |                       [126,138)| 335|   196.9881|\n#> |                     |                       [138,236]| 275|   199.6400|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyweight|                    [39.7, 69.8)| 319|   193.8903|\n#> |                     |                    [69.8, 81.5)| 316|   197.1424|\n#> |                     |                    [81.5, 97.2)| 317|   192.4984|\n#> |                     |                    [97.2,178.4]| 315|   188.8508|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           bodyheight|                       [144,163)| 317|   198.7003|\n#> |                     |                       [163,169)| 320|   193.7750|\n#> |                     |                       [169,176)| 314|   189.8790|\n#> |                     |                       [176,201]| 316|   190.0000|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                  bmi|                     [16.3,24.9)| 322|   188.8043|\n#> |                     |                     [24.9,28.7)| 315|   198.5016|\n#> |                     |                     [28.7,33.4)| 317|   197.5016|\n#> |                     |                     [33.4,64.5]| 313|   187.6262|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                waist|                   [ 65.0, 90.7)| 320|   188.9688|\n#> |                     |                   [ 90.7,100.4)| 315|   199.6413|\n#> |                     |                   [100.4,111.3)| 316|   197.3892|\n#> |                     |                   [111.3,161.5]| 316|   186.4747|\n#> +---------------------+--------------------------------+----+-----------+\n#> |                smoke|                       Every.day| 448|   191.5938|\n#> |                     |                      Not.at.all| 665|   194.6451|\n#> |                     |                       Some.days| 154|   190.8117|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              alcohol|                               1| 336|   191.0387|\n#> |                     |                               2| 371|   192.0809|\n#> |                     |                          [3, 5)| 295|   195.9356|\n#> |                     |                          [5,15]| 265|   193.9849|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        triglycerides|                      [ 18,  85)| 320|   172.2344|\n#> |                     |                      [ 85, 128)| 319|   185.6834|\n#> |                     |                      [128, 203)| 314|   199.4140|\n#> |                     |                      [203,3061]| 314|   215.5860|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            uric.acid|                      [1.6, 4.7)| 348|   188.9310|\n#> |                     |                      [4.7, 5.6)| 305|   191.8033|\n#> |                     |                      [5.6, 6.6)| 307|   195.7720|\n#> |                     |                      [6.6,18.0]| 307|   196.4430|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              protein|                       [5.7,6.9)| 336|   189.8631|\n#> |                     |                       [6.9,7.2)| 328|   192.3201|\n#> |                     |                       [7.2,7.5)| 310|   193.4258|\n#> |                     |                       [7.5,9.0]| 293|   197.3413|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            bilirubin|                       [0.0,0.5)| 506|   195.7391|\n#> |                     |                             0.5| 212|   192.2264|\n#> |                     |                       [0.6,0.8)| 310|   192.0645|\n#> |                     |                       [0.8,3.3]| 239|   189.6318|\n#> +---------------------+--------------------------------+----+-----------+\n#> |           phosphorus|                       [1.8,3.4)| 362|   188.0387|\n#> |                     |                       [3.4,3.7)| 309|   192.5405|\n#> |                     |                       [3.7,4.1)| 323|   195.5542|\n#> |                     |                       [4.1,6.1]| 273|   197.5421|\n#> +---------------------+--------------------------------+----+-----------+\n#> |               sodium|                       [124,138)| 362|   191.9420|\n#> |                     |                       [138,140)| 495|   194.2929|\n#> |                     |                             140| 206|   191.7864|\n#> |                     |                       [141,148]| 204|   193.5882|\n#> +---------------------+--------------------------------+----+-----------+\n#> |            potassium|                     [2.60,3.79)| 320|   191.5375|\n#> |                     |                     [3.79,3.99)| 328|   192.3628|\n#> |                     |                     [3.99,4.20)| 308|   196.9643|\n#> |                     |                     [4.20,5.51]| 311|   191.6592|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             globulin|                       [1.6,2.6)| 350|   189.9429|\n#> |                     |                       [2.6,2.9)| 388|   199.0052|\n#> |                     |                       [2.9,3.1)| 230|   193.3783|\n#> |                     |                       [3.1,5.5]| 299|   188.9197|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              calcium|                      [8.4, 9.2)| 371|   186.0323|\n#> |                     |                      [9.2, 9.4)| 294|   188.3605|\n#> |                     |                      [9.4, 9.7)| 395|   197.4430|\n#> |                     |                      [9.7,11.1]| 207|   204.2126|\n#> +---------------------+--------------------------------+----+-----------+\n#> |        physical.work|                              No| 895|   194.0078|\n#> |                     |                             Yes| 372|   190.9167|\n#> +---------------------+--------------------------------+----+-----------+\n#> |physical.recreational|                              No|1002|   193.5359|\n#> |                     |                             Yes| 265|   191.4528|\n#> +---------------------+--------------------------------+----+-----------+\n#> |             diabetes|                              No|1064|   194.8036|\n#> |                     |                             Yes| 203|   184.1724|\n#> +---------------------+--------------------------------+----+-----------+\n#> |              Overall|                                |1267|   193.1002|\n#> +---------------------+--------------------------------+----+-----------+\nplot(var.summ)\n\n\n\nShow the code\nsummary(analytic$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.00   62.00   70.00   70.37   78.00  112.00\n\nanalytic$diastolicBP[analytic$diastolicBP == 0] <- NA\n\n# Bivariate Summaries Computed Separately by a Series of Predictors\nvar.summ2 <- spearman2(cholesterol~ ., data = analytic[sel.names])\nplot(var.summ2)\n\n\n\n\nRegression: Linear regression\nWe can also use regression analysis to examine the association:\n\n\n\n\n\n\nTip\n\n\n\nWe use lm function to fit the linear regression\n\n\n\nShow the code# set up formula with just 1 variable\nformula0 <- as.formula(\"cholesterol~triglycerides\")\n\n# fitting regression on the analytic2 data\nfit0 <- lm(formula0,data = analytic2)\n\n# extract results\nsummary(fit0)\n#> \n#> Call:\n#> lm(formula = formula0, data = analytic2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -111.651  -26.157   -2.661   22.549  166.752 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.716e+02  1.127e+00  152.23   <2e-16 ***\n#> triglycerides 1.275e-01  5.456e-03   23.37   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 37.38 on 2632 degrees of freedom\n#> Multiple R-squared:  0.1718, Adjusted R-squared:  0.1715 \n#> F-statistic:   546 on 1 and 2632 DF,  p-value: < 2.2e-16\n\n# extract just the coefficients/estimates\ncoef(fit0)\n#>   (Intercept) triglycerides \n#>   171.6147531     0.1274909\n\n# extract confidence intervals\nconfint(fit0)\n#>                     2.5 %      97.5 %\n#> (Intercept)   169.4042284 173.8252779\n#> triglycerides   0.1167919   0.1381899\n\n# residual plots\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\nDiagnosis\nIdentifying problematic data\n\nShow the coderequire(olsrr)\n# Outlier\nplot(cholesterol ~ triglycerides, data = analytic2)\n\n\n\nShow the codesubset(analytic2, triglycerides > 1500)\n\n\n\n  \n\n\nShow the code\n# leverage\nols_plot_resid_lev(fit0)\n\n\n\nShow the codeanalytic2$lev <- hat(model.matrix(fit0))\nplot(analytic2$lev)\n\n\n\nShow the codesummary(analytic2$lev)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.0003796 0.0004062 0.0004773 0.0007593 0.0005831 0.1800021\nwhich(analytic2$lev > 0.05)\n#> [1] 1102\nsubset(analytic2, lev > 0.05)\n\n\n\n  \n\n\nShow the code\n# Residual\nanalytic2$rstudent.values <- rstudent(fit0)\nplot(analytic2$rstudent.values)\n\n\n\nShow the codewhich(analytic2$rstudent.values < -5)\n#> integer(0)\n# Heteroskedasticity: Test for constant variance\n#ols_test_breusch_pagan(fit0, rhs = TRUE)\n\n\nDeleting suspicious data\n\nShow the code# condition 1: triglycerides above 1500 needs deleting\nanalytic2b <- subset(analytic2, triglycerides < 1500)\ndim(analytic2b)\n#> [1] 2632   34\n\n# condition 2: leverage above 0.05 needs deleting\nanalytic3 <- subset(analytic2b, lev < 0.05)\ndim(analytic3)\n#> [1] 2632   34\n\n# Check how many observations are deleted\nnrow(analytic2)-nrow(analytic3)\n#> [1] 2\n\n\nRefitting in cleaned data\n\nShow the code### Re-fit in data analytic3 (without problematic data)\nformula0\n#> cholesterol ~ triglycerides\nfit0 <- lm(formula0,data = analytic3)\n\nrequire(Publish)\npublish(fit0)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\nShow the code\nrequire(car)\n# component+residual plot or partial-residual plot\ncrPlots(fit0)\n\n\n\n\npolynomial order 2\n\nShow the codeformula1 <- as.formula(\"cholesterol~poly(triglycerides,2)\")\nformula1 <- as.formula(\"cholesterol~triglycerides^2\")\nfit1 <- lm(formula1,data = analytic3)\npublish(fit1)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit1)\n\n\n\nShow the code\n# compare fit0 and fit1 models\nanova(fit0,fit1)\n\n\n\n  \n\n\n\npolynomial order 3\n\nShow the code# Fit a polynomial of order 3\nformula2 <- as.formula(\"cholesterol~poly(triglycerides,3)\")\nformula2 <- as.formula(\"cholesterol~triglycerides^3\")\nfit2 <- lm(formula2,data = analytic3)\npublish(fit2)\n#>       Variable Units Coefficient           CI.95 p-value \n#>    (Intercept)            171.74 [169.37;174.11] < 1e-04 \n#>  triglycerides              0.13     [0.11;0.14] < 1e-04\n\n# Partial Residual Plots\ncrPlots(fit2)\n\n\n\nShow the code\n# compare fit1 and fit2 models\nanova(fit1,fit2)\n\n\n\n  \n\n\n\nMultiple covariates\n\nShow the code# include everything!\nformula3 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit3 <- lm(formula3, data = analytic3)\npublish(fit3)\n#>               Variable                            Units Coefficient           CI.95     p-value \n#>            (Intercept)                                       280.02 [133.42;426.62]   0.0001852 \n#>                 gender                           Female         Ref                             \n#>                                                    Male      -11.99  [-16.41;-7.57]     < 1e-04 \n#>                    age                                         0.35     [0.23;0.47]     < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                             \n#>                                                  Others        7.52    [3.68;11.36]   0.0001270 \n#>                   race                            Black         Ref                             \n#>                                                Hispanic       -6.15  [-10.87;-1.44]   0.0106253 \n#>                                                   Other       -5.37   [-10.92;0.18]   0.0579281 \n#>                                                   White       -0.95    [-5.21;3.30]   0.6603698 \n#>              education                          College         Ref                             \n#>                                             High.School        2.90    [-0.28;6.08]   0.0743132 \n#>                                                  School       -2.54    [-8.61;3.54]   0.4134016 \n#>                married                          Married         Ref                             \n#>                                           Never.married       -5.72   [-9.63;-1.81]   0.0041887 \n#>                                      Previously.married        0.31    [-3.54;4.17]   0.8730460 \n#>                 income                             <25k         Ref                             \n#>                                        Between.25kto54k       -0.97    [-4.87;2.93]   0.6261315 \n#>                                        Between.55kto99k        2.29    [-1.98;6.56]   0.2928564 \n#>                                                Over100k        2.44    [-2.27;7.14]   0.3099380 \n#>            diastolicBP                                         0.38     [0.25;0.50]     < 1e-04 \n#>             systolicBP                                         0.02    [-0.08;0.12]   0.6668119 \n#>                    bmi                                        -2.55   [-4.29;-0.81]   0.0041392 \n#>             bodyweight                                         0.82     [0.19;1.45]   0.0105518 \n#>             bodyheight                                        -0.89   [-1.55;-0.24]   0.0074286 \n#>                  waist                                        -0.02    [-0.29;0.26]   0.9020424 \n#>          triglycerides                                         0.12     [0.11;0.14]     < 1e-04 \n#>              uric.acid                                         1.27     [0.08;2.47]   0.0369190 \n#>                protein                                         4.99   [-0.77;10.74]   0.0897748 \n#>              bilirubin                                        -5.43  [-10.53;-0.33]   0.0370512 \n#>             phosphorus                                        -0.18    [-2.81;2.45]   0.8939361 \n#>                 sodium                                        -0.97   [-1.66;-0.29]   0.0052516 \n#>              potassium                                         1.04    [-3.44;5.52]   0.6487979 \n#>               globulin                                        -2.25    [-8.22;3.71]   0.4591138 \n#>                calcium                                        12.02    [6.98;17.07]     < 1e-04 \n#>          physical.work                               No         Ref                             \n#>                                                     Yes       -0.45    [-3.68;2.79]   0.7858787 \n#>  physical.recreational                               No         Ref                             \n#>                                                     Yes        1.35    [-1.94;4.65]   0.4210703 \n#>               diabetes                               No         Ref                             \n#>                                                     Yes      -19.11 [-23.37;-14.85]     < 1e-04\n\n\nColinearity\n\n\nRule of thumb: variables with VIF > 4 needs further investigation\n\nShow the codecar::vif(fit3)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.694171  1        1.641393\n#> age                     2.164388  1        1.471186\n#> born                    1.611478  1        1.269440\n#> race                    2.463445  3        1.162137\n#> education               1.435876  2        1.094660\n#> married                 1.481141  2        1.103187\n#> income                  1.402249  3        1.057964\n#> diastolicBP             1.271126  1        1.127442\n#> systolicBP              1.594986  1        1.262928\n#> bmi                    81.811969  1        9.044997\n#> bodyweight            101.102349  1       10.054966\n#> bodyheight             21.863188  1        4.675809\n#> waist                  11.913719  1        3.451626\n#> triglycerides           1.219331  1        1.104233\n#> uric.acid               1.603290  1        1.266211\n#> protein                 3.622385  1        1.903256\n#> bilirubin               1.185035  1        1.088593\n#> phosphorus              1.116982  1        1.056874\n#> sodium                  1.120920  1        1.058735\n#> potassium               1.178381  1        1.085533\n#> globulin                3.371211  1        1.836086\n#> calcium                 1.591677  1        1.261617\n#> physical.work           1.087315  1        1.042744\n#> physical.recreational   1.226830  1        1.107624\n#> diabetes                1.210715  1        1.100325\ncollinearity <- ols_vif_tol(fit3)\ncollinearity\n\n\n\n  \n\n\nShow the code\n# VIF > 4\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + # bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit4 <- lm(formula4, data = analytic3)\npublish(fit4)\n#>               Variable                            Units Coefficient           CI.95    p-value \n#>            (Intercept)                                       136.87  [34.96;238.79]   0.008533 \n#>                 gender                           Female         Ref                            \n#>                                                    Male      -13.06  [-16.60;-9.53]    < 1e-04 \n#>                    age                                         0.35     [0.24;0.46]    < 1e-04 \n#>                   born Born in 50 US states or Washingt         Ref                            \n#>                                                  Others        7.88    [4.06;11.69]    < 1e-04 \n#>                   race                            Black         Ref                            \n#>                                                Hispanic       -5.79  [-10.34;-1.24]   0.012740 \n#>                                                   Other       -4.88   [-10.33;0.57]   0.079497 \n#>                                                   White       -0.85    [-5.02;3.33]   0.690720 \n#>              education                          College         Ref                            \n#>                                             High.School        2.85    [-0.32;6.02]   0.078008 \n#>                                                  School       -2.45    [-8.49;3.60]   0.427694 \n#>                married                          Married         Ref                            \n#>                                           Never.married       -5.74   [-9.65;-1.83]   0.004088 \n#>                                      Previously.married        0.34    [-3.52;4.20]   0.861981 \n#>                 income                             <25k         Ref                            \n#>                                        Between.25kto54k       -0.87    [-4.77;3.03]   0.663123 \n#>                                        Between.55kto99k        2.46    [-1.79;6.71]   0.256585 \n#>                                                Over100k        2.63    [-2.07;7.32]   0.272886 \n#>            diastolicBP                                         0.37     [0.25;0.50]    < 1e-04 \n#>             systolicBP                                         0.03    [-0.07;0.13]   0.544971 \n#>                    bmi                                        -0.31   [-0.54;-0.08]   0.009302 \n#>          triglycerides                                         0.12     [0.11;0.14]    < 1e-04 \n#>              uric.acid                                         1.36     [0.16;2.55]   0.025926 \n#>                protein                                         4.77   [-0.98;10.51]   0.104059 \n#>              bilirubin                                        -6.06  [-11.14;-0.98]   0.019519 \n#>             phosphorus                                        -0.08    [-2.71;2.55]   0.954561 \n#>                 sodium                                        -1.03   [-1.71;-0.35]   0.003175 \n#>              potassium                                         0.89    [-3.58;5.37]   0.695615 \n#>               globulin                                        -2.20    [-8.15;3.75]   0.469150 \n#>                calcium                                        12.20    [7.16;17.25]    < 1e-04 \n#>          physical.work                               No         Ref                            \n#>                                                     Yes       -0.44    [-3.68;2.80]   0.790297 \n#>  physical.recreational                               No         Ref                            \n#>                                                     Yes        1.24    [-2.03;4.51]   0.457666 \n#>               diabetes                               No         Ref                            \n#>                                                     Yes      -19.03 [-23.26;-14.80]    < 1e-04\n\n# check if there is still any problematic variable\n# with high collinearity problem\ncollinearity <- ols_vif_tol(fit4)\ncollinearity[collinearity$VIF>4,]\n\n\n\n  \n\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")"
  },
  {
    "objectID": "predictivefactors3.html",
    "href": "predictivefactors3.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Explore relationships for binary outcome variable\nLoad data\n\nShow the codeload(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")\n\n\nCreating binary variable\nLet us create a binary variable using the ifelse function:\n\nShow the code# Binary variable\nanalytic3$cholesterol.bin <- ifelse(analytic3$cholesterol < 200, \"healthy\", \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>      1586      1046\n\n# Changing the reference category\nanalytic3$cholesterol.bin <- as.factor(analytic3$cholesterol.bin)\nanalytic3$cholesterol.bin <- relevel(analytic3$cholesterol.bin, ref = \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#> \n#> unhealthy   healthy \n#>      1046      1586\n\n\nModelling data\n\n\n\n\n\n\nTip\n\n\n\nWe use the glm function to run generalized linear models. The default family is gaussian with identity link. Setting binomial family with logit link (logit link is default for binomial family) means fitting logistic regression.\n\n\n\nShow the code# Regression model\nformula5x <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\n\n# Summary\nfit5x <- glm(formula5x, family = binomial(), data = analytic3)\npublish(fit5x)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.68 [1.27;2.23]   0.000313 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.69 [0.54;0.88]   0.002636 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.29 [0.95;1.75]   0.107104 \n#>                                                   Other      1.24 [0.87;1.79]   0.234062 \n#>                                                   White      1.10 [0.83;1.44]   0.505147 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.082168 \n#>                                                  School      1.23 [0.84;1.82]   0.292070 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.67]   0.052969 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.345688 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.01 [0.78;1.29]   0.957537 \n#>                                        Between.55kto99k      0.90 [0.69;1.19]   0.462854 \n#>                                                Over100k      0.90 [0.66;1.21]   0.472137 \n#>            diastolicBP                                       0.98 [0.97;0.98]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.029513 \n#>                    bmi                                       1.11 [0.99;1.24]   0.065627 \n#>             bodyweight                                       0.96 [0.92;1.00]   0.045338 \n#>             bodyheight                                       1.05 [1.00;1.09]   0.030995 \n#>                  waist                                       1.01 [0.99;1.02]   0.464825 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.273792 \n#>                protein                                       0.61 [0.42;0.89]   0.009192 \n#>              bilirubin                                       1.19 [0.86;1.66]   0.292632 \n#>             phosphorus                                       0.96 [0.81;1.13]   0.610931 \n#>                 sodium                                       1.06 [1.02;1.11]   0.007980 \n#>              potassium                                       0.95 [0.71;1.26]   0.729218 \n#>               globulin                                       1.38 [0.94;2.01]   0.101667 \n#>                calcium                                       0.64 [0.47;0.89]   0.007026 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.392539 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.05 [0.85;1.29]   0.681388 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.68 [2.02;3.56]    < 1e-04\n\n# VIF\ncar::vif(fit5x)\n#>                             GVIF Df GVIF^(1/(2*Df))\n#> gender                  2.735258  1        1.653862\n#> age                     2.121098  1        1.456399\n#> born                    1.664094  1        1.289998\n#> race                    2.585539  3        1.171544\n#> education               1.458430  2        1.098933\n#> married                 1.432595  2        1.094034\n#> income                  1.426911  3        1.061043\n#> diastolicBP             1.297308  1        1.138994\n#> systolicBP              1.614374  1        1.270580\n#> bmi                    81.928815  1        9.051454\n#> bodyweight            103.125772  1       10.155086\n#> bodyheight             22.647853  1        4.758976\n#> waist                  11.493710  1        3.390237\n#> triglycerides           1.258340  1        1.121758\n#> uric.acid               1.636512  1        1.279262\n#> protein                 3.684816  1        1.919587\n#> bilirubin               1.186181  1        1.089119\n#> phosphorus              1.117915  1        1.057315\n#> sodium                  1.123193  1        1.059808\n#> potassium               1.181358  1        1.086903\n#> globulin                3.427401  1        1.851324\n#> calcium                 1.543019  1        1.242183\n#> physical.work           1.090958  1        1.044490\n#> physical.recreational   1.218558  1        1.103883\n#> diabetes                1.212365  1        1.101074\n\n\nAUC\nLet us measure the accuracy for classification models fit5x.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the roc function to build a ROC curve and auc function to calculate the AUC (are under the ROC curve) value.\n\n\n\nShow the coderequire(pROC)\npred.y <- predict(fit5x, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7411\n\nauc(rocobj)\n#> Area under the curve: 0.7411\n\n\nRe-modelling\nLet us re-fit the model and measure the AUC:\n\nShow the codeformula5 <- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\npublish(fit5)\n#>               Variable                            Units OddsRatio       CI.95    p-value \n#>                 gender                           Female       Ref                        \n#>                                                    Male      1.86 [1.48;2.33]    < 1e-04 \n#>                    age                                       0.97 [0.97;0.98]    < 1e-04 \n#>                   born Born in 50 US states or Washingt       Ref                        \n#>                                                  Others      0.67 [0.52;0.85]   0.001233 \n#>                   race                            Black       Ref                        \n#>                                                Hispanic      1.26 [0.94;1.69]   0.128165 \n#>                                                   Other      1.21 [0.85;1.73]   0.284083 \n#>                                                   White      1.11 [0.85;1.45]   0.460652 \n#>              education                          College       Ref                        \n#>                                             High.School      0.84 [0.68;1.02]   0.083806 \n#>                                                  School      1.22 [0.83;1.80]   0.305514 \n#>                married                          Married       Ref                        \n#>                                           Never.married      1.29 [1.00;1.68]   0.049983 \n#>                                      Previously.married      0.89 [0.70;1.13]   0.339401 \n#>                 income                             <25k       Ref                        \n#>                                        Between.25kto54k      1.00 [0.78;1.28]   0.999445 \n#>                                        Between.55kto99k      0.90 [0.68;1.17]   0.425427 \n#>                                                Over100k      0.89 [0.66;1.20]   0.447012 \n#>            diastolicBP                                       0.98 [0.97;0.99]    < 1e-04 \n#>             systolicBP                                       1.01 [1.00;1.01]   0.042769 \n#>                    bmi                                       1.01 [0.99;1.02]   0.496430 \n#>          triglycerides                                       0.99 [0.99;0.99]    < 1e-04 \n#>              uric.acid                                       0.96 [0.89;1.03]   0.242942 \n#>                protein                                       0.62 [0.43;0.89]   0.010343 \n#>              bilirubin                                       1.24 [0.89;1.72]   0.203993 \n#>             phosphorus                                       0.95 [0.80;1.12]   0.539847 \n#>                 sodium                                       1.06 [1.02;1.11]   0.006777 \n#>              potassium                                       0.96 [0.72;1.28]   0.790080 \n#>               globulin                                       1.37 [0.94;2.00]   0.102430 \n#>                calcium                                       0.64 [0.46;0.88]   0.005772 \n#>          physical.work                               No       Ref                        \n#>                                                     Yes      0.91 [0.74;1.12]   0.382281 \n#>  physical.recreational                               No       Ref                        \n#>                                                     Yes      1.04 [0.85;1.29]   0.682962 \n#>               diabetes                               No       Ref                        \n#>                                                     Yes      2.69 [2.03;3.57]    < 1e-04\n\n# VIF\ncar::vif(fit5)\n#>                           GVIF Df GVIF^(1/(2*Df))\n#> gender                1.749947  1        1.322856\n#> age                   1.850160  1        1.360206\n#> born                  1.640947  1        1.280994\n#> race                  2.345460  3        1.152669\n#> education             1.430721  2        1.093676\n#> married               1.432015  2        1.093923\n#> income                1.409064  3        1.058819\n#> diastolicBP           1.289411  1        1.135523\n#> systolicBP            1.605248  1        1.266984\n#> bmi                   1.477795  1        1.215646\n#> triglycerides         1.246395  1        1.116421\n#> uric.acid             1.624039  1        1.274378\n#> protein               3.648367  1        1.910070\n#> bilirubin             1.177643  1        1.085193\n#> phosphorus            1.114298  1        1.055603\n#> sodium                1.117463  1        1.057101\n#> potassium             1.176914  1        1.084857\n#> globulin              3.395946  1        1.842809\n#> calcium               1.542486  1        1.241969\n#> physical.work         1.089742  1        1.043907\n#> physical.recreational 1.197719  1        1.094404\n#> diabetes              1.200402  1        1.095629\n\n\n\nShow the code#### AUC\npred.y <- predict(fit5, type = \"response\")\nrocobj <- roc(analytic3$cholesterol.bin, pred.y)\n#> Setting levels: control = unhealthy, case = healthy\n#> Setting direction: controls < cases\nrocobj\n#> \n#> Call:\n#> roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#> \n#> Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) < 1586 cases (analytic3$cholesterol.bin healthy).\n#> Area under the curve: 0.7406\nauc(rocobj)\n#> Area under the curve: 0.7406\n\n\nSave data\n\nShow the codesave.image(file = \"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nReferences"
  },
  {
    "objectID": "predictivefactors4.html",
    "href": "predictivefactors4.html",
    "title": "Overfitting and performance",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nNow we will fit the final model that we decided at the end of previous part of the lab.\n\nShow the codeformula4 <- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4 <- lm(formula4, data = analytic3)\nsummary(fit4)\n#> \n#> Call:\n#> lm(formula = formula4, data = analytic3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -115.465  -23.695   -2.598   20.017  177.264 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               136.871606  51.998527   2.632  0.00853 ** \n#> genderMale                -13.064857   1.802099  -7.250 5.48e-13 ***\n#> age                         0.351838   0.056116   6.270 4.22e-10 ***\n#> bornOthers                  7.877420   1.947498   4.045 5.39e-05 ***\n#> raceHispanic               -5.790547   2.323010  -2.493  0.01274 *  \n#> raceOther                  -4.879882   2.781673  -1.754  0.07950 .  \n#> raceWhite                  -0.847635   2.130149  -0.398  0.69072    \n#> educationHigh.School        2.851633   1.617435   1.763  0.07801 .  \n#> educationSchool            -2.446765   3.084409  -0.793  0.42769    \n#> marriedNever.married       -5.739509   1.997152  -2.874  0.00409 ** \n#> marriedPreviously.married   0.342206   1.968165   0.174  0.86198    \n#> incomeBetween.25kto54k     -0.867063   1.990253  -0.436  0.66312    \n#> incomeBetween.55kto99k      2.462130   2.169757   1.135  0.25658    \n#> incomeOver100k              2.626046   2.394560   1.097  0.27289    \n#> diastolicBP                 0.374971   0.062238   6.025 1.93e-09 ***\n#> systolicBP                  0.029976   0.049515   0.605  0.54497    \n#> bmi                        -0.309530   0.118927  -2.603  0.00930 ** \n#> triglycerides               0.124806   0.006427  19.419  < 2e-16 ***\n#> uric.acid                   1.357242   0.609012   2.229  0.02593 *  \n#> protein                     4.767008   2.931636   1.626  0.10406    \n#> bilirubin                  -6.060791   2.593508  -2.337  0.01952 *  \n#> phosphorus                 -0.076472   1.341957  -0.057  0.95456    \n#> sodium                     -1.026686   0.347679  -2.953  0.00318 ** \n#> potassium                   0.893507   2.283488   0.391  0.69561    \n#> globulin                   -2.198037   3.036091  -0.724  0.46915    \n#> calcium                    12.202366   2.574400   4.740 2.25e-06 ***\n#> physical.workYes           -0.439108   1.651078  -0.266  0.79030    \n#> physical.recreationalYes    1.238756   1.667670   0.743  0.45767    \n#> diabetesYes               -19.032748   2.158825  -8.816  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 35.22 on 2603 degrees of freedom\n#> Multiple R-squared:  0.2415, Adjusted R-squared:  0.2334 \n#> F-statistic: 29.61 on 28 and 2603 DF,  p-value: < 2.2e-16\n\n\nDesign Matrix\nExpands factors to a set of dummy variables.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the model.matrix function to construct a design/model matrix, such as expand factor variables to a matrix of dummy variable\n\n\n\nShow the codehead(model.matrix(fit4))\n#>    (Intercept) genderMale age bornOthers raceHispanic raceOther raceWhite\n#> 1            1          1  62          0            0         0         1\n#> 2            1          1  53          1            0         0         1\n#> 4            1          0  56          0            0         0         1\n#> 5            1          0  42          0            0         0         0\n#> 10           1          1  22          0            0         0         0\n#> 11           1          0  32          1            1         0         0\n#>    educationHigh.School educationSchool marriedNever.married\n#> 1                     0               0                    0\n#> 2                     1               0                    0\n#> 4                     0               0                    0\n#> 5                     0               0                    0\n#> 10                    0               0                    1\n#> 11                    0               0                    0\n#>    marriedPreviously.married incomeBetween.25kto54k incomeBetween.55kto99k\n#> 1                          0                      0                      1\n#> 2                          1                      0                      0\n#> 4                          0                      0                      1\n#> 5                          1                      1                      0\n#> 10                         0                      1                      0\n#> 11                         0                      1                      0\n#>    incomeOver100k diastolicBP systolicBP  bmi triglycerides uric.acid protein\n#> 1               0          70        128 27.8           158       4.2     7.5\n#> 2               0          88        146 30.8           170       7.0     7.4\n#> 4               0          72        132 42.4            93       5.4     6.1\n#> 5               0          70        100 20.3            52       3.3     7.7\n#> 10              0          70        110 28.0            77       6.0     7.4\n#> 11              0          70        120 28.2           295       5.2     7.4\n#>    bilirubin phosphorus sodium potassium globulin calcium physical.workYes\n#> 1        0.5        4.7    136      4.30      2.9     9.8                0\n#> 2        0.6        4.4    140      4.55      2.9     9.8                0\n#> 4        0.3        3.8    141      4.08      2.3     8.9                0\n#> 5        0.3        3.2    136      3.50      3.4     9.3                0\n#> 10       0.2        5.3    139      4.16      3.0     9.3                0\n#> 11       0.4        3.1    138      4.31      2.9    10.3                0\n#>    physical.recreationalYes diabetesYes\n#> 1                         0           1\n#> 2                         0           0\n#> 4                         0           0\n#> 5                         0           0\n#> 10                        1           0\n#> 11                        0           0\n\n# Dimension of the model matrix\ndim(model.matrix(fit4))\n#> [1] 2632   29\n\n# Number of parameters = intercept + slopes\np <- dim(model.matrix(fit4))[2] \np\n#> [1] 29\n\n\nCheck prediction\n\nShow the codeobs.y <- analytic3$cholesterol\nsummary(obs.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    81.0   163.0   189.0   191.5   216.0   362.0\n\n# Predict the above fit on analytic3 data\npred.y <- predict(fit4, analytic3)\nsummary(pred.y)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   136.3   178.2   189.4   191.5   202.4   337.6\nn <- length(pred.y)\nn\n#> [1] 2632\nplot(obs.y,pred.y)\nlines(lowess(obs.y,pred.y), col = \"red\")\n\n\n\nShow the code\n# Prediction on a new data: fictitious.data\nstr(fictitious.data)\n#> 'data.frame':    4121 obs. of  33 variables:\n#>  $ ID                   : num  83732 83733 83734 83735 83736 ...\n#>  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n#>  $ age                  : num  62 53 78 56 42 72 22 32 56 46 ...\n#>  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#>  $ race                 : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ education            : chr  \"College\" \"High.School\" \"High.School\" \"College\" ...\n#>  $ married              : chr  \"Married\" \"Previously.married\" \"Married\" \"Married\" ...\n#>  $ income               : chr  \"Between.55kto99k\" \"<25k\" \"<25k\" \"Between.55kto99k\" ...\n#>  $ weight               : num  135630 25282 12576 102079 18235 ...\n#>  $ psu                  : num  1 1 1 1 2 1 2 1 2 1 ...\n#>  $ strata               : num  125 125 131 131 126 128 128 125 126 121 ...\n#>  $ diastolicBP          : num  70 88 46 72 70 58 70 70 116 94 ...\n#>  $ systolicBP           : num  128 146 138 132 100 116 110 120 178 144 ...\n#>  $ bodyweight           : num  94.8 90.4 83.4 109.8 55.2 ...\n#>  $ bodyheight           : num  184 171 170 161 165 ...\n#>  $ bmi                  : num  27.8 30.8 28.8 42.4 20.3 28.6 28 28.2 33.6 27.6 ...\n#>  $ waist                : num  101.1 107.9 116.5 110.1 80.4 ...\n#>  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Not.at.all\" \"Not.at.all\" ...\n#>  $ alcohol              : num  1 6 0 1 1 0 8 1 0 1 ...\n#>  $ cholesterol          : num  173 265 229 174 204 190 164 190 145 242 ...\n#>  $ cholesterolM2        : num  4.47 6.85 5.92 4.5 5.28 4.91 4.24 4.91 3.75 6.26 ...\n#>  $ triglycerides        : num  158 170 299 93 52 52 77 295 121 497 ...\n#>  $ uric.acid            : num  4.2 7 7.3 5.4 3.3 4.9 6 5.2 4.8 6.5 ...\n#>  $ protein              : num  7.5 7.4 7.3 6.1 7.7 7.1 7.4 7.4 6.9 6.8 ...\n#>  $ bilirubin            : num  0.5 0.6 0.5 0.3 0.3 0.5 0.2 0.4 0.4 0.5 ...\n#>  $ phosphorus           : num  4.7 4.4 3.6 3.8 3.2 3.7 5.3 3.1 4.1 3.6 ...\n#>  $ sodium               : num  136 140 140 141 136 140 139 138 140 138 ...\n#>  $ potassium            : num  4.3 4.55 4.7 4.08 3.5 4.2 4.16 4.31 4.5 4.27 ...\n#>  $ globulin             : num  2.9 2.9 2.8 2.3 3.4 3 3 2.9 2.9 2.6 ...\n#>  $ calcium              : num  9.8 9.8 9.7 8.9 9.3 9.3 9.3 10.3 9.5 9.3 ...\n#>  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ physical.recreational: chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ diabetes             : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:885] 16 30 39 48 50 58 61 65 67 68 ...\n#>   ..- attr(*, \"names\")= chr [1:885] \"27\" \"68\" \"90\" \"112\" ...\npred.y.new1 <- predict(fit4, fictitious.data)\nsummary(pred.y.new1)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   128.7   178.9   190.6   192.5   203.3   557.4\n\n\nMeasuring prediction error\nContinuous outcomes\nR2\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find SSE\nSSE <- sum( (obs.y - pred.y)^2 )\nSSE\n#> [1] 3228460\n\n# Find SST\nmean.obs.y <- mean(obs.y)\nSST <- sum( (obs.y - mean.obs.y)^2 )\nSST\n#> [1] 4256586\n\n# Find R2\nR.2 <- 1- SSE/SST\nR.2\n#> [1] 0.2415378\n\nrequire(caret)\nR2(pred.y, obs.y)\n#> [1] 0.2415378\n\n\nRMSE\n\n\nSee Wikipedia (2023b)\n\nShow the code# Find RMSE\nRmse <- sqrt(SSE/(n-p)) \nRmse\n#> [1] 35.21767\n\nRMSE(pred.y, obs.y)\n#> [1] 35.02311\n\n\nAdj R2\n\n\nSee Wikipedia (2023a)\n\nShow the code# Find adj R2\nadjR2 <- 1-(1-R.2)*((n-1)/(n-p))\nadjR2\n#> [1] 0.2333791\n\n\nWriting function\nSyntax for Writing Functions\n\nShow the codefunc_name <- function (argument) {\n  A statement or multiple lines of statements\n  return(output)\n}\n\n\nExample of a simple function\n\nShow the codef1 <- function(a,b){\n  result <- a + b\n  return(result)\n}\nf1(a=1,b=3)\n#> [1] 4\nf1(a=1,b=6)\n#> [1] 7\n# setting default values\nf1 <- function(a=1,b=1){\n  result <- a + b\n  return(result)\n}\nf1()\n#> [1] 2\nf1(b = 10)\n#> [1] 11\n\n\nA bit more complicated\n\nShow the code# one argument\nmodel.fit <- function(data.for.fitting){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#> 184.3131838  -7.8095595   0.2225745  11.1557140\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#> 176.1286576  -4.8256829   0.3375009   7.7186190\n\n\n\nShow the code# adding one more argument: digits\nmodel.fit <- function(data.for.fitting, digits=2){\n  formulax <- as.formula(\"cholesterol~gender + age + born\")\n  fitx <- lm(formulax, data = data.for.fitting)\n  result <- coef(fitx)\n  result <- round(result,digits)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#> (Intercept)  genderMale         age  bornOthers \n#>      184.31       -7.81        0.22       11.16\nmodel.fit(data.for.fitting=analytic3)\n#> (Intercept)  genderMale         age  bornOthers \n#>      176.13       -4.83        0.34        7.72\n\n\nFunction that gives performance measures\nlet us create a function that will give us the performance measures:\n\nShow the codeperform <- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p <- dim(model.matrix(model.fit))[2]\n  \n  # predicted value\n  pred.y <- predict(model.fit, new.data)\n  \n  # sample size\n  n <- length(pred.y)\n  \n  # outcome\n  new.data.y <- as.numeric(new.data[,y.name])\n  \n  # R2\n  R2 <- caret:::R2(pred.y, new.data.y)\n  \n  # adj R2 using alternate formula\n  df.residual <- n-p\n  adjR2 <- 1-(1-R2)*((n-1)/df.residual)\n  \n  # RMSE\n  RMSE <-  caret:::RMSE(pred.y, new.data.y)\n  \n  # combine all of the results\n  res <- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  \n  # returning object\n  return(res)\n}\nperform(new.data = analytic3, y.name = \"cholesterol\", model.fit = fit4)\n#>         n  p    R2 adjR2   RMSE\n#> [1,] 2632 29 0.242 0.233 35.023\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance."
  },
  {
    "objectID": "predictivefactors5.html",
    "href": "predictivefactors5.html",
    "title": "Data spliting",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data anf files\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nData spliting to avoid model overfitting\n\n\nKDnuggets (2023)\n\nKuhn (2023a)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use the createDataPartition function to split a dataset into training and testing datasets\n\n\n\nShow the code# Using a seed to randomize in a reproducible way \nset.seed(123)\nsplit <- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)\nstr(split)\n#>  int [1:1844, 1] 3 4 5 8 9 13 14 16 20 21 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr \"Resample1\"\ndim(split)\n#> [1] 1844    1\n\n# Approximate train data\ndim(analytic3)*.7 \n#> [1] 1842.4   24.5\n\n# Approximate test data\ndim(analytic3)*(1-.7) \n#> [1] 789.6  10.5\n\n\nSplit the data\nNow let us split the dataset into training and testing:\n\nShow the code# Create train data\ntrain.data <- analytic3[split,]\ndim(train.data)\n#> [1] 1844   35\n\n# Create test data\ntest.data <- analytic3[-split,]\ndim(test.data)\n#> [1] 788  35\n\n\nOur next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.\nTrain the model\n\nShow the codeformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\nfit4.train1 <- lm(formula4, data = train.data)\nsummary(fit4.train1)\n#> \n#> Call:\n#> lm(formula = formula4, data = train.data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -91.973 -23.719  -1.563  20.586 178.542 \n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                72.716792  59.916086   1.214  0.22504    \n#> genderMale                -11.293629   2.136545  -5.286 1.40e-07 ***\n#> age                         0.306235   0.066376   4.614 4.23e-06 ***\n#> bornOthers                  7.220858   2.300658   3.139  0.00172 ** \n#> raceHispanic               -6.727473   2.709718  -2.483  0.01313 *  \n#> raceOther                  -4.865771   3.237066  -1.503  0.13298    \n#> raceWhite                  -1.468522   2.494981  -0.589  0.55621    \n#> educationHigh.School        1.626097   1.920289   0.847  0.39722    \n#> educationSchool            -4.853095   3.585185  -1.354  0.17602    \n#> marriedNever.married       -5.298265   2.332033  -2.272  0.02321 *  \n#> marriedPreviously.married   1.202448   2.305191   0.522  0.60199    \n#> incomeBetween.25kto54k     -1.736495   2.360385  -0.736  0.46202    \n#> incomeBetween.55kto99k      0.170505   2.565896   0.066  0.94703    \n#> incomeOver100k              1.712359   2.860226   0.599  0.54946    \n#> diastolicBP                 0.355813   0.074380   4.784 1.86e-06 ***\n#> systolicBP                  0.037464   0.059848   0.626  0.53140    \n#> bmi                        -0.282881   0.139160  -2.033  0.04222 *  \n#> triglycerides               0.123797   0.007613  16.261  < 2e-16 ***\n#> uric.acid                   1.006499   0.712871   1.412  0.15815    \n#> protein                     1.721623   3.468969   0.496  0.61975    \n#> bilirubin                  -6.143411   3.006858  -2.043  0.04118 *  \n#> phosphorus                  0.093824   1.575489   0.060  0.95252    \n#> sodium                     -0.604286   0.400694  -1.508  0.13170    \n#> potassium                  -0.583525   2.715189  -0.215  0.82986    \n#> globulin                   -0.278970   3.614404  -0.077  0.93849    \n#> calcium                    15.679677   3.054968   5.133 3.17e-07 ***\n#> physical.workYes           -1.099540   1.960321  -0.561  0.57494    \n#> physical.recreationalYes    0.834737   1.953960   0.427  0.66928    \n#> diabetesYes               -19.932101   2.580138  -7.725 1.83e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 34.68 on 1815 degrees of freedom\n#> Multiple R-squared:  0.2433, Adjusted R-squared:  0.2316 \n#> F-statistic: 20.84 on 28 and 1815 DF,  p-value: < 2.2e-16\n\n\nExtract performance measures\n\n\n\n\n\n\nTip\n\n\n\nBelow we use the perform function that we saved to evaluate the model performances\n\n\n\nShow the codeperform(new.data = train.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma   logLik      AIC\n#> [1,] 1844 29        1815 2182509 2884109 0.243 0.232 34.677 -9140.98 18341.96\n#>           BIC\n#> [1,] 18507.55\nperform(new.data = test.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>        n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 788 29         759 1057454 1372214 0.229 0.201 37.326 -3955.936 7971.873\n#>           BIC\n#> [1,] 8111.958\nperform(new.data = analytic3,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2 sigma    logLik      AIC\n#> [1,] 2632 29        2603 3239962 4256586 0.239 0.231 35.28 -13098.82 26257.64\n#>           BIC\n#> [1,] 26433.91\nperform(new.data = fictitious.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#>         n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 4121 29        4092 5306559 6912485 0.232 0.227 36.011 -20601.92 41263.84\n#>           BIC\n#> [1,] 41453.55\n\n\n\n\nFor more on model training and tuning, see Kuhn (2023b)\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023a. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.\n\n\n———. 2023b. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html."
  },
  {
    "objectID": "predictivefactors6.html",
    "href": "predictivefactors6.html",
    "title": "Cross-vaildation",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nk-fold cross-vaildation\n\n\nSee Wikipedia (2023)\n\nShow the codek = 5\ndim(analytic3)\n#> [1] 2632   35\nset.seed(567)\n\n# Create folds (based on the outcome)\nfolds <- createFolds(analytic3$cholesterol, k = k, list = TRUE, \n                     returnTrain = TRUE)\nmode(folds)\n#> [1] \"list\"\n\n# Approximate training data size\ndim(analytic3)*4/5\n#> [1] 2105.6   28.0\n\n# Approximate test data size\ndim(analytic3)/5  \n#> [1] 526.4   7.0\n\nlength(folds[[1]])\n#> [1] 2105\nlength(folds[[2]])\n#> [1] 2107\nlength(folds[[3]])\n#> [1] 2106\nlength(folds[[4]])\n#> [1] 2105\nlength(folds[[5]])\n#> [1] 2105\n\nstr(folds[[1]])\n#>  int [1:2105] 1 3 5 6 8 10 11 12 13 14 ...\nstr(folds[[2]])\n#>  int [1:2107] 1 2 3 4 5 6 7 8 9 12 ...\nstr(folds[[3]])\n#>  int [1:2106] 2 4 5 7 8 9 10 11 12 14 ...\nstr(folds[[4]])\n#>  int [1:2105] 1 2 3 4 6 7 8 9 10 11 ...\nstr(folds[[5]])\n#>  int [1:2105] 1 2 3 4 5 6 7 9 10 11 ...\n\n\nCalculation for Fold 1\n\nShow the codefold.index <- 1\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1]  1  3  5  6  8 10\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\nformula4\n#> cholesterol ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\nmodel.fit <- lm(formula4, data = fold1.train)\npredictions <- predict(model.fit, newdata = fold1.test)\n\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual      SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 527 29         498 637317.5 830983.2 0.233  0.19 35.774 -2618.471 5296.942\n#>           BIC\n#> [1,] 5424.958\n\n\nCalculation for Fold 2\n\nShow the codefold.index <- 2\nfold1.train.ids <- folds[[fold.index]]\nhead(fold1.train.ids)\n#> [1] 1 2 3 4 5 6\n\nfold1.train <- analytic3[fold1.train.ids,]\nfold1.test <- analytic3[-fold1.train.ids,]\n\nmodel.fit <- lm(formula4, data = fold1.train)\n\npredictions <- predict(model.fit, newdata = fold1.test)\nperform(new.data=fold1.test, y.name = \"cholesterol\", model.fit = model.fit)\n#>        n  p df.residual    SSE      SST    R2 adjR2  sigma    logLik      AIC\n#> [1,] 525 29         496 615243 785326.6 0.217 0.172 35.219 -2600.282 5260.564\n#>           BIC\n#> [1,] 5388.466\n\n\nUsing caret package to automate\n\n\nSee Kuhn (2023)\n\nShow the code# Using Caret package\nset.seed(567)\n\n# make a 5-fold CV\nctrl<-trainControl(method = \"cv\",number = 5)\n\n# fit the model with formula = formula4\n# use training method lm\nfit4.cv<-train(formula4, trControl = ctrl,\n               data = analytic3, method = \"lm\")\nfit4.cv\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2105, 2106, 2105, 2106 \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.62758  0.2194187  27.85731\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# extract results from each test data \nsummary.res <- fit4.cv$resample\nsummary.res\n\n\n\n  \n\n\nShow the codemean(fit4.cv$resample$Rsquared)\n#> [1] 0.2194187\nsd(fit4.cv$resample$Rsquared)\n#> [1] 0.02755561\n\n# # extract adj R2\n# k <- 5\n# p <- 2\n# n <- round(nrow(analytic3)/k)\n# summary.res$adjR2 <- 1-(1-fit4.cv$resample$Rsquared)*((n-1)/(n-p))\n# summary.res\n\n\nReferences\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
  },
  {
    "objectID": "predictivefactors7.html",
    "href": "predictivefactors7.html",
    "title": "Bootstrap",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nShow the codeload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\n\nResampling a vector\n\nShow the codefake.data <- 1:5\nfake.data\n#> [1] 1 2 3 4 5\n\n\n\nShow the coderesampled.fake.data <- sample(fake.data, size = length(fake.data), replace = TRUE)\nresampled.fake.data\n#> [1] 4 1 3 3 1\n\nselected.fake.data <- unique(resampled.fake.data)\nselected.fake.data\n#> [1] 4 1 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#> [1] 2 5\n\n\nThe samples not selected are known as the out-of-bag samples\n\nShow the codeB <- 10\nfor (i in 1:B){\n  new.boot.sample <- sample(fake.data, size = length(fake.data), replace = TRUE)\n  print(new.boot.sample)\n}\n#> [1] 1 4 4 1 4\n#> [1] 5 3 1 3 3\n#> [1] 2 5 4 5 3\n#> [1] 4 4 3 5 4\n#> [1] 4 5 3 5 5\n#> [1] 3 5 3 3 2\n#> [1] 4 4 5 3 2\n#> [1] 1 4 2 4 3\n#> [1] 2 5 5 4 4\n#> [1] 2 3 3 5 4\n\n\nCalculating SD of a statistics\nIdea:\n\nNot sure about what distribution is appropriate to make inference?\nIf that is the case, calculating CI is hard.\nresample and get a new bootstrap sample\ncalculate a statistic (say, mean) from that sample\nfind SD of those statistic (say, means)\nUse those SD to calculate CI\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 5\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\nstr(resamples)\n#> List of 5\n#>  $ : int [1:5] 4 4 2 3 5\n#>  $ : int [1:5] 4 2 2 1 3\n#>  $ : int [1:5] 2 2 1 2 2\n#>  $ : int [1:5] 4 4 3 1 4\n#>  $ : int [1:5] 3 2 4 4 2\n\nB.means <- sapply(resamples, mean)\nB.means\n#> [1] 3.6 2.4 1.8 3.2 3.0\nmean(B.means)\n#> [1] 2.8\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.7071068\n\n\n\nShow the codemean(fake.data)\n#> [1] 3\nB <- 200\nresamples <- lapply(1:B, function(i) sample(fake.data, replace = TRUE))\n# str(resamples)\n\nB.means <- sapply(resamples, mean)\nB.medians <- sapply(resamples, median)\nmean(B.means)\n#> [1] 3.018\n\n# SD of the distribution of means\nsd(B.means)\n#> [1] 0.6366337\nmean(B.medians)\n#> [1] 3.05\nhist(B.means)\n\n\n\nShow the code\n# SD of the distribution of medians\nsd(B.medians)\n#> [1] 0.996224\nhist(B.medians)\n\n\n\n\nResampling a data or matrix\n\nShow the codeanalytic.mini <- head(analytic)\nkable(analytic.mini[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n1\n83732\nMale\n62\n\n\n2\n83733\nMale\n53\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n21\n83752\nFemale\n30\n\n\n\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n1\n83732\nMale\n62\n\n\n21.1\n83752\nFemale\n30\n\n\n1.1\n83732\nMale\n62\n\n\n1.2\n83732\nMale\n62\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83732\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83741 83747 83750\n\n\n\nShow the codeanalytic.boot <- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n21\n83752\nFemale\n30\n\n\n21.1\n83752\nFemale\n30\n\n\n21.2\n83752\nFemale\n30\n\n\n21.3\n83752\nFemale\n30\n\n\n19\n83750\nMale\n45\n\n\n\n\nShow the codeselected.subjects <- unique(analytic.boot$ID)\nselected.subjects\n#> [1] 83733 83752 83750\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#> [1] 83732 83741 83747\n\n\nThe caret package / boot\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time).\n\nShow the codeset.seed(234)\nctrl<-trainControl(method = \"boot\", number = 50)\nfit4.boot2<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared  MAE     \n#>   35.58231  0.22375   27.77634\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2$resample$Rsquared)\n#> [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#> [1] 0.01693917\n\n\nMethod boot632\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#> Linear Regression \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.33279  0.2277843  27.58945\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.02259778\n\n\nMethod boot632 for stepwise\nA stable model\n\n\nSee Kuhn (2023)\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\nShow the codectrl <- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b<-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   22 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.01922833\n\n\nAn unstable model\n\nShow the codectrl<-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b<-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", trace = 0)\nfit4.boot2b\n#> Linear Regression with Stepwise Selection \n#> \n#> 2632 samples\n#>   25 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE    \n#>   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n\n\n\n  \n\n\nShow the codemean(fit4.boot2b$resample$Rsquared)\n#> [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#> [1] 0.0176326\n\n\nNote that SD should be higher for larger B.\nOptimism corrected bootstrap\n\n\nSee Bondarenko and Consulting (2023)\nSteps:\n\nFit a model M to entire data D and estimate predictive ability R2.\nIterate from b=1 to B:\n\nTake a resample from the original data, and name it D.star\nFit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\nUse the bootstrap model M.star to get predictive ability on D, R2.fullData\n\n\nOptimism Opt is calculated as mean(R2.boot - R2.fullData)\nCalculate optimism corrected performance as R2-Opt.\n\n\nShow the codeR2.opt <- function(data, fit, B, y.name = \"cholesterol\"){\n  D <- data\n  y.index <- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M <- fit\n  pred.y <- predict(M, D)\n  n <- length(pred.y)\n  y <- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app <- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot <- vector (mode = \"numeric\", length = B)\n  R2.fullData <- vector (mode = \"numeric\", length = B)\n  opt <- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index <- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star <- D[boot.index,]\n    M.star <- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y <- predict(M.star, new.data = D.star)\n    y.index <- which(names(D.star)==y.name)\n    D.star$y <- as.numeric(D.star[,y.index])\n    R2.boot[i] <- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y <- predict(M.star, newdata=D)\n    R2.fullData[i] <- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] <- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res <- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc <- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x <- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#> $R2.oc\n#> [1] 0.2238703\n#> \n#> $R2.app\n#> [1] 0.2415378\n#> \n#> $boot.res\n#>       R2.boot R2.fullData   opt\n#>  [1,]    0.23        0.24 -0.01\n#>  [2,]    0.24        0.23  0.01\n#>  [3,]    0.26        0.24  0.03\n#>  [4,]    0.25        0.23  0.02\n#>  [5,]    0.26        0.24  0.02\n#>  [6,]    0.26        0.23  0.03\n#>  [7,]    0.21        0.24 -0.03\n#>  [8,]    0.25        0.23  0.02\n#>  [9,]    0.24        0.23  0.01\n#> [10,]    0.27        0.23  0.03\n#> [11,]    0.25        0.23  0.01\n#> [12,]    0.24        0.23  0.01\n#> [13,]    0.26        0.23  0.03\n#> [14,]    0.25        0.24  0.02\n#> [15,]    0.25        0.23  0.02\n#> [16,]    0.24        0.23  0.00\n#> [17,]    0.25        0.23  0.02\n#> [18,]    0.26        0.24  0.03\n#> [19,]    0.24        0.24  0.01\n#> [20,]    0.27        0.24  0.03\n#> [21,]    0.27        0.24  0.04\n#> [22,]    0.26        0.23  0.02\n#> [23,]    0.23        0.23  0.00\n#> [24,]    0.23        0.23  0.00\n#> [25,]    0.26        0.23  0.03\n#> [26,]    0.26        0.23  0.03\n#> [27,]    0.27        0.23  0.04\n#> [28,]    0.27        0.24  0.03\n#> [29,]    0.27        0.23  0.04\n#> [30,]    0.24        0.23  0.00\n#> [31,]    0.25        0.23  0.02\n#> [32,]    0.25        0.24  0.02\n#> [33,]    0.26        0.24  0.02\n#> [34,]    0.23        0.24  0.00\n#> [35,]    0.25        0.23  0.02\n#> [36,]    0.26        0.23  0.03\n#> [37,]    0.26        0.23  0.03\n#> [38,]    0.23        0.24  0.00\n#> [39,]    0.26        0.23  0.03\n#> [40,]    0.27        0.23  0.03\n#> [41,]    0.24        0.23  0.01\n#> [42,]    0.24        0.24  0.00\n#> [43,]    0.28        0.23  0.04\n#> [44,]    0.25        0.24  0.02\n#> [45,]    0.25        0.23  0.02\n#> [46,]    0.26        0.24  0.02\n#> [47,]    0.25        0.23  0.02\n#> [48,]    0.25        0.23  0.02\n#> [49,]    0.25        0.24  0.02\n#> [50,]    0.23        0.23 -0.01\n\n\nBinary outcome\nAUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\nAUC = 1 (perfect classification) AUC = 0.5 (random classification)\n\nShow the codeset.seed(234)\nformula5\n#> cholesterol.bin ~ gender + age + born + race + education + married + \n#>     income + diastolicBP + systolicBP + bmi + triglycerides + \n#>     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#>     globulin + calcium + physical.work + physical.recreational + \n#>     diabetes\n\n# Bootstrap\nctrl<-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot<-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (50 reps) \n#> Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#> [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#> [1] 0.01166374\n\n# CV\nctrl <- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv <- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#> Generalized Linear Model \n#> \n#> 2632 samples\n#>   22 predictor\n#>    2 classes: 'unhealthy', 'healthy' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (5 fold) \n#> Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n\n\n\n  \n\n\nShow the codemean(fit5.cv$resample$ROC)\n#> [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#> [1] 0.02683386\n\n\n\nShow the coderequire(DescTools)\nfit5 <- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#> [1] 0.1998676\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nBondarenko, Vadim, and FI Consulting. 2023. “The Bootstrap Approach to Managing Model Uncertainty.” https://rstudio-pubs-static.s3.amazonaws.com/90467_c70206f3dc864d53bf36072207ee011d.html.\n\n\nKuhn, Max. 2023. “Available Models.” https://topepo.github.io/caret/available-models.html."
  },
  {
    "objectID": "predictivefactorsF.html",
    "href": "predictivefactorsF.html",
    "title": "R functions (P)",
    "section": "",
    "text": "The list of new R functions introduced in this Predictive factors lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggregate \n    base/stats \n    To see summary by groups, e.g., by gender \n  \n\n anova \n    base/stats \n    To compare models \n  \n\n auc \n    pROC \n    To compute the AUC (area under the ROC curve) value \n  \n\n BrierScore \n    DescTools \n    To calculate the Brier score \n  \n\n coef \n    base/stats \n    To see the coefficients of a fitted model \n  \n\n cor \n    base/stats \n    To see the correlation between numeric variables \n  \n\n corrplot \n    corrplot \n    To visualize a correlation matrix \n  \n\n createDataPartition \n    caret \n    To split a dataset into training and testing sets \n  \n\n createFolds \n    caret \n    To create k folds based on the outcome variable \n  \n\n crPlots \n    car \n    To see partial residual plot \n  \n\n describeBy \n    psych \n    To see summary by groups, e.g., by gender \n  \n\n glm \n    base/stats \n    To run generalized linear models \n  \n\n group_by \n    dplyr \n    To group by variables \n  \n\n hat \n    base/stats \n    To return a hat matrix \n  \n\n ifelse \n    base \n    To set an condition, e.g., creating a categorical variable from a numerical variable based on a condition \n  \n\n kable \n    knitr \n    To create a nice table \n  \n\n layout \n    base/graphics \n    To specify plot arrangement \n  \n\n lines \n    base/graphics \n    To draw a line graph \n  \n\n lm \n    base/stats \n    To fit a linear regression \n  \n\n lowess \n    base/stats \n    To smooth a scatter plot \n  \n\n model.matrix \n    base/stats \n    To construct a design/model matrix, e.g., a matrix with covariate values \n  \n\n ols_plot_resid_lev \n    olsrr \n    To visualize the residuals vs leverage plot \n  \n\n ols_vif_tol \n    olsrr \n    To calculate tolerance and variance inflation factor \n  \n\n predict \n    base/stats \n    `predict` is a generic function that is used for prediction, e.g., predicting probability of an event from a model \n  \n\n R2 \n    caret \n    To calculate the R-squared value \n  \n\n RMSE \n    caret \n    To calculate the RMSE value \n  \n\n roc \n    pROC \n    To build a ROC curve \n  \n\n sample \n    base \n    To take/draw random samples with or without replacement \n  \n\n save.image \n    base \n    To save an R object \n  \n\n spearman2 \n    Hmisc \n    To compute the square of Spearman's rank correlation \n  \n\n summarize \n    dplyr \n    To see summary \n  \n\n tapply \n    base \n    To apply a function over an array, e.g., to see the summary of a variable by gender \n  \n\n train \n    caret \n    To fit the model with tuning hyperparameters \n  \n\n trainControl \n    caret \n    To tune the hyperparameters, i.e., controlling the parameters to train the model \n  \n\n varclus \n    Hmisc \n    We use the `varclus` function to identify collinear predictors with cluster analysis \n  \n\n vif \n    car \n    To calculate variance inflation factor \n  \n\n which \n    base \n    To see which indices are TRUE"
  },
  {
    "objectID": "predictivefactorsQ.html",
    "href": "predictivefactorsQ.html",
    "title": "Quiz (P)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydata.html#cchs-revisiting-picot",
    "href": "surveydata.html#cchs-revisiting-picot",
    "title": "Survey data analysis",
    "section": "CCHS: Revisiting PICOT",
    "text": "CCHS: Revisiting PICOT\nThe tutorial focuses on revisiting a research question concerning the relationship between osteoarthritis (OA) and cardiovascular disease (CVD) in Canadian adults, utilizing data from the Canadian Community Health Survey (CCHS) spanning from 2001 to 2005. The approach follows the PICOT framework, which specifies the target population, outcome, exposure and control groups, and timeline. The tutorial provides detailed steps for data preparation and analysis, from loading the necessary R packages to subsetting data based on a comprehensive set of variables like age, sex, marital status, and income among others. The variables are recoded into broader categories for easier analysis. The tutorial then combines different cycles of CCHS data into one comprehensive dataset. Potential confounders are also identified to better understand the relationship between OA and CVD."
  },
  {
    "objectID": "surveydata.html#cchs-assessing-data",
    "href": "surveydata.html#cchs-assessing-data",
    "title": "Survey data analysis",
    "section": "CCHS: Assessing data",
    "text": "CCHS: Assessing data\nThis tutorial provides a comprehensive guide to data preparation and quality assessment. It emphasizes the importance of checking for missing data and visualizing it, creating summary tables to look for zero-cells in variables, and generating frequency tables for various variables to examine data distribution. Specific attention is given to the presence of problematic variables. Data dictionaries from different cycles are also consulted to ensure variable compatibility. After identifying and modifying problematic data, the tutorial also explains how to set appropriate reference levels for factors in the dataset and offers an option to create a new dataset that excludes missing values (although this is not generally recommended)."
  },
  {
    "objectID": "surveydata.html#cchs-bivariate-analysis",
    "href": "surveydata.html#cchs-bivariate-analysis",
    "title": "Survey data analysis",
    "section": "CCHS: Bivariate analysis",
    "text": "CCHS: Bivariate analysis\nThis tutorial outlines how to examine relationships between two variables using R. The tutorial covers data preparation steps such as accumulating survey weights across cycles. It also highlights the handling of missing data and survey design specifications for weighted analyses. Descriptive weighted statistics are generated in tables, stratified by exposure and outcome, to provide insights for survey weighted logistic regression analysis. Additionally, proportions and design effects are calculated to account for the complex survey design’s impact on statistical estimates. The tutorial employs specialized chi-square tests, such as, Rao-Scott and Thomas-Rao modifications, to assess associations between variables, accounting for the survey’s complex design."
  },
  {
    "objectID": "surveydata.html#cchs-regression-analysis",
    "href": "surveydata.html#cchs-regression-analysis",
    "title": "Survey data analysis",
    "section": "CCHS: Regression analysis",
    "text": "CCHS: Regression analysis\nThis tutorial offers a comprehensive guide on conducting complex regression analyses on survey data using R. The tutorial starts by conducting basic data checks. It then performs both simple and multivariable logistic regression to explore the relationship between cardiovascular disease and osteoarthritis. Model fit is assessed using Akaike Information Criterion (AIC) and pseudo R-squared metrics. Variable selection techniques such as backward elimination and stepwise regression guided by AIC are applied to hone the model. The tutorial also delves into assessing interaction effects among variables like age, sex, and diabetes, incorporating significant interactions into the final model."
  },
  {
    "objectID": "surveydata.html#cchs-model-performance",
    "href": "surveydata.html#cchs-model-performance",
    "title": "Survey data analysis",
    "section": "CCHS: Model performance",
    "text": "CCHS: Model performance\nThe tutorial guides users through the process of evaluating logistic regression models fitted to complex survey data in R, focusing primarily on the Receiver Operating Characteristic (ROC) curves and Archer and Lemeshow Goodness of Fit tests. It introduces a specialized function for plotting ROC curves and calculating the Area Under the Curve (AUC) to gauge the model’s predictive accuracy, while taking survey weights into account. Grading guidelines for AUC values are provided for model discrimination quality. For model fit, the Archer and Lemeshow test is used. The tutorial also covers additional functionalities for dealing with strata and clusters in the survey data."
  },
  {
    "objectID": "surveydata.html#nhanes-predicting-blood-pressure",
    "href": "surveydata.html#nhanes-predicting-blood-pressure",
    "title": "Survey data analysis",
    "section": "NHANES: Predicting blood pressure",
    "text": "NHANES: Predicting blood pressure\nThe tutorial provides a comprehensive guide for analyzing health survey data with a focus on how demographic factors like race, age, gender, and marital status relate to blood pressure levels using NHANES dataset. The tutorial constructs both bivariate and multivariate regression models. Additionally, the tutorial incorporates complex survey designs by creating a new survey design object that factors in sampling weight, strata, and clusters. It also generates box plots and summary statistics to visualize variations in blood pressure across different demographic groups, considering survey design. The tutorial emphasizes the importance of accounting for survey design features to avoid biased estimates and discusses the challenges of model overfitting and optimism when shifting from inference to prediction, recommending optimism-correction techniques."
  },
  {
    "objectID": "surveydata.html#nhanes-predicting-cholesterol-level",
    "href": "surveydata.html#nhanes-predicting-cholesterol-level",
    "title": "Survey data analysis",
    "section": "NHANES: Predicting cholesterol level",
    "text": "NHANES: Predicting cholesterol level\nIn the study using NHANES data, the goal was to predict cholesterol levels in adults based on various predictors such as gender, country of birth, race, education, marital status, income, BMI, and diabetes. The data was filtered to include only adults 18 years and older, and multiple statistical tests were conducted. Linear regression and logistic regression models were fitted, with results suggesting an association between gender and cholesterol level. Various statistical tests, including Wald tests and backward elimination, were employed to optimize the model. The study found that income was not a significant predictor for cholesterol levels, and interaction terms did not improve the model. Despite utilizing survey design features, the model had poor discriminatory power. However, Archer-Lemeshow Goodness of Fit test showed that the model fits the data well. The inclusion of age as an additional predictor led to different odds ratios, and the AIC value suggested that adding age improved the model."
  },
  {
    "objectID": "surveydata.html#nhanes-properly-subsetting-a-design-object",
    "href": "surveydata.html#nhanes-properly-subsetting-a-design-object",
    "title": "Survey data analysis",
    "section": "NHANES: Properly subsetting a design object",
    "text": "NHANES: Properly subsetting a design object\nThe tutorial provides a comprehensive guide on how to handle and analyze a subset of complex survey data from the NHANES study using R. It begins by checking for missing data. The focus is on subsetting data based on complete information, emphasizing the importance of accounting for the full complex survey design to obtain unbiased variance estimates. Logistic regression is then run on this subset, with the tutorial explicitly differentiating between correct and incorrect approaches to consider the survey design. Finally, variable selection methods like backward elimination are discussed to determine significant predictors, emphasizing the retention of variables deemed important based on prior research.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial."
  },
  {
    "objectID": "surveydata1.html#references",
    "href": "surveydata1.html#references",
    "title": "CCHS: Revisiting PICOT",
    "section": "References",
    "text": "References\n\n\n\n\nCanada, Statistics. 2005. “Canadian Community Health Survey (CCHS), Cycle 3.1.” Author Ottawa.\n\n\nKarim, Ehsan. 2023. “Case Study 2: Risk of Cardiovascular Disease Among Osteoarthritis Patients.” https://ssc.ca/en/case-study/case-study-2-risk-cardiovascular-disease-among-osteoarthritis-patients.\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624."
  },
  {
    "objectID": "surveydata2.html",
    "href": "surveydata2.html",
    "title": "CCHS: Assessing data",
    "section": "",
    "text": "Let us load all the necessary packages for data manipulation, statistical analysis, and plotting.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\n\n\nLoad data\nData loading that we saved earlier:\n\nShow the codeload(\"Data/surveydata/cchs123.RData\")\nls()\n#> [1] \"analytic\" \"cc123a\"\n\n\nChecking\nCheck the data for missingness\nChecks the dimensions of the data and runs functions to explore missing data, stratifying by some variables. Additionally, it plots the missing data for visualization.\n\nShow the codedim(analytic)\n#> [1] 397173     24\nrequire(\"tableone\")\n#CreateTableOne(data = analytic, includeNA = TRUE)\nCreateTableOne(data = analytic, strata = \"CVD\", includeNA = TRUE)\n#>                       Stratified by CVD\n#>                        event                 no event              p      test\n#>   n                        25524                371121                        \n#>   CVD (%)                                                             NaN     \n#>      event                 25524 (100.0)             0 (  0.0)                \n#>      no event                  0 (  0.0)        371121 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years             330 (  1.3)         48293 ( 13.0)                \n#>      30-39 years             580 (  2.3)         63194 ( 17.0)                \n#>      40-49 years            1498 (  5.9)         63549 ( 17.1)                \n#>      50-59 years            3635 ( 14.2)         57300 ( 15.4)                \n#>      60-64 years            2720 ( 10.7)         22497 (  6.1)                \n#>      65 years and over     16496 ( 64.6)         64198 ( 17.3)                \n#>      teen                    265 (  1.0)         52090 ( 14.0)                \n#>   sex = Male (%)           12506 ( 49.0)        169776 ( 45.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single            13287 ( 52.1)        188687 ( 50.8)                \n#>      single                12207 ( 47.8)        181811 ( 49.0)                \n#>      NA                       30 (  0.1)           623 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white              1276 (  5.0)         37323 ( 10.1)                \n#>      White                 23629 ( 92.6)        325178 ( 87.6)                \n#>      NA                      619 (  2.4)          8620 (  2.3)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              11547 ( 45.2)        112678 ( 30.4)                \n#>      2nd grad.              3310 ( 13.0)         61355 ( 16.5)                \n#>      Other 2nd grad.        1323 (  5.2)         27643 (  7.4)                \n#>      Post-2nd grad.         8744 ( 34.3)        163052 ( 43.9)                \n#>      NA                      600 (  2.4)          6393 (  1.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       11664 ( 45.7)         89506 ( 24.1)                \n#>      $30,000-$49,999        4871 ( 19.1)         72994 ( 19.7)                \n#>      $50,000-$79,999        3193 ( 12.5)         81861 ( 22.1)                \n#>      $80,000 or more        1905 (  7.5)         73768 ( 19.9)                \n#>      NA                     3891 ( 15.2)         52992 ( 14.3)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight             504 (  2.0)          9600 (  2.6)                \n#>      healthy weight         7176 ( 28.1)        141200 ( 38.0)                \n#>      Overweight            12104 ( 47.4)        153887 ( 41.5)                \n#>      NA                     5740 ( 22.5)         66434 ( 17.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                 3642 ( 14.3)         94844 ( 25.6)                \n#>      Inactive              15494 ( 60.7)        174976 ( 47.1)                \n#>      Moderate               4928 ( 19.3)         88480 ( 23.8)                \n#>      NA                     1460 (  5.7)         12821 (  3.5)                \n#>   doctor (%)                                                       <0.001     \n#>      No                     1134 (  4.4)         57425 ( 15.5)                \n#>      Yes                   24384 ( 95.5)        313282 ( 84.4)                \n#>      NA                        6 (  0.0)           414 (  0.1)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed      20041 ( 78.5)        266358 ( 71.8)                \n#>      stressed               5184 ( 20.3)         76986 ( 20.7)                \n#>      NA                      299 (  1.2)         27777 (  7.5)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker         4481 ( 17.6)         93253 ( 25.1)                \n#>      Former smoker         13927 ( 54.6)        143421 ( 38.6)                \n#>      Never smoker           6981 ( 27.4)        132891 ( 35.8)                \n#>      NA                      135 (  0.5)          1556 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker       15852 ( 62.1)        279583 ( 75.3)                \n#>      Former driker          6820 ( 26.7)         48373 ( 13.0)                \n#>      Never drank            2421 (  9.5)         38195 ( 10.3)                \n#>      NA                      431 (  1.7)          4970 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving      4284 ( 16.8)         79088 ( 21.3)                \n#>      4-6 daily serving     10527 ( 41.2)        148684 ( 40.1)                \n#>      6+ daily serving       5047 ( 19.8)         73729 ( 19.9)                \n#>      NA                     5666 ( 22.2)         69620 ( 18.8)                \n#>   bp (%)                                                           <0.001     \n#>      No                    12611 ( 49.4)        315344 ( 85.0)                \n#>      Yes                   12857 ( 50.4)         55037 ( 14.8)                \n#>      NA                       56 (  0.2)           740 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                    23378 ( 91.6)        267481 ( 72.1)                \n#>      Yes                    1449 (  5.7)          3043 (  0.8)                \n#>      NA                      697 (  2.7)        100597 ( 27.1)                \n#>   diab (%)                                                         <0.001     \n#>      No                    20461 ( 80.2)        353817 ( 95.3)                \n#>      Yes                    5038 ( 19.7)         17138 (  4.6)                \n#>      NA                       25 (  0.1)           166 (  0.0)                \n#>   province = South (%)     25271 ( 99.0)        363659 ( 98.0)     <0.001     \n#>   weight (mean (SD))      152.58 (181.69)       203.40 (244.28)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                     7968 ( 31.2)        122798 ( 33.1)                \n#>      21                     9027 ( 35.4)        124838 ( 33.6)                \n#>      31                     8529 ( 33.4)        123485 ( 33.3)                \n#>   ID (mean (SD))       199839.07 (114705.35) 198466.74 (114661.51)  0.064     \n#>   OA (%)                                                           <0.001     \n#>      Control               12655 ( 49.6)        301675 ( 81.3)                \n#>      OA                     6522 ( 25.6)         34346 (  9.3)                \n#>      NA                     6347 ( 24.9)         35100 (  9.5)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years             2295 (  9.0)         24409 (  6.6)                \n#>      not immigrant         21342 ( 83.6)        316353 ( 85.2)                \n#>      recent                  159 (  0.6)         10476 (  2.8)                \n#>      NA                     1728 (  6.8)         19883 (  5.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND            519 (  2.0)          7398 (  2.0)                \n#>      PEI                     567 (  2.2)          7172 (  1.9)                \n#>      NOVA SCOTIA            1308 (  5.1)         14015 (  3.8)                \n#>      NEW BRUNSWICK          1223 (  4.8)         13786 (  3.7)                \n#>      QU\\xc9BEC              1380 (  5.4)         20625 (  5.6)                \n#>      ONTARIO                8596 ( 33.7)        115053 ( 31.0)                \n#>      MANITOBA               1339 (  5.2)         22074 (  5.9)                \n#>      SASKATCHEWAN           1542 (  6.0)         21782 (  5.9)                \n#>      ALBERTA                1837 (  7.2)         38238 ( 10.3)                \n#>      BRITISH COLUMBIA       2847 ( 11.2)         46834 ( 12.6)                \n#>      YUKON/NWT/NUNAVT        173 (  0.7)          4884 (  1.3)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                 3839 ( 15.0)         52850 ( 14.2)                \n#>      NFLD & LAB.             274 (  1.1)          3832 (  1.0)                \n#>      YUKON/NWT/NUNA.          80 (  0.3)          2578 (  0.7)\nCreateTableOne(data = analytic, strata = \"OA\", includeNA = TRUE)\n#>                       Stratified by OA\n#>                        Control               OA                    p      test\n#>   n                       314542                 40943                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 12655 (  4.0)          6522 ( 15.9)                \n#>      no event             301675 ( 95.9)         34346 ( 83.9)                \n#>      NA                      212 (  0.1)            75 (  0.2)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           46805 ( 14.9)           537 (  1.3)                \n#>      30-39 years           59233 ( 18.8)          1622 (  4.0)                \n#>      40-49 years           55598 ( 17.7)          4128 ( 10.1)                \n#>      50-59 years           43746 ( 13.9)          8994 ( 22.0)                \n#>      60-64 years           15772 (  5.0)          5100 ( 12.5)                \n#>      65 years and over     41661 ( 13.2)         20436 ( 49.9)                \n#>      teen                  51727 ( 16.4)           126 (  0.3)                \n#>   sex = Male (%)          153889 ( 48.9)         11627 ( 28.4)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           158065 ( 50.3)         21794 ( 53.2)                \n#>      single               155952 ( 49.6)         19099 ( 46.6)                \n#>      NA                      525 (  0.2)            50 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             34028 ( 10.8)          1803 (  4.4)                \n#>      White                273378 ( 86.9)         38241 ( 93.4)                \n#>      NA                     7136 (  2.3)           899 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              92831 ( 29.5)         14539 ( 35.5)                \n#>      2nd grad.             52077 ( 16.6)          6291 ( 15.4)                \n#>      Other 2nd grad.       24099 (  7.7)          2484 (  6.1)                \n#>      Post-2nd grad.       140400 ( 44.6)         16887 ( 41.2)                \n#>      NA                     5135 (  1.6)           742 (  1.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       68530 ( 21.8)         16233 ( 39.6)                \n#>      $30,000-$49,999       61697 ( 19.6)          8360 ( 20.4)                \n#>      $50,000-$79,999       72657 ( 23.1)          6348 ( 15.5)                \n#>      $80,000 or more       67458 ( 21.4)          4191 ( 10.2)                \n#>      NA                    44200 ( 14.1)          5811 ( 14.2)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8660 (  2.8)           715 (  1.7)                \n#>      healthy weight       123416 ( 39.2)         12631 ( 30.9)                \n#>      Overweight           123898 ( 39.4)         20715 ( 50.6)                \n#>      NA                    58568 ( 18.6)          6882 ( 16.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                84269 ( 26.8)          6968 ( 17.0)                \n#>      Inactive             143058 ( 45.5)         23604 ( 57.7)                \n#>      Moderate              75703 ( 24.1)          9176 ( 22.4)                \n#>      NA                    11512 (  3.7)          1195 (  2.9)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53335 ( 17.0)          2221 (  5.4)                \n#>      Yes                  260802 ( 82.9)         38717 ( 94.6)                \n#>      NA                      405 (  0.1)             5 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     223212 ( 71.0)         31769 ( 77.6)                \n#>      stressed              63923 ( 20.3)          8998 ( 22.0)                \n#>      NA                    27407 (  8.7)           176 (  0.4)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        79521 ( 25.3)          8087 ( 19.8)                \n#>      Former smoker        117745 ( 37.4)         20267 ( 49.5)                \n#>      Never smoker         116006 ( 36.9)         12428 ( 30.4)                \n#>      NA                     1270 (  0.4)           161 (  0.4)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      239223 ( 76.1)         28622 ( 69.9)                \n#>      Former driker         37042 ( 11.8)          8668 ( 21.2)                \n#>      Never drank           34185 ( 10.9)          3128 (  7.6)                \n#>      NA                     4092 (  1.3)           525 (  1.3)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     68629 ( 21.8)          6571 ( 16.0)                \n#>      4-6 daily serving    125177 ( 39.8)         17214 ( 42.0)                \n#>      6+ daily serving      62121 ( 19.7)          9123 ( 22.3)                \n#>      NA                    58615 ( 18.6)          8035 ( 19.6)                \n#>   bp (%)                                                           <0.001     \n#>      No                   275443 ( 87.6)         25551 ( 62.4)                \n#>      Yes                   38442 ( 12.2)         15341 ( 37.5)                \n#>      NA                      657 (  0.2)            51 (  0.1)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213719 ( 67.9)         39007 ( 95.3)                \n#>      Yes                    2131 (  0.7)          1214 (  3.0)                \n#>      NA                    98692 ( 31.4)           722 (  1.8)                \n#>   diab (%)                                                         <0.001     \n#>      No                   301943 ( 96.0)         36211 ( 88.4)                \n#>      Yes                   12442 (  4.0)          4705 ( 11.5)                \n#>      NA                      157 (  0.0)            27 (  0.1)                \n#>   province = South (%)    307761 ( 97.8)         40507 ( 98.9)     <0.001     \n#>   weight (mean (SD))      211.50 (251.46)       159.00 (188.84)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106231 ( 33.8)         12052 ( 29.4)                \n#>      21                   104530 ( 33.2)         14750 ( 36.0)                \n#>      31                   103781 ( 33.0)         14141 ( 34.5)                \n#>   ID (mean (SD))       197003.20 (115147.95) 204459.43 (113014.25) <0.001     \n#>   OA (%)                                                              NaN     \n#>      Control              314542 (100.0)             0 (  0.0)                \n#>      OA                        0 (  0.0)         40943 (100.0)                \n#>      NA                        0 (  0.0)             0 (  0.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            19385 (  6.2)          3622 (  8.8)                \n#>      not immigrant        268962 ( 85.5)         34509 ( 84.3)                \n#>      recent                10187 (  3.2)           151 (  0.4)                \n#>      NA                    16008 (  5.1)          2661 (  6.5)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6315 (  2.0)           725 (  1.8)                \n#>      PEI                    5892 (  1.9)           817 (  2.0)                \n#>      NOVA SCOTIA           11081 (  3.5)          1880 (  4.6)                \n#>      NEW BRUNSWICK         11517 (  3.7)          1693 (  4.1)                \n#>      QU\\xc9BEC             19111 (  6.1)          2035 (  5.0)                \n#>      ONTARIO               95651 ( 30.4)         13669 ( 33.4)                \n#>      MANITOBA              18050 (  5.7)          2272 (  5.5)                \n#>      SASKATCHEWAN          17941 (  5.7)          2166 (  5.3)                \n#>      ALBERTA               32207 ( 10.2)          3608 (  8.8)                \n#>      BRITISH COLUMBIA      40034 ( 12.7)          4873 ( 11.9)                \n#>      YUKON/NWT/NUNAVT       4446 (  1.4)           273 (  0.7)                \n#>      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#>      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#>      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#>      NOT STATED                0 (  0.0)             0 (  0.0)                \n#>      QUEBEC                46817 ( 14.9)          6366 ( 15.5)                \n#>      NFLD & LAB.            3145 (  1.0)           403 (  1.0)                \n#>      YUKON/NWT/NUNA.        2335 (  0.7)           163 (  0.4)\n\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\nLook for zero-cells\nCreates two new variables based on age groups and generates summary tables. It also comments on the presence of ‘zero cells’ in one of the variables, which might require further handling.\n\nShow the codeanalytic$age.65p <- analytic$age.teen <- 0\nanalytic$age.teen[analytic$age == \"teen\"] <- 1\nanalytic$age.65p[analytic$age == \"65 years and over\"] <- 1\nCreateTableOne(data = analytic, strata = \"age.teen\", includeNA = TRUE)\n#>                       Stratified by age.teen\n#>                        0                     1                     p      test\n#>   n                       344786                 52387                        \n#>   CVD (%)                                                          <0.001     \n#>      event                 25259 ( 7.3)            265 (  0.5)                \n#>      no event             319031 (92.5)          52090 ( 99.4)                \n#>      NA                      496 ( 0.1)             32 (  0.1)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (14.1)              0 (  0.0)                \n#>      30-39 years           63810 (18.5)              0 (  0.0)                \n#>      40-49 years           65111 (18.9)              0 (  0.0)                \n#>      50-59 years           61035 (17.7)              0 (  0.0)                \n#>      60-64 years           25265 ( 7.3)              0 (  0.0)                \n#>      65 years and over     80913 (23.5)              0 (  0.0)                \n#>      teen                      0 ( 0.0)          52387 (100.0)                \n#>   sex = Male (%)          155980 (45.2)          26543 ( 50.7)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           201528 (58.5)            685 (  1.3)                \n#>      single               142639 (41.4)          51660 ( 98.6)                \n#>      NA                      619 ( 0.2)             42 (  0.1)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             31107 ( 9.0)           7534 ( 14.4)                \n#>      White                305497 (88.6)          43725 ( 83.5)                \n#>      NA                     8182 ( 2.4)           1128 (  2.2)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              83649 (24.3)          40776 ( 77.8)                \n#>      2nd grad.             59205 (17.2)           5548 ( 10.6)                \n#>      Other 2nd grad.       24580 ( 7.1)           4420 (  8.4)                \n#>      Post-2nd grad.       170707 (49.5)           1265 (  2.4)                \n#>      NA                     6645 ( 1.9)            378 (  0.7)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       93630 (27.2)           7701 ( 14.7)                \n#>      $30,000-$49,999       69798 (20.2)           8142 ( 15.5)                \n#>      $50,000-$79,999       73596 (21.3)          11512 ( 22.0)                \n#>      $80,000 or more       63697 (18.5)          12018 ( 22.9)                \n#>      NA                    44065 (12.8)          13014 ( 24.8)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            7277 ( 2.1)           2839 (  5.4)                \n#>      healthy weight       138611 (40.2)           9922 ( 18.9)                \n#>      Overweight           163701 (47.5)           2520 (  4.8)                \n#>      NA                    35197 (10.2)          37106 ( 70.8)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                74738 (21.7)          23833 ( 45.5)                \n#>      Inactive             176573 (51.2)          14166 ( 27.0)                \n#>      Moderate              82158 (23.8)          11349 ( 21.7)                \n#>      NA                    11317 ( 3.3)           3039 (  5.8)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    49874 (14.5)           8749 ( 16.7)                \n#>      Yes                  294763 (85.5)          43342 ( 82.7)                \n#>      NA                      149 ( 0.0)            296 (  0.6)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     265391 (77.0)          21353 ( 40.8)                \n#>      stressed              78044 (22.6)           4253 (  8.1)                \n#>      NA                     1351 ( 0.4)          26781 ( 51.1)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88986 (25.8)           8866 ( 16.9)                \n#>      Former smoker        150004 (43.5)           7566 ( 14.4)                \n#>      Never smoker         104332 (30.3)          35685 ( 68.1)                \n#>      NA                     1464 ( 0.4)            270 (  0.5)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      268269 (77.8)          27464 ( 52.4)                \n#>      Former driker         50929 (14.8)           4370 (  8.3)                \n#>      Never drank           20754 ( 6.0)          19916 ( 38.0)                \n#>      NA                     4834 ( 1.4)            637 (  1.2)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72392 (21.0)          11049 ( 21.1)                \n#>      4-6 daily serving    139753 (40.5)          19627 ( 37.5)                \n#>      6+ daily serving      66831 (19.4)          12026 ( 23.0)                \n#>      NA                    65810 (19.1)           9685 ( 18.5)                \n#>   bp (%)                                                           <0.001     \n#>      No                   276318 (80.1)          51846 ( 99.0)                \n#>      Yes                   67763 (19.7)            308 (  0.6)                \n#>      NA                      705 ( 0.2)            233 (  0.4)                \n#>   copd (%)                                                         <0.001     \n#>      No                   291191 (84.5)              0 (  0.0)                \n#>      Yes                    4508 ( 1.3)              0 (  0.0)                \n#>      NA                    49087 (14.2)          52387 (100.0)                \n#>   diab (%)                                                         <0.001     \n#>      No                   322448 (93.5)          52141 ( 99.5)                \n#>      Yes                   22032 ( 6.4)            199 (  0.4)                \n#>      NA                      306 ( 0.1)             47 (  0.1)                \n#>   province = South (%)    338450 (98.2)          51001 ( 97.4)     <0.001     \n#>   weight (mean (SD))      201.76 (245.97)       189.09 (205.24)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   113323 (32.9)          17557 ( 33.5)                \n#>      21                   115548 (33.5)          18524 ( 35.4)                \n#>      31                   115915 (33.6)          16306 ( 31.1)                \n#>   ID (mean (SD))       199143.77 (114810.36) 194922.59 (113553.38) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              262815 (76.2)          51727 ( 98.7)                \n#>      OA                    40817 (11.8)            126 (  0.2)                \n#>      NA                    41154 (11.9)            534 (  1.0)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            25976 ( 7.5)            770 (  1.5)                \n#>      not immigrant        289651 (84.0)          48427 ( 92.4)                \n#>      recent                 8710 ( 2.5)           1934 (  3.7)                \n#>      NA                    20449 ( 5.9)           1256 (  2.4)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6646 ( 1.9)           1278 (  2.4)                \n#>      PEI                    6802 ( 2.0)            942 (  1.8)                \n#>      NOVA SCOTIA           13337 ( 3.9)           2004 (  3.8)                \n#>      NEW BRUNSWICK         13057 ( 3.8)           1968 (  3.8)                \n#>      QU\\xc9BEC             19186 ( 5.6)           2826 (  5.4)                \n#>      ONTARIO              107768 (31.3)          16053 ( 30.6)                \n#>      MANITOBA              20362 ( 5.9)           3092 (  5.9)                \n#>      SASKATCHEWAN          20160 ( 5.8)           3201 (  6.1)                \n#>      ALBERTA               34293 ( 9.9)           5834 ( 11.1)                \n#>      BRITISH COLUMBIA      43431 (12.6)           6336 ( 12.1)                \n#>      YUKON/NWT/NUNAVT       4170 ( 1.2)            894 (  1.7)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                49806 (14.4)           6958 ( 13.3)                \n#>      NFLD & LAB.            3602 ( 1.0)            509 (  1.0)                \n#>      YUKON/NWT/NUNA.        2166 ( 0.6)            492 (  0.9)                \n#>   age.teen (mean (SD))      0.00 (0.00)           1.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.23 (0.42)           0.00 (0.00)      <0.001\n# copd has zero cells\n# analytic$age[analytic$age == 'teen'] <- NA (will set this if we use copd)\n\n\n\nShow the codeCreateTableOne(data = analytic, strata = \"age.65p\", includeNA = TRUE)\n#>                       Stratified by age.65p\n#>                        0                     1                     p      test\n#>   n                       316260                 80913                        \n#>   CVD (%)                                                          <0.001     \n#>      event                  9028 ( 2.9)          16496 ( 20.4)                \n#>      no event             306923 (97.0)          64198 ( 79.3)                \n#>      NA                      309 ( 0.1)            219 (  0.3)                \n#>   age (%)                                                          <0.001     \n#>      20-29 years           48652 (15.4)              0 (  0.0)                \n#>      30-39 years           63810 (20.2)              0 (  0.0)                \n#>      40-49 years           65111 (20.6)              0 (  0.0)                \n#>      50-59 years           61035 (19.3)              0 (  0.0)                \n#>      60-64 years           25265 ( 8.0)              0 (  0.0)                \n#>      65 years and over         0 ( 0.0)          80913 (100.0)                \n#>      teen                  52387 (16.6)              0 (  0.0)                \n#>   sex = Male (%)          150152 (47.5)          32371 ( 40.0)     <0.001     \n#>   married (%)                                                      <0.001     \n#>      not single           163660 (51.7)          38553 ( 47.6)                \n#>      single               152077 (48.1)          42222 ( 52.2)                \n#>      NA                      523 ( 0.2)            138 (  0.2)                \n#>   race (%)                                                         <0.001     \n#>      Non-white             35329 (11.2)           3312 (  4.1)                \n#>      White                274000 (86.6)          75222 ( 93.0)                \n#>      NA                     6931 ( 2.2)           2379 (  2.9)                \n#>   edu (%)                                                          <0.001     \n#>      < 2ndary              84832 (26.8)          39593 ( 48.9)                \n#>      2nd grad.             53974 (17.1)          10779 ( 13.3)                \n#>      Other 2nd grad.       25305 ( 8.0)           3695 (  4.6)                \n#>      Post-2nd grad.       147385 (46.6)          24587 ( 30.4)                \n#>      NA                     4764 ( 1.5)           2259 (  2.8)                \n#>   income (%)                                                       <0.001     \n#>      $29,999 or less       62513 (19.8)          38818 ( 48.0)                \n#>      $30,000-$49,999       62296 (19.7)          15644 ( 19.3)                \n#>      $50,000-$79,999       77283 (24.4)           7825 (  9.7)                \n#>      $80,000 or more       72566 (22.9)           3149 (  3.9)                \n#>      NA                    41602 (13.2)          15477 ( 19.1)                \n#>   bmi (%)                                                          <0.001     \n#>      Underweight            8588 ( 2.7)           1528 (  1.9)                \n#>      healthy weight       124932 (39.5)          23601 ( 29.2)                \n#>      Overweight           136225 (43.1)          29996 ( 37.1)                \n#>      NA                    46515 (14.7)          25788 ( 31.9)                \n#>   phyact (%)                                                       <0.001     \n#>      Active                85384 (27.0)          13187 ( 16.3)                \n#>      Inactive             144019 (45.5)          46720 ( 57.7)                \n#>      Moderate              76602 (24.2)          16905 ( 20.9)                \n#>      NA                    10255 ( 3.2)           4101 (  5.1)                \n#>   doctor (%)                                                       <0.001     \n#>      No                    53972 (17.1)           4651 (  5.7)                \n#>      Yes                  261866 (82.8)          76239 ( 94.2)                \n#>      NA                      422 ( 0.1)             23 (  0.0)                \n#>   stress (%)                                                       <0.001     \n#>      Not too stressed     215454 (68.1)          71290 ( 88.1)                \n#>      stressed              73402 (23.2)           8895 ( 11.0)                \n#>      NA                    27404 ( 8.7)            728 (  0.9)                \n#>   smoke (%)                                                        <0.001     \n#>      Current smoker        88068 (27.8)           9784 ( 12.1)                \n#>      Former smoker        115111 (36.4)          42459 ( 52.5)                \n#>      Never smoker         111879 (35.4)          28138 ( 34.8)                \n#>      NA                     1202 ( 0.4)            532 (  0.7)                \n#>   drink (%)                                                        <0.001     \n#>      Current drinker      245156 (77.5)          50577 ( 62.5)                \n#>      Former driker         35401 (11.2)          19898 ( 24.6)                \n#>      Never drank           31888 (10.1)           8782 ( 10.9)                \n#>      NA                     3815 ( 1.2)           1656 (  2.0)                \n#>   fruit (%)                                                        <0.001     \n#>      0-3 daily serving     72908 (23.1)          10533 ( 13.0)                \n#>      4-6 daily serving    124621 (39.4)          34759 ( 43.0)                \n#>      6+ daily serving      61855 (19.6)          17002 ( 21.0)                \n#>      NA                    56876 (18.0)          18619 ( 23.0)                \n#>   bp (%)                                                           <0.001     \n#>      No                   282174 (89.2)          45990 ( 56.8)                \n#>      Yes                   33346 (10.5)          34725 ( 42.9)                \n#>      NA                      740 ( 0.2)            198 (  0.2)                \n#>   copd (%)                                                         <0.001     \n#>      No                   213221 (67.4)          77970 ( 96.4)                \n#>      Yes                    1791 ( 0.6)           2717 (  3.4)                \n#>      NA                   101248 (32.0)            226 (  0.3)                \n#>   diab (%)                                                         <0.001     \n#>      No                   305027 (96.4)          69562 ( 86.0)                \n#>      Yes                   10974 ( 3.5)          11257 ( 13.9)                \n#>      NA                      259 ( 0.1)             94 (  0.1)                \n#>   province = South (%)    309016 (97.7)          80435 ( 99.4)     <0.001     \n#>   weight (mean (SD))      215.36 (255.33)       140.38 (160.88)    <0.001     \n#>   cycle (%)                                                        <0.001     \n#>      11                   106647 (33.7)          24233 ( 29.9)                \n#>      21                   105506 (33.4)          28566 ( 35.3)                \n#>      31                   104107 (32.9)          28114 ( 34.7)                \n#>   ID (mean (SD))       197072.98 (115035.66) 204504.77 (112956.66) <0.001     \n#>   OA (%)                                                           <0.001     \n#>      Control              272881 (86.3)          41661 ( 51.5)                \n#>      OA                    20507 ( 6.5)          20436 ( 25.3)                \n#>      NA                    22872 ( 7.2)          18816 ( 23.3)                \n#>   immigrate (%)                                                    <0.001     \n#>      > 10 years            17607 ( 5.6)           9139 ( 11.3)                \n#>      not immigrant        273622 (86.5)          64456 ( 79.7)                \n#>      recent                10325 ( 3.3)            319 (  0.4)                \n#>      NA                    14706 ( 4.6)           6999 (  8.7)                \n#>   province.check (%)                                                  NaN     \n#>      NEWFOUNDLAND           6665 ( 2.1)           1259 (  1.6)                \n#>      PEI                    5993 ( 1.9)           1751 (  2.2)                \n#>      NOVA SCOTIA           11896 ( 3.8)           3445 (  4.3)                \n#>      NEW BRUNSWICK         11856 ( 3.7)           3169 (  3.9)                \n#>      QU\\xc9BEC             18128 ( 5.7)           3884 (  4.8)                \n#>      ONTARIO               97660 (30.9)          26161 ( 32.3)                \n#>      MANITOBA              17967 ( 5.7)           5487 (  6.8)                \n#>      SASKATCHEWAN          17507 ( 5.5)           5854 (  7.2)                \n#>      ALBERTA               33445 (10.6)           6682 (  8.3)                \n#>      BRITISH COLUMBIA      39394 (12.5)          10373 ( 12.8)                \n#>      YUKON/NWT/NUNAVT       4765 ( 1.5)            299 (  0.4)                \n#>      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#>      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#>      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#>      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#>      QUEBEC                45226 (14.3)          11538 ( 14.3)                \n#>      NFLD & LAB.            3279 ( 1.0)            832 (  1.0)                \n#>      YUKON/NWT/NUNA.        2479 ( 0.8)            179 (  0.2)                \n#>   age.teen (mean (SD))      0.17 (0.37)           0.00 (0.00)      <0.001     \n#>   age.65p (mean (SD))       0.00 (0.00)           1.00 (0.00)      <0.001\nanalytic$age.65p <- analytic$age.teen <- NULL\n\n\nProduces frequency tables for multiple variable combinations to check the distribution of the data and identify issues.\n\nShow the codetable(analytic$province.check,analytic$fruit)\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  2991              3401             1237\n#>   PEI                           2280              3654             1506\n#>   NOVA SCOTIA                   3089              4804             1974\n#>   NEW BRUNSWICK                 2989              4730             1880\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      28752             59466            30746\n#>   MANITOBA                      4561              7669             3095\n#>   SASKATCHEWAN                  4173              7390             3003\n#>   ALBERTA                      10828             18901             8251\n#>   BRITISH COLUMBIA             10726             24422            12390\n#>   YUKON/NWT/NUNAVT              1829              2045             1023\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\ntable(analytic$age)\n#> \n#>       20-29 years       30-39 years       40-49 years       50-59 years \n#>             48652             63810             65111             61035 \n#>       60-64 years 65 years and over              teen \n#>             25265             80913             52387\ntable(analytic$copd, analytic$age)\n#>      \n#>       20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   No            0       63645       64735       60203       24638\n#>   Yes           0         117         320         768         586\n#>      \n#>       65 years and over  teen\n#>   No              77970     0\n#>   Yes              2717     0\ntable(analytic$stress, analytic$age) \n#>                   \n#>                    20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#>   Not too stressed       37117       45494       45316       45226       20948\n#>   stressed               11472       18197       19639       15623        4218\n#>                   \n#>                    65 years and over  teen\n#>   Not too stressed             71290 21353\n#>   stressed                      8895  4253\n\n\n\nuniverse 15 + is not an issue for stress as age starts from 20\n\ncopd is problematic!\n\nCreates tables to look at the distribution of a specific variable across different cycles (time periods) of the survey. Notes differences and issues.\n\n\nfruit variable measured in an optional component (not available in all cycles)\n\n\nShow the codetable(analytic$province.check[analytic$cycle==11],\n      analytic$fruit[analytic$cycle==11])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1512              1643              689\n#>   PEI                           1084              1738              773\n#>   NOVA SCOTIA                   1732              2500             1036\n#>   NEW BRUNSWICK                 1663              2363              934\n#>   QU\\xc9BEC                     5568             10502             5786\n#>   ONTARIO                      10437             19478             8809\n#>   MANITOBA                      2604              4214             1526\n#>   SASKATCHEWAN                  2386              3957             1387\n#>   ALBERTA                       4391              7050             2664\n#>   BRITISH COLUMBIA              4321              9350             4278\n#>   YUKON/NWT/NUNAVT               999              1014              448\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n\n\n\nShow the codetable(analytic$province.check[analytic$cycle==21],\n      analytic$fruit[analytic$cycle==21])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                  1479              1758              548\n#>   PEI                            615               947              344\n#>   NOVA SCOTIA                   1357              2304              938\n#>   NEW BRUNSWICK                 1326              2367              946\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       9365             20356            10933\n#>   MANITOBA                      1957              3455             1569\n#>   SASKATCHEWAN                  1787              3433             1616\n#>   ALBERTA                       3326              6376             3046\n#>   BRITISH COLUMBIA              3186              7727             4224\n#>   YUKON/NWT/NUNAVT               830              1031              575\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                        5655             12396             7966\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# a different QUEBEC spelling used\n\n\n\nShow the codetable(analytic$province.check[analytic$cycle==31],\n      analytic$fruit[analytic$cycle==31])\n#>                   \n#>                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#>   NEWFOUNDLAND                     0                 0                0\n#>   PEI                            581               969              389\n#>   NOVA SCOTIA                      0                 0                0\n#>   NEW BRUNSWICK                    0                 0                0\n#>   QU\\xc9BEC                        0                 0                0\n#>   ONTARIO                       8950             19632            11004\n#>   MANITOBA                         0                 0                0\n#>   SASKATCHEWAN                     0                 0                0\n#>   ALBERTA                       3111              5475             2541\n#>   BRITISH COLUMBIA              3219              7345             3888\n#>   YUKON/NWT/NUNAVT                 0                 0                0\n#>   NOT APPLICABLE                   0                 0                0\n#>   DON'T KNOW                       0                 0                0\n#>   REFUSAL                          0                 0                0\n#>   NOT STATED                       0                 0                0\n#>   QUEBEC                           0                 0                0\n#>   NFLD & LAB.                      0                 0                0\n#>   YUKON/NWT/NUNA.                  0                 0                0\n# The real problem!\n\n\n\nLook at data dictionaries in all cycles\n\ncycle 1.1 FVCADTOT Universe: All respondents\ncycle 2.1 FVCCDTOT Universe: All respondents\ncycle 3.1 FVCEDTOT Universe: Respondents with FVCEFOPT = 1\n\n\n\nBelow we delete or modify problematic data, and removes unnecessary variables. Checks the dimensions before and after data cleanup.\n\nShow the codedim(analytic)\n#> [1] 397173     24\nanalytic1 <- analytic\n# analytic1$South[analytic1$province.check == \"NFLD & LAB.\"] <- NA\n# analytic1$South[analytic1$province.check == \"YUKON/NWT/NUNA.\"] <- NA\n# analytic1 <- subset(analytic, province.check != \"NFLD & LAB.\" & \n#                       province.check != \"YUKON/NWT/NUNA.\" )\ndim(analytic1)\n#> [1] 397173     24\n\nanalytic1$copd <- NULL # will bring this later for missing data analysis\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n# analytic1 <- droplevels.data.frame(analytic1)\nanalytic1$province.check <- NULL # we already have simplified province variable\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n\n\nSet appropriate reference\nSave the original data (with missing values)!\n\nShow the codeanalytic.miss <- analytic1\n\n\nRelevels factors in the dataset so that a specific level is set as the reference level. This is often needed for statistical analysis.\n\nShow the codeanalytic.miss$smoke <- relevel(as.factor(analytic.miss$smoke), ref='Never smoker')\nanalytic.miss$drink <- relevel(as.factor(analytic.miss$drink), ref='Never drank')\nanalytic.miss$province <- relevel(as.factor(analytic.miss$province), ref='South')\nanalytic.miss$immigrate <- relevel(as.factor(analytic.miss$immigrate), ref='not immigrant')\n\n\nComplete data options\nCreates a new dataset that omits all rows containing any missing values. This is generally not recommended for most data analysis, as it can introduce bias.\n\nShow the code# Wrong thing to do for survey data analysis!!\nanalytic2 <- as.data.frame(na.omit(analytic1)) \ndim(analytic2)\n#> [1] 185613     22\n# tab1 <- CreateTableOne(data = analytic2, strata = \"OA\", includeNA = TRUE)\n# print(tab1, test=FALSE, showAllLevels = TRUE)\n\n\nSaving dataset\nLet us check the dimensions of multiple data objects and then save them to a file for future use.\n\nShow the codedim(cc123a)\n#> [1] 397173     25\ndim(analytic)\n#> [1] 397173     24\ndim(analytic.miss)\n#> [1] 397173     22\ndim(analytic2)\n#> [1] 185613     22\nsave(analytic.miss, analytic2, file = \"Data/surveydata/cchs123b.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata3.html",
    "href": "surveydata3.html",
    "title": "CCHS: Bivariate analysis",
    "section": "",
    "text": "The following tutorial is performing bivariate analysis on our CCHS analytic dataset to examine relationships between two variables (association question).\nWe load several R packages required for bivariate analysis, statistical tests, and data visualization.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\n\n\nLoad data\nWe load the dataset into the R environment and lists all available variables and objects.\n\nShow the codeload(\"Data/surveydata/cchs123b.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"\n\n\nPreparing data\nWeights\nHere, the weights of survey respondents are accumulated, to account for the combination of different cycles of the data.\n\nShow the codeanalytic.miss$weight <- analytic.miss$weight/3 # 3 cycles combined\n\n\nFixing variable types\nWe convert several variables to categorical or “factor” types, which are better suited for some statistical analysis when variables have categories.\n\nShow the codevar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"OA\", \"immigrate\")\nanalytic.miss[var.names] <- lapply(analytic.miss[var.names] , factor)\nstr(analytic.miss)\n#> 'data.frame':    397173 obs. of  22 variables:\n#>  $ CVD      : Factor w/ 2 levels \"event\",\"no event\": 1 2 2 2 2 2 2 2 2 2 ...\n#>  $ age      : Factor w/ 7 levels \"20-29 years\",..: 6 6 2 6 1 6 3 7 1 1 ...\n#>  $ sex      : Factor w/ 2 levels \"Female\",\"Male\": 1 1 2 1 1 2 2 2 1 2 ...\n#>  $ married  : Factor w/ 2 levels \"not single\",\"single\": 2 2 1 2 2 1 1 2 2 2 ...\n#>  $ race     : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ edu      : Factor w/ 4 levels \"< 2ndary\",\"2nd grad.\",..: 2 4 4 4 4 4 4 1 4 4 ...\n#>  $ income   : Factor w/ 4 levels \"$29,999 or less\",..: 1 1 4 1 2 2 1 1 NA 4 ...\n#>  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#>  $ phyact   : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 2 2 2 2 2 1 1 2 3 ...\n#>  $ doctor   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ stress   : Factor w/ 2 levels \"Not too stressed\",..: 1 1 2 1 1 1 1 NA 1 1 ...\n#>  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#>  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#>  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#>  $ bp       : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ diab     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ weight   : num  47.6 23.8 56.1 23.8 65.4 ...\n#>  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ OA       : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\n\nThe code identifies rows where data is missing and labels them for later analyses.\n\nShow the codeanalytic.miss$miss <- 1\nhead(analytic.miss$ID) # full data\n#> [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#> [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#> [1]  3  5  7 10 11 13\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] <- 0\ntable(analytic.miss$miss)\n#> \n#>      0      1 \n#> 185613 211560\n\n\nSetting Design\nThe code sets up the survey design, specifying weights (but no specific clustering and stratification, as they are unavailable for CCHS public access data), for use in survey-weighted analyses.\n\nShow the coderequire(survey)\nsummary(analytic.miss$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nw.design0 <- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#> [1] 80.34263\n\n\nThis creates a subset of the data where there are no missing values. Note that subset was done to the design object w.design0, not the data analytic.miss.\n\nShow the codew.design <- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsd(weights(w.design))\n#> [1] 84.97819\n\n\nBivariate analysis\nTable 1 (weighted)\nStratified by exposure\nThese tables contain descriptive statistics, stratified by different categories. They can be useful for understanding how variables relate to the exposure or outcome in the data.\n\nShow the coderequire(tableone)\nvar.names <- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"OA\"\n# tab1 <- CreateTableOne(var = var.names, strata= \"OA\", data=analytic.miss, test = TRUE)\n# print(tab1)\ntab2 <- svyCreateTableOne(var = var.names, strata= \"OA\", \n                          data=w.design, test = TRUE)\nprint(tab2)\n#>                        Stratified by OA\n#>                         Control            OA                p      test\n#>   n                     12124961.5         1153392.2                    \n#>   CVD = no event (%)    11786450.6 (97.2)  1020965.5 (88.5)  <0.001     \n#>   age (%)                                                    <0.001     \n#>      20-29 years         2668880.8 (22.0)    28317.5 ( 2.5)             \n#>      30-39 years         3009426.7 (24.8)    77159.3 ( 6.7)             \n#>      40-49 years         3108300.9 (25.6)   211515.1 (18.3)             \n#>      50-59 years         1900845.3 (15.7)   350264.7 (30.4)             \n#>      60-64 years          546041.8 ( 4.5)   163724.7 (14.2)             \n#>      65 years and over    624706.7 ( 5.2)   322033.4 (27.9)             \n#>      teen                 266759.3 ( 2.2)      377.4 ( 0.0)             \n#>   sex = Male (%)         6374765.5 (52.6)   379850.5 (32.9)  <0.001     \n#>   married = single (%)   4120611.0 (34.0)   367647.7 (31.9)  <0.001     \n#>   race = White (%)      10312228.2 (85.0)  1081778.6 (93.8)  <0.001     \n#>   edu (%)                                                    <0.001     \n#>      < 2ndary            1752318.3 (14.5)   309652.8 (26.8)             \n#>      2nd grad.           2314713.1 (19.1)   203437.5 (17.6)             \n#>      Other 2nd grad.     1078645.2 ( 8.9)    79255.1 ( 6.9)             \n#>      Post-2nd grad.      6979284.9 (57.6)   561046.8 (48.6)             \n#>   income (%)                                                 <0.001     \n#>      $29,999 or less     2051640.6 (16.9)   353862.9 (30.7)             \n#>      $30,000-$49,999     2436063.7 (20.1)   272484.1 (23.6)             \n#>      $50,000-$79,999     3495902.5 (28.8)   275115.8 (23.9)             \n#>      $80,000 or more     4141354.6 (34.2)   251929.4 (21.8)             \n#>   bmi (%)                                                    <0.001     \n#>      Underweight          346004.9 ( 2.9)    22064.6 ( 1.9)             \n#>      healthy weight      6019004.1 (49.6)   431570.2 (37.4)             \n#>      Overweight          5759952.5 (47.5)   699757.4 (60.7)             \n#>   phyact (%)                                                 <0.001     \n#>      Active              3037314.2 (25.1)   216879.5 (18.8)             \n#>      Inactive            5982492.3 (49.3)   647856.2 (56.2)             \n#>      Moderate            3105154.9 (25.6)   288656.5 (25.0)             \n#>   doctor = Yes (%)      10087473.8 (83.2)  1090763.9 (94.6)  <0.001     \n#>   stress = stressed (%)  3123770.9 (25.8)   301895.2 (26.2)   0.420     \n#>   smoke (%)                                                  <0.001     \n#>      Never smoker        4043479.9 (33.3)   320323.7 (27.8)             \n#>      Current smoker      3219168.6 (26.5)   275835.7 (23.9)             \n#>      Former smoker       4862313.0 (40.1)   557232.7 (48.3)             \n#>   drink (%)                                                  <0.001     \n#>      Never drank          678435.7 ( 5.6)    66085.0 ( 5.7)             \n#>      Current drinker    10297713.4 (84.9)   887808.2 (77.0)             \n#>      Former driker       1148812.3 ( 9.5)   199498.9 (17.3)             \n#>   fruit (%)                                                  <0.001     \n#>      0-3 daily serving   3214156.0 (26.5)   236483.8 (20.5)             \n#>      4-6 daily serving   6001124.3 (49.5)   588323.8 (51.0)             \n#>      6+ daily serving    2909681.1 (24.0)   328584.5 (28.5)             \n#>   bp = Yes (%)           1212548.2 (10.0)   347269.8 (30.1)  <0.001     \n#>   diab = Yes (%)          377876.2 ( 3.1)   104541.0 ( 9.1)  <0.001     \n#>   province = North (%)     27124.3 ( 0.2)     1825.1 ( 0.2)  <0.001     \n#>   immigrate (%)                                              <0.001     \n#>      not immigrant       9898636.6 (81.6)   994682.5 (86.2)             \n#>      > 10 years          1384672.6 (11.4)   146879.8 (12.7)             \n#>      recent               841652.3 ( 6.9)    11829.8 ( 1.0)\n\n\nStratified by outcome\nThis table is generally useful for logistic regression analysis\n\nShow the codevar.names <- c(\"OA\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"CVD\"\ntab3 <- svyCreateTableOne(var = var.names, strata= \"CVD\", data=w.design, test = TRUE)\nprint(tab3)\n#>                        Stratified by CVD\n#>                         event            no event           p      test\n#>   n                     470937.5         12807416.1                    \n#>   OA = OA (%)           132426.7 (28.1)   1020965.5 ( 8.0)  <0.001     \n#>   age (%)                                                   <0.001     \n#>      20-29 years         14966.0 ( 3.2)   2682232.3 (20.9)             \n#>      30-39 years         24105.5 ( 5.1)   3062480.5 (23.9)             \n#>      40-49 years         63520.1 (13.5)   3256296.0 (25.4)             \n#>      50-59 years        122613.0 (26.0)   2128497.0 (16.6)             \n#>      60-64 years         72328.3 (15.4)    637438.1 ( 5.0)             \n#>      65 years and over  172742.2 (36.7)    773997.8 ( 6.0)             \n#>      teen                  662.3 ( 0.1)    266474.4 ( 2.1)             \n#>   sex = Male (%)        267743.8 (56.9)   6486872.2 (50.6)  <0.001     \n#>   married = single (%)  143747.0 (30.5)   4344511.8 (33.9)  <0.001     \n#>   race = White (%)      434591.6 (92.3)  10959415.2 (85.6)  <0.001     \n#>   edu (%)                                                   <0.001     \n#>      < 2ndary           147338.4 (31.3)   1914632.6 (14.9)             \n#>      2nd grad.           77705.6 (16.5)   2440445.0 (19.1)             \n#>      Other 2nd grad.     30921.3 ( 6.6)   1126979.0 ( 8.8)             \n#>      Post-2nd grad.     214972.2 (45.6)   7325359.4 (57.2)             \n#>   income (%)                                                <0.001     \n#>      $29,999 or less    164929.4 (35.0)   2240574.2 (17.5)             \n#>      $30,000-$49,999    109988.2 (23.4)   2598559.6 (20.3)             \n#>      $50,000-$79,999    103091.1 (21.9)   3667927.1 (28.6)             \n#>      $80,000 or more     92928.7 (19.7)   4300355.3 (33.6)             \n#>   bmi (%)                                                   <0.001     \n#>      Underweight          8844.4 ( 1.9)    359225.0 ( 2.8)             \n#>      healthy weight     173475.1 (36.8)   6277099.2 (49.0)             \n#>      Overweight         288617.9 (61.3)   6171091.9 (48.2)             \n#>   phyact (%)                                                <0.001     \n#>      Active              85140.3 (18.1)   3169053.4 (24.7)             \n#>      Inactive           274968.8 (58.4)   6355379.7 (49.6)             \n#>      Moderate           110828.4 (23.5)   3282983.0 (25.6)             \n#>   doctor = Yes (%)      445493.3 (94.6)  10732744.5 (83.8)  <0.001     \n#>   stress = stressed (%) 113282.5 (24.1)   3312383.7 (25.9)   0.023     \n#>   smoke (%)                                                 <0.001     \n#>      Never smoker       119434.6 (25.4)   4244368.9 (33.1)             \n#>      Current smoker      97328.0 (20.7)   3397676.3 (26.5)             \n#>      Former smoker      254174.9 (54.0)   5165370.9 (40.3)             \n#>   drink (%)                                                 <0.001     \n#>      Never drank         29444.3 ( 6.3)    715076.4 ( 5.6)             \n#>      Current drinker    344405.1 (73.1)  10841116.6 (84.6)             \n#>      Former driker       97088.1 (20.6)   1251223.1 ( 9.8)             \n#>   fruit (%)                                                  0.001     \n#>      0-3 daily serving  111803.5 (23.7)   3338836.3 (26.1)             \n#>      4-6 daily serving  233403.5 (49.6)   6356044.7 (49.6)             \n#>      6+ daily serving   125730.4 (26.7)   3112535.1 (24.3)             \n#>   bp = Yes (%)          209257.0 (44.4)   1350561.0 (10.5)  <0.001     \n#>   diab = Yes (%)         78762.9 (16.7)    403654.4 ( 3.2)  <0.001     \n#>   province = North (%)     702.8 ( 0.1)     28246.6 ( 0.2)   0.005     \n#>   immigrate (%)                                             <0.001     \n#>      not immigrant      389553.0 (82.7)  10503766.2 (82.0)             \n#>      > 10 years          69008.0 (14.7)   1462544.4 (11.4)             \n#>      recent              12376.5 ( 2.6)    841105.5 ( 6.6)\n\n\nHow did they calculate the p-values? Hint: svychisq (see below).\nProportions and Design Effect\nThis part computes proportions and design effects, which help understand the influence of the sampling design on the estimated statistics.\n\nShow the coderequire(survey)\n# Computing survey statistics on subsets of a survey defined by factor(s).\nfit0a <- svyby(~CVD,~OA,design=w.design, svymean,deff=TRUE)\nfit0a\n\n\n\n  \n\n\nShow the codeconfint(fit0a)\n#>                          2.5 %    97.5 %\n#> Control:CVDevent    0.02681661 0.0290204\n#> OA:CVDevent         0.10847000 0.1211599\n#> Control:CVDno event 0.97097960 0.9731834\n#> OA:CVDno event      0.87884010 0.8915300\n# 7.45% OA patients estimated to have CVD event.\n# 95% CI:  (0.067, 0.0816)\n\n\nLet\n\n\n\\(\\theta\\) = parameter (population slope) and\n\n\n\\(\\hat(\\theta)\\) = statistic (estimated slope).\n\n\\(b = \\frac{\\sum[w (y_i-\\bar{y}) (x_i-\\bar{x})]}{\\sum[w (x_i-\\bar{x})^2]}\\)\nDE = Effect of complex survey on the SEs, relative to a SRS of equal size.\n\n\\(D^2(\\hat{\\theta}) = \\frac{Var(\\hat{\\theta})_{Complex Survey}}{Var(\\hat{\\theta})_{SRS}}\\)\n\\(D^2(\\hat{\\theta}) = \\frac{SE(\\hat{\\theta})^2_{Complex Survey}}{SE(\\hat{\\theta})^2_{SRS}}\\)\n\nNote:\n\nSE increases as value of weight increases (CCHS).\nNHANES has more things to worry about (strata, PSU)\n\nDEFF = 2 means that the variance of the sample proportion, when choosing the sample by complex survey sampling, is nearly 2 times as large as the variance of the same estimator under simple random sampling/SRS.\n\nShow the codefit0b <- svyby(~CVD,~diab,design=w.design, svymean,deff=TRUE)\nfit0b\n\n\n\n  \n\n\nShow the codeconfint(fit0b)\n#>                     2.5 %     97.5 %\n#> No:CVDevent     0.0295633 0.03173344\n#> Yes:CVDevent    0.1505917 0.17594247\n#> No:CVDno event  0.9682666 0.97043670\n#> Yes:CVDno event 0.8240575 0.84940825\n\n\nTesting association\nHere, Chi-square tests are conducted to test the association between different variables. Two variants of the test are used: Rao-Scott and Thomas-Rao modifications. These adaptations are used when the data come from a complex survey design.\n\nTests for hypothesis\n\nRao-Scott modifications (chi-sq)\nThomas-Rao modifications (F)\n\n\n\n\nShow the code# Rao-Scott modifications (chi-sq)\nsvychisq(~CVD+OA,design=w.design, statistic=\"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"Chisq\")\n#> X-squared = 3249.7, df = 1, p-value < 2.2e-16\n\n# Thomas-Rao modifications (F)\nsvychisq(~CVD+OA,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + OA, design = w.design, statistic = \"F\")\n#> F = 1863, ndf = 1, ddf = 185612, p-value < 2.2e-16\n\n# Both provide strong evidence to reject the null hypothesis.\n# Conclusion: there is a significant (at 5%) association \n# between CVD prevalence and OA.\nsvychisq(~CVD+fruit,design=w.design, statistic=\"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + fruit, design = w.design, statistic = \"F\")\n#> F = 7.1241, ndf = 1.9758e+00, ddf = 3.6673e+05, p-value = 0.0008503\nsvychisq(~CVD+province,design=w.design, statistic=\"Chisq\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~CVD + province, design = w.design, statistic = \"Chisq\")\n#> X-squared = 1.4848, df = 1, p-value = 0.00492\n\n\nSaving data\nFinally, the dataset, along with any new variables or subsets created during the analysis, is saved for future use.\n\nShow the codesave(w.design, analytic.miss, analytic2, file = \"Data/surveydata/cchs123w.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata4.html",
    "href": "surveydata4.html",
    "title": "CCHS: Regression",
    "section": "",
    "text": "This tutorial is for a complex data analysis, specifically using regression techniques to analyze survey data.\nLoads necessary R packages for the analysis\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(MASS)\n\n\nLoad data\nLoads a dataset and provides some quick data checks, like the dimensions and summary of weights.\n\nShow the codeload(\"Data/surveydata/cchs123w.RData\")\nls()\n#> [1] \"analytic.miss\" \"analytic2\"     \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\nsummary(weights(w.design))\n#> Length  Class   Mode \n#>      0   NULL   NULL\n\n\nLogistic for complex survey\nPerforms a simple logistic regression using the complex survey data, focusing on the relationship between cardiovascular disease and osteoarthritis.\n\nShow the codeformula0 <- as.formula(I(CVD==\"event\") ~ OA)\n\n## Crude regression\nfit2 <- svyglm(formula0, \n              design = w.design, \n              family = binomial(logit))\nrequire(Publish)\npublish(fit2)\n#>  Variable   Units OddsRatio       CI.95 p-value \n#>        OA Control       Ref                     \n#>                OA      4.52 [4.19;4.87]  <1e-04\n\n\nMultivariable analysis\nRuns a more complex logistic regression model, adding multiple covariates to better understand the relationship.\n\nShow the codeformula1 <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race + \n              edu + income + bmi + phyact + doctor + stress + \n              smoke + drink + fruit + bp + diab + province + immigrate)\n\nfit3 <- svyglm(formula1, \n              design = w.design, \n              family = binomial(logit))\npublish(fit3)\n#>   Variable             Units OddsRatio         CI.95     p-value \n#>         OA           Control       Ref                           \n#>                           OA      1.52   [1.40;1.66]     < 1e-04 \n#>        age       20-29 years       Ref                           \n#>                  30-39 years      1.29   [0.98;1.69]   0.0707636 \n#>                  40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                  50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                  60-64 years      9.71  [7.68;12.29]     < 1e-04 \n#>            65 years and over     15.85 [12.57;20.00]     < 1e-04 \n#>                         teen      0.46   [0.20;1.07]   0.0707295 \n#>        sex            Female       Ref                           \n#>                         Male      1.73   [1.60;1.88]     < 1e-04 \n#>    married        not single       Ref                           \n#>                       single      0.97   [0.90;1.05]   0.5209444 \n#>       race         Non-white       Ref                           \n#>                        White      1.44   [1.19;1.75]   0.0002219 \n#>        edu          < 2ndary       Ref                           \n#>                    2nd grad.      0.90   [0.80;1.00]   0.0512330 \n#>              Other 2nd grad.      0.97   [0.83;1.13]   0.6737665 \n#>               Post-2nd grad.      0.93   [0.85;1.02]   0.1016562 \n#>     income   $29,999 or less       Ref                           \n#>              $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>              $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>              $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>        bmi       Underweight       Ref                           \n#>               healthy weight      0.86   [0.67;1.10]   0.2350526 \n#>                   Overweight      0.88   [0.69;1.12]   0.3033213 \n#>     phyact            Active       Ref                           \n#>                     Inactive      1.21   [1.10;1.34]   0.0001345 \n#>                     Moderate      1.08   [0.97;1.21]   0.1771985 \n#>     doctor                No       Ref                           \n#>                          Yes      1.75   [1.49;2.06]     < 1e-04 \n#>     stress  Not too stressed       Ref                           \n#>                     stressed      1.30   [1.18;1.42]     < 1e-04 \n#>      smoke      Never smoker       Ref                           \n#>               Current smoker      1.18   [1.05;1.32]   0.0050518 \n#>                Former smoker      1.21   [1.11;1.33]     < 1e-04 \n#>      drink       Never drank       Ref                           \n#>              Current drinker      0.82   [0.68;0.98]   0.0290605 \n#>                Former driker      1.13   [0.93;1.36]   0.2133779 \n#>      fruit 0-3 daily serving       Ref                           \n#>            4-6 daily serving      0.94   [0.86;1.03]   0.1758214 \n#>             6+ daily serving      1.09   [0.97;1.23]   0.1311029 \n#>         bp                No       Ref                           \n#>                          Yes      2.35   [2.16;2.55]     < 1e-04 \n#>       diab                No       Ref                           \n#>                          Yes      1.86   [1.66;2.07]     < 1e-04 \n#>   province             South       Ref                           \n#>                        North      1.21   [0.90;1.62]   0.2103030 \n#>  immigrate     not immigrant       Ref                           \n#>                   > 10 years      1.02   [0.89;1.16]   0.8057243 \n#>                       recent      1.06   [0.73;1.53]   0.7651069\n\n\nModel fit assessment\nVariability explained\nPseudo-R-square values indicate how much of the total variability in the outcomes is explainable by the fitted model (analogous to R-square)\n\nCox/Snell (never reaches max 1)\nNagelkerke R-square (scaled to max 1)\n\n\nThe larger Cox & Snell estimate is the better the model.\nThese Pseudo-R-square values should be interpreted with caution (if not ignored).\nThey offer little confidence in interpreting the model fit.\nSurvey weighted version of them are available.\nNot trivial to decide which statistic to use under complex surveys.\n\nEvaluates the model fit using Akaike Information Criterion (AIC) and pseudo R-squared metrics.\n\nShow the codefit3 <- svyglm(formula1, \n              design = w.design, \n              family = quasibinomial(logit)) # publish does not work\nAIC(fit3) \n#>        eff.p          AIC     deltabar \n#>    67.752064 45362.706032     2.053093\n\n# AIC for survey weighted regressions\npsrsq(fit3, method = \"Cox-Snell\")\n#> [1] 0.06091896\npsrsq(fit3, method = \"Nagelkerke\")\n#> [1] 0.2307586\n# Nagelkerke and Cox-Snell pseudo-rsquared statistics\n\n\nBackward Elimination\n\nModel comparisons\n\nLRT-aprroximation\nWald-based\n\n\n\nChecking one by one\nChecks the significance of each variable one by one and removes those that are not statistically significant.\n\nShow the coderound(sort(summary(fit3)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.                ageteen \n#>                   0.03                   0.05                   0.07 \n#>         age30-39 years      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.18                   0.18                   0.21 \n#>     drinkFormer driker      bmihealthy weight          bmiOverweight \n#>                   0.21                   0.24                   0.30 \n#>          marriedsingle     eduOther 2nd grad.        immigraterecent \n#>                   0.52                   0.67                   0.77 \n#>    immigrate> 10 years \n#>                   0.81\n# bmiOverweight is associated with largest p-value\n# but what about other categories?\n\nregTermTest(fit3,~bmi) # coef of all bmi cat = 0\n#> Wald test for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> F =  0.7591291  on  2  and  185579  df: p= 0.46808\nfit4 <- update(fit3, .~. -bmi) \n\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for bmi\n#>  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#> Working 2logLR =  1.424634 p= 0.49071 \n#> (scale factors:  1.1 0.93 );  denominator df= 185579\n# high p-value (in both wald and Anova) makes it more likely that you should exclude bmi\nAIC(fit3,fit4) \n#>         eff.p      AIC deltabar\n#> [1,] 67.75206 45362.71 2.053093\n#> [2,] 64.30460 45358.26 2.074342\nround(sort(summary(fit4)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.00 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#> fruit4-6 daily serving         phyactModerate          provinceNorth \n#>                   0.17                   0.17                   0.21 \n#>     drinkFormer driker          marriedsingle     eduOther 2nd grad. \n#>                   0.21                   0.53                   0.67 \n#>        immigraterecent    immigrate> 10 years \n#>                   0.76                   0.81\n\n\nUsing AIC to automate\nUses stepwise regression guided by AIC to automatically select the most important variables.\n\nShow the coderequire(MASS)\nformula1b <- as.formula(I(CVD==\"event\") ~ OA + age + sex)\nfit1b <- svyglm(formula1b, \n              design = w.design, \n              family = binomial(logit))\nfit5 <- stepAIC(fit1b, direction = \"backward\")\n#> Start:  AIC=47384.51\n#> I(CVD == \"event\") ~ OA + age + sex\n#> \n#>        Df Deviance   AIC\n#> <none>       47353 47385\n#> - sex   1    47634 47658\n#> - OA    1    47679 47702\n#> - age   6    54414 54291\n\n\n\nShow the codepublish(fit5)\n#>  Variable             Units OddsRatio         CI.95   p-value \n#>        OA           Control       Ref                         \n#>                          OA      1.81   [1.66;1.97]   < 1e-04 \n#>       age       20-29 years       Ref                         \n#>                 30-39 years      1.40   [1.07;1.84]   0.01374 \n#>                 40-49 years      3.39   [2.69;4.27]   < 1e-04 \n#>                 50-59 years      9.42  [7.55;11.76]   < 1e-04 \n#>                 60-64 years     17.78 [14.22;22.23]   < 1e-04 \n#>           65 years and over     33.82 [27.26;41.97]   < 1e-04 \n#>                        teen      0.45   [0.20;1.02]   0.05462 \n#>       sex            Female       Ref                         \n#>                        Male      1.57   [1.46;1.69]   < 1e-04\nround(sort(summary(fit5)$coef[,\"Pr(>|t|)\"]),2)\n#>          (Intercept) age65 years and over       age60-64 years \n#>                 0.00                 0.00                 0.00 \n#>       age50-59 years                 OAOA              sexMale \n#>                 0.00                 0.00                 0.00 \n#>       age40-49 years       age30-39 years              ageteen \n#>                 0.00                 0.01                 0.05\n\n\nUsing AIC, but keeping importants\nSimilar to the previous step but ensures certain important variables remain in the model.\n\nShow the codeformula1c <- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate)\nscope <- list(upper = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate,\n              lower = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab)\n\nfit1c <- svyglm(formula1c, design = w.design, family = binomial(logit))\n\nfitstep <- step(fit1c, scope = scope, trace = FALSE, k = 2, direction = \"backward\")\n# k = 2 gives the genuine AIC\n\n\n\nShow the codepublish(fitstep)\n#>  Variable             Units OddsRatio         CI.95     p-value \n#>        OA           Control       Ref                           \n#>                          OA      1.52   [1.40;1.66]     < 1e-04 \n#>       age       20-29 years       Ref                           \n#>                 30-39 years      1.29   [0.98;1.70]   0.0697699 \n#>                 40-49 years      2.74   [2.17;3.47]     < 1e-04 \n#>                 50-59 years      6.24   [4.97;7.83]     < 1e-04 \n#>                 60-64 years      9.71  [7.68;12.28]     < 1e-04 \n#>           65 years and over     15.85 [12.57;19.98]     < 1e-04 \n#>                        teen      0.46   [0.20;1.07]   0.0705660 \n#>       sex            Female       Ref                           \n#>                        Male      1.73   [1.60;1.88]     < 1e-04 \n#>   married        not single       Ref                           \n#>                      single      0.97   [0.90;1.05]   0.5042496 \n#>      race         Non-white       Ref                           \n#>                       White      1.41   [1.18;1.70]   0.0001986 \n#>       edu          < 2ndary       Ref                           \n#>                   2nd grad.      0.90   [0.80;1.00]   0.0524940 \n#>             Other 2nd grad.      0.97   [0.83;1.13]   0.6732446 \n#>              Post-2nd grad.      0.93   [0.85;1.02]   0.1045975 \n#>    income   $29,999 or less       Ref                           \n#>             $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>             $50,000-$79,999      0.64   [0.58;0.72]     < 1e-04 \n#>             $80,000 or more      0.58   [0.51;0.66]     < 1e-04 \n#>       bmi       Underweight       Ref                           \n#>              healthy weight      0.86   [0.67;1.10]   0.2316915 \n#>                  Overweight      0.88   [0.69;1.12]   0.2982852 \n#>    phyact            Active       Ref                           \n#>                    Inactive      1.22   [1.10;1.34]   0.0001227 \n#>                    Moderate      1.08   [0.97;1.21]   0.1754422 \n#>     fruit 0-3 daily serving       Ref                           \n#>           4-6 daily serving      0.94   [0.86;1.03]   0.1807666 \n#>            6+ daily serving      1.09   [0.97;1.23]   0.1295281 \n#>        bp                No       Ref                           \n#>                         Yes      2.35   [2.16;2.55]     < 1e-04 \n#>      diab                No       Ref                           \n#>                         Yes      1.85   [1.66;2.07]     < 1e-04 \n#>    doctor                No       Ref                           \n#>                         Yes      1.75   [1.49;2.05]     < 1e-04 \n#>    stress  Not too stressed       Ref                           \n#>                    stressed      1.30   [1.18;1.42]     < 1e-04 \n#>     smoke      Never smoker       Ref                           \n#>              Current smoker      1.17   [1.05;1.31]   0.0053412 \n#>               Former smoker      1.21   [1.10;1.33]     < 1e-04 \n#>     drink       Never drank       Ref                           \n#>             Current drinker      0.82   [0.68;0.98]   0.0254942 \n#>               Former driker      1.12   [0.93;1.36]   0.2205315\nround(sort(summary(fitstep)$coef[,\"Pr(>|t|)\"]),2)\n#>            (Intercept)   age65 years and over                  bpYes \n#>                   0.00                   0.00                   0.00 \n#>         age60-64 years         age50-59 years                sexMale \n#>                   0.00                   0.00                   0.00 \n#>                diabYes                   OAOA  income$80,000 or more \n#>                   0.00                   0.00                   0.00 \n#>         age40-49 years  income$50,000-$79,999              doctorYes \n#>                   0.00                   0.00                   0.00 \n#>  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#>                   0.00                   0.00                   0.00 \n#>         phyactInactive              raceWhite    smokeCurrent smoker \n#>                   0.00                   0.00                   0.01 \n#>   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#>                   0.03                   0.05                   0.07 \n#>                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#>                   0.07                   0.10                   0.13 \n#>         phyactModerate fruit4-6 daily serving     drinkFormer driker \n#>                   0.18                   0.18                   0.22 \n#>      bmihealthy weight          bmiOverweight          marriedsingle \n#>                   0.23                   0.30                   0.50 \n#>     eduOther 2nd grad. \n#>                   0.67\n\n\nAssess interactions\nCheck biologically interesting ones.\nCheck one by one\nChecks if there is a significant interaction effect between ‘age’ and ‘sex’.\n\nShow the codefit8a <- update(fitstep, .~. + interaction(age,sex))\nanova(fitstep, fit8a) # keep interaction\n#> Working (Rao-Scott+F) LRT for interaction(age, sex)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(age, sex), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  40.16528 p= 1.2167e-06 \n#> (scale factors:  1.3 1.2 1.2 0.93 0.78 0.71 );  denominator df= 185576\n\n\nChecks if there is a significant interaction effect between ‘sex’ and ‘diabetes’.\n\nShow the codefit8b <- update(fitstep, .~. + interaction(sex,diab))\nanova(fitstep, fit8b) # Do not keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(sex, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(sex, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  0.4591456 p= 0.49597 \n#> df=1;  denominator df= 185581\n\n\nChecks if there is a significant interaction effect between ‘BMI’ and ‘diabetes’.\n\nShow the codefit8c <- update(fitstep, .~. + interaction(bmi,diab))\nanova(fitstep, fit8c) # keep this interaction\n#> Working (Rao-Scott+F) LRT for interaction(bmi, diab)\n#>  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#>     race + edu + income + bmi + phyact + fruit + bp + diab + \n#>     doctor + stress + smoke + drink + interaction(bmi, diab), \n#>     design = w.design, family = binomial(logit))\n#> Working 2logLR =  7.92727 p= 0.02533 \n#> (scale factors:  1.4 0.6 );  denominator df= 185580\n\n\nAdd all significant interactions in 1 model\nUpdates the model to include significant interaction terms.\nNote that we have 0 effect modifier, 2 interactions\n\nShow the codefit9 <- update(fitstep, .~. + interaction(age,sex) + interaction(bmi,diab))\nrequire(jtools)\nsumm(fit9, confint = TRUE, digits = 3)\n\n\n\n\n Observations \n    185613 \n  \n\n Dependent variable \n    I(CVD == \"event\") \n  \n\n Type \n    Survey-weighted generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n Pseudo-R² (Cragg-Uhler) \n    0.233 \n  \n\n Pseudo-R² (McFadden) \n    0.207 \n  \n\n AIC \n    41548.160 \n  \n\n\n\n   \n    Est. \n    2.5% \n    97.5% \n    t val. \n    p \n  \n\n\n (Intercept) \n    -5.590 \n    -6.046 \n    -5.135 \n    -24.069 \n    0.000 \n  \n\n OAOA \n    0.438 \n    0.352 \n    0.523 \n    10.043 \n    0.000 \n  \n\n age30-39 years \n    0.299 \n    -0.118 \n    0.717 \n    1.405 \n    0.160 \n  \n\n age40-49 years \n    1.158 \n    0.794 \n    1.522 \n    6.236 \n    0.000 \n  \n\n age50-59 years \n    2.181 \n    1.831 \n    2.531 \n    12.212 \n    0.000 \n  \n\n age60-64 years \n    2.619 \n    2.263 \n    2.976 \n    14.395 \n    0.000 \n  \n\n age65 years and over \n    3.033 \n    2.680 \n    3.387 \n    16.833 \n    0.000 \n  \n\n ageteen \n    -0.695 \n    -1.704 \n    0.314 \n    -1.350 \n    0.177 \n  \n\n sexMale \n    -0.028 \n    -0.447 \n    0.390 \n    -0.133 \n    0.895 \n  \n\n marriedsingle \n    -0.016 \n    -0.096 \n    0.064 \n    -0.382 \n    0.703 \n  \n\n raceWhite \n    0.348 \n    0.165 \n    0.531 \n    3.724 \n    0.000 \n  \n\n edu2nd grad. \n    -0.107 \n    -0.217 \n    0.003 \n    -1.906 \n    0.057 \n  \n\n eduOther 2nd grad. \n    -0.039 \n    -0.192 \n    0.114 \n    -0.498 \n    0.618 \n  \n\n eduPost-2nd grad. \n    -0.081 \n    -0.173 \n    0.010 \n    -1.746 \n    0.081 \n  \n\n income$30,000-$49,999 \n    -0.304 \n    -0.400 \n    -0.208 \n    -6.215 \n    0.000 \n  \n\n income$50,000-$79,999 \n    -0.444 \n    -0.552 \n    -0.336 \n    -8.034 \n    0.000 \n  \n\n income$80,000 or more \n    -0.555 \n    -0.684 \n    -0.426 \n    -8.424 \n    0.000 \n  \n\n bmihealthy weight \n    -1.406 \n    -2.677 \n    -0.135 \n    -2.167 \n    0.030 \n  \n\n bmiOverweight \n    -1.110 \n    -2.375 \n    0.154 \n    -1.721 \n    0.085 \n  \n\n phyactInactive \n    0.194 \n    0.094 \n    0.293 \n    3.817 \n    0.000 \n  \n\n phyactModerate \n    0.077 \n    -0.034 \n    0.187 \n    1.353 \n    0.176 \n  \n\n fruit4-6 daily serving \n    -0.062 \n    -0.155 \n    0.031 \n    -1.313 \n    0.189 \n  \n\n fruit6+ daily serving \n    0.094 \n    -0.023 \n    0.211 \n    1.579 \n    0.114 \n  \n\n bpYes \n    0.861 \n    0.778 \n    0.944 \n    20.354 \n    0.000 \n  \n\n diabYes \n    1.745 \n    0.462 \n    3.027 \n    2.667 \n    0.008 \n  \n\n doctorYes \n    0.535 \n    0.372 \n    0.697 \n    6.447 \n    0.000 \n  \n\n stressstressed \n    0.256 \n    0.164 \n    0.347 \n    5.483 \n    0.000 \n  \n\n smokeCurrent smoker \n    0.152 \n    0.039 \n    0.266 \n    2.628 \n    0.009 \n  \n\n smokeFormer smoker \n    0.177 \n    0.082 \n    0.272 \n    3.654 \n    0.000 \n  \n\n drinkCurrent drinker \n    -0.205 \n    -0.381 \n    -0.029 \n    -2.277 \n    0.023 \n  \n\n drinkFormer driker \n    0.116 \n    -0.072 \n    0.303 \n    1.210 \n    0.226 \n  \n\n interaction(age, sex)30-39 years.Female \n    -0.083 \n    -0.622 \n    0.457 \n    -0.300 \n    0.764 \n  \n\n interaction(age, sex)40-49 years.Female \n    -0.302 \n    -0.766 \n    0.161 \n    -1.278 \n    0.201 \n  \n\n interaction(age, sex)50-59 years.Female \n    -0.810 \n    -1.258 \n    -0.362 \n    -3.543 \n    0.000 \n  \n\n interaction(age, sex)60-64 years.Female \n    -0.787 \n    -1.237 \n    -0.337 \n    -3.428 \n    0.001 \n  \n\n interaction(age, sex)65 years and over.Female \n    -0.603 \n    -1.035 \n    -0.170 \n    -2.733 \n    0.006 \n  \n\n interaction(age, sex)teen.Female \n    -0.129 \n    -1.806 \n    1.548 \n    -0.151 \n    0.880 \n  \n\n interaction(bmi, diab)healthy weight.No \n    1.380 \n    0.085 \n    2.675 \n    2.089 \n    0.037 \n  \n\n interaction(bmi, diab)Overweight.No \n    1.085 \n    -0.204 \n    2.373 \n    1.650 \n    0.099 \n  \n\n\n Standard errors: Robust\n\n\n\n\nShow the codefit9 <- update(fitstep, .~. + age:sex + bmi:diab)\npublish(fit9)\n#>                                            Variable             Units OddsRatio         CI.95     p-value \n#>                                                  OA           Control       Ref                           \n#>                                                                    OA      1.55   [1.42;1.69]     < 1e-04 \n#>                                             married        not single       Ref                           \n#>                                                                single      0.98   [0.91;1.07]   0.7026897 \n#>                                                race         Non-white       Ref                           \n#>                                                                 White      1.42   [1.18;1.70]   0.0001960 \n#>                                                 edu          < 2ndary       Ref                           \n#>                                                             2nd grad.      0.90   [0.81;1.00]   0.0566536 \n#>                                                       Other 2nd grad.      0.96   [0.83;1.12]   0.6183383 \n#>                                                        Post-2nd grad.      0.92   [0.84;1.01]   0.0807393 \n#>                                              income   $29,999 or less       Ref                           \n#>                                                       $30,000-$49,999      0.74   [0.67;0.81]     < 1e-04 \n#>                                                       $50,000-$79,999      0.64   [0.58;0.71]     < 1e-04 \n#>                                                       $80,000 or more      0.57   [0.50;0.65]     < 1e-04 \n#>                                              phyact            Active       Ref                           \n#>                                                              Inactive      1.21   [1.10;1.34]   0.0001349 \n#>                                                              Moderate      1.08   [0.97;1.21]   0.1760803 \n#>                                               fruit 0-3 daily serving       Ref                           \n#>                                                     4-6 daily serving      0.94   [0.86;1.03]   0.1892549 \n#>                                                      6+ daily serving      1.10   [0.98;1.23]   0.1142759 \n#>                                                  bp                No       Ref                           \n#>                                                                   Yes      2.37   [2.18;2.57]     < 1e-04 \n#>                                              doctor                No       Ref                           \n#>                                                                   Yes      1.71   [1.45;2.01]     < 1e-04 \n#>                                              stress  Not too stressed       Ref                           \n#>                                                              stressed      1.29   [1.18;1.42]     < 1e-04 \n#>                                               smoke      Never smoker       Ref                           \n#>                                                        Current smoker      1.16   [1.04;1.30]   0.0085960 \n#>                                                         Former smoker      1.19   [1.09;1.31]   0.0002579 \n#>                                               drink       Never drank       Ref                           \n#>                                                       Current drinker      0.81   [0.68;0.97]   0.0227934 \n#>                                                         Former driker      1.12   [0.93;1.35]   0.2264702 \n#>               age(20-29 years): sex(Male vs Female)                        0.97   [0.64;1.48]   0.8945322 \n#>               age(30-39 years): sex(Male vs Female)                        1.06   [0.75;1.49]   0.7583565 \n#>               age(40-49 years): sex(Male vs Female)                        1.32   [1.07;1.62]   0.0091527 \n#>               age(50-59 years): sex(Male vs Female)                        2.18   [1.85;2.58]     < 1e-04 \n#>               age(60-64 years): sex(Male vs Female)                        2.14   [1.80;2.53]     < 1e-04 \n#>         age(65 years and over): sex(Male vs Female)                        1.78   [1.58;1.99]     < 1e-04 \n#>                      age(teen): sex(Male vs Female)                        1.11   [0.22;5.62]   0.9033924 \n#>        sex(Female): age(30-39 years vs 20-29 years)                        1.24   [0.87;1.77]   0.2276609 \n#>        sex(Female): age(40-49 years vs 20-29 years)                        2.35   [1.75;3.16]     < 1e-04 \n#>        sex(Female): age(50-59 years vs 20-29 years)                        3.94   [2.95;5.27]     < 1e-04 \n#>        sex(Female): age(60-64 years vs 20-29 years)                        6.25   [4.66;8.39]     < 1e-04 \n#>  sex(Female): age(65 years and over vs 20-29 years)                       11.37  [8.60;15.02]     < 1e-04 \n#>               sex(Female): age(teen vs 20-29 years)                        0.44   [0.11;1.69]   0.2320840 \n#>          sex(Male): age(30-39 years vs 20-29 years)                        1.35   [0.89;2.05]   0.1599938 \n#>          sex(Male): age(40-49 years vs 20-29 years)                        3.18   [2.21;4.58]     < 1e-04 \n#>          sex(Male): age(50-59 years vs 20-29 years)                        8.85  [6.24;12.56]     < 1e-04 \n#>          sex(Male): age(60-64 years vs 20-29 years)                       13.73  [9.61;19.61]     < 1e-04 \n#>    sex(Male): age(65 years and over vs 20-29 years)                       20.77 [14.59;29.57]     < 1e-04 \n#>                 sex(Male): age(teen vs 20-29 years)                        0.50   [0.18;1.37]   0.1769155 \n#>                   bmi(Underweight): diab(Yes vs No)                        5.72  [1.59;20.63]   0.0076561 \n#>                bmi(healthy weight): diab(Yes vs No)                        1.44   [1.19;1.75]   0.0002221 \n#>                    bmi(Overweight): diab(Yes vs No)                        1.93   [1.70;2.20]     < 1e-04 \n#>        diab(No): bmi(healthy weight vs Underweight)                        0.97   [0.76;1.25]   0.8394972 \n#>            diab(No): bmi(Overweight vs Underweight)                        0.97   [0.76;1.25]   0.8400722 \n#>       diab(Yes): bmi(healthy weight vs Underweight)                        0.25   [0.07;0.87]   0.0302066 \n#>           diab(Yes): bmi(Overweight vs Underweight)                        0.33   [0.09;1.17]   0.0852377\n\n\n\nShow the codebasic.model <- eval(fit5$call[[2]])\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\n\naic.int.model <- eval(fit9$call[[2]])\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\n\n\nSaving data\nSaves the final regression models for future use.\n\nShow the codesave(basic.model, aic.int.model, file = \"Data/surveydata/cchs123w2.RData\")\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata5.html",
    "href": "surveydata5.html",
    "title": "CCHS: Performance",
    "section": "",
    "text": "The tutorial outlines the process for evaluating the performance of logistic regression models fitted to complex survey data using R. It focuses on two major aspects: creating Receiver Operating Characteristic (ROC) curves and conducting Archer and Lemeshow Goodness of Fit tests. Here AUC is a measure to evaluate the predictive accuracy of the model, and Archer and Lemeshow test is a statistical test to evaluate how well your model fits the observed data.\nWe start by importing the required R packages.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\n\nLoad data\nIt loads two datasets from the specified paths.\n\nShow the codeload(\"Data/surveydata/cchs123w.RData\")\nload(\"Data/surveydata/cchs123w2.RData\")\nls()\n#> [1] \"aic.int.model\" \"analytic.miss\" \"analytic2\"     \"basic.model\"  \n#> [5] \"w.design\"\ndim(analytic.miss)\n#> [1] 397173     23\ndim(analytic2)\n#> [1] 185613     22\n\n\nThree different logistic regression models are fitted to the data:\n\nShow the codelibrary(survey)\nsimple.model <- as.formula(I(CVD==\"event\") ~ OA)\nfit0 <- svyglm(simple.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nbasic.model\n#> I(CVD == \"event\") ~ OA + age + sex\n#> attr(,\"variables\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"factors\")\n#>                   OA age sex\n#> I(CVD == \"event\")  0   0   0\n#> OA                 1   0   0\n#> age                0   1   0\n#> sex                0   0   1\n#> attr(,\"term.labels\")\n#> [1] \"OA\"  \"age\" \"sex\"\n#> attr(,\"order\")\n#> [1] 1 1 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\n#> attr(,\"predvars\")\n#> list(I(CVD == \"event\"), OA, age, sex)\n#> attr(,\"dataClasses\")\n#> I(CVD == \"event\")                OA               age               sex \n#>         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#>         (weights) \n#>         \"numeric\"\nfit5 <- svyglm(basic.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\naic.int.model\n#> I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#>     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#>     drink + age:sex + bmi:diab\nfit9 <- svyglm(aic.int.model,\n              design = w.design,\n              family = binomial(logit))\n#> Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nModel performance\nROC curve\nThis section defines a function, svyROCw, to plot the ROC curves and calculate the area under the curve (AUC). The function can handle both weighted and unweighted survey data.\n\nThe appropriateness of the fitted logistic regression model needs to be examined before it is accepted for use.\nPlotting the pairs of - sensitivities vs - 1-specificities on a scatter plot provides a Receiver Operating Characteristic (ROC) curve.\nThe area under the ROC curve = AUC / C-statistic.\nROC/AUC should consider weights for complex surveys.\n\nGrading Guidelines for AUC values:\n\n0.90-1.0 excellent discrimination (unusual)\n0.80-0.90 good discrimination\n0.70-0.80 fair discrimination\n0.60-0.70 poor discrimination\n0.50-0.60 failed discrimination\n\n\nShow the coderequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit=fit,outcome=analytic2$CVD==\"event\", weight = NULL){\n  # ROC curve for\n  # Survey Data with Logistic Regression\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { # library(WeightedROC)\n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\n\n\n\nShow the codesummary(analytic2$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    1.17   71.56  137.95  214.61  261.91 7154.95\nanalytic2$corrected.weight <- weights(w.design)\nsummary(analytic2$corrected.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    0.39   23.85   45.98   71.54   87.30 2384.98\nsvyROCw(fit=fit0,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the codesvyROCw(fit=fit5,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the codesvyROCw(fit=fit9,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\nShow the code# This function does not take in to account of strata/cluster\n\n\nArcher and Lemeshow test\nThis test helps to evaluate how well the model fits the data. A Goodness of Fit (GOF) function AL.gof is defined. If the p-value from this test is greater than a certain threshold (e.g., 0.05), the model fit is considered acceptable.\n\nHosmer Lemeshow-type tests are most useful as a very crude way to screen for fit problems, and should not be taken as a definitive diagnostic of a ‘good’ fit.\n\nproblem in small sample size\nDependent on G (group)\n\n\nArcher and Lemeshow (2006) extended the standard Hosmer and Lemeshow GOF test for complex surveys.\nAfter fitting the survey weighted logistic regression, the F-adjusted mean residual goodness-of-fit test could suggest\n\nno evidence of lack of fit (if P-value > a reasonable cut-point, e.g., 0.05)\nevidence of lack of fit (if P-value < a reasonable cut-point, e.g., 0.05)\n\n\n\n\nShow the codeAL.gof <- function(fit=fit, data = analytic2, \n                   weight = \"corrected.weight\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=~1, \n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\n\nShow the codeAL.gof(fit0, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.20807e-22  on  1  and  185611  df: p= 1\nAL.gof(fit5, analytic2, weight =\"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.795204  on  8  and  185604  df: p= 0.0042898\nAL.gof(fit9, analytic2, weight = \"corrected.weight\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  2.650332  on  9  and  185603  df: p= 0.0045417\n\n\nAdditional function\nIf the survey data contains strata and cluster, then the following function will be useful:\n\nShow the codeAL.gof2 <- function(fit=fit7, data = analytic, \n                   weight = \"corrected.weight\", psu = \"psu\", strata= \"strata\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r <- residuals(fit, type=\"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  decilemodel<- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata6.html#a-note-about-predictive-models",
    "href": "surveydata6.html#a-note-about-predictive-models",
    "title": "NHANES: Blood Pressure",
    "section": "A note about Predictive models",
    "text": "A note about Predictive models\nIn statistical analyses involving survey data, it’s crucial to account for the survey’s design features. These features can include sampling weights, stratification, and clustering, among others. Ignoring these could lead to biased estimates and incorrect conclusions. In the tutorial you mentioned, such survey design features are considered, making the analysis more robust and reliable in terms of inference.\nHowever, when the goal shifts from inference to prediction, additional challenges come into play. Specifically, the model may perform well on the data used to fit it (the “training” data) but not generalize well to new, unseen data. This discrepancy between training performance and generalization to new data is often referred to as “overfitting,” and the optimism of the model refers to the extent to which it overestimates its predictive performance on new data based on its performance on the training data.\nOptimism-correction techniques are methodologies designed to address this issue. They allow you to evaluate how well your model is likely to perform on new data, not just the data you used to build it. Methods for optimism correction often involve techniques like cross-validation, bootstrapping, or specialized types of model validation that help in estimating the ‘true’ predictive performance of the model. Some of these techniques were discussed in the predictive modelling chapter.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata7.html",
    "href": "surveydata7.html",
    "title": "NHANES: Cholesterol",
    "section": "",
    "text": "Show the code# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(tableone)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(dplyr)\n\n\nPreprocessing\nAnalytic data set\nWe will use cholesterolNHANES15part1.RData in this prediction goal question (predicting cholesterol in adults).\nFor this exercise, we are assuming that:\n\noutcome: cholesterol\n\npredictors:\n\ngender\nwhether born in US\nrace\neducation\nwhether married\nincome level\nBMI\nwhether has diabetes\n\n\n\nsurvey features:\n\nsurvey weights\nstrata\ncluster/PSU; where strata is nested within clusters\n\n– restrict to those participants who are of 18 years of age or older\n\n\n\nShow the codeload(\"Data/surveydata/cholesterolNHANES15part1.rdata\") #Loading the dataset\nls()\n#>  [1] \"analytic\"           \"analytic.with.miss\" \"analytic1\"         \n#>  [4] \"analytic2\"          \"analytic2b\"         \"analytic3\"         \n#>  [7] \"collinearity\"       \"correlationMatrix\"  \"diff.boot\"         \n#> [10] \"extract.boot.fun\"   \"extract.fit\"        \"extract.lm.fun\"    \n#> [13] \"fictitious.data\"    \"fit0\"               \"fit1\"              \n#> [16] \"fit2\"               \"fit3\"               \"fit4\"              \n#> [19] \"fit5\"               \"formula0\"           \"formula1\"          \n#> [22] \"formula2\"           \"formula3\"           \"formula4\"          \n#> [25] \"formula5\"           \"k.folds\"            \"numeric.names\"     \n#> [28] \"perform\"            \"pred.y\"             \"rocobj\"            \n#> [31] \"sel.names\"          \"var.cluster\"        \"var.summ\"          \n#> [34] \"var.summ2\"\n\n\nRetaining only useful variables\n\nShow the code# Data dimensions\ndim(analytic)\n#> [1] 1267   33\n\n# Variable names\nnames(analytic)\n#>  [1] \"ID\"                    \"gender\"                \"age\"                  \n#>  [4] \"born\"                  \"race\"                  \"education\"            \n#>  [7] \"married\"               \"income\"                \"weight\"               \n#> [10] \"psu\"                   \"strata\"                \"diastolicBP\"          \n#> [13] \"systolicBP\"            \"bodyweight\"            \"bodyheight\"           \n#> [16] \"bmi\"                   \"waist\"                 \"smoke\"                \n#> [19] \"alcohol\"               \"cholesterol\"           \"cholesterolM2\"        \n#> [22] \"triglycerides\"         \"uric.acid\"             \"protein\"              \n#> [25] \"bilirubin\"             \"phosphorus\"            \"sodium\"               \n#> [28] \"potassium\"             \"globulin\"              \"calcium\"              \n#> [31] \"physical.work\"         \"physical.recreational\" \"diabetes\"\n\n#Subsetting dataset with variables needed:\nrequire(dplyr)\nanadata <- select(analytic, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\n# new data sizes\ndim(anadata)\n#> [1] 1267   13\n\n# retained variable names\nnames(anadata)\n#>  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#>  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#> [11] \"weight\"      \"psu\"         \"strata\"\n\n#Restricting to participants who are 18 or older\nsummary(anadata$age) #The age range is already 20-80\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   20.00   36.00   51.00   49.91   63.00   80.00\n\n#Recoding the born variable\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born in 50 US states or Washingt                           Others \n#>                              991                              276 \n#>                             <NA> \n#>                                0\nlevels(anadata$born)\n#> NULL\nanadata$born <- car::recode(anadata$born,\n                            \"'Born in 50 US states or Washingt' = 'Born.in.US';\n                            'Others' = 'Others';\n                            else=NA\")\ntable(anadata$born, useNA = \"always\")\n#> \n#> Born.in.US     Others       <NA> \n#>        991        276          0\n\n\nChecking the data for missing\n\nShow the coderequire(DataExplorer)\nplot_missing(anadata) #no missing data\n\n\n\n\nPreparing factor and continuous variables appropriately\n\nShow the codevars = c(\"cholesterol\", \"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\nnumeric.names <- c(\"cholesterol\", \"bmi\")\nfactor.names <- vars[!vars %in% numeric.names] \n\nanadata[factor.names] <- apply(X = anadata[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanadata[numeric.names] <- apply(X = anadata[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\n\n\nTable 1\n\nShow the codelibrary(tableone)\ntab1 <- CreateTableOne(data = anadata, includeNA = TRUE, vars = vars)\nprint(tab1, showAllLevels = TRUE,  varLabels = TRUE)\n#>                          \n#>                           level              Overall       \n#>   n                                            1267        \n#>   cholesterol (mean (SD))                    193.10 (43.22)\n#>   gender (%)              Female                496 (39.1) \n#>                           Male                  771 (60.9) \n#>   born (%)                Born.in.US            991 (78.2) \n#>                           Others                276 (21.8) \n#>   race (%)                Black                 246 (19.4) \n#>                           Hispanic              337 (26.6) \n#>                           Other                 132 (10.4) \n#>                           White                 552 (43.6) \n#>   education (%)           College               648 (51.1) \n#>                           High.School           523 (41.3) \n#>                           School                 96 ( 7.6) \n#>   married (%)             Married               751 (59.3) \n#>                           Never.married         226 (17.8) \n#>                           Previously.married    290 (22.9) \n#>   income (%)              <25k                  344 (27.2) \n#>                           Between.25kto54k      435 (34.3) \n#>                           Between.55kto99k      297 (23.4) \n#>                           Over100k              191 (15.1) \n#>   bmi (mean (SD))                             29.58 (6.84) \n#>   diabetes (%)            No                   1064 (84.0) \n#>                           Yes                   203 (16.0)\n\n\nLinear regression when cholesterol is continuous\nFit a linear regression, and report the VIFs.\n\nShow the code#Fitting initial regression\n\nfit0 <- lm(cholesterol ~ gender + born + race + education +\n              married + income + bmi + diabetes,\n            data = anadata)\n\nlibrary(Publish)\npublish(fit0)\n#>     Variable              Units Coefficient           CI.95    p-value \n#>  (Intercept)                         198.90 [184.82;212.97]    < 1e-04 \n#>       gender             Female         Ref                            \n#>                            Male       -6.82  [-11.76;-1.89]   0.006854 \n#>         born         Born.in.US         Ref                            \n#>                          Others       15.65    [8.54;22.75]    < 1e-04 \n#>         race              Black         Ref                            \n#>                        Hispanic       -2.75   [-10.61;5.10]   0.492333 \n#>                           Other       -3.95   [-13.61;5.72]   0.423740 \n#>                           White        5.36   [-1.20;11.92]   0.109403 \n#>    education            College         Ref                            \n#>                     High.School        3.51    [-1.61;8.63]   0.179871 \n#>                          School        0.31   [-9.63;10.24]   0.951841 \n#>      married            Married         Ref                            \n#>                   Never.married      -11.05  [-17.67;-4.44]   0.001082 \n#>              Previously.married        4.72   [-1.43;10.86]   0.132468 \n#>       income               <25k         Ref                            \n#>                Between.25kto54k       -0.48    [-6.72;5.75]   0.879480 \n#>                Between.55kto99k        3.41   [-3.60;10.43]   0.340491 \n#>                        Over100k        2.24   [-6.02;10.51]   0.595131 \n#>          bmi                          -0.21    [-0.56;0.15]   0.257105 \n#>     diabetes                 No         Ref                            \n#>                             Yes      -10.61  [-17.21;-4.02]   0.001652\n\n#Checking VIFs\ncar::vif(fit0) \n#>               GVIF Df GVIF^(1/(2*Df))\n#> gender    1.065810  1        1.032381\n#> born      1.578258  1        1.256288\n#> race      1.684064  3        1.090753\n#> education 1.280113  2        1.063683\n#> married   1.225520  2        1.052156\n#> income    1.277005  3        1.041595\n#> bmi       1.086953  1        1.042570\n#> diabetes  1.073619  1        1.036156\n\n\nAll VIFs are small.\nTest of association when cholesterol is binary\nDichotomize the outcome such that cholesterol<200 is labeled as ‘healthy’; otherwise label it as ‘unhealthy’, and name it ‘cholesterol.bin’. Test the association between this binary variable and gender.\n\nShow the code#Creating binary variable for cholesterol\nanadata$cholesterol.bin <- ifelse(anadata$cholesterol <200, \"healthy\", \"unhealthy\")\n#If cholesterol is <200, then \"healthy\", if not, \"unhealthy\"\n\ntable(anadata$cholesterol.bin)\n#> \n#>   healthy unhealthy \n#>       738       529\nanadata$cholesterol.bin <- as.factor(anadata$cholesterol.bin)\nanadata$cholesterol.bin <- relevel(anadata$cholesterol.bin, ref = \"unhealthy\")\n\n\nTest of association between cholesterol and gender (no survey features)\n\nShow the code# Simple Chi-square testing\nchisq.chol.gen <- chisq.test(anadata$cholesterol.bin, anadata$gender)\nchisq.chol.gen\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  anadata$cholesterol.bin and anadata$gender\n#> X-squared = 5.1321, df = 1, p-value = 0.02349\n\n\nSetting up survey design\n\nShow the coderequire(survey)\nsummary(anadata$weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\nw.design <- svydesign(id = ~psu, weights = ~weight, strata = ~strata,\n                      nest = TRUE, data = anadata)\nsummary(weights(w.design))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    5470   19540   30335   48904   63822  224892\n\n\nTest of association accounting for survey design\n\nShow the code#Rao-Scott modifications (chi-sq)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#> X-squared = 11.092, df = 1, p-value = 0.02365\n\n#Thomas-Rao modifications (F)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\") \n#> \n#>  Pearson's X^2: Rao & Scott adjustment\n#> \n#> data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\")\n#> F = 5.1205, ndf = 1, ddf = 15, p-value = 0.03891\n\n\nAll three tests indicate strong evidence to reject the H0. There seems to be an association between gender and cholesterol level (healthy/unhealthy)\nTable 1\nCreate a Table 1 (summarizing the covariates) stratified by the binary outcome: cholesterol.bin, utilizing the above survey features.\n\nShow the code# Creating Table 1 stratified by binary outcome (cholesterol)\n# Using the survey features\n\nvars2 = c(\"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\n\n\nkableone <- function(x, ...) {\n  capture.output(x <- print(x, showAllLevels= TRUE, padColnames = TRUE, insertLevel = TRUE))\n  knitr::kable(x, ...)\n}\nkableone(svyCreateTableOne(var = vars2, strata= \"cholesterol.bin\", data=w.design, test = TRUE)) \n\n\n\n\n\n\n\n\n\n\n\n\nlevel\nunhealthy\nhealthy\np\ntest\n\n\n\nn\n\n27369732.3\n34591444.0\n\n\n\n\ngender (%)\nFemale\n13573865.5 (49.6)\n13917447.5 (40.2)\n0.039\n\n\n\n\nMale\n13795866.8 (50.4)\n20673996.5 (59.8)\n\n\n\n\nborn (%)\nBorn.in.US\n23772751.7 (86.9)\n31532673.3 (91.2)\n0.028\n\n\n\n\nOthers\n3596980.6 (13.1)\n3058770.7 ( 8.8)\n\n\n\n\nrace (%)\nBlack\n1832118.3 ( 6.7)\n3696893.4 (10.7)\n0.015\n\n\n\n\nHispanic\n3263992.3 (11.9)\n3921344.6 (11.3)\n\n\n\n\n\nOther\n1887156.6 ( 6.9)\n2601870.3 ( 7.5)\n\n\n\n\n\nWhite\n20386465.2 (74.5)\n24371335.7 (70.5)\n\n\n\n\neducation (%)\nCollege\n15855712.5 (57.9)\n20945710.7 (60.6)\n0.522\n\n\n\n\nHigh.School\n10615218.7 (38.8)\n12434827.2 (35.9)\n\n\n\n\n\nSchool\n898801.1 ( 3.3)\n1210906.1 ( 3.5)\n\n\n\n\nmarried (%)\nMarried\n17489306.2 (63.9)\n21170020.0 (61.2)\n0.005\n\n\n\n\nNever.married\n3086474.4 (11.3)\n7175237.2 (20.7)\n\n\n\n\n\nPreviously.married\n6793951.8 (24.8)\n6246186.8 (18.1)\n\n\n\n\nincome (%)\n<25k\n4760281.8 (17.4)\n6364208.6 (18.4)\n0.915\n\n\n\n\nBetween.25kto54k\n8682481.6 (31.7)\n10786198.6 (31.2)\n\n\n\n\n\nBetween.55kto99k\n6939847.0 (25.4)\n9190388.2 (26.6)\n\n\n\n\n\nOver100k\n6987121.9 (25.5)\n8250648.6 (23.9)\n\n\n\n\nbmi (mean (SD))\n\n29.35 (6.13)\n29.64 (7.05)\n0.593\n\n\n\ndiabetes (%)\nNo\n25080412.0 (91.6)\n30006523.6 (86.7)\n0.012\n\n\n\n\nYes\n2289320.3 ( 8.4)\n4584920.4 (13.3)\n\n\n\n\n\n\n\nLogistic regression model\nRun a logistic regression model using the same variables, utilizing the survey features. Report the corresponding odds ratios and the 95% confidence intervals.\n\nShow the codeformula1 <- as.formula(I(cholesterol.bin==\"unhealthy\") ~ gender + born +\n                         race + education + married + income + bmi +\n                         diabetes)\n\nfit1 <- svyglm(formula1,\n               design = w.design, \n               family = binomial(link = \"logit\"))\n\npublish(fit1)\n#>   Variable              Units OddsRatio       CI.95  p-value \n#>     gender             Female       Ref                      \n#>                          Male      0.70 [0.49;0.98]   0.2866 \n#>       born         Born.in.US       Ref                      \n#>                        Others      2.10 [1.41;3.13]   0.1707 \n#>       race              Black       Ref                      \n#>                      Hispanic      1.15 [0.80;1.67]   0.5871 \n#>                         Other      1.11 [0.69;1.80]   0.7406 \n#>                         White      1.46 [1.00;2.14]   0.3003 \n#>  education            College       Ref                      \n#>                   High.School      1.21 [0.96;1.52]   0.3563 \n#>                        School      0.86 [0.52;1.43]   0.6712 \n#>    married            Married       Ref                      \n#>                 Never.married      0.54 [0.32;0.90]   0.2526 \n#>            Previously.married      1.31 [0.92;1.87]   0.3704 \n#>     income               <25k       Ref                      \n#>              Between.25kto54k      1.03 [0.61;1.73]   0.9408 \n#>              Between.55kto99k      1.02 [0.66;1.56]   0.9525 \n#>                      Over100k      1.12 [0.73;1.72]   0.6920 \n#>        bmi                         1.00 [0.97;1.03]   0.9361 \n#>   diabetes                 No       Ref                      \n#>                           Yes      0.62 [0.41;0.95]   0.2720\n\n\nWald test (survey version)\nPerform a Wald test (survey version) to test the null hypothesis that all coefficients associated with the income variable are zero, and interpret.\n\nShow the code#Testing the H0 that all coefficients associated with the income variable are zero\nregTermTest(fit1, ~income)\n#> Wald test for income\n#>  in svyglm(formula = formula1, design = w.design, family = binomial(link = \"logit\"))\n#> F =  0.1050099  on  3  and  1  df: p= 0.94611\n\n\nThe Wald test here gives a large p-value; We do not have evidence to reject the H0 of coefficient being 0. If the coefficient for income variable is 0, this means that the outcome in the model (cholesterol) is not affected by income. This suggests that removing income from the model does not statistically improve the model fit. So we can remove income variable from the model.\nBackward elimination\nRun a backward elimination (using the AIC criteria) on the above logistic regression fit (keeping important variables gender, race, bmi, diabetes in the model), and report the odds ratios and the 95% confidence intervals from the resulting final logistic regression fit.\n\nShow the code#Running backward elimination based on AIC\nrequire(MASS)\nscope <- list(upper = ~ gender + born + race + education + \n                married + income + bmi + diabetes,\n              lower = ~ gender + race + bmi + diabetes)\n\nfit3 <- step(fit1, scope = scope, trace = FALSE,\n                k = 2, direction = \"backward\")\n\n#Odds Ratios\npublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\n\n\nBorn and married are also found to be useful on top of gender + race + bmi + diabetes.\nInteraction terms\nChecking interaction terms\n– gender and whether married\n– gender and whether born in the US\n– gender and diabetes\n– whether married and diabetes\n\nShow the code#gender and married\nfit4 <- update(fit3, .~. + interaction(gender, married))\nanova(fit3, fit4)\n#> Working (Rao-Scott+F) LRT for interaction(gender, married)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     married), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.7461308 p= 0.70903 \n#> (scale factors:  1.1 0.93 );  denominator df= 4\n\n\nDo not include interaction term\n\nShow the code#gender and born in us\nfit5 <- update(fit3, .~. + interaction(gender, born))\nanova(fit3, fit5)\n#> Working (Rao-Scott+F) LRT for interaction(gender, born)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     born), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.4635299 p= 0.52441 \n#> df=1;  denominator df= 5\n\n\nDo not include interaction term\n\nShow the code#gender and diabetes\nfit6 <- update(fit3, .~. + interaction(gender, diabetes))\nanova(fit3, fit6)\n#> Working (Rao-Scott+F) LRT for interaction(gender, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(gender, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  1.222596 p= 0.32211 \n#> df=1;  denominator df= 5\n\n\nDo not include interaction term\n\nShow the code#married and diabetes\nfit7 <- update(fit3, .~. + interaction(married, diabetes))\nanova(fit3, fit7)\n#> Working (Rao-Scott+F) LRT for interaction(married, diabetes)\n#>  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#>     born + race + married + bmi + diabetes + interaction(married, \n#>     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#> Working 2logLR =  0.3207507 p= 0.84547 \n#> (scale factors:  1.4 0.62 );  denominator df= 4\n\n\nDo not include interaction term\nNone of the interaction terms are improving the model fit.\nAUC\nReport AUC of the final model (only using weight argument) and interpret.\nAUC of the final model (only using weight argument) and interpret\n\nShow the coderequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw <- function(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight){\n  if (is.null(weight)){ # require(ROCR)\n    prob <- predict(fit, type = \"response\")\n  pred <- prediction(as.vector(prob), outcome)\n  perf <- performance(pred, \"tpr\", \"fpr\")\n  auc <- performance(pred, measure = \"auc\")\n  auc <- auc@y.values[[1]]\n  roc.data <- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { \n    outcome <- as.numeric(outcome)\n  pred <- predict(fit, type = \"response\")\n  tp.fp <- WeightedROC(pred, outcome, weight)\n  auc <- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\nsvyROCw(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight)\n\n\n\n\nThe area under the curve in the final model is 0.611, using the survey weighted ROC. The AUC of 0.611 indicates that this model has poor discrimination.\nArcher-Lemeshow Goodness of fit\nReport Archer-Lemeshow Goodness of fit test and interpret (utilizing all the survey features).\n\nShow the code#Archer-Lemeshow Goodness of fit test utilizing all survey features\nAL.gof2 <- function(fit = fit3, data = anadata, \n                   weight = \"weight\", psu = \"psu\", strata = \"strata\"){\n  r <- residuals(fit, type = \"response\") \n  f<-fitted(fit) \n  breaks.g <- c(-Inf, quantile(f, (1:9)/10), Inf)\n  breaks.g <- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g<- cut(f, breaks.g)\n  data2g <- cbind(data,r,g)\n  newdesign <- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                         data=data2g, nest = TRUE)\n  decilemodel <- svyglm(r~g, design=newdesign) \n  res <- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nAL.gof2(fit3, anadata, weight = \"weight\", psu = \"psu\", strata = \"strata\")\n#> Wald test for g\n#>  in svyglm(formula = r ~ g, design = newdesign)\n#> F =  0.7569326  on  9  and  6  df: p= 0.66075\n\n\nArcher and Lemeshow GoF test was used to test the fit of this model. The p-value of 0.3043, which is greater than 0.05. This means that there is no evidence of lack of fit to this model.\nAdd age as a predictor for linear regression\nFit another logistic regression (similar to Q1) with the above-mentioned predictors (as obtained in Q7) and age, utilizing the survey features. What difference do you see from the previous fit results?\n\nShow the codeaic.int.model <- eval(fit3$call[[2]])\naic.int.model\n#> I(cholesterol.bin == \"unhealthy\") ~ gender + born + race + married + \n#>     bmi + diabetes\n\nformula3 <- as.formula(cholesterol.bin ~ gender + born + race + married + bmi + diabetes + age)\nfit9 <- svyglm(formula3,\n               design = w.design,\n               family = binomial(link=\"logit\"))\nsummary(fit9)\n#> \n#> Call:\n#> svyglm(formula = formula3, design = w.design, family = binomial(link = \"logit\"))\n#> \n#> Survey design:\n#> svydesign(id = ~psu, weights = ~weight, strata = ~strata, nest = TRUE, \n#>     data = anadata)\n#> \n#> Coefficients:\n#>                             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)                1.0657919  0.6260889   1.702   0.1494  \n#> genderMale                 0.3821902  0.1703394   2.244   0.0749 .\n#> bornOthers                -0.6912102  0.2026407  -3.411   0.0190 *\n#> raceHispanic              -0.2442019  0.1823190  -1.339   0.2381  \n#> raceOther                 -0.1570271  0.2306208  -0.681   0.5262  \n#> raceWhite                 -0.3638735  0.2029676  -1.793   0.1330  \n#> marriedNever.married       0.4029107  0.2637962   1.527   0.1872  \n#> marriedPreviously.married -0.2096009  0.1620478  -1.293   0.2524  \n#> bmi                       -0.0002237  0.0134117  -0.017   0.9873  \n#> diabetesYes                0.6534019  0.2456333   2.660   0.0449 *\n#> age                       -0.0151364  0.0042038  -3.601   0.0155 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1.000456)\n#> \n#> Number of Fisher Scoring iterations: 4\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\n\n\nComparing with previous model fit\n\nShow the codepublish(fit3)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      0.71 [0.51;0.98]   0.08558 \n#>      born         Born.in.US       Ref                       \n#>                       Others      2.01 [1.37;2.96]   0.01184 \n#>      race              Black       Ref                       \n#>                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#>                        Other      1.11 [0.68;1.81]   0.69539 \n#>                        White      1.46 [0.99;2.17]   0.10469 \n#>   married            Married       Ref                       \n#>                Never.married      0.54 [0.32;0.90]   0.05770 \n#>           Previously.married      1.30 [0.93;1.80]   0.17125 \n#>       bmi                         1.00 [0.97;1.03]   0.95146 \n#>  diabetes                 No       Ref                       \n#>                          Yes      0.61 [0.40;0.91]   0.05445\npublish(fit9)\n#>  Variable              Units OddsRatio       CI.95   p-value \n#>    gender             Female       Ref                       \n#>                         Male      1.47 [1.05;2.05]   0.07487 \n#>      born         Born.in.US       Ref                       \n#>                       Others      0.50 [0.34;0.75]   0.01902 \n#>      race              Black       Ref                       \n#>                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#>                        Other      0.85 [0.54;1.34]   0.52619 \n#>                        White      0.69 [0.47;1.03]   0.13299 \n#>   married            Married       Ref                       \n#>                Never.married      1.50 [0.89;2.51]   0.18721 \n#>           Previously.married      0.81 [0.59;1.11]   0.25238 \n#>       bmi                         1.00 [0.97;1.03]   0.98734 \n#>  diabetes                 No       Ref                       \n#>                          Yes      1.92 [1.19;3.11]   0.04488 \n#>       age                         0.98 [0.98;0.99]   0.01553\nAIC(fit3)\n#>       eff.p         AIC    deltabar \n#>   11.121795 1706.792543    1.235755\nAIC(fit9)\n#>      eff.p        AIC   deltabar \n#>   12.14310 1694.15974    1.21431\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content."
  },
  {
    "objectID": "surveydata8.html",
    "href": "surveydata8.html",
    "title": "NHANES: Subsetting",
    "section": "",
    "text": "The tutorial demonstrates how to work with subset of complex survey data, specifically focusing on an NHANES example.\nThe required packages are loaded.\n\nShow the code# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\n\n\nLoad data\nSurvey data is loaded into the R environment.\n\nShow the codeload(\"Data/surveydata/NHANES17.RData\")\nls()\n#> [1] \"analytic\"           \"analytic.with.miss\"\n\n\nCheck missingness\nA subset of variables is selected, and the presence of missing data is visualized.\n\nShow the codeVars <- c(\"ID\", \n          \"weight\", \n          \"psu\", \n          \"strata\", \n          \"gender\", \n          \"born\", \n          \"race\", \n          \"bmi\", \n          \"cholesterol\", \n          \"diabetes\")\nanalytic.full.data <- analytic.with.miss[,Vars]\n\n\nA new variable is also created to categorize cholesterol levels as “healthy” or “unhealthy.”\n\nShow the codeanalytic.full.data$cholesterol.bin <- ifelse(analytic.full.data$cholesterol <200, \"healthy\", \"unhealthy\")\nanalytic.full.data$cholesterol <- NULL\n\nrequire(DataExplorer)\nplot_missing(analytic.full.data)\n\n\n\n\nSubsetting Complex Survey data\nWe are subsetting based on whether the subjects have missing observation (e.g., only retaining those with complete information). This is often an eligibility criteria in studies. In missing data analysis, we will learn more about more appropriate approaches.\n\nShow the codedim(analytic.full.data)\n#> [1] 9254   10\nhead(analytic.full.data$ID) # full data\n#> [1] 93703 93704 93705 93706 93707 93708\nanalytic.complete.case.only <- as.data.frame(na.omit(analytic.full.data))\ndim(analytic.complete.case.only)\n#> [1] 6636   10\nhead(analytic.complete.case.only$ID) # complete case\n#> [1] 93705 93706 93707 93708 93709 93711\nhead(analytic.full.data$ID[analytic.full.data$ID %in% analytic.complete.case.only$ID])\n#> [1] 93705 93706 93707 93708 93709 93711\n\n\nBelow we show how to identify who has missing observations vs not based on full (analytic.full.data) and complete case (analytic.complete.case.only) data. See Heeringa et al (2010) book page 114 (section 4.5.3 “Preparation for Subclass analyses”) and also page 218 (section 7.5.4 “appropriate analysis: incorporating all Sample Design Features”). This is done for 2 reasons:\n\nfull complex survey design structure is taken into account, so that variance estimation is done correctly. If one or more PSU were excluded because none of the complete cases were observed in those PSU, the sub-population (complete cases) will not have complete information of how many PSU were actually present in the original complex design. Then in the population, a reduced number of PSUs would be used to calculate variance (number of SPU is a component of the variance calculation formula, see equation (5.2) in Heeringa et al (2010) textbook. Same is true for strata.), and will result in a wrong/biased variance estimate. Also see West et al. doi: 10.1177/1536867X0800800404\nsize of sub-population (here, those with complete cases) is recognized as a random variable; not just a fixed size.\n\n\nShow the code# assign missing indicator\nanalytic.full.data$miss <- 1 \n# assign missing indicator = 0 if the observation is available\nanalytic.full.data$miss[analytic.full.data$ID %in% analytic.complete.case.only$ID] <- 0\n\n\n\nShow the codetable(analytic.full.data$miss)\n#> \n#>    0    1 \n#> 6636 2618\n# IDs not in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==1])\n#> [1] 93703 93704 93710 93720 93724 93725\n# IDs in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==0])\n#> [1] 93705 93706 93707 93708 93709 93711\n\n\nLogistic regression on sub-population\nA logistic regression model is run on the subset of data that has no missing values. Here, it distinguishes between correct and incorrect approaches to account for the complex survey design.\n\nShow the coderequire(survey)\nrequire(Publish)\nmodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\n\nWrong approach\n\nShow the codew.design.wrong <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.complete.case.only, #wrong!!\n                       nest = TRUE)\n\n\nCorrect approach\n\nShow the codew.design0 <- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.full.data, \n                       nest = TRUE)\n\n# retain only those that have complete observation / no missing\nw.design <- subset(w.design0, miss == 0)# this is the subset design\n\n\nFull model\n\nShow the codefit <- svyglm(model.formula, family = quasibinomial, \n              design = w.design) # subset design\npublish(fit)\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\n\nVariable selection\nFinally, we discuss variable selection methods. We employ backward elimination to determine which variables are significant predictors while retaining an important variable in the model. If unsure about usefulness of some (gender, born, race, bmi) variables in predicting the outcome, check via backward elimination while keeping important variable (diabetes, say, that has been established in the literature) in the model\n\nShow the codemodel.formula <- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nscope <- list(upper = ~ diabetes+gender+born+race+bmi, lower = ~ diabetes)\n\nfit <- svyglm(model.formula, design=w.design, # subset design\n              family=quasibinomial)\n\nfitstep <- step(fit,  scope = scope, trace = FALSE, direction = \"backward\")\npublish(fitstep) # final model\n#>  Variable                            Units Coefficient           CI.95     p-value \n#>  diabetes                               No         Ref                             \n#>                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#>    gender                           Female         Ref                             \n#>                                       Male        0.22     [0.03;0.40]   0.0568343 \n#>      born Born in 50 US states or Washingt         Ref                             \n#>                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#>                                    Refused      -12.26 [-13.65;-10.88]     < 1e-04 \n#>      race                            Black         Ref                             \n#>                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#>                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#>                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#>       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\n\nAlso see (Stata 2023) for further details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nStata. 2023. “How Can i Analyze a Subpopulation of My Survey Data in Stata?” https://stats.oarc.ucla.edu/stata/faq/how-can-i-analyze-a-subpopulation-of-my-survey-data-in-stata/."
  },
  {
    "objectID": "surveydataF.html",
    "href": "surveydataF.html",
    "title": "R functions (D)",
    "section": "",
    "text": "The list of new R functions introduced in this Survey data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n AIC \n    base/stats \n    To extract the AIC value of a model \n  \n\n as.character \n    base \n    To create a character vector \n  \n\n as.numeric \n    base \n    To create a numeric vector \n  \n\n eval \n    base \n    To evaluate an expression \n  \n\n fitted \n    base/stats \n    To extract fitted values of a model \n  \n\n ls \n    base \n    To see the list of objects \n  \n\n psrsq \n    survey \n    To compute the Nagelkerke and Cox-Snell pseudo R-squared statistics for survey data \n  \n\n regTermTest \n    survey \n    To test for an additional variable in a regression model \n  \n\n residuals \n    base/stats \n    To extract residuals of a model \n  \n\n stepAIC \n    MASS \n    To choose a model by stepwise AIC \n  \n\n step \n    base/stats \n    To choose a model by stepwise AIC but it can keep the pre-specified variables in the model \n  \n\n summ \n    jtools \n    To show/publish regression tables \n  \n\n svyboxplot \n    survey \n    To produce a box plot for survey data \n  \n\n svyby \n    survey \n    To see the summary statistics for a survey design \n  \n\n svychisq \n    survey \n    To test the bivariate assocaition between two categorical variables for survey data \n  \n\n svyCreateTableOne \n    tableone \n    To create a frequency table with a survey design \n  \n\n svydesign \n    survey \n    To create a design for the survey data analysis \n  \n\n svyglm \n    survey \n    To run design-adjusted generalized linear models \n  \n\n update \n    base/stats \n    To update and re-fit a regression model"
  },
  {
    "objectID": "surveydataQ.html",
    "href": "surveydataQ.html",
    "title": "Quiz (D)",
    "section": "",
    "text": "Downloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File > Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file."
  },
  {
    "objectID": "surveydataE.html#problem-statement",
    "href": "surveydataE.html#problem-statement",
    "title": "Exercise (D)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Flegal et al. (2021).\nWe will reproduce some results from the article. The authors used NHANES 2013-14 dataset to create their main analytic dataset. The dataset contains 10,175 subjects with 12 relevant variables:\n\nSEQN: Respondent sequence number\nRIDAGEYR: Age in years at screening\nRIAGENDR: Gender\nDMDEDUC2: Education level\nRIDRETH3: Race/ethnicity\nRIDEXPRG: Pregnancy status at exam\nWTINT2YR: Full sample 2 year weights\nSDMVPSU: Masked variance pseudo-PSU\nSDMVSTRA: Masked variance pseudo-stratum\nBMXBMI: Body mass index in kg/m**2\nSMQ020: Whether smoked at least 100 cigarettes in life\nSMQ040: Current status of smoking (Do you now smoke cigarettes?)"
  },
  {
    "objectID": "surveydataE.html#question-1-creating-data-and-table",
    "href": "surveydataE.html#question-1-creating-data-and-table",
    "title": "Exercise (D)",
    "section": "Question 1: Creating data and table",
    "text": "Question 1: Creating data and table\n1(a) Importing dataset\n\nShow the code# you have to download the data in the same folder\nload(\"Data/surveydata/Flegal2016.RData\")\nls()\n#> [1] \"dat.full\"\nnames(dat.full)\n#>  [1] \"SEQN\"     \"RIDAGEYR\" \"RIAGENDR\" \"DMDEDUC2\" \"RIDRETH3\" \"RIDEXPRG\"\n#>  [7] \"WTINT2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BMXBMI\"   \"SMQ020\"   \"SMQ040\"\n\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria described in the second paragraph of the Methods section.\n\nHint: The authors restricted their study to\n\nadults aged 20 years and more,\nnon-missing body mass index, and\nnon-pregnant.\n\n\n\nYour analytic sample size should be 5,455, as described in the first sentence in the Results section.\n\nShow the code# 20+\ndat.analytic <- subset(dat.full, RIDAGEYR>=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic <- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ndat.analytic <- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\n\ndim(dat.analytic)\n#> [1] 5455   12\n\n\n1(c) Reproduce Table 1\nReproduce Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Please be advised to order the categories as shown in the table. tableone package could be helpful.\nHint 2: the authors did not show the results for the Other race category. But in your table, you could include all race categories.\n\n\nShow the codelibrary(tableone)\n\ndat <- dat.analytic\n\n# Age\ndat$age <- cut(dat$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat$gender <- dat$RIAGENDR\n\n# Race/Hispanic origin group\ndat$race <- dat$RIDRETH3\ndat$race <- car::recode(dat$race, \" 'Non-Hispanic White'='White'; 'Non-Hispanic Black'=\n                        'Black'; 'Non-Hispanic Asian'='Asian'; c('Mexican American',\n                        'Other Hispanic')='Hispanic'; 'Other Race - Including Multi-Rac'=\n                        'Other'; else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                      \"Hispanic\", \"Other\"))\n\n# Table 1: Overall \ntab11 <- CreateTableOne(vars = \"age\", strata = \"race\", data = dat, test = F, \n                        addOverall = T)\n\n# Table 1: Male\ntab12 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(dat, gender == \"Male\"))\n\n# Table 1: Female\ntab13 <- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(dat, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1a <- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1a, format = \"f\") # Showing only frequencies \n#> $Overall\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           5455    2343  1115  623   1214     160  \n#>   age                                                 \n#>      [20,40)  1810     734   362  216    412      86  \n#>      [40,60)  1896     759   383  251    449      54  \n#>      [60,Inf) 1749     850   370  156    353      20  \n#> \n#> $Male\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2638    1130  556   300   573      79   \n#>   age                                                 \n#>      [20,40)   909     386  182   106   189      46   \n#>      [40,60)   897     360  179   120   215      23   \n#>      [60,Inf)  832     384  195    74   169      10   \n#> \n#> $Female\n#>              Stratified by race\n#>               Overall White Black Asian Hispanic Other\n#>   n           2817    1213  559   323   641      81   \n#>   age                                                 \n#>      [20,40)   901     348  180   110   223      40   \n#>      [40,60)   999     399  204   131   234      31   \n#>      [60,Inf)  917     466  175    82   184      10"
  },
  {
    "objectID": "surveydataE.html#question-2",
    "href": "surveydataE.html#question-2",
    "title": "Exercise (D)",
    "section": "Question 2",
    "text": "Question 2\n2(a) Reproduce Table 1 with survey features [15% grade]\nNot in this article but in many other articles, you would see n comes from the analytic sample and % comes from the survey design that accounts for survey features such as strata, clusters and survey weights. In Question 1, you see how n comes from the analytic sample. Your task for Question 2(a) is to create % part of the Table 1 with survey features, i.e., % should come from the survey design that accounts for strata, clusters and survey weights.\n\nHint 1: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 2: Generate age, gender, and race variable in your full data (codes shown in Question 1 could be helpful).\nHint 3: Subset the design.\nHint 4: Reproduce Table 1 with the design. svyCreateTableOne could be a helpful function.\n\n\nShow the code## Create all variables in the full data\n# Age\ndat.full$age <- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat.full$gender <- dat.full$RIAGENDR\n\n# Race/Hispanic origin group\ndat.full$race <- dat.full$RIDRETH3\ndat.full$race <- car::recode(dat.full$race, \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black'='Black'; 'Non-Hispanic Asian'='Asian'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             'Other Race - Including Multi-Rac'='Other'; \n                             else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                  \"Hispanic\", \"Other\"))\n\n## Subset the design\n# your codes here\n\n\n## Table 1 \n# your codes here\n\n#print(tab1b, format = \"p\") # Showing only percentages  \n\n\n2(b) Reproduce Table 3 [50% grade]\nReproduce the first column of Table 3 of the article (i.e., among men, explore the relationship between obesity and four predictors shown in the table).\n\nHint 1: If necessary, re-level or re-order the levels. Use Publish package to report the estimates.\nHint 2: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 3: The authors used SAS to produce the results vs. We are using R. The estimates could be slightly different (in second decimal point) from the estimates presented in Table 3, but they should be approximately similar.\nHint 4: You need to generate two variables, smoking status and education. The unweighted frequencies should be matched with the frequencies in eTable 1 and eTable 2.\n\nYour odds ratios could be look like as follows:\n\n\n\n\n\n\nShow the code## Recode Obese, Smoking status, Education - work on full data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Reproduce Table 3 - column 1\n# your codes here\n\n\n2(c) Model selection [25% grade]\nFrom the literature, you know that age and race needs to be adjusted in the model, but you are not sure about smoking and education. Run an AIC based backward selection process to figure out whether you want to add smoking or education, or both in the final model in 2(b). What is your conclusion [Expected answer: one short sentence]?\n\nHint 1: You need to make sure your design (that is based on eligibility) is free from missing values. Even after applying eligibility criteria, you may have some missing values on multiple variables (see eTable 1 and eTable 2). This is especially important for model selection process.\nHint 2: Work with the analytic data, keep only the relevant variables, and then remove missing values. Finally, subset the design and then select your final model.\n\n\nShow the code## Recode Obese, Smoking status, Education - work on analytic data\n# your codes here\n\n\n## Remove missing values - work on analytic data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Model selection \n# your codes here\n\n\n2(d) Testing for interactions [10% grade]\nCheck whether the interaction between age and smoking should be added in the 2(b) model (yes or no answer required, along with the code and p-value):\n\nShow the code# your codes here"
  },
  {
    "objectID": "missingdata.html",
    "href": "missingdata.html",
    "title": "Missing data analysis",
    "section": "",
    "text": "Danger\n\n\n\nThis tutorial is under construction! We are working hard to make this the best tutorial it can be. Thank you for your patience.\n\n\nThis chapter is going to be about:\n\nmissing data and imputation\nmultiple imputation in complex survey data\nmultiple imputation then deletion (MID)\nestimating model performance from multiple imputed datasets\ndealing with subpopulations with missing observations\ntesting MCAR assumption empirically in the data\neffect modification within multiple imputation"
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R functions (M)",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n aggr \n    VIM \n    To calculate/plot the missing values in the variables \n  \n\n boxplot \n    base/graphics \n    To produce a box plot \n  \n\n bwplot \n    mice \n    To produce box plot to compare the imputed and observed values \n  \n\n colMeans \n    base \n    To compute the column-wise mean, i.e., mean for each variable/column \n  \n\n complete \n    mice \n    To extract the imputed dataset \n  \n\n complete.cases \n    base/stats \n    To select the complete cases, i.e., observations without missing values \n  \n\n D1 \n    mice \n    To conduct the multivariate Wald test with D1-statistic \n  \n\n densityplot \n    mice \n    To produce desnsity plots \n  \n\n expression \n    base \n    To set/create an expression \n  \n\n imputationList \n    mice \n    To combine multiple imputed datasets \n  \n\n marginplot \n    VIM \n    To draw a scatterplot with additional information when there are missing values \n  \n\n mcar_test \n    naniar \n    To conduct Little's MCAR test \n  \n\n md.pattern \n    mice \n    To see the pattern of the missing data \n  \n\n mice \n    mice \n    To impute missing data where the argument m represents the number of multiple imputation \n  \n\n MIcombine \n    mitools \n    To combine/pool the results using Rubin's rule \n  \n\n MIextract \n    mitools \n    To extract parameters from a list of outputs \n  \n\n na.test \n    misty \n    To conduct Little's MCAR test \n  \n\n parlmice \n    mice \n    To run `mice` function in parallel, i.e., parallel computing of mice \n  \n\n plot_missing \n    DataExplorer \n    To plot the profile of missing values, e.g., the percentage of missing per variable \n  \n\n pool \n    mice \n    To pool the results using Rubin's rule \n  \n\n pool.compare \n    mice \n    To compare two nested models \n  \n\n pool_mi \n    miceadds \n    To combine/pool the results using Rubin's rule \n  \n\n quickpred \n    mice \n    To set imputation model based on the correlation \n  \n\n sim_slopes \n    interactions \n    To perform simple slope analyses \n  \n\n TestMCARNormality \n    MissMech \n    To test multivariate normality and homoscedasticity in the context of missing data \n  \n\n unlist \n    base \n    To convert a list to a vector"
  },
  {
    "objectID": "missingdataE.html#problem-statement",
    "href": "missingdataE.html#problem-statement",
    "title": "Exercise (M)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n\nid: Respondent sequence number\nsurvey.weight: Full sample 4 year interview weight\npsu: Masked pseudo PSU\nstrata: Masked pseudo strata (strata is nested within PSU)\n\n4 Outcome variables\n\nweight.loss.behavior: doing lifestyle behavior changes - controlling or losing weight\nexercise.behavior: doing lifestyle behavior changes - increasing exercise\nsalt.behavior: doing lifestyle behavior changes - reducing salt in diet\nfat.behavior: doing lifestyle behavior changes - reducing fat in diet\n\n4 predictors (i.e., exposure variables)\n\nweight.loss.advice: told by a doctor or health professional - to control/lose weight\nexercise.advice: told by a doctor or health professional - to exercise\nsalt.advice: told by a doctor or health professional - to reduce salt in diet\nfat.advice: told by a doctor or health professional - to reduce fat/calories\n\nConfounders and other variables\n\ngender: Gender\nage: Age in years at screening\nincome: The ratio of family income to federal poverty level\nrace: Race/Ethnicity\nbmi: Body Mass Index in kg/m\\(^2\\)\n\ncomorbidity: Comorbidity index\nDIQ010: Self-report to have been informed by a provider to have diabetes\nBPQ020: Self-report to have been informed by a provider to have hypertension"
  },
  {
    "objectID": "missingdataE.html#question-1-analytic-dataset",
    "href": "missingdataE.html#question-1-analytic-dataset",
    "title": "Exercise (M)",
    "section": "Question 1: Analytic dataset",
    "text": "Question 1: Analytic dataset\n1(a) Importing dataset\n\nShow the code# download the data in the same folder\nload(\"Data/missingdata/Williams2021.RData\")\n\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\nShow the code# Drop < 18 years\ndat <- dat.full\ndat <- dat[dat$age >= 18,] \n\n# Eligibility\ndat <- dat[dat$DIQ010==\"Yes\" | dat$BPQ020==\"Yes\",] \n\n# Dataset with missing values in outcomes, predictors, and confounders\ndat.with.miss <- dat\nnrow(dat.with.miss) # N = 4,746\n#> [1] 4746\n\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the “Self-reported behavior change and receipt of advice” paragraph.\n\n\nShow the codedat <- dat.with.miss\n\n# Drop missing or don't know outcomes \ndat <- dat[complete.cases(dat$weight.loss.behavior),]\ndat <- dat[complete.cases(dat$exercise.behavior),]\ndat <- dat[complete.cases(dat$salt.behavior),]\ndat <- dat[complete.cases(dat$fat.behavior),]\n\n# Drop missing or don't know predictors\ndat <- dat[complete.cases(dat$weight.loss.advice),]\ndat <- dat[complete.cases(dat$exercise.advice),]\ndat <- dat[complete.cases(dat$salt.advice),]\ndat <- dat[complete.cases(dat$fat.advice),] \n\n# Dataset without missing in outcomes and predictors but missing in confounders \ndat.with.miss2 <- dat\nnrow(dat.with.miss2) # N = 4,716\n#> [1] 4716\n\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nHint 2: You may need to generate the Condition variable.\nHint 3: age and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\nShow the codedat <- dat.with.miss2\n\n# Create the condition variable\ndat$condition <- NA\ndat$condition[dat$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat$condition[dat$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat$condition[dat$BPQ020 == \"Yes\" & dat$DIQ010 == \"Yes\"] <- \"Both\"\ndat$condition <- factor(dat$condition, levels=c(\"Hypertension Only\", \"Diabetes Only\",\n                                                \"Both\"))\ntable(dat$condition, useNA = \"always\")\n#> \n#> Hypertension Only     Diabetes Only              Both              <NA> \n#>              3004               533              1179                 0\n\n\n\nShow the code# First column of Table 1\nvars <- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 <- CreateTableOne(vars = vars, data = dat, includeNA = F)\nprint(tab1, format = \"f\")\n#>                          \n#>                           Overall      \n#>   n                        4716        \n#>   gender = Male            2332        \n#>   age (mean (SD))         59.94 (14.96)\n#>   income                               \n#>      <100%                  881        \n#>      100-199%              1193        \n#>      200-299%               672        \n#>      300-399%               424        \n#>      400+%                  930        \n#>   race                                 \n#>      Hispanic              1161        \n#>      Non-Hispanic white    1630        \n#>      Non-Hispanic black    1239        \n#>      Others                 686        \n#>   bmi                                  \n#>      Reference              753        \n#>      Overweight            1372        \n#>      Obese                 2287        \n#>   condition                            \n#>      Hypertension Only     3004        \n#>      Diabetes Only          533        \n#>      Both                  1179        \n#>   comorbidity (mean (SD))  1.29 (1.45)"
  },
  {
    "objectID": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "href": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "title": "Exercise (M)",
    "section": "Question 2: Dealing with missing values in confoudners [100% grade]",
    "text": "Question 2: Dealing with missing values in confoudners [100% grade]\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nHint 1: There are four outcome variables and four predictor variables used in the study.\nHint 2: The authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\nShow the code# Create the condition variable in the analytic dataset\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\"] <- \"Hypertension Only\"\ndat.with.miss2$condition[dat.with.miss2$DIQ010 == \"Yes\"] <- \"Diabetes Only\"\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\" & \n                          dat.with.miss2$DIQ010 == \"Yes\"] <- \"Both\"\ndat.with.miss2$condition <- factor(dat.with.miss2$condition, \n                                  levels=c(\"Hypertension Only\", \"Diabetes Only\", \"Both\"))\n\n# Variables of interest\nvars <- c(\n  # Outcome\n  \"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\",\n  \n  #Predictors\n  \"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\",\n   \n  # Confounders       \n  \"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\n\n# Plot missing values using DataExplorer\nplot_missing(dat.with.miss2[,vars])\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nPerform multiple imputations to deal with missing values only in confounders. Use the dataset created in Dataset with missing values only in confounders (dat.with.miss2). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to lose weights, i.e., create only the first column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types. lapply function could be helpful.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 5: Set your seed to 123.\nHint 6: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 7: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 8: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\nShow the code## Setup the data such that the variables are of appropriate types\nfactor.names <- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \n                  \"fat.behavior\", \"weight.loss.advice\", \"exercise.advice\", \n                  \"salt.advice\", \"fat.advice\", \"gender\", \"income\", \"race\", \"bmi\", \n                  \"condition\")\n# your codes \n\n\n## Change the reference categories\n# your codes\n\n\n## Imputation model set up\n# your codes\n\n\n## Regression analysis\n# your codes\n\n\n## Pooled estimates\n# your codes"
  },
  {
    "objectID": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "href": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "title": "Exercise (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Include all 4 outcomes and 4 predictors in your imputation model.\nHint 5: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 6: Set your seed to 123.\nHint 7: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 8: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 9: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\nShow the code## Create a missing indicator so that MID can be applied\n# your codes here\n\n## MID\n# your codes here"
  },
  {
    "objectID": "propensityscore.html",
    "href": "propensityscore.html",
    "title": "Propensity score",
    "section": "",
    "text": "Danger\n\n\n\nThis tutorial is under construction! We are working hard to make this the best tutorial it can be. Thank you for your patience.\n\n\nThis chapter is going to be about:\n\nCovariate matching using CCHS\nPropensity score matching using CCHS\nPropensity score matching using NHANES\nPropensity score matching using NHANES when some variables have missing observations"
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R functions (S)",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n bal.plot \n    cobalt \n    To produce a overalp/balance plot for propensity scoes \n  \n\n bal.tab \n    cobalt \n    To check the balance at each category of covariates \n  \n\n CreateCatTable \n    tableone \n    To create a frequency table with categorical variables only \n  \n\n do.call \n    base \n    To execute a function call \n  \n\n love.plot \n    cobalt \n    To plot the standardized mean differences at each category of covariates \n  \n\n match.data \n    MatchIt \n    To extract the matched dataste from a matchit object \n  \n\n matchit \n    MatchIt \n    To match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m. \n  \n\n rownames \n    base \n    Names of the rows"
  },
  {
    "objectID": "propensityscoreE.html#problem-statement",
    "href": "propensityscoreE.html#problem-statement",
    "title": "Exercise (S)",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe will use the article by Moon et al. (2021):\nWe will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\n\nSEQN: Respondent sequence number\nstrata: Masked pseudo strata (strata is nested within PSU)\npsu: Masked pseudo PSU\nsurvey.weight: Full sample 8 year interview weight divided by 4\nsurvey.cycle: NHANES cycle\n\nOutcome variable\n\ncvd: Cardiovascular disease\n\nExposure\n\nnocturia: Binary nocturia\n\nConfounders and other variables\n\nage: Age in years at screening\ngender: Gender\nrace: Race/Ethnicity\nsmoking: 100+ cigarettes in life\nalcohol: Alcohol consumption (12+ drinks in 1 year)\nsleep: Sleep duration, h\nbmi: Body Mass Index in kg/m\\(^2\\)\n\nsystolic: Systolic blood pressure, mmHg\ndiastolic: Diastolic blood pressure, mmHg\ntcholesterol: Total cholesterol, mg/dl\ntriglycerides: Triglycerides, mg/dl\nhdl: HDL‐cholesterol, mg/dl\ndiabetes: Diabetes mellitus\nhypertension: Hypertension\n\nTwo important warnings before we start:\n\nIn this paper, there is insufficient information to create the analytic dataset. This is mainly because of not sufficiently defining the covariates and not explicitly explaining the inclusion/exclusion criteria.\nThe authors did incorrect analyses. For example, they didn’t consider survey features. Since we will utilize survey features in our analysis, our results will likely be different than the results shown by the authors in Table 2."
  },
  {
    "objectID": "propensityscoreE.html#question-1-0-grade",
    "href": "propensityscoreE.html#question-1-0-grade",
    "title": "Exercise (S)",
    "section": "Question 1: [0% grade]",
    "text": "Question 1: [0% grade]\n1(a) Importing dataset\n\nShow the codeload(file = \"Data/propensityscore/Moon2021.RData\")\n\n\n1(b) Subsetting according to eligibility\n\nShow the code# Age 20+\ndat.analytic <- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic <- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic <- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars <- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic <- dat.analytic[,vars]\n\n# Complete case\ndat.analytic <- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#> [1] 15404    21\n\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, hypertension, diabetes mellitus, and survey cycles.\n\nHint 1: the authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nHint 2: Adjust the model for age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, and survey cycle.\nHint 3: Use Publish package to report the odds ratio with the 95% CI and p-value.\n\n\nShow the code# Create an indicator variable in the full data\ndat.full$miss <- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] <- 0\n\n# Design setup\nsvy.design0 <- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design <- subset(svy.design0, miss == 0)\n\n# Design-adjusted logistic\nfit.logit <- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#>       Variable              Units OddsRatio         CI.95     p-value \n#>       nocturia                 <2       Ref                           \n#>                                2+      1.44   [1.21;1.71]   0.0001496 \n#>            age            [20,40)       Ref                           \n#>                           [40,60)      4.21   [3.05;5.82]     < 1e-04 \n#>                           [60,80)     11.46  [7.89;16.64]     < 1e-04 \n#>                          [80,Inf)     25.28 [17.51;36.50]     < 1e-04 \n#>         gender               Male       Ref                           \n#>                            Female      0.68   [0.58;0.79]     < 1e-04 \n#>           race          Hispanics       Ref                           \n#>                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#>                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#>                       Other races      1.55   [1.05;2.30]   0.0319116 \n#>            bmi                         1.02   [1.01;1.03]   0.0003273 \n#>        smoking                 No       Ref                           \n#>                               Yes      1.74   [1.46;2.07]     < 1e-04 \n#>        alcohol                 No       Ref                           \n#>                               Yes      0.92   [0.59;1.45]   0.7273627 \n#>          sleep                         0.96   [0.90;1.01]   0.1146287 \n#>   tcholesterol                         0.99   [0.99;0.99]     < 1e-04 \n#>  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#>            hdl                         0.99   [0.98;1.00]   0.0416900 \n#>   hypertension                 No       Ref                           \n#>                               Yes      2.73   [2.27;3.29]     < 1e-04 \n#>       diabetes                 No       Ref                           \n#>                               Yes      1.83   [1.51;2.22]     < 1e-04 \n#>   survey.cycle            2005-06       Ref                           \n#>                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#>                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#>                           2011-11      0.82   [0.68;0.99]   0.0398975"
  },
  {
    "objectID": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "href": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "title": "Exercise (S)",
    "section": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]",
    "text": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Question 1) using the propensity score 1:1 matching analysis as per DuGoff et al. (2014) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia <2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare your results with the results reported by the authors. [Expected answer: 2-3 sentences]\n\n\nShow the code# your codes here"
  },
  {
    "objectID": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "href": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "title": "Exercise (S)",
    "section": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]",
    "text": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Questions 1 and 2) using the propensity score 1:4 matching analysis as per Austin et al. (2018) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia <2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD <0.1 as a good covariate balancing. Remember, you need to multiply matching weights and survey weights to get survey-based estimates.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare the results with Question 2. What’s the overall conclusion? [Expected answer: 2-3 sentences]\n\n\nShow the code# your codes here"
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "Danger\n\n\n\nThis tutorial is under construction! We are working hard to make this the best tutorial it can be. Thank you for your patience.\n\n\nThis chapter is going to be about:\n\nanalysis reporting guidelines"
  },
  {
    "objectID": "reportingF.html",
    "href": "reportingF.html",
    "title": "R functions (G)",
    "section": "",
    "text": "The list of new R functions introduced in this reporting lab component are below:"
  },
  {
    "objectID": "machinelearning.html#key-references",
    "href": "machinelearning.html#key-references",
    "title": "Machine learning",
    "section": "Key References",
    "text": "Key References\n\nWatch the reference video \n(Bi et al. 2019)\n(Liu et al. 2019)\n(Kuhn et al. 2013)"
  },
  {
    "objectID": "machinelearning.html#additional-useful-references",
    "href": "machinelearning.html#additional-useful-references",
    "title": "Machine learning",
    "section": "Additional useful references",
    "text": "Additional useful references\n\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)\n\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. “What Is Machine Learning? A Primer for the Epidemiologist.” American Journal of Epidemiology 188 (12): 2222–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. “Over-Fitting and Model Tuning.” Applied Predictive Modeling, 61–92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. “How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature.” Jama 322 (18): 1806–16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. “Predictor Selection.” Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395–429."
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R functions (L)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\n Function_name \n    Package_name \n    Use \n  \n\n\n fancyRpartPlot \n    rattle \n    To plot an rpart object \n  \n\n fviz_nbclust \n    factoextra \n    To visualize the optimal number of clusters \n  \n\n kmeans \n    base/stats \n    To conduct K-Means cluster analysis \n  \n\n lowess \n    base/stats \n    To perform scatter plot smoothing aka lowess smoothing \n  \n\n rpart \n    rpart \n    To fit a classification tree (CART) \n  \n\n terms \n    base/stats \n    To extarct terms objects \n  \n\n varImp \n    caret \n    To calculate the variable importance measure"
  },
  {
    "objectID": "machinelearningCausal.html",
    "href": "machinelearningCausal.html",
    "title": "ML in causal inference",
    "section": "",
    "text": "Danger\n\n\n\nThis tutorial is under construction! We are working hard to make this the best tutorial it can be. Thank you for your patience.\n\n\nThis chapter is going to be about:\n\nMachine learning and their use in causal inference such as propensity score modelling\nTMLE in medical research"
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R functions (C)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\n Function.name \n    Package.name \n    Use \n  \n\n\n ExtractSmd \n    tableone \n    To extract the standardized mean differences of a tableone object \n  \n\n listWrappers \n    SuperLearner \n    To see the list of wrapper functions, i.e., list of learners, in SuperLearner \n  \n\n Match \n    Matching \n    To match an exposed/treated to M unexposed/controls"
  }
]