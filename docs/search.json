[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Epidemiological Methods",
    "section": "",
    "text": "The Project\nWelcome to a website designed to bridge a unique gap in the health research world. This platform specifically targets the complex intersection of health research and advanced statistics; a niche often perceived as challenging by many newcomers. Whether you’re taking your first steps into health research or you’re grappling with the intricacies of advanced statistical methods, you are in the right place. Here, we offer:\nThis hub is a part of an open educational initiative, meaning it is available to everyone. We hope to raise the standard of health research methodology through this endeavor.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#what-we-aim-to-achieve",
    "href": "index.html#what-we-aim-to-achieve",
    "title": "Advanced Epidemiological Methods",
    "section": "What We Aim to Achieve",
    "text": "What We Aim to Achieve\nWe are on a mission to:\n\nEquip public health learners with hands-on experience.\nTeach the nuances of applying advanced epidemiological methods using real data.\nOffer a unique open textbook that’s enriched with interactive tools and quizzes for a self-paced learning experience.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#dive-into-our-modules",
    "href": "index.html#dive-into-our-modules",
    "title": "Advanced Epidemiological Methods",
    "section": "Dive into Our Modules",
    "text": "Dive into Our Modules\nEmbark on a journey through\n\n1 introductory module about R (indicator W),\n10 core learning modules (letters in the parentheses: A, E, Q, R, P, D, M, S, L, C are the chapter indicators), and\n5 bonus modules, with U, N, T, I, G being bonus chapter indicators.\n\nIndicators are listed along with quizzes, R functions, and exercises associated with the corresponding chapters. Only key chapters have exercises.\n\n\n\n\nModule\nTopics.Indicators\nDescriptions\n\n\n\n1\nR for Data Wrangling (W)\nGet to know R.\n\n\n2\nAccessing (A) Survey Data Resources\nUnderstand and source reliable national survey data.\n\n\n3\nExploratory (E) Data Analysis\nData Summarization and Visualization\n\n\n4\nCrafting Analytic Data for Research Questions (Q)\nCustomize data to your research query.\n\n\n5\nSimulation (U)\nUnderstand the role of simulation in evaluating statistical estimators.\n\n\n6\nCausal Roles (R)\nDelve into the concept of confounding and its implications.\n\n\n7\nPredictive (P) modeling\nIntroduction to key concepts of prediction modelling.\n\n\n8\nComplex Survey Data (D) Analysis\nHandle data sets obtained from complex survey designs.\n\n\n9\nMissing (M) Data Analysis\nUnderstand and tackle missingness in your data.\n\n\n10\nPropensity Score (S) Analysis\nDive deeper into advanced observational data analyses.\n\n\n11\nMachine Learning (L)\nIntroduction to machine learning algorithms, and applications.\n\n\n12\nIntergrating Machine Learners in Causal (C) Inference\nDiscusses the potential pitfalls and challenges in merging machine learning with causal inference, and a way forward.\n\n\n13\nNon-binary Outcomes (N)\nStatistical techniques to deal with complex or non-binary outcomes\n\n\n14\nLongitudinal Analysis (T)\nLongitudinal data analysis techniques\n\n\n15\nMediation Analysis (I)\nMediation: decomposing the total effect\n\n\n16\nScientific Writing Tools (G)\nTools and guides for scientific writing and collaboration.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tutorial is designed with a consistent structure across all chapters to provide a cohesive and thorough learning experience. Here is what you can expect in each chapter:\n\nOverview: The first page of each chapter offers a concise summary that outlines the key learning objectives, topics covered, and what you can expect to gain from the chapter. The overview page will also feature links to the data sources used in the tutorials as well as a form where you can report any bugs or issues you encounter. This helps you quickly grasp the chapter’s essence and set learning expectations.\nConcepts: Selected core chapters will include a concept page, where materials (e.g., slides, video lessons, additional FAQs where available) will be included. All of the videos linked here (for lessons or labs) are hosted in YouTube, where users can automatically generate subtitles and captions.\nTutorial topics: Immediately following the overview/concepts, you will find in-depth tutorials that cover each topic in detail. These are designed to provide comprehensive insights and are spread across multiple pages for easier navigation and understanding.\nSummary of R functions: Each chapter includes a succinct summary of the R functions used in the tutorials. This serves as a quick reference guide for learners to understand the tools they will be applying.\nChapter-specific quiz: For those interested in self-assessment, each chapter concludes with an optional quiz. This is a self-paced learning tool to help reinforce the chapter’s key concepts.\nWeb-App: A few chapters include shiny apps. Users can work with these apps directly from this website, or will have the option to download and run the app locally.\nPractice exercises: Finally, practice exercises are available for selected chapters to help you apply what you have learned in a hands-on manner. These exercises are designed to reinforce your understanding and give you practical experience with the chapter’s topics. Some of these practice exercises may be used in future versions of the course, so you may see references to submitting assignments or the points value of a question in an assignment.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#how-our-content-is-presented",
    "href": "index.html#how-our-content-is-presented",
    "title": "Advanced Epidemiological Methods",
    "section": "How Our Content is Presented",
    "text": "How Our Content is Presented\nAll our resources are hosted on an easy-to-access GitHub page. The format? Engaging text, reproducible software codes, clear analysis outputs, and crisp videos that distill complex topics. And do not miss our quiz section at the end of each module for a quick self-check on what you have learned. This document is created using quarto and R.\nThe content was primarily designed for a website in HTML format; however, a PDF version based on the website has also been created. Although the formatting is not perfect for this converted PDF, this PDF can be downloaded from here and used for offline reading.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#open-copyright-license",
    "href": "index.html#open-copyright-license",
    "title": "Advanced Epidemiological Methods",
    "section": "Open Copyright License",
    "text": "Open Copyright License\nCC-BY 4.0",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#grant-applicants",
    "href": "index.html#grant-applicants",
    "title": "Advanced Epidemiological Methods",
    "section": "Grant Applicants",
    "text": "Grant Applicants\nDive into this captivating content, brought to life with the generous support of the UBC OER Fund Implementation Grant and further supported by UBC SPPH. The foundation of this content traces back to the PI’s work over five years while instructing SPPH 604 (2018-2022). That knowledge has now been transformed into an open educational resource, thanks to this grant. Meet the innovative minds behind the grant proposal below.\n\n\n\n\nRole\nTeam_Member\nAffiliation\n\n\n\nPrincipal Applicant (PI)\nDr. M Ehsan Karim\nUBC School of Population and Public Health\n\n\nCo-applicant (Co-I)\nDr. Suborna Ahmed\nUBC Department of Forest Resources Management\n\n\nTrainee co-applicants\nMd Belal Hossain\nUBC School of Population and Public Health\n\n\n\nFardowsa Yusuf\nUBC School of Population and Public Health\n\n\n\nHanna Frank\nUBC School of Population and Public Health\n\n\n\nDr. Michael Asamoah-Boaheng\nUBC Department of Emergency Medicine\n\n\n\nChuyi (Astra) Zheng\nUBC Faculty of Arts\n\n\n\n\n\nA presentation about the output of this grant in the OER Project Virtual Showcase and Poster Session (March 7, 2024):\n\nToggle Show/Hide\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe PI was selected as a recipient of the AMS UBC OER Champion Award 2024–2025 in recognition of their outstanding commitment to equitable, inclusive, and innovative education—particularly through leadership in advancing affordable access via Open Educational Resources (OERs).",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced Epidemiological Methods",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe also want to acknowledge contributors to the course material development, who were not part of this current OER grant, include Sebastian Santana Ortiz (particularly the Writing tools chapter), Pierce Gorun (particularly the Simulation chapter), Esteban Valencia, Derek Ouyang, Kate McLeod (from UBC School of Population and Public Health), Kamila Romanowski (from Experimental Medicine) and Mohammad Atiquzzaman (UBC Pharmaceutical Sciences). Numerous pieces of student feedback were also incorporated in order to update the content.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Advanced Epidemiological Methods",
    "section": "How to Cite",
    "text": "How to Cite\n\n\nStyle\nCitation\n\n\n\nAPA\nKarim M. E., Epi-OER team ( 2025 ). Advanced Epidemiological Methods . Retrieved from https://ehsanx.github.io/EpiMethods/ on October 04, 2025 .\n\n\nMLA\nKarim M. E., Epi-OER team . \" Advanced Epidemiological Methods .\" Web. October 04, 2025 &lt; https://ehsanx.github.io/EpiMethods/ &gt;.\n\n\nChicago\nKarim M. E., Epi-OER team . \" Advanced Epidemiological Methods .\" 2025 . Web. October 04, 2025 &lt; https://ehsanx.github.io/EpiMethods/ &gt;.\n\n\nHarvard\nKarim M. E., Epi-OER team ( 2025 ) ' Advanced Epidemiological Methods '. Available at: https://ehsanx.github.io/EpiMethods/ (Accessed: October 04, 2025 ).\n\n\nVancouver\nKarim M. E., Epi-OER team . Advanced Epidemiological Methods . 2025 . [Online]. Available at: https://ehsanx.github.io/EpiMethods/ (Accessed October 04, 2025 ).\n\n\nIEEE\nKarim M. E., Epi-OER team , \" Advanced Epidemiological Methods ,\" 2025 , [Online]. Available: https://ehsanx.github.io/EpiMethods/ . Accessed on: October 04, 2025 .\n\n\nAMA\nKarim M. E., Epi-OER team Advanced Epidemiological Methods . 2025 . [Online]. Available at: https://ehsanx.github.io/EpiMethods/ (Accessed October 04, 2025 ).\n\n\n\n\n\nEpi-OER team: Hossain MB, Frank HA, Yusuf FL, Ahmed SS, Asamoah-Boaheng M, Zheng C (team is listed in order of contribution to the creation of this book)\n\nThe BibTex format can be downloaded from here.",
    "crumbs": [
      "The Project"
    ]
  },
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Background\nThe realm of data science is vast, and one of its foundational pillars is data wrangling. Data wrangling, often known as data munging, is the process of transforming raw data into a more digestible and usable format for analysis. In the context of R, a powerful statistical programming language, data wrangling becomes an essential skill for any data enthusiast. This chapter is dedicated to imparting practical knowledge on various data manipulation, import, and summarization techniques in R. Through a series of meticulously crafted tutorials, you will be equipped with the tools and techniques to handle, transform, and visualize data efficiently.\nIn this chapter, we embark on a structured journey through the intricate world of data wrangling in R. We begin by laying a solid foundation with R Basics, ensuring you grasp the essential elements of R programming. Once grounded in the basics, we progress to understanding the core R Data Types, diving deep into matrices, lists, and data frames. With a firm grasp of these structures, we introduce Automating Tasks to empower you with techniques that streamline the handling of vast datasets. Following this, we delve into the practical aspects of Importing Datasets, showcasing various methods to bring data from different formats into R. Building on this, Data Manipulation comes next, where we explore the myriad ways to modify and reshape your datasets to suit analytical needs. We then turn our attention to Importing External Data, offering a hands-on demonstration of how to integrate specific external datasets into your R environment. As we approach the chapter’s culmination, we emphasize the importance of Summary Tables in medical research, teaching you the art and science of data summarization. Finally, we wrap up with R Markdown, providing a comprehensive guide on how to seamlessly document your R code and analytical findings, ensuring your work is both reproducible and presentable.",
    "crumbs": [
      "Data wrangling"
    ]
  },
  {
    "objectID": "wrangling.html#background",
    "href": "wrangling.html#background",
    "title": "Data wrangling",
    "section": "",
    "text": "Introducing R Basics at the outset of an epidemiological methods tutorial book is akin to laying the foundation before building a house. It ensures that all readers, regardless of their prior experience, start on the same page, understanding the fundamental tools and language of R. This foundational knowledge not only smoothens the learning curve but also boosts confidence, allowing learners to focus on complex epidemiological techniques without being bogged down by the intricacies of the R language. In essence, mastering the basics first ensures a more cohesive and effective learning experience as the material advances.\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Data wrangling"
    ]
  },
  {
    "objectID": "wrangling.html#overview-of-tutorials",
    "href": "wrangling.html#overview-of-tutorials",
    "title": "Data wrangling",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nR Basics\nThis tutorial introduces the basics of R programming. It covers topics such as setting up R and RStudio, using R as a calculator, creating variables, working with vectors, plotting data, and accessing help resources.\n\n\nR Data Types\nThis tutorial covers three primary data structures in R: matrices, lists, and data frames. Matrices are two-dimensional arrays with elements of the same type, and their manipulation includes reshaping and combining. Lists in R are versatile collections that can store various R objects, including matrices. Data frames, on the other hand, are akin to matrices but permit columns of diverse data types. The tutorial offers guidance on creating, modifying, and merging data frames and checking their dimensions.\n\n\nAutomating Tasks\nMedical data analysis often grapples with vast and intricate data sets. Manual handling isn’t just tedious; it’s error-prone, especially given the critical decisions hinging on the results. This tutorial introduces automation techniques in R, a leading language for statistical analysis. By learning to use loops and functions, you can automate repetitive tasks, minimize errors, and conduct analyses more efficiently. Dive in to enhance your data handling skills.\n\n\nImporting Dataset from the Local Computer\nThis tutorial focuses on importing data into R. It demonstrates how to import data from CSV and SAS formats using functions like read.csv and sasxport.get. It also includes examples of loading specific variables, dropping variables, subsetting observations based on certain criteria, and handling missing values.\n\n\nData Manipulation\nThis tutorial explores various data manipulation techniques in R. It covers topics such as dropping variables from a dataset, keeping specific variables, subsetting observations based on specific criteria, converting variable types (e.g., factors, strings), and handling missing values.\n\n\nImport External Data Over the Internet\nThis tutorial provides examples of importing external data into R. It includes specific examples of importing a CSV file (Employee Salaries - 2017 data) and a SAS file (NHANES 2015-2016 data). It also demonstrates how to save a working dataset in different formats, such as CSV and RData.\n\n\nSummary Tables\nThis tutorial emphasizes the importance of data summarization in medical research and epidemiology, specifically how to summarize medical data using R. It demonstrates creating “Table 1”, a typical descriptive statistics table in research papers, with examples that use the built-in R functions and specialized packages to efficiently summarize and stratify data.\n\n\nR Markdown\nThis beginner-friendly tutorial guides you through working with R Markdown (RMD) files in RStudio, a popular IDE for R. The tutorial covers installing prerequisites, creating a new RMD file, and the basics of “knitting” to compile the document into various formats like HTML, PDF, or Word. It delves into embedding R code chunks and plain text within the RMD file, using the knitr package for document rendering. Tips for troubleshooting common issues and additional resources for further learning are also provided.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Data wrangling"
    ]
  },
  {
    "objectID": "wrangling1a.html",
    "href": "wrangling1a.html",
    "title": "R basics",
    "section": "",
    "text": "Start using R\nTo get started with R, follow these steps:\n\nDownload and Install R: Grab the newest version from the official R website. &gt; Tip: Download from a Comprehensive R Archive Network (CRAN) server near your geographic location.\nDownload and Install RStudio: You can get it from this link. &gt; Note: RStudio serves as an Integrated Development Environment (IDE) offering a user-friendly interface. It facilitates operations such as executing R commands, preserving scripts, inspecting results, managing data, and more.\nBegin with RStudio: Once you open RStudio, delve into using R. For starters, employ the R syntax for script preservation, allowing future code adjustments and additions.\nBasic syntax\n\n\n\n\n\n\nTip\n\n\n\nR, a versatile programming language for statistics and data analysis, can execute numerous tasks. Let’s break down some of the fundamental aspects of R’s syntax.\n\n\n\nUsing R as a Calculator\n\nSimilar to how you’d use a traditional calculator for basic arithmetic operations, R can perform these functions with ease. For instance:\n\n# Simple arithmetic\n1 + 1\n#&gt; [1] 2\n\nThis is a basic addition, resulting in 2.\nA more intricate calculation:\n\n# Complex calculation involving \n# multiplication, subtraction, division, powers, and square root\n20 * 5 - 10 * (3/4) * (2^3) + sqrt(25)\n#&gt; [1] 45\n\nThis demonstrates R’s capability to handle complex arithmetic operations.\n\nVariable Assignment in R\n\nR allows you to store values in variables, acting like labeled containers that can be recalled and manipulated later. For example,\n\n# Assigning a value of 2 to variable x1\nx1 &lt;- 2\nprint(x1)\n#&gt; [1] 2\n\nSimilarly:\n\nx2 &lt;- 9\nx2\n#&gt; [1] 9\n\n\nCreating New Variables Using Existing Ones\n\nYou can combine and manipulate previously assigned variables to create new ones.\n\n# Using variable x1 \n# to compute its square and assign to y1\ny1 &lt;- x1^2\ny1\n#&gt; [1] 4\n\nYou can also use multiple variables in a single expression:\n\ny2 &lt;- 310 - x1 + 2*x2 - 5*y1^3\ny2\n#&gt; [1] 6\n\n\nCreating Functions\n\nFunctions act as reusable blocks of code. Once defined, they can be called multiple times with different arguments. Here’s how to define a function that squares a number:\n\nz &lt;- function(x) {x^2}\n\nCall the function\n\nz(x = 0.5)\n#&gt; [1] 0.25\nz(x = 2)\n#&gt; [1] 4\n\nR also comes with a plethora of built-in functions. Examples include exp (exponential function) and rnorm (random number generation from a normal distribution).\n\nUtilizing Built-In Functions\n\nFor instance, using the exponential function:\n\n# Calling functions\nexp(x1)\n#&gt; [1] 7.389056\nlog(exp(x1))\n#&gt; [1] 2\n\nThe rnorm function can generate random samples from a normal distribution: below we are generating 10 random sampling from the normal distribution with mean 0 and standard deviation 1:\n\nrnorm(n = 10, mean = 0, sd = 1)\n#&gt;  [1]  0.09548954 -0.15380317 -0.78119328  0.92746313 -2.14464859 -0.65474698\n#&gt;  [7] -0.28007179 -0.85171595 -1.18924278 -0.17224185\n\nAs random number generation relies on algorithms, results will differ with each execution.\n\n# Random sampling (again)\nrnorm(n = 10, mean = 0, sd = 1)\n#&gt;  [1] -0.2861384  0.9275671 -1.5442804  1.8067612 -0.6306749 -0.1746498\n#&gt;  [7]  0.8449699 -0.3598907  0.4424825  0.7613802\n\nHowever, by setting a seed, we can reproduce identical random results:\n\n# Random sampling (again, but with a seed)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#&gt;  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#&gt;  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\n\n# random sampling (reproducing the same numbers)\nset.seed(11)\nrnorm(n = 10, mean = 0, sd = 1)\n#&gt;  [1] -0.59103110  0.02659437 -1.51655310 -1.36265335  1.17848916 -0.93415132\n#&gt;  [7]  1.32360565  0.62491779 -0.04572296 -1.00412058\n\nAs we can see, when we set the same seed, we get exactly the same random number. This is very important for reproducing the same results. There are many other pre-existing functions in R.\n\nSeeking Help in R\n\n\n\n\n\n\n\nTip\n\n\n\nR’s help function, invoked with ?function_name, provides detailed documentation on functions, assisting users with unclear or forgotten arguments:\n\n\n\n# Searching for help if you know \n# the exact name of the function with a question mark\n?curve\n\nBelow is an example of using the pre-exiting function for plotting a curve ranging from -10 to 10.\n\n# Plotting a function\ncurve(z, from = -10, to = 10, xlab = \"x\", ylab = \"Squared x\")\n\n\n\n\n\n\n\nIf some of the arguments are difficult to remember or what else could be done with that function, we could use the help function. For example, we can simply type help(curve) or ?curve to get help on the curve function:\n\n\n\n\n\n\nTip\n\n\n\nIf you’re uncertain about a function’s precise name, two question marks can assist in the search:\n\n\n\n# Searching for help if don't know \n# the exact name of the function\n??boxplot\n\n\nCreating Vectors\n\nVectors are sequences of data elements of the same basic type. Here are some methods to create them:\n\n# Creating vectors in different ways\nx3 &lt;- c(1, 2, 3, 4, 5)\nprint(x3)\n#&gt; [1] 1 2 3 4 5\n\nx4 &lt;- 1:7\nprint(x4)\n#&gt; [1] 1 2 3 4 5 6 7\n\nx5 &lt;- seq(from = 0, to = 100, by = 10)\nprint(x5)\n#&gt;  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nx6 &lt;- seq(10, 30, length = 7)\nx6\n#&gt; [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n\n\nPlotting in R\n\nR provides numerous plotting capabilities. For instance, the plot function can create scatter plots and line graphs:\n\n# Scatter plot\nplot(x5, type = \"p\", main = \"Scatter plot\")\n\n\n\n\n\n\n\n\n# Line graph\nplot(x = x6, y = x6^2, type = \"l\", main = \"Line graph\")\n\n\n\n\n\n\n\n\nCharacter Vectors Apart from numeric values, R also allows for character vectors. For example, we can create a sex variable coded as females, males and other.\n\n\n# Character vector\nsex &lt;- c(\"females\", \"males\", \"other\")\nsex\n#&gt; [1] \"females\" \"males\"   \"other\"\n\nTo determine a variable’s type, use the mode function:\n\n# Check data type\nmode(sex)\n#&gt; [1] \"character\"\n\nPackage Management\nPackages in R are collections of functions and datasets developed by the community. They enhance the capability of R by adding new functions for data analysis, visualization, data import, and more. Understanding how to install and load packages is essential for effective R programming.\n\nInstalling Packages from CRAN\n\nThe CRAN is a major source of R packages. You can install them directly from within R using the install.packages() function.\n\n# Installing the 'ggplot2' package\ninstall.packages(\"ggplot2\")\n\n\nLoading a Package\n\nAfter a package is installed, it must be loaded to use its functions. This is done with the library() function.\n\n# Loading the 'ggplot2' package\nlibrary(ggplot2)\n\nYou only need to install a package once, but you’ll need to load it every time you start a new R session and want to use its functions.\n\nUpdating Packages\n\nR packages are frequently updated. To ensure you have the latest version of a package, use the update.packages() function.\n\n# Updating all installed packages\n# could be time consuming!\nupdate.packages(ask = FALSE)  \n# 'ask = FALSE' updates all without asking for confirmation\n\n\nListing Installed Packages\n\nYou can view all the installed packages on your R setup using the installed.packages() function.\n\n# Listing installed packages\ninstalled.packages()[, \"Package\"]\n\n\nRemoving a Package\n\nIf you no longer need a package, it can be removed using the remove.packages() function.\n\n# Removing the 'ggplot2' package\nremove.packages(\"ggplot2\")\n\n\nInstalling Packages from Other Sources\n\nWhile CRAN is the primary source, sometimes you might need to install packages from GitHub or other repositories. The devtools package provides a function for this.\n\n# Installing devtools first\ninstall.packages(\"devtools\")\n# Loading devtools\nlibrary(devtools)\n# Install a package from GitHub\n# https://github.com/ehsanx/simMSM\ninstall_github(\"ehsanx/simMSM\")\n\nWhen you are working on a project, it’s a good practice to list and install required packages at the beginning of your R script.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Data wrangling",
      "R basics"
    ]
  },
  {
    "objectID": "wrangling1b.html",
    "href": "wrangling1b.html",
    "title": "Data types",
    "section": "",
    "text": "Matrix\n\n\n\n\n\n\nTip\n\n\n\nIn R, matrices are two-dimensional rectangular data sets, which can be created using the matrix() function. It’s essential to remember that all the elements of a matrix must be of the same type, such as all numeric or all character.\n\n\nTo construct a matrix, we often start with a vector and specify how we want to reshape it. For instance:\n\n# Matrix 1\nx &lt;- 1:10\nmatrix1 &lt;- matrix(x, nrow = 5, ncol = 2, byrow = TRUE)\nmatrix1\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    3    4\n#&gt; [3,]    5    6\n#&gt; [4,]    7    8\n#&gt; [5,]    9   10\n\nHere, the vector x contains numbers from 1 to 10. We reshape it into a matrix with 5 rows and 2 columns. The byrow = TRUE argument means the matrix will be filled row-wise, with numbers from the vector.\nConversely, if you want the matrix to be filled column-wise, you’d set byrow = FALSE:\n\n# matrix 2\nmatrix2 &lt;- matrix(x, nrow = 5, ncol = 2, byrow = FALSE)\nmatrix2\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    6\n#&gt; [2,]    2    7\n#&gt; [3,]    3    8\n#&gt; [4,]    4    9\n#&gt; [5,]    5   10\n\nYou can also combine or concatenate matrices. cbind() joins matrices by columns while rbind() joins them by rows.\n\n# Merging 2 matrices\ncbind(matrix1, matrix2)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    1    6\n#&gt; [2,]    3    4    2    7\n#&gt; [3,]    5    6    3    8\n#&gt; [4,]    7    8    4    9\n#&gt; [5,]    9   10    5   10\n\n\n# Appending 2 matrices\nrbind(matrix1, matrix2)\n#&gt;       [,1] [,2]\n#&gt;  [1,]    1    2\n#&gt;  [2,]    3    4\n#&gt;  [3,]    5    6\n#&gt;  [4,]    7    8\n#&gt;  [5,]    9   10\n#&gt;  [6,]    1    6\n#&gt;  [7,]    2    7\n#&gt;  [8,]    3    8\n#&gt;  [9,]    4    9\n#&gt; [10,]    5   10\n\nCreating an empty matrix is also possible:\n\nmatrix(nrow=5, ncol=5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]   NA   NA   NA   NA   NA\n#&gt; [2,]   NA   NA   NA   NA   NA\n#&gt; [3,]   NA   NA   NA   NA   NA\n#&gt; [4,]   NA   NA   NA   NA   NA\n#&gt; [5,]   NA   NA   NA   NA   NA\n\nList\n\n\n\n\n\n\nTip\n\n\n\nIn R, lists can be seen as a collection where you can store a variety of different objects under a single name. This includes vectors, matrices, or even other lists. It’s very versatile because its components can be of any type of R object, such as vector, matrix, array, dataframe, table, list, and so on.\n\n\nFor instance:\n\n# List of 2 matrices\nlist1 &lt;- list(matrix1, matrix2)\nlist1\n#&gt; [[1]]\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    3    4\n#&gt; [3,]    5    6\n#&gt; [4,]    7    8\n#&gt; [5,]    9   10\n#&gt; \n#&gt; [[2]]\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    6\n#&gt; [2,]    2    7\n#&gt; [3,]    3    8\n#&gt; [4,]    4    9\n#&gt; [5,]    5   10\n\nLists can also be expanded to include multiple items:\n\nx6 &lt;- seq(10, 30, length = 7)\nsex &lt;- c(\"females\", \"males\", \"other\")\n# Expanding list to include more items\nlist2 &lt;- list(list1, x6, sex, matrix1)\nlist2 \n#&gt; [[1]]\n#&gt; [[1]][[1]]\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    3    4\n#&gt; [3,]    5    6\n#&gt; [4,]    7    8\n#&gt; [5,]    9   10\n#&gt; \n#&gt; [[1]][[2]]\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    6\n#&gt; [2,]    2    7\n#&gt; [3,]    3    8\n#&gt; [4,]    4    9\n#&gt; [5,]    5   10\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [1] 10.00000 13.33333 16.66667 20.00000 23.33333 26.66667 30.00000\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"females\" \"males\"   \"other\"  \n#&gt; \n#&gt; [[4]]\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    3    4\n#&gt; [3,]    5    6\n#&gt; [4,]    7    8\n#&gt; [5,]    9   10\n\nCombining different types of data into a single matrix converts everything to a character type:\n\n# A matrix with numeric and character variables\nid &lt;- c(1, 2)\nscore &lt;- c(85, 85)\nsex &lt;- c(\"M\", \"F\")\nnew.matrix &lt;- cbind(id, score, sex)\nnew.matrix\n#&gt;      id  score sex\n#&gt; [1,] \"1\" \"85\"  \"M\"\n#&gt; [2,] \"2\" \"85\"  \"F\"\n\nTo check the type of data in your matrix:\n\nmode(new.matrix)\n#&gt; [1] \"character\"\n\nData frame\n\n\n\n\n\n\nTip\n\n\n\nAs we can see combining both numeric and character variables into a matrix ended up with a matrix of character values. To keep the numeric variables as numeric and character variables as character, we can use the data.frame function.\n\n\n\nCreating a data frame\n\n\n\nA data frame is similar to a matrix but allows for columns of different types (numeric, character, factor, etc.). It’s a standard format for storing data sets in R.\n\ndf &lt;- data.frame(id, score, sex)\ndf\n\n\n  \n\n\n\nTo check the mode or type of your data frame:\n\nmode(df)\n#&gt; [1] \"list\"\n\n\nExtract elements\n\nData frames allow easy extraction and modification of specific elements. For example, we can extract the values on the first row and first column as follow:\n\ndf[1,1]\n#&gt; [1] 1\n\nSimilarly, the first column can be extracted as follows:\n\ndf[,1]\n#&gt; [1] 1 2\n\nThe first row can be extracted as follows:\n\ndf[1,]\n\n\n  \n\n\n\nColumns can also be accessed using $. Below we are calling column id:\n\ndf$id\n#&gt; [1] 1 2\n\n\nModifying values\n\nWe can edit the values in the data frame as well. For example, we can change the score from 85 to 90 for the id 1:\n\ndf$score[df$id == 1] &lt;- 90\ndf\n\n\n  \n\n\n\nWe can also change the name of the variables/columns:\n\ncolnames(df) &lt;- c(\"Studyid\", \"Grade\", \"Sex\")\ndf\n\n\n  \n\n\n\n\nCombining data frames\n\nWe can also merge another data frame with the same variables using the rbind function:\n\n# Create a new dataset\ndf2 &lt;- data.frame(Studyid = c(10, 15, 50), Grade = c(75, 90, 65), Sex = c(\"F\", \"M\", \"M\"))\n\n# Combining two data frames\ndf.new &lt;- rbind(df, df2)\n\n# Print the first 6 rows\nhead(df.new)\n\n\n  \n\n\n\n\nChecking the dimensions\n\nTo see the dimension of the data frame (i.e., number of rows and columns), we can use the dim function:\n\ndim(df.new)\n#&gt; [1] 5 3\n\nAs we can see, we have 5 rows and 3 columns. We can use the nrow and ncol functions respectively for the same output:\n\nnrow(df.new)\n#&gt; [1] 5\nncol(df.new)\n#&gt; [1] 3",
    "crumbs": [
      "Data wrangling",
      "Data types"
    ]
  },
  {
    "objectID": "wrangling1c.html",
    "href": "wrangling1c.html",
    "title": "Automating tasks",
    "section": "",
    "text": "Repeating a task\n\n\n\n\n\n\nTip\n\n\n\nThe for loop is a control flow statement in R that lets you repeat a particular task multiple times. This repetition is based on a sequence of numbers or values in a vector.\n\n\nConsider a simple real-life analogy: Imagine you are filling water in 10 bottles, one by one. Instead of doing it manually 10 times, you can set a machine to do it in a loop until all 10 bottles are filled.\n\nExample 1\n\nLet’s initiate a counter k at 0 and add 5 to k with each iteration of the loop (i.e., every time it “runs”). After 10 cycles, the loop will stop, but not before printing k in each cycle.\n\n# Looping and adding\nk &lt;- 0\nfor (i in 1:10){\n  k &lt;- k + 5\n  print(k)\n}\n#&gt; [1] 5\n#&gt; [1] 10\n#&gt; [1] 15\n#&gt; [1] 20\n#&gt; [1] 25\n#&gt; [1] 30\n#&gt; [1] 35\n#&gt; [1] 40\n#&gt; [1] 45\n#&gt; [1] 50\n\n\nExample 2\n\nWe create a variable x5 containing the values of 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. Let us print the first 5 values using the for loop function:\n\nx5 &lt;- seq(from = 0, to = 100, by = 10)\n# Looping through a vector\nk &lt;- 1:5\nfor (ii in k){\n  print(x5[ii])\n}\n#&gt; [1] 0\n#&gt; [1] 10\n#&gt; [1] 20\n#&gt; [1] 30\n#&gt; [1] 40\n\nThis loop cycles through the first five values of a previously created variable x5 and prints them. Each value printed corresponds to the positions 1 to 5 in x5.\n\nExample 3\n\nLet us use the for loop in a more complicated scenario. First, we create a vector of numeric values and square it:\n\n# Create a vector\nk &lt;- c(1, 3, 6, 2, 0)\nk^2\n#&gt; [1]  1  9 36  4  0\n\nThis is just squaring each value in the vector k.\n\nExample 4\n\nUsing the for loop function, we can create the same vector of square values as in Example 3. To do so, (i) we create a null object, (ii) use the loop for each of the elements in the vector (k), (iii) square each of the elements, and (iv) store each of the elements of the new vector. In the example below, the length of k is 5, and the loop will run from the first to the fifth element of k. Also, k.sq[1] is the first stored value for squared-k, and k.sq[2] is the second stored value for squared-k, and so on.\n\n# Looping through a vector with function\nk.sq &lt;- NULL\nfor (i in 1:length(k)){\n  k.sq[i] &lt;- k[i]^2\n}\n\n# Print the values\nk.sq\n#&gt; [1]  1  9 36  4  0\n\nHere, we achieve the same result as the third example but use a for loop. We prepare an empty object k.sq and then use the loop to square each value in k, storing the result in k.sq.\n\nExample 5\n\n\ndf.new &lt;- data.frame(\n  Studyid = c(1, 2, 10, 15, 50),\n  Grade = c(90, 85, 75, 90, 65),\n  Sex = c('M', 'F', 'F', 'M', 'M')\n)\n# Looping through a data frame\nfor (i in 1:nrow(df.new)){\n  print(df.new[i,\"Sex\"])\n}\n#&gt; [1] \"M\"\n#&gt; [1] \"F\"\n#&gt; [1] \"F\"\n#&gt; [1] \"M\"\n#&gt; [1] \"M\"\n\nThis loop prints the “Sex” column value for each row in the df.new data frame.\nFunctions\n\n\n\n\n\n\nTip\n\n\n\nA function in R is a piece of code that can take inputs, process them, and return an output. There are functions built into R, like mean(), which calculates the average of a set of numbers.\n\n\n\nBuilt-in function\n\n\n# Calculating a mean from a vector\nVector &lt;- 1:100\nmean(Vector)\n#&gt; [1] 50.5\n\nHere, we’re using the built-in mean() function to find the average of numbers from 1 to 100.\n\nCustom-made function\n\nTo understand how functions work, sometimes it’s helpful to build our own. Now we will create our own function to calculate the mean, where we will use the following equation to calculate it:\n\\(\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n},\\)\nwhere \\(x_1\\), \\(x_2\\),…, \\(x_n\\) are the values in the vector and \\(n\\) is the sample size. Let us create the function for calculation the mean:\nThis function, mean.own, calculates the average. We add up all the numbers in a vector (Sum &lt;- sum(x)) and divide by the number of items in that vector (n &lt;- length(x)). The result is then returned.\n\nmean.own &lt;- function(x){\n  Sum &lt;- sum(x)\n  n &lt;- length(x)\n  return(Sum/n)\n}\n\nBy using our custom-made function, we calculate the mean of numbers from 1 to 100, getting the same result as the built-in mean() function.\n\nmean.own(Vector)\n#&gt; [1] 50.5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Data wrangling",
      "Automating tasks"
    ]
  },
  {
    "objectID": "wrangling2.html",
    "href": "wrangling2.html",
    "title": "Importing dataset",
    "section": "",
    "text": "Introduction to Data Importing\nBefore analyzing data in R, one of the first steps you’ll typically undertake is importing your dataset. R provides numerous methods to do this, depending on the format of your dataset.\nDatasets come in a variety of file formats, with .csv (Comma-Separated Values) and .txt (Text file) being among the most common. While R’s interface offers manual ways to load these datasets, knowing how to code this step ensures better reproducibility and automation.\nImporting .txt files\nA .txt data file can be imported using the read.table function. As an example, consider you have a dataset named grade in the specified path.\nLet’s briefly glance at the file without concerning ourselves with its formatting.\n\n# Read and print the content of the TXT file\ncontent &lt;- readLines(\"Data/wrangling/grade.txt\")\ncat(content, sep = \"\\n\")\n#&gt; Studyid Grade Sex\n#&gt; 1    90   M\n#&gt; 2    85   F\n#&gt; 10    75   F\n#&gt; 15    90   M\n#&gt; 50    65   M\n\nUsing the read.table function, you can load this dataset in R properly. It’s important to specify header = TRUE if the first row of your dataset contains variable names.\n\nTip: Always ensure the header argument matches the structure of your dataset. If your dataset contains variable names, set header = TRUE.\n\n\n## Read a text dataset\ngrade &lt;- read.table(\"Data/wrangling/grade.txt\", header = TRUE)\n\n# Display the first few rows of the dataset\nhead(grade)\n\n\n  \n\n\n\nImporting .csv files\nSimilarly, .csv files can be loaded using the read.csv function. Here’s how you can load a .csv dataset named mpg:\n\n## Read a csv dataset\nmpg &lt;- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n# Display the first few rows of the dataset\nhead(mpg)\n\n\n  \n\n\n\nWhile we’ve discussed two popular data formats, R can handle a plethora of other formats. For further details, refer to Quick-R (2023). Notably, some datasets come built-in with R packages, like the mpg dataset in the ggplot2 package. To load such a dataset:\n\ndata(mpg, package = \"ggplot2\")\nhead(mpg)\n\n\n  \n\n\n\nTo understand more about the variables and the dataset’s structure, you can consult the documentation:\n\n?mpg\n\nData Screening and Understanding Your Dataset\ndim(), nrow(), ncol(), and str() are incredibly handy functions when initially exploring your dataset.\nOnce your data is in R, the next logical step is to get familiar with it. Knowing the dimensions of your dataset, types of variables, and the first few entries can give you a quick sense of what you’re dealing with.\nFor instance, str (short for structure) is a concise way to display information about your data. It reveals the type of each variable, the first few entries, and the total number of observations:\n\nstr(mpg)\n#&gt; tibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n#&gt;  $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n#&gt;  $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n#&gt;  $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n#&gt;  $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n#&gt;  $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n#&gt;  $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n#&gt;  $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n#&gt;  $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n#&gt;  $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n#&gt;  $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nIn summary, becoming proficient in data importing and initial screening is a fundamental step in any data analysis process in R. It ensures that subsequent stages of data manipulation and analysis are based on a clear understanding of the dataset at hand.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nQuick-R. 2023. “Importing Data.” https://www.statmethods.net/input/importingdata.html.",
    "crumbs": [
      "Data wrangling",
      "Importing dataset"
    ]
  },
  {
    "objectID": "wrangling3.html",
    "href": "wrangling3.html",
    "title": "Data manipulation",
    "section": "",
    "text": "Data manipulation is a foundational skill for data analysis. This guide introduces common methods for subsetting datasets, handling variable types, creating summary tables, and dealing with missing values using R.\nLoad dataset\nUnderstanding the dataset’s structure is the first step in data manipulation. Here, we’re using the mpg dataset, which provides information on various car models:\n\nmpg &lt;- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\nSubset\nOften, you’ll need to subset your data for analysis. Here, we’ll explore different methods to both drop unwanted variables and keep desired observations.\nDrop variables\nSometimes, only part of the variables will be used in your analysis. Therefore, you may want to drop the variables you do not need. There are multiple ways to drop variables from a dataset. Below are two examples without using any package and using the dplyr package.\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[, c(columns_names_you_want_to_KEEP)]\n\n\nSay, we want to keep only three variables in the mpg dataset: manufacturer, model and cyl. For Option 1 (without package), we can use the following R codes to keep these three variables:\n\nmpg1 &lt;- mpg[, c(\"manufacturer\", \"model\", \"cyl\")]\nhead(mpg1)\n\n\n  \n\n\n\nHere mpg1 is a new dataset containing only three variables (manufacturer, model and cyl).\n\n\n\n\n\n\nTip\n\n\n\nOption 2: use select in dplyr\nselect(dataset.name, c(columns_names_you_want_to_KEEP))\n\n\nFor Option 2, the dplyr package offers the select function, which provides a more intuitive way to subset data.\n\nmpg2 &lt;- select(mpg, c(\"manufacturer\", \"model\", \"cyl\"))\nhead(mpg2)\n\n\n  \n\n\n\nWe can also exclude any variables from the dataset by using the minus (-) sign with the select function. For example, we we want to drop trans, drv, and cty from the mpg dataset, we can use the following codes:\n\nmpg3 &lt;- select(mpg, -c(\"trans\", \"drv\", \"cty\"))\nhead(mpg3)\n\n\n  \n\n\n\nThis mpg3 is a new dataset from mpg after dropping three variables (trans, drv, and cty).\nKeep observations\nIt often happens that we only want to investigate a subset of a population which only requires a subset of our dataset. In this case, we need to subset the dataset to meet certain requirements. Again, there are multiple ways to do this task. Below is an example without a package and with the dplyr package:\n\n\n\n\n\n\nTip\n\n\n\nOption 1: No package needed\ndataset.name[rows_you_want_to_KEEP, ]\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 2: No package needed\nsubset(dataset.name, rows_you_want_to_KEEP)\n\n\n\n\n\n\n\n\nTip\n\n\n\nOption 3: use filter in dplyr\nfilter(dataset.name, rows_you_want_to_KEEP)\n\n\nWe can use the logical tests for the rows you want to keep or drop.\n\n\n\n\n\n\nTip\n\n\n\nCommon logical tests are:\n\n\n\n\nSyntax\nMeaning\n\n\n\nX &lt;(=) Y\nSmaller (equal) than\n\n\nX &gt;(=) Y\nLarger (equal) than\n\n\nX == Y\nEqual to\n\n\nX != Y\nNot equal to\n\n\nis.na(X)\nis NA/missing?\n\n\n\n\n\n\n\nSay, we want to keep the observations for which cars are manufactured in 2008. We can use the following R codes to do it:\n\n# Option 1\nmpg4 &lt;- mpg[mpg$year == \"2008\",]\nhead(mpg4)\n\n\n  \n\n\n\nThe following codes with the subset and filter function will do the same:\n\n# Option 2\nmpg5 &lt;- subset(mpg, year == \"2008\")\nhead(mpg5)\n\n\n  \n\n\n\n\n# Option 3\nmpg6 &lt;- filter(mpg, year == \"2008\") \nhead(mpg6)\n\n\n  \n\n\n\nThe filter function can also work when you have multiple criteria (i.e., multiple logical tests) to satisfy. Here, we need Boolean operators to connect different logical tests.\n\n\n\n\n\n\nTip\n\n\n\nCommon boolean operators are:\n\n\n\n\nSyntax\nMeaning\n\n\n\n&\nand\n\n\n|\nor\n\n\n!\nnot\n\n\n==\nequals to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&lt;\nless than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;=\nless than or equal to\n\n\n\n\n\n\n\nSay, we want to keep the observations for 6 and 8 cylinders (cyl) and engine displacement (displ) greater than or equal to 4 litres. We can use the following codes to do the task:\n\nmpg7 &lt;- filter(mpg, cyl %in% c(\"6\",\"8\") & displ &gt;= 4)\nhead(mpg7)\n\n\n  \n\n\n\n\n\nThe %in% operator is used to determine whether the values of the first argument are present in the second argument.\nHandling Variable Types\n\n\n\n\n\n\nTip\n\n\n\nMost common types of variable in R are\n\nnumbers,\nfactors and\nstrings(or character).\n\nUnderstanding and manipulating these types are crucial for data analysis.\n\n\n\nIdentifying Variable Type\n\nWhen we analyze the data, we usually just deal with numbers and factors. If there are variables are strings, we could convert them to factors using as.factors(variable.name)\n\nmode(mpg$trans)\n#&gt; [1] \"character\"\n\n\nstr(mpg$trans)\n#&gt;  chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" \"auto(l5)\" ...\n\n\nConverting Characters to Factors\n\nSometimes, it’s necessary to treat text data as categorical by converting them into factors. as.numeric() converts other types of variables to numbers. For a factor variable, we usually we want to access the categories (or levels) it has. We can use a build-in function to explore: levels(variable.name)\n\n# no levels for character\nlevels(mpg$trans)\n#&gt; NULL\n\n\n## Ex check how many different trans the dataset has\nmpg$trans &lt;- as.factor(mpg$trans)\nlevels(mpg$trans)\n#&gt;  [1] \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"   \"auto(l6)\"  \n#&gt;  [6] \"auto(s4)\"   \"auto(s5)\"   \"auto(s6)\"   \"manual(m5)\" \"manual(m6)\"\n\nThe levels usually will be ordered alphabetically. The first level is called “baseline”. However, the users may/may not want to keep this baseline and want to relevel/change the reference group. We can do it using the relevel function:\nrelevel(variable.name, ref=)\n\nmpg$trans &lt;- relevel(mpg$trans, ref = \"auto(s6)\")\nlevels(mpg$trans)\n#&gt;  [1] \"auto(s6)\"   \"auto(av)\"   \"auto(l3)\"   \"auto(l4)\"   \"auto(l5)\"  \n#&gt;  [6] \"auto(l6)\"   \"auto(s4)\"   \"auto(s5)\"   \"manual(m5)\" \"manual(m6)\"\nnlevels(mpg$trans)\n#&gt; [1] 10\n\nThe factor function can also be used to combine multiple factors into one factor.\n\n## EX re-group trans to \"auto\" and \"manual\"\nlevels(mpg$trans) &lt;- list(auto = c(\"auto(av)\", \"auto(l3)\", \"auto(l4)\", \"auto(l5)\", \"auto(l6)\", \n                                   \"auto(s4)\", \"auto(s5)\", \"auto(s6)\"), \n                          manual = c(\"manual(m5)\", \"manual(m6)\"))\nlevels(mpg$trans)\n#&gt; [1] \"auto\"   \"manual\"\n\nYou can also change the order of all factors using the following code: factor(variable.name, levels = c(“new order”))\n\n## EX. Change the order of trans to manual\nmpg$trans &lt;- factor(mpg$trans, levels = c(\"manual\", \"auto\"))\nlevels(mpg$trans)\n#&gt; [1] \"manual\" \"auto\"\n\n\n\nIn R, the use of factors with multiple levels is primarily a memory optimization strategy. While users may not directly see this, R assigns internal numerical identifiers to each level, which is a more memory-efficient way of handling such data. Unlike some other software packages that generate multiple dummy variables to represent a single variable, R’s approach is generally more resource-efficient.\n\nConverting back from Factors Characters\n\nYou can also convert factor back to character using the as.character function.\n\n# Convert factor back to character\nmpg$trans &lt;- as.character(mpg$trans)\nlevels(mpg$trans) # no levels for character\n#&gt; NULL\n\nConvert continuous variables to categorical variables\n\n\n\n\n\n\nTip\n\n\n\nifelse, cut, recode all are helpful functions to convert numerical variables to categorical variables.\n\n\nLet’s see the summary of the cty variable first.\n\nsummary(mpg$cty)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    9.00   14.00   17.00   16.86   19.00   35.00\n\nsay, we may want to change continuous ‘cty’ into groups 0-14, 15-18, and 18-40. Below is an example with the cut function.\n\n## EX. change the cty into two categories (0,14], (14,18] and (18,40]\nmpg$cty.num &lt;- cut(mpg$cty, c(0, 14, 18, 40), right = TRUE)\ntable(mpg$cty.num)\n#&gt; \n#&gt;  (0,14] (14,18] (18,40] \n#&gt;      73      85      76\n\n\n## Try this: do you see a difference?: [0,14), [14,18) and [18,40)\nmpg$cty.num2 &lt;- cut(mpg$cty, c(0, 14, 18, 40), right = FALSE)\ntable(mpg$cty.num2)\n#&gt; \n#&gt;  [0,14) [14,18) [18,40) \n#&gt;      54      78     102\n\n\n\n] stands for closed interval, i.e., right = TRUE. On the other hand, ) means open interval. Hence, there will be a huge difference when setting right = TRUE vs. right = FALSE\nMissing value\n\n\n\n\n\n\nTip\n\n\n\nIncomplete datasets can distort analysis. Identifying and managing these missing values is thus crucial.\n\n\nWe can check how many missing values we have by: table(is.na(variable.name))\nLet’s us check whether the cty variable contains any missing values:\n\ntable(is.na(mpg$cty))\n#&gt; \n#&gt; FALSE \n#&gt;   234\n\nIf you want to return all non-missing values, i.e., complete case values: na.omit(variable.name). For more extensive methods on handling missing values, see subsequent tutorials.",
    "crumbs": [
      "Data wrangling",
      "Data manipulation"
    ]
  },
  {
    "objectID": "wrangling4.html",
    "href": "wrangling4.html",
    "title": "Import external data",
    "section": "",
    "text": "# Load required packages\nlibrary(dplyr)\nrequire(Hmisc)\nlibrary(haven)\n\nWhen dealing with data analysis in R, it’s common to need to import external data. This tutorial will walk you through importing data in different formats.\nCSV format data\nCSV stands for “Comma-Separated Values” and it’s a widely used format for data. We’ll be looking at the “Employee Salaries - 2017” dataset, which contains salary information for permanent employees of Montgomery County in 2017.\n\n\nEmployee Salaries - 2017 data\n\n\n\n\n\n\nTip\n\n\n\nWe’ll be loading the Employee_Salaries_-_2017.csv dataset into R from its saved location at Data/wrangling/. Do note, the directory path might vary for you based on where you’ve stored the downloaded data.\n\n\n\ndata.download &lt;- read.csv(\"Data/wrangling/Employee_Salaries_-_2017.csv\")\n\nHere, the read.csv function reads the data from the CSV file and stores it in a variable called data.download.\nTo understand the structure of our dataset, We can see the number of rows and columns and the names of the columns/variables as follows:\n\ndim(data.download) # check dimension / row / column numbers\n#&gt; [1] 9398   12\nnrow(data.download) # check row numbers\n#&gt; [1] 9398\nnames(data.download) # check column names\n#&gt;  [1] \"Full.Name\"                \"Gender\"                  \n#&gt;  [3] \"Current.Annual.Salary\"    \"X2017.Gross.Pay.Received\"\n#&gt;  [5] \"X2017.Overtime.Pay\"       \"Department\"              \n#&gt;  [7] \"Department.Name\"          \"Division\"                \n#&gt;  [9] \"Assignment.Category\"      \"Employee.Position.Title\" \n#&gt; [11] \"Position.Under.Filled\"    \"Date.First.Hired\"\n\n\n\n\n\n\n\nTip\n\n\n\nhead shows the first 6 elements of an object, giving you a sneak peek into the data you’re dealing with, while tail shows the last 6 elements.\n\n\nWe can see the first see six rows of the dataset as follows:\n\nhead(data.download)\n\n\n  \n\n\n\nNext, for learning purposes, let’s artificially assign all male genders in our dataset as missing:\n\n# Assigning male gender as missing\ndata.download$Gender[data.download$Gender == \"M\"] &lt;- NA\nhead(data.download)\n\n\n  \n\n\n\nThis chunk sets the Gender column’s value to NA (missing) wherever the gender is “M”. This is a form of data manipulation, sometimes used to handle missing or incorrect data. If you want to work with datasets that exclude any missing values:\n\n\n\n\n\n\nTip\n\n\n\nna.omit and complete.cases are useful functions to to create datasets with non-NA values\n\n\n\n# deleting/dropping missing components\ndata.download2 &lt;- na.omit(data.download)\nhead(data.download2)\n\n\n  \n\n\ndim(data.download2)\n#&gt; [1] 3806   12\n\nHere, na.omit is used to remove rows with any missing values. This can be essential when preparing data for certain analyses.\nAlternatively, we could have selected only females to drop all males:\n\ndata.download3 &lt;- filter(data.download, Gender != \"M\")\nhead(data.download3)\n\n\n  \n\n\n\nAnd to check the size of this new dataset:\n\n# new dimension / row / column numbers\ndim(data.download3)\n#&gt; [1] 3806   12\n\nSAS format data\n\n\n\n\n\n\nTip\n\n\n\nSAS is another data format, commonly used in professional statistics and analytics.\n\n\nLet’s explore importing a SAS dataset. We download a SAS formatted dataset from the CDC website. In particular, we are interested in DEMO component from 2015-16.\n\n\nUS CDC website\n\n\n# Link\nx &lt;- \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\"\n\n# Data\n# NHANES1516data &lt;- sasxport.get(x)\nNHANES1516data &lt;- read_xpt(x)\n\n# Check dimension / row / column numbers\ndim(NHANES1516data) \n#&gt; [1] 9971   47\n\n# Check row numbers\nnrow(NHANES1516data) \n#&gt; [1] 9971\n\n# Check first 10 names\nnames(NHANES1516data)[1:10] \n#&gt;  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#&gt;  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\"\n\nThe sasxport.get function retrieves the SAS dataset. The following lines, just like before, help understand its structure.\nTo analyze some of the data:\n\ntable(NHANES1516data$riagendr) # tabulating gender variable\n#&gt; &lt; table of extent 0 &gt;\n\n\n\nVerify these numbers from CDC website\nThis code creates a frequency table of the riagendr variable, which represents gender.",
    "crumbs": [
      "Data wrangling",
      "Import external data"
    ]
  },
  {
    "objectID": "wrangling5.html",
    "href": "wrangling5.html",
    "title": "Summary tables",
    "section": "",
    "text": "Medical research and epidemiology often involve large, complex datasets. Data summarization is a vital step that transforms these vast datasets into concise, understandable insights. In medical contexts, these summaries can highlight patterns, indicate data inconsistencies, and guide further research. This tutorial will teach you how to use R to efficiently summarize medical data.\nIn epidemiology and medical research, “Table 1” typically refers to the first table in a research paper or report that provides descriptive statistics of the study population. It offers a snapshot of the baseline characteristics of the study groups, whether in a cohort study, clinical trial, or any other study design.\n\n# Data\nmpg &lt;- read.csv(\"Data/wrangling/mpg.csv\", header = TRUE)\n\n# Frequency table for drv\ntable(mpg$drv)\n#&gt; \n#&gt;   4   f   r \n#&gt; 103 106  25\n\n# Frequency table for manufacturer\ntable(mpg$manufacturer)\n#&gt; \n#&gt;       audi  chevrolet      dodge       ford      honda    hyundai       jeep \n#&gt;         18         19         37         25          9         14          8 \n#&gt; land rover    lincoln    mercury     nissan    pontiac     subaru     toyota \n#&gt;          4          3          4         13          5         14         34 \n#&gt; volkswagen \n#&gt;         27\n\n## Ex create a summary table between manufacturer and drv\ntable(mpg$drv, mpg$manufacturer)\n#&gt;    \n#&gt;     audi chevrolet dodge ford honda hyundai jeep land rover lincoln mercury\n#&gt;   4   11         4    26   13     0       0    8          4       0       4\n#&gt;   f    7         5    11    0     9      14    0          0       0       0\n#&gt;   r    0        10     0   12     0       0    0          0       3       0\n#&gt;    \n#&gt;     nissan pontiac subaru toyota volkswagen\n#&gt;   4      4       0     14     15          0\n#&gt;   f      9       5      0     19         27\n#&gt;   r      0       0      0      0          0\n\nThe first line reads a CSV file. It uses the table() function to generate a contingency table (cross-tabulation) between two categorical variables: drv (drive) and manufacturer. It essentially counts how many times each combination of drv and manufacturer appears in the dataset.\n\n## Get the percentage summary using prop.table\nprop.table(table(mpg$drv, mpg$manufacturer), margin = 2)\n#&gt;    \n#&gt;          audi chevrolet     dodge      ford     honda   hyundai      jeep\n#&gt;   4 0.6111111 0.2105263 0.7027027 0.5200000 0.0000000 0.0000000 1.0000000\n#&gt;   f 0.3888889 0.2631579 0.2972973 0.0000000 1.0000000 1.0000000 0.0000000\n#&gt;   r 0.0000000 0.5263158 0.0000000 0.4800000 0.0000000 0.0000000 0.0000000\n#&gt;    \n#&gt;     land rover   lincoln   mercury    nissan   pontiac    subaru    toyota\n#&gt;   4  1.0000000 0.0000000 1.0000000 0.3076923 0.0000000 1.0000000 0.4411765\n#&gt;   f  0.0000000 0.0000000 0.0000000 0.6923077 1.0000000 0.0000000 0.5588235\n#&gt;   r  0.0000000 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#&gt;    \n#&gt;     volkswagen\n#&gt;   4  0.0000000\n#&gt;   f  1.0000000\n#&gt;   r  0.0000000\n## margin = 1 sum across row, 2 across col\n\nThis code calculates the column-wise proportion (as percentages) for each combination of drv and manufacturer. The prop.table() function is used to compute the proportions. The margin = 2 argument indicates that the proportions are to be computed across columns (margin = 1 would compute them across rows).\ntableone package\n\n\n\n\n\n\nTip\n\n\n\nCreateTableOne function from tableone package could be a very useful function to see the summary table. Type ?tableone::CreateTableOne to see for more details.\n\n\nThis section introduces the tableone package, which offers the CreateTableOne function. This function helps in creating “Table 1” type summary tables, commonly used in epidemiological studies.\n\nrequire(tableone)\n#&gt; Loading required package: tableone\nCreateTableOne(vars = c(\"cyl\", \"drv\", \"hwy\", \"cty\"), data = mpg, \n               strata = \"trans\", includeNA = TRUE, test = FALSE)\n#&gt;                  Stratified by trans\n#&gt;                   auto(av)       auto(l3)       auto(l4)      auto(l5)     \n#&gt;   n                   5              2             83            39        \n#&gt;   cyl (mean (SD))  5.20 (1.10)    4.00 (0.00)    6.14 (1.62)   6.56 (1.45) \n#&gt;   drv (%)                                                                  \n#&gt;      4                0 (  0.0)      0 (  0.0)     34 (41.0)     29 (74.4) \n#&gt;      f                5 (100.0)      2 (100.0)     37 (44.6)      8 (20.5) \n#&gt;      r                0 (  0.0)      0 (  0.0)     12 (14.5)      2 ( 5.1) \n#&gt;   hwy (mean (SD)) 27.80 (2.59)   27.00 (4.24)   21.96 (5.64)  20.72 (6.04) \n#&gt;   cty (mean (SD)) 20.00 (2.00)   21.00 (4.24)   15.94 (3.98)  14.72 (3.49) \n#&gt;                  Stratified by trans\n#&gt;                   auto(l6)      auto(s4)      auto(s5)      auto(s6)     \n#&gt;   n                   6             3             3            16        \n#&gt;   cyl (mean (SD))  7.33 (1.03)   5.33 (2.31)   6.00 (2.00)   6.00 (1.59) \n#&gt;   drv (%)                                                                \n#&gt;      4                2 (33.3)      2 (66.7)      1 (33.3)      7 (43.8) \n#&gt;      f                2 (33.3)      1 (33.3)      2 (66.7)      8 (50.0) \n#&gt;      r                2 (33.3)      0 ( 0.0)      0 ( 0.0)      1 ( 6.2) \n#&gt;   hwy (mean (SD)) 20.00 (2.37)  25.67 (1.15)  25.33 (6.66)  25.19 (3.99) \n#&gt;   cty (mean (SD)) 13.67 (1.86)  18.67 (2.31)  17.33 (5.03)  17.38 (3.22) \n#&gt;                  Stratified by trans\n#&gt;                   manual(m5)    manual(m6)   \n#&gt;   n                  58            19        \n#&gt;   cyl (mean (SD))  5.00 (1.30)   6.00 (1.76) \n#&gt;   drv (%)                                    \n#&gt;      4               21 (36.2)      7 (36.8) \n#&gt;      f               33 (56.9)      8 (42.1) \n#&gt;      r                4 ( 6.9)      4 (21.1) \n#&gt;   hwy (mean (SD)) 26.29 (5.99)  24.21 (5.75) \n#&gt;   cty (mean (SD)) 19.26 (4.56)  16.89 (3.83)\n\nThe CreateTableOne function is used to create a summary table for the variables cyl, drv, hwy, and cty from the mpg dataset. The strata = trans argument means that the summary is stratified by the trans variable. The includeNA = TRUE argument means that missing values (NAs) are included in the summary. The test = FALSE argument indicates that no statistical tests should be applied to the data (often tests are used to compare groups in the table).\ntable1 package\nThis section introduces another package, table1, which can also be used to create “Table 1” type summary tables.\n\nrequire(table1)\n#&gt; Loading required package: table1\n#&gt; \n#&gt; Attaching package: 'table1'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     units, units&lt;-\ntable1(~ cyl + drv + hwy + cty | trans, data=mpg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nauto(av)(N=5)\nauto(l3)(N=2)\nauto(l4)(N=83)\nauto(l5)(N=39)\nauto(l6)(N=6)\nauto(s4)(N=3)\nauto(s5)(N=3)\nauto(s6)(N=16)\nmanual(m5)(N=58)\nmanual(m6)(N=19)\nOverall(N=234)\n\n\n\ncyl\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n5.20 (1.10)\n4.00 (0)\n6.14 (1.62)\n6.56 (1.45)\n7.33 (1.03)\n5.33 (2.31)\n6.00 (2.00)\n6.00 (1.59)\n5.00 (1.30)\n6.00 (1.76)\n5.89 (1.61)\n\n\nMedian [Min, Max]\n6.00 [4.00, 6.00]\n4.00 [4.00, 4.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n8.00 [6.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n4.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n6.00 [4.00, 8.00]\n\n\ndrv\n\n\n\n\n\n\n\n\n\n\n\n\n\nf\n5 (100%)\n2 (100%)\n37 (44.6%)\n8 (20.5%)\n2 (33.3%)\n1 (33.3%)\n2 (66.7%)\n8 (50.0%)\n33 (56.9%)\n8 (42.1%)\n106 (45.3%)\n\n\n4\n0 (0%)\n0 (0%)\n34 (41.0%)\n29 (74.4%)\n2 (33.3%)\n2 (66.7%)\n1 (33.3%)\n7 (43.8%)\n21 (36.2%)\n7 (36.8%)\n103 (44.0%)\n\n\nr\n0 (0%)\n0 (0%)\n12 (14.5%)\n2 (5.1%)\n2 (33.3%)\n0 (0%)\n0 (0%)\n1 (6.3%)\n4 (6.9%)\n4 (21.1%)\n25 (10.7%)\n\n\nhwy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n27.8 (2.59)\n27.0 (4.24)\n22.0 (5.64)\n20.7 (6.04)\n20.0 (2.37)\n25.7 (1.15)\n25.3 (6.66)\n25.2 (3.99)\n26.3 (5.99)\n24.2 (5.75)\n23.4 (5.95)\n\n\nMedian [Min, Max]\n27.0 [25.0, 31.0]\n27.0 [24.0, 30.0]\n22.0 [14.0, 41.0]\n19.0 [12.0, 36.0]\n19.0 [18.0, 23.0]\n25.0 [25.0, 27.0]\n27.0 [18.0, 31.0]\n26.0 [18.0, 29.0]\n26.0 [16.0, 44.0]\n26.0 [12.0, 32.0]\n24.0 [12.0, 44.0]\n\n\ncty\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n20.0 (2.00)\n21.0 (4.24)\n15.9 (3.98)\n14.7 (3.49)\n13.7 (1.86)\n18.7 (2.31)\n17.3 (5.03)\n17.4 (3.22)\n19.3 (4.56)\n16.9 (3.83)\n16.9 (4.26)\n\n\nMedian [Min, Max]\n19.0 [18.0, 23.0]\n21.0 [18.0, 24.0]\n16.0 [11.0, 29.0]\n14.0 [9.00, 25.0]\n13.0 [12.0, 16.0]\n20.0 [16.0, 20.0]\n18.0 [12.0, 22.0]\n17.0 [12.0, 22.0]\n19.0 [11.0, 35.0]\n16.0 [9.00, 23.0]\n17.0 [9.00, 35.0]\n\n\n\n\n\n\nThe table1() function is used to generate a summary table for the specified variables. The formula-like syntax (~ cyl + drv + hwy + cty | trans) indicates that the summary should be stratified by the trans variable.",
    "crumbs": [
      "Data wrangling",
      "Summary tables"
    ]
  },
  {
    "objectID": "wrangling6.html",
    "href": "wrangling6.html",
    "title": "R Markdown",
    "section": "",
    "text": "Introduction\nWelcome to this tutorial on working with RMD files in RStudio! RStudio is an IDE that makes R programming easier and more efficient, while R Markdown (RMD) is a file format that enables you to create dynamic reports with R code and narrative text. Using R Markdown within RStudio allows you to compile your analyses and reports into a single, easily shareable document in multiple formats like HTML, PDF, or Word.",
    "crumbs": [
      "Data wrangling",
      "R Markdown"
    ]
  },
  {
    "objectID": "wrangling6.html#introduction",
    "href": "wrangling6.html#introduction",
    "title": "R Markdown",
    "section": "",
    "text": "RStudio is an Integrated Development Environment (IDE), which is a software application that provides comprehensive facilities for software development. An IDE typically includes a text editor, tools for building and running code, and debugging utilities.",
    "crumbs": [
      "Data wrangling",
      "R Markdown"
    ]
  },
  {
    "objectID": "wrangling6.html#prerequisites",
    "href": "wrangling6.html#prerequisites",
    "title": "R Markdown",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this tutorial, make sure you have both R and RStudio installed on your computer, as explaied in an earlier tutorial.",
    "crumbs": [
      "Data wrangling",
      "R Markdown"
    ]
  },
  {
    "objectID": "wrangling6.html#knitting-rmd",
    "href": "wrangling6.html#knitting-rmd",
    "title": "R Markdown",
    "section": "Knitting RMD",
    "text": "Knitting RMD\nThis document shows how to work with an RMD file. We can create dynamic documents, e.g., a document with simple plain text combined with R code and its outputs. Note that RMD files are designed to be used with the R package rmarkdown. In RStudio IDE, the rmarkdown package could be already installed.\n\n# install.packages(\"rmarkdown\")\n\nOpen on RStudio\nLet us open an RMD file in RStudio. From the file menu (on the side), we can create a new RMD document. First, we need to click on the + symbol to create a new file, as follows:\n\n\n\n\n\n\n\n\nSecond, we need to click R markdown... to create a new RMD document as follows:\n\n\n\n\n\n\n\n\nWe will see a pop-up window as follows:\n\n\n\n\n\n\n\n\nWe can select whether we want to convert our RMD file to an HTML, PDF, or a Word document. These options can also be selected later. Let us select the default option (HTML) and press OK. We will see the markdown file as shown in the picture below:\n\n\n\n\n\n\n\n\nKnit\nWe use the knit option to create a document (e.g., making a PDF, HTML, or Word document) from the RMD file. Before knitting, we need to save the file. Let us save the file as working.RMD. After saving the file, we can knit it by clicking on the knit option, as shown below:\n\n\nThe term “knit” may sound a bit strange in the context of programming. However, it aptly describes the process of combining your R code and narrative text to produce a cohesive, final document. Think of it as “weaving” your code and text together into various output formats like HTML, PDF, or Word.\n\n\n\n\n\n\n\n\nOnce we knit the file, it will produce an HTML output, since our default option was HTML.\n\n\n\n\n\n\n\n\nFor formats other than HTML (e.g., PDF or Word), we can click on the dropdown menu:\n\n\n\n\n\n\n\n\nLet us select Knit to Word and knit it. Once the file is rendered, RStudio will show us a preview of the output in a word file and save the file in our working directory. We can also see that Word is added as another output:\n\n\nWhen you’re working in RStudio, all your files and outputs will be saved in a ‘working directory.’ This is simply the folder on your computer where RStudio will look for files and save outputs. To find out what your current working directory is, you can run the command getwd() in the R console.\n\n\n\n\n\n\n\n\nIn the R terminal, we can see that a Word document is created, which is stored in our working directory:\n\n\n\n\n\n\n\n\nSimilarly, we can create a pdf by clicking Knit to PDF option from the Knit menu. However, we could see an error message as follows:\n\n\n\n\n\n\n\n\nIt is important to note that RStudio does not build PDF documents from scratch. If we want to create PDF documents using RMD, we must have a LaTeX distribution installed on our computer. There are several options for LaTeX distributions, including MiKTeX, MacTeX, TeX Live, and so on. However, the recommended option for R Markdown users is TinyTeX. We can install TinyTeX using the R package tinytex. To install the package, run the following command: install.packages(\"tinytex\").\n\n\nLaTeX is a typesetting system commonly used for technical and scientific documentation. It is required for converting R Markdown documents to PDF format because it provides the text formatting commands that the rmarkdown package uses behind the scenes.\n\n\n\n\n\n\n\n\nOnce the tinytex package installation is complete, we can type tinytex::install_tinytex() to install the LaTeX distribution on our computer.\n\n\n\n\n\n\n\n\nTinyTeX is a large package (~123 MB). The installation time will vary depending on your machine. Once the installation is complete, we can click Knit to PDF. Similar to the Word file, RStudio will display a preview of the PDF output and save the PDF in our working directory. We will also see that a PDF file has been created:\n\n\n\n\n\n\n\n\nWorking with RMD\nNow we are ready to start writing plain text intermixed with embedded R code. For plain text, we can use the whitespace:\n\n\n\n\n\n\n\n\nOn the other hand, to embed a chunk of R code into our report, we use R code chunks. An R chunk surrounds the code with two lines that each contain three backticks. After the first set of backticks, we include {r}, which alerts knitr that we are going to include a chunk of R code:\n\n\nCode chunks are segments of code that are contained within an R Markdown document. They allow you to run R code within the document itself, making your report dynamic and reproducible.\n\n\n\n\n\n\n\n\nBelow are some codes:\n\n\n\n\n\n\n\n\nWe can knit the file to see the document, which will include plain text, R code, and outputs from the R code. We can also see the output from a code chunk without knitting the entire file. For example, we can click the arrow on the right-hand side to execute the current code chunk:\n\n\n\n\n\n\n\n\nNow we can see the following outputs:\n\n\n\n\n\n\n\n\nPlease also explore the drop down menu under Run to see the further options, including run the current code chunk, run all code chunk above, etc.\n\n\n\n\n\n\n\n\nTo omit the code from the final report while still including the results, add the argument echo = FALSE. This will place a copy of the results into your report.\n\n\nBesides echo = FALSE, there are several other options you can include in your code chunks to control their behavior, like eval = FALSE if you don’t want to evaluate the code, or message = FALSE to hide messages. Take a look at the author’s page of comprehensive list of chunk options.\nIn the final report (e.g., Word or PDF), we often want to omit the code and only show the outputs. To do this, we added the argument echo = FALSE in the R code chunk:\n\n\n\n\n\n\n\n\nThe resulting output will look as follows:",
    "crumbs": [
      "Data wrangling",
      "R Markdown"
    ]
  },
  {
    "objectID": "wrangling6.html#tips-and-troubleshooting",
    "href": "wrangling6.html#tips-and-troubleshooting",
    "title": "R Markdown",
    "section": "Tips and Troubleshooting",
    "text": "Tips and Troubleshooting\n\nIf the knit button is grayed out, make sure you have saved your RMD file first.\nEncountering LaTeX errors? Make sure you’ve installed a LaTeX distribution like TinyTeX.\n\n\n\n\n\n\n\nTip\n\n\n\nThe following links could also be useful if you want to learn more:\n\nR Markdown Cheat Sheet\nIntroduction to R Markdown\nR Markdown: The Definitive Guide\nReports with R Markdown\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Data wrangling",
      "R Markdown"
    ]
  },
  {
    "objectID": "wranglingF.html",
    "href": "wranglingF.html",
    "title": "R Functions (W)",
    "section": "",
    "text": "Note\n\n\n\nThis review/summary page provides an extensive list of R functions tailored for data wrangling tasks that we have used in this chapter. Each function is systematically described, highlighting its primary package source and its specific utility.\n\n\nTo learn more about these functions, readers can:\n\nUse R’s Built-in Help System: For each function, access its documentation by prefixing the function name with a question mark in the R console, e.g., ?as.factor. This displays the function’s manual page with descriptions, usage, and examples.\nSearch Websites: Simply Google, or visit the CRAN website to search for specific function documentation. Websites like Stack Overflow and RStudio Community often have discussions related to R functions.\nTutorials and Online Courses: Platforms like DataCamp, Coursera, and edX offer R courses that cover many functions in depth. Also there are examples of dedicated R tutorial websites that you might find useful. One example is “Introduction to R for health data analysis” by Ehsan Karim, An Hoang and Qu.\nBooks: There are numerous R programming books, such as “R for Data Science” by Hadley Wickham and “The Art of R Programming” by Norman Matloff.\nWorkshops and Webinars: Institutions and organizations occasionally offer R programming workshops or webinars.\n\nWhenever in doubt, exploring existing resources can be highly beneficial.\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nas.factor\nbase\nConverts a variable to factors. `as.factor` is a wrapper for the `factor` function.\n\n\ncbind\nbase\nMerges matrices.\n\n\nCreateTableOne\ntableone\nCreates a frequency table.\n\n\ndata.frame\nbase\nCreates a dataset with both numeric and character variables. Requires unique column names and equal length for all variables.\n\n\ndim\nbase\nReturns the dimensions of a data frame (rows x columns).\n\n\nfilter\ndplyr\nSubsets a dataset by selecting a sub-population.\n\n\nfunction\nbase\nUsed to define custom functions, e.g., for calculating standard deviation.\n\n\nhead\nbase\nDisplays the first six elements of an object (e.g., a dataset). `tail` displays the last six.\n\n\nis.na\nbase\nChecks for missing values in a variable.\n\n\nlevels\nbase\nDisplays the levels of a factor variable.\n\n\nlist\nbase\nStores vectors, matrices, or lists of differing types.\n\n\nmode\nbase\nDetermines the type of a variable.\n\n\nna.omit\nbase/stats\nRemoves all rows with missing values from a dataset.\n\n\nnames\nbase\nDisplays names of objects, e.g., variable names of a data frame.\n\n\nnlevels\nbase\nShows the number of levels in a factor variable.\n\n\nnrow\nbase\nReturns the dimensions of a data frame. `nrow` gives row count and `ncol` gives column count.\n\n\nplot\nbase/graphics\nDraws scatter plots or line graphs.\n\n\nprint\nbase\nPrints the output to console.\n\n\nprop.table\nbase\nDisplays percentage summary for a table.\n\n\nrbind\nbase\nAppends matrices row-wise.\n\n\nread.csv\nbase/utils\nReads data from a CSV file.\n\n\nrelevel\nbase/stats\nChanges the reference group of a factor variable.\n\n\nsasxport.get\nHmisc\nLoads data in the SAS format.\n\n\nsave\nbase\nSaves R objects, such as datasets.\n\n\nselect\ndplyr\nSelects specified variables from a dataset.\n\n\nset.seed\nbase\nSets a seed for random number generation ensuring reproducibility.\n\n\nstr\nbase/utils\nDisplays the structure of a dataset, including data type of variables.\n\n\nsubset\nbase, dplyr\nSubsets a dataset by selecting a sub-population.\n\n\nsummary\nbase\nProvides a summary of an object, like variable statistics.\n\n\ntable\nbase\nDisplays frequency counts for a variable.\n\n\nwrite.csv\nbase/utils\nSaves a data frame to a CSV file in a specified directory.",
    "crumbs": [
      "Data wrangling",
      "R Functions (W)"
    ]
  },
  {
    "objectID": "wranglingQ.html",
    "href": "wranglingQ.html",
    "title": "Quiz (W)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Data wrangling",
      "Quiz (W)"
    ]
  },
  {
    "objectID": "wranglingQ.html#live-quiz",
    "href": "wranglingQ.html#live-quiz",
    "title": "Quiz (W)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Data wrangling",
      "Quiz (W)"
    ]
  },
  {
    "objectID": "wranglingQ.html#download-quiz",
    "href": "wranglingQ.html#download-quiz",
    "title": "Quiz (W)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here. If not downloading immediately, right-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Data wrangling",
      "Quiz (W)"
    ]
  },
  {
    "objectID": "wranglingS.html",
    "href": "wranglingS.html",
    "title": "App (W)",
    "section": "",
    "text": "Below is an example of an app that utilizes the mpg dataset from the ggplot2 package following the tutorial materials. Users can subset the data and generate a stratified Table 1 from it.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveW\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, tableone and ggplot2 packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Data wrangling",
      "App (W)"
    ]
  },
  {
    "objectID": "wranglingE.html",
    "href": "wranglingE.html",
    "title": "Exercise 1 (W)",
    "section": "",
    "text": "Problem Statement\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nYou can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\n# Load required packages\nlibrary(dplyr)\nlibrary(tableone)\n# Data import: name it rhc\n#rhc &lt;- ...(\"Data/wrangling/rhc.csv\", ...)",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingE.html#problem-1-basic-manipulation-60",
    "href": "wranglingE.html#problem-1-basic-manipulation-60",
    "title": "Exercise 1 (W)",
    "section": "Problem 1: Basic Manipulation [60%]",
    "text": "Problem 1: Basic Manipulation [60%]\n\nContinuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\nSet reference: Change the reference category for gender to Male\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\ncomorbidities:\n\n\nCreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nReport maximum and minimum values of numcom:\n\n\n# See head of comorbidities\n# head(rhc[,c(\"cardiohx\", \"chfhx\", \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n#             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \"transhx\", \"amihx\")])\n\n# your codes here\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\nage, sex, race, cat1, ca, dnr1, aps1, surv2md1, numcom, adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21, ph1, crea1, alb1, scoma1, swang1\nname the dataset as rhc2",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingE.html#problem-2-table-1-10",
    "href": "wranglingE.html#problem-2-table-1-10",
    "title": "Exercise 1 (W)",
    "section": "Problem 2: Table 1 [10%]",
    "text": "Problem 2: Table 1 [10%]\nRe-produce the sample table 1 from the rhc2 data (see the Table below). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\nNo RHC\nRHC\n\n\n\nn\n3551\n2184\n\n\nage (%)\n\n\n\n\n   [-Inf,50)\n   884 (24.9)\n   540 (24.7)\n\n\n   [50,60)\n   546 (15.4)\n   371 (17.0)\n\n\n   [60,70)\n   812 (22.9)\n   577 (26.4)\n\n\n   [70,80)\n   809 (22.8)\n   529 (24.2)\n\n\n   [80, Inf)\n   500 (14.1)\n   167 ( 7.6)\n\n\nsex = Female (%)\n  1637 (46.1)\n   906 (41.5)\n\n\nrace (%)\n\n\n\n\n   white\n  2753 (77.5)\n  1707 (78.2)\n\n\n   black\n   585 (16.5)\n   335 (15.3)\n\n\n   other\n   213 ( 6.0)\n   142 ( 6.5)\n\n\ncat1 (%)\n\n\n\n\n   ARF\n  1581 (44.5)\n   909 (41.6)\n\n\n   CHF\n   247 ( 7.0)\n   209 ( 9.6)\n\n\n   Other\n   955 (26.9)\n   208 ( 9.5)\n\n\n   MOSF\n   768 (21.6)\n   858 (39.3)\n\n\nca (%)\n\n\n\n\n   None\n  2652 (74.7)\n  1727 (79.1)\n\n\n   Localized (Yes)\n   638 (18.0)\n   334 (15.3)\n\n\n   Metastatic\n   261 ( 7.4)\n   123 ( 5.6)\n\n\ndnr1 = Yes (%)\n   499 (14.1)\n   155 ( 7.1)\n\n\naps1 (mean (SD))\n 50.93 (18.81)\n 60.74 (20.27)\n\n\nsurv2md1 (mean (SD))\n  0.61 (0.19)\n  0.57 (0.20)\n\n\nnumcom (mean (SD))\n  1.52 (1.17)\n  1.48 (1.13)\n\n\nadld3p (mean (SD))\n  1.24 (1.86)\n  1.02 (1.69)\n\n\ndas2d3pc (mean (SD))\n 20.37 (5.48)\n 20.70 (5.03)\n\n\ntemp1 (mean (SD))\n 37.63 (1.74)\n 37.59 (1.83)\n\n\nhrt1 (mean (SD))\n112.87 (40.94)\n118.93 (41.47)\n\n\nmeanbp1 (mean (SD))\n 84.87 (38.87)\n 68.20 (34.24)\n\n\nresp1 (mean (SD))\n 28.98 (13.95)\n 26.65 (14.17)\n\n\nwblc1 (mean (SD))\n 15.26 (11.41)\n 16.27 (12.55)\n\n\npafi1 (mean (SD))\n240.63 (116.66)\n192.43 (105.54)\n\n\npaco21 (mean (SD))\n 39.95 (14.24)\n 36.79 (10.97)\n\n\nph1 (mean (SD))\n  7.39 (0.11)\n  7.38 (0.11)\n\n\ncrea1 (mean (SD))\n  1.92 (2.03)\n  2.47 (2.05)\n\n\nalb1 (mean (SD))\n  3.16 (0.67)\n  2.98 (0.93)\n\n\nscoma1 (mean (SD))\n 22.25 (31.37)\n 18.97 (28.26)",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingE.html#problem-3-table-1-for-subset-10",
    "href": "wranglingE.html#problem-3-table-1-for-subset-10",
    "title": "Exercise 1 (W)",
    "section": "Problem 3: Table 1 for subset [10%]",
    "text": "Problem 3: Table 1 for subset [10%]\nProduce a similar table as Problem 2 but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingE.html#problem-4-considering-eligibility-criteria-20",
    "href": "wranglingE.html#problem-4-considering-eligibility-criteria-20",
    "title": "Exercise 1 (W)",
    "section": "Problem 4: Considering eligibility criteria [20%]",
    "text": "Problem 4: Considering eligibility criteria [20%]\nProduce a similar table as Problem 2 but only for the subjects who meet all of the following eligibility criteria: (i) age is equal to or above 50, (ii) age is below 80 (iii) Glasgow Coma Score is below 61 and (iv) Primary disease categories are either ARF or MOSF. [Hint: droplevels.data.frame can be a useful function]",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingE.html#optional-0",
    "href": "wranglingE.html#optional-0",
    "title": "Exercise 1 (W)",
    "section": "Optional [0%]",
    "text": "Optional [0%]\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\nCount how many NAs does that variable have?\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nVector &lt;- 1:100\n\n#variance.est &lt;- function(?){?}\n\n#variance.est(Vector)\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nvar(Vector)\n#&gt; [1] 841.6667",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 (W)"
    ]
  },
  {
    "objectID": "wranglingEsolution.html",
    "href": "wranglingEsolution.html",
    "title": "Exercise 1 Solution (W)",
    "section": "",
    "text": "Problem 1: Basic Manipulation\nUse the functions we learned in Lab 1 to complete Lab 1 Exercise. We will use Right Heart Catheterization Dataset saved in the folder named ‘Data/wrangling/’. The variable list and description can be accessed from Vanderbilt Biostatistics website.\nA paper you can access the original table from this paper (doi: 10.1001/jama.1996.03540110043030). We have modified the table and corrected some issues. Please knit your file once you finished and submit the knitted file ONLY.\nrhc$age &lt;- cut(rhc$age, c(-Inf, 50, 60, 70, 80, Inf), right = F)\ntable(rhc$age)\n#&gt; \n#&gt; [-Inf,50)   [50,60)   [60,70)   [70,80) [80, Inf) \n#&gt;      1424       917      1389      1338       667\nrhc$race &lt;- factor(rhc$race, levels = c(\"white\", \"black\", \"other\"))\nlevels(rhc$race)\n#&gt; [1] \"white\" \"black\" \"other\"\nrhc$sex &lt;- as.factor(rhc$sex)\nrhc$sex &lt;- relevel(rhc$sex, ref = \"Male\")\nlevels(rhc$sex)\n#&gt; [1] \"Male\"   \"Female\"\nnlevels(as.factor(rhc$cat1)) # There are nine levels\n#&gt; [1] 9\n\nrhc$cat1 &lt;- as.factor(rhc$cat1)\nlevels(rhc$cat1) &lt;- list(ARF = \"ARF\", CHF = \"CHF\", \n                         MOSF = c(\"MOSF w/Malignancy\", \"MOSF w/Sepsis\"),\n                         Other = c(\"Cirrhosis\", \"Colon Cancer\", \"Coma\", \"COPD\", \n                                   \"Lung Cancer\"))\nrhc$cat1 &lt;- factor(rhc$cat1, levels = c(\"ARF\", \"CHF\", \"MOSF\", \"Other\"))\nlevels(rhc$cat1)\n#&gt; [1] \"ARF\"   \"CHF\"   \"MOSF\"  \"Other\"\nrhc$ca &lt;- as.factor(rhc$ca)\nlevels(rhc$ca) &lt;- list(Metastatic = \"Metastatic\", None = \"No\", \"Localized (Yes)\" = \"Yes\")\nrhc$ca &lt;- factor(rhc$ca, levels = c(\"None\", \"Localized (Yes)\", \"Metastatic\"))\nlevels(rhc$ca)\n#&gt; [1] \"None\"            \"Localized (Yes)\" \"Metastatic\"\n# See head of comorbidities\n# head(rhc[,c(\"cardiohx\", \"chfhx\", \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n#             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \"transhx\", \"amihx\")])\n\n# number of comorbidities\nrhc$numcom &lt;- rowSums(rhc[,c(\"cardiohx\", \"chfhx\", \"dementhx\", \"psychhx\", \"chrpulhx\", \n                         \"renalhx\", \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n                         \"transhx\", \"amihx\") ])\n\n# maximum and minimum\ncbind(Maximum = max(rhc$numcom), Minimum = min(rhc$numcom))\n#&gt;      Maximum Minimum\n#&gt; [1,]       6       0\nrhc2 &lt;- select(rhc, \"age\", \"sex\", \"race\",\"cat1\", \"ca\", \"dnr1\", \"aps1\", \"surv2md1\", \n               \"numcom\", \"adld3p\", \"das2d3pc\", \"temp1\", \"hrt1\", \"meanbp1\", \"resp1\", \n               \"wblc1\", \"pafi1\", \"paco21\", \"ph1\", \"crea1\", \"alb1\", \"scoma1\", \"swang1\")",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 Solution (W)"
    ]
  },
  {
    "objectID": "wranglingEsolution.html#problem-1-basic-manipulation",
    "href": "wranglingEsolution.html#problem-1-basic-manipulation",
    "title": "Exercise 1 Solution (W)",
    "section": "",
    "text": "Continuous to Categories: Change the Age variable into categories below 50, 50 to below 60, 60 to below 70, 70 to below 80, 80 and above [Hint: the cut function could be helpful]\n\n\n\nRe-order: Re-order the levels of race to white, black and other\n\n\n\nSet reference: Change the reference category for gender to Male\n\n\n\nCount levels: Check how many levels does the variable “cat1” (Primary disease category) have? Regroup the levels for disease categories to “ARF”,“CHF”,“MOSF”,“Other”. [Hint: the nlevels and list functions could be helpful]\n\n\n\nRename levels: Rename the levels of “ca” (Cancer) to “Metastatic”,“None” and “Localized (Yes)”, then re-order the levels to “None”,“Localized (Yes)” and “Metastatic”\n\n\n\ncomorbidities:\n\n\nCreate a new variable called “numcom” to count number of comorbidities illness for each person (12 categories) [Hint: the rowSums command could be helpful],\nReport maximum and minimum values of numcom:\n\n\n\nAnlaytic data: Create a dataset that has only the following variables\n\n\nage, sex, race, cat1, ca, dnr1, aps1, surv2md1, numcom, adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21, ph1, crea1, alb1, scoma1, swang1\nname the dataset as rhc2",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 Solution (W)"
    ]
  },
  {
    "objectID": "wranglingEsolution.html#problem-2-table-1",
    "href": "wranglingEsolution.html#problem-2-table-1",
    "title": "Exercise 1 Solution (W)",
    "section": "Problem 2: Table 1",
    "text": "Problem 2: Table 1\nRe-produce the sample table 1 from the rhc2 data (see the Table below). In your table, the variables should be ordered as the same as the sample. Please re-level or re-order the levels if needed. [Hint: the tableone package might be useful]\n\n\n\nNo RHC\nRHC\n\n\n\nn\n3551\n2184\n\n\nage (%)\n\n\n\n\n   [-Inf,50)\n   884 (24.9)\n   540 (24.7)\n\n\n   [50,60)\n   546 (15.4)\n   371 (17.0)\n\n\n   [60,70)\n   812 (22.9)\n   577 (26.4)\n\n\n   [70,80)\n   809 (22.8)\n   529 (24.2)\n\n\n   [80, Inf)\n   500 (14.1)\n   167 ( 7.6)\n\n\nsex = Female (%)\n  1637 (46.1)\n   906 (41.5)\n\n\nrace (%)\n\n\n\n\n   white\n  2753 (77.5)\n  1707 (78.2)\n\n\n   black\n   585 (16.5)\n   335 (15.3)\n\n\n   other\n   213 ( 6.0)\n   142 ( 6.5)\n\n\ncat1 (%)\n\n\n\n\n   ARF\n  1581 (44.5)\n   909 (41.6)\n\n\n   CHF\n   247 ( 7.0)\n   209 ( 9.6)\n\n\n   Other\n   955 (26.9)\n   208 ( 9.5)\n\n\n   MOSF\n   768 (21.6)\n   858 (39.3)\n\n\nca (%)\n\n\n\n\n   None\n  2652 (74.7)\n  1727 (79.1)\n\n\n   Localized (Yes)\n   638 (18.0)\n   334 (15.3)\n\n\n   Metastatic\n   261 ( 7.4)\n   123 ( 5.6)\n\n\ndnr1 = Yes (%)\n   499 (14.1)\n   155 ( 7.1)\n\n\naps1 (mean (SD))\n 50.93 (18.81)\n 60.74 (20.27)\n\n\nsurv2md1 (mean (SD))\n  0.61 (0.19)\n  0.57 (0.20)\n\n\nnumcom (mean (SD))\n  1.52 (1.17)\n  1.48 (1.13)\n\n\nadld3p (mean (SD))\n  1.24 (1.86)\n  1.02 (1.69)\n\n\ndas2d3pc (mean (SD))\n 20.37 (5.48)\n 20.70 (5.03)\n\n\ntemp1 (mean (SD))\n 37.63 (1.74)\n 37.59 (1.83)\n\n\nhrt1 (mean (SD))\n112.87 (40.94)\n118.93 (41.47)\n\n\nmeanbp1 (mean (SD))\n 84.87 (38.87)\n 68.20 (34.24)\n\n\nresp1 (mean (SD))\n 28.98 (13.95)\n 26.65 (14.17)\n\n\nwblc1 (mean (SD))\n 15.26 (11.41)\n 16.27 (12.55)\n\n\npafi1 (mean (SD))\n240.63 (116.66)\n192.43 (105.54)\n\n\npaco21 (mean (SD))\n 39.95 (14.24)\n 36.79 (10.97)\n\n\nph1 (mean (SD))\n  7.39 (0.11)\n  7.38 (0.11)\n\n\ncrea1 (mean (SD))\n  1.92 (2.03)\n  2.47 (2.05)\n\n\nalb1 (mean (SD))\n  3.16 (0.67)\n  2.98 (0.93)\n\n\nscoma1 (mean (SD))\n 22.25 (31.37)\n 18.97 (28.26)\n\n\n\n\nrhc2$cat1 &lt;- factor(rhc2$cat1, levels = c(\"ARF\", \"CHF\", \"Other\", \"MOSF\"))\n\nvars &lt;- c(\"age\", \"sex\", \"race\", \"cat1\", \"ca\", \"dnr1\", \"aps1\", \"surv2md1\", \"numcom\", \n          \"adld3p\", \"das2d3pc\", \"temp1\", \"hrt1\", \"meanbp1\", \"resp1\", \"wblc1\", \n          \"pafi1\", \"paco21\", \"ph1\", \"crea1\", \"alb1\", \"scoma1\")\n\nlibrary(tableone)\ntab1a &lt;- CreateTableOne(vars = vars, data = rhc2, strata = \"swang1\", includeNA = TRUE, \n                        test = F)\nprint(tab1a)\n#&gt;                       Stratified by swang1\n#&gt;                        No RHC          RHC            \n#&gt;   n                      3551            2184         \n#&gt;   age (%)                                             \n#&gt;      [-Inf,50)            884 (24.9)      540 (24.7)  \n#&gt;      [50,60)              546 (15.4)      371 (17.0)  \n#&gt;      [60,70)              812 (22.9)      577 (26.4)  \n#&gt;      [70,80)              809 (22.8)      529 (24.2)  \n#&gt;      [80, Inf)            500 (14.1)      167 ( 7.6)  \n#&gt;   sex = Female (%)       1637 (46.1)      906 (41.5)  \n#&gt;   race (%)                                            \n#&gt;      white               2753 (77.5)     1707 (78.2)  \n#&gt;      black                585 (16.5)      335 (15.3)  \n#&gt;      other                213 ( 6.0)      142 ( 6.5)  \n#&gt;   cat1 (%)                                            \n#&gt;      ARF                 1581 (44.5)      909 (41.6)  \n#&gt;      CHF                  247 ( 7.0)      209 ( 9.6)  \n#&gt;      Other                955 (26.9)      208 ( 9.5)  \n#&gt;      MOSF                 768 (21.6)      858 (39.3)  \n#&gt;   ca (%)                                              \n#&gt;      None                2652 (74.7)     1727 (79.1)  \n#&gt;      Localized (Yes)      638 (18.0)      334 (15.3)  \n#&gt;      Metastatic           261 ( 7.4)      123 ( 5.6)  \n#&gt;   dnr1 = Yes (%)          499 (14.1)      155 ( 7.1)  \n#&gt;   aps1 (mean (SD))      50.93 (18.81)   60.74 (20.27) \n#&gt;   surv2md1 (mean (SD))   0.61 (0.19)     0.57 (0.20)  \n#&gt;   numcom (mean (SD))     1.52 (1.17)     1.48 (1.13)  \n#&gt;   adld3p (mean (SD))     1.24 (1.86)     1.02 (1.69)  \n#&gt;   das2d3pc (mean (SD))  20.37 (5.48)    20.70 (5.03)  \n#&gt;   temp1 (mean (SD))     37.63 (1.74)    37.59 (1.83)  \n#&gt;   hrt1 (mean (SD))     112.87 (40.94)  118.93 (41.47) \n#&gt;   meanbp1 (mean (SD))   84.87 (38.87)   68.20 (34.24) \n#&gt;   resp1 (mean (SD))     28.98 (13.95)   26.65 (14.17) \n#&gt;   wblc1 (mean (SD))     15.26 (11.41)   16.27 (12.55) \n#&gt;   pafi1 (mean (SD))    240.63 (116.66) 192.43 (105.54)\n#&gt;   paco21 (mean (SD))    39.95 (14.24)   36.79 (10.97) \n#&gt;   ph1 (mean (SD))        7.39 (0.11)     7.38 (0.11)  \n#&gt;   crea1 (mean (SD))      1.92 (2.03)     2.47 (2.05)  \n#&gt;   alb1 (mean (SD))       3.16 (0.67)     2.98 (0.93)  \n#&gt;   scoma1 (mean (SD))    22.25 (31.37)   18.97 (28.26)",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 Solution (W)"
    ]
  },
  {
    "objectID": "wranglingEsolution.html#problem-3-table-1-for-subset",
    "href": "wranglingEsolution.html#problem-3-table-1-for-subset",
    "title": "Exercise 1 Solution (W)",
    "section": "Problem 3: Table 1 for subset",
    "text": "Problem 3: Table 1 for subset\nProduce a similar table as Problem 2 but with only male sex and ARF primary disease category (cat1). Add the overall column in the same table. [Hint: filter command could be useful]\n\nrhc2.M &lt;- filter(rhc2, sex == \"Male\" & cat1 == \"ARF\")\nvars &lt;- c(\"age\", \"race\", \"ca\", \"dnr1\", \"aps1\", \"surv2md1\", \"numcom\", \"adld3p\",\n          \"das2d3pc\", \"temp1\", \"hrt1\", \"meanbp1\", \"resp1\", \"wblc1\", \"pafi1\",\n          \"paco21\", \"ph1\", \"crea1\", \"alb1\", \"scoma1\")\n\ntab1b &lt;- CreateTableOne(vars = vars, data = rhc2.M, strata = \"swang1\", includeNA = T, \n               addOverall = T, test = F)\nprint(tab1b)\n#&gt;                       Stratified by swang1\n#&gt;                        Overall         No RHC          RHC           \n#&gt;   n                      1382             888             494        \n#&gt;   age (%)                                                            \n#&gt;      [-Inf,50)            382 (27.6)      267 (30.1)      115 (23.3) \n#&gt;      [50,60)              198 (14.3)      127 (14.3)       71 (14.4) \n#&gt;      [60,70)              299 (21.6)      174 (19.6)      125 (25.3) \n#&gt;      [70,80)              340 (24.6)      201 (22.6)      139 (28.1) \n#&gt;      [80, Inf)            163 (11.8)      119 (13.4)       44 ( 8.9) \n#&gt;   race (%)                                                           \n#&gt;      white               1116 (80.8)      700 (78.8)      416 (84.2) \n#&gt;      black                192 (13.9)      141 (15.9)       51 (10.3) \n#&gt;      other                 74 ( 5.4)       47 ( 5.3)       27 ( 5.5) \n#&gt;   ca (%)                                                             \n#&gt;      None                1068 (77.3)      670 (75.5)      398 (80.6) \n#&gt;      Localized (Yes)      253 (18.3)      173 (19.5)       80 (16.2) \n#&gt;      Metastatic            61 ( 4.4)       45 ( 5.1)       16 ( 3.2) \n#&gt;   dnr1 = Yes (%)          136 ( 9.8)      105 (11.8)       31 ( 6.3) \n#&gt;   aps1 (mean (SD))      54.51 (18.94)   51.90 (18.21)   59.20 (19.34)\n#&gt;   surv2md1 (mean (SD))   0.61 (0.17)     0.63 (0.17)     0.59 (0.17) \n#&gt;   numcom (mean (SD))     1.34 (1.16)     1.32 (1.16)     1.38 (1.15) \n#&gt;   adld3p (mean (SD))     1.00 (1.79)     1.00 (1.78)     1.01 (1.80) \n#&gt;   das2d3pc (mean (SD))  21.74 (5.62)    21.67 (5.72)    21.87 (5.44) \n#&gt;   temp1 (mean (SD))     37.96 (1.71)    38.02 (1.69)    37.84 (1.76) \n#&gt;   hrt1 (mean (SD))     115.96 (39.26)  115.52 (39.39)  116.76 (39.06)\n#&gt;   meanbp1 (mean (SD))   79.08 (36.38)   83.69 (36.81)   70.80 (34.11)\n#&gt;   resp1 (mean (SD))     29.01 (14.35)   30.27 (14.21)   26.73 (14.33)\n#&gt;   wblc1 (mean (SD))     15.80 (12.03)   15.92 (11.50)   15.58 (12.93)\n#&gt;   pafi1 (mean (SD))    188.09 (100.74) 208.05 (102.50) 152.20 (86.70)\n#&gt;   paco21 (mean (SD))    37.45 (10.03)   38.08 (10.56)   36.32 (8.89) \n#&gt;   ph1 (mean (SD))        7.40 (0.10)     7.40 (0.10)     7.39 (0.10) \n#&gt;   crea1 (mean (SD))      2.22 (2.25)     2.10 (2.33)     2.45 (2.09) \n#&gt;   alb1 (mean (SD))       3.07 (0.68)     3.12 (0.66)     2.98 (0.70) \n#&gt;   scoma1 (mean (SD))    18.42 (27.05)   19.48 (28.07)   16.51 (25.02)",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 Solution (W)"
    ]
  },
  {
    "objectID": "wranglingEsolution.html#optional",
    "href": "wranglingEsolution.html#optional",
    "title": "Exercise 1 Solution (W)",
    "section": "Optional",
    "text": "Optional\nOptional 1: Missing values\n\nAny variables included in rhc2 data had missing values? Name that variable. [Hint: apply function could be helpful]\n\n\nany(is.na(rhc2))\n#&gt; [1] TRUE\n\nmiss.var &lt;- apply(rhc2, 2, function(x) any(is.na(x)))\nmiss.var[miss.var == TRUE] # adld3p variable contains missing values\n#&gt; adld3p \n#&gt;   TRUE\n\n\nCount how many NAs does that variable have?\n\n\ntable(is.na(rhc2$adld3p))[2] # Total NAs in adld3p\n#&gt; TRUE \n#&gt; 4296\n\n\nProduce a table 1 for a complete case data (no missing observations) stratified by swang1.\n\n\nrhc.complete &lt;- rhc2[complete.cases(rhc2), ]\nvars &lt;- c(\"age\", \"sex\", \"race\", \"cat1\", \"ca\", \"dnr1\", \"aps1\", \"surv2md1\", \"numcom\", \n          \"adld3p\", \"das2d3pc\", \"temp1\", \"hrt1\", \"meanbp1\", \"resp1\", \"wblc1\", \n          \"pafi1\", \"paco21\", \"ph1\", \"crea1\", \"alb1\", \"scoma1\")\n\nCreateTableOne(vars = vars, data = rhc.complete, strata = \"swang1\", includeNA = F, \n               test = F)\n#&gt;                       Stratified by swang1\n#&gt;                        No RHC          RHC            \n#&gt;   n                      1049             390         \n#&gt;   age (%)                                             \n#&gt;      [-Inf,50)            264 (25.2)      113 (29.0)  \n#&gt;      [50,60)              160 (15.3)       85 (21.8)  \n#&gt;      [60,70)              261 (24.9)       99 (25.4)  \n#&gt;      [70,80)              238 (22.7)       70 (17.9)  \n#&gt;      [80, Inf)            126 (12.0)       23 ( 5.9)  \n#&gt;   sex = Female (%)        480 (45.8)      137 (35.1)  \n#&gt;   race (%)                                            \n#&gt;      white                813 (77.5)      297 (76.2)  \n#&gt;      black                176 (16.8)       67 (17.2)  \n#&gt;      other                 60 ( 5.7)       26 ( 6.7)  \n#&gt;   cat1 (%)                                            \n#&gt;      ARF                  429 (40.9)      127 (32.6)  \n#&gt;      CHF                  174 (16.6)      129 (33.1)  \n#&gt;      Other                266 (25.4)       24 ( 6.2)  \n#&gt;      MOSF                 180 (17.2)      110 (28.2)  \n#&gt;   ca (%)                                              \n#&gt;      None                 797 (76.0)      324 (83.1)  \n#&gt;      Localized (Yes)      171 (16.3)       46 (11.8)  \n#&gt;      Metastatic            81 ( 7.7)       20 ( 5.1)  \n#&gt;   dnr1 = Yes (%)           87 ( 8.3)       11 ( 2.8)  \n#&gt;   aps1 (mean (SD))      48.36 (16.34)   49.38 (19.71) \n#&gt;   surv2md1 (mean (SD))   0.70 (0.15)     0.69 (0.17)  \n#&gt;   numcom (mean (SD))     1.74 (1.22)     1.76 (1.23)  \n#&gt;   adld3p (mean (SD))     1.24 (1.86)     1.02 (1.69)  \n#&gt;   das2d3pc (mean (SD))  20.36 (7.28)    20.36 (6.96)  \n#&gt;   temp1 (mean (SD))     37.35 (1.66)    37.24 (1.61)  \n#&gt;   hrt1 (mean (SD))     112.23 (38.20)  108.66 (39.22) \n#&gt;   meanbp1 (mean (SD))   87.35 (37.97)   70.91 (33.38) \n#&gt;   resp1 (mean (SD))     30.43 (11.65)   25.25 (12.73) \n#&gt;   wblc1 (mean (SD))     14.45 (11.16)   14.75 (13.09) \n#&gt;   pafi1 (mean (SD))    250.90 (112.53) 238.90 (104.11)\n#&gt;   paco21 (mean (SD))    41.77 (14.86)   37.16 (8.57)  \n#&gt;   ph1 (mean (SD))        7.39 (0.10)     7.40 (0.09)  \n#&gt;   crea1 (mean (SD))      2.03 (2.27)     2.22 (2.05)  \n#&gt;   alb1 (mean (SD))       3.26 (0.65)     3.19 (0.64)  \n#&gt;   scoma1 (mean (SD))     5.25 (15.83)    6.54 (17.20)\n\nOptional 2: Calculating variance of a sample\nWrite a function for Bessel’s correction to calculate an unbiased estimate of the population variance from a finite sample (a vector of 100 observations, consisting of numbers from 1 to 100).\n\nVector &lt;- 1:100\n\nvariance.est &lt;- function(Vector){\n  n &lt;- 0\n  sumx &lt;- 0\n  sumsq &lt;- 0\n  for (i in Vector){\n    n &lt;- n + 1\n    sumx &lt;- sumx + i\n    sumsq &lt;- sumsq + i * i\n    varx &lt;- (sumsq - (sumx * sumx) / n) / (n - 1)\n  }\n  return(varx)\n}\n\nvariance.est(Vector)\n#&gt; [1] 841.6667\n\nHint: Take a closer look at the functions, loops and algorithms shown in lab materials. Use a for loop, utilizing the following pseudocode of the algorithm:\n\n\n\n\n\n\n\n\nVerify that estimated variance with the following variance function output in R:\n\nvar(Vector)\n#&gt; [1] 841.6667",
    "crumbs": [
      "Data wrangling",
      "Exercise 1 Solution (W)"
    ]
  },
  {
    "objectID": "accessing.html",
    "href": "accessing.html",
    "title": "Accessing data",
    "section": "",
    "text": "Background\nSurveys serve as a pivotal tool for collecting and evaluating health-related information on a national scale. More often than not, it’s the governmental bodies that take the lead in gathering this data. Recognizing the value of this information, many governments not only compile and analyze these datasets but also ensure they are accessible to the public, especially for research purposes. In this guide, we will delve into an array of survey methodologies, provide illustrative examples, and guide you through the process of downloading pertinent data from both Canadian and American repositories. To make this more tangible, we will conclude with a hands-on example, showcasing how to replicate findings from an academic paper that leveraged one of these publicly available datasets.",
    "crumbs": [
      "Accessing data"
    ]
  },
  {
    "objectID": "accessing.html#background",
    "href": "accessing.html#background",
    "title": "Accessing data",
    "section": "",
    "text": "Now that we are familiar with R basics and data wrangling, we are now dedicating a chapter to accessing and downloading nationally representative survey datasets is pivotal for a hands-on epidemiological tutorial. These datasets, often rich in information and reflective of diverse populations, serve as the backbone for real-world analysis. By guiding learners on how to obtain these datasets, the book ensures that they not only grasp theoretical concepts but also gain practical experience working with authentic, large-scale data. This approach bridges the gap between theory and practice, allowing readers to apply learned techniques on datasets that mirror real-world complexities, thereby enhancing the relevance and applicability of their analytical skills.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Accessing data"
    ]
  },
  {
    "objectID": "accessing.html#overview-of-tutorials",
    "href": "accessing.html#overview-of-tutorials",
    "title": "Accessing data",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nSurvey data sources\nThe tutorial lists primary complex survey data sources, including the Canadian Community Health Survey and National Health and Nutrition Examination Survey, with several offering dedicated R packages for data access.\n\n\nDescriptions of data sources\nThis tutorial provides comprehensive instructions on how to import and process health survey datasets, specifically focusing on the Canadian Community Health Survey (CCHS), National Health and Nutrition Examination Survey (NHANES), and National Health Interview Survey (NHIS).\n\n\nImporting CCHS to R\nThe section provides detailed steps for importing the Canadian Community Health Survey dataset from the UBC library into RStudio, with processing options using SAS, the free software PSPP, and directly in R.\n\n\nImporting NHANES to R\nThe tutorial guides users on how to access and import the NHANES dataset from the CDC website into RStudio, detailing the dataset’s structure and providing methods both manually and using an R package.\n\n\nReproducing results\nThe tutorial guides users through accessing, processing, and analyzing NHANES data to reproduce the results from a referenced article using R code.\n\n\nImporting NHIS to R\nThis chapter serves as a tutorial on accessing and importing the National Health Interview Survey (NHIS) dataset from the US Centers for Disease Control and Prevention (CDC) website into RStudio. The NHIS is an annual cross-sectional survey managed by the CDC, offering insights into population disease prevalence, disability extent, and utilization of health care services. The data files are available in various formats, including ASCII, CSV, and SAS. Users can combine datasets from different years; however, tracing the same individual across cycles is not feasible. The chapter provides step-by-step guidance on downloading the NHIS dataset, particularly the ‘Adult’ data from 2021, verifying the imported data, and merging datasets within the same survey cycle using the unique household identifier.\n\n\nLinking mortality data\nThis tutorial provides instructions on how to link public-use US mortality data with NHANES dataset, with the option to do the same with NHIS. The process involves downloading the mortality data in dat format from the CDC website, loading it into the R environment, and then merging it with the NHANES data using a unique identifier. Various variables related to mortality status, underlying causes of death, and other health-related factors are explained, and Table 1 is created with unweighted and weighted statistics.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThe subsequent chapter on Research Questions serves as a valuable guide for constructing an analytics-driven data set tailored to your specific research queries. It will cover crucial aspects such as the types of variables to collect and how to set eligibility criteria, followed by approaches to data analysis based on your research questions. It’s important to note that research questions can fall into two main categories: predictive or causal. For a deeper understanding of variable selection and analytical tools suited to these types of questions, the chapters on the Roles of Variables and Predictive Models offer insightful guidance.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Accessing data"
    ]
  },
  {
    "objectID": "accessing0.html",
    "href": "accessing0.html",
    "title": "Concepts (A)",
    "section": "",
    "text": "Model-based approach\nThe model-based approach to statistical analysis is heavily reliant on the specification of a probability model for data generation, typically assuming that data come from an infinite population that follows a specific distribution, such as the Normal distribution. Inferences about the population, including point estimates and hypothesis testing, are made based on how well the sample data fit these model assumptions.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#design-based-approach",
    "href": "accessing0.html#design-based-approach",
    "title": "Concepts (A)",
    "section": "Design-based approach",
    "text": "Design-based approach\nThe design-based approach emphasizes the use of sampling methods and the design of the study itself to make inferences about a real/finite population. The design-based approach takes into account the actual structure of the data collection process to make inferences, ensuring that each unit in the population has a known and often non-zero chance of being included in the sample, thus addressing the potential biases and variance issues arising from the sampling design. This approach is critical in understanding and analyzing data from surveys with complex designs, including those with stratification, clustering, and weighting.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#reading-list",
    "href": "accessing0.html#reading-list",
    "title": "Concepts (A)",
    "section": "Reading list",
    "text": "Reading list\nKey reference:\n\n(Heeringa, West, and Berglund 2017) (chapter 2)\n\nOptional reading:\n\n(Lumley 2011) (chapter 1)\n(Vittinghoff et al. 2011) (chapter 12)\n(Bilder and Loughin 2014) (section 6.3)",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#video-lessons",
    "href": "accessing0.html#video-lessons",
    "title": "Concepts (A)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nModel-based approach\n\n\n\nIn statistical inference, one of the primary frameworks is the model-based approach. This method assumes that the data we have collected is a realization of a larger, underlying random process that can be described by a statistical model.\n\nPopulation: The population is considered infinite and governed by a reasonable probability distribution.\nSample: The sample is assumed to be a random sample, with each observation selected with equal probability and being independent of the others.\nGeneralization: The goal is to make inferences about the parameters of the underlying model or the data-generating process, not just the specific finite population from which the sample was drawn.\n\n\nReview materials from pre-requisite statistics courses (optional)\n\n\n\n\n\n\n\n\n\n\n\n\nDesign-Based Approach\n\n\n\nThe analysis of complex survey data is almost always conducted within the design-based framework. This approach is fundamentally different from the model-based approach in its assumptions and goals.\n\nPopulation: The population is viewed as a fixed and finite collection of units (e.g., all non-institutionalized adults in the U.S. at a specific time).\nSample: The sample is drawn using a known probability mechanism, where the probability of selection is known for every individual. Crucially, observations may be dependent on one another due to the sampling design.\nGeneralization: Inference is made about the parameters of the specific finite population from which the sample was drawn. The results are not intended to be generalized to other populations or an abstract data-generating process.\n\nThe key distinction between the two frameworks is the population to which the results can be generalized.\n\n\n\n\n\n\n\n\nFeature\nModel-Based Inference\nDesign-Based Inference\n\n\n\n\nPopulation Assumption\nInfinite; a realization of an underlying random process.\nFixed and finite (e.g., the population of a country).\n\n\nSource of Randomness\nThe assumed statistical model that generates the data.\nThe known, probabilistic sampling mechanism.\n\n\nTarget of Inference\nParameters of the superpopulation model.\nParameters of the finite population.\n\n\n\n\nComplex surveys\nComplex surveys do not use a simple random sample (SRS). Instead, they employ sophisticated, multi-stage sample designs to increase logistical convenience and ensure that specific groups of interest are adequately represented. The main pillars of these designs are :\n\nStratification: The process of dividing the population into distinct subgroups, or “strata,” before sampling begins (e.g., by geography or urban/rural status). This is done to ensure that key groups are represented with reasonable precision and generally works to decrease the standard error of estimates.\nClustering: A technique where natural groupings of individuals (e.g., counties, city blocks) are sampled first. Subsequent sampling then occurs within the selected clusters. This is done primarily for convenience and to reduce data collection costs, but it tends to increase the standard error of estimates because individuals within a cluster are often more similar to each other than to the general population.\nWeighting: A survey weight is assigned to each participant in a complex survey to ensure that the sample is representative of the target population. Conceptually, a respondent’s weight is the number of people in the population that they represent.\n\nIn NHANES, the final interview weight is constructed in a three-step process to account for the complex design: (a) Base Weight (Probability of Selection), (b) Nonresponse Adjustment, (c) Post-stratification. We will learn more about them later.\n\n\nStatistical Inference\nStatistical inference is the process of drawing conclusions about a population from a sample. As reviewed above, this can be done through a model-based or design-based framework. When complex sampling designs are used, the observations are no longer independent and identically distributed (I.I.D.), which is a core assumption of standard statistical methods. Failure to account for the survey design invalidates these methods, rendering the resulting coefficients, p-values, and confidence intervals useless for making valid inferences about the population. Therefore, all analyses must be conducted using specialized, design-based methods that properly account for the survey’s structure.\n\n\nNHANES\nThe National Health and Nutrition Examination Survey (NHANES) is a major program of the National Center for Health Statistics (NCHS) and serves as a primary example throughout this course.\n\nPurpose: NHANES is designed to assess the health and nutritional status of the adult and child population in the United States. It is unique in that it combines in-home interviews with comprehensive physical examinations and laboratory tests conducted in Mobile Examination Centers (MECs).\nHistory: Early surveys were conducted periodically (NHANES I, II, III). Since 1999, NHANES has been a continuous survey, with data released in two-year cycles to ensure stable and reliable estimates.\n\n\n\nNHANES Sampling Design\nThe NHANES sample is not a simple random sample. It uses a complex, four-stage probability sampling design:\n\nStage 1: Primary Sampling Units (PSUs): The U.S. is divided into PSUs, which are typically counties. These PSUs are grouped into strata, and a sample of PSUs is selected from each stratum.\nStage 2: Segments: Each selected PSU is further divided into smaller geographic areas called segments (e.g., city blocks), and a sample of these segments is drawn.\nStage 3: Households: Within each selected segment, a list of all housing units is compiled, and a sample of households is randomly selected.\nStage 4: Individuals: Finally, within each selected household, individuals are randomly chosen from a list of all household members based on specific screening criteria.\n\nFor public-use data files, the true PSU and strata identifiers are masked to protect participant confidentiality. Instead, NHANES provides masked variance pseudo-stratum (SDMVSTRA) and pseudo-PSU (SDMVPSU) variables, which must be used for correct variance estimation.\n\n\nHow to Find NHANES Data from the CDC Website\nWhile the course provides curated datasets, it is also important to know how to find data directly from the source. To find NHANES data on the official Centers for Disease Control and Prevention (CDC) website, follow these steps:\n\nNavigate to the main NHANES page on the CDC website.\nLook for a section titled “Questionnaires, Datasets, and Related Documentation”.\nOn this page, you will find a variable search tool. You can use this tool to search for specific variables by keyword across all survey cycles.\nEach survey cycle (e.g., “NHANES 2017-2018”) will have its own page with links to data files organized by component (Demographics, Dietary, Examination, Laboratory, Questionnaire).\n\n\nWhat is included in this Video Lesson:\n\nModel-based approach review: 0:00\nDesign-based approach: 1:15\nTypes of sampling techniques 6:46\nStatistical inference 8:25\nNHANES 12:02\nSurvey weight 20:40\nCCHS download 23:45\nNHANES download 24:50\nNHANES sampling design 27:24\nHow to find NHANES data from CDC website 27:42\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#complex-surveys",
    "href": "accessing0.html#complex-surveys",
    "title": "Concepts (A)",
    "section": "Complex surveys",
    "text": "Complex surveys\nComplex surveys do not use a simple random sample (SRS). Instead, they employ sophisticated, multi-stage sample designs to increase logistical convenience and ensure that specific groups of interest are adequately represented. The main pillars of these designs are :\n\nStratification: The process of dividing the population into distinct subgroups, or “strata,” before sampling begins (e.g., by geography or urban/rural status). This is done to ensure that key groups are represented with reasonable precision and generally works to decrease the standard error of estimates.\nClustering: A technique where natural groupings of individuals (e.g., counties, city blocks) are sampled first. Subsequent sampling then occurs within the selected clusters. This is done primarily for convenience and to reduce data collection costs, but it tends to increase the standard error of estimates because individuals within a cluster are often more similar to each other than to the general population.\nWeighting: A survey weight is assigned to each participant in a complex survey to ensure that the sample is representative of the target population. Conceptually, a respondent’s weight is the number of people in the population that they represent.\n\nIn NHANES, the final interview weight is constructed in a three-step process to account for the complex design: (a) Base Weight (Probability of Selection), (b) Nonresponse Adjustment, (c) Post-stratification. We will learn more about them later.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#statistical-inference",
    "href": "accessing0.html#statistical-inference",
    "title": "Concepts (A)",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nStatistical inference is the process of drawing conclusions about a population from a sample. As reviewed above, this can be done through a model-based or design-based framework. When complex sampling designs are used, the observations are no longer independent and identically distributed (I.I.D.), which is a core assumption of standard statistical methods. Failure to account for the survey design invalidates these methods, rendering the resulting coefficients, p-values, and confidence intervals useless for making valid inferences about the population. Therefore, all analyses must be conducted using specialized, design-based methods that properly account for the survey’s structure.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#nhanes",
    "href": "accessing0.html#nhanes",
    "title": "Concepts (A)",
    "section": "NHANES",
    "text": "NHANES\nThe National Health and Nutrition Examination Survey (NHANES) is a major program of the National Center for Health Statistics (NCHS) and serves as a primary example throughout this course.\n\nPurpose: NHANES is designed to assess the health and nutritional status of the adult and child population in the United States. It is unique in that it combines in-home interviews with comprehensive physical examinations and laboratory tests conducted in Mobile Examination Centers (MECs).\nHistory: Early surveys were conducted periodically (NHANES I, II, III). Since 1999, NHANES has been a continuous survey, with data released in two-year cycles to ensure stable and reliable estimates.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#nhanes-sampling-design",
    "href": "accessing0.html#nhanes-sampling-design",
    "title": "Concepts (A)",
    "section": "NHANES Sampling Design",
    "text": "NHANES Sampling Design\nThe NHANES sample is not a simple random sample. It uses a complex, four-stage probability sampling design:\n\nStage 1: Primary Sampling Units (PSUs): The U.S. is divided into PSUs, which are typically counties. These PSUs are grouped into strata, and a sample of PSUs is selected from each stratum.\nStage 2: Segments: Each selected PSU is further divided into smaller geographic areas called segments (e.g., city blocks), and a sample of these segments is drawn.\nStage 3: Households: Within each selected segment, a list of all housing units is compiled, and a sample of households is randomly selected.\nStage 4: Individuals: Finally, within each selected household, individuals are randomly chosen from a list of all household members based on specific screening criteria.\n\nFor public-use data files, the true PSU and strata identifiers are masked to protect participant confidentiality. Instead, NHANES provides masked variance pseudo-stratum (SDMVSTRA) and pseudo-PSU (SDMVPSU) variables, which must be used for correct variance estimation.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#how-to-find-nhanes-data-from-the-cdc-website",
    "href": "accessing0.html#how-to-find-nhanes-data-from-the-cdc-website",
    "title": "Concepts (A)",
    "section": "How to Find NHANES Data from the CDC Website",
    "text": "How to Find NHANES Data from the CDC Website\nWhile the course provides curated datasets, it is also important to know how to find data directly from the source. To find NHANES data on the official Centers for Disease Control and Prevention (CDC) website, follow these steps:\n\nNavigate to the main NHANES page on the CDC website.\nLook for a section titled “Questionnaires, Datasets, and Related Documentation”.\nOn this page, you will find a variable search tool. You can use this tool to search for specific variables by keyword across all survey cycles.\nEach survey cycle (e.g., “NHANES 2017-2018”) will have its own page with links to data files organized by component (Demographics, Dietary, Examination, Laboratory, Questionnaire).\n\n\nWhat is included in this Video Lesson:\n\nModel-based approach review: 0:00\nDesign-based approach: 1:15\nTypes of sampling techniques 6:46\nStatistical inference 8:25\nNHANES 12:02\nSurvey weight 20:40\nCCHS download 23:45\nNHANES download 24:50\nNHANES sampling design 27:24\nHow to find NHANES data from CDC website 27:42\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#video-lesson-slides",
    "href": "accessing0.html#video-lesson-slides",
    "title": "Concepts (A)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#links",
    "href": "accessing0.html#links",
    "title": "Concepts (A)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides\nModel-based approach (Review/optional content)\nDesign-based approach",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing0.html#references",
    "href": "accessing0.html#references",
    "title": "Concepts (A)",
    "section": "References",
    "text": "References\n\n\n\n\nBilder, Christopher R, and Thomas M Loughin. 2014. Analysis of Categorical Data with r. CRC Press.\n\n\nHeeringa, Steven G, Brady T West, and Patricia A Berglund. 2017. Applied Survey Data Analysis. Chapman; Hall/CRC.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using r. Vol. 565. John Wiley & Sons.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2011. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Springer Science & Business Media.",
    "crumbs": [
      "Accessing data",
      "Concepts (A)"
    ]
  },
  {
    "objectID": "accessing1.html",
    "href": "accessing1.html",
    "title": "Survey data sources",
    "section": "",
    "text": "The tutorial discusses complex survey data and highlights potential data sources. Key datasets with survey features include the Canadian Community Health Survey (CCHS), the National Health and Nutrition Examination Survey (NHANES), and the European Social Survey (ESS), among others. Many of these sources, like NHANES and ESS, have specific R packages for data retrieval. In addition, there are other data sources such as the Vanderbilt Biostatistics Datasets and the World Bank Open Data, with the latter also offering dedicated R packages for data access.\n\nSurvey features\nGenerally survey features include - Strata - Cluster/primary sampling unit (PSU) - Weight, e.g., interview weight, survey weight\n\n\nDataset with survey features\n\nCanadian Community Health Survey - Annual Component CCHS\n\nDownload link UBC library\n\nNational Health and Nutrition Examination Survey NHANES\n\nR packages to download data: nhanesA, RNHANES\n\nNational Longitudinal Study of Adolescent to Adult Health [Add Health], 1994-2008 ICPSR 21600\nEuropean Social Survey ESS\n\nR package to download data: essurvey\n\nBehavioral Risk Factor Surveillance System BRFSS\nBureau of Economic Analysis BEA\nUS National Vital Statistics System NVSS\nDemographic and Health Surveys DHS\n\n\n\nOthers\n\nVanderbilt Biostatistics Datasets link\nWorld Bank Open Data WBOD\n\nR packages to download data: wbstats, WDI",
    "crumbs": [
      "Accessing data",
      "Survey data sources"
    ]
  },
  {
    "objectID": "accessing1i.html",
    "href": "accessing1i.html",
    "title": "Descriptions",
    "section": "",
    "text": "This tutorial introduces CCHS as a cross-sectional survey that collects health-related data and discusses its objectives and data usage. Additionally, it highlights the survey’s evolution and redesigns. For NHANES, the tutorial covers the importance of the dataset, its sampling procedures, history, data files, and documents. It also discusses how to combine data from different cycles, handle missing data, and deal with outliers. NHIS, another CDC-supported survey, is briefly introduced as a source of annual health-related data. There are some changes in the dataset due to the COVID-19 pandemic, e.g., fewer variables compared to pre-pandemic datasets. Otherwise, the main purpose of these datasets remained the same across survey cycles.\nCCHS\nOverview\nCCHS is a cross-sectional survey that collects vital health-related data, including health status, healthcare utilization, and health determinants, from the Canadian population. Available in both official languages, this survey relies on a substantial sample size to provide reliable estimates at various geographical levels every two years.\nObjectives of the CCHS\nThe CCHS has four primary objectives: supporting health surveillance programs at national, provincial, and intra-provincial levels; offering a single data source for health research on small populations and rare characteristics; providing timely and easily accessible information to a diverse user community; and maintaining flexibility to address emerging health issues within the population.\nData Products and Usage\nThe CCHS generates annual microdata files and combines two years of data for analysis. Users can also combine data from different years to study specific populations or rare characteristics. The data is primarily used for health surveillance and population health research, benefiting federal and provincial health departments, social service agencies, government bodies, and researchers from various fields. Non-profit health organizations and the media also utilize CCHS results to raise awareness about health concerns.\nEvolution and Redesigns\nThe CCHS started collecting data in 2001, transitioning to annual data collection in 2007 with a sample size adjustment to 65,000 respondents per year. It has undergone two significant redesigns to enhance its utility. The 2015 redesign updated sampling methods, adopted a new sample frame, modernized health content, and reviewed the target population. In 2022, the survey underwent another redesign, further updating content and transitioning to an online electronic questionnaire (EQ) for direct self-reporting by selected respondents. Both redesigns involved extensive consultations with stakeholders, including federal, provincial, and territorial partners, health region authorities, and academics.\nNHANES\n\n\nCDC,NCHS (2023)\nThe National Health and Nutrition Examination Survey (NHANES), conducted by the National Center for Health Statistics (NCHS), is a cornerstone of public health research in the United States. It’s designed to assess the health and nutritional status of both adults and children across the nation. What makes NHANES particularly powerful is its unique combination of\n\nin-home interviews and\ncomprehensive health examinations conducted in mobile examination centers (MECs)\n\nObjective physical measurements such as blood pressure, dental exams, and anthropometrics, and\nBiological specimens, such as blood and urine, for laboratory testing)\n\n\n\nThrough this dual approach, NHANES collects a wide array of data, allowing researchers and policymakers to get a clearer picture of the nation’s health, track trends in diseases and risk factors, and develop informed public health policies. The data covers the noninstitutionalized civilian U.S. population, meaning it includes most people except for those in settings like nursing homes, prisons, or on active military duty. These surveys have been administered in two-year cycles since 1999.\nThe Blueprint: The NHANES Sample Design 🗺️\n\n\nSampling Procedure: - Not obtained via simple random sample - Multistage sample designs - A sample weight is assigned to each sample person where weight = the number of people in the target population represented by that sample person in NHANES.\nNHANES uses a complex, probabilistic multi-stage sampling method. This ensures that the relatively small number of people who participate can accurately reflect the entire country. It is a probabilistic sample, meaning we know the probability of selection for all individuals. However, the sample is unlikely to be representative on its own, as some under- or oversampling occurs. For example, households with characteristics like being African American, Mexican American, low-income White American, or having persons aged 60+ are often oversampled.\nTo obtain population-level estimates, it is crucial to utilize the survey’s design features (weights, strata, and PSU/clusters).\nThe design involves four distinct stages:\nStage 1: Primary Sampling Units (PSUs)\nThe first step is to divide the 50 states into approximately 3,100 counties or geographically contiguous areas, which serve as PSUs. Each PSU is assigned to a stratum (e.g., based on urban/rural status or size). Counties are then randomly selected, often using a probability proportionate to size (PPS) method.\nStage 2: Segments\nEach selected county is broken down into smaller geographic areas called segments, typically containing at least 50-100 housing units. These segments are then randomly selected, also often using PPS.\nStage 3: Dwelling Units (DUs)\nWithin each chosen segment, a list of all dwelling units (which includes houses, apartments, and some group quarters like dorms) is created. From this list, a random subsample of households is selected.\nStage 4: Individuals\nIn the final stage, all eligible individuals within the selected households are listed. From this list, a subsample of individuals is chosen to participate in the survey based on their sex, age, race and Hispanic origin, and income.\nSurvey History 🔄\nThe NHANES program has a long history, evolving from the earlier National Health Examination Surveys (NHES) to its current continuous format.\n\n\n\n\n\n\n\n\nKey evolutions include:\n\nFrom Cyclical to Continuous: Early NHANES surveys were conducted in separate cycles (NHANES I, II, and III). Since 1999, NHANES has been a continuous, annual survey, which allows for more timely data and the ability to track trends more effectively. The data is typically released in two-year cycles to ensure stable and reliable estimates.\nChanges in Oversampling: The groups that are oversampled have changed over time. For example, from 1999 to 2006, the survey oversampled Mexican-American individuals. Starting in 2007, this was expanded to include all Hispanic persons. In 2011, an oversample of Asian persons was added to the design. The income threshold for oversampling low-income non-Hispanic white and other individuals also changed from at or below 130% of the federal poverty level to at or below 185% in the 2015-2018 survey cycle.\nAdapting to Challenges: The COVID-19 pandemic disrupted the 2019-2020 data collection cycle. To address this, the partially completed 2019-2020 data were combined with the full 2017-2018 cycle to create a nationally representative “prepandemic” dataset covering 2017 to March 2020.\nNHANES Data Files and Documents 📂\nFile Format\nThe Continuous NHANES files are stored on the NHANES website in the SAS transport file format (.xpt). This format can be imported into most major statistical packages.\nContinuous NHANES Components\n\n\nBroadly, continuous NHANES data are available in 5 categories: - Demographics - Dietary - Examination - Laboratory - Questionnaire\nNHANES Tutorials\nTo manage file size and documentation, the data is separated into five main components:\n\n\n\n\n\n\n\n\nData Analysis Considerations\nCombining Data\nIt is possible to combine datasets from different two-year cycles to increase sample size and statistical power. However, since NHANES is a cross-sectional survey, identifying the same person across different cycles is not possible in the public-use datasets. When appending data, ensure that variable names and definitions are consistent across the cycles being combined. Within a single cycle, each participant has a unique identifier, SEQN, which should be used for merging different data files (e.g., demographics and laboratory data).\nMissing Data and Outliers\n\n\nCDC (2023)\nThe CDC provides the following guidance for handling missing data and outliers:\n\n\nMissing Data &lt;10%: If less than 10% of data for a variable is missing, you can generally proceed with analysis. If it’s more than 10%, you should assess whether the missingness is systematic and consider imputation or weight adjustments.\n\n“Refused” or “Don’t Know”: These responses should be recoded as missing values to avoid distorting statistical results.\n\nOutliers: Be cautious of outliers, especially those with large survey weights, as they can disproportionately influence estimates. Analysts must decide whether to include or exclude these influential points.\nNHANES Documents\nEach data file is accompanied by essential documentation to guide its use.\n\n\n\n\n\n\n\n\n\n\nHelpful Websites: - NHANES Design - Variable Search\nMaking Sense of the Data: Survey Weights and Variance Estimation ⚖️\nBecause of its complex design and oversampling, analyzing NHANES data isn’t as simple as plugging the numbers into standard statistical software. Two key concepts are crucial for accurate analysis:\nSurvey Weights\nEach participant in NHANES is assigned a survey weight. This weight represents the number of people in the U.S. population that the participant represents. For example, a single participant might represent 20,000 other people with similar characteristics. These weights are essential because they account for: * The unequal probabilities of selection at different stages of sampling. * Nonresponse from individuals who were selected but chose not to participate. * Differences between the sample and the overall U.S. population.\nUsing these weights allows researchers to produce estimates that are truly representative of the national population.\nVariance Estimation\nStandard statistical tests often assume a simple random sample, which NHANES is not. Because of its clustered design, observations are not independent. Therefore, special techniques are needed to accurately calculate the variance and standard errors of estimates. The National Center for Health Statistics (NCHS) recommends using the Taylor series linearization method for variance estimation in NHANES data. Public-use data files include masked variance units to facilitate this process while protecting participant confidentiality.\n\nPSU variables and\nstrata variables\nConclusion\nNHANES is an invaluable resource for medical and public health professionals. Its rigorous, multi-stage design ensures that the data collected are representative of the U.S. population. By understanding the intricacies of its design, including the stages of sampling, the use of oversampling, and the importance of survey weights and proper variance estimation, students can confidently and accurately utilize this rich dataset to answer pressing public health questions.\nNHIS\nLike NHANES, National Health Interview Survey (NHIS) is supported by the CDC and is a large-scale multi-stage cross-sectional survey. The NHIS survey includes information on population disease prevalence, extent of disability, and use of health care services. In contrast to the NHANES that provides data every 2 years, NHIS provides data annually.\n\n\nTo obtain population-level estimate, we must utilize the survey features (weights, strata, PSU/cluster)\nReferences\n\n\n\n\nCDC. 2023. “NHANES Web Tutorial Frequently Asked Questions (FAQs).” https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/faq.aspx.\n\n\nCDC,NCHS. 2023. “National Health and Nutrition Examination Survey Data.” https://wwwn.cdc.gov/nchs/nhanes/.",
    "crumbs": [
      "Accessing data",
      "Descriptions"
    ]
  },
  {
    "objectID": "accessing2.html",
    "href": "accessing2.html",
    "title": "Importing CCHS to R",
    "section": "",
    "text": "Overview\nThis section provides comprehensive instructions on how to import the Canadian Community Health Survey (CCHS) dataset from the UBC library site to the RStudio environment. The process starts with downloading the CCHS data from the UBC library site and includes step-by-step visual guides for each stage. Three primary options are provided to process and format the data:\n\nUsing the commercial software SAS.\nUtilizing the free software PSPP, an alternative to SPSS.\nDirectly processing the data in R.\n\nFor each option, users are guided on how to download, install, access, read, save, and check the dataset. The objective is to help users acquire, visualize, and manipulate the CCHS dataset seamlessly using various software applications.\nDownloading CCHS data from UBC\n\n\nStep 1: Go to dvn.library.ubc.ca, and press ‘log-in’\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Select ‘UBC’ from the dropdown menu\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Enter your CWL or UBC library authentication information\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Once you log-in, search the term ‘cchs’ in the search-box\n\n\n\n\n\n\n\n\n\n\n\nStep 5: For illustrative purposes, let us work with the Cycle 3.1 of the CCHS dataset from the list of results. In that case, type ‘cchs 3.1’\n\n\n\n\n\n\n\n\n\n\n\nStep 6: CCHS Cycle 3.1 information\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Choose the ‘Data: CD’ from the menu\n\n\n\n\n\n\n\n\n\n\n\nStep 8: Download the entire data (about 159 MB) as a zip file\n\n\n\n\n\n\n\n\n\n\n\nStep 9: Accept the ‘terms of use’\n\n\n\n\n\n\n\n\n\n\n\nStep 10: Select a directory to download the zip file. The path of the download directory is important (we need to use this path exactly later). For example, below we are in \"C:\\CCHS\\\" folder, but we will create a “Data” folder there, so that the download path is \"C:\\CCHS\\Data\\\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 11: Extract the zip file\n\n\n\n\n\n\n\n\n\n\n\nStep 12: Be patient with the extraction\n\n\n\n\n\n\n\n\n\n\n\nStep 13: Once extraction is complete, take a look at the folders inside. You will see that there is a folder named ‘SAS_SPSS’\n\n\n\n\n\n\n\n\n\nReading and Formatting the data\nOption 1: Processing data using SAS\nSAS is a commercial software. You may be able to get access to educational version. In case you don’t have access to it, later we outline how to use free packages to read these datasets.\n\n\nStep 1: Inside that ‘SAS_SPSS’ folder, find the file hs_pfe.sas. It is a long file, but we are going to work on part of it. First thing we want to do it to change all the directory names to where you have unzipped the downloaded file (for example, here the zip file was extracted to C:/CCHS/Data/cchs_cycle3-1CD/). We only need the first part of the code (as shown below; only related to data ‘hs’). Delete the rest of the codes for now. The resulting code should like like this:\n\n\n%include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_pfe.sas\";\n\ndata hs;\n        %let datafid=\"C:\\CCHS\\Data\\cchs_cycle3-1CD\\Data\\hs.txt\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_fmt.sas\";\n        %include \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_lbe.sas\";\nrun;\n\nOnce the modifications are done, submit the codes in SAS. Note that, the name of the data is ‘hs’.\n\n\n\n\n\n\n\n\n\n\nStep 2: Once you submit the code, you can check the log window in SAS to see how the code submission went. It should tell you how many observations and variables were read.\n\n\n\n\n\n\n\n\n\n\n\nStep 3: If you want to view the dataset, you can go to ‘Explorer’ window within SAS.\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Generally, if you haven’t specified where to load the files, SAS will by default save the data into a library called ‘Work’\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Open that folder, and you will be able to find the dataset ‘Hs’.\n\n\n\n\n\n\n\n\n\n\n\nStep 6: Right click on the data, and click ‘open’ to view the datafile.\n\n\n\n\n\n\n\n\n\n\n\nStep 7: To export the data into a CSV format data (so that we can read this data into other software packages), ckick ‘Menu’.\n\n\n\n\n\n\n\n\n\n\n\nStep 8: then press ‘Export Data’.\n\n\n\n\n\n\n\n\n\n\n\nStep 9: choose the library and the data.\n\n\n\n\n\n\n\n\n\n\n\nStep 10: choose the format in which you may want to save the existing data.\n\n\n\n\n\n\n\n\n\n\n\nStep 11: also specify where you want to save the csv file and the name of that file (e.g., cchs3.csv).\n\n\n\n\n\n\n\n\n\n\n\nStep 12: go to that directory to see the file cchs3.csv\n\n\n\n\n\n\n\n\n\n\n\nStep 13: If you want to save the file in SAS format, you can do so by writing the following sas code into the ‘Editor’ window. Here we are saving the data Hs within the Work library in to a data called cchs3 within the SASLib library. Note that, the directory name has to be where you want to save the output file.\n\n\nLIBNAME SASLib \"C:\\CCHS\\Data\";\nDATA SASLib.cchs3;\n    set Work.Hs;\nrun;\n\nSubmit these codes into SAS:\n\n\n\n\n\n\n\n\n\n\nStep 13: go to that directory to see the file cchs3.sas7dbat\n\n\n\n\n\n\n\n\n\nOption 2: Processing data using PSPP (Free)\nPSPP is a free package; alternative to commercial software SPSS. We can use the same SPSS codes to read the datafile into PSPP, and save.\n\n\nStep 1: Get the free PSPP software from the website: www.gnu.org/software/pspp/\n\n\nPSPP is available for GNU/Hurd, GNU/Linux, Darwin (Mac OS X), OpenBSD, NetBSD, FreeBSD, and Windows\n\n\n\n\n\n\n\n\nFor windows, download appropriate version.\n\n\n\n\n\n\n\n\nDownload the file\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the icon shorcut after installing\n\n\n\n\n\n\n\n\n\n\nStep 2: Open PSPP\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Go to ‘file’ menu and click ‘open’\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Specify the readfile.sps file from the ‘SAS_SPSS’ folder.\n\n\n\n\n\n\n\n\n\nYou will see the following file:\n\n\n\n\n\n\n\n\n\n\nStep 5: Similar to before, change the directories as appropriate. Get rid of the extra lines of codes. Resulting codes are as follows (you can copy and replace the code in the file with the following codes):\n\n\nfile handle infile/name = 'C:\\CCHS\\Data\\cchs_cycle3-1CD\\DATA\\hs.txt'.\ndata list file = infile notable/.\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hs_i.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvale.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsvare.sps\".\ninclude file = \"C:\\CCHS\\Data\\cchs_cycle3-1CD\\SAS_SPSS\\Layouts\\hs\\hsmiss.sps\".\nexecute.\n\n\n\n\n\n\n\n\n\nFor Mac users, it should be as follows (e.g., username should be your user name, if you are saving under the path \"/Users/username/CCHS/Data/\"):\n\nfile handle infile/name =\"/Users/username/CCHS/Data/cchs_cycle3-1CD/Data/hs.txt\".\ndata list file = infile notable/.\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hs_i.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvale.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsvare.sps\".\ninclude file = \"/Users/username/CCHS/Data/cchs_cycle3-1CD/SAS_SPSS/Layouts/hs/hsmiss.sps\".\n\nexecute.\n\n\n\nStep 6: Run the codes.\n\n\n\n\n\n\n\n\n\n\n\nStep 7: This is a large data, and will take some time to load the data into the PSPP data editor. Be patient.\n\n\n\n\n\n\n\n\n\nOnce loading is complete, it will show the ‘output’ and ‘data view’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that, you will get error message, if your files were not in the correct path. In our example, the path was \"C:\\CCHS\\Data\\\" for the zip file content (see the previous steps).\n\n\nStep 7: You can also check the ‘variable view’.\n\n\n\n\n\n\n\n\n\n\n\nStep 8: Save the data by clicking ‘File’ and then ‘save as …’\n\n\n\n\n\n\n\n\n\n\n\nStep 9: Specify the name of the datafile and the location / folder to save the data file.\n\n\n\n\n\n\n\n\n\n\n\nStep 10: See the SAV file saved in the directory.\n\n\n\n\n\n\n\n\n\n\n\nStep 11: To save CSV format data, use the following syntax.\n\n\nSAVE TRANSLATE\n  /OUTFILE=\"C:/CCHS/Data/cchs3b.csv\"  \n  /TYPE=CSV\n  /FIELDNAMES      \n  /CELLS=VALUES.\n\nNote that, for categorical data, you can either save values or labels. For our purpose, we prefer values, and hence saved with values here.\n\n\n\n\n\n\n\n\n\n\nStep 12: See the CSV file saved in the directory extracted from PSPP.\n\n\n\n\n\n\n\n\n\nOption 3: Processing data using SPSS\nLog into ubc.onthehub.com to download SPSS. With your CWL account, UBC students should be able to download it. UBC IT website for SPSS says:\nThe SPSS software license with UBC specifies that SPSS must only be used by UBC Faculty, Students, and Research Staff and only for Teaching and non-commercial Research purposes related to UBC.\nBoth network (for UBC owened devices) or standalone / home versions (for non-UBC owened devices) should be available. Once downloaded, same process of importing CCHS data in PSPP can also be applied on SPSS (same syntax files should work).\nProcessing data in R\nDownload software\n\n\nStep 1: Download either ‘R’ from CRAN www.r-project.org or ‘R open’ from Microsoft mran.microsoft.com/open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Download RStudio from www.rstudio.com/\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Open RStudio\n\n\n\n\n\n\n\n\n\nImport, export and load data into R\n\n\nStep 1: Set working directory\n\n\nsetwd(\"C:/CCHS/Data/\") # or something appropriate\n\n\n\nStep 2: Read the dataset created from PSPP with cell values. We can also do a small check to see if the cell values are visible. For example, we choose a variable ‘CCCE_05A’, and tabulate it.\n\n\nHs &lt;- read.csv(\"cchs3b.csv\", header = TRUE)\ntable(Hs$CCCE_05A)\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Save the RData file from R into a folder SurveyData:\n\n\nsave(Hs, file = \"SurveyData/cchs3.RData\")\n\n\n\nStep 4: See the RData file saved in the directory extracted from R.\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Close R / RStudio and restart it. Environment window within RStudio should be empty.\n\n\n\n\n\n\n\n\n\n\n\nStep 6: Load the saved RData into R. Environment window within RStudio should have ‘Hs’ dataset.\n\n\nload(\"SurveyData/cchs3.RData\")",
    "crumbs": [
      "Accessing data",
      "Importing CCHS to R"
    ]
  },
  {
    "objectID": "accessing3.html",
    "href": "accessing3.html",
    "title": "Importing NHANES to R",
    "section": "",
    "text": "This tutorial provides comprehensive instructions on accessing the National Health and Nutrition Examination Survey (NHANES) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment. It covers accessing NHANES Data:\n\nDirectly from the CDC website: A step-by-step guide with accompanying images, illustrating how to navigate the CDC website, download the data, and interpret the accompanying codebook.\nUsing R packages, specifically the nhanesA package: A concise guide on how to download and get summaries of the NHANES data using this R package.\n\n\n# Load required packages\n#devtools::install_github(\"warnes/SASxport\")\n# library(SASxport)\nlibrary(foreign)\nlibrary(nhanesA)\nlibrary(knitr)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nuse.saved.chche &lt;- TRUE\n\n\n\nBefore installing a package from GitHub, it’s better to check whether you installed the right version of Rtools\nAccessing NHANES Data Directly from the CDC website\nIn the following example, we will see how to download ‘Demographics’ data, and check associated variable in that dataset.\n\n\n\n\n\n\n\n\n\n\nNHANES 1999-2000 and onward survey datasets are publicly available at wwwn.cdc.gov/nchs/nhanes/\n\n\nStep 1: Say, for example, we are interested about the NHANES 2015-2016 survey. Clicking the associated link in the above Figure gets us to the page for the corresponding cycle (see below).\n\n\n\n\n\n\n\n\n\n\n\nStep 2: There are various types of data available for this survey. Let’s explore the demographic information from this cycle. These data are mostly available in the form of SAS XPT format (see below).\n\n\n\n\n\n\n\n\n\n\n\nStep 3: We can download the XPT data in the local PC folder and read the data into R as as follows:\n\n\nDEMO &lt;- read.xport(\"Data/accessing/DEMO_I.XPT\")\n\n\n\nStep 4: Once data is imported in RStudio, we will see the DEMO object listed under data window (see below):\n\n\n\n\n\n\n\n\n\n\n\nStep 5: We can also check the variable names in this DEMO dataset as follows:\n\n\nnames(DEMO)\n#&gt;  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#&gt;  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#&gt; [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#&gt; [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#&gt; [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#&gt; [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#&gt; [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#&gt; [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the DEMO data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\n\n\n\nStep 7: There is a column name associated with each column, e.g., DMDHSEDU in the first column in the above Figure. To understand what the column names mean in this Figure, we need to take a look at the codebook. To access codebook, click the 'DEMO|Doc' link (in step 2). This will show the data documentation and associated codebook (see the next Figure).\n\n\n\n\n\n\n\n\n\n\n\nStep 8: We can see a link for the column or variable DMDHSEDU in the table of content (in the above Figure). Clicking that link will provide us further information about what this variable means (see the next Figure).\n\n\n\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count and cumulative (from the above Figure) matches with what we get from the DEMO data we just imported (particularly, for the DMDHSEDU variable):\n\n\ntable(DEMO$DMDHSEDU) # Frequency table\n#&gt; \n#&gt;    1    2    3    4    5    7    9 \n#&gt;  619  511  980 1462 1629    2   23\ncumsum(table(DEMO$DMDHSEDU)) # Cumulative frequency table\n#&gt;    1    2    3    4    5    7    9 \n#&gt;  619 1130 2110 3572 5201 5203 5226\nlength(is.na(DEMO$DMDHSEDU)) # Number of non-NA observations\n#&gt; [1] 9971\n\nAccessing NHANES Data Using R Packages\nnhanesA package\n\nlibrary(nhanesA)\n\n\n\n\n\n\n\nTip\n\n\n\nR package nhanesA provides a convenient way to download and analyze NHANES survey data.\n\n\n\n\nRNHANES (Susmann 2016) is another packages for downloading the NHANES data easily.\n\n\nStep 1: Witin the CDC website, NHANES data are available in 5 categories\n\nDemographics (DEMO)\nDietary (DIET)\nExamination (EXAM)\nLaboratory (LAB)\nQuestionnaire (Q)\n\n\n\nTo get a list of available variables within a data file, we run the following command (e.g., we check variable names within DEMO data):\n\nnhanesTables(data_group='DEMO', year=2015)\n\n\n  \n\n\n\n\n\nStep 2: We can obtain the summaries of the downloaded data as follows (see below):\n\n\ndemo &lt;- nhanes('DEMO_I')\nnames(demo)\n#&gt;  [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\"\n#&gt;  [7] \"RIDRETH1\" \"RIDRETH3\" \"RIDEXMON\" \"RIDEXAGM\" \"DMQMILIZ\" \"DMQADFC\" \n#&gt; [13] \"DMDBORN4\" \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDMARTL\"\n#&gt; [19] \"RIDEXPRG\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\"\n#&gt; [25] \"FIAINTRP\" \"MIALANG\"  \"MIAPROXY\" \"MIAINTRP\" \"AIALANGA\" \"DMDHHSIZ\"\n#&gt; [31] \"DMDFMSIZ\" \"DMDHHSZA\" \"DMDHHSZB\" \"DMDHHSZE\" \"DMDHRGND\" \"DMDHRAGE\"\n#&gt; [37] \"DMDHRBR4\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"WTINT2YR\" \"WTMEC2YR\"\n#&gt; [43] \"SDMVPSU\"  \"SDMVSTRA\" \"INDHHIN2\" \"INDFMIN2\" \"INDFMPIR\"\ntable(demo$DMDHSEDU) # Frequency table\n#&gt; \n#&gt;                                Less Than 9th Grade \n#&gt;                                                619 \n#&gt; 9-11th Grade (Includes 12th grade with no diploma) \n#&gt;                                                511 \n#&gt;                 High School Grad/GED or Equivalent \n#&gt;                                                980 \n#&gt;                          Some College or AA degree \n#&gt;                                               1462 \n#&gt;                          College Graduate or above \n#&gt;                                               1629 \n#&gt;                                            Refused \n#&gt;                                                  2 \n#&gt;                                         Don't Know \n#&gt;                                                 23\ncumsum(table(demo$DMDHSEDU)) # Cumulative frequency table\n#&gt;                                Less Than 9th Grade \n#&gt;                                                619 \n#&gt; 9-11th Grade (Includes 12th grade with no diploma) \n#&gt;                                               1130 \n#&gt;                 High School Grad/GED or Equivalent \n#&gt;                                               2110 \n#&gt;                          Some College or AA degree \n#&gt;                                               3572 \n#&gt;                          College Graduate or above \n#&gt;                                               5201 \n#&gt;                                            Refused \n#&gt;                                               5203 \n#&gt;                                         Don't Know \n#&gt;                                               5226\nlength(is.na(demo$DMDHSEDU)) # Number of non-NA observations\n#&gt; [1] 9971\n\nImport data issue\nSometimes, you might see a warning message when downloading NHANES data using an R package. For example, simpleWarning in download.file(url, tf, mode = “wb”, quiet = TRUE): cannot open URL, or 404 data Not Found.\nThe possible reason could be the NHANES server was down when you tried to connect. In that case, try later with the same codes. Also, check the name of the variables carefully. The name of the same variable could be different in different survey cycles. It is also possible that some variables are not available in all cycles.\nReferences\n\n\n\n\nSusmann, Herb. 2016. RNHANES: Facilitates Analysis of CDC NHANES Data. https://CRAN.R-project.org/package=RNHANES.",
    "crumbs": [
      "Accessing data",
      "Importing NHANES to R"
    ]
  },
  {
    "objectID": "accessing4.html",
    "href": "accessing4.html",
    "title": "Reproducing NHANES results",
    "section": "",
    "text": "Example article\nThe section instructs on reproducing the results from a specific article, detailing the eligibility criteria and variables of interest, guiding the user through accessing, merging, and filtering relevant NHANES data, and then recoding and comparing the results to ensure they match with the original article’s findings, all supported with visual aids and R code examples.\nLet us use the article by Flegal et al. (2016) as our reference. DOI:10.1001/jama.2016.6458.",
    "crumbs": [
      "Accessing data",
      "Reproducing NHANES results"
    ]
  },
  {
    "objectID": "accessing4.html#references",
    "href": "accessing4.html#references",
    "title": "Reproducing NHANES results",
    "section": "References",
    "text": "References\n\n\n\n\nDhana, A. 2023. “R & Python for Data Science.” https://datascienceplus.com/.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91.",
    "crumbs": [
      "Accessing data",
      "Reproducing NHANES results"
    ]
  },
  {
    "objectID": "accessing5.html",
    "href": "accessing5.html",
    "title": "Importing NHIS to R",
    "section": "",
    "text": "This tutorial provides instructions on accessing the National Health Interview Survey (NHIS) dataset from the US Centers for Disease Control and Prevention (CDC) website and importing it into the RStudio environment.\nNHIS datafile and documents\nThe NHIS files are stored in the NHIS website in different formats. You can import this data in any statistical package that supports these file formats, e.g., ASCII, CSV, SAS.\n\n\nNHIS Data, Questionnaires and Related Documentation\n\nIn the recent NHIS (2019 or later), data are available in 5 categories:\n\nInterview data for adults\nInterview data for children\nImputed income for adults\nImputed income for children\nParadata\n\n\nIn the earlier NHIS (before 2019), data are available in 8 categories:\n\nFamily file\nHousehold file\nPerson file\nChild file\nAdult file\nImputed income\nFunctioning and disability\nParadata\n\n\n\nCombining data\nDifferent cycles\nIt is possible to combine datasets from different years/cycles together in NHIS. Similar to NHANES, identification of the same person in NHIS across different cycles is not possible in the public release datasets. For appending data from different cycles, please make sure that the variable names/labels are the same/identical in years under consideration (in some years, names and labels do change).\nWithin the same cycle\nWithin NHIS datasets in a given cycle, each sampled person has a household number (HHX), family number (FMX), and a person number within family (FPX). We can create a unique identifier based on these three variables and merge the datasets.\nAccessing NHIS Data\n\n\nNHIS survey datasets are publicly available at https://www.cdc.gov/nchs/nhis/\nUnlike NHANES where a R package is available to download the dataset, NHIS datasets need to be downloaded directly from the CDC website. In the following example, we will see how to download ‘Adult’ data from 2021 NHIS, and check associated variable in that dataset.\n\n\n\n\n\n\n\n\n\n\nStep 1: Say, for example, we are interested to download the adult dataset in the CSV format:\n\n\n\n\n\n\n\n\n\n\n\nStep 2: We can download the data in the local PC folder, unzip it, and then read the data into R as as follows:\n\n\nadult21 &lt;- read.csv(\"Data/accessing/adult21.csv\", header = T)\n\n\n\nStep 3: Once data is imported in RStudio, we will see the adult21 object listed under data window (see below):\n\n\n\n\n\n\n\n\n\n\n\nStep 4: We can check the variable names in this adult21 dataset using the names function.\n\n\nnames(adult21)\n\n\n\nStep 5: We can check how many unique adults are in this adult21 dataset. Note that the HHX variable in the dataset is the unique household identifier, where only one adult per household was selected for interview. We can use this HHX variable to merge adult datafile with other datafiles (e.g., child data).\n\n\nlength(unique(adult21$HHX))\n#&gt; [1] 29482\n\n\n\nStep 6: We can open the data in RStudio in the dataview window (by clicking the adult21 data from the data window). The next Figure shows only a few columns and rows from this large dataset. Note that there are some values marked as “NA”, which represents missing values.\n\n\n\n\n\n\n\n\n\n\n\nStep 7: To understand what the column names mean in this Figure, we need to take a look at the codebook, which is also available on the CDC website:\n\n\n\n\n\n\n\n\n\n\n\nStep 8: We can see a check for the column or variables, e.g., REGION, in the codebook:\n\n\n\n\n\n\n\n\n\n\n\nStep 9: We can assess if the numbers reported under count matches with what we get from the adult21 data we just imported (particularly, for the REGION variable):\n\n\n# Frequency table\ntable(adult21$REGION, useNA = \"always\") \n#&gt; \n#&gt;     1     2     3     4  &lt;NA&gt; \n#&gt;  4775  6327 10731  7649     0\n\nSimilarly, we can download the child data and open it in R:\n\nchild21 &lt;- read.csv(\"Data/accessing/child21.csv\", header = T)\n\n\n\n\n\n\n\n\n\nLet’s check how many unique children are in this child21 dataset:\n\nlength(unique(child21$HHX))\n#&gt; [1] 8261\n\nNow let’s check for the column or variables, e.g., SEX_C, in the codebook:\n\n\n\n\n\n\n\n\nWe can assess if the numbers reported under count matches with what we get from the child21 data we just imported:\n\n# Frequency table\ntable(child21$SEX_C, useNA = \"always\") \n#&gt; \n#&gt;    1    2    7 &lt;NA&gt; \n#&gt; 4257 4002    2    0\n\nMerging within the same cycle\n\n\n\n\n\n\nNote\n\n\n\nWe can use HHX variable to merge different datafiles within the same survey cycle.\n\n\nAs mentioned earlier, HHX variable in the dataset is the unique household identifier. We can use this HHX variable to merge different datafiles within the same survey cycle. Say, we are interested in merging child age (AGEP_C) and sex (SEX_C) variables with the adult datafile. We can use the merge function as follows:\n\ndat &lt;- merge(adult21, child21[,c(\"HHX\", \"AGEP_C\", \"SEX_C\")], by = \"HHX\", all = T)\n\n\n\n\n\n\n\n\n\nAs we can see, there are data from 30,673 unique households, suggesting that not all children are sampled from the same household of sampled adults.\n\nlength(unique(dat$HHX))\n#&gt; [1] 30673\n\nTable 1\nNow we will use the adult21 dataset to create Table 1 with utilizing survey features (i.e., psu, strata, and weights). For that, let us create/recode some variables:\n\n\nIn a following chapter about survey data analysis, we will explain what these survey features mean.\n\n# Heart attack\nadult21$heart.attack &lt;- car::recode(adult21$MIEV_A, \" 2 = 'No'; 1 = 'Yes'; else = NA\", \n                                levels = c(\"No\", \"Yes\"), as.factor = T)\ntable(adult21$heart.attack, useNA = \"always\")\n#&gt; \n#&gt;    No   Yes  &lt;NA&gt; \n#&gt; 28378  1078    26\n\n# Diabetes\nadult21$diabetes &lt;- car::recode(adult21$DIBEV_A, \" 2 = 'No'; 1 = 'Yes'; else = NA\", \n                            levels = c(\"No\", \"Yes\"), as.factor = T)\ntable(adult21$diabetes, useNA = \"always\")\n#&gt; \n#&gt;    No   Yes  &lt;NA&gt; \n#&gt; 26318  3134    30\n\n# Sex\nadult21$sex &lt;- car::recode(adult21$SEX_A, \" '1'='Male'; '2'='Female'; else=NA\", \n                       levels = c(\"Female\", \"Male\"), as.factor = T)\ntable(adult21$sex, useNA = \"always\")\n#&gt; \n#&gt; Female   Male   &lt;NA&gt; \n#&gt;  16102  13378      2\n\n# Pseudo-PSU\nadult21$psu &lt;- adult21$PPSU\nadult21$psu &lt;- as.factor(adult21$psu)\ntable(adult21$psu, useNA = \"always\")\n#&gt; \n#&gt;    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n#&gt; 1952 1599 1141  792  669  461  549  662  629  840  521  359  491  421  405  346 \n#&gt;   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n#&gt;  156  221  348  531  596  656  588  593  383  647  374  294  218   57  310  334 \n#&gt;   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n#&gt;  269  369  410  447  180  279  263   85   70  208  175  224  302  270  396  376 \n#&gt;   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n#&gt;  202  240  303  253  349  249   75   67  257  203  234  327  410  364  252  247 \n#&gt;   65   66   67   68   72   73   74   75   76   77   78   79   80   81   82   87 \n#&gt;  199  149   96   22   65   42  108  131   37   41   31   28   50   46   64   31 \n#&gt;   89   90   91   92   93   97   98   99  100  101  102  103  104  108  109  110 \n#&gt;   81   28  132  170   64   86   45   32  144  128  129  171  117   63   48   13 \n#&gt;  114  127  128  134  139  140  150  151  152  153 &lt;NA&gt; \n#&gt;   54   44   10   29   69   18   46   50   49   24    0\n\n# Pseudo-stratum\nadult21$strata &lt;- adult21$PSTRAT\nadult21$strata &lt;- as.factor(adult21$strata)\ntable(adult21$strata, useNA = \"always\")\n#&gt; \n#&gt;  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115 \n#&gt;  736  714  589  480  499  605  733  748  757  629  623  614  158  914  386  603 \n#&gt;  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131 \n#&gt;  192  678  661  842  549  510  606  306  517  385  633  418  265  801  449  558 \n#&gt;  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147 \n#&gt;  558  563  434  532  595  576  494  370  644  485  460  738  625  368  412  650 \n#&gt;  148  149  150  151 &lt;NA&gt; \n#&gt;  672  531  556 1061    0\n\n# Sampling weight\nadult21$sweight &lt;- adult21$WTFA\nsummary(adult21$sweight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   793.2  4698.3  7402.6  8586.9 10671.1 71378.0\n\n# Drop the missing values associated with Heart attack, Diabetes, Sex\ndat.analytic &lt;- adult21[complete.cases(adult21$heart.attack),]\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$diabetes),]\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$sex),]\ndim(dat.analytic)\n#&gt; [1] 29435   628\n\nFirst, we will create the survey design. Second, we will report Table 1 with heart attack and sex variable, stratified by diabetes.\n\nlibrary(tableone)\nlibrary(survey)\n\n# Indicator in the full data\nadult21$indicator &lt;- 1\nadult21$indicator[adult21$HHX %in% dat.analytic$HHX] &lt;- 0\ntable(adult21$indicator)\n#&gt; \n#&gt;     0     1 \n#&gt; 29435    47\n\n# Survey design\nw.design &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~sweight, data = adult21, nest = T)\n\n# Subset\nw.design0 &lt;- subset(w.design, indicator == 0)\n\n# Table 1\ntab1 &lt;- svyCreateTableOne(var = c(\"heart.attack\", \"sex\"), strata= \"diabetes\", \n                          data = w.design0, test = FALSE)\nprint(tab1)\n#&gt;                         Stratified by diabetes\n#&gt;                          No                  Yes               \n#&gt;   n                      228524605.2         24325386.4        \n#&gt;   heart.attack = Yes (%)   5335722.7 ( 2.3)   2358175.6 ( 9.7) \n#&gt;   sex = Male (%)         109610086.2 (48.0)  12510287.1 (51.4)\n\nRegression analysis\nLet’s run a regression analysis with utilizing survey features.\n\nlibrary(Publish)\n\n# Design-adjusted logistic\nfit1 &lt;- svyglm(I(heart.attack == \"Yes\") ~ diabetes + sex, design = w.design0, family = binomial)\npublish(fit1)\n#&gt;  Variable  Units OddsRatio       CI.95 p-value \n#&gt;  diabetes     No       Ref                     \n#&gt;              Yes      4.42 [3.76;5.21]  &lt;1e-04 \n#&gt;       sex Female       Ref                     \n#&gt;             Male      2.04 [1.75;2.39]  &lt;1e-04",
    "crumbs": [
      "Accessing data",
      "Importing NHIS to R"
    ]
  },
  {
    "objectID": "accessing7.html",
    "href": "accessing7.html",
    "title": "Linking mortality data",
    "section": "",
    "text": "This tutorial provides instructions on linking public-use US mortality data with the NHANES dataset. One can also follow the same steps to link the mortality data with the NHIS.\nDownload mortality data\nThe public-use mortality data can be downloaded directly from the CDC website. Datasets are available in .dat format, separately for each cycle of NHANES and NHIS.\n\n\n\n\n\n\n\n\nOn the same website, CDC also provided R, SAS, and Stata codes with instructions on how to download the datasets directly from the website.\n\n\n\n\n\n\n\n\nWe can click on the desired survey link to download and save the datasets on our own hard drive. The dataset will be directly downloaded to our specified download folder. Alternatively, we can right-click on the desired survey link and select Save link as...\n\n\n\n\n\n\n\n\nNote that the data file is saved as &lt;survey name&gt;_MORT_2019_PUBLIC.dat. In our example, we downloaded mortality data for the NHANES 2013-14 participants. Hence, the name of the file should be NHANES_2013_2014_MORT_2019_PUBLIC.dat.\n\n\n\n\n\n\n\n\nLink mortality data to NHANES\nLet us link the mortality data to the NHANES 2013-14 cycle. The steps are as follows:\n\nDownload morality data for the NHANES 2013-14 cycle\nLoad the morality data on the R environment\nLoad NHANES 2013-14 cycle\nMerge two datasets using the unique identifier\n\nDownload morality data\nWe can follow the steps described above to download the mortality dataset directly from the CDC website.\nLoad the morality data on the R environment\nTo load the dataset, we can use the read_fwf function from the readr package.\n\nlibrary(readr)\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\ndat.mort &lt;- read_fwf(\n  file = \"Data/accessing/NHANES_2013_2014_MORT_2019_PUBLIC.dat\",\n  col_types = \"iiiiiiii\",\n  fwf_cols(SEQN = c(1,6), \n           eligstat = c(15,15),\n           mortstat = c(16,16),\n           ucod_leading = c(17,19),\n           diabetes = c(20,20),\n           hyperten = c(21,21),\n           permth_int = c(43,45),\n           permth_exm = c(46,48)),\n  na = c(\"\", \".\"))\n\nhead(dat.mort)\n\n\n  \n\n\n\nIn the code chuck above,\n\nSEQN: unique identifier for NHANES\n\neligstat: Eligibility Status for Mortality Follow-up\n\n1 = Eligible\n2 = Under age 18, not available for public release\n3 = Ineligible\n\n\n\nmortstat: Mortality Status\n\n0 = Assumed alive\n1 = Assumed deceased\nNA = Ineligible or under age 18\n\n\n\nucod_leading: Underlying Cause of Death\n\n1 = Diseases of heart (I00-I09, I11, I13, I20-I51)\n2 = Malignant neoplasms (C00-C97)\n3 = Chronic lower respiratory diseases (J40-J47)\n4 = Accidents (unintentional injuries) (V01-X59, Y85-Y86)\n5 = Cerebrovascular diseases (I60-I69)\n6 = Alzheimer’s disease (G30)\n7 = Diabetes mellitus (E10-E14)\n8 = Influenza and pneumonia (J09-J18)\n9 = Nephritis, nephrotic syndrome and nephrosis (N00-N07, N17-N19, N25-N27)\n10 = All other causes\nNA = Ineligible, under age 18, assumed alive, or no cause of death data available\n\n\n\ndiabetes: Diabetes Flag from Multiple Cause of Death (MCOD)\n\n0 = No - Condition not listed as a multiple cause of death\n1 = Yes - Condition listed as a multiple cause of death\nNA = Assumed alive, under age 18, ineligible for mortality follow-up, or MCOD not available\n\n\n\nhyperten: Hypertension Flag from Multiple Cause of Death (MCOD)\n\n0 = No - Condition not listed as a multiple cause of death\n1 = Yes - Condition listed as a multiple cause of death\nNA = Assumed alive, under age 18, ineligible for mortality follow-up, or MCOD not available\n\n\npermth_int: Person-Months of Follow-up from NHANES Interview date\npermth_exm: Person-Months of Follow-up from NHANES Mobile Examination Center (MEC) Date\n\nLet us see the basic summary statistics of some variables:\n\n# Mortality Status\ntable(dat.mort$mortstat, useNA = \"always\")\n#&gt; \n#&gt;    0    1 &lt;NA&gt; \n#&gt; 5633  467 4075\n\n# Person-Months of Follow-up from NHANES Interview date\nsummary(dat.mort$permth_int)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    1.00   65.00   72.00   70.34   79.00   85.00    4075\n\n# Underlying Cause of Death\ntable(dat.mort$ucod_leading, useNA = \"always\")\n#&gt; \n#&gt;    1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n#&gt;  136   99   24   14   28   17   21    9   16  103 9708\n\nLoad NHANES 2013-14 cycle\nLet the open the NHANES 2013-14 dataset we created in the previous chapter on Reproducing results.\n\n# Load data\nload(\"Data/accessing/analyticNHANES2013.RData\")\nls()\n#&gt; [1] \"analytic.data3\" \"dat.mort\"       \"merged.data\"\n\n# NHANES 2013-14\nhead(analytic.data3)\n\n\n  \n\n\ndim(analytic.data3)\n#&gt; [1] 5455   15\n\nMerge mortality data and NHANES 2013-14 using unique identifier\nLet us merge the mortality and NHANES datasets using the SEQN variable.\n\n# Merge datasets\ndat.nhanes &lt;- merge(analytic.data3, dat.mort, by = \"SEQN\", all.x = T)\ndim(dat.nhanes)\n#&gt; [1] 5455   22\nhead(dat.nhanes)\n\n\n  \n\n\n\nTable 1\nNow we will use the dat.nhanes dataset to create Table 1 with utilizing survey features (i.e., psu, strata, and survey weights). First, we will create the survey design. Second, we will report Table 1 with age, sex, race, eligibility, all-cause mortality status, diabetes-related death, hypertension-related death, and follow-up times.\n\nlibrary(tableone)\nlibrary(survey)\n\n# Make eligibility and mortality status as factor variable\nfactor.vars &lt;- c(\"eligstat\", \"mortstat\", \"diabetes\", \"hyperten\")\ndat.nhanes[,factor.vars] &lt;- lapply(dat.nhanes[,factor.vars] , factor)\n\n# Survey design\nw.design &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~survey.weight, \n                      data = dat.nhanes, nest = T)\n\n# Table 1 - unweighted frequency or mean\ntab1a &lt;- CreateTableOne(var = c(\"AgeCat\", \"Gender\", \"Race\", \"eligstat\", \"mortstat\", \n                                \"diabetes\", \"hyperten\", \"permth_int\", \"permth_exm\"),\n                        data = dat.nhanes, includeNA = T)\nprint(tab1a, showAllLevels = T, format = \"f\")\n#&gt;                         \n#&gt;                          level    Overall      \n#&gt;   n                                5455        \n#&gt;   AgeCat                 [0,20)       0        \n#&gt;                          [20,40)   1810        \n#&gt;                          [40,60)   1896        \n#&gt;                          [60,Inf)  1749        \n#&gt;   Gender                 Female    2817        \n#&gt;                          Male      2638        \n#&gt;   Race                   White     2343        \n#&gt;                          Black     1115        \n#&gt;                          Asian      623        \n#&gt;                          Hispanic  1214        \n#&gt;                          &lt;NA&gt;       160        \n#&gt;   eligstat               1         5445        \n#&gt;                          3           10        \n#&gt;   mortstat               0         5030        \n#&gt;                          1          415        \n#&gt;                          &lt;NA&gt;        10        \n#&gt;   diabetes               0          374        \n#&gt;                          1           41        \n#&gt;                          &lt;NA&gt;      5040        \n#&gt;   hyperten               0          344        \n#&gt;                          1           71        \n#&gt;                          &lt;NA&gt;      5040        \n#&gt;   permth_int (mean (SD))          70.40 (12.18)\n#&gt;   permth_exm (mean (SD))          69.49 (12.20)\n\n# Table 1 - weighted percentage or mean\ntab1b &lt;- svyCreateTableOne(var = c(\"AgeCat\", \"Gender\", \"Race\", \"eligstat\", \"mortstat\", \n                                \"diabetes\", \"hyperten\", \"permth_int\", \"permth_exm\"), \n                           data = w.design, includeNA = T)\nprint(tab1b, showAllLevels = T, format = \"p\")\n#&gt;                         \n#&gt;                          level    Overall             \n#&gt;   n                                217464332.1        \n#&gt;   AgeCat (%)             [0,20)            0.0        \n#&gt;                          [20,40)          35.5        \n#&gt;                          [40,60)          37.5        \n#&gt;                          [60,Inf)         27.0        \n#&gt;   Gender (%)             Female           51.4        \n#&gt;                          Male             48.6        \n#&gt;   Race (%)               White            66.1        \n#&gt;                          Black            11.4        \n#&gt;                          Asian             5.2        \n#&gt;                          Hispanic         14.7        \n#&gt;                          &lt;NA&gt;              2.7        \n#&gt;   eligstat (%)           1                99.9        \n#&gt;                          3                 0.1        \n#&gt;   mortstat (%)           0                93.6        \n#&gt;                          1                 6.2        \n#&gt;                          &lt;NA&gt;              0.1        \n#&gt;   diabetes (%)           0                 5.5        \n#&gt;                          1                 0.7        \n#&gt;                          &lt;NA&gt;             93.8        \n#&gt;   hyperten (%)           0                 5.2        \n#&gt;                          1                 1.0        \n#&gt;                          &lt;NA&gt;             93.8        \n#&gt;   permth_int (mean (SD))                 70.71 (11.28)\n#&gt;   permth_exm (mean (SD))                 69.80 (11.31)",
    "crumbs": [
      "Accessing data",
      "Linking mortality data"
    ]
  },
  {
    "objectID": "accessing8.html",
    "href": "accessing8.html",
    "title": "Replicate Data Creation",
    "section": "",
    "text": "National Health and Nutrition Examination Survey (NHANES)\nThe National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews, physical examinations, and laboratory data, providing comprehensive health information. NHANES data are collected by the National Center for Health Statistics (NCHS), part of the Centers for Disease Control and Prevention (CDC). NHANES has data releases in two-year cycles. Each cycle contains a representative sample of the U.S. population, collected using a complex, multistage probability sampling design. NHANES is also linked to mortality data through the National Death Index (NDI), allowing researchers to examine the relationship between health status, risk factors, and mortality.\nCurrent Analysis\nThe NHANES datasets used in this analysis for lab exercise (2) are extracted from multiple cycles (1999-2014) and harmonized to ensure consistency across survey waves. The variables examined in this study include demographic information, health status, medical conditions, prescription medication use, and mortality status.\nGeneral aim\nThe primary aim of this analysis is to investigate the association between benzodiazepine use, with or without opioids, and all-cause mortality in adults aged 20 years or older. The study will compare the mortality risk between benzodiazepine users and a comparator group using selective serotonin reuptake inhibitors (SSRIs). The analysis will adjust for potential confounders.\n\n\n\n\n\n\n\n\nPICOT of the Analysis\n\nP (Population): Adults aged 20 years or older who participated in the NHANES from 1999 to 2015.\nI (Intervention): Use of benzodiazepines with or without opioids.\nC (Comparison): Selective serotonin reuptake inhibitors (SSRIs) as an active comparator group. This group includes individuals using SSRIs but not opioids or benzodiazepines.\nO (Outcome): All-cause mortality, ascertained via linkage to the National Death Index. The hazard ratios for mortality were compared between the intervention groups and the SSRI comparator group.\nT (Time): 1999 to 2015. A median follow-up time of 6.7 years (range, 0.2 to 16.8 years) was used to measure outcomes.\nVariables under consideration\n1. Mortality Data\nThe mortality data is critical for assessing all-cause mortality and linking it to other variables such as medication use and comorbidities. NHANES data is linked to the National Death Index (NDI) for mortality follow-up.\n\n\nSEQN: Participant ID (unique identifier for each survey participant)\n\nmortstat: Mortality status (1 = deceased, 0 = alive)\n\npermth_int: Number of months between the date of NHANES exam and death or last follow-up (used to calculate follow-up time for mortality analysis)\n\npermth_exm: Number of months between the date of examination and mortality ascertainment (useful for establishing the timeline of events)\n\nCYCLE: NHANES survey cycle (e.g., 1999-2000, 2001-2002), used for cohort stratification\n2. Demographic Data\nDemographic data is important for adjusting models based on confounders such as age, gender, and socioeconomic status.\n\n\nSEQN: Participant ID (unique identifier for each survey participant)\n\nCYCLE: NHANES survey cycle (e.g., 1999-2000, 2001-2002)\n\nSDMVSTRA: Masked variance pseudo-stratum (used for complex survey design to account for the stratified sample design)\n\nSDMVPSU: Masked variance pseudo-primary sampling unit (PSU) (used for adjusting the sampling structure in analyses)\n\nWTMEC2YR: Sample MEC (Mobile Examination Center) exam weight for 2-year analysis (used to weight the sample to be nationally representative)\n\nRIDSTATR: Interview/examination status (1 = Interviewed only, 2 = Both interviewed and MEC examined, used for completeness of the data)\n\nRIDAGEYR: Age in years at screening (collected as part of the demographic questionnaire, age is a critical confounder in mortality analyses)\n\nRIAGENDR: Gender (1 = Male, 2 = Female, often included in stratified analyses)\n\nDMDEDUC2: Education level (assessed for adults aged 20 years or older, higher education often correlates with better health outcomes)\n\nINDFMPIR: Family monthly poverty income ratio (calculated from household income and family size, used as a proxy for socioeconomic status)\n\nRIDRETH1: Race/ethnicity (1 = Mexican American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other Race, including multiracial)\n\nDMDMARTL: Marital status (1 = Married, 2 = Widowed, 3 = Divorced, marital status can impact social support, which is linked to health outcomes)\n3. Smoking Questionnaire (SMQ) Data\nSmoking is a significant risk factor for many chronic conditions and can interact with medication use.\n\n\nSMOKING_HARMONIZED: Harmonized smoking variable [SMQ020/SMQ040] created by harmonizing data across survey cycles.\n\n\nSMQ020: Smoking status (Have you smoked at least 100 cigarettes in your life? Yes/No). This determines if the participant is ever a smoker.\n\nSMQ040: Smoking frequency (Do you now smoke cigarettes? Responses: Every day, Some days, Not at all), used to assess current smoking behavior.\n\n\n4. Blood Pressure (BPQ) Data\nBlood pressure is a well-established predictor of cardiovascular disease and is often controlled by medications that could influence mortality.\n\n\nBPQ020: Ever told had high blood pressure (diagnosis of hypertension, one of the most important cardiovascular risk factors)\n\nLIPIDEM_HARMONIZED: Harmonized lipidemia variable [BPQ060/BPQ080], combining data on cholesterol treatment across cycles.\n\n\nBPQ060: Taking prescription for hypertension (Are you now taking prescribed medicine for high blood pressure?)\n\nBPQ080: Blood pressure checked in past 6 months (Have you had your blood pressure checked by a doctor or health professional in the past 6 months?)\n\n\n5. Diabetes Questionnaire (DIQ) Data\nDiabetes is a major chronic condition that affects many body systems, and managing blood sugar levels is crucial in preventing complications.\n\n\nDIQ010: Doctor told you have diabetes (Has a doctor or other health professional ever told you that you have diabetes?)\n6. Medical Conditions (MCQ) Data\nThis section includes a variety of conditions that are strongly linked to mortality outcomes, and their interactions with medication use are key areas of study.\n\n\nMCQ160F: Ever told you had a stroke (Has a doctor or other health professional ever told you that you had a stroke?)\n\nMCQ160E: Ever told you had a heart attack (Has a doctor or other health professional ever told you that you had a heart attack?)\n\nMCQ160B: Ever told you had congestive heart failure (Has a doctor or health professional ever told you that you had congestive heart failure?)\n\nPULMOND_HARMONIZED: Harmonized pulmonary condition variable [MCQ160G/MCQ160K/MCQ160O] (this variable combines data on chronic lung diseases like asthma and emphysema)\n\n\nMCQ160G: Ever told you had emphysema? (Has a doctor or other health professional ever told you that you had emphysema?)\n\nMCQ160K: Ever told you had chronic bronchitis? (Has a doctor or other health professional ever told you that you had chronic bronchitis?)\n\nMCQ160O: Ever told you had asthma? (Has a doctor or other health professional ever told you that you had asthma?)\n\n\n\nMCQ160L: Ever told you had liver condition (Has a doctor or other health professional ever told you that you had liver disease?)\n\nMCQ160A: Ever told you had arthritis (Has a doctor or other health professional ever told you that you have arthritis?)\n\nMCQ220: Ever told you had cancer or malignancy (Have you ever been told by a doctor or other health professional that you had cancer or a malignancy?)\n7. Kidney Conditions (KIQ) Data\nKidney disease can complicate treatment for other conditions and impacts drug clearance, influencing the effectiveness and toxicity of medications.\n\n\nKIDNEYD_HARMONIZED: Harmonized kidney disease variable [KIQ020/KIQ022], capturing the presence of chronic kidney disease across survey cycles.\n\n\nKIQ020: Ever told you had weak/failing kidneys (Has a doctor or other health professional ever told you that you had weak or failing kidneys?)\n\nKIQ022: On regular dialysis (Are you now on regular dialysis?)\n\n\n8. Alcohol Use (ALQ) Data\nAlcohol use can interact with many medications, influencing their effectiveness and the risk of side effects.\n\n\nREGULAR_DRINKING: Harmonized regular drinker status [ALQ100/ALD100/ALQ110/ALQ120U/ALQ120Q]\n\n\nALQ100: Had at least 12 drinks in a year (Have you had at least 12 drinks of any kind of alcoholic beverage in any one year?)\n\nALQ120Q: Number of days per week/month/year drank alcohol over the past 12 months (On how many days per week, per month, or per year did you have at least one alcoholic drink?)\n\nALQ120U: Time unit for drinking frequency (week, month, or year; used in conjunction with ALQ120Q to specify drinking frequency over the past 12 months)\n\nALQ130: Days had 5 or more drinks in past year (During the past 12 months, on how many days did you have 5 or more drinks of any alcoholic beverage?)\n\n\n9. Physical Functioning Questionnaire (PFQ) Data\nPhysical functioning measures are important indicators of overall health and functional status, especially in older adults.\n\n\nPFQ090: Emergency room care in the last 12 months (During the past 12 months, have you been to a hospital emergency room about your health?)\n10. Health Questionnaire (HUQ) Data\nHealthcare utilization and self-reported health are important predictors of future healthcare needs and mortality.\n\n\nANYHOSP_INLASTYEAR: Harmonized indicator of any hospital visit in last year [HUD070/HUQ070/HUQ071]\n\n\nHUQ070: Overnight stay in the hospital in the past 12 months (In the past 12 months, how many times have you been hospitalized for one night or more?)\n\nHUD070: Overnight hospital stays in the past 12 months (In the past 12 months, how many nights have you spent in the hospital?)\n\nHUQ071: Seen a mental health professional in the past year (In the past 12 months, have you seen or talked to a mental health professional?)\n\n\n\nNUMHOSP_INLASTYEAR: Harmonized number of hospitalizations in last year [HUD080/HUQ080] (How many overnight hospital stays did you have in the past 12 months?)\n\nREGULAR_EDCARE: Harmonized type or place most often go for healthcare [HUQ040/HUQ041]\n\nHUQ090: Seen a mental health professional in past year (In the past 12 months, have you seen or talked to a mental health professional?)\n\nHUQ010: General health condition (Would you say your health in general is: Excellent, Very Good, Good, Fair, or Poor?)\n\nHUQ070, HUQ071: Hospital visits in the past year (indicator of healthcare utilization)\n11. Prescription Drug Data (RXQ)\nThe prescription drug data is extracted from the RXQ_RX and RXQ_DRUG tables, focusing on different classes of medications. The following are the specific drug classes and their associated codes from the dataset:\n\n\nbenzodiazepines: Medications typically used for anxiety, insomnia, and other conditions. The following drug codes were used for benzodiazepines:\n\n\nd00238, d00917, a56545, d00384, d00189, d03492, d00329, h00012, d05416, d03462, d00198, d00148, a54760, d04557, d00197, d00149, d00168, d00040, h00001, d00397, d00915, d00910, d04452, d07994\n\n\n\n\nopioids: Prescription pain medications. The drug codes for opioids include:\n\n\na10129, d00012, d00017, d00050, d00233, d00255, d00308, d00329, d00334, d00360, d00824, d00833, d00840, d03075, d03340, d03346, d03352, d03353, d03356, d03357, d03361, d03362, d03363, d03364, d03366, d03367, d03375, d03393, d03394, d03396, d03398, d03399, d03403, d03404, d03407, d03416, d03423, d03424, d03425, d03426, d03428, d03429, d03430, d03431, d03432, d03433, d03434, d03435, d03436, d03470, d03576, d03630, d03676, d03682, d03826, d03915, d04152, d04225, d04269, d04752, d04766, d04819, d04870, d04880, d04904, d04925, d05426, d06058, d06669, d07453, h00008, h00018\n\n\n\n\nSSRIs (Selective serotonin reuptake inhibitors): These are a class of antidepressants used to treat depression and anxiety disorders. The following drug codes represent SSRIs:\n\n\nd00236, d00880, d03157, d03804, d04332, d04812\n\n\n\n\nantidepressants: This category includes other antidepressants like SNRIs, MAOIs, or TCAs. The codes include:\n\nThese are medications used for treating major depressive disorders.\nAntidepressants codes: Derived from RDX/RX_DRUG data tables and include various types of antidepressants.\n\n\nantidepres_othr: Other antidepressants reported. Includes drugs classified under various types of antidepressants not captured by the major classes (SNRIs, MAOIs, SSRIs).\n\nantipsychotics: Medications used to manage psychosis (including schizophrenia and bipolar disorder). The codes for antipsychotics include:\n\n\nd00148, d00917, d00197, d00329, d03462, d04557, d05416, d03492, d00149, d00198, d00384, d00915, d04452, d00910, d07994, d00397, h00012, a54760, d00168, d00040, h00001\n\n\n\n\nanalgesics: Pain relief medications.\n\nCodes include: d00012, d03403, d03404, d03407, d03416, d03423, d03424, d03425, d03426, d03428, d03429, d03430, d03431, d03432, d03433, d03434, d03435, d03436, d03470\n\n\n\n\nmuscle_relax: Medications used to relax muscles and relieve pain caused by muscle conditions.\n\nCodes include: d03340, d03356\n\n\n\n\nanticonvulsants: Medications used to prevent or treat seizures.\n\nCodes include: d03423, d03436\n\n\n\n\nhormonalagents: Medications used for hormone replacement or modulation (such as estrogen, progesterone).\n\nCodes include: d04052, d00119\n\n\n\n\ngastrointestin: Medications used for gastrointestinal issues, such as GERD or ulcers.\n\nCodes include: d03075, d00149\n\n\n\n\ncardi_metab_ag: Cardiovascular and metabolic agents, including medications for hypertension, cholesterol management, etc.\n\nCodes include: d00578, d00913\n\n\n\n\nresp_antihist: Respiratory medications and antihistamines used for asthma, allergies, or other respiratory conditions.\n\nCodes include: d00221, d00360\n\n\n\ncns_morethan2: Indicates cases where &gt; 2 CNS (central nervous system) medications are reported.\nncns_lessthan5: Indicates cases where &lt; 5 NCNS (non-CNS) medications are reported.\n\nThe drug codes are mapped to each class of medications using the NHANES RXQ_RX and RXQ_DRUG tables.\nCode Hints\n\n# Loading packages\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")      \n# To wrangle data using Tidyverse principles\nif (!require(\"nhanesA\")) install.packages(\"nhanesA\")          \n# To load NHANES data directly from CDC\nif (!require(\"rio\")) install.packages(\"rio\")\n# A package to simplify data import/export\nlibrary(tidyverse)\nlibrary(nhanesA)\nlibrary(rio)\n\nMORTALITY DATA TABLE\nThis section loads the mortality data from NHANES, which has been linked to the National Death Index (NDI). Mortality data are critical for survival analyses, allowing us to study the relationship between health risk factors, treatments (such as medication use), and all-cause mortality. Variables like mortality status (mortstat) and follow-up time (permth_int, permth_exm) are used to calculate the time-to-event and identify if and when a participant passed away. See previous tutorial about how to access such data.\n\n## Specifying files\ninfile &lt;- list.files(paste0(getwd(), '/Data/nhanes-external-data/'), full.names = TRUE)\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'mortstat', 'permth_int', 'permth_exm', 'CYCLE')\n\n## Loading data\nd.nhanes &lt;-\n  lapply(X = infile[-10],\n         FUN = function(.x) read_fwf(file = .x,\n                                     col_types = \"ciiiiiiidd\",\n                                     fwf_cols(SEQN = c(1,6), \n                                              eligstat = c(15,15),\n                                              mortstat = c(16,16),\n                                              ucod_leading = c(17,19),\n                                              diabetes = c(20,20),\n                                              hyperten = c(21,21),\n                                              permth_int = c(43,45),\n                                              permth_exm = c(46,48)),\n                                     na = c(\"\", \".\"))) |&gt; \n  bind_rows(.id = 'CYCLE') |&gt;\n  select(any_of(vnames)) |&gt;\n  \n  ## Generating a survey cycle identifier\n  mutate(CYCLE = case_when(CYCLE == 1 ~ '1999-2000',\n                           CYCLE == 2 ~ '2001-2002',\n                           CYCLE == 3 ~ '2003-2004',\n                           CYCLE == 4 ~ '2005-2006',\n                           CYCLE == 5 ~ '2007-2008',\n                           CYCLE == 6 ~ '2009-2010',\n                           CYCLE == 7 ~ '2011-2012',\n                           CYCLE == 8 ~ '2013-2014',\n                           CYCLE == 9 ~ '2015-2016 [UNUSED]'))\n\nDEMOGRAPHIC DATA TABLE\nDemographic data from NHANES, including age, gender, race/ethnicity, and socioeconomic status, are essential covariates in the analysis. These variables help in adjusting for potential confounders when studying health outcomes like mortality. NHANES data is collected in multiple cycles, and variables like sampling weights (WTMEC2YR), masked variance strata (SDMVSTRA, SDMVPSU), and interview status (RIDSTATR) ensure accurate analysis using the complex survey design. See previous tutorial about how to access demographic data.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'CYCLE', 'SDMVSTRA', 'SDMVPSU', 'WTMEC2YR', 'RIDSTATR',\n            'RIDAGEYR', 'RIAGENDR', 'DMDEDUC2', 'INDFMPIR', 'RIDRETH1', 'DMDMARTL')\n\n## Loading data\n## NOTE: A variable `CYCLE` is generated reflecting the survey cycle\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'DEMO')   |&gt; mutate(CYCLE = '1999-2000') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_B') |&gt; mutate(CYCLE = '2001-2002') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_C') |&gt; mutate(CYCLE = '2003-2004') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_D') |&gt; mutate(CYCLE = '2005-2006') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_E') |&gt; mutate(CYCLE = '2007-2008') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_F') |&gt; mutate(CYCLE = '2009-2010') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_G') |&gt; mutate(CYCLE = '2011-2012') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DEMO_H') |&gt; mutate(CYCLE = '2013-2014') |&gt; select(any_of(vnames))) |&gt;\n  tibble() |&gt;\n  \n  ## Correcting inconsistencies with RIDSTATR coding\n  mutate(RIDSTATR = fct_collapse(RIDSTATR,\n                                 'interview only' = c('Interviewed Only', 'Interviewed only'),\n                                 'int + mec exam' = c('Both Interviewed and MEC examined',\n                                                      'Both interviewed and MEC examined'))) |&gt;\n  \n  ## Resetting `Don't know` and `Refused` factor levels to missing\n  mutate(across(where(is.factor), ~fct_recode(., \n                                              NULL = \"Don't know\",\n                                              NULL = \"Don't Know\",\n                                              NULL = \"Refused\")))\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.integer(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.integer(SEQN)), \n            by = join_by(SEQN, CYCLE))\n\nSMOKING QUESTIONNAIRE (SMQ) DATA TABLE\nSmoking is a significant health risk factor associated with various diseases. This section harmonizes data across cycles for the smoking variables. SMQ020 captures whether a participant has ever smoked, while SMQ040 assesses their current smoking frequency (daily, some days, or not at all). Harmonized smoking data is used in regression models to adjust for smoking-related risks.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'SMQ020', 'SMQ040')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'SMQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'SMQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nBLOOD PRESSURE (BPQ) DATA TABLE\nBlood pressure data are vital for assessing cardiovascular risk. High blood pressure (hypertension) is one of the primary risk factors for heart disease, stroke, and mortality. The data captures whether participants were diagnosed with high blood pressure (BPQ020), whether they are taking medications for hypertension (BPQ060), and if they have had their blood pressure checked recently (BPQ080).\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'BPQ020', 'BPQ060', 'BPQ080')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'BPQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'BPQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble() \n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nDIABETES QUESTIONNAIRE (DIQ) DATA TABLE\nDiabetes is a major chronic condition that impacts overall health and mortality risk. This section loads data on diabetes diagnoses (DIQ010). Diabetes status is essential in the analysis as it affects multiple organ systems and interacts with other risk factors like medication use, making it an important variable in mortality analysis.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'DIQ010')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'DIQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'DIQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nMEDICAL CONDITIONS (MCQ) DATA TABLE\nNHANES collects self-reported data on various medical conditions, many of which are associated with higher mortality. The data includes conditions such as stroke (MCQ160F), heart attack (MCQ160E), congestive heart failure (MCQ160B), and liver disease (MCQ160L). Harmonized variables like PULMOND_HARMONIZED combine data across cycles for conditions like emphysema, chronic bronchitis, and asthma. This section integrates these variables for assessing the health status of participants.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'MCQ160F', 'MCQ160E', 'MCQ160B', 'MCQ160G', 'MCQ160K', 'MCQ160O', 'MCQ160L',\n            'MCQ160A', 'MCQ220')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'MCQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'MCQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nKIDNEY CONDITIONS (KIQ) DATA TABLE\nChronic kidney disease (CKD) is associated with increased mortality due to its effects on cardiovascular health and drug metabolism. This section captures kidney disease status (KIQ020) and whether participants are on dialysis (KIQ022). These variables are used to stratify participants by kidney function in the analysis.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'KIQ020', 'KIQ022')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'KIQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'KIQ_U_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nALCOHOL USE (ALQ) DATA TABLE\nAlcohol use affects overall health and interacts with many medications. NHANES collects data on drinking habits, including the frequency of alcohol consumption (ALQ120Q, ALQ120U) and binge drinking behavior (ALQ130). Harmonized variables like REGULAR_DRINKING are derived from these data to capture regular alcohol use, which is a critical covariate in mortality models.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'ALQ100', 'ALD100', 'ALQ101', 'ALQ110', 'ALQ120Q', 'ALQ120U', 'ALQ130')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'ALQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'ALQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nPHYSICAL FUNCTIONING QUESTIONNAIRE (PFQ) DATA TABLE\nThe Physical Functioning Questionnaire assesses participants’ functional status, which is particularly important in older adults. This section includes data on emergency room visits in the past 12 months (PFQ090), which indicates acute health problems and healthcare utilization, often linked with higher mortality risk.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'PFQ090')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'PFQ')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'PFQ_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble()\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nHEALTH QUESTIONNAIRE (HUQ) DATA TABLE\nThe Health Questionnaire provides insight into healthcare utilization and self-reported health status, both of which are predictors of mortality. This section integrates variables like mental health visits (HUQ090), overall health ratings (HUQ010), and hospitalizations (ANYHOSP_INLASTYEAR, NUMHOSP_INLASTYEAR). These variables help capture participants’ interactions with the healthcare system and their overall well-being.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'HUQ090', 'HUQ010', 'HUQ030', 'HUQ040', 'HUQ041', 'HUQ020', 'HUQ070', \n            'HUQ071', 'HUD080', 'HUD070', 'HUQ080')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'HUQ')   |&gt; select(any_of(vnames)) |&gt; mutate(HUD080 = as.integer(HUD080)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'HUQ_H') |&gt; select(any_of(vnames)) |&gt; mutate(HUD080 = as.integer(HUD080))) |&gt;\n  tibble() |&gt;\n  \n  ## Standardizing coding of HUD080, for which valid values are only 1-6\n  mutate(HUD080 = case_when(HUD080 %in% 1:6 ~ HUD080, HUD080 == 7 | HUD080 == 99999 ~ NA))\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nPRESCRIPTION MEDICATIONS - DRUG DATA (RXQ_DRUG) TABLE\nPrescription drug data from NHANES is essential for analyzing the impact of medications on mortality. This section focuses on drug classes such as benzodiazepines, opioids, SSRIs, and other medications. Drug data are harmonized across cycles to account for polypharmacy and potential drug interactions. Variables like benzodiazepines, opioids, and SSRIs are used to assess whether the participant was taking these medications, which are the main exposures in the study.\n\n## BENZODIAZEPINES\nd.bz &lt;- c(\"d00238\", \"d00917\", \"a56545\", \"d00384\", \"d00189\", \n          \"d03492\", \"d00329\", \"h00012\", \"d05416\", \n          \"d03462\", \"d00198\", \"d00148\", \"a54760\", \"d04557\", \n          \"d00197\", \"d00149\", \"d00168\", \"d00040\",\n          \"h00001\", \"d00397\", \"d00915\", \"d00910\", \"d04452\", \n          \"d07994\")\n\n# OPIOIDS\nd.op &lt;- c(\"a10129\", \"d00012\", \"d00017\", \"d00050\", \"d00233\", \n          \"d00255\", \"d00308\", \"d00329\", \"d00334\", \n          \"d00360\", \"d00824\", \"d00833\", \"d00840\", \"d03075\", \n          \"d03340\", \"d03346\", \"d03352\", \"d03353\", \n          \"d03356\", \"d03357\", \"d03361\", \"d03362\", \"d03363\", \n          \"d03364\", \"d03366\", \"d03367\", \"d03375\", \n          \"d03393\", \"d03394\", \"d03396\", \"d03398\", \"d03399\", \n          \"d03403\", \"d03404\", \"d03407\", \"d03416\", \n          \"d03423\", \"d03424\", \"d03425\", \"d03426\", \"d03428\", \n          \"d03429\", \"d03430\", \"d03431\", \"d03432\", \n          \"d03433\", \"d03434\", \"d03435\", \"d03436\", \"d03470\", \n          \"d03576\", \"d03630\", \"d03676\", \"d03682\", \n          \"d03826\", \"d03915\", \"d04152\", \"d04225\", \"d04269\", \n          \"d04752\", \"d04766\", \"d04819\", \"d04870\", \n          \"d04880\", \"d04904\", \"d04925\", \"d05426\", \"d06058\", \n          \"d06669\", \"d07453\", \"h00008\", \"h00018\")\n\n# SSRIs\nd.ss &lt;- c(\"d00236\", \"d00880\", \"d03157\", \"d03804\", \"d04332\", \n          \"d04812\")\n\n## Wrangling prescription data\nref.rxq &lt;- \n  \n  ##-- Importing data\n  rio::import(infile[10], setclass = 'tibble') |&gt;\n  \n  ##-- Subsetting columns\n  select(RXDDRGID, \n         RXDDCI1C, RXDDCI2C, RXDDCI3C, RXDDCI4C,                       # Third-level category ID\n         RXDDCI1B, RXDDCI2B, RXDDCI3B, RXDDCI4B,                       # Second-level category ID\n         RXDDCI1A, RXDDCI2A, RXDDCI3A, RXDDCI4A) |&gt;                    # First-level category ID\n  \n  ##-- Changing `NA` to `0` in order to sum rowwise\n  mutate(across(c(RXDDCI1C:RXDDCI4A), ~ifelse(is.na(.), 0, .))) |&gt;\n  \n  ##-- RX classification\n  ##-- NOTE: Xu et al. (2020) provide insufficient instruction for replication, thus classification\n  ##--       here relies upon labels from the \"Multum Lexicon Therapeutic Classification Scheme\"\n  ##--       which can be found in the RXQ_DRUG table\n  mutate(.benzodiazepines = RXDDRGID %in% d.bz) |&gt;\n  mutate(.opioids = RXDDRGID %in% d.op) |&gt;\n  mutate(.ssris = RXDDRGID %in% d.ss) |&gt;\n  mutate(.antidepressants = rowSums(across(c(RXDDCI1C:RXDDCI4C), ~ . %in% c(250, 307, 308))) &gt; 0) |&gt;\n  mutate(.antidepres_othr = rowSums(across(c(RXDDCI1C:RXDDCI4C), ~ . %in% c(76, 209, 306))) &gt; 0) |&gt;\n  mutate(.antipsychotics = rowSums(across(c(RXDDCI1C:RXDDCI4C), ~ . %in% c(77, 210, 280, 341))) &gt; 0) |&gt;\n  mutate(.analgesics = rowSums(across(c(RXDDCI1B:RXDDCI4B), ~ . %in% c(58))) &gt; 0) |&gt;\n  mutate(.muscle_relaxants = rowSums(across(c(RXDDCI1B:RXDDCI4B), ~ . %in% c(73))) &gt; 0) |&gt;\n  mutate(.anticonvulsants = rowSums(across(c(RXDDCI1B:RXDDCI4B), ~ . %in% c(64))) &gt; 0) |&gt;\n  mutate(.hormonal_agents = rowSums(across(c(RXDDCI1A:RXDDCI4A), ~ . %in% c(97))) &gt; 0) |&gt;\n  mutate(.gastrointestinal_agents = rowSums(across(c(RXDDCI1A:RXDDCI4A), ~ . %in% c(87))) &gt; 0) |&gt;\n  mutate(.cardi_metab_agents = rowSums(across(c(RXDDCI1A:RXDDCI4A), ~ . %in% c(40, 331, 358))) &gt; 0) |&gt;\n  mutate(.respiratory_antihist = rowSums(across(c(RXDDCI1A:RXDDCI4A), ~ . %in% c(122))) |\n           rowSums(across(c(RXDDCI1C:RXDDCI4C), ~ . %in% c(246, 267, 382))) &gt; 0) |&gt;\n  mutate(.cns = rowSums(across(c(RXDDCI1A:RXDDCI4A), ~ . %in% c(57))) &gt; 0) |&gt;\n  select(RXDDRGID, starts_with('.'))\n\nPRESCRIPTION MEDICATIONS (RXQ_RX) TABLE\nThis chunk extracts prescription medication data from the NHANES RXQ_RX dataset, which records the medications reported by participants. Variables like RXD030 (indicating if the participant has taken any prescription medication in the past month) and RXDUSE (another medication use indicator) are included. The data is joined with the RXQ_DRUG table to classify medications into categories (e.g., benzodiazepines, opioids, SSRIs, etc.) for analysis. It converts the data into a wide format, where medication use across different categories is captured for each participant.\n\n## Specifying variables\nvnames &lt;- c('SEQN', 'RXD030', 'RXDUSE', 'RXDDRGID')\n\n## Loading data\nd.add &lt;-\n  bind_rows(nhanes(translated = TRUE, nh_table = 'RXQ_RX')   |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_B') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_C') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_D') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_E') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_F') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_G') |&gt; select(any_of(vnames)),\n            nhanes(translated = TRUE, nh_table = 'RXQ_RX_H') |&gt; select(any_of(vnames))) |&gt;\n  tibble() |&gt;\n  \n  ##-- Joining the RXQ_DRUG data table\n  left_join(x = _, y = ref.rxq, by = join_by(RXDDRGID)) |&gt;\n  \n  ##-- RXDDRGID is dropped\n  select(-RXDDRGID) |&gt;\n  \n  ##-- The long-form data are converted to wide-form\n  mutate(index = row_number(), .by = SEQN) |&gt;\n  pivot_wider(id_cols = c(SEQN, RXD030, RXDUSE),\n              names_from = index,\n              values_from = starts_with('.'))\n\n## Joining data tables\nd.nhanes &lt;- \n  full_join(x = d.nhanes |&gt; mutate(SEQN = as.numeric(SEQN)), \n            y = d.add    |&gt; mutate(SEQN = as.numeric(SEQN)), \n            by = join_by(SEQN))\n\nMerging\nThe merged dataset includes the main NHANES data, with harmonized variables from multiple sources. The process involves harmonizing factors like smoking, drinking, and medical conditions (e.g., pulmonary disease, kidney disease). Variables are standardized across different NHANES cycles for consistency. Key variables such as SMOKING_HARMONIZED, LIPIDEM_HARMONIZED, and REGULAR_DRINKING capture important lifestyle and medical information. The data also harmonizes prescription drug information by categorizing participants based on whether they reported taking medications like benzodiazepines, opioids, SSRIs, or other drug classes.\n\nd.final &lt;-\n  d.nhanes |&gt;\n  \n  ## Converting `Don't know` and `Refused` responses to NA for factors\n  mutate(across(where(is.factor), \n                ~fct_recode(., NULL = \"Don't know\", NULL = \"Don't Know\", NULL = \"Refused\"))) |&gt;\n  \n  ## Harmonizing `smoking` [SMQ020/SMQ040]\n  mutate(SMOKING_HARMONIZED = case_when(SMQ020 == 'No' ~ 'LOGICAL SKIP',\n                                        str_detect(SMQ040, 'Not at all') ~ 'Not at all',\n                                        str_detect(SMQ040, 'Some days') ~ 'Some days',\n                                        str_detect(SMQ040, 'Every day') ~ 'Every day') |&gt;\n           factor()) |&gt;\n  \n  ## Harmonizing 'lipidemia' [BPQ060/BPQ080]\n  mutate(LIPIDEM_HARMONIZED = case_when(.default = BPQ080,\n                                        BPQ060 == 'No' & CYCLE %in% c('1999-2000',\n                                                                      '2001-2002',\n                                                                      '2003-2004',\n                                                                      '2005-2006',\n                                                                      '2007-2008',\n                                                                      '2009-2010') ~ 'No')) |&gt;\n  \n  ## Harmonizing `pulmonary disease` [MCQ160G/MCQ160K/MCQ160O]\n  mutate(PULMOND_HARMONIZED = case_when(.default = 'No',\n                                        MCQ160G == 'Yes' | MCQ160K == 'Yes' | MCQ160O == 'Yes' ~ 'Yes',\n                                        is.na(MCQ160G) & is.na(MCQ160K) & is.na(MCQ160O) ~ NA) |&gt;\n           factor()) |&gt;\n  \n  ## Harmonizing `kidney disease` [KIQ020/KIQ022]\n  mutate(KIDNEYD_HARMONIZED = case_when(CYCLE == '1999-2000' ~ KIQ020,\n                                        !is.na(CYCLE) & CYCLE != '1999-2000' ~ KIQ022) |&gt;\n           factor()) |&gt;\n  \n  ## Harmonizing `regular drinking` [ALQ120Q/ALQ120U]\n  ## NOTE: Operationalized as \"on average &gt;= 1 times per week in last year (i.e., at least 52 times in past year).\n  ##       Here, we estimate the number of \"average drinking days\" per year and use that as an indicator.\n  ##       Respondents who were not shown ALQ120U are coded as 'No' (e.g., &lt; 12 drinks in past year)\n  mutate(ALQ120Q = case_when(ALQ120Q %in% c(777, 999) ~ NA, .default = ALQ120Q)) |&gt;\n  mutate(REGULAR_DRINKING = case_when(ALD100 == 'No' & ALQ110 %in% c('No', 'Yes') ~ 'No',\n                                      ALQ100 == 'No' & ALQ110 %in% c('No', 'Yes') ~ 'No',\n                                      ALQ101 == 'No' & ALQ110 %in% c('No', 'Yes') ~ 'No',\n                                      ALQ120U == 'Year'  & (ALQ120Q * 1)  &gt;= 52 ~ 'Yes',\n                                      ALQ120U == 'Year'  & (ALQ120Q * 1)  &lt;  52 ~ 'No',\n                                      ALQ120U == 'Month' & (ALQ120Q * 12) &gt;= 52 ~ 'Yes',\n                                      ALQ120U == 'Month' & (ALQ120Q * 12) &lt;  52 ~ 'No',\n                                      ALQ120U == 'Week'  & (ALQ120Q * 52) &gt;= 52 ~ 'Yes',\n                                      ALQ120U == 'Week'  & (ALQ120Q * 52) &lt;  52 ~ 'No')) |&gt;\n  \n  ## Harmonizing `any hospitalization in &lt; 1 yr` [HUD070/HUQ070/HUQ071]\n  mutate(ANYHOSP_INLASTYEAR = case_when(CYCLE == '1999-2000' ~ HUQ070,\n                                        CYCLE == '2001-2002' ~ HUD070,\n                                        CYCLE %in% c('2003-2004',\n                                                     '2005-2006',\n                                                     '2007-2008',\n                                                     '2009-2010',\n                                                     '2011-2012',\n                                                     '2013-2014') ~ HUQ071)) |&gt;\n  \n  ## Harmonizing `&gt; 2 overnight hospitalizations in 1 yr` [HUD080/HUQ080]\n  mutate(NUMHOSP_INLASTYEAR = case_when(CYCLE == '1999-2000' & HUQ070 == 'No'  ~ 0,\n                                        CYCLE == '1999-2000' & HUQ070 == 'Yes' ~ HUD080,\n                                        CYCLE == '2003-2004' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2003-2004' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2005-2006' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2005-2006' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2007-2008' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2007-2008' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2009-2010' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2009-2010' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2011-2012' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2011-2012' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2013-2014' & HUQ071 == 'No'  ~ 0,\n                                        CYCLE == '2013-2014' & HUQ071 == 'Yes' ~ HUD080,\n                                        CYCLE == '2001-2002' & HUD070 == 'No'  ~ 0,\n                                        CYCLE == '2001-2002' & HUD070 == 'Yes' & HUQ080 &lt; 6 ~ HUQ080,\n                                        CYCLE == '2001-2002' & HUD070 == 'Yes' & HUQ080 %in% 6:12 ~ 6) |&gt;\n           factor(levels = 0:6, labels = c('0', '1', '2', '3', '4', '5', '6 or more'))) |&gt;\n  \n  ## Harmonizing `regular ED care` [HUQ030/HUQ040/HUQ041]\n  mutate(REGULAR_EDCARE = case_when(CYCLE != '2013-2014' & HUQ030 == 'There is no place' ~ 'There is no place',\n                                    CYCLE != '2013-2014' & HUQ030 != 'There is no place' ~ HUQ040,\n                                    CYCLE == '2013-2014' & HUQ030 == 'There is no place' ~ 'There is no place',\n                                    CYCLE == '2013-2014' & HUQ030 != 'There is no place' ~ HUQ041) |&gt;\n           factor()) |&gt;\n  \n  ## Harmonizing `taken prescription medication in last month` [RDX030/RDXUSE]\n  mutate(TAKERX_INLASTMONTH = case_when(CYCLE %in% c('1999-2000', '2001-2002') ~ RXD030,\n                                        CYCLE %in% c('2003-2004',\n                                                     '2005-2006',\n                                                     '2007-2008',\n                                                     '2009-2010',\n                                                     '2011-2012',\n                                                     '2013-2014') ~ RXDUSE)) |&gt;\n  \n  ## To generate an indicator reflecting whether a given RX is observed across all RXs reported\n  ## by a given participant, we convert the TRUE/FALSE RX series into 0/1 integer vectors.\n  ## Values of NA are converted to 0, then the max value across all RXs is taken. Afterwards,\n  ## the resulting summary/indicator vectors are converted into factors ('No' v/s 'Yes'), and\n  ## values of 'No' are replaced with NA if `TAKERX_INLASTMONTH` is missing.\n  mutate(across(starts_with('.'), as.integer)) |&gt;\n  mutate(across(starts_with('.'), ~ifelse(is.na(.), 0, .))) |&gt;\n  mutate(benzodiazepines = do.call(pmax, c(across(starts_with(\".benzodiazepines\"))))) |&gt;\n  mutate(opioids = do.call(pmax, c(across(starts_with(\".opioids\"))))) |&gt;\n  mutate(ssris = do.call(pmax, c(across(starts_with(\".ssris\"))))) |&gt;\n  mutate(antidepressants = do.call(pmax, c(across(starts_with(\".antidepressants\"))))) |&gt;\n  mutate(antidepres_othr = do.call(pmax, c(across(starts_with(\".antidepres_othr\"))))) |&gt;\n  mutate(antipsychotics = do.call(pmax, c(across(starts_with(\".antipsychotics\"))))) |&gt;\n  mutate(analgesics = do.call(pmax, c(across(starts_with(\".analgesics\"))))) |&gt;\n  mutate(muscle_relax = do.call(pmax, c(across(starts_with(\".muscle_relaxants\"))))) |&gt;\n  mutate(anticonvulsant = do.call(pmax, c(across(starts_with(\".anticonvulsants\"))))) |&gt;\n  mutate(hormonalagents = do.call(pmax, c(across(starts_with(\".hormonal_agents\"))))) |&gt;\n  mutate(gastrointestin = do.call(pmax, c(across(starts_with(\".gastrointestinal_agents\"))))) |&gt;\n  mutate(cardi_metab_ag = do.call(pmax, c(across(starts_with(\".cardi_metab_agents\"))))) |&gt;\n  mutate(resp_antihist = do.call(pmax, c(across(starts_with(\".respiratory_antihist\"))))) |&gt;\n  mutate(across(c(benzodiazepines, opioids, ssris, antidepressants, antidepres_othr,\n                  antipsychotics, analgesics, muscle_relax, anticonvulsant, hormonalagents,\n                  gastrointestin, cardi_metab_ag, resp_antihist),\n                ~ifelse(is.na(TAKERX_INLASTMONTH), NA, .))) |&gt;\n  mutate(across(c(benzodiazepines, opioids, ssris, antidepressants, antidepres_othr,\n                  antipsychotics, analgesics, muscle_relax, anticonvulsant, hormonalagents,\n                  gastrointestin, cardi_metab_ag, resp_antihist),\n                ~factor(., levels = 0:1, labels = c('No', 'Yes')))) |&gt;\n  \n  ## The process above is repeated for the `&gt; 2 CNS medications` and `&lt; 5 Non-CNS medication`\n  ## indicators, with a slight variation in the steps.\n  mutate(cnsmax = do.call(pmax, c(across(starts_with(\".cns\"))))) |&gt;\n  mutate(cns_morethan2  = rowSums(across(starts_with('.cns'), ~ . %in% c(1))) &gt; 2) |&gt;\n  mutate(ncns_lessthan5 = rowSums(across(starts_with('.cns'), ~ . %in% c(0))) &lt; 5) |&gt;\n  mutate(across(c(cns_morethan2, ncns_lessthan5), ~ifelse(is.na(TAKERX_INLASTMONTH), NA, .))) |&gt;\n  mutate(across(c(cns_morethan2, ncns_lessthan5), \n                ~factor(., levels = c(FALSE, TRUE), labels = c('No', 'Yes')))) |&gt;\n  \n  ## Finalizing columns for export\n  select(\n    SEQN,                  # Respondent sequence number\n    mortstat,              # Derived or harmonized variable\n    permth_int,            # Derived or harmonized variable (related to mortality linkage)\n    permth_exm,            # Derived or harmonized variable (related to mortality linkage)\n    CYCLE,                 # Derived or harmonized variable (likely study cycle)\n    SDMVSTRA,              # Masked variance pseudo-stratum\n    SDMVPSU,               # Masked variance pseudo-PSU\n    WTMEC2YR,              # Full sample 2-year MEC exam weight\n    RIDSTATR,              # Interview/examination status\n    RIDAGEYR,              # Age in years at screening\n    RIAGENDR,              # Gender\n    DMDEDUC2,              # Education level (adults 20+)\n    INDFMPIR,              # Family monthly poverty income ratio\n    RIDRETH1,              # Race/ethnicity\n    DMDMARTL,              # Marital status\n    SMOKING_HARMONIZED,    # Harmonized smoking variable [SMQ020/SMQ040]\n    BPQ020,                # Ever told had high blood pressure\n    LIPIDEM_HARMONIZED,    # Lipidemia harmonized [BPQ060/BPQ080]\n    MCQ160F,               # Ever told you had a stroke\n    MCQ160E,               # Ever told you had a heart attack\n    DIQ010,                # Doctor told you have diabetes\n    MCQ160B,               # Ever told you had congestive heart failure\n    PULMOND_HARMONIZED,    # Harmonized pulmonary condition variable [MCQ160G/MCQ160K/MCQ160O]\n    MCQ160L,               # Ever told you had liver condition\n    MCQ160A,               # Ever told you had arthritis\n    KIDNEYD_HARMONIZED,    # Harmonized kidney disease variable [KIQ020/KIQ022]\n    MCQ220,                # Ever told you had cancer or malignancy\n    ANYHOSP_INLASTYEAR,    # Harmonized indicator of any hospital visit in last year [HUD070/HUQ070/HUQ071]\n    HUQ090,                # Seen a mental health professional in past year\n    HUQ010,                # General health condition [Excellent/Very Good/Good/Fair/Poor]\n    PFQ090,                # Last 12 months, received care at emergency room\n    REGULAR_EDCARE,        # Harmonized type or place most often go for healthcare [HUQ040/HUQ041]\n    HUQ020,                # Health condition compared to a year ago\n    NUMHOSP_INLASTYEAR,    # Harmonized number of hospitalizations in last year [HUD080/HUQ080]\n    TAKERX_INLASTMONTH,    # Harmonized taken prescription/medicine in past month [RDX030/RDXUSE]\n    benzodiazepines,       # Any benzodiazepines reported, harmonized from RDX/RDX_DRUG data tables\n    opioids,               # Any opioids reported, harmonized from RDX/RDX_DRUG data tables\n    ssris,                 # Any ssris reported, harmonized from RDX/RDX_DRUG data tables\n    antidepressants,       # Any anti-depressants (SNRIs, MAOIs, or TCAs) reported, harmonized from RDX/RDX_DRUG data tables\n    antidepres_othr,       # Any other anti-depressants reported, harmonized from RDX/RDX_DRUG data tables\n    antipsychotics,        # Any anti-psychotics reported, harmonized from RDX/RDX_DRUG data tables\n    analgesics,            # Any analgesics reported, harmonized from RDX/RDX_DRUG data tables\n    muscle_relax,          # Any muscle relaxants reported, harmonized from RDX/RDX_DRUG data tables\n    anticonvulsant,        # Any anti-convulsants reported, harmonized from RDX/RDX_DRUG data tables\n    hormonalagents,        # Any hormonal agents reported, harmonized from RDX/RDX_DRUG data tables\n    gastrointestin,        # Any gastrointestinal agents reported, harmonized from RDX/RDX_DRUG data tables\n    cardi_metab_ag,        # Any cardiac or metabolic medications reported, harmonized from RDX/RDX_DRUG data tables\n    resp_antihist,         # Any respiratory medicators or antihistamines reported, harmonized from RDX/RDX_DRUG data tables\n    cns_morethan2,         # &gt; 2 CNS medications reported, harmonized from RDX/RDX_DRUG data tables\n    ncns_lessthan5,        # &lt; 5 NCNS medications reported, harmonized from RDX/RDX_DRUG data tables\n    REGULAR_DRINKING       # Harmonized regular drinker status [ALQ100/ALD100/ALQ110/ALQ120U/ALQ120Q]\n  )\n\nExporting and Saving Data\nThis chunk exports the harmonized and processed NHANES dataset to an RDS file for future use. It also subsets the data based on specific NHANES cycles (1999-2000, 2001-2002, etc.), allowing for cycle-specific analysis. This ensures that both the full dataset and individual cycle-specific datasets are available for later use in mortality and health outcome analyses. The data includes key demographic, lifestyle, and prescription medication variables needed for analysis, particularly the survival and mortality data.\n\n## Exporting data\nrio::export(x = d.final, file = 'xu-et-al-2020-nhanes-data.rds')\n\n# Mortality data - cycle specific\ndat.mortality99 &lt;- subset(d.nhanes, CYCLE == '1999-2000')\ndat.mortality01 &lt;- subset(d.nhanes, CYCLE == '2001-2002')\ndat.mortality03 &lt;- subset(d.nhanes, CYCLE == '2003-2004')\ndat.mortality05 &lt;- subset(d.nhanes, CYCLE == '2005-2006')\ndat.mortality07 &lt;- subset(d.nhanes, CYCLE == '2007-2008')\ndat.mortality09 &lt;- subset(d.nhanes, CYCLE == '2009-2010')\ndat.mortality11 &lt;- subset(d.nhanes, CYCLE == '2011-2012')\ndat.mortality13 &lt;- subset(d.nhanes, CYCLE == '2013-2014')\n\n# NHANES data - cycle specific\nd.final &lt;- readRDS(\"xu-et-al-2020-nhanes-data.rds\")\nd.final$mortstat &lt;- d.final$permth_int &lt;- d.final$permth_exm &lt;- NULL\n\ndat.nhanes99 &lt;- subset(d.final, CYCLE == '1999-2000')\ndat.nhanes01 &lt;- subset(d.final, CYCLE == '2001-2002')\ndat.nhanes03 &lt;- subset(d.final, CYCLE == '2003-2004')\ndat.nhanes05 &lt;- subset(d.final, CYCLE == '2005-2006')\ndat.nhanes07 &lt;- subset(d.final, CYCLE == '2007-2008')\ndat.nhanes09 &lt;- subset(d.final, CYCLE == '2009-2010')\ndat.nhanes11 &lt;- subset(d.final, CYCLE == '2011-2012')\ndat.nhanes13 &lt;- subset(d.final, CYCLE == '2013-2014')\n\n# Save mortality and NHANES datasets\nsave(dat.mortality99, dat.nhanes99,\n     dat.mortality01, dat.nhanes01,\n     dat.mortality03, dat.nhanes03,\n     dat.mortality05, dat.nhanes05,\n     dat.mortality07, dat.nhanes07,\n     dat.mortality09, dat.nhanes09,\n     dat.mortality11, dat.nhanes11,\n     dat.mortality13, dat.nhanes13,\n     file = \"Data/nhanes_mortality_1999_2014.RData\")",
    "crumbs": [
      "Accessing data",
      "Replicate Data Creation"
    ]
  },
  {
    "objectID": "accessingF.html",
    "href": "accessingF.html",
    "title": "R Functions (A)",
    "section": "",
    "text": "The section introduces a set of R functions useful for accessing and processing complex survey data, providing their descriptions and the packages they belong to.\n\n\n\n\n\nFunction_name\nPackage_name\nDescription\n\n\n\napply\nbase\nApplies a function over an array or matrix.\n\n\ncut\nbase\nConverts a numeric variable to a factor variable.\n\n\nmerge\nbase/data.table\nMerges multiple datasets.\n\n\nnames\nbase\nRetrieves the names of an object.\n\n\nnhanes\nnhanesA\nDownloads a NHANES datafile.\n\n\nnhanesTables\nnhanesA\nLists available variables within a datafile.\n\n\nnhanesTranslate\nnhanesA\nEncodes categorical variables to match with certain standards, e.g., CDC website.\n\n\nrecode\ncar\nRecodes a variable.\n\n\n\n\n\n\nFor more information, visit the resources mentioned earlier.",
    "crumbs": [
      "Accessing data",
      "R Functions (A)"
    ]
  },
  {
    "objectID": "accessingQ.html",
    "href": "accessingQ.html",
    "title": "Quiz (A)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Accessing data",
      "Quiz (A)"
    ]
  },
  {
    "objectID": "accessingQ.html#live-quiz",
    "href": "accessingQ.html#live-quiz",
    "title": "Quiz (A)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Accessing data",
      "Quiz (A)"
    ]
  },
  {
    "objectID": "accessingQ.html#download-quiz",
    "href": "accessingQ.html#download-quiz",
    "title": "Quiz (A)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here. If not downloading immediately, right-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Accessing data",
      "Quiz (A)"
    ]
  },
  {
    "objectID": "accessingS.html",
    "href": "accessingS.html",
    "title": "App (A)",
    "section": "",
    "text": "Below is an example of an app that utilizes NHANES demographic datasets following the tutorial materials. Users can tabulate and visualize the data summaries from the downloaded data from a selected NHANES cycle.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveA\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, nhanesA, DataExplorer, Hmisc, dplyr, and tableone packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Accessing data",
      "App (A)"
    ]
  },
  {
    "objectID": "accessingE.html",
    "href": "accessingE.html",
    "title": "Exercise 1 (A)",
    "section": "",
    "text": "Problem Statement\nWe will use the article by Palis, Marchand, and Oviedo-Joekes (2020), DOI: 10.1080/09638237.2018.1437602.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A)"
    ]
  },
  {
    "objectID": "accessingE.html#problem-statement",
    "href": "accessingE.html#problem-statement",
    "title": "Exercise 1 (A)",
    "section": "",
    "text": "Download the CCHS MH topical index\n\nDownload the CCHS MH Data Dictionary",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A)"
    ]
  },
  {
    "objectID": "accessingE.html#question-1-60-grade",
    "href": "accessingE.html#question-1-60-grade",
    "title": "Exercise 1 (A)",
    "section": "Question 1: [60% grade]",
    "text": "Question 1: [60% grade]\n1(a) Importing dataset\n\n# Importing dataset\nload(\"Data/accessing/cchsMH.RData\")\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper\n\nIdentify the variable needed for eligibility criteria\n\nHint\n\nRead the first paragraph of Analytic sample (page 2) for the eligibility criteria\nEligibility criteria was determined based on only one variable. Only work with ‘YES’ category.\n\n\n# your code here\n\n1(c) Retaining necessary variables\nIn the dataset, retain only the variables associated with outcome measure, explanatory variable, potential confounders and survey weight. There should be eight variables (one outcome, one exposure, five confounders, and one survey weight).\nHere are the steps:\n\nIdentify the outcome variable\nIdentify the explanatory variable\nIdentify the potential confounders\nIdentify the survey weight variable\n\nHint\n\nRead\n\n\nfirst and second paragraphs of Study variables for the outcome, explanatory and confounding variables\nthird paragraph of the Statistical analyses for the survey weights variable.\n\n\nThere were five potential confounders.\nPotentially useful functions for this exercise:\n\n\n%in%\nlevels\nrecode\nsubset\nas.factor\nrelevel\nor dplyr ways: filter, select\n\n\n\n\n\n# your code here\n\n1(d) Creating analytic dataset\nOutcome variable has a category ‘NOT STATED’, but for our analysis, we will omit anyone associated with this category. Similarly, for explanatory variable, we have categories such as DON’T KNOW, REFUSAL and NOT STATED. We will omit anyone with these categories.\n\nAssign missing values for categories such as DON’T KNOW, REFUSAL and NOT STATED.\nRecode the variables as shown in Table 1 in the article. You can use any function/package of your choice. Here is an example (but feel free to use other functions. In R there are many other ways to do this same task.\n\n\n## your code here\n# levels(your.data.frame$your.age.variable) &lt;- \n#   list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n#        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n#        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n#        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n#        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n#        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n#        \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n1(e) Number of columns and variable names\nReport the number of columns in your analytic dataset, and the variable names.\n\n# your code here",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A)"
    ]
  },
  {
    "objectID": "accessingE.html#question-2-table-1-20-grade",
    "href": "accessingE.html#question-2-table-1-20-grade",
    "title": "Exercise 1 (A)",
    "section": "Question 2: Table 1 [20% grade]",
    "text": "Question 2: Table 1 [20% grade]\nReproduce Table 1 presented in the article (or see below). Omit the ‘Main source of income’ variable from the table. The table you produce should report numbers as follows, with all columns as shown in the table. In other words, the numbers should match.\n\n\n\n\n\n\n\n\n\nSelf-rated Mental Health Variable\nTotal n(%)\nPoor or Fair n(%)\nGood n(%)\nVery good or excellent n(%)\n\n\n\nStudy sample\n2628 (100)\n1002 (38.1)\n885 (33.7)\n741 (28.2)\n\n\nCommunity belonging\n\n\n\n\n\n\n- Very weak\n480 (18.3)\n282 (28.1)\n118 (13.3)a\n80 (10.8)a\n\n\n- Somewhat weak\n857 (32.6)\n358 (35.7)\n309 (34.9)\n190 (25.6)\n\n\n- Somewhat strong\n1005 (38.2)\n288 (28.7)\n362 (40.9)\n355 (47.9)\n\n\n- Very strong\n286 (10.9)\n74 (7.4)a\n96 (10.8)a\n116 (15.7)a\n\n\nSex\n\n\n\n\n\n\n- Females\n1407 (53.5)\n616 (61.5)\n487 (55.0)\n304 (41.0)\n\n\n- Males\n1221 (46.5)\n386 (38.5)\n398 (45.0)\n437 (59.0)\n\n\nAge group\n\n\n\n\n\n\n- 15 to 24 years\n740 (28.2)\n191 (19.1)\n264 (29.8)\n285 (38.5)\n\n\n- 25 to 34 years\n475 (18.1)\n141 (14.1)\n167 (18.9)\n167 (22.5)\n\n\n- 35 to 44 years\n393 (15.0)\n185 (18.5)\n119 (13.4)a\n89 (12.0)a\n\n\n- 45 to 54 years\n438 (16.6)\n220 (22.0)\n139 (15.7)\n79 (10.7)a\n\n\n- 55 to 64 years\n379 (14.4)\n198 (19.7)\n113 (12.8)a\n68 (9.2)a\n\n\n- 65 years or older\n203 (7.7)\n67 (6.6)a\n83 (8.4)a\n53 (7.1)b\n\n\nRace/Ethnicity\n\n\n\n\n\n\n- Non-white\n458 (17.4)\n184 (18.4)\n140 (15.8)\n134 (18.1)\n\n\n- White\n2170 (82.6)\n818 (81.6)\n745 (84.2)\n607 (81.9)\n\n\nMain source of income\n\n\n\n\n\n\n- Employment Income^d\n1054 (40.1)\n289 (28.8)\n386 (43.6)\n379 (51.1)\n\n\n- Worker’s Compensation^e\n160 (6.1)\n91 (9.1)a\n44 (5.0)b\n25 (3.4)c\n\n\n- Senior Benefits^f\n134 (5.1)\n57 (5.7)a\n42 (4.7)b\n35 (4.7)\n\n\n- Other^g\n184 (7.0)\n82 (8.2)a\n60 (6.8)a\n42 (5.7)b\n\n\n- Not applicable^h\n851 (32.4)\n402 (40.1)\n263 (29.7)\n186 (25.1)\n\n\n- Not Stated^i\n245 (9.3)\n81 (8.1)a\n90 (10.2)a\n74 (10.0)\n\n\n\n\\(^a\\) Coefficient of variation between 16.6 and 25.0%. \\(^b\\) Coefficient of variation between 25.1 and 33.3%. \\(^c\\) Coefficient of variation &gt; 33.3%. \\(^d\\) Employment Income: Wages/salaries or self-employment. \\(^e\\) Worker’s compensation: Employment insurance or worker’s compensation or social assistance/welfare. \\(^f\\) Senior Benefits: Benefits from Canada or Quebec Pension Plan or job related retirement pensions, superannuation and annuities or RRSP/RRIF of Old Age Security and Guaranteed Income Supplement. \\(^g\\) Other: Dividends/interest or child tax benefit or child support or alimony or other or no income. \\(^h\\) Not applicable: Respondents who live in a household with only one person. The income variable “main source of personal income” is applicable only to those that live in a household of more than one person. \\(^i\\) Not Stated: Question was not answered (don’t know, refusal, not stated).\n\n# your code here\nrequire(tableone)",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A)"
    ]
  },
  {
    "objectID": "accessingE.html#question-3-20-grade",
    "href": "accessingE.html#question-3-20-grade",
    "title": "Exercise 1 (A)",
    "section": "Question 3: [20% grade]",
    "text": "Question 3: [20% grade]\n3(a) Subset\nSubset the dataset excluding ‘Very good or excellent’ responses from the self-rated mental health variable\n\n# your code here\n\n3(b) Recode\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\n# your code here\n\n3(c) Regression\nRun a logistic regression model for finding the relationship between community belonging (Reference: Very weak) and self-rated mental health (Reference: Poor) among respondents with mental or substance use disorders. Adjust the model for three confounders: sex, age, and race/ethnicity. Do not need to report summary of the model.\n\n# your code here\n\n3(d) Reporting odds ratio\nReport the odds ratios and associated confidence intervals. Publish or jtools package could be useful to report the odds ratios with confidence intervals.\n\n# your code here\n\n\n\n\n\nPalis, Heather, Kirsten Marchand, and Eugenia Oviedo-Joekes. 2020. “The Relationship Between Sense of Community Belonging and Self-Rated Mental Health Among Canadians with Mental or Substance Use Disorders.” Journal of Mental Health 29 (2): 168–75.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html",
    "href": "accessingEsolution.html",
    "title": "Exercise 1 Solution (A)",
    "section": "",
    "text": "Question 1:\nWe will use the following article:\nPalis, Marchand & Oviedo-Joekes. (2020). The relationship between sense of community belonging and self-rated mental health among Canadians with mental or substance use disorders. Journal of Mental Health, 29(2): 168-175. DOI: 10.1080/09638237.2018.1437602 (available in the “Library Online Course Reserves”: open link.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#a-importing-dataset",
    "href": "accessingEsolution.html#a-importing-dataset",
    "title": "Exercise 1 Solution (A)",
    "section": "1(a) Importing dataset",
    "text": "1(a) Importing dataset\n\n# Importing dataset\nload(\"Data/accessing/cchsMH.RData\")",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#b-subsetting-according-to-eligibility",
    "href": "accessingEsolution.html#b-subsetting-according-to-eligibility",
    "title": "Exercise 1 Solution (A)",
    "section": "1(b) Subsetting according to eligibility",
    "text": "1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper\n\nIdentify the variable needed for eligibility criteria\n\nHint\n\nRead the first paragraph of Analytic sample (page 2) for the eligibility criteria\nEligibility criteria was determined based on only one variable. Only work with ‘YES’ category.\n\n\n# Subsetting according to eligibility\ndat &lt;- subset(cmh, MHPFY==\"YES\")",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#c-retaining-necessary-variables",
    "href": "accessingEsolution.html#c-retaining-necessary-variables",
    "title": "Exercise 1 Solution (A)",
    "section": "1(c) Retaining necessary variables",
    "text": "1(c) Retaining necessary variables\nIn the dataset, retain only the variables associated with outcome measure, explanatory variable, potential confounders and survey weight. There should be eight variables (one outcome, one exposure, five confounders, and one survey weight).\nHere are the steps:\n\nIdentify the outcome variable\nIdentify the explanatory variable\nIdentify the potential confounders\nIdentify the survey weight variable\n\nHint\n\nRead\n\n\nfirst and second paragraphs of Study variables for the outcome, explanatory and confounding variables\nthird paragraph of the Statistical analyses for the survey weights variable.\n\n\nThere were five potential confounders.\nPotentially useful functions for this exercise:\n\n\n%in%\nlevels\nrecode\nsubset\nas.factor\nrelevel\nor dplyr ways: filter, select\n\n\n\n\n\ndat &lt;- with(dat, data.frame(srmh = SCR_082, # Outcome - SMRH\n                            community = GEN_10, # explanatory - community belonging\n                            sex = DHH_SEX, # sex\n                            age = DHHGAGE, # age\n                            race = SDCGCGT, # respondent's racial identity\n                            income = INCG7, # main source of income\n                            help = PNC_01A, # received help for problems\n                            weight = WTS_M)) # sampling weight",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#e-creating-analytic-dataset",
    "href": "accessingEsolution.html#e-creating-analytic-dataset",
    "title": "Exercise 1 Solution (A)",
    "section": "1(e) Creating analytic dataset",
    "text": "1(e) Creating analytic dataset\nOutcome variable has a category ‘NOT STATED’, but for our analysis, we will omit anyone associated with this category. Similarly, for explanatory variable, we have categories such as DON’T KNOW, REFUSAL and NOT STATED. We will omit anyone with these categories.\n\nAssign missing values for categories such as DON’T KNOW, REFUSAL and NOT STATED.\nRecode the variables as shown in Table 1 in the article. You can use any function/package of your choice. Here is an example (but feel free to use other functions. In R there are many other ways to do this same task.\n\n\n## your code here\n# levels(your.data.frame$your.age.variable) &lt;- \n#   list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n#        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n#        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n#        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n#        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n#        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n#        \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n# Outcome variable: Self-rated Mental Health\n#table(dat$srmh, useNA = \"always\")\ndat$srmh &lt;- car::recode(dat$srmh, \" c('FAIR','POOR') = 'Poor or Fair'; \n                        'GOOD' = 'Good'; c('EXCELLENT', 'VERY GOOD') = \n                        'Very good or excellent'; else = NA \")\ndat$srmh &lt;- factor(dat$srmh, levels=c(\"Poor or Fair\", \"Good\", \"Very good or excellent\"))\n\n# Explanatory variable: Community belonging\n#table(dat$community, useNA = \"always\")\ndat$community &lt;- recode(dat$community, recodes = \" 'VERY STRONG' = 'Very strong';\n                        'SOMEWHAT STRONG' = 'Somewhat strong'; 'SOMEWHAT WEAK' = \n                        'Somewhat weak'; 'VERY WEAK' = 'Very weak'; else = NA \")\ndat$community &lt;- factor(dat$community, levels = c(\"Very weak\", \"Somewhat weak\",\n                                                  \"Somewhat strong\", \"Very strong\"))\n\n# Sex\n#table(dat$sex, useNA = \"always\")\ndat$sex &lt;- recode(dat$sex, recodes = \"'MALE' = 'Males'; 'FEMALE' = 'Females'; \n                  else = NA\")\n\n# Age group\n#table(dat$age, useNA = \"always\")\nlevels(dat$age) &lt;- list(\"15 to 24 years\" = c(\"15 TO 19 YEARS\", \"20 TO 24 YEARS\"),\n                        \"25 to 34 years\" = c(\"25 TO 29 YEARS\", \"30 TO 34 YEARS\"),\n                        \"35 to 44 years\" = c(\"35 TO 39 YEARS\", \"40 TO 44 YEARS\"),\n                        \"45 to 54 years\" = c(\"45 TO 49 YEARS\", \"50 TO 54 YEARS\"),\n                        \"55 to 64 years\" = c(\"55 TO 59 YEARS\", \"60 TO 64 YEARS\"),\n                        \"65 years or older\" = c(\"65 TO 69 YEARS\", \"70 TO 74 YEARS\", \n                                                \"75 TO 79 YEARS\", \"80 YEARS OR MORE\"))\n\n# Race/Ethnicity\n#table(dat$race, useNA = \"always\")\ndat$race &lt;- recode(dat$race, \" 'WHITE'='White'; 'NON-WHITE'='Non-white'; else=NA \")\n\n# Income\n#table(dat$income, useNA = \"always\") \nlevels(dat$income) &lt;- list(\"Employment Income\" = \"EMPLOYMENT INC.\",\n                           \"Worker's Compensation\" = \"EI/WORKER'S COMP\",\n                           \"Senior Benefits\" = \"SENIOR BENEFITS\", \n                           \"Other\" = \"OTHER\",\n                           \"Not applicable\" = \"NOT APPLICABLE\")",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#f-number-of-columns-and-variable-names",
    "href": "accessingEsolution.html#f-number-of-columns-and-variable-names",
    "title": "Exercise 1 Solution (A)",
    "section": "1(f) Number of columns and variable names",
    "text": "1(f) Number of columns and variable names\nReport the number of columns in your analytic dataset, and the variable names.\n\n# Number of columns\nncol(dat)\n#&gt; [1] 8\n\n# Variable names\nnames(dat)\n#&gt; [1] \"srmh\"      \"community\" \"sex\"       \"age\"       \"race\"      \"income\"   \n#&gt; [7] \"help\"      \"weight\"",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#a-subset",
    "href": "accessingEsolution.html#a-subset",
    "title": "Exercise 1 Solution (A)",
    "section": "3(a) Subset",
    "text": "3(a) Subset\nSubset the dataset excluding ‘Very good or excellent’ responses from the self-rated mental health variable\n\ndat3 &lt;- dplyr::filter(dat, srmh != \"Very good or excellent\")",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#b-recode",
    "href": "accessingEsolution.html#b-recode",
    "title": "Exercise 1 Solution (A)",
    "section": "3(b) Recode",
    "text": "3(b) Recode\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\ndat3$srmh &lt;- recode(dat3$srmh, recodes = \" 'Poor or Fair' = 'Poor'; 'Good' = 'Good'; \n                    else = NA\", levels = c(\"Poor\", \"Good\"))",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#c-regression",
    "href": "accessingEsolution.html#c-regression",
    "title": "Exercise 1 Solution (A)",
    "section": "3(c) Regression",
    "text": "3(c) Regression\nRun a logistic regression model for finding the relationship between community belonging (Reference: Very weak) and self-rated mental health (Reference: Poor) among respondents with mental or substance use disorders. Adjust the model for three confounders: sex, age, and race/ethnicity. Do not need to report summary of the model.\n\nfit &lt;- glm(I(srmh==\"Good\") ~ community + sex + age + race, data = dat3, \n           family = binomial)",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingEsolution.html#d-reporting-odds-ratio",
    "href": "accessingEsolution.html#d-reporting-odds-ratio",
    "title": "Exercise 1 Solution (A)",
    "section": "3(d) Reporting odds ratio",
    "text": "3(d) Reporting odds ratio\nReport the odds ratios and associated confidence intervals. Publish or jtools package could be useful to report the odds ratios with confidence intervals.\n\nrequire(Publish)\npublish(fit)\n#&gt;   Variable             Units OddsRatio       CI.95    p-value \n#&gt;  community         Very weak       Ref                        \n#&gt;                Somewhat weak      1.93 [1.48;2.53]    &lt; 1e-04 \n#&gt;              Somewhat strong      2.90 [2.22;3.80]    &lt; 1e-04 \n#&gt;                  Very strong      3.32 [2.27;4.85]    &lt; 1e-04 \n#&gt;        sex           Females       Ref                        \n#&gt;                        Males      1.32 [1.09;1.60]   0.003993 \n#&gt;        age    15 to 24 years       Ref                        \n#&gt;               25 to 34 years      0.85 [0.63;1.15]   0.292243 \n#&gt;               35 to 44 years      0.45 [0.33;0.61]    &lt; 1e-04 \n#&gt;               45 to 54 years      0.45 [0.34;0.61]    &lt; 1e-04 \n#&gt;               55 to 64 years      0.41 [0.30;0.56]    &lt; 1e-04 \n#&gt;            65 years or older      0.87 [0.59;1.27]   0.468623 \n#&gt;       race         Non-white       Ref                        \n#&gt;                        White      1.32 [1.03;1.71]   0.030025",
    "crumbs": [
      "Accessing data",
      "Exercise 1 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE1vibep1.html",
    "href": "accessingE1vibep1.html",
    "title": "Exercise 1 (A) Vibe Part 1",
    "section": "",
    "text": "Load data and required packages\nIn this tutorial, we will be going through the following exercise (found here).\nThis exercise replicates the analysis done in this article:\nPalis, Marchand & Oviedo-Joekes (2020). The relationship between sense of community belonging and self-rated mental health among Canadians with mental or substance use disorders. Journal of Mental Health, 29(2): 168–175. DOI: 10.1080/09638237.2018.1437602\nWe will go through the exercise questions and evaluate how well Gemini performs on each task. These tasks include applying eligibility criteria, subsetting the data in terms of variables and retaining those necessary for analysis, creating an analytic dataset, creating a Table 1, and building a logistic regression model, and presenting odds ratios with 95% confidence intervals. Additionally, we will use what we learned from the previous tutorial (found here) to streamline these exercises.\nIn this part of the tutorial, we will go through the first set of tasks, which includes loading, subsetting the data according to eligibility criteria and necessary variables, creating an analytic dataset, and reporting the number of columns and variable names in this dataset. These tutorials were completed in RStudio (Posit team 2023) using R version 4.3.2 (R Core Team 2023), with the following packages: dplyr (Wickham et al. 2023), car (Fox and Weisberg 2019), and forcats (Wickham 2023).\nFor this tutorial, I used Gemini’s free 2.5 Flash model.\nThe data used in this exercise can be found here.\npackages &lt;- c(\"dplyr\", \"car\", \"forcats\")\n\nlapply(packages, function(pkg) {\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg, dependencies = TRUE)\n  library(pkg, character.only = TRUE)\n})\n\ndirectory &lt;- \"Data/accessing\"  # Enter the path to your working directory here\n\nload(paste0(directory, \"/cchsMH.RData\"))\nThe data dictionary for this dataset can be found here. Download this, as we will need to send it to Gemini.\n(Note that in the exercise, exercise 1a is for loading the dataset. We will start at 1b as we have already loaded the data)",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A) Vibe Part 1"
    ]
  },
  {
    "objectID": "accessingE1vibep1.html#references",
    "href": "accessingE1vibep1.html#references",
    "title": "Exercise 1 (A) Vibe Part 1",
    "section": "References",
    "text": "References\n\n\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nPosit team. 2023. RStudio: Integrated Development Environment for r. Boston, MA: Posit Software, PBC. http://www.posit.co/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2023. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A) Vibe Part 1"
    ]
  },
  {
    "objectID": "accessingE1vibep2.html",
    "href": "accessingE1vibep2.html",
    "title": "Exercise 1 (A) Vibe Part 2",
    "section": "",
    "text": "Load data and required packages\nIn this tutorial, we will be going through the following exercise (found here).\nThis exercise replicates the analysis done in this article:\nPalis, Marchand & Oviedo-Joekes (2020). The relationship between sense of community belonging and self-rated mental health among Canadians with mental or substance use disorders. Journal of Mental Health, 29(2): 168–175. DOI: 10.1080/09638237.2018.1437602\nWe will go through the exercise questions and evaluate how well Gemini performs on each task. These tasks include applying eligibility criteria, subsetting the data in terms of variables and retaining those necessary for analysis, creating an analytic dataset, creating a Table 1, and building a logistic regression model, and presenting odds ratios with 95% confidence intervals. Additionally, we will use what we learned from the previous tutorial (found here) to streamline these exercises.\nIn this part of the tutorial, we will go through the second set of tasks, which includes creating a Table 1, performing logistic regression and producing odds ratios and confidence intervals. These tutorials were completed in RStudio (Posit team 2023) using R version 4.3.2 (R Core Team 2023), with the following packages: dplyr (Wickham et al. 2023), car (Fox and Weisberg 2019), tableone (Yoshida and Bartel 2022), and Publish (Gerds and Ozenne 2023).\nFor this tutorial, I used Gemini’s free 2.5 Flash model.\nLoad the data saved in the previous tutorial. If you just completed the last tutorial and still have the data loaded in your environment, you can skip this step.\npackages &lt;- c(\"dplyr\", \"car\", \"tableone\", \"Publish\")\n\nlapply(packages, function(pkg) {\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg, dependencies = TRUE)\n  library(pkg, character.only = TRUE)\n})\n\n\ndirectory &lt;- \"Data/accessing\"  # Enter the path to your working directory here\n\nload(paste0(directory, \"/analytic_data.RData\"))",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A) Vibe Part 2"
    ]
  },
  {
    "objectID": "accessingE1vibep2.html#references",
    "href": "accessingE1vibep2.html#references",
    "title": "Exercise 1 (A) Vibe Part 2",
    "section": "References",
    "text": "References\n\n\nExercise 2a: Recode\nFor this first exercise, we need to recreate the Table 1 from the article. However, we need to omit the ‘Main source of income’ variable from the table.\nLet’s send Gemini the following prompt:\nPrompt Sent to Gemini:\n\n\n\n\n\n\nUser Prompt\n\n\n\nReproduce Table 1 from the article using the tableone package in RStudio, based on the cleaned analytic dataset.\n\n\nStratify the table by the variable srmh (this will be used for the columns).\nInclude the following variables in the table:\n\ncommunity\nsex\nage\nrace\n\n\nExclude the variable ‘Main source of income’ (income) from the table.\nDisplay counts and percentages for categorical variables, and means with standard deviations for continuous variables.\nEnsure that the frequencies and percentages match those reported in the published Table 1.\nUse the kableone function to print the table.\n\n\n\nGemini’s response:\nHere is the code to create a Table 1 using the tableone packages:\n\n\n\n\n\n\nGemini Response\n\n\n\n\n# Define the variables to be included in Table 1\nmyVars &lt;- c(\"community\", \"sex\", \"age\", \"race\")\n\n# Define the stratifying variable (self-rated mental health)\nstrata &lt;- \"srmh\"\n\n# Create Table 1 object with specified options\ntableOne &lt;- CreateTableOne(\n  vars = myVars,\n  data = analytic_data,\n  strata = strata,\n  includeNA = FALSE, # Do not include NA counts/percentages\n  test = FALSE,      # Do not perform or display statistical tests (p-values)\n  addOverall = TRUE  # Add an overall column\n)\n\n# Print the table using kableone with desired formatting options\nprint(\n  kableone(\n    tableOne,\n    showAllLevels = TRUE,  # Show all levels for categorical variables\n    padColnames = TRUE,    # Pad column names for alignment\n    insertLevel = TRUE     # Insert a level column for hierarchical variables\n  )\n)\n#&gt; \n#&gt; \n#&gt; |              |      level       |   Overall  |Poor or Fair |    Good   |Very good or excellent |\n#&gt; |:-------------|:-----------------|:-----------|:------------|:----------|:----------------------|\n#&gt; |n             |                  |2628        |1002         |885        |741                    |\n#&gt; |community (%) |Very weak         |480 (18.3)  |282 (28.1)   |118 (13.3) |80 (10.8)              |\n#&gt; |              |Somewhat weak     |857 (32.6)  |358 (35.7)   |309 (34.9) |190 (25.6)             |\n#&gt; |              |Somewhat strong   |1005 (38.2) |288 (28.7)   |362 (40.9) |355 (47.9)             |\n#&gt; |              |Very strong       |286 (10.9)  |74 ( 7.4)    |96 (10.8)  |116 (15.7)             |\n#&gt; |sex (%)       |Females           |1407 (53.5) |616 (61.5)   |487 (55.0) |304 (41.0)             |\n#&gt; |              |Males             |1221 (46.5) |386 (38.5)   |398 (45.0) |437 (59.0)             |\n#&gt; |age (%)       |15 to 24 years    |740 (28.2)  |191 (19.1)   |264 (29.8) |285 (38.5)             |\n#&gt; |              |25 to 34 years    |475 (18.1)  |141 (14.1)   |167 (18.9) |167 (22.5)             |\n#&gt; |              |35 to 44 years    |393 (15.0)  |185 (18.5)   |119 (13.4) |89 (12.0)              |\n#&gt; |              |45 to 54 years    |438 (16.7)  |220 (22.0)   |139 (15.7) |79 (10.7)              |\n#&gt; |              |55 to 64 years    |379 (14.4)  |198 (19.8)   |113 (12.8) |68 ( 9.2)              |\n#&gt; |              |65 years or older |203 ( 7.7)  |67 ( 6.7)    |83 ( 9.4)  |53 ( 7.2)              |\n#&gt; |race (%)      |Non-white         |458 (17.4)  |184 (18.4)   |140 (15.8) |134 (18.1)             |\n#&gt; |              |White             |2170 (82.6) |818 (81.6)   |745 (84.2) |607 (81.9)             |\n\n\n\n\nThat matches the table presented in the paper exactly! Gemini was able to create the table using the tableone package and printed it cleanly with the kableone function.\n\n\nExercise 3a: Regression\nFor this next exercise, we need to subset the dataset such that it excludes individuals who responded either “Very good”, or “Excellent” on the self-rated mental health variables. Then following this we need to recode this variable and make it binary: ‘Good’ vs. ‘Poor’. We then want to convert this variable to a factor with ‘Poor’ as the reference level.\nLet’s send Gemini the following prompt:\nPrompt Sent to Gemini:\n\n\n\n\n\n\nUser Prompt\n\n\n\nSubset the dataset in RStudio, excluding individuals who responded “Very good” or “Excellent” on the self-rated mental health variable.\n\n\nGemini’s response:\n\n\n\n\n\n\nGemini Response\n\n\n\n\n# Subset the dataset to exclude individuals with \"Very good or excellent\" self-rated mental health\nsubset_data &lt;- analytic_data %&gt;%\n  filter(srmh != \"Very good or excellent\")\n\n# You can check the table of srmh in the new dataset\ntable(subset_data$srmh)\n#&gt; \n#&gt;           Poor or Fair                   Good Very good or excellent \n#&gt;                   1002                    885                      0\n\n\n\n\nThat worked as intended. Now let’s send the prompt to recode the variable as a binary factor.\nPrompt Sent to Gemini:\n\n\n\n\n\n\nUser Prompt\n\n\n\nRecode self-rated mental health variable and make it a binary variable: ‘Good’ vs. ‘Poor’ (simplifying category labels only). Convert that variable to a factor variable with ‘Poor’ being the reference level.\n\n\nGemini’s response:\n\n\n\n\n\n\nGemini Response\n\n\n\nHere’s the R code to recode the srmh variable in your subset_data into a binary factor variable with ‘Good’ and ‘Poor’ categories, setting ‘Poor’ as the reference level.\n\n# Recode the 'srmh' variable into a binary 'Good' vs. 'Poor' factor\nsubset_data &lt;- subset_data %&gt;%\n  mutate(\n    srmh_binary = car::recode(srmh,\n                              \"'Good' = 'Good';\n                               'Poor or Fair' = 'Poor'\"\n                              # Any other values (like NA from previous steps) would remain NA\n    ) %&gt;%\n      factor(levels = c(\"Poor\", \"Good\")) # Set 'Poor' as the reference level\n  )\n\n# You can now check the levels and a table of the new binary variable\nlevels(subset_data$srmh_binary)\n#&gt; [1] \"Poor\" \"Good\"\ntable(subset_data$srmh_binary, useNA = \"always\")\n#&gt; \n#&gt; Poor Good &lt;NA&gt; \n#&gt; 1002  885    0\n\n\n\n\nThat worked great. The self-rated mental health variable was correctly recoded into a binary factor with ‘Poor’ as the reference level. The code Gemini provided also included checks to confirm the recoding was done correctly\n\n\nExercise 3b: Reporting odds ratio\nFor this next exercise, we need to construct a logistic regression model to examine the relationship between community belonging and self-rated mental health among respondents with mental or substance use disorders. The model should be adjusted for three confounders: sex, age, and race/ethnicity.\nOnce the model is constructed, report the odds ratios and their associated confidence intervals. To do so we will use the publish package.\nPrompt Sent to Gemini:\n\n\n\n\n\n\nUser Prompt\n\n\n\nConstruct a logistic regression model to examine the relationship between community belonging (explanatory variable) and self-rated mental health (outcome variable) among respondents with mental or substance use disorders.\nAdjust the model for three confounders:\n\nSex\nAge\nRace/ethnicity\n\nAfter fitting the model:\n\nReport the odds ratios and their 95% confidence intervals\nUse the publish package to present the results in a clear and interpretable format\nProvide the R code to complete this task\n\n\n\nGemini’s response:\n\n\n\n\n\n\nGemini Response\n\n\n\n\nmodel &lt;- glm(srmh_binary ~ community + sex + age + race,\n             data = subset_data,\n             family = binomial(link = \"logit\"))\n\nresults_table &lt;- publish(model, level = 0.95)\n#&gt;   Variable             Units OddsRatio       CI.95    p-value \n#&gt;  community         Very weak       Ref                        \n#&gt;                Somewhat weak      1.93 [1.48;2.53]    &lt; 1e-04 \n#&gt;              Somewhat strong      2.90 [2.22;3.80]    &lt; 1e-04 \n#&gt;                  Very strong      3.32 [2.27;4.85]    &lt; 1e-04 \n#&gt;        sex           Females       Ref                        \n#&gt;                        Males      1.32 [1.09;1.60]   0.003993 \n#&gt;        age    15 to 24 years       Ref                        \n#&gt;               25 to 34 years      0.85 [0.63;1.15]   0.292243 \n#&gt;               35 to 44 years      0.45 [0.33;0.61]    &lt; 1e-04 \n#&gt;               45 to 54 years      0.45 [0.34;0.61]    &lt; 1e-04 \n#&gt;               55 to 64 years      0.41 [0.30;0.56]    &lt; 1e-04 \n#&gt;            65 years or older      0.87 [0.59;1.27]   0.468623 \n#&gt;       race         Non-white       Ref                        \n#&gt;                        White      1.32 [1.03;1.71]   0.030025\n\nprint(results_table)\n#&gt;     Variable             Units OddsRatio       CI.95    p-value\n#&gt; 1  community         Very weak       Ref                       \n#&gt; 2                Somewhat weak      1.93 [1.48;2.53]    &lt; 1e-04\n#&gt; 3              Somewhat strong      2.90 [2.22;3.80]    &lt; 1e-04\n#&gt; 4                  Very strong      3.32 [2.27;4.85]    &lt; 1e-04\n#&gt; 5        sex           Females       Ref                       \n#&gt; 6                        Males      1.32 [1.09;1.60]   0.003993\n#&gt; 7        age    15 to 24 years       Ref                       \n#&gt; 8               25 to 34 years      0.85 [0.63;1.15]   0.292243\n#&gt; 9               35 to 44 years      0.45 [0.33;0.61]    &lt; 1e-04\n#&gt; 10              45 to 54 years      0.45 [0.34;0.61]    &lt; 1e-04\n#&gt; 11              55 to 64 years      0.41 [0.30;0.56]    &lt; 1e-04\n#&gt; 12           65 years or older      0.87 [0.59;1.27]   0.468623\n#&gt; 13      race         Non-white       Ref                       \n#&gt; 14                       White      1.32 [1.03;1.71]   0.030025\n\n\n\nThat worked well. Gemini successfully built the logistic regression model, adjusted for the correct confounders, and presented the odds ratios with confidence intervals using the publish package. The code ran smoothly and the output was clear and interpretable.\n\n\nSummary\nUsing Gemini, we were able to complete the second set of tasks. In all instances, Gemini was able to complete the tasks without error, including identifying the correct variables to use when creating the table one, when subsetting the data, and constructing the logistic regression model. It also was successful and printing the table and regression results in in a markdown friendly manner\nIn this set of exercises, Gemini again performed well and returned accurate results. Still, it’s important to approach each step with care. Even when outputs look correct, reviewing them closely ensures the work aligns with the intended analysis and avoids potential oversights.\n\n\nStep-by-Step Plan for Using Generative AI in Data Analysis\n\nBe clear and specific\nDefine the task directly and avoid vague instructions.\nProvide starter code or examples\nHelp the model understand your expectations and structure.\nInclude variable structure\nShare the output of str() or a data dictionary so the model knows variable types.\nState factor levels explicitly\nList how variables should be recoded or grouped rather than relying on inference.\nReference sources directly\nIf you’re asking the model to replicate something (e.g., a published Table 1), summarize it clearly instead of assuming it can interpret the reference alone.\nReview the output\nCheck that the generated code and results match your goals and are statistically valid.\nBe prepared to troubleshoot\nIf the model’s response is incomplete or incorrect, use your understanding and available documentation to fix it.\nUse AI as a guide, not a replacement\nGenerative AI can streamline your work, but knowledge and software familiarity are still essential.\n(NEW) Double Check any work produced by AI\nWhile some AI models may perform well and produce exactly what you ask for, it’s still important to carry out thorough checks to ensure the output works as intended.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A) Vibe Part 2"
    ]
  },
  {
    "objectID": "accessingE1vibep2.html#references-1",
    "href": "accessingE1vibep2.html#references-1",
    "title": "Exercise 1 (A) Vibe Part 2",
    "section": "References",
    "text": "References\n\n\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nGerds, Thomas A., and Brice Ozenne. 2023. Publish: Format Output of Various Routines in a Suitable Way for Reports and Publication. https://CRAN.R-project.org/package=Publish.\n\n\nPosit team. 2023. RStudio: Integrated Development Environment for r. Boston, MA: Posit Software, PBC. http://www.posit.co/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create ’Table 1’ to Describe Baseline Characteristics with or Without Propensity Score Weights. https://CRAN.R-project.org/package=tableone.",
    "crumbs": [
      "Accessing data",
      "Exercise 1 (A) Vibe Part 2"
    ]
  },
  {
    "objectID": "accessingE2.html",
    "href": "accessingE2.html",
    "title": "Exercise 2 (A)",
    "section": "",
    "text": "Problem Statement\nWe will use the following article:\nThe authors exposed the relationship between benzodiazepine use with or without opioid use (exposure) and all-cause mortality (outcome), adjusting for confounders. The exposure and confounders information was extracted from the National Health and Nutrition Examination Surveys (NHANES) 1999-2014 cycles, and public-use mortality data was downloaded from the CDC website. Our main tasks include merging the datasets, applying eligibility criteria, and creating Table 1. Please note that recreating the analytic dataset after applying eligibility criteria was difficult due to the level of detail provided by the authors in the main manuscript and supplemental materials. Therefore, the numbers in your Table 1 will differ from Table 1 of the paper. However, you should be able to reproduce the numbers for this exercise given by the instructor.",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#problem-statement",
    "href": "accessingE2.html#problem-statement",
    "title": "Exercise 2 (A)",
    "section": "",
    "text": "Xu KY, Hartz SM, Borodovsky JT, Bierut LJ, Grucza RA. Association between benzodiazepine use with or without opioid use and all-cause mortality in the United States, 1999-2015. JAMA Network Open. 2020;3(12):e2028557. DOI: 10.1001/jamanetworkopen.2020.28557.",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#a-importing-dataset",
    "href": "accessingE2.html#a-importing-dataset",
    "title": "Exercise 2 (A)",
    "section": "1(a) Importing dataset",
    "text": "1(a) Importing dataset\nSee previous tutorial for hints about how the analytic data was created.\n\n# Importing dataset\nload(\"Data/accessing/nhanes_mortality_1999_2014.RData\")\nls()\n#&gt;  [1] \"dat.mortality01\" \"dat.mortality03\" \"dat.mortality05\" \"dat.mortality07\"\n#&gt;  [5] \"dat.mortality09\" \"dat.mortality11\" \"dat.mortality13\" \"dat.mortality99\"\n#&gt;  [9] \"dat.nhanes01\"    \"dat.nhanes03\"    \"dat.nhanes05\"    \"dat.nhanes07\"   \n#&gt; [13] \"dat.nhanes09\"    \"dat.nhanes11\"    \"dat.nhanes13\"    \"dat.nhanes99\"\n\nThere are 16 datasets (8 NHANES, 8 mortality):\n\ndat.nhanes99: NHANES data for 1999-2000 cycle\ndat.nhanes01: NHANES data for 2001-2002 cycle\ndat.nhanes03: NHANES data for 2003-2004 cycle\ndat.nhanes05: NHANES data for 2005-2006 cycle\ndat.nhanes07: NHANES data for 2007-2008 cycle\ndat.nhanes09: NHANES data for 2009-2010 cycle\ndat.nhanes11: NHANES data for 2011-2012 cycle\ndat.nhanes13: NHANES data for 2013-2014 cycle\ndat.mortality99: Mortality data for 1999-2000 cycle\ndat.mortality01: Mortality data for 2001-2003 cycle\ndat.mortality03: Mortality data for 2003-2004 cycle\ndat.mortality05: Mortality data for 2005-2006 cycle\ndat.mortality07: Mortality data for 2007-2008 cycle\ndat.mortality09: Mortality data for 2009-2010 cycle\ndat.mortality11: Mortality data for 2010-2012 cycle\ndat.mortality13: Mortality data for 2011-2014 cycle\n\nThe mortality datasets have the following variables:\n\n\nSEQN: Respondent sequence number. We will use the ‘SEQN’ variable to merge an NHANES cycle with its corresponding mortality data.\n\nmortstat: All-cause mortality status\n\npermth_int: Person-Months of Follow-up from NHANES Interview date\n\npermth_exm: Person-Months of Follow-up from NHANES Mobile Examination Center (MEC) Date\n\nCYCLE: Survey cycle\n\nThe NHANES datasets have the following variables:\n\n\nSEQN: Respondent sequence number\n\nCYCLE: Survey cycle\n\nSDMVSTRA: Masked variance pseudo-stratum\n\nSDMVPSU: Masked variance pseudo-PSU\n\nWTMEC2YR: Full sample 2-year MEC exam weight\n\nRIDSTATR: Interview/examination status\n\nRIDAGEYR: Age in years at screening\n\nRIAGENDR: Gender\n\nDMDEDUC2: Education level (adults 20+)\n\nINDFMPIR: Family monthly poverty income ratio\n\nRIDRETH1: Race/ethnicity\n\nDMDMARTL: Marital status\n\nSMOKING_HARMONIZED: Harmonized smoking variable [SMQ020/SMQ040]\n\nBPQ020: Ever told had high blood pressure\n\nLIPIDEM_HARMONIZED: Lipidemia harmonized [BPQ060/BPQ080]\n\nMCQ160F: Ever told you had a stroke\n\nMCQ160E: Ever told you had a heart attack\n\nDIQ010: Doctor told you have diabetes\n\nMCQ160B: Ever told you had congestive heart failure\n\nPULMOND_HARMONIZED: Harmonized pulmonary condition variable [MCQ160G/MCQ160K/MCQ160O]\n\nMCQ160L: Ever told you had liver condition\n\nMCQ160A: Ever told you had arthritis\n\nKIDNEYD_HARMONIZED: Harmonized kidney disease variable [KIQ020/KIQ022]\n\nMCQ220: Ever told you had cancer or malignancy\n\nANYHOSP_INLASTYEAR: Harmonized indicator of any hospital visit in last year [HUD070/HUQ070/HUQ071]\n\nHUQ090: Seen a mental health professional in past year\n\nHUQ010: General health condition [Excellent/Very Good/Good/Fair/Poor]\n\nPFQ090: Last 12 months, received care at emergency room\n\nREGULAR_EDCARE: Harmonized type or place most often go for healthcare [HUQ040/HUQ041]\n\nHUQ020: Health condition compared to a year ago\n\nNUMHOSP_INLASTYEAR: Harmonized number of hospitalizations in last year [HUD080/HUQ080]\n\nTAKERX_INLASTMONTH: Harmonized taken prescription/medicine in past month [RDX030/RDXUSE]\n\nbenzodiazepines: Any benzodiazepines reported, harmonized from RDX/RDX_DRUG data tables\n\nopioids: Any opioids reported, harmonized from RDX/RDX_DRUG data tables\n\nssris: Any ssris reported, harmonized from RDX/RDX_DRUG data tables\n\nantidepressants: Any anti-depressants (SNRIs, MAOIs, or TCAs) reported, harmonized from RDX/RDX_DRUG data tables\n\nantidepres_othr: Any other anti-depressants reported, harmonized from RDX/RDX_DRUG data tables\n\nantipsychotics: Any anti-psychotics reported, harmonized from RDX/RDX_DRUG data tables\n\nanalgesics: Any analgesics reported, harmonized from RDX/RDX_DRUG data tables\n\nmuscle_relax: Any muscle relaxants reported, harmonized from RDX/RDX_DRUG data tables\n\nanticonvulsant: Any anti-convulsants reported, harmonized from RDX/RDX_DRUG data tables\n\nhormonalagents: Any hormonal agents reported, harmonized from RDX/RDX_DRUG data tables\n\ngastrointestin: Any gastrointestinal agents reported, harmonized from RDX/RDX_DRUG data tables\n\ncardi_metab_ag: Any cardiac or metabolic medications reported, harmonized from RDX/RDX_DRUG data tables\n\nresp_antihist: Any respiratory medicators or antihistamines reported, harmonized from RDX/RDX_DRUG data tables\n\ncns_morethan2: &gt; 2 CNS medications reported, harmonized from RDX/RDX_DRUG data tables\n\nncns_lessthan5: &lt; 5 NCNS medications reported, harmonized from RDX/RDX_DRUG data tables\n\nREGULAR_DRINKING: Harmonized regular drinker status [ALQ100/ALD100/ALQ110/ALQ120U/ALQ120Q]\n\nWe refer this OER website for guidance on downloading mortality data and linking to NHANES. You can also review the R script (xu-et-al-2020-data-preparation.r) to see how the variables listed above were downloaded and derived.",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#b-merging-datasets",
    "href": "accessingE2.html#b-merging-datasets",
    "title": "Exercise 2 (A)",
    "section": "1(b) Merging datasets",
    "text": "1(b) Merging datasets\nMerge the mortality datasets with the NHANES datasets.\nHint:\n\n\nSEQN is the unique identifier in the datasets.\nMake sure that your merged dataset includes information for 82,091 individuals.\n\n\n# your code here\n\n# Mortality data\n# dat.mort &lt;- rbind(..\n\n# NHANES\n# dat.nhanes &lt;- \n\n# Merging\n# dat &lt;- merge(..",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#c-subsetting-according-to-eligibility",
    "href": "accessingE2.html#c-subsetting-according-to-eligibility",
    "title": "Exercise 2 (A)",
    "section": "1(c) Subsetting according to eligibility",
    "text": "1(c) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper (see Figure 1 of the paper). The authors dropped individuals:\n\nAged less than 20 years\nDid not participate in mobile examination interviews\nMissing prescription medication data or refused to answer\nMissing data on other covariates (medical comorbidities, alcohol, smoking, blood pressure, and health care use)\nDied within 1 year of follow-up\nWere not taking SSRIs, benzodiazepines, or opioids\n\nHint 1: The following variables could be used to subset based on eligibility:\n\n\nRIDAGEYR for age\n\nRIDSTATR for mobile examination interviews\n\nTAKERX_INLASTMONTH for prescription medication\nThe following for the missing covariate information: BPQ020, LIPIDEM_HARMONIZED, MCQ160F, MCQ160E, DIQ010, MCQ160B, PULMOND_HARMONIZED, MCQ160L, MCQ160A, KIDNEYD_HARMONIZED, MCQ220, SMOKING_HARMONIZED, REGULAR_DRINKING, REGULAR_EDCARE\n\n\nmortstat and permth_int for death within 1 year of follow-up\n\nbenzodiazepines, ssris, opioids for Benzodiazepine use with or without opioid use\n\nHint 2: After subsetting, the analytic dataset contains information for 4,049 individuals (do not match with the paper)\n\n# your code here\n\n# 20+\n# dat1 &lt;- subset(dat, ..\n\n# Did not participate in mobile examination interviews\n# dat2 &lt;- subset(dat1, ...\n\n# Missing prescription medication data or refused to answer\n# dat3 &lt;- subset(dat2, ...\n\n\n# Missing data on other covariates \n# dat4 &lt;- ...\n\n# Died within 1 y of follow-up\n# dat5 &lt;- subset(dat4, ...\n\n# Were not taking SSRIs, benzodiazepines, or opioids\n# dat6 &lt;- subset(dat5,",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#d-creating-the-exposure-variable",
    "href": "accessingE2.html#d-creating-the-exposure-variable",
    "title": "Exercise 2 (A)",
    "section": "1(d) Creating the exposure variable",
    "text": "1(d) Creating the exposure variable\nCreate the exposure variable (Benzodiazepine Use With or Without Opioid Use).\nHint: It’s a categorical variable with four categories as shown in Figure 1 or Table 1 of the paper. If your analytic sample size is 4,049, you could have the following frequencies:\n\n\nexposure\nn\n\n\n\nBZDs plus opioids\n408\n\n\nBZDs only\n961\n\n\nOpioids only\n1,434\n\n\nNeither\n1,246\n\n\n\n\n# your code here\n\n# library(dplyr)\n# dat6 &lt;- dat6 %&gt;% \n#   mutate(exposure = case_when(...",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#a-recoding",
    "href": "accessingE2.html#a-recoding",
    "title": "Exercise 2 (A)",
    "section": "2(a) Recoding",
    "text": "2(a) Recoding\nRecode the confounders/covariates in shown in Table 1, except for the following:\n\nAntimicrobial drugs\n\n\\(&lt;5\\) Non-CNS medications\nDisabled\n\nThe main reason these four variables were not considered is that the authors provided insufficient detail in the main manuscript or supplemental materials to derive these variables. An example of recoding some of the variables is given below. But you can use other functions as well.\n\nlibrary(car)\n\n# Age\n#summary(dat6$RIDAGEYR)\ndat6$age &lt;- dat6$RIDAGEYR\ndat6$age.cat &lt;- car::recode(dat6$RIDAGEYR, \" 60:70 = '60-70y'; else = 'Others' \", \n                            as.factor = T)\ndat6$age.cat &lt;- relevel(dat6$age.cat, ref = \"Others\")\ntable(dat6$age.cat, useNA = \"always\")\n  \n# Male\ntable(dat6$RIAGENDR, useNA = \"always\")\ndat6$gender &lt;- relevel(dat6$RIAGENDR, ref = \"Female\")\n\n# College graduate \n#table(dat6$DMDEDUC2, useNA = \"always\")\ndat6$education &lt;- car::recode(dat6$DMDEDUC2, \" c('College Graduate or above', \n                           'College graduate or above') = 'College graduate'; \n                           c('Less Than 9th Grade', \n                           '9-11th Grade (Includes 12th grade with no diploma)',\n                           'High School Grad/GED or Equivalent', 'Less than 9th grade', \n                           '9-11th grade (Includes 12th grade with no diploma)', \n                           'High school graduate/GED or equivalent',\n                           'Some College or AA degree', 'Some college or AA degree') = \n                           'Less than college'; else = NA \", as.factor = T)\ndat6$education &lt;- relevel(dat6$education, ref = \"Less than college\")\ntable(dat6$education, useNA = \"always\")\n\n# Poverty to income ratio \n# table(dat6$INDFMPIR, useNA = \"always\")\n## Your code here\n# dat6$poverty.ratio &lt;- car::recode(dat6$INDFMPIR ...\n\n# White\n# table(dat6$RIDRETH1, useNA = \"always\")\n## Your code here\n# dat6$race &lt;- car::recode(dat6$RIDRETH1,...\n\n# Partnered\n#table(dat6$DMDMARTL, useNA = \"always\")\n## Your code here\n# dat6$marital &lt;- car::recode(dat6$DMDMARTL, ...\ntable(dat6$marital, useNA = \"always\")\n\n# Smoking\n#table(dat6$SMOKING_HARMONIZED, useNA = \"always\")\ndat6$smoking &lt;- car::recode(dat6$SMOKING_HARMONIZED, \" c('Every day', 'Some days') = \n                            'Yes'; c('LOGICAL SKIP', 'Not at all') = 'No';\n                            else = NA\", as.factor = T)\ntable(dat6$smoking, useNA = \"always\")\n\n# Hypertension\n# table(dat6$BPQ020, useNA = \"always\")\n## Your code here\n# dat6$hypertension &lt;- car::recode(dat6$BPQ020, ...\ntable(dat6$hypertension, useNA = \"always\")\n\n# Hyperlipidemia\n#table(dat6$LIPIDEM_HARMONIZED, useNA = \"always\")\ndat6$hyperlipidemia &lt;- car::recode(dat6$LIPIDEM_HARMONIZED, \" 'No' = 'No';\n                                   'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$hyperlipidemia, useNA = \"always\")\n\n# Stroke\n#table(dat6$MCQ160F, useNA = \"always\")\ndat6$stroke &lt;- car::recode(dat6$MCQ160F, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\", \n                           as.factor = T)\ntable(dat6$stroke, useNA = \"always\")\n\n# Myocardial infarction\n#table(dat6$MCQ160E, useNA = \"always\")\ndat6$heart.attack &lt;- car::recode(dat6$MCQ160E, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\", \n                                 as.factor = T)\ntable(dat6$heart.attack, useNA = \"always\")\n\n# Diabetes\n#table(dat6$DIQ010, useNA = \"always\")\ndat6$diabetes &lt;- car::recode(dat6$DIQ010, \" c('No', 'Borderline') = 'No'; \n                             'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$diabetes, useNA = \"always\")\n\n# Congestive heart failure\n#table(dat6$MCQ160B, useNA = \"always\")\ndat6$heart.failure &lt;- car::recode(dat6$MCQ160B, \" 'No' = 'No'; 'Yes' = 'Yes';\n                                  else = NA\", as.factor = T)\ntable(dat6$heart.failure, useNA = \"always\")\n\n# Pulmonary disease \n#table(dat6$PULMOND_HARMONIZED, useNA = \"always\")\ndat6$pulmonary.disease &lt;- car::recode(dat6$PULMOND_HARMONIZED, \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$pulmonary.disease, useNA = \"always\")\n\n# Liver disease \n#table(dat6$MCQ160L, useNA = \"always\")\ndat6$liver.disease &lt;- car::recode(dat6$MCQ160L, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                  else = NA\", as.factor = T)\ntable(dat6$liver.disease, useNA = \"always\")\n\n# Arthritis     \n#table(dat6$MCQ160A, useNA = \"always\")\ndat6$arthritis &lt;- car::recode(dat6$MCQ160A, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                              else = NA\", as.factor = T)\ntable(dat6$arthritis, useNA = \"always\")\n\n# Kidney    disease \n#table(dat6$KIDNEYD_HARMONIZED, useNA = \"always\")\ndat6$kidney.disease &lt;- car::recode(dat6$KIDNEYD_HARMONIZED, \" 'No' = 'No'; \n                                   'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$kidney.disease, useNA = \"always\")\n\n# Cancer        \n#table(dat6$MCQ220, useNA = \"always\")\ndat6$cancer &lt;- car::recode(dat6$MCQ220, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\",\n                           as.factor = T)\ntable(dat6$cancer, useNA = \"always\")\n\n# Regular   drinking    \n#table(dat6$REGULAR_DRINKING, useNA = \"always\")\ndat6$regular.drinking &lt;- car::recode(dat6$REGULAR_DRINKING, \" 'No' = 'No'; \n                                     'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$regular.drinking, useNA = \"always\")\n\n# Antimicrobial drugs   - not available\n# table(dat6$antimicrobial.drugs, useNA = \"always\")\n\n# Hormonal  agents  \ntable(dat6$hormonalagents, useNA = \"always\")\n\n# Anticonvulsants       \ntable(dat6$anticonvulsant, useNA = \"always\")\n\n# Any   analgesics  \ntable(dat6$analgesics, useNA = \"always\")\n\n# Muscle    relaxants   \ntable(dat6$muscle_relax, useNA = \"always\")\n\n# Gastrointestinal  agents  \ntable(dat6$gastrointestin, useNA = \"always\")\n\n# Cardiac   or  metabolic medications       \ntable(dat6$cardi_metab_ag, useNA = \"always\")\n\n# Respiratory   medications or antihistamines       \ntable(dat6$resp_antihist, useNA = \"always\")\n\n# &gt;2 CNS    medications\ntable(dat6$cns_morethan2, useNA = \"always\")\n\n# &lt;5 Non-CNS    medications \ntable(dat6$ncns_lessthan5, useNA = \"always\")\n\n# Antidepressants   (SNRIs, MAOIs,  or  TCAs)\ntable(dat6$antidepressants, useNA = \"always\")\n\n# Other antidepressants \ntable(dat6$antidepres_othr, useNA = \"always\")\n\n# Antipsychotics        \ntable(dat6$antipsychotics, useNA = \"always\")\n\n# Any   hospitalization in &lt;1y\n#table(dat6$ANYHOSP_INLASTYEAR, useNA = \"always\")\ndat6$hospitalization.any &lt;- car::recode(dat6$ANYHOSP_INLASTYEAR, \" 'No' = 'No'; \n                                     'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$hospitalization.any, useNA = \"always\")\n\n# Any   psychiatric visit in &lt;1y\n#table(dat6$HUQ090, useNA = \"always\")\ndat6$hospitalization.psy &lt;- car::recode(dat6$HUQ090, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                        else = NA\", as.factor = T)\ntable(dat6$hospitalization.psy, useNA = \"always\")\n\n# Good current  health\n#table(dat6$HUQ010, useNA = \"always\")\ndat6$good.current.health &lt;- car::recode(dat6$HUQ010, \" c('Excellent,', 'Very good,',\n                                        'Good,') = 'Yes'; c('Fair, or', 'Poor?') = 'No';\n                                        else = NA\", as.factor = T)\ntable(dat6$good.current.health, useNA = \"always\")\n\n# Require   special health equipment        \n#table(dat6$PFQ090, useNA = \"always\")\ndat6$special.health &lt;- car::recode(dat6$PFQ090, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                   else = NA\", as.factor = T)\ntable(dat6$special.health, useNA = \"always\")\n\n# Disabled - not available      \n# table(dat6$disabled, useNA = \"always\")\n\n# Regular   ED  care\n#table(dat6$REGULAR_EDCARE, useNA = \"always\")\ndat6$regular.ED.care &lt;- car::recode(dat6$REGULAR_EDCARE, \" 'Hospital emergency room' = \n                                    'Yes'; else = 'No'\",  as.factor = T)\ntable(dat6$regular.ED.care, useNA = \"always\")\n\n# Worsening health  \n#table(dat6$HUQ020, useNA = \"always\")\ndat6$worsening.health &lt;- car::recode(dat6$HUQ020, \" c('Better,', 'About the same?') = 'No'; \n                                     'Worse, or' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$worsening.health, useNA = \"always\")\n\n# &gt;2 Overnight  hospitalizations in &lt;1y\n#table(dat6$NUMHOSP_INLASTYEAR, useNA = \"always\")\ndat6$hospitalization.3plus &lt;- car::recode(dat6$NUMHOSP_INLASTYEAR, \" c('0', '1', '2') = 'No'; \n                                        c('3', '4', '5', '6 or more') = 'Yes';\n                                        else = NA\", as.factor = T)\ntable(dat6$hospitalization.3plus, useNA = \"always\")",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#b-table-1",
    "href": "accessingE2.html#b-table-1",
    "title": "Exercise 2 (A)",
    "section": "2(b) Table 1",
    "text": "2(b) Table 1\nReproduce Table 1 presented in the above paper. Please note that the numbers in your Table 1 will differ from Table 1 of the paper. But the numbers could look as follows:\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall\nBZDs plus opioids\nBZDs only\nOpioids only\nNeither\n\n\n\nn\n4049\n408\n961\n1434\n1246\n\n\nage (mean (SD))\n53.47 (17.06)\n55.13 (14.86)\n56.34 (17.06)\n51.54 (17.23)\n52.94 (17.21)\n\n\nage.cat = 60-70y (%)\n835 (20.6)\n84 (20.6)\n190 (19.8)\n306 (21.3)\n255 (20.5)\n\n\ngender = Male (%)\n1423 (35.1)\n157 (38.5)\n311 (32.4)\n591 (41.2)\n364 (29.2)\n\n\neducation = College graduate (%)\n749 (18.5)\n46 (11.3)\n193 (20.1)\n208 (14.5)\n302 (24.2)\n\n\npoverty.ratio = More than 2 (%)\n1980 (51.9)\n154 (39.8)\n491 (54.3)\n626 (46.2)\n709 (60.7)\n\n\nrace = White (%)\n2540 (62.7)\n280 (68.6)\n656 (68.3)\n735 (51.3)\n869 (69.7)\n\n\nmarital = Partnered (%)\n2240 (55.9)\n223 (54.9)\n517 (54.4)\n780 (55.1)\n720 (58.3)\n\n\nsmoking = Yes (%)\n1066 (26.3)\n157 (38.5)\n227 (23.6)\n431 (30.1)\n251 (20.1)\n\n\nhypertension = Yes (%)\n1954 (48.3)\n225 (55.1)\n457 (47.6)\n693 (48.3)\n579 (46.5)\n\n\nhyperlipidemia = Yes (%)\n1723 (42.6)\n191 (46.8)\n442 (46.0)\n540 (37.7)\n550 (44.1)\n\n\nstroke = Yes (%)\n263 ( 6.5)\n39 ( 9.6)\n56 ( 5.8)\n82 ( 5.7)\n86 ( 6.9)\n\n\nheart.attack = Yes (%)\n279 ( 6.9)\n38 ( 9.3)\n64 ( 6.7)\n95 ( 6.6)\n82 ( 6.6)\n\n\ndiabetes = Yes (%)\n645 (15.9)\n63 (15.4)\n133 (13.8)\n252 (17.6)\n197 (15.8)\n\n\nheart.failure = Yes (%)\n232 ( 5.7)\n36 ( 8.8)\n53 ( 5.5)\n77 ( 5.4)\n66 ( 5.3)\n\n\npulmonary.disease = Yes (%)\n578 (14.3)\n95 (23.3)\n117 (12.2)\n214 (14.9)\n152 (12.2)\n\n\nliver.disease = Yes (%)\n242 ( 6.0)\n36 ( 8.8)\n59 ( 6.1)\n88 ( 6.1)\n59 ( 4.7)\n\n\narthritis = Yes (%)\n1989 (49.1)\n290 (71.1)\n417 (43.4)\n791 (55.2)\n491 (39.4)\n\n\nkidney.disease = Yes (%)\n191 ( 4.7)\n27 ( 6.6)\n44 ( 4.6)\n72 ( 5.0)\n48 ( 3.9)\n\n\ncancer = Yes (%)\n571 (14.1)\n81 (19.9)\n154 (16.0)\n190 (13.2)\n146 (11.7)\n\n\nregular.drinking = Yes (%)\n1219 (30.1)\n109 (26.7)\n301 (31.3)\n395 (27.5)\n414 (33.2)\n\n\nhormonalagents = Yes (%)\n1004 (24.8)\n116 (28.4)\n265 (27.6)\n282 (19.7)\n341 (27.4)\n\n\nanticonvulsant = Yes (%)\n822 (20.3)\n187 (45.8)\n399 (41.5)\n153 (10.7)\n83 ( 6.7)\n\n\nanalgesics = Yes (%)\n2167 (53.5)\n403 (98.8)\n176 (18.3)\n1388 (96.8)\n200 (16.1)\n\n\nmuscle_relax = Yes (%)\n394 ( 9.7)\n96 (23.5)\n53 ( 5.5)\n206 (14.4)\n39 ( 3.1)\n\n\ngastrointestin = Yes (%)\n1055 (26.1)\n163 (40.0)\n257 (26.7)\n352 (24.5)\n283 (22.7)\n\n\ncardi_metab_ag = Yes (%)\n2205 (54.5)\n259 (63.5)\n559 (58.2)\n716 (49.9)\n671 (53.9)\n\n\nresp_antihist = Yes (%)\n952 (23.5)\n151 (37.0)\n150 (15.6)\n444 (31.0)\n207 (16.6)\n\n\ncns_morethan2 = Yes (%)\n635 (15.7)\n238 (58.3)\n127 (13.2)\n243 (16.9)\n27 ( 2.2)\n\n\nantidepressants = Yes (%)\n208 ( 5.1)\n52 (12.7)\n88 ( 9.2)\n51 ( 3.6)\n17 ( 1.4)\n\n\nantidepres_othr = Yes (%)\n472 (11.7)\n71 (17.4)\n119 (12.4)\n135 ( 9.4)\n147 (11.8)\n\n\nantipsychotics = Yes (%)\n193 ( 4.8)\n26 ( 6.4)\n68 ( 7.1)\n25 ( 1.7)\n74 ( 5.9)\n\n\nhospitalization.any = Yes (%)\n905 (22.4)\n121 (29.7)\n206 (21.4)\n376 (26.2)\n202 (16.2)\n\n\nhospitalization.psy = Yes (%)\n856 (21.1)\n95 (23.3)\n271 (28.2)\n155 (10.8)\n335 (26.9)\n\n\ngood.current.health = Yes (%)\n2527 (62.4)\n173 (42.4)\n627 (65.2)\n824 (57.5)\n903 (72.5)\n\n\nspecial.health = Yes (%)\n793 (19.6)\n144 (35.3)\n139 (14.5)\n337 (23.5)\n173 (13.9)\n\n\nregular.ED.care = Yes (%)\n98 ( 2.4)\n13 ( 3.2)\n15 ( 1.6)\n57 ( 4.0)\n13 ( 1.0)\n\n\nworsening.health = Yes (%)\n865 (21.4)\n131 (32.1)\n197 (20.5)\n359 (25.1)\n178 (14.3)\n\n\nhospitalization.3plus = Yes(%)\n124 ( 3.1)\n22 ( 5.4)\n21 ( 2.2)\n48 ( 3.3)\n33 ( 2.6)\n\n\n\n\n# your code here\n\nlibrary(tableone)\nvars &lt;- c(\"age\", \"age.cat\", \"gender\", \"education\", \"poverty.ratio\", \"race\", \n          \"marital\", \"smoking\", \"hypertension\", \"hyperlipidemia\", \"stroke\", \n          \"heart.attack\", \"diabetes\", \"heart.failure\", \"pulmonary.disease\", \n          \"liver.disease\", \"arthritis\", \"kidney.disease\", \"cancer\", \n          \"regular.drinking\", #\"antimicrobial.drugs\", \n          \"hormonalagents\", \"anticonvulsant\", \"analgesics\", \"muscle_relax\", \n          \"gastrointestin\", \"cardi_metab_ag\", \"resp_antihist\", \"cns_morethan2\", \n          #\"ncns_lessthan5\", \n          \"antidepressants\", \"antidepres_othr\", \"antipsychotics\", \n          \"hospitalization.any\", \"hospitalization.psy\", \"good.current.health\",\n          \"special.health\", #\"disabled\",\n          \"regular.ED.care\", \"worsening.health\", \"hospitalization.3plus\"\n          )\n# tab1 &lt;- CreateTableOne(...",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#a-regression",
    "href": "accessingE2.html#a-regression",
    "title": "Exercise 2 (A)",
    "section": "3(a) Regression",
    "text": "3(a) Regression\nConsider all-cause mortality as a binary variable. Run logistic regression model for finding the association between Benzodiazepine Use With or Without Opioid Use (the exposure variable created in 1(d)) and all-cause mortality (the outcome variable). Adjust the model for three confounders: sex, age, and race/ethnicity. Also, use the Neither as the reference category for the exposure variable. Please note that we will not be using survey features (e.g., PSU, strata, weight) in this exercise. However, we will learn how to utilize these survey features in the Survey data analysis lab.\n\n# your code here\n\n# fit &lt;- glm(",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2.html#b-reporting-odds-ratio",
    "href": "accessingE2.html#b-reporting-odds-ratio",
    "title": "Exercise 2 (A)",
    "section": "3(b) Reporting odds ratio",
    "text": "3(b) Reporting odds ratio\nReport the odds ratios and associated confidence intervals.\n\n# your code here\n\n# library(Publish)",
    "crumbs": [
      "Accessing data",
      "Exercise 2 (A)"
    ]
  },
  {
    "objectID": "accessingE2solution.html",
    "href": "accessingE2solution.html",
    "title": "Exercise 2 Solution (A)",
    "section": "",
    "text": "Question I:\nWe will use the following article:\nThe authors exposed the relationship between benzodiazepine use with or without opioid use (exposure) and all-cause mortality (outcome), adjusting for confounders. The exposure and confounders information was extracted from the National Health and Nutrition Examination Surveys (NHANES) 1999-2014 cycles, and public-use mortality data was downloaded from the CDC website. Our main tasks include merging the datasets, applying eligibility criteria, and creating Table 1. Please note that recreating the analytic dataset after applying eligibility criteria was difficult due to the level of detail provided by the authors in the main manuscript and supplemental materials. Therefore, the numbers in your Table 1 will differ from Table 1 of the paper. However, you should be able to reproduce the numbers for this exercise given by the instructor.",
    "crumbs": [
      "Accessing data",
      "Exercise 2 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE2solution.html#question-i",
    "href": "accessingE2solution.html#question-i",
    "title": "Exercise 2 Solution (A)",
    "section": "",
    "text": "1(a) Importing dataset\n\n# Importing dataset\nload(\"Data/accessing/nhanes_mortality_1999_2014.RData\")\nls()\n#&gt;  [1] \"dat.mortality01\" \"dat.mortality03\" \"dat.mortality05\" \"dat.mortality07\"\n#&gt;  [5] \"dat.mortality09\" \"dat.mortality11\" \"dat.mortality13\" \"dat.mortality99\"\n#&gt;  [9] \"dat.nhanes01\"    \"dat.nhanes03\"    \"dat.nhanes05\"    \"dat.nhanes07\"   \n#&gt; [13] \"dat.nhanes09\"    \"dat.nhanes11\"    \"dat.nhanes13\"    \"dat.nhanes99\"\n\nThere are 16 datasets (8 NHANES, 8 mortality):\n\ndat.nhanes99: NHANES data for 1999-2000 cycle\ndat.nhanes01: NHANES data for 2001-2002 cycle\ndat.nhanes03: NHANES data for 2003-2004 cycle\ndat.nhanes05: NHANES data for 2005-2006 cycle\ndat.nhanes07: NHANES data for 2007-2008 cycle\ndat.nhanes09: NHANES data for 2009-2010 cycle\ndat.nhanes11: NHANES data for 2011-2012 cycle\ndat.nhanes13: NHANES data for 2013-2014 cycle\ndat.mortality99: Mortality data for 1999-2000 cycle\ndat.mortality01: Mortality data for 2001-2003 cycle\ndat.mortality03: Mortality data for 2003-2004 cycle\ndat.mortality05: Mortality data for 2005-2006 cycle\ndat.mortality07: Mortality data for 2007-2008 cycle\ndat.mortality09: Mortality data for 2009-2010 cycle\ndat.mortality11: Mortality data for 2010-2012 cycle\ndat.mortality13: Mortality data for 2011-2014 cycle\n\nThe mortality datasets have the following variables:\n\n\nSEQN: Respondent sequence number. We will use the ‘SEQN’ variable to merge an NHANES cycle with its corresponding mortality data.\n\nmortstat: All-cause mortality status\n\npermth_int: Person-Months of Follow-up from NHANES Interview date\n\npermth_exm: Person-Months of Follow-up from NHANES Mobile Examination Center (MEC) Date\n\nCYCLE: Survey cycle\n\nThe NHANES datasets have the following variables:\n\n\nSEQN: Respondent sequence number\n\nCYCLE: Survey cycle\n\nSDMVSTRA: Masked variance pseudo-stratum\n\nSDMVPSU: Masked variance pseudo-PSU\n\nWTMEC2YR: Full sample 2-year MEC exam weight\n\nRIDSTATR: Interview/examination status\n\nRIDAGEYR: Age in years at screening\n\nRIAGENDR: Gender\n\nDMDEDUC2: Education level (adults 20+)\n\nINDFMPIR: Family monthly poverty income ratio\n\nRIDRETH1: Race/ethnicity\n\nDMDMARTL: Marital status\n\nSMOKING_HARMONIZED: Harmonized smoking variable [SMQ020/SMQ040]\n\nBPQ020: Ever told had high blood pressure\n\nLIPIDEM_HARMONIZED: Lipidemia harmonized [BPQ060/BPQ080]\n\nMCQ160F: Ever told you had a stroke\n\nMCQ160E: Ever told you had a heart attack\n\nDIQ010: Doctor told you have diabetes\n\nMCQ160B: Ever told you had congestive heart failure\n\nPULMOND_HARMONIZED: Harmonized pulmonary condition variable [MCQ160G/MCQ160K/MCQ160O]\n\nMCQ160L: Ever told you had liver condition\n\nMCQ160A: Ever told you had arthritis\n\nKIDNEYD_HARMONIZED: Harmonized kidney disease variable [KIQ020/KIQ022]\n\nMCQ220: Ever told you had cancer or malignancy\n\nANYHOSP_INLASTYEAR: Harmonized indicator of any hospital visit in last year [HUD070/HUQ070/HUQ071]\n\nHUQ090: Seen a mental health professional in past year\n\nHUQ010: General health condition [Excellent/Very Good/Good/Fair/Poor]\n\nPFQ090: Last 12 months, received care at emergency room\n\nREGULAR_EDCARE: Harmonized type or place most often go for healthcare [HUQ040/HUQ041]\n\nHUQ020: Health condition compared to a year ago\n\nNUMHOSP_INLASTYEAR: Harmonized number of hospitalizations in last year [HUD080/HUQ080]\n\nTAKERX_INLASTMONTH: Harmonized taken prescription/medicine in past month [RDX030/RDXUSE]\n\nbenzodiazepines: Any benzodiazepines reported, harmonized from RDX/RDX_DRUG data tables\n\nopioids: Any opioids reported, harmonized from RDX/RDX_DRUG data tables\n\nssris: Any ssris reported, harmonized from RDX/RDX_DRUG data tables\n\nantidepressants: Any anti-depressants (SNRIs, MAOIs, or TCAs) reported, harmonized from RDX/RDX_DRUG data tables\n\nantidepres_othr: Any other anti-depressants reported, harmonized from RDX/RDX_DRUG data tables\n\nantipsychotics: Any anti-psychotics reported, harmonized from RDX/RDX_DRUG data tables\n\nanalgesics: Any analgesics reported, harmonized from RDX/RDX_DRUG data tables\n\nmuscle_relax: Any muscle relaxants reported, harmonized from RDX/RDX_DRUG data tables\n\nanticonvulsant: Any anti-convulsants reported, harmonized from RDX/RDX_DRUG data tables\n\nhormonalagents: Any hormonal agents reported, harmonized from RDX/RDX_DRUG data tables\n\ngastrointestin: Any gastrointestinal agents reported, harmonized from RDX/RDX_DRUG data tables\n\ncardi_metab_ag: Any cardiac or metabolic medications reported, harmonized from RDX/RDX_DRUG data tables\n\nresp_antihist: Any respiratory medicators or antihistamines reported, harmonized from RDX/RDX_DRUG data tables\n\ncns_morethan2: &gt; 2 CNS medications reported, harmonized from RDX/RDX_DRUG data tables\n\nncns_lessthan5: &lt; 5 NCNS medications reported, harmonized from RDX/RDX_DRUG data tables\n\nREGULAR_DRINKING: Harmonized regular drinker status [ALQ100/ALD100/ALQ110/ALQ120U/ALQ120Q]\n\nWe refer this OER website for guidance on downloading mortality data and linking to NHANES. You can also review the R script (xu-et-al-2020-data-preparation.r) to see how the variables listed above were downloaded and derived.\n1(b) Merging datasets\nMerge the mortality datasets with the NHANES datasets.\nHint:\n\n\nSEQN is the unique identifier in the datasets.\nMake sure that your merged dataset includes information for 82,091 individuals.\n\n\n# Mortality data\ndat.mort &lt;- rbind(dat.mortality99, dat.mortality01, dat.mortality03, dat.mortality05,\n                  dat.mortality07, dat.mortality09, dat.mortality11, dat.mortality13)\ndat.mort$SEQN &lt;- as.numeric(dat.mort$SEQN)\n\n# NHANES\ndat.nhanes &lt;- rbind(dat.nhanes99, dat.nhanes01, dat.nhanes03, dat.nhanes05,\n                  dat.nhanes07, dat.nhanes09, dat.nhanes11, dat.nhanes13)\ndat.nhanes$CYCLE &lt;- NULL\n\n# Merging\ndat &lt;- merge(dat.mort, dat.nhanes, by = \"SEQN\", all = T)\n#table(dat$CYCLE.x, dat$CYCLE.y, useNA = \"always\")\ndat$CYCLE.y &lt;- NULL\ncolnames(dat)[colnames(dat) == \"CYCLE.x\"] &lt;- \"CYCLE\"\n\n1(c) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria / restriction specified in the paper (see Figure 1 of the paper). The authors dropped individuals:\n\nAged less than 20 years\nDid not participate in mobile examination interviews\nMissing prescription medication data or refused to answer\nMissing data on other covariates (medical comorbidities, alcohol, smoking, blood pressure, and health care use)\nDied within 1 year of follow-up\nWere not taking SSRIs, benzodiazepines, or opioids\n\nHint 1: The following variables could be used to subset based on eligibility:\n\n\nRIDAGEYR for age\n\nRIDSTATR for mobile examination interviews\n\nTAKERX_INLASTMONTH for prescription medication\nThe following for the missing covariate information: BPQ020, LIPIDEM_HARMONIZED, MCQ160F, MCQ160E, DIQ010, MCQ160B, PULMOND_HARMONIZED, MCQ160L, MCQ160A, KIDNEYD_HARMONIZED, MCQ220, SMOKING_HARMONIZED, REGULAR_DRINKING, REGULAR_EDCARE\n\n\nmortstat and permth_int for death within 1 year of follow-up\n\nbenzodiazepines, ssris, opioids for Benzodiazepine use with or without opioid use\n\nHint 2: After subsetting, the analytic dataset contains information for 4,049 individuals (do not match with the paper)\n\n# 20+\ndat1 &lt;- subset(dat, RIDAGEYR&gt;= 20) # N = 43,793\n\n# Did not participate in mobile examination interviews\ndat2 &lt;- subset(dat1, RIDSTATR == \"int + mec exam\") # N = 41,659\nnrow(dat1) - nrow(dat2) # 2,134 dropped\n#&gt; [1] 2134\n\n# Missing prescription medication data or refused to answer\ndat3 &lt;- subset(dat2, TAKERX_INLASTMONTH %in% c(\"No\", \"Yes\")) # N = 41,614\nnrow(dat2) - nrow(dat3) # 45 dropped\n#&gt; [1] 45\n\n# Missing data on other covariates \ndat4 &lt;- dat3[complete.cases(dat3$BPQ020),]\ndat4 &lt;- dat4[complete.cases(dat4$LIPIDEM_HARMONIZED),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ160F),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ160E),]\ndat4 &lt;- dat4[complete.cases(dat4$DIQ010),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ160B),]\ndat4 &lt;- dat4[complete.cases(dat4$PULMOND_HARMONIZED),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ160L),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ160A),]\ndat4 &lt;- dat4[complete.cases(dat4$KIDNEYD_HARMONIZED),]\ndat4 &lt;- dat4[complete.cases(dat4$MCQ220),]\ndat4 &lt;- dat4[complete.cases(dat4$SMOKING_HARMONIZED),]\ndat4 &lt;- dat4[complete.cases(dat4$REGULAR_DRINKING),]\ndat4 &lt;- dat4[complete.cases(dat4$REGULAR_EDCARE),] # N = 31,850\n\nnrow(dat3) - nrow(dat4) # 9,764 dropped\n#&gt; [1] 9764\n\n# Died within 1 y of follow-up\ndat5 &lt;- subset(dat4, mortstat == 0 | (mortstat == 1 & permth_int &gt; 12)) # N = 31,591\nnrow(dat4) - nrow(dat5) # 2,59 dropped\n#&gt; [1] 259\n\n# Were not taking SSRIs, benzodiazepines, or opioids\ndat6 &lt;- subset(dat5, benzodiazepines == \"Yes\" | ssris == \"Yes\" | opioids == \"Yes\") # N = 4,049\nnrow(dat5) - nrow(dat6) # 27,542 dropped\n#&gt; [1] 27542\n\nnrow(dat6) # N = 4,049\n#&gt; [1] 4049\n\n1(d) Creating the exposure variable\nCreate the exposure variable (Benzodiazepine Use With or Without Opioid Use).\nHint: It’s a categorical variable with four categories as shown in Figure 1 or Table 1 of the paper. If your analytic sample size is 4,049, you could have the following frequencies:\n\n\nexposure\nn\n\n\n\nBZDs plus opioids\n408\n\n\nBZDs only\n961\n\n\nOpioids only\n1,434\n\n\nNeither\n1,246\n\n\n\n\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\ndat6 &lt;- dat6 %&gt;% \n  mutate(exposure = case_when(\n    benzodiazepines == \"Yes\" & opioids == \"Yes\" ~ 'BZDs plus opioids',\n    benzodiazepines == \"Yes\" & opioids == \"No\" ~ 'BZDs only',\n    benzodiazepines == \"No\" & opioids == \"Yes\" ~ 'Opioids only',\n    benzodiazepines == \"No\" & opioids == \"No\" & ssris == \"Yes\" ~ 'Neither')\n  )\ndat6$exposure &lt;- factor(dat6$exposure, levels = c(\"BZDs plus opioids\", \"BZDs only\", \n                                                  \"Opioids only\", \"Neither\"))\ntable(dat6$exposure, useNA = \"always\")\n#&gt; \n#&gt; BZDs plus opioids         BZDs only      Opioids only           Neither \n#&gt;               408               961              1434              1246 \n#&gt;              &lt;NA&gt; \n#&gt;                 0",
    "crumbs": [
      "Accessing data",
      "Exercise 2 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE2solution.html#question-ii",
    "href": "accessingE2solution.html#question-ii",
    "title": "Exercise 2 Solution (A)",
    "section": "Question II:",
    "text": "Question II:\n2(a) Recoding\nRecode the confounders/covariates in shown in Table 1, except for the following:\n\nAntimicrobial drugs\n\n\\(&lt;5\\) Non-CNS medications\nDisabled\n\nThe main reason these four variables were not considered is that the authors provided insufficient detail in the main manuscript or supplemental materials to derive these variables. An example of recoding some of the variables is given below. But you can use other functions as well.\n\nlibrary(car)\n\n# Age\n#summary(dat6$RIDAGEYR)\ndat6$age &lt;- dat6$RIDAGEYR\ndat6$age.cat &lt;- car::recode(dat6$RIDAGEYR, \" 60:70 = '60-70y'; else = 'Others' \", \n                            as.factor = T)\ndat6$age.cat &lt;- relevel(dat6$age.cat, ref = \"Others\")\ntable(dat6$age.cat, useNA = \"always\")\n#&gt; \n#&gt; Others 60-70y   &lt;NA&gt; \n#&gt;   3214    835      0\n  \n# Male\ntable(dat6$RIAGENDR, useNA = \"always\")\n#&gt; \n#&gt;   Male Female   &lt;NA&gt; \n#&gt;   1423   2626      0\ndat6$gender &lt;- relevel(dat6$RIAGENDR, ref = \"Female\")\n\n# College graduate \n#table(dat6$DMDEDUC2, useNA = \"always\")\ndat6$education &lt;- car::recode(dat6$DMDEDUC2, \" c('College Graduate or above', \n                           'College graduate or above') = 'College graduate'; \n                           c('Less Than 9th Grade', \n                           '9-11th Grade (Includes 12th grade with no diploma)',\n                           'High School Grad/GED or Equivalent', 'Less than 9th grade', \n                           '9-11th grade (Includes 12th grade with no diploma)', \n                           'High school graduate/GED or equivalent',\n                           'Some College or AA degree', 'Some college or AA degree') = \n                           'Less than college'; else = NA \", as.factor = T)\ndat6$education &lt;- relevel(dat6$education, ref = \"Less than college\")\ntable(dat6$education, useNA = \"always\")\n#&gt; \n#&gt; Less than college  College graduate              &lt;NA&gt; \n#&gt;              3296               749                 4\n\n# Poverty to income ratio \n#table(dat6$INDFMPIR, useNA = \"always\")\ndat6$poverty.ratio &lt;- car::recode(dat6$INDFMPIR, \" 0:2 = 'LE 2'; 2:5 = 'More than 2';\n                                  else = NA \", as.factor = T)\ntable(dat6$poverty.ratio, useNA = \"always\")\n#&gt; \n#&gt;        LE 2 More than 2        &lt;NA&gt; \n#&gt;        1835        1980         234\n\n# White\n#table(dat6$RIDRETH1, useNA = \"always\")\ndat6$race &lt;- car::recode(dat6$RIDRETH1, \" 'Non-Hispanic White' = 'White'; \n                         else = 'Non-White' \", as.factor = T)\ntable(dat6$race, useNA = \"always\")\n#&gt; \n#&gt; Non-White     White      &lt;NA&gt; \n#&gt;      1509      2540         0\n\n# Partnered\n#table(dat6$DMDMARTL, useNA = \"always\")\ndat6$marital &lt;- car::recode(dat6$DMDMARTL, \" c('Married', 'Living with partner') = \n                            'Partnered'; c('Widowed', 'Divorced', 'Separated', \n                            'Never married') = 'Single';else = NA\", as.factor = T)\ndat6$marital &lt;- relevel(dat6$marital, ref = \"Single\")\ntable(dat6$marital, useNA = \"always\")\n#&gt; \n#&gt;    Single Partnered      &lt;NA&gt; \n#&gt;      1768      2240        41\n\n# Smoking\n#table(dat6$SMOKING_HARMONIZED, useNA = \"always\")\ndat6$smoking &lt;- car::recode(dat6$SMOKING_HARMONIZED, \" c('Every day', 'Some days') = \n                            'Yes'; c('LOGICAL SKIP', 'Not at all') = 'No';\n                            else = NA\", as.factor = T)\ntable(dat6$smoking, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2983 1066    0\n\n# Hypertension\n#table(dat6$BPQ020, useNA = \"always\")\ndat6$hypertension &lt;- car::recode(dat6$BPQ020, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                 else = NA\", as.factor = T)\ntable(dat6$hypertension, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2095 1954    0\n\n# Hyperlipidemia\n#table(dat6$LIPIDEM_HARMONIZED, useNA = \"always\")\ndat6$hyperlipidemia &lt;- car::recode(dat6$LIPIDEM_HARMONIZED, \" 'No' = 'No';\n                                   'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$hyperlipidemia, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2326 1723    0\n\n# Stroke\n#table(dat6$MCQ160F, useNA = \"always\")\ndat6$stroke &lt;- car::recode(dat6$MCQ160F, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\", \n                           as.factor = T)\ntable(dat6$stroke, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3786  263    0\n\n# Myocardial infarction\n#table(dat6$MCQ160E, useNA = \"always\")\ndat6$heart.attack &lt;- car::recode(dat6$MCQ160E, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\", \n                                 as.factor = T)\ntable(dat6$heart.attack, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3770  279    0\n\n# Diabetes\n#table(dat6$DIQ010, useNA = \"always\")\ndat6$diabetes &lt;- car::recode(dat6$DIQ010, \" c('No', 'Borderline') = 'No'; \n                             'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$diabetes, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3404  645    0\n\n# Congestive heart failure\n#table(dat6$MCQ160B, useNA = \"always\")\ndat6$heart.failure &lt;- car::recode(dat6$MCQ160B, \" 'No' = 'No'; 'Yes' = 'Yes';\n                                  else = NA\", as.factor = T)\ntable(dat6$heart.failure, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3817  232    0\n\n# Pulmonary disease \n#table(dat6$PULMOND_HARMONIZED, useNA = \"always\")\ndat6$pulmonary.disease &lt;- car::recode(dat6$PULMOND_HARMONIZED, \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$pulmonary.disease, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3471  578    0\n\n# Liver disease \n#table(dat6$MCQ160L, useNA = \"always\")\ndat6$liver.disease &lt;- car::recode(dat6$MCQ160L, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                  else = NA\", as.factor = T)\ntable(dat6$liver.disease, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3807  242    0\n\n# Arthritis     \n#table(dat6$MCQ160A, useNA = \"always\")\ndat6$arthritis &lt;- car::recode(dat6$MCQ160A, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                              else = NA\", as.factor = T)\ntable(dat6$arthritis, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2060 1989    0\n\n# Kidney    disease \n#table(dat6$KIDNEYD_HARMONIZED, useNA = \"always\")\ndat6$kidney.disease &lt;- car::recode(dat6$KIDNEYD_HARMONIZED, \" 'No' = 'No'; \n                                   'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$kidney.disease, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3858  191    0\n\n# Cancer        \n#table(dat6$MCQ220, useNA = \"always\")\ndat6$cancer &lt;- car::recode(dat6$MCQ220, \" 'No' = 'No'; 'Yes' = 'Yes'; else = NA\",\n                           as.factor = T)\ntable(dat6$cancer, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3478  571    0\n\n# Regular   drinking    \n#table(dat6$REGULAR_DRINKING, useNA = \"always\")\ndat6$regular.drinking &lt;- car::recode(dat6$REGULAR_DRINKING, \" 'No' = 'No'; \n                                     'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$regular.drinking, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2830 1219    0\n\n# Antimicrobial drugs   - not available\n# table(dat6$antimicrobial.drugs, useNA = \"always\")\n\n# Hormonal  agents  \ntable(dat6$hormonalagents, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3045 1004    0\n\n# Anticonvulsants       \ntable(dat6$anticonvulsant, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3227  822    0\n\n# Any   analgesics  \ntable(dat6$analgesics, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1882 2167    0\n\n# Muscle    relaxants   \ntable(dat6$muscle_relax, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3655  394    0\n\n# Gastrointestinal  agents  \ntable(dat6$gastrointestin, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 2994 1055    0\n\n# Cardiac   or  metabolic medications       \ntable(dat6$cardi_metab_ag, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1844 2205    0\n\n# Respiratory   medications or antihistamines       \ntable(dat6$resp_antihist, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3097  952    0\n\n# &gt;2 CNS    medications\ntable(dat6$cns_morethan2, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3414  635    0\n\n# &lt;5 Non-CNS    medications \ntable(dat6$ncns_lessthan5, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4049    0    0\n\n# Antidepressants   (SNRIs, MAOIs,  or  TCAs)\ntable(dat6$antidepressants, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3841  208    0\n\n# Other antidepressants \ntable(dat6$antidepres_othr, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3577  472    0\n\n# Antipsychotics        \ntable(dat6$antipsychotics, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3856  193    0\n\n# Any   hospitalization in &lt;1y\n#table(dat6$ANYHOSP_INLASTYEAR, useNA = \"always\")\ndat6$hospitalization.any &lt;- car::recode(dat6$ANYHOSP_INLASTYEAR, \" 'No' = 'No'; \n                                     'Yes' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$hospitalization.any, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3144  905    0\n\n# Any   psychiatric visit in &lt;1y\n#table(dat6$HUQ090, useNA = \"always\")\ndat6$hospitalization.psy &lt;- car::recode(dat6$HUQ090, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                        else = NA\", as.factor = T)\ntable(dat6$hospitalization.psy, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3193  856    0\n\n# Good current  health\n#table(dat6$HUQ010, useNA = \"always\")\ndat6$good.current.health &lt;- car::recode(dat6$HUQ010, \" c('Excellent,', 'Very good,',\n                                        'Good,') = 'Yes'; c('Fair, or', 'Poor?') = 'No';\n                                        else = NA\", as.factor = T)\ntable(dat6$good.current.health, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 1520 2527    2\n\n# Require   special health equipment        \n#table(dat6$PFQ090, useNA = \"always\")\ndat6$special.health &lt;- car::recode(dat6$PFQ090, \" 'No' = 'No'; 'Yes' = 'Yes'; \n                                   else = NA\", as.factor = T)\ntable(dat6$special.health, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3256  793    0\n\n# Disabled - not available      \n# table(dat6$disabled, useNA = \"always\")\n\n# Regular   ED  care\n#table(dat6$REGULAR_EDCARE, useNA = \"always\")\ndat6$regular.ED.care &lt;- car::recode(dat6$REGULAR_EDCARE, \" 'Hospital emergency room' = \n                                    'Yes'; else = 'No'\",  as.factor = T)\ntable(dat6$regular.ED.care, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3951   98    0\n\n# Worsening health  \n#table(dat6$HUQ020, useNA = \"always\")\ndat6$worsening.health &lt;- car::recode(dat6$HUQ020, \" c('Better,', 'About the same?') = 'No'; \n                                     'Worse, or' = 'Yes'; else = NA\", as.factor = T)\ntable(dat6$worsening.health, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3183  865    1\n\n# &gt;2 Overnight  hospitalizations in &lt;1y\n#table(dat6$NUMHOSP_INLASTYEAR, useNA = \"always\")\ndat6$hospitalization.3plus &lt;- car::recode(dat6$NUMHOSP_INLASTYEAR, \" c('0', '1', '2') = 'No'; \n                                        c('3', '4', '5', '6 or more') = 'Yes';\n                                        else = NA\", as.factor = T)\ntable(dat6$hospitalization.3plus, useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3925  124    0\n\n2(b) Table 1\nReproduce Table 1 presented in the above paper. Please note that the numbers in your Table 1 will differ from Table 1 of the paper. But the numbers could look as follows:\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall\nBZDs plus opioids\nBZDs only\nOpioids only\nNeither\n\n\n\nn\n4049\n408\n961\n1434\n1246\n\n\nage (mean (SD))\n53.47 (17.06)\n55.13 (14.86)\n56.34 (17.06)\n51.54 (17.23)\n52.94 (17.21)\n\n\nage.cat = 60-70y (%)\n835 (20.6)\n84 (20.6)\n190 (19.8)\n306 (21.3)\n255 (20.5)\n\n\ngender = Male (%)\n1423 (35.1)\n157 (38.5)\n311 (32.4)\n591 (41.2)\n364 (29.2)\n\n\neducation = College graduate (%)\n749 (18.5)\n46 (11.3)\n193 (20.1)\n208 (14.5)\n302 (24.2)\n\n\npoverty.ratio = More than 2 (%)\n1980 (51.9)\n154 (39.8)\n491 (54.3)\n626 (46.2)\n709 (60.7)\n\n\nrace = White (%)\n2540 (62.7)\n280 (68.6)\n656 (68.3)\n735 (51.3)\n869 (69.7)\n\n\nmarital = Partnered (%)\n2240 (55.9)\n223 (54.9)\n517 (54.4)\n780 (55.1)\n720 (58.3)\n\n\nsmoking = Yes (%)\n1066 (26.3)\n157 (38.5)\n227 (23.6)\n431 (30.1)\n251 (20.1)\n\n\nhypertension = Yes (%)\n1954 (48.3)\n225 (55.1)\n457 (47.6)\n693 (48.3)\n579 (46.5)\n\n\nhyperlipidemia = Yes (%)\n1723 (42.6)\n191 (46.8)\n442 (46.0)\n540 (37.7)\n550 (44.1)\n\n\nstroke = Yes (%)\n263 ( 6.5)\n39 ( 9.6)\n56 ( 5.8)\n82 ( 5.7)\n86 ( 6.9)\n\n\nheart.attack = Yes (%)\n279 ( 6.9)\n38 ( 9.3)\n64 ( 6.7)\n95 ( 6.6)\n82 ( 6.6)\n\n\ndiabetes = Yes (%)\n645 (15.9)\n63 (15.4)\n133 (13.8)\n252 (17.6)\n197 (15.8)\n\n\nheart.failure = Yes (%)\n232 ( 5.7)\n36 ( 8.8)\n53 ( 5.5)\n77 ( 5.4)\n66 ( 5.3)\n\n\npulmonary.disease = Yes (%)\n578 (14.3)\n95 (23.3)\n117 (12.2)\n214 (14.9)\n152 (12.2)\n\n\nliver.disease = Yes (%)\n242 ( 6.0)\n36 ( 8.8)\n59 ( 6.1)\n88 ( 6.1)\n59 ( 4.7)\n\n\narthritis = Yes (%)\n1989 (49.1)\n290 (71.1)\n417 (43.4)\n791 (55.2)\n491 (39.4)\n\n\nkidney.disease = Yes (%)\n191 ( 4.7)\n27 ( 6.6)\n44 ( 4.6)\n72 ( 5.0)\n48 ( 3.9)\n\n\ncancer = Yes (%)\n571 (14.1)\n81 (19.9)\n154 (16.0)\n190 (13.2)\n146 (11.7)\n\n\nregular.drinking = Yes (%)\n1219 (30.1)\n109 (26.7)\n301 (31.3)\n395 (27.5)\n414 (33.2)\n\n\nhormonalagents = Yes (%)\n1004 (24.8)\n116 (28.4)\n265 (27.6)\n282 (19.7)\n341 (27.4)\n\n\nanticonvulsant = Yes (%)\n822 (20.3)\n187 (45.8)\n399 (41.5)\n153 (10.7)\n83 ( 6.7)\n\n\nanalgesics = Yes (%)\n2167 (53.5)\n403 (98.8)\n176 (18.3)\n1388 (96.8)\n200 (16.1)\n\n\nmuscle_relax = Yes (%)\n394 ( 9.7)\n96 (23.5)\n53 ( 5.5)\n206 (14.4)\n39 ( 3.1)\n\n\ngastrointestin = Yes (%)\n1055 (26.1)\n163 (40.0)\n257 (26.7)\n352 (24.5)\n283 (22.7)\n\n\ncardi_metab_ag = Yes (%)\n2205 (54.5)\n259 (63.5)\n559 (58.2)\n716 (49.9)\n671 (53.9)\n\n\nresp_antihist = Yes (%)\n952 (23.5)\n151 (37.0)\n150 (15.6)\n444 (31.0)\n207 (16.6)\n\n\ncns_morethan2 = Yes (%)\n635 (15.7)\n238 (58.3)\n127 (13.2)\n243 (16.9)\n27 ( 2.2)\n\n\nantidepressants = Yes (%)\n208 ( 5.1)\n52 (12.7)\n88 ( 9.2)\n51 ( 3.6)\n17 ( 1.4)\n\n\nantidepres_othr = Yes (%)\n472 (11.7)\n71 (17.4)\n119 (12.4)\n135 ( 9.4)\n147 (11.8)\n\n\nantipsychotics = Yes (%)\n193 ( 4.8)\n26 ( 6.4)\n68 ( 7.1)\n25 ( 1.7)\n74 ( 5.9)\n\n\nhospitalization.any = Yes (%)\n905 (22.4)\n121 (29.7)\n206 (21.4)\n376 (26.2)\n202 (16.2)\n\n\nhospitalization.psy = Yes (%)\n856 (21.1)\n95 (23.3)\n271 (28.2)\n155 (10.8)\n335 (26.9)\n\n\ngood.current.health = Yes (%)\n2527 (62.4)\n173 (42.4)\n627 (65.2)\n824 (57.5)\n903 (72.5)\n\n\nspecial.health = Yes (%)\n793 (19.6)\n144 (35.3)\n139 (14.5)\n337 (23.5)\n173 (13.9)\n\n\nregular.ED.care = Yes (%)\n98 ( 2.4)\n13 ( 3.2)\n15 ( 1.6)\n57 ( 4.0)\n13 ( 1.0)\n\n\nworsening.health = Yes (%)\n865 (21.4)\n131 (32.1)\n197 (20.5)\n359 (25.1)\n178 (14.3)\n\n\nhospitalization.3plus = Yes(%)\n124 ( 3.1)\n22 ( 5.4)\n21 ( 2.2)\n48 ( 3.3)\n33 ( 2.6)\n\n\n\n\nlibrary(tableone)\nvars &lt;- c(\"age\", \"age.cat\", \"gender\", \"education\", \"poverty.ratio\", \"race\", \n          \"marital\", \"smoking\", \"hypertension\", \"hyperlipidemia\", \"stroke\", \n          \"heart.attack\", \"diabetes\", \"heart.failure\", \"pulmonary.disease\", \n          \"liver.disease\", \"arthritis\", \"kidney.disease\", \"cancer\", \n          \"regular.drinking\", #\"antimicrobial.drugs\", \n          \"hormonalagents\", \"anticonvulsant\", \"analgesics\", \"muscle_relax\", \n          \"gastrointestin\", \"cardi_metab_ag\", \"resp_antihist\", \"cns_morethan2\", \n          #\"ncns_lessthan5\", \n          \"antidepressants\", \"antidepres_othr\", \"antipsychotics\", \n          \"hospitalization.any\", \"hospitalization.psy\", \"good.current.health\",\n          \"special.health\", #\"disabled\",\n          \"regular.ED.care\", \"worsening.health\", \"hospitalization.3plus\"\n          )\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"exposure\", data = dat6, test = F,\n                       addOverall = T)\nprint(tab1, showAllLevels = F)\n#&gt;                                   Stratified by exposure\n#&gt;                                    Overall       BZDs plus opioids\n#&gt;   n                                 4049           408            \n#&gt;   age (mean (SD))                  53.47 (17.06) 55.13 (14.86)    \n#&gt;   age.cat = 60-70y (%)               835 (20.6)     84 (20.6)     \n#&gt;   gender = Male (%)                 1423 (35.1)    157 (38.5)     \n#&gt;   education = College graduate (%)   749 (18.5)     46 (11.3)     \n#&gt;   poverty.ratio = More than 2 (%)   1980 (51.9)    154 (39.8)     \n#&gt;   race = White (%)                  2540 (62.7)    280 (68.6)     \n#&gt;   marital = Partnered (%)           2240 (55.9)    223 (54.9)     \n#&gt;   smoking = Yes (%)                 1066 (26.3)    157 (38.5)     \n#&gt;   hypertension = Yes (%)            1954 (48.3)    225 (55.1)     \n#&gt;   hyperlipidemia = Yes (%)          1723 (42.6)    191 (46.8)     \n#&gt;   stroke = Yes (%)                   263 ( 6.5)     39 ( 9.6)     \n#&gt;   heart.attack = Yes (%)             279 ( 6.9)     38 ( 9.3)     \n#&gt;   diabetes = Yes (%)                 645 (15.9)     63 (15.4)     \n#&gt;   heart.failure = Yes (%)            232 ( 5.7)     36 ( 8.8)     \n#&gt;   pulmonary.disease = Yes (%)        578 (14.3)     95 (23.3)     \n#&gt;   liver.disease = Yes (%)            242 ( 6.0)     36 ( 8.8)     \n#&gt;   arthritis = Yes (%)               1989 (49.1)    290 (71.1)     \n#&gt;   kidney.disease = Yes (%)           191 ( 4.7)     27 ( 6.6)     \n#&gt;   cancer = Yes (%)                   571 (14.1)     81 (19.9)     \n#&gt;   regular.drinking = Yes (%)        1219 (30.1)    109 (26.7)     \n#&gt;   hormonalagents = Yes (%)          1004 (24.8)    116 (28.4)     \n#&gt;   anticonvulsant = Yes (%)           822 (20.3)    187 (45.8)     \n#&gt;   analgesics = Yes (%)              2167 (53.5)    403 (98.8)     \n#&gt;   muscle_relax = Yes (%)             394 ( 9.7)     96 (23.5)     \n#&gt;   gastrointestin = Yes (%)          1055 (26.1)    163 (40.0)     \n#&gt;   cardi_metab_ag = Yes (%)          2205 (54.5)    259 (63.5)     \n#&gt;   resp_antihist = Yes (%)            952 (23.5)    151 (37.0)     \n#&gt;   cns_morethan2 = Yes (%)            635 (15.7)    238 (58.3)     \n#&gt;   antidepressants = Yes (%)          208 ( 5.1)     52 (12.7)     \n#&gt;   antidepres_othr = Yes (%)          472 (11.7)     71 (17.4)     \n#&gt;   antipsychotics = Yes (%)           193 ( 4.8)     26 ( 6.4)     \n#&gt;   hospitalization.any = Yes (%)      905 (22.4)    121 (29.7)     \n#&gt;   hospitalization.psy = Yes (%)      856 (21.1)     95 (23.3)     \n#&gt;   good.current.health = Yes (%)     2527 (62.4)    173 (42.4)     \n#&gt;   special.health = Yes (%)           793 (19.6)    144 (35.3)     \n#&gt;   regular.ED.care = Yes (%)           98 ( 2.4)     13 ( 3.2)     \n#&gt;   worsening.health = Yes (%)         865 (21.4)    131 (32.1)     \n#&gt;   hospitalization.3plus = Yes (%)    124 ( 3.1)     22 ( 5.4)     \n#&gt;                                   Stratified by exposure\n#&gt;                                    BZDs only     Opioids only  Neither      \n#&gt;   n                                  961          1434          1246        \n#&gt;   age (mean (SD))                  56.34 (17.06) 51.54 (17.23) 52.94 (17.21)\n#&gt;   age.cat = 60-70y (%)               190 (19.8)    306 (21.3)    255 (20.5) \n#&gt;   gender = Male (%)                  311 (32.4)    591 (41.2)    364 (29.2) \n#&gt;   education = College graduate (%)   193 (20.1)    208 (14.5)    302 (24.2) \n#&gt;   poverty.ratio = More than 2 (%)    491 (54.3)    626 (46.2)    709 (60.7) \n#&gt;   race = White (%)                   656 (68.3)    735 (51.3)    869 (69.7) \n#&gt;   marital = Partnered (%)            517 (54.4)    780 (55.1)    720 (58.3) \n#&gt;   smoking = Yes (%)                  227 (23.6)    431 (30.1)    251 (20.1) \n#&gt;   hypertension = Yes (%)             457 (47.6)    693 (48.3)    579 (46.5) \n#&gt;   hyperlipidemia = Yes (%)           442 (46.0)    540 (37.7)    550 (44.1) \n#&gt;   stroke = Yes (%)                    56 ( 5.8)     82 ( 5.7)     86 ( 6.9) \n#&gt;   heart.attack = Yes (%)              64 ( 6.7)     95 ( 6.6)     82 ( 6.6) \n#&gt;   diabetes = Yes (%)                 133 (13.8)    252 (17.6)    197 (15.8) \n#&gt;   heart.failure = Yes (%)             53 ( 5.5)     77 ( 5.4)     66 ( 5.3) \n#&gt;   pulmonary.disease = Yes (%)        117 (12.2)    214 (14.9)    152 (12.2) \n#&gt;   liver.disease = Yes (%)             59 ( 6.1)     88 ( 6.1)     59 ( 4.7) \n#&gt;   arthritis = Yes (%)                417 (43.4)    791 (55.2)    491 (39.4) \n#&gt;   kidney.disease = Yes (%)            44 ( 4.6)     72 ( 5.0)     48 ( 3.9) \n#&gt;   cancer = Yes (%)                   154 (16.0)    190 (13.2)    146 (11.7) \n#&gt;   regular.drinking = Yes (%)         301 (31.3)    395 (27.5)    414 (33.2) \n#&gt;   hormonalagents = Yes (%)           265 (27.6)    282 (19.7)    341 (27.4) \n#&gt;   anticonvulsant = Yes (%)           399 (41.5)    153 (10.7)     83 ( 6.7) \n#&gt;   analgesics = Yes (%)               176 (18.3)   1388 (96.8)    200 (16.1) \n#&gt;   muscle_relax = Yes (%)              53 ( 5.5)    206 (14.4)     39 ( 3.1) \n#&gt;   gastrointestin = Yes (%)           257 (26.7)    352 (24.5)    283 (22.7) \n#&gt;   cardi_metab_ag = Yes (%)           559 (58.2)    716 (49.9)    671 (53.9) \n#&gt;   resp_antihist = Yes (%)            150 (15.6)    444 (31.0)    207 (16.6) \n#&gt;   cns_morethan2 = Yes (%)            127 (13.2)    243 (16.9)     27 ( 2.2) \n#&gt;   antidepressants = Yes (%)           88 ( 9.2)     51 ( 3.6)     17 ( 1.4) \n#&gt;   antidepres_othr = Yes (%)          119 (12.4)    135 ( 9.4)    147 (11.8) \n#&gt;   antipsychotics = Yes (%)            68 ( 7.1)     25 ( 1.7)     74 ( 5.9) \n#&gt;   hospitalization.any = Yes (%)      206 (21.4)    376 (26.2)    202 (16.2) \n#&gt;   hospitalization.psy = Yes (%)      271 (28.2)    155 (10.8)    335 (26.9) \n#&gt;   good.current.health = Yes (%)      627 (65.2)    824 (57.5)    903 (72.5) \n#&gt;   special.health = Yes (%)           139 (14.5)    337 (23.5)    173 (13.9) \n#&gt;   regular.ED.care = Yes (%)           15 ( 1.6)     57 ( 4.0)     13 ( 1.0) \n#&gt;   worsening.health = Yes (%)         197 (20.5)    359 (25.1)    178 (14.3) \n#&gt;   hospitalization.3plus = Yes (%)     21 ( 2.2)     48 ( 3.3)     33 ( 2.6)",
    "crumbs": [
      "Accessing data",
      "Exercise 2 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE2solution.html#question-iii",
    "href": "accessingE2solution.html#question-iii",
    "title": "Exercise 2 Solution (A)",
    "section": "Question III:",
    "text": "Question III:\n3(a) Regression\nConsider all-cause mortality as a binary variable. Run logistic regression model for finding the association between Benzodiazepine Use With or Without Opioid Use (the exposure variable created in 1(d)) and all-cause mortality (the outcome variable). Adjust the model for three confounders: sex, age, and race/ethnicity. Also, use the Neither as the reference category for the exposure variable. Please note that we will not be using survey features (e.g., PSU, strata, weight) in this exercise. However, we will learn how to utilize these survey features in the Survey data analysis lab.\n\n#table(dat6$mortstat)\ndat6$exposure &lt;- relevel(dat6$exposure, ref = \"Neither\")\n\n# Logistic\nfit &lt;- glm(mortstat ~ exposure + age + gender + race, data = dat6, \n           family = \"binomial\")\n\n3(b) Reporting odds ratio\nReport the odds ratios and associated confidence intervals.\n\nlibrary(Publish)\npublish(fit)\n#&gt;  Variable             Units OddsRatio       CI.95    p-value \n#&gt;  exposure           Neither       Ref                        \n#&gt;           BZDs plus opioids      1.84 [1.37;2.48]    &lt; 1e-04 \n#&gt;                   BZDs only      1.08 [0.85;1.37]   0.516343 \n#&gt;                Opioids only      1.38 [1.11;1.73]   0.004381 \n#&gt;       age                        1.10 [1.09;1.11]    &lt; 1e-04 \n#&gt;    gender            Female       Ref                        \n#&gt;                        Male      1.70 [1.42;2.04]    &lt; 1e-04 \n#&gt;      race         Non-White       Ref                        \n#&gt;                       White      1.16 [0.96;1.39]   0.120046",
    "crumbs": [
      "Accessing data",
      "Exercise 2 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE3.html",
    "href": "accessingE3.html",
    "title": "Exercise 3 (A)",
    "section": "",
    "text": "Exercise: Phased Multi-Year NHANES Data Wrangling\nInstructions: Use the R programming language and functions from the tidyverse, nhanesA, tableone, and naniar packages to complete this exercise. We will build up our dataset in phases, starting with a single year and expanding to multiple survey cycles.\nPlease knit your final R Markdown file and submit the knitted HTML or PDF document ONLY.\n\nSetup: Load Packages\nFirst, run the following code block to ensure the required packages are installed and loaded into your R session.\n\n# Load required packages\n\nProblem 1: Import and Translate Single-Year Data\nDownload the Demographic (DEMO) data for the 2013-2014 NHANES cycle, using translated = TRUE to automatically convert coded values into text labels.\n\n# Download and translate the 2013-2014 demographic data\n\n# Check the first few rows to see the translated values\n\nProblem 2: Add Body Measures Data for a Single Year\nDownload the Body Measures (BMX) data for the same 2013-2014 cycle and merge it with the demographic data from Problem 1.\n\n# Download the 2013-2014 body measures data\n\n# Merge the BMX data with the DEMO data from Problem 1\n\n# Check the dimensions of the merged dataset\n\nProblem 3: Import and Merge Multi-Cycle Data with Translation\nExpand to multiple years, using translated = TRUE for all downloads. Merge both the Demographic (DEMO) and Body Measures (BMX) data for all three NHANES cycles: 2013-2014 (H), 2015-2016 (I), and 2017-2018 (J). Combine them into a single dataframe named nhanes_raw.\n\n# Define the cycles to download\n\n# Create a list to store data from each cycle\n\n# Loop through each cycle, download with translation, and merge the data\n\n# Combine the data from all cycles into one dataframe\n\n# Check the dimensions of the final raw dataset\n\nProblem 4: Data Cleaning and Filtering\nUsing the nhanes_raw dataset, we will now create our clean dataset. This involves filtering our population to adults and then creating our analysis variables.\n\n\nFilter for Adults: Keep only participants aged 20 years or older.\n\nRename Variables: RIAGENDR to Sex, RIDAGEYR to Age, RIDRETH3 to RaceEthnicity, BMXBMI to BMI.\n\nGroup RaceEthnicity: Combine “Mexican American” and “Other Hispanic” into a single “Hispanic” category.\n\nCreate AgeGroup: Categorize Age into “20-39”, “40-59”, and “60+”.\n\nCreate BMICat: Categorize BMI into “Underweight”, “Normal weight”, “Overweight”, and “Obese”.\n\n\n# nhanes_clean &lt;- ..\n  # Step 1: Filter the data to include only adults\n   \n  # Step 2: Rename variables\n   \n  # Create new variables\n   \n    # Convert the new character RaceEthnicity to a factor with the desired level order\n     \n    \n    # Step 4: Create AgeGroup (now without NAs because we filtered)\n     \n                   \n    # Step 5: Create BMICat\n     \n\n# Check the structure to confirm variables are correct\n\nProblem 5: Create Final Analytic Dataset\nCreate a final, analysis-ready dataset named nhanes_analysis that includes only the key variables.\n\n\n# Display the structure of the final analytic dataset\n\nProblem 6: Investigate Missing Data\nNow that our data is correctly filtered and processed, let’s re-examine the missing data patterns. The missingness should be much lower.\n\n# 1. Count missing values for each column\n\n# 2. Visualize the missing data\n\nProblem 7: Create a Descriptive Table\nFinally, with the data correctly loaded and cleaned for our adult population, create the summary table of sample characteristics, stratified by Sex.\n\n# Define the variables for the table\n\n# Create the table, stratified by Sex\n\n# Print the table, showing all levels and missing data counts",
    "crumbs": [
      "Accessing data",
      "Exercise 3 (A)"
    ]
  },
  {
    "objectID": "accessingE3solution.html",
    "href": "accessingE3solution.html",
    "title": "Exercise 3 Solution (A)",
    "section": "",
    "text": "Exercise: Phased Multi-Year NHANES Data Wrangling\nInstructions: Use the R programming language and functions from the tidyverse, nhanesA, tableone, and naniar packages to complete this exercise. We will build up our dataset in phases, starting with a single year and expanding to multiple survey cycles.\nPlease knit your final R Markdown file and submit the knitted HTML or PDF document ONLY.\n\nSetup: Load Packages\nFirst, run the following code block to ensure the required packages are installed and loaded into your R session.\n\n# Load required packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, nhanesA, tableone, naniar)\n\nProblem 1: Import and Translate Single-Year Data\nDownload the Demographic (DEMO) data for the 2013-2014 NHANES cycle, using translated = TRUE to automatically convert coded values into text labels.\n\n# Download and translate the 2013-2014 demographic data\nnhanes_demo_1314 &lt;- nhanes(\"DEMO_H\", translated = TRUE)\n# Check the first few rows to see the translated values\nhead(nhanes_demo_1314[, c(\"SEQN\", \"RIAGENDR\", \"RIDAGEYR\", \"RIDRETH3\")])\n\n\n  \n\n\n\nProblem 2: Add Body Measures Data for a Single Year\nDownload the Body Measures (BMX) data for the same 2013-2014 cycle and merge it with the demographic data from Problem 1.\n\n# Download the 2013-2014 body measures data\nnhanes_bmx_1314 &lt;- nhanes(\"BMX_H\", translated = TRUE)\n# Merge the BMX data with the DEMO data from Problem 1\nnhanes_1314_complete &lt;- full_join(nhanes_demo_1314, nhanes_bmx_1314, by = \"SEQN\")\n# Check the dimensions of the merged dataset\ndim(nhanes_1314_complete)\n#&gt; [1] 10175    72\n\nProblem 3: Import and Merge Multi-Cycle Data with Translation\nExpand to multiple years, using translated = TRUE for all downloads. Merge both the Demographic (DEMO) and Body Measures (BMX) data for all three NHANES cycles: 2013-2014 (H), 2015-2016 (I), and 2017-2018 (J). Combine them into a single dataframe named nhanes_raw.\n\n# Define the cycles to download\ncycles &lt;- c(\"H\", \"I\", \"J\")\n# Create a list to store data from each cycle\nall_cycles_data &lt;- list()\n# Loop through each cycle, download with translation, and merge the data\nfor (cycle_code in cycles) {\n  demo_data &lt;- nhanes(paste0(\"DEMO_\", cycle_code), translated = TRUE)\n  bmx_data &lt;- nhanes(paste0(\"BMX_\", cycle_code), translated = TRUE)\n  all_cycles_data[[cycle_code]] &lt;- full_join(demo_data, bmx_data, by = \"SEQN\")\n}\n# Combine the data from all cycles into one dataframe\nnhanes_raw &lt;- bind_rows(all_cycles_data)\n# Check the dimensions of the final raw dataset\ndim(nhanes_raw)\n#&gt; [1] 29400    78\n\nProblem 4: Data Cleaning and Filtering\nUsing the nhanes_raw dataset, we will now create our clean dataset. This involves filtering our population to adults and then creating our analysis variables.\n\n\nFilter for Adults: Keep only participants aged 20 years or older.\n\nRename Variables: RIAGENDR to Sex, RIDAGEYR to Age, RIDRETH3 to RaceEthnicity, BMXBMI to BMI.\n\nGroup RaceEthnicity: Combine “Mexican American” and “Other Hispanic” into a single “Hispanic” category.\n\nCreate AgeGroup: Categorize Age into “20-39”, “40-59”, and “60+”.\n\nCreate BMICat: Categorize BMI into “Underweight”, “Normal weight”, “Overweight”, and “Obese”.\n\n\nnhanes_clean &lt;- nhanes_raw %&gt;%\n  # Step 1: Filter the data to include only adults\n  filter(RIDAGEYR &gt;= 20) %&gt;%\n  # Step 2: Rename variables\n  rename(\n    Sex = RIAGENDR,\n    Age = RIDAGEYR,\n    RaceEthnicity = RIDRETH3,\n    BMI = BMXBMI\n  ) %&gt;%\n  # Create new variables\n  mutate(\n    # Step 3: Group the detailed race categories into broader ones\n    RaceEthnicity = case_when(\n      RaceEthnicity %in% c(\"Mexican American\", \"Other Hispanic\") ~ \"Hispanic\",\n      RaceEthnicity == \"Non-Hispanic White\" ~ \"Non-Hispanic White\",\n      RaceEthnicity == \"Non-Hispanic Black\" ~ \"Non-Hispanic Black\",\n      RaceEthnicity == \"Non-Hispanic Asian\" ~ \"Non-Hispanic Asian\",\n      TRUE ~ \"Other\" # Groups \"Other Race - Including Multi-Racial\" and any NAs\n    ),\n    # Convert the new character RaceEthnicity to a factor with the desired level order\n    RaceEthnicity = factor(RaceEthnicity,\n                           levels = c(\"Non-Hispanic White\", \"Non-Hispanic Black\", \"Hispanic\", \"Non-Hispanic Asian\", \"Other\")),\n    \n    # Step 4: Create AgeGroup (now without NAs because we filtered)\n    AgeGroup = cut(Age,\n                   breaks = c(19, 39, 59, Inf),\n                   labels = c(\"20-39\", \"40-59\", \"60+\"),\n                   right = TRUE),\n                   \n    # Step 5: Create BMICat\n    BMICat = cut(BMI,\n                 breaks = c(0, 18.5, 25, 30, Inf),\n                 labels = c(\"Underweight\", \"Normal weight\", \"Overweight\", \"Obese\"),\n                 right = FALSE)\n  )\n\n# Check the structure to confirm variables are correct\nstr(nhanes_clean[, c(\"SEQN\", \"Sex\", \"Age\", \"AgeGroup\", \"RaceEthnicity\", \"BMI\")])\n#&gt; 'data.frame':    17057 obs. of  6 variables:\n#&gt;  $ SEQN         : num  73557 73558 73559 73561 73562 ...\n#&gt;  $ Sex          : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#&gt;  $ Age          : num  69 54 72 73 56 61 42 56 65 26 ...\n#&gt;  $ AgeGroup     : Factor w/ 3 levels \"20-39\",\"40-59\",..: 3 2 3 3 2 3 2 2 3 1 ...\n#&gt;  $ RaceEthnicity: Factor w/ 5 levels \"Non-Hispanic White\",..: 2 1 1 1 3 1 3 1 1 1 ...\n#&gt;  $ BMI          : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n\nProblem 5: Create Final Analytic Dataset\nCreate a final, analysis-ready dataset named nhanes_analysis that includes only the key variables.\n\nnhanes_analysis &lt;- select(nhanes_clean, SEQN, Sex, Age, AgeGroup, RaceEthnicity, BMI, BMICat)\n\n# Display the structure of the final analytic dataset\nstr(nhanes_analysis)\n#&gt; 'data.frame':    17057 obs. of  7 variables:\n#&gt;  $ SEQN         : num  73557 73558 73559 73561 73562 ...\n#&gt;  $ Sex          : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#&gt;  $ Age          : num  69 54 72 73 56 61 42 56 65 26 ...\n#&gt;  $ AgeGroup     : Factor w/ 3 levels \"20-39\",\"40-59\",..: 3 2 3 3 2 3 2 2 3 1 ...\n#&gt;  $ RaceEthnicity: Factor w/ 5 levels \"Non-Hispanic White\",..: 2 1 1 1 3 1 3 1 1 1 ...\n#&gt;  $ BMI          : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n#&gt;  $ BMICat       : Factor w/ 4 levels \"Underweight\",..: 3 3 3 2 4 4 NA 3 2 2 ...\n\nProblem 6: Investigate Missing Data\nNow that our data is correctly filtered and processed, let’s re-examine the missing data patterns. The missingness should be much lower.\n\n# 1. Count missing values for each column\nmissing_counts &lt;- colSums(is.na(nhanes_analysis))\nprint(missing_counts)\n#&gt;          SEQN           Sex           Age      AgeGroup RaceEthnicity \n#&gt;             0             0             0             0             0 \n#&gt;           BMI        BMICat \n#&gt;           956           956\n\n# 2. Visualize the missing data\ngg_miss_var(nhanes_analysis, show_pct = TRUE) +\n  labs(title = \"Missing Data in NHANES Adult Participants (2013-2018)\")\n\n\n\n\n\n\n\nProblem 7: Create a Descriptive Table\nFinally, with the data correctly loaded and cleaned for our adult population, create the summary table of sample characteristics, stratified by Sex.\n\n# Define the variables for the table\nvars &lt;- c(\"Age\", \"AgeGroup\", \"RaceEthnicity\", \"BMI\", \"BMICat\")\n\n# Create the table, stratified by Sex\ntab1 &lt;- CreateTableOne(vars = vars, data = nhanes_analysis, strata = \"Sex\", addOverall = TRUE, test = FALSE)\n\n# Print the table, showing all levels and missing data counts\nprint(tab1, smd = FALSE, showAllLevels = TRUE, missing = TRUE)\n#&gt;                    Stratified by Sex\n#&gt;                     level              Overall       Male         \n#&gt;   n                                    17057          8207        \n#&gt;   Age (mean (SD))                      50.03 (17.74) 50.23 (17.83)\n#&gt;   AgeGroup (%)      20-39               5594 (32.8)   2687 (32.7) \n#&gt;                     40-59               5571 (32.7)   2612 (31.8) \n#&gt;                     60+                 5892 (34.5)   2908 (35.4) \n#&gt;   RaceEthnicity (%) Non-Hispanic White  6270 (36.8)   3094 (37.7) \n#&gt;                     Non-Hispanic Black  3673 (21.5)   1759 (21.4) \n#&gt;                     Hispanic            4290 (25.2)   1978 (24.1) \n#&gt;                     Non-Hispanic Asian  2168 (12.7)   1034 (12.6) \n#&gt;                     Other                656 ( 3.8)    342 ( 4.2) \n#&gt;   BMI (mean (SD))                      29.49 (7.21)  28.93 (6.33) \n#&gt;   BMICat (%)        Underweight          247 ( 1.5)    105 ( 1.4) \n#&gt;                     Normal weight       4244 (26.4)   1966 (25.5) \n#&gt;                     Overweight          5168 (32.1)   2853 (37.0) \n#&gt;                     Obese               6442 (40.0)   2790 (36.2) \n#&gt;                    Stratified by Sex\n#&gt;                     Female        Missing\n#&gt;   n                  8850                \n#&gt;   Age (mean (SD))   49.86 (17.66) 0.0    \n#&gt;   AgeGroup (%)       2907 (32.8)  0.0    \n#&gt;                      2959 (33.4)         \n#&gt;                      2984 (33.7)         \n#&gt;   RaceEthnicity (%)  3176 (35.9)  0.0    \n#&gt;                      1914 (21.6)         \n#&gt;                      2312 (26.1)         \n#&gt;                      1134 (12.8)         \n#&gt;                       314 ( 3.5)         \n#&gt;   BMI (mean (SD))   30.01 (7.90)  5.6    \n#&gt;   BMICat (%)          142 ( 1.7)  5.6    \n#&gt;                      2278 (27.2)         \n#&gt;                      2315 (27.6)         \n#&gt;                      3652 (43.5)",
    "crumbs": [
      "Accessing data",
      "Exercise 3 Solution (A)"
    ]
  },
  {
    "objectID": "accessingE3vibe.html",
    "href": "accessingE3vibe.html",
    "title": "Exercise 3 (A) Vibe",
    "section": "",
    "text": "Load Required Packages\nIn this tutorial, we will be going through the following exercise (found here).\nWe will go through the exercise problems and evaluate how well Gemini performs on each task. These tasks include importing and merging data, cleaning and filtering observations, and once the final dataset is completed, examining missing data, and displaying the data in a table.\nIn this part of the tutorial, we will go through the first set of tasks, which includes loading, subsetting the data according to eligibility criteria and necessary variables, creating an analytic dataset, and reporting the number of columns and variable names in this dataset. These tutorials were completed in RStudio (Posit team 2023) using R version 4.3.2 (R Core Team 2023), with the following packages: tidyverse (Wickham et al. 2019), and nhanesA (Ale et al. 2024), tableone (Yoshida and Bartel 2022), and naniar (Tierney and Cook 2023)\nFor this tutorial, I used Gemini’s free 2.5 Flash model.\nFirst, we’ll run the following code to install and load all required packages into our R session.\npackages &lt;- c(\"tidyverse\", \"nhanesA\", \"tableone\", \"naniar\")\nlapply(packages, function(pkg) {\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg, dependencies = TRUE)\n  library(pkg, character.only = TRUE)\n})",
    "crumbs": [
      "Accessing data",
      "Exercise 3 (A) Vibe"
    ]
  },
  {
    "objectID": "accessingE3vibe.html#references",
    "href": "accessingE3vibe.html#references",
    "title": "Exercise 3 (A) Vibe",
    "section": "References",
    "text": "References\n\n\n\n\nAle, Laha, Robert Gentleman, Teresa Filshtein Sonmez, Deepayan Sarkar, and Christopher Endres. 2024. “nhanesA: Achieving Transparency and Reproducibility in NHANES Research.” Database Apr 15. https://doi.org/10.1093/database/baae028.\n\n\nPosit team. 2023. RStudio: Integrated Development Environment for r. Boston, MA: Posit Software, PBC. http://www.posit.co/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nTierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” Journal of Statistical Software 105 (7): 1–31. https://doi.org/10.18637/jss.v105.i07.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create ’Table 1’ to Describe Baseline Characteristics with or Without Propensity Score Weights. https://CRAN.R-project.org/package=tableone.",
    "crumbs": [
      "Accessing data",
      "Exercise 3 (A) Vibe"
    ]
  },
  {
    "objectID": "exploring.html",
    "href": "exploring.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Background\nNow that we have accessed our data, it is essential to spend some time familiarizing ourselves with it. Conducting an exploratory data analysis (EDA) helps us understand the data’s characteristics by visualizing it or using quantitative measures to summarize variables. These steps can reveal outliers and uncover interesting or unexpected patterns in the data. The results of the exploratory analysis will guide our decisions in subsequent analyses.\nIn this guide, we will demonstrate a wide array of exploratory methods for different variable types, with hands-on examples. This will provide a comprehensive toolbox of EDA strategies.",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exploring.html#background",
    "href": "exploring.html#background",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In the previous chapter, we learned how to access external survey datasets. In this chapter, we will focus on the first step of working with data: exploratory data analysis. EDA is crucial for gaining an overview and understanding of the dataset. The next chapter will address two major types of research questions and will go through examples of each. These discussions will lead us into more detailed chapters on the different approaches taken when analyzing each type of research question.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll the datasets used in this tutorial can be accessed from this GitHub repository folder.",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exploring.html#overview-of-tutorials",
    "href": "exploring.html#overview-of-tutorials",
    "title": "Exploratory Data Analysis",
    "section": "Overview of Tutorials",
    "text": "Overview of Tutorials\n\nExploring Individual Variables\nThis tutorial introduces basic methods for summarizing and visualizing continuous and categorical variables. These methods provide an overview of the types of variables in the dataset and how they behave.\n\n\nExploring Pairwise Variable Relationships\nThis tutorial introduces methods to explore and visualize pairwise variable relationships. Examining these relationships allows us to identify correlations and discover potentially interesting patterns for further investigation.\n\n\nUseful Packages for Exploratory Data Analysis\nThis tutorial provides detailed instructions on how to import and process health survey datasets.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThe upcoming chapter on Research Questions serves as a guide for constructing an analytics-driven dataset tailored to specific research queries. It will cover critical aspects such as selecting relevant variables and setting eligibility criteria, followed by approaches to data analysis based on the research questions. Research questions generally fall into two categories: predictive and causal. For a deeper understanding of variable selection and analytical tools suited to these questions, refer to the chapters on Roles of Variables and Predictive Models.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nPlease fill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exploring0.html",
    "href": "exploring0.html",
    "title": "Univariate",
    "section": "",
    "text": "This tutorial covers various methods to explore the properties of individual variables in a dataset. Understanding individual variables helps us recognize the types of variables present and their behavior. For instance, visualizing variable distributions informs our modeling choices and highlights key features of the data.\nRevisiting RHC Data\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial reuses data from earlier examples, including those related to a predictive question, machine learning with a continuous outcome, and machine learning with a binary outcome.\n\n\n\nObsData &lt;- readRDS(file = \"Data/machinelearningCausal/rhcAnalyticTest.RDS\")\nhead(ObsData)\n\n\n  \n\n\n\nFirst Steps\nTo begin, it’s important to assess the size of the dataset and the number of variables it contains. Use the dim function to check the dataset dimensions:\n\ndim(ObsData)\n#&gt; [1] 5735   52\n\nR will output the number of rows (observations) followed by the number of columns (variables).\nNext, we need to examine which variables are present. We can list the column names (variables) with the colnames function:\n\ncolnames(ObsData)\n#&gt;  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#&gt;  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#&gt;  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#&gt; [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#&gt; [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#&gt; [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#&gt; [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#&gt; [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#&gt; [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#&gt; [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#&gt; [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#&gt; [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#&gt; [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#&gt; [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#&gt; [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#&gt; [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#&gt; [49] \"income\"                \"Length.of.Stay\"        \"Death\"                \n#&gt; [52] \"RHC.use\"\n\nWe should also inspect the types of these variables. This can be done individually:\n\nclass(ObsData$Cancer)\n#&gt; [1] \"factor\"\n\nOr for all variables at once:\n\nsapply(ObsData, class)\n#&gt;      Disease.category                Cancer        Cardiovascular \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;         Congestive.HF              Dementia           Psychiatric \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;             Pulmonary                 Renal               Hepatic \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;              GI.Bleed                 Tumor     Immunosupperssion \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;           Transfer.hx                    MI                   age \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;                   sex                   edu              DASIndex \n#&gt;              \"factor\"             \"numeric\"             \"numeric\" \n#&gt;          APACHE.score    Glasgow.Coma.Score        blood.pressure \n#&gt;             \"integer\"             \"integer\"             \"numeric\" \n#&gt;                   WBC            Heart.rate      Respiratory.rate \n#&gt;             \"numeric\"             \"integer\"             \"numeric\" \n#&gt;           Temperature           PaO2vs.FIO2               Albumin \n#&gt;             \"numeric\"             \"numeric\"             \"numeric\" \n#&gt;            Hematocrit             Bilirubin            Creatinine \n#&gt;             \"numeric\"             \"numeric\"             \"numeric\" \n#&gt;                Sodium             Potassium                 PaCo2 \n#&gt;             \"integer\"             \"numeric\"             \"numeric\" \n#&gt;                    PH                Weight            DNR.status \n#&gt;             \"numeric\"             \"numeric\"              \"factor\" \n#&gt;     Medical.insurance      Respiratory.Diag   Cardiovascular.Diag \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;     Neurological.Diag Gastrointestinal.Diag            Renal.Diag \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;        Metabolic.Diag      Hematologic.Diag           Sepsis.Diag \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;           Trauma.Diag       Orthopedic.Diag                  race \n#&gt;              \"factor\"              \"factor\"              \"factor\" \n#&gt;                income        Length.of.Stay                 Death \n#&gt;              \"factor\"             \"integer\"             \"numeric\" \n#&gt;               RHC.use \n#&gt;             \"numeric\"\n\nBasic Quantitative Summaries\nR’s built-in summary function provides key statistics. For a continuous variable like blood.pressure, it outputs the minimum, maximum, mean, median, and quartile values:\n\nsummary(ObsData$blood.pressure)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.00   50.00   63.00   78.52  115.00  259.00\n\nFor a factor variable like Disease.category, the summary function gives counts for each category:\n\nsummary(ObsData$Disease.category)\n#&gt;   ARF   CHF Other  MOSF \n#&gt;  2490   456  1163  1626\n\nVisualizing Continuous Variables\nTo explore continuous variables, histograms are useful for visualizing distributions:\n\nggplot(data = ObsData) + geom_histogram(aes(x = blood.pressure))\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\nYou can adjust the binwidth argument to change the detail of the histogram:\n\nggplot(data = ObsData) + geom_histogram(aes(x = blood.pressure), binwidth = 5)\n\n\n\n\n\n\n\nFor multiple continuous variables, we can display several histograms simultaneously using the ggpubr package:\n\nplot1 &lt;- ggplot(data = ObsData) + geom_histogram(aes(x = blood.pressure), binwidth = 10)\nplot2 &lt;- ggplot(data = ObsData) + geom_histogram(aes(x = Temperature), binwidth = 1)\nplot3 &lt;- ggplot(data = ObsData) + geom_histogram(aes(x = Weight), binwidth = 10)\nplot4 &lt;- ggplot(data = ObsData) + geom_histogram(aes(x = Length.of.Stay), binwidth = 10)\nggarrange(plot1, plot2, plot3, plot4)\n\n\n\n\n\n\n\nVisualizing Factor Variables\nThe distribution of factor variables can be visualized using bar charts:\n\nggplot(data = ObsData) + geom_bar(aes(x = Disease.category))\n\n\n\n\n\n\n\nYou can also display the bar chart horizontally:\n\nggplot(data = ObsData) + geom_bar(aes(y = Disease.category))\n\n\n\n\n\n\n\nTo visualize multiple factor variables side by side:\n\nplot1 &lt;- ggplot(data = ObsData) + geom_bar(aes(y = Disease.category))\nplot2 &lt;- ggplot(data = ObsData) + geom_bar(aes(y = Medical.insurance))\nplot3 &lt;- ggplot(data = ObsData) + geom_bar(aes(y = age))\nplot4 &lt;- ggplot(data = ObsData) + geom_bar(aes(y = race))\nggarrange(plot1, plot2, plot3, plot4)\n\n\n\n\n\n\n\nExploring Missing Data\nBefore diving deeper into the analysis, it’s important to understand how much of your data is missing. You can check for missing values in your dataset using the is.na() function in R.\nTo see how many missing values each column contains:\n\nmiss &lt;- sapply(ObsData, function(x) sum(is.na(x)))\nkable(miss)\n\n\n\n\nx\n\n\n\nDisease.category\n0\n\n\nCancer\n0\n\n\nCardiovascular\n0\n\n\nCongestive.HF\n0\n\n\nDementia\n0\n\n\nPsychiatric\n0\n\n\nPulmonary\n0\n\n\nRenal\n0\n\n\nHepatic\n0\n\n\nGI.Bleed\n0\n\n\nTumor\n0\n\n\nImmunosupperssion\n0\n\n\nTransfer.hx\n0\n\n\nMI\n0\n\n\nage\n0\n\n\nsex\n0\n\n\nedu\n0\n\n\nDASIndex\n0\n\n\nAPACHE.score\n0\n\n\nGlasgow.Coma.Score\n0\n\n\nblood.pressure\n0\n\n\nWBC\n0\n\n\nHeart.rate\n0\n\n\nRespiratory.rate\n0\n\n\nTemperature\n0\n\n\nPaO2vs.FIO2\n0\n\n\nAlbumin\n0\n\n\nHematocrit\n0\n\n\nBilirubin\n0\n\n\nCreatinine\n0\n\n\nSodium\n0\n\n\nPotassium\n0\n\n\nPaCo2\n0\n\n\nPH\n0\n\n\nWeight\n0\n\n\nDNR.status\n0\n\n\nMedical.insurance\n0\n\n\nRespiratory.Diag\n0\n\n\nCardiovascular.Diag\n0\n\n\nNeurological.Diag\n0\n\n\nGastrointestinal.Diag\n0\n\n\nRenal.Diag\n0\n\n\nMetabolic.Diag\n0\n\n\nHematologic.Diag\n0\n\n\nSepsis.Diag\n0\n\n\nTrauma.Diag\n0\n\n\nOrthopedic.Diag\n0\n\n\nrace\n0\n\n\nincome\n0\n\n\nLength.of.Stay\n0\n\n\nDeath\n0\n\n\nRHC.use\n0\n\n\n\n\n\nOnce identified, you can either impute the missing values or remove them, depending on the type of analysis and dataset.\nFor example, to remove rows with missing data:\n\nna.omit(ObsData)\n\n\n  \n\n\n\nAlternatively, you can impute missing values using methods such as mean imputation or more advanced techniques:\n\nObsData$blood.pressure[is.na(ObsData$blood.pressure)] &lt;- mean(ObsData$blood.pressure, na.rm = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\nWe will learn more about exploring missing data in the package section, and then further details about imputation in the upcoming missing data chapter.\n\n\nDetecting Outliers\nOutliers are extreme values that can distort statistical analysis. To detect outliers in continuous variables, we can use boxplots or calculate Z-scores.\nFor example, using a boxplot to identify outliers in the blood.pressure variable:\n\nggplot(data = ObsData, aes(x = \"\", y = blood.pressure)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nTo detect outliers using Z-scores:\n\nz_scores &lt;- scale(ObsData$blood.pressure)\noutliers &lt;- which(abs(z_scores) &gt; 3)\nObsData[outliers, ]",
    "crumbs": [
      "Exploratory Data Analysis",
      "Univariate"
    ]
  },
  {
    "objectID": "exploring1.html",
    "href": "exploring1.html",
    "title": "Bivariate",
    "section": "",
    "text": "In this tutorial, we will demonstrate methods for exploring pairwise relationships in the data. By doing so, we can identify which variables are correlated and uncover relationships that may be worth further examination in more detailed analyses.\nRevisiting RHC Data\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial utilizes the same dataset as in previous tutorials, such as those focusing on a predictive question, machine learning with a continuous outcome, and machine learning with a binary outcome.\n\n\nCategorical Variables\nStacked bar charts allow us to visualize the counts for combinations of two categorical variables:\n\nggplot(ObsData, aes(y = Medical.insurance)) +\n  geom_bar(aes(fill = Disease.category)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nNumerical Variables\nFor numerical variables, scatterplots can be used to observe any patterns or relationships between two variables:\n\nggplot(ObsData, aes(x=Weight, y=APACHE.score)) + \n  geom_point()\n\n\n\n\n\n\n\nAnother useful tool for examining pairwise relationships between numerical variables is a correlation heatmap:\n\n# Select all numerical variables\nObsData_num &lt;- select(ObsData, c(\"edu\", \"blood.pressure\", \"WBC\", \"Heart.rate\", \"Respiratory.rate\", \"Temperature\", \"PH\", 'Weight', \"Length.of.Stay\"))\n\n# Create the correlation matrix\ncorr_mat &lt;- round(cor(ObsData_num), 2)\nhead(corr_mat)\n#&gt;                    edu blood.pressure   WBC Heart.rate Respiratory.rate\n#&gt; edu               1.00          -0.04 -0.02       0.05             0.03\n#&gt; blood.pressure   -0.04           1.00 -0.03       0.06             0.03\n#&gt; WBC              -0.02          -0.03  1.00       0.03             0.01\n#&gt; Heart.rate        0.05           0.06  0.03       1.00             0.28\n#&gt; Respiratory.rate  0.03           0.03  0.01       0.28             1.00\n#&gt; Temperature       0.07           0.01 -0.01       0.22             0.14\n#&gt;                  Temperature    PH Weight Length.of.Stay\n#&gt; edu                     0.07  0.04  -0.06           0.02\n#&gt; blood.pressure          0.01  0.13  -0.02          -0.02\n#&gt; WBC                    -0.01 -0.06  -0.01           0.03\n#&gt; Heart.rate              0.22  0.03   0.02           0.07\n#&gt; Respiratory.rate        0.14 -0.01  -0.01          -0.01\n#&gt; Temperature             1.00  0.14   0.02           0.09\n\n# Reshape the correlation matrix using the reshape2 package\nmelted_corr_mat &lt;- reshape2::melt(corr_mat)\n\n\n# Plot the correlation heatmap\nggplot(data = melted_corr_mat, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))\n\n\n\n\n\n\n\nNumerical-Categorical Relationships\nTo explore the relationship between a numerical and a categorical variable, boxplots can be utilized:\n\nObsData$RHC.use &lt;- as.factor(ObsData$RHC.use)\n\nggplot(ObsData, aes(x = RHC.use, y = APACHE.score)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nAlso, we can compare the distribution of blood.pressure across different levels of Disease.category using box plots. This is another example of exploring how continuous variables behave across different groups.\n\nggplot(data = ObsData, aes(x = Disease.category, y = blood.pressure)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nSimilarly, you can create density plots for better visualization of distributions across groups:\n\nggplot(data = ObsData, aes(x = blood.pressure, fill = Disease.category)) + \n  geom_density(alpha = 0.5)",
    "crumbs": [
      "Exploratory Data Analysis",
      "Bivariate"
    ]
  },
  {
    "objectID": "exploring2.html",
    "href": "exploring2.html",
    "title": "Useful Packages",
    "section": "",
    "text": "This tutorial introduces a variety of methods to explore a dataset, including summary statistics, variable distributions, correlations, and handling missing data.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial uses the same dataset as previous tutorials, including working with a predictive question, machine learning with a continuous outcome, and machine learning with a binary outcome.\n\n\nTableOne\nTableOne is an R package that provides a simple method to create the classic “Table 1” seen in health research papers, summarizing the characteristics of the dataset. It also offers functions like svyCreateTableOne for survey data, allowing users to account for strata and weights, and display counts and proportions for both weighted and unweighted data.\n\nrequire(tableone)\n#&gt; Loading required package: tableone\n\n\nCreateTableOne(vars = c(\"Disease.category\", \"Cancer\", \"Cardiovascular\", \"Congestive.HF\", \n                        \"Dementia\", \"Psychiatric\", \"Pulmonary\", \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\",\n                        \"Immunosuppression\", \"Transfer.hx\", \"MI\", \"age\", \"sex\", \"edu\", \"DASIndex\",\n                        \"APACHE.score\", \"Glasgow.Coma.Score\", \"blood.pressure\", \"WBC\", \"Heart.rate\",\n                        \"Respiratory.rate\", \"Temperature\", \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\",\n                        \"Bilirubin\", \"Creatinine\", \"Sodium\", \"Potassium\", \"PaCo2\", \"PH\", \"Weight\",\n                        \"DNR.status\", \"Medical.insurance\", \"Respiratory.Diag\", \"Cardiovascular.Diag\",\n                        \"Neurological.Diag\", \"Gastrointestinal.Diag\", \"Renal.Diag\", \"Metabolic.Diag\",\n                        \"Hematologic.Diag\", \"Sepsis.Diag\", \"Trauma.Diag\", \"Orthopedic.Diag\", \"race\", \n                        \"income\", \"Length.of.Stay\", \"Death\"),\n               strata = \"RHC.use\",\n               data = ObsData,\n               includeNA = TRUE,\n               test = TRUE)\n#&gt; Warning in ModuleReturnVarsExist(vars, data): The data frame does not have:\n#&gt; Immunosuppression Dropped\n#&gt;                                  Stratified by RHC.use\n#&gt;                                   0               1               p      test\n#&gt;   n                                 3551            2184                     \n#&gt;   Disease.category (%)                                            &lt;0.001     \n#&gt;      ARF                            1581 (44.5)      909 (41.6)              \n#&gt;      CHF                             247 ( 7.0)      209 ( 9.6)              \n#&gt;      Other                           955 (26.9)      208 ( 9.5)              \n#&gt;      MOSF                            768 (21.6)      858 (39.3)              \n#&gt;   Cancer (%)                                                       0.001     \n#&gt;      None                           2652 (74.7)     1727 (79.1)              \n#&gt;      Localized (Yes)                 638 (18.0)      334 (15.3)              \n#&gt;      Metastatic                      261 ( 7.4)      123 ( 5.6)              \n#&gt;   Cardiovascular = 1 (%)             567 (16.0)      446 (20.4)   &lt;0.001     \n#&gt;   Congestive.HF = 1 (%)              596 (16.8)      425 (19.5)    0.011     \n#&gt;   Dementia = 1 (%)                   413 (11.6)      151 ( 6.9)   &lt;0.001     \n#&gt;   Psychiatric = 1 (%)                286 ( 8.1)      100 ( 4.6)   &lt;0.001     \n#&gt;   Pulmonary = 1 (%)                  774 (21.8)      315 (14.4)   &lt;0.001     \n#&gt;   Renal = 1 (%)                      149 ( 4.2)      106 ( 4.9)    0.268     \n#&gt;   Hepatic = 1 (%)                    265 ( 7.5)      136 ( 6.2)    0.084     \n#&gt;   GI.Bleed = 1 (%)                   131 ( 3.7)       54 ( 2.5)    0.014     \n#&gt;   Tumor = 1 (%)                      872 (24.6)      444 (20.3)   &lt;0.001     \n#&gt;   Transfer.hx = 1 (%)                335 ( 9.4)      327 (15.0)   &lt;0.001     \n#&gt;   MI = 1 (%)                         105 ( 3.0)       95 ( 4.3)    0.007     \n#&gt;   age (%)                                                         &lt;0.001     \n#&gt;      [-Inf,50)                       884 (24.9)      540 (24.7)              \n#&gt;      [50,60)                         546 (15.4)      371 (17.0)              \n#&gt;      [60,70)                         812 (22.9)      577 (26.4)              \n#&gt;      [70,80)                         809 (22.8)      529 (24.2)              \n#&gt;      [80, Inf)                       500 (14.1)      167 ( 7.6)              \n#&gt;   sex = Female (%)                  1637 (46.1)      906 (41.5)    0.001     \n#&gt;   edu (mean (SD))                  11.57 (3.13)    11.86 (3.16)    0.001     \n#&gt;   DASIndex (mean (SD))             20.37 (5.48)    20.70 (5.03)    0.023     \n#&gt;   APACHE.score (mean (SD))         50.93 (18.81)   60.74 (20.27)  &lt;0.001     \n#&gt;   Glasgow.Coma.Score (mean (SD))   22.25 (31.37)   18.97 (28.26)  &lt;0.001     \n#&gt;   blood.pressure (mean (SD))       84.87 (38.87)   68.20 (34.24)  &lt;0.001     \n#&gt;   WBC (mean (SD))                  15.26 (11.41)   16.27 (12.55)   0.002     \n#&gt;   Heart.rate (mean (SD))          112.87 (40.94)  118.93 (41.47)  &lt;0.001     \n#&gt;   Respiratory.rate (mean (SD))     28.98 (13.95)   26.65 (14.17)  &lt;0.001     \n#&gt;   Temperature (mean (SD))          37.63 (1.74)    37.59 (1.83)    0.429     \n#&gt;   PaO2vs.FIO2 (mean (SD))         240.63 (116.66) 192.43 (105.54) &lt;0.001     \n#&gt;   Albumin (mean (SD))               3.16 (0.67)     2.98 (0.93)   &lt;0.001     \n#&gt;   Hematocrit (mean (SD))           32.70 (8.79)    30.51 (7.42)   &lt;0.001     \n#&gt;   Bilirubin (mean (SD))             2.00 (4.43)     2.71 (5.33)   &lt;0.001     \n#&gt;   Creatinine (mean (SD))            1.92 (2.03)     2.47 (2.05)   &lt;0.001     \n#&gt;   Sodium (mean (SD))              137.04 (7.68)   136.33 (7.60)    0.001     \n#&gt;   Potassium (mean (SD))             4.08 (1.04)     4.05 (1.01)    0.321     \n#&gt;   PaCo2 (mean (SD))                39.95 (14.24)   36.79 (10.97)  &lt;0.001     \n#&gt;   PH (mean (SD))                    7.39 (0.11)     7.38 (0.11)   &lt;0.001     \n#&gt;   Weight (mean (SD))               65.04 (29.50)   72.36 (27.73)  &lt;0.001     \n#&gt;   DNR.status = Yes (%)               499 (14.1)      155 ( 7.1)   &lt;0.001     \n#&gt;   Medical.insurance (%)                                           &lt;0.001     \n#&gt;      Medicaid                        454 (12.8)      193 ( 8.8)              \n#&gt;      Medicare                        947 (26.7)      511 (23.4)              \n#&gt;      Medicare & Medicaid             251 ( 7.1)      123 ( 5.6)              \n#&gt;      No insurance                    186 ( 5.2)      136 ( 6.2)              \n#&gt;      Private                         967 (27.2)      731 (33.5)              \n#&gt;      Private & Medicare              746 (21.0)      490 (22.4)              \n#&gt;   Respiratory.Diag = Yes (%)        1481 (41.7)      632 (28.9)   &lt;0.001     \n#&gt;   Cardiovascular.Diag = Yes (%)     1007 (28.4)      924 (42.3)   &lt;0.001     \n#&gt;   Neurological.Diag = Yes (%)        575 (16.2)      118 ( 5.4)   &lt;0.001     \n#&gt;   Gastrointestinal.Diag = Yes (%)    522 (14.7)      420 (19.2)   &lt;0.001     \n#&gt;   Renal.Diag = Yes (%)               147 ( 4.1)      148 ( 6.8)   &lt;0.001     \n#&gt;   Metabolic.Diag = Yes (%)           172 ( 4.8)       93 ( 4.3)    0.337     \n#&gt;   Hematologic.Diag = Yes (%)         239 ( 6.7)      115 ( 5.3)    0.029     \n#&gt;   Sepsis.Diag = Yes (%)              515 (14.5)      516 (23.6)   &lt;0.001     \n#&gt;   Trauma.Diag = Yes (%)               18 ( 0.5)       34 ( 1.6)   &lt;0.001     \n#&gt;   Orthopedic.Diag = Yes (%)            3 ( 0.1)        4 ( 0.2)    0.516     \n#&gt;   race (%)                                                         0.425     \n#&gt;      white                          2753 (77.5)     1707 (78.2)              \n#&gt;      black                           585 (16.5)      335 (15.3)              \n#&gt;      other                           213 ( 6.0)      142 ( 6.5)              \n#&gt;   income (%)                                                      &lt;0.001     \n#&gt;      $11-$25k                        713 (20.1)      452 (20.7)              \n#&gt;      $25-$50k                        500 (14.1)      393 (18.0)              \n#&gt;      &gt; $50k                          257 ( 7.2)      194 ( 8.9)              \n#&gt;      Under $11k                     2081 (58.6)     1145 (52.4)              \n#&gt;   Length.of.Stay (mean (SD))       19.53 (23.59)   24.86 (28.90)  &lt;0.001     \n#&gt;   Death (mean (SD))                 0.63 (0.48)     0.68 (0.47)   &lt;0.001\n\ntable1\nThe table1 package is useful for generating descriptive summary tables commonly used in medical research. Below are examples of how to use it.\nBasic usage of table1:\n\nrequire(table1)\n#&gt; Loading required package: table1\n#&gt; \n#&gt; Attaching package: 'table1'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     units, units&lt;-\n\n\nObsData$RHC.use.factor &lt;- factor(ObsData$RHC.use, \n                          levels = c(0, 1), \n                          labels = c(\"No RHC\", \"Received RHC\"))\n# Generate a basic Table 1 summarizing characteristics of ObsData, grouped by 'RHC.use'\ntable1(~ age + sex + APACHE.score + Medical.insurance | RHC.use.factor, data = ObsData)\n\n\n\n\n\n\n\n\n\n\n\nNo RHC(N=3551)\nReceived RHC(N=2184)\nOverall(N=5735)\n\n\n\nage\n\n\n\n\n\n[-Inf,50)\n884 (24.9%)\n540 (24.7%)\n1424 (24.8%)\n\n\n[50,60)\n546 (15.4%)\n371 (17.0%)\n917 (16.0%)\n\n\n[60,70)\n812 (22.9%)\n577 (26.4%)\n1389 (24.2%)\n\n\n[70,80)\n809 (22.8%)\n529 (24.2%)\n1338 (23.3%)\n\n\n[80, Inf)\n500 (14.1%)\n167 (7.6%)\n667 (11.6%)\n\n\nsex\n\n\n\n\n\nMale\n1914 (53.9%)\n1278 (58.5%)\n3192 (55.7%)\n\n\nFemale\n1637 (46.1%)\n906 (41.5%)\n2543 (44.3%)\n\n\nAPACHE.score\n\n\n\n\n\nMean (SD)\n50.9 (18.8)\n60.7 (20.3)\n54.7 (20.0)\n\n\nMedian [Min, Max]\n50.0 [3.00, 147]\n60.0 [9.00, 135]\n54.0 [3.00, 147]\n\n\nMedical.insurance\n\n\n\n\n\nMedicaid\n454 (12.8%)\n193 (8.8%)\n647 (11.3%)\n\n\nMedicare\n947 (26.7%)\n511 (23.4%)\n1458 (25.4%)\n\n\nMedicare & Medicaid\n251 (7.1%)\n123 (5.6%)\n374 (6.5%)\n\n\nNo insurance\n186 (5.2%)\n136 (6.2%)\n322 (5.6%)\n\n\nPrivate\n967 (27.2%)\n731 (33.5%)\n1698 (29.6%)\n\n\nPrivate & Medicare\n746 (21.0%)\n490 (22.4%)\n1236 (21.6%)\n\n\n\n\n\n\nCustomizing labels and formats\n\n# Label variables and modify the format\nlabels &lt;- list(\n  age = \"Age (years)\",\n  APACHE.score = \"APACHE II Score\",\n  Medical.insurance = \"Medical Insurance Status\"\n)\n\ntable1(~ age + sex + APACHE.score + Medical.insurance | RHC.use.factor, data = ObsData,\n       label = labels, caption = \"Table 1: Summary of patient characteristics\")\n\n\n\nTable 1: Summary of patient characteristics\n\n\n\n\n\n\n\n\nNo RHC(N=3551)\nReceived RHC(N=2184)\nOverall(N=5735)\n\n\n\nage\n\n\n\n\n\n[-Inf,50)\n884 (24.9%)\n540 (24.7%)\n1424 (24.8%)\n\n\n[50,60)\n546 (15.4%)\n371 (17.0%)\n917 (16.0%)\n\n\n[60,70)\n812 (22.9%)\n577 (26.4%)\n1389 (24.2%)\n\n\n[70,80)\n809 (22.8%)\n529 (24.2%)\n1338 (23.3%)\n\n\n[80, Inf)\n500 (14.1%)\n167 (7.6%)\n667 (11.6%)\n\n\nsex\n\n\n\n\n\nMale\n1914 (53.9%)\n1278 (58.5%)\n3192 (55.7%)\n\n\nFemale\n1637 (46.1%)\n906 (41.5%)\n2543 (44.3%)\n\n\nAPACHE.score\n\n\n\n\n\nMean (SD)\n50.9 (18.8)\n60.7 (20.3)\n54.7 (20.0)\n\n\nMedian [Min, Max]\n50.0 [3.00, 147]\n60.0 [9.00, 135]\n54.0 [3.00, 147]\n\n\nMedical.insurance\n\n\n\n\n\nMedicaid\n454 (12.8%)\n193 (8.8%)\n647 (11.3%)\n\n\nMedicare\n947 (26.7%)\n511 (23.4%)\n1458 (25.4%)\n\n\nMedicare & Medicaid\n251 (7.1%)\n123 (5.6%)\n374 (6.5%)\n\n\nNo insurance\n186 (5.2%)\n136 (6.2%)\n322 (5.6%)\n\n\nPrivate\n967 (27.2%)\n731 (33.5%)\n1698 (29.6%)\n\n\nPrivate & Medicare\n746 (21.0%)\n490 (22.4%)\n1236 (21.6%)\n\n\n\n\n\n\nHandling missing data\n\n# Including missing values in the summary table\ntable1(~ age + sex + APACHE.score + Medical.insurance | RHC.use.factor, data = ObsData,\n       overall = TRUE, render.missing = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nNo RHC(N=3551)\nReceived RHC(N=2184)\nTRUE(N=5735)\n\n\n\nage\n\n\n\n\n\n[-Inf,50)\n884 (24.9%)\n540 (24.7%)\n1424 (24.8%)\n\n\n[50,60)\n546 (15.4%)\n371 (17.0%)\n917 (16.0%)\n\n\n[60,70)\n812 (22.9%)\n577 (26.4%)\n1389 (24.2%)\n\n\n[70,80)\n809 (22.8%)\n529 (24.2%)\n1338 (23.3%)\n\n\n[80, Inf)\n500 (14.1%)\n167 (7.6%)\n667 (11.6%)\n\n\nsex\n\n\n\n\n\nMale\n1914 (53.9%)\n1278 (58.5%)\n3192 (55.7%)\n\n\nFemale\n1637 (46.1%)\n906 (41.5%)\n2543 (44.3%)\n\n\nAPACHE.score\n\n\n\n\n\nMean (SD)\n50.9 (18.8)\n60.7 (20.3)\n54.7 (20.0)\n\n\nMedian [Min, Max]\n50.0 [3.00, 147]\n60.0 [9.00, 135]\n54.0 [3.00, 147]\n\n\nMedical.insurance\n\n\n\n\n\nMedicaid\n454 (12.8%)\n193 (8.8%)\n647 (11.3%)\n\n\nMedicare\n947 (26.7%)\n511 (23.4%)\n1458 (25.4%)\n\n\nMedicare & Medicaid\n251 (7.1%)\n123 (5.6%)\n374 (6.5%)\n\n\nNo insurance\n186 (5.2%)\n136 (6.2%)\n322 (5.6%)\n\n\nPrivate\n967 (27.2%)\n731 (33.5%)\n1698 (29.6%)\n\n\nPrivate & Medicare\n746 (21.0%)\n490 (22.4%)\n1236 (21.6%)\n\n\n\n\n\nObsData$RHC.use.factor &lt;- NULL\n\ngtsummary\ngtsummary provides highly customizable functions to construct tables. It allows renaming variables, adding captions, and selecting specific measures for variable types. It is particularly useful for creating clean, customized Table 1s and includes options for survey data.\n\nrequire(gt)\n#&gt; Loading required package: gt\nrequire(gtsummary)\n#&gt; Loading required package: gtsummary\n\n\ntbl_summary(ObsData)\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 5,7351\n\n\n\n\nDisease.category\n\n\n\n    ARF\n2,490 (43%)\n\n\n    CHF\n456 (8.0%)\n\n\n    Other\n1,163 (20%)\n\n\n    MOSF\n1,626 (28%)\n\n\nCancer\n\n\n\n    None\n4,379 (76%)\n\n\n    Localized (Yes)\n972 (17%)\n\n\n    Metastatic\n384 (6.7%)\n\n\nCardiovascular\n\n\n\n    0\n4,722 (82%)\n\n\n    1\n1,013 (18%)\n\n\nCongestive.HF\n\n\n\n    0\n4,714 (82%)\n\n\n    1\n1,021 (18%)\n\n\nDementia\n\n\n\n    0\n5,171 (90%)\n\n\n    1\n564 (9.8%)\n\n\nPsychiatric\n\n\n\n    0\n5,349 (93%)\n\n\n    1\n386 (6.7%)\n\n\nPulmonary\n\n\n\n    0\n4,646 (81%)\n\n\n    1\n1,089 (19%)\n\n\nRenal\n\n\n\n    0\n5,480 (96%)\n\n\n    1\n255 (4.4%)\n\n\nHepatic\n\n\n\n    0\n5,334 (93%)\n\n\n    1\n401 (7.0%)\n\n\nGI.Bleed\n\n\n\n    0\n5,550 (97%)\n\n\n    1\n185 (3.2%)\n\n\nTumor\n\n\n\n    0\n4,419 (77%)\n\n\n    1\n1,316 (23%)\n\n\nImmunosupperssion\n\n\n\n    0\n4,192 (73%)\n\n\n    1\n1,543 (27%)\n\n\nTransfer.hx\n\n\n\n    0\n5,073 (88%)\n\n\n    1\n662 (12%)\n\n\nMI\n\n\n\n    0\n5,535 (97%)\n\n\n    1\n200 (3.5%)\n\n\nage\n\n\n\n    [-Inf,50)\n1,424 (25%)\n\n\n    [50,60)\n917 (16%)\n\n\n    [60,70)\n1,389 (24%)\n\n\n    [70,80)\n1,338 (23%)\n\n\n    [80, Inf)\n667 (12%)\n\n\nsex\n\n\n\n    Male\n3,192 (56%)\n\n\n    Female\n2,543 (44%)\n\n\nedu\n12.0 (10.0, 13.0)\n\n\nDASIndex\n19.7 (16.1, 23.4)\n\n\nAPACHE.score\n54 (41, 67)\n\n\nGlasgow.Coma.Score\n0 (0, 41)\n\n\nblood.pressure\n63 (50, 115)\n\n\nWBC\n14 (8, 20)\n\n\nHeart.rate\n124 (97, 141)\n\n\nRespiratory.rate\n30 (14, 38)\n\n\nTemperature\n38.09 (36.09, 39.00)\n\n\nPaO2vs.FIO2\n203 (133, 317)\n\n\nAlbumin\n3.50 (2.60, 3.50)\n\n\nHematocrit\n30 (26, 36)\n\n\nBilirubin\n1.01 (0.80, 1.40)\n\n\nCreatinine\n1.50 (1.00, 2.40)\n\n\nSodium\n136 (132, 142)\n\n\nPotassium\n3.80 (3.40, 4.60)\n\n\nPaCo2\n37 (31, 42)\n\n\nPH\n7.40 (7.34, 7.46)\n\n\nWeight\n70 (56, 84)\n\n\nDNR.status\n654 (11%)\n\n\nMedical.insurance\n\n\n\n    Medicaid\n647 (11%)\n\n\n    Medicare\n1,458 (25%)\n\n\n    Medicare & Medicaid\n374 (6.5%)\n\n\n    No insurance\n322 (5.6%)\n\n\n    Private\n1,698 (30%)\n\n\n    Private & Medicare\n1,236 (22%)\n\n\nRespiratory.Diag\n2,113 (37%)\n\n\nCardiovascular.Diag\n1,931 (34%)\n\n\nNeurological.Diag\n693 (12%)\n\n\nGastrointestinal.Diag\n942 (16%)\n\n\nRenal.Diag\n295 (5.1%)\n\n\nMetabolic.Diag\n265 (4.6%)\n\n\nHematologic.Diag\n354 (6.2%)\n\n\nSepsis.Diag\n1,031 (18%)\n\n\nTrauma.Diag\n52 (0.9%)\n\n\nOrthopedic.Diag\n7 (0.1%)\n\n\nrace\n\n\n\n    white\n4,460 (78%)\n\n\n    black\n920 (16%)\n\n\n    other\n355 (6.2%)\n\n\nincome\n\n\n\n    $11-$25k\n1,165 (20%)\n\n\n    $25-$50k\n893 (16%)\n\n\n    &gt; $50k\n451 (7.9%)\n\n\n    Under $11k\n3,226 (56%)\n\n\nLength.of.Stay\n14 (7, 25)\n\n\nDeath\n3,722 (65%)\n\n\nRHC.use\n2,184 (38%)\n\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n\n\n\n\ntbl_summary(ObsData, by = RHC.use,\n            statistic = list(\n                              all_continuous() ~ \"{mean} ({sd})\",\n                              all_categorical() ~ \"{n} ({p}%)\"\n                            ),\n            digits = all_continuous() ~ 2) |&gt; \n  as_gt() |&gt;\n  gt::tab_source_note(gt::md(\"*Add note here.*\"))\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n0\nN = 3,5511\n\n\n1\nN = 2,1841\n\n\n\n\nDisease.category\n\n\n\n\n    ARF\n1,581 (45%)\n909 (42%)\n\n\n    CHF\n247 (7.0%)\n209 (9.6%)\n\n\n    Other\n955 (27%)\n208 (9.5%)\n\n\n    MOSF\n768 (22%)\n858 (39%)\n\n\nCancer\n\n\n\n\n    None\n2,652 (75%)\n1,727 (79%)\n\n\n    Localized (Yes)\n638 (18%)\n334 (15%)\n\n\n    Metastatic\n261 (7.4%)\n123 (5.6%)\n\n\nCardiovascular\n\n\n\n\n    0\n2,984 (84%)\n1,738 (80%)\n\n\n    1\n567 (16%)\n446 (20%)\n\n\nCongestive.HF\n\n\n\n\n    0\n2,955 (83%)\n1,759 (81%)\n\n\n    1\n596 (17%)\n425 (19%)\n\n\nDementia\n\n\n\n\n    0\n3,138 (88%)\n2,033 (93%)\n\n\n    1\n413 (12%)\n151 (6.9%)\n\n\nPsychiatric\n\n\n\n\n    0\n3,265 (92%)\n2,084 (95%)\n\n\n    1\n286 (8.1%)\n100 (4.6%)\n\n\nPulmonary\n\n\n\n\n    0\n2,777 (78%)\n1,869 (86%)\n\n\n    1\n774 (22%)\n315 (14%)\n\n\nRenal\n\n\n\n\n    0\n3,402 (96%)\n2,078 (95%)\n\n\n    1\n149 (4.2%)\n106 (4.9%)\n\n\nHepatic\n\n\n\n\n    0\n3,286 (93%)\n2,048 (94%)\n\n\n    1\n265 (7.5%)\n136 (6.2%)\n\n\nGI.Bleed\n\n\n\n\n    0\n3,420 (96%)\n2,130 (98%)\n\n\n    1\n131 (3.7%)\n54 (2.5%)\n\n\nTumor\n\n\n\n\n    0\n2,679 (75%)\n1,740 (80%)\n\n\n    1\n872 (25%)\n444 (20%)\n\n\nImmunosupperssion\n\n\n\n\n    0\n2,644 (74%)\n1,548 (71%)\n\n\n    1\n907 (26%)\n636 (29%)\n\n\nTransfer.hx\n\n\n\n\n    0\n3,216 (91%)\n1,857 (85%)\n\n\n    1\n335 (9.4%)\n327 (15%)\n\n\nMI\n\n\n\n\n    0\n3,446 (97%)\n2,089 (96%)\n\n\n    1\n105 (3.0%)\n95 (4.3%)\n\n\nage\n\n\n\n\n    [-Inf,50)\n884 (25%)\n540 (25%)\n\n\n    [50,60)\n546 (15%)\n371 (17%)\n\n\n    [60,70)\n812 (23%)\n577 (26%)\n\n\n    [70,80)\n809 (23%)\n529 (24%)\n\n\n    [80, Inf)\n500 (14%)\n167 (7.6%)\n\n\nsex\n\n\n\n\n    Male\n1,914 (54%)\n1,278 (59%)\n\n\n    Female\n1,637 (46%)\n906 (41%)\n\n\nedu\n11.57 (3.13)\n11.86 (3.16)\n\n\nDASIndex\n20.37 (5.48)\n20.70 (5.03)\n\n\nAPACHE.score\n50.93 (18.81)\n60.74 (20.27)\n\n\nGlasgow.Coma.Score\n22.25 (31.37)\n18.97 (28.26)\n\n\nblood.pressure\n84.87 (38.87)\n68.20 (34.24)\n\n\nWBC\n15.26 (11.41)\n16.27 (12.55)\n\n\nHeart.rate\n112.87 (40.94)\n118.93 (41.47)\n\n\nRespiratory.rate\n28.98 (13.95)\n26.65 (14.17)\n\n\nTemperature\n37.63 (1.74)\n37.59 (1.83)\n\n\nPaO2vs.FIO2\n240.63 (116.66)\n192.43 (105.54)\n\n\nAlbumin\n3.16 (0.67)\n2.98 (0.93)\n\n\nHematocrit\n32.70 (8.79)\n30.51 (7.42)\n\n\nBilirubin\n2.00 (4.43)\n2.71 (5.33)\n\n\nCreatinine\n1.92 (2.03)\n2.47 (2.05)\n\n\nSodium\n137.04 (7.68)\n136.33 (7.60)\n\n\nPotassium\n4.08 (1.04)\n4.05 (1.01)\n\n\nPaCo2\n39.95 (14.24)\n36.79 (10.97)\n\n\nPH\n7.39 (0.11)\n7.38 (0.11)\n\n\nWeight\n65.04 (29.50)\n72.36 (27.73)\n\n\nDNR.status\n499 (14%)\n155 (7.1%)\n\n\nMedical.insurance\n\n\n\n\n    Medicaid\n454 (13%)\n193 (8.8%)\n\n\n    Medicare\n947 (27%)\n511 (23%)\n\n\n    Medicare & Medicaid\n251 (7.1%)\n123 (5.6%)\n\n\n    No insurance\n186 (5.2%)\n136 (6.2%)\n\n\n    Private\n967 (27%)\n731 (33%)\n\n\n    Private & Medicare\n746 (21%)\n490 (22%)\n\n\nRespiratory.Diag\n1,481 (42%)\n632 (29%)\n\n\nCardiovascular.Diag\n1,007 (28%)\n924 (42%)\n\n\nNeurological.Diag\n575 (16%)\n118 (5.4%)\n\n\nGastrointestinal.Diag\n522 (15%)\n420 (19%)\n\n\nRenal.Diag\n147 (4.1%)\n148 (6.8%)\n\n\nMetabolic.Diag\n172 (4.8%)\n93 (4.3%)\n\n\nHematologic.Diag\n239 (6.7%)\n115 (5.3%)\n\n\nSepsis.Diag\n515 (15%)\n516 (24%)\n\n\nTrauma.Diag\n18 (0.5%)\n34 (1.6%)\n\n\nOrthopedic.Diag\n3 (&lt;0.1%)\n4 (0.2%)\n\n\nrace\n\n\n\n\n    white\n2,753 (78%)\n1,707 (78%)\n\n\n    black\n585 (16%)\n335 (15%)\n\n\n    other\n213 (6.0%)\n142 (6.5%)\n\n\nincome\n\n\n\n\n    $11-$25k\n713 (20%)\n452 (21%)\n\n\n    $25-$50k\n500 (14%)\n393 (18%)\n\n\n    &gt; $50k\n257 (7.2%)\n194 (8.9%)\n\n\n    Under $11k\n2,081 (59%)\n1,145 (52%)\n\n\nLength.of.Stay\n19.53 (23.59)\n24.86 (28.90)\n\n\nDeath\n2,236 (63%)\n1,486 (68%)\n\n\n\n\n\n1 n (%); Mean (SD)\n\n\nAdd note here.\n\n\n\n\n\n\nDataExplorer\nDataExplorer offers functions for initial data exploration, including various visualizations. Below are some examples using the RHC dataset.\n\nrequire(DataExplorer)\n#&gt; Loading required package: DataExplorer\n\nThe introduce function provides an overview of the dataset dimensions, variable types, and missingness.\n\nintroduce(ObsData)\n\n\n  \n\n\n\nPlot the amount of missing data per variable:\n\nplot_missing(ObsData)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nVisualize categorical variable distributions with the plot_bar function:\n\nplot_bar(ObsData)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the distribution of numerical variables with histograms:\n\nplot_histogram(ObsData)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile-quantile plots can be used to assess whether numerical variables are normally distributed:\n\nplot_qq(ObsData)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate a correlation plot to show relationships between variables:\n\nplot_correlation(ObsData)\n#&gt; Registered S3 method overwritten by 'plyr':\n#&gt;   method    from  \n#&gt;   [.indexed table1\n\n\n\n\n\n\n\nBoxplots can visualize variable distributions based on treatment or outcome:\n\nplot_boxplot(ObsData, by=\"RHC.use\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate a full PDF report with the create_report function:\n\ncreate_report(ObsData)\n\nGGally\nGGally provides methods to combine multiple ggplot2 plots, enabling visualization of several variables at once.\n\nrequire(GGally)\n#&gt; Loading required package: GGally\n#&gt; Loading required package: ggplot2\n\n\nggpairs(ObsData, \n        columns = c('age', 'sex', 'edu', 'blood.pressure', 'Medical.insurance'),\n        ggplot2::aes(color=as.factor(RHC.use)))\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\nmodelsummary\nmodelsummary provides functions to visualize data, including summaries, correlations, and Table 1-style tables.\n\nrequire(modelsummary)\n#&gt; Loading required package: modelsummary\nrequire(rmarkdown)\n#&gt; Loading required package: rmarkdown\nrequire(markdown)\n#&gt; Loading required package: markdown\n\nOverview of each variable:\n\ndatasummary_skim(ObsData)\n\n\n\n    \n\n\n      \n\n \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n\n\nedu\n                  42\n                  0\n                  11.7\n                  3.1\n                  0.0\n                  12.0\n                  30.0\n                  \n                \n\nDASIndex\n                  1023\n                  0\n                  20.5\n                  5.3\n                  11.0\n                  19.7\n                  33.0\n                  \n                \n\nAPACHE.score\n                  123\n                  0\n                  54.7\n                  20.0\n                  3.0\n                  54.0\n                  147.0\n                  \n                \n\nGlasgow.Coma.Score\n                  11\n                  0\n                  21.0\n                  30.3\n                  0.0\n                  0.0\n                  100.0\n                  \n                \n\nblood.pressure\n                  178\n                  0\n                  78.5\n                  38.0\n                  0.0\n                  63.0\n                  259.0\n                  \n                \n\nWBC\n                  520\n                  0\n                  15.6\n                  11.9\n                  0.0\n                  14.1\n                  192.0\n                  \n                \n\nHeart.rate\n                  189\n                  0\n                  115.2\n                  41.2\n                  0.0\n                  124.0\n                  250.0\n                  \n                \n\nRespiratory.rate\n                  72\n                  0\n                  28.1\n                  14.1\n                  0.0\n                  30.0\n                  100.0\n                  \n                \n\nTemperature\n                  118\n                  0\n                  37.6\n                  1.8\n                  27.0\n                  38.1\n                  43.0\n                  \n                \n\nPaO2vs.FIO2\n                  1342\n                  0\n                  222.3\n                  115.0\n                  11.6\n                  202.5\n                  937.5\n                  \n                \n\nAlbumin\n                  57\n                  0\n                  3.1\n                  0.8\n                  0.3\n                  3.5\n                  29.0\n                  \n                \n\nHematocrit\n                  450\n                  0\n                  31.9\n                  8.4\n                  2.0\n                  30.0\n                  66.2\n                  \n                \n\nBilirubin\n                  266\n                  0\n                  2.3\n                  4.8\n                  0.1\n                  1.0\n                  58.2\n                  \n                \n\nCreatinine\n                  148\n                  0\n                  2.1\n                  2.1\n                  0.1\n                  1.5\n                  25.1\n                  \n                \n\nSodium\n                  73\n                  0\n                  136.8\n                  7.7\n                  101.0\n                  136.0\n                  178.0\n                  \n                \n\nPotassium\n                  81\n                  0\n                  4.1\n                  1.0\n                  1.1\n                  3.8\n                  11.9\n                  \n                \n\nPaCo2\n                  266\n                  0\n                  38.7\n                  13.2\n                  1.0\n                  37.0\n                  156.0\n                  \n                \n\nPH\n                  96\n                  0\n                  7.4\n                  0.1\n                  6.6\n                  7.4\n                  7.8\n                  \n                \n\nWeight\n                  922\n                  0\n                  67.8\n                  29.1\n                  0.0\n                  70.0\n                  244.0\n                  \n                \n\nLength.of.Stay\n                  164\n                  0\n                  21.6\n                  25.9\n                  2.0\n                  14.0\n                  394.0\n                  \n                \n\nDeath\n                  2\n                  0\n                  0.6\n                  0.5\n                  0.0\n                  1.0\n                  1.0\n                  \n                \n\nRHC.use\n                  2\n                  0\n                  0.4\n                  0.5\n                  0.0\n                  0.0\n                  1.0\n                  \n                \n\n\n                  \n                  N\n                  %\n                   \n                   \n                   \n                   \n                   \n                \n\nDisease.category\n                  ARF\n                  2490\n                  43.4\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  CHF\n                  456\n                  8.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Other\n                  1163\n                  20.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  MOSF\n                  1626\n                  28.4\n                   \n                   \n                   \n                   \n                   \n                \n\nCancer\n                  None\n                  4379\n                  76.4\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Localized (Yes)\n                  972\n                  16.9\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Metastatic\n                  384\n                  6.7\n                   \n                   \n                   \n                   \n                   \n                \n\nCardiovascular\n                  0\n                  4722\n                  82.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  1013\n                  17.7\n                   \n                   \n                   \n                   \n                   \n                \n\nCongestive.HF\n                  0\n                  4714\n                  82.2\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  1021\n                  17.8\n                   \n                   \n                   \n                   \n                   \n                \n\nDementia\n                  0\n                  5171\n                  90.2\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  564\n                  9.8\n                   \n                   \n                   \n                   \n                   \n                \n\nPsychiatric\n                  0\n                  5349\n                  93.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  386\n                  6.7\n                   \n                   \n                   \n                   \n                   \n                \n\nPulmonary\n                  0\n                  4646\n                  81.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  1089\n                  19.0\n                   \n                   \n                   \n                   \n                   \n                \n\nRenal\n                  0\n                  5480\n                  95.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  255\n                  4.4\n                   \n                   \n                   \n                   \n                   \n                \n\nHepatic\n                  0\n                  5334\n                  93.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  401\n                  7.0\n                   \n                   \n                   \n                   \n                   \n                \n\nGI.Bleed\n                  0\n                  5550\n                  96.8\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  185\n                  3.2\n                   \n                   \n                   \n                   \n                   \n                \n\nTumor\n                  0\n                  4419\n                  77.1\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  1316\n                  22.9\n                   \n                   \n                   \n                   \n                   \n                \n\nImmunosupperssion\n                  0\n                  4192\n                  73.1\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  1543\n                  26.9\n                   \n                   \n                   \n                   \n                   \n                \n\nTransfer.hx\n                  0\n                  5073\n                  88.5\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  662\n                  11.5\n                   \n                   \n                   \n                   \n                   \n                \n\nMI\n                  0\n                  5535\n                  96.5\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  1\n                  200\n                  3.5\n                   \n                   \n                   \n                   \n                   \n                \n\nage\n                  [-Inf,50)\n                  1424\n                  24.8\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  [50,60)\n                  917\n                  16.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  [60,70)\n                  1389\n                  24.2\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  [70,80)\n                  1338\n                  23.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  [80, Inf)\n                  667\n                  11.6\n                   \n                   \n                   \n                   \n                   \n                \n\nsex\n                  Male\n                  3192\n                  55.7\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Female\n                  2543\n                  44.3\n                   \n                   \n                   \n                   \n                   \n                \n\nDNR.status\n                  No\n                  5081\n                  88.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  654\n                  11.4\n                   \n                   \n                   \n                   \n                   \n                \n\nMedical.insurance\n                  Medicaid\n                  647\n                  11.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Medicare\n                  1458\n                  25.4\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Medicare & Medicaid\n                  374\n                  6.5\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  No insurance\n                  322\n                  5.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Private\n                  1698\n                  29.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Private & Medicare\n                  1236\n                  21.6\n                   \n                   \n                   \n                   \n                   \n                \n\nRespiratory.Diag\n                  No\n                  3622\n                  63.2\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  2113\n                  36.8\n                   \n                   \n                   \n                   \n                   \n                \n\nCardiovascular.Diag\n                  No\n                  3804\n                  66.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  1931\n                  33.7\n                   \n                   \n                   \n                   \n                   \n                \n\nNeurological.Diag\n                  No\n                  5042\n                  87.9\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  693\n                  12.1\n                   \n                   \n                   \n                   \n                   \n                \n\nGastrointestinal.Diag\n                  No\n                  4793\n                  83.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  942\n                  16.4\n                   \n                   \n                   \n                   \n                   \n                \n\nRenal.Diag\n                  No\n                  5440\n                  94.9\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  295\n                  5.1\n                   \n                   \n                   \n                   \n                   \n                \n\nMetabolic.Diag\n                  No\n                  5470\n                  95.4\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  265\n                  4.6\n                   \n                   \n                   \n                   \n                   \n                \n\nHematologic.Diag\n                  No\n                  5381\n                  93.8\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  354\n                  6.2\n                   \n                   \n                   \n                   \n                   \n                \n\nSepsis.Diag\n                  No\n                  4704\n                  82.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  1031\n                  18.0\n                   \n                   \n                   \n                   \n                   \n                \n\nTrauma.Diag\n                  No\n                  5683\n                  99.1\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  52\n                  0.9\n                   \n                   \n                   \n                   \n                   \n                \n\nOrthopedic.Diag\n                  No\n                  5728\n                  99.9\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Yes\n                  7\n                  0.1\n                   \n                   \n                   \n                   \n                   \n                \n\nrace\n                  white\n                  4460\n                  77.8\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  black\n                  920\n                  16.0\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  other\n                  355\n                  6.2\n                   \n                   \n                   \n                   \n                   \n                \n\nincome\n                  $11-$25k\n                  1165\n                  20.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  $25-$50k\n                  893\n                  15.6\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  &gt; $50k\n                  451\n                  7.9\n                   \n                   \n                   \n                   \n                   \n                \n\n\n                  Under $11k\n                  3226\n                  56.3\n                   \n                   \n                   \n                   \n                   \n                \n\n\n\n\n\n\nCreate a Table 1 using datasummary_balance:\n\ndatasummary_balance(~ RHC.use, ObsData)\n\n\n\n    \n\n\n      \n\n\n \n \n0\n1\n\n\n \n                  \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n              \n\n\n\nedu\n                  \n                  11.6\n                  3.1\n                  11.9\n                  3.2\n                \n\nDASIndex\n                  \n                  20.4\n                  5.5\n                  20.7\n                  5.0\n                \n\nAPACHE.score\n                  \n                  50.9\n                  18.8\n                  60.7\n                  20.3\n                \n\nGlasgow.Coma.Score\n                  \n                  22.3\n                  31.4\n                  19.0\n                  28.3\n                \n\nblood.pressure\n                  \n                  84.9\n                  38.9\n                  68.2\n                  34.2\n                \n\nWBC\n                  \n                  15.3\n                  11.4\n                  16.3\n                  12.5\n                \n\nHeart.rate\n                  \n                  112.9\n                  40.9\n                  118.9\n                  41.5\n                \n\nRespiratory.rate\n                  \n                  29.0\n                  13.9\n                  26.7\n                  14.2\n                \n\nTemperature\n                  \n                  37.6\n                  1.7\n                  37.6\n                  1.8\n                \n\nPaO2vs.FIO2\n                  \n                  240.6\n                  116.7\n                  192.4\n                  105.5\n                \n\nAlbumin\n                  \n                  3.2\n                  0.7\n                  3.0\n                  0.9\n                \n\nHematocrit\n                  \n                  32.7\n                  8.8\n                  30.5\n                  7.4\n                \n\nBilirubin\n                  \n                  2.0\n                  4.4\n                  2.7\n                  5.3\n                \n\nCreatinine\n                  \n                  1.9\n                  2.0\n                  2.5\n                  2.1\n                \n\nSodium\n                  \n                  137.0\n                  7.7\n                  136.3\n                  7.6\n                \n\nPotassium\n                  \n                  4.1\n                  1.0\n                  4.0\n                  1.0\n                \n\nPaCo2\n                  \n                  40.0\n                  14.2\n                  36.8\n                  11.0\n                \n\nPH\n                  \n                  7.4\n                  0.1\n                  7.4\n                  0.1\n                \n\nWeight\n                  \n                  65.0\n                  29.5\n                  72.4\n                  27.7\n                \n\nLength.of.Stay\n                  \n                  19.5\n                  23.6\n                  24.9\n                  28.9\n                \n\nDeath\n                  \n                  0.6\n                  0.5\n                  0.7\n                  0.5\n                \n\n\n                  \n                  N\n                  Pct.\n                  N\n                  Pct.\n                \n\nDisease.category\n                  ARF\n                  1581\n                  44.5\n                  909\n                  41.6\n                \n\n\n                  CHF\n                  247\n                  7.0\n                  209\n                  9.6\n                \n\n\n                  Other\n                  955\n                  26.9\n                  208\n                  9.5\n                \n\n\n                  MOSF\n                  768\n                  21.6\n                  858\n                  39.3\n                \n\nCancer\n                  None\n                  2652\n                  74.7\n                  1727\n                  79.1\n                \n\n\n                  Localized (Yes)\n                  638\n                  18.0\n                  334\n                  15.3\n                \n\n\n                  Metastatic\n                  261\n                  7.4\n                  123\n                  5.6\n                \n\nCardiovascular\n                  0\n                  2984\n                  84.0\n                  1738\n                  79.6\n                \n\n\n                  1\n                  567\n                  16.0\n                  446\n                  20.4\n                \n\nCongestive.HF\n                  0\n                  2955\n                  83.2\n                  1759\n                  80.5\n                \n\n\n                  1\n                  596\n                  16.8\n                  425\n                  19.5\n                \n\nDementia\n                  0\n                  3138\n                  88.4\n                  2033\n                  93.1\n                \n\n\n                  1\n                  413\n                  11.6\n                  151\n                  6.9\n                \n\nPsychiatric\n                  0\n                  3265\n                  91.9\n                  2084\n                  95.4\n                \n\n\n                  1\n                  286\n                  8.1\n                  100\n                  4.6\n                \n\nPulmonary\n                  0\n                  2777\n                  78.2\n                  1869\n                  85.6\n                \n\n\n                  1\n                  774\n                  21.8\n                  315\n                  14.4\n                \n\nRenal\n                  0\n                  3402\n                  95.8\n                  2078\n                  95.1\n                \n\n\n                  1\n                  149\n                  4.2\n                  106\n                  4.9\n                \n\nHepatic\n                  0\n                  3286\n                  92.5\n                  2048\n                  93.8\n                \n\n\n                  1\n                  265\n                  7.5\n                  136\n                  6.2\n                \n\nGI.Bleed\n                  0\n                  3420\n                  96.3\n                  2130\n                  97.5\n                \n\n\n                  1\n                  131\n                  3.7\n                  54\n                  2.5\n                \n\nTumor\n                  0\n                  2679\n                  75.4\n                  1740\n                  79.7\n                \n\n\n                  1\n                  872\n                  24.6\n                  444\n                  20.3\n                \n\nImmunosupperssion\n                  0\n                  2644\n                  74.5\n                  1548\n                  70.9\n                \n\n\n                  1\n                  907\n                  25.5\n                  636\n                  29.1\n                \n\nTransfer.hx\n                  0\n                  3216\n                  90.6\n                  1857\n                  85.0\n                \n\n\n                  1\n                  335\n                  9.4\n                  327\n                  15.0\n                \n\nMI\n                  0\n                  3446\n                  97.0\n                  2089\n                  95.7\n                \n\n\n                  1\n                  105\n                  3.0\n                  95\n                  4.3\n                \n\nage\n                  [-Inf,50)\n                  884\n                  24.9\n                  540\n                  24.7\n                \n\n\n                  [50,60)\n                  546\n                  15.4\n                  371\n                  17.0\n                \n\n\n                  [60,70)\n                  812\n                  22.9\n                  577\n                  26.4\n                \n\n\n                  [70,80)\n                  809\n                  22.8\n                  529\n                  24.2\n                \n\n\n                  [80, Inf)\n                  500\n                  14.1\n                  167\n                  7.6\n                \n\nsex\n                  Male\n                  1914\n                  53.9\n                  1278\n                  58.5\n                \n\n\n                  Female\n                  1637\n                  46.1\n                  906\n                  41.5\n                \n\nDNR.status\n                  No\n                  3052\n                  85.9\n                  2029\n                  92.9\n                \n\n\n                  Yes\n                  499\n                  14.1\n                  155\n                  7.1\n                \n\nMedical.insurance\n                  Medicaid\n                  454\n                  12.8\n                  193\n                  8.8\n                \n\n\n                  Medicare\n                  947\n                  26.7\n                  511\n                  23.4\n                \n\n\n                  Medicare & Medicaid\n                  251\n                  7.1\n                  123\n                  5.6\n                \n\n\n                  No insurance\n                  186\n                  5.2\n                  136\n                  6.2\n                \n\n\n                  Private\n                  967\n                  27.2\n                  731\n                  33.5\n                \n\n\n                  Private & Medicare\n                  746\n                  21.0\n                  490\n                  22.4\n                \n\nRespiratory.Diag\n                  No\n                  2070\n                  58.3\n                  1552\n                  71.1\n                \n\n\n                  Yes\n                  1481\n                  41.7\n                  632\n                  28.9\n                \n\nCardiovascular.Diag\n                  No\n                  2544\n                  71.6\n                  1260\n                  57.7\n                \n\n\n                  Yes\n                  1007\n                  28.4\n                  924\n                  42.3\n                \n\nNeurological.Diag\n                  No\n                  2976\n                  83.8\n                  2066\n                  94.6\n                \n\n\n                  Yes\n                  575\n                  16.2\n                  118\n                  5.4\n                \n\nGastrointestinal.Diag\n                  No\n                  3029\n                  85.3\n                  1764\n                  80.8\n                \n\n\n                  Yes\n                  522\n                  14.7\n                  420\n                  19.2\n                \n\nRenal.Diag\n                  No\n                  3404\n                  95.9\n                  2036\n                  93.2\n                \n\n\n                  Yes\n                  147\n                  4.1\n                  148\n                  6.8\n                \n\nMetabolic.Diag\n                  No\n                  3379\n                  95.2\n                  2091\n                  95.7\n                \n\n\n                  Yes\n                  172\n                  4.8\n                  93\n                  4.3\n                \n\nHematologic.Diag\n                  No\n                  3312\n                  93.3\n                  2069\n                  94.7\n                \n\n\n                  Yes\n                  239\n                  6.7\n                  115\n                  5.3\n                \n\nSepsis.Diag\n                  No\n                  3036\n                  85.5\n                  1668\n                  76.4\n                \n\n\n                  Yes\n                  515\n                  14.5\n                  516\n                  23.6\n                \n\nTrauma.Diag\n                  No\n                  3533\n                  99.5\n                  2150\n                  98.4\n                \n\n\n                  Yes\n                  18\n                  0.5\n                  34\n                  1.6\n                \n\nOrthopedic.Diag\n                  No\n                  3548\n                  99.9\n                  2180\n                  99.8\n                \n\n\n                  Yes\n                  3\n                  0.1\n                  4\n                  0.2\n                \n\nrace\n                  white\n                  2753\n                  77.5\n                  1707\n                  78.2\n                \n\n\n                  black\n                  585\n                  16.5\n                  335\n                  15.3\n                \n\n\n                  other\n                  213\n                  6.0\n                  142\n                  6.5\n                \n\nincome\n                  $11-$25k\n                  713\n                  20.1\n                  452\n                  20.7\n                \n\n\n                  $25-$50k\n                  500\n                  14.1\n                  393\n                  18.0\n                \n\n\n                  &gt; $50k\n                  257\n                  7.2\n                  194\n                  8.9\n                \n\n\n                  Under $11k\n                  2081\n                  58.6\n                  1145\n                  52.4\n                \n\n\n\n\n\n\nYou can also customize the appearance of tables generated with modelsummary. For example, you can adjust the number of digits displayed in the summaries.\n\ndatasummary_balance(~ RHC.use, ObsData, \n                    fmt=\"%.2f\", \n                    output=\"markdown\")\n#&gt; Warning: Please install the `estimatr` package or set `dinm=FALSE` to suppress\n#&gt; this warning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nMean\nMean\nStd. Dev.\n\n\n\n\nedu\n\n11.57\n3.13\n11.86\n3.16\n\n\nDASIndex\n\n20.37\n5.48\n20.70\n5.03\n\n\nAPACHE.score\n\n50.93\n18.81\n60.74\n20.27\n\n\nGlasgow.Coma.Score\n\n22.25\n31.37\n18.97\n28.26\n\n\nblood.pressure\n\n84.87\n38.87\n68.20\n34.24\n\n\nWBC\n\n15.26\n11.41\n16.27\n12.55\n\n\nHeart.rate\n\n112.87\n40.94\n118.93\n41.47\n\n\nRespiratory.rate\n\n28.98\n13.95\n26.65\n14.17\n\n\nTemperature\n\n37.63\n1.74\n37.59\n1.83\n\n\nPaO2vs.FIO2\n\n240.63\n116.66\n192.43\n105.54\n\n\nAlbumin\n\n3.16\n0.67\n2.98\n0.93\n\n\nHematocrit\n\n32.70\n8.79\n30.51\n7.42\n\n\nBilirubin\n\n2.00\n4.43\n2.71\n5.33\n\n\nCreatinine\n\n1.92\n2.03\n2.47\n2.05\n\n\nSodium\n\n137.04\n7.68\n136.33\n7.60\n\n\nPotassium\n\n4.08\n1.04\n4.05\n1.01\n\n\nPaCo2\n\n39.95\n14.24\n36.79\n10.97\n\n\nPH\n\n7.39\n0.11\n7.38\n0.11\n\n\nWeight\n\n65.04\n29.50\n72.36\n27.73\n\n\nLength.of.Stay\n\n19.53\n23.59\n24.86\n28.90\n\n\nDeath\n\n0.63\n0.48\n0.68\n0.47\n\n\n\n\nN\nPct.\nN\nPct.\n\n\nDisease.category\nARF\n1581\n44.5\n909\n41.6\n\n\n\nCHF\n247\n7.0\n209\n9.6\n\n\n\nOther\n955\n26.9\n208\n9.5\n\n\n\nMOSF\n768\n21.6\n858\n39.3\n\n\nCancer\nNone\n2652\n74.7\n1727\n79.1\n\n\n\nLocalized (Yes)\n638\n18.0\n334\n15.3\n\n\n\nMetastatic\n261\n7.4\n123\n5.6\n\n\nCardiovascular\n0\n2984\n84.0\n1738\n79.6\n\n\n\n1\n567\n16.0\n446\n20.4\n\n\nCongestive.HF\n0\n2955\n83.2\n1759\n80.5\n\n\n\n1\n596\n16.8\n425\n19.5\n\n\nDementia\n0\n3138\n88.4\n2033\n93.1\n\n\n\n1\n413\n11.6\n151\n6.9\n\n\nPsychiatric\n0\n3265\n91.9\n2084\n95.4\n\n\n\n1\n286\n8.1\n100\n4.6\n\n\nPulmonary\n0\n2777\n78.2\n1869\n85.6\n\n\n\n1\n774\n21.8\n315\n14.4\n\n\nRenal\n0\n3402\n95.8\n2078\n95.1\n\n\n\n1\n149\n4.2\n106\n4.9\n\n\nHepatic\n0\n3286\n92.5\n2048\n93.8\n\n\n\n1\n265\n7.5\n136\n6.2\n\n\nGI.Bleed\n0\n3420\n96.3\n2130\n97.5\n\n\n\n1\n131\n3.7\n54\n2.5\n\n\nTumor\n0\n2679\n75.4\n1740\n79.7\n\n\n\n1\n872\n24.6\n444\n20.3\n\n\nImmunosupperssion\n0\n2644\n74.5\n1548\n70.9\n\n\n\n1\n907\n25.5\n636\n29.1\n\n\nTransfer.hx\n0\n3216\n90.6\n1857\n85.0\n\n\n\n1\n335\n9.4\n327\n15.0\n\n\nMI\n0\n3446\n97.0\n2089\n95.7\n\n\n\n1\n105\n3.0\n95\n4.3\n\n\nage\n[-Inf,50)\n884\n24.9\n540\n24.7\n\n\n\n[50,60)\n546\n15.4\n371\n17.0\n\n\n\n[60,70)\n812\n22.9\n577\n26.4\n\n\n\n[70,80)\n809\n22.8\n529\n24.2\n\n\n\n[80, Inf)\n500\n14.1\n167\n7.6\n\n\nsex\nMale\n1914\n53.9\n1278\n58.5\n\n\n\nFemale\n1637\n46.1\n906\n41.5\n\n\nDNR.status\nNo\n3052\n85.9\n2029\n92.9\n\n\n\nYes\n499\n14.1\n155\n7.1\n\n\nMedical.insurance\nMedicaid\n454\n12.8\n193\n8.8\n\n\n\nMedicare\n947\n26.7\n511\n23.4\n\n\n\nMedicare & Medicaid\n251\n7.1\n123\n5.6\n\n\n\nNo insurance\n186\n5.2\n136\n6.2\n\n\n\nPrivate\n967\n27.2\n731\n33.5\n\n\n\nPrivate & Medicare\n746\n21.0\n490\n22.4\n\n\nRespiratory.Diag\nNo\n2070\n58.3\n1552\n71.1\n\n\n\nYes\n1481\n41.7\n632\n28.9\n\n\nCardiovascular.Diag\nNo\n2544\n71.6\n1260\n57.7\n\n\n\nYes\n1007\n28.4\n924\n42.3\n\n\nNeurological.Diag\nNo\n2976\n83.8\n2066\n94.6\n\n\n\nYes\n575\n16.2\n118\n5.4\n\n\nGastrointestinal.Diag\nNo\n3029\n85.3\n1764\n80.8\n\n\n\nYes\n522\n14.7\n420\n19.2\n\n\nRenal.Diag\nNo\n3404\n95.9\n2036\n93.2\n\n\n\nYes\n147\n4.1\n148\n6.8\n\n\nMetabolic.Diag\nNo\n3379\n95.2\n2091\n95.7\n\n\n\nYes\n172\n4.8\n93\n4.3\n\n\nHematologic.Diag\nNo\n3312\n93.3\n2069\n94.7\n\n\n\nYes\n239\n6.7\n115\n5.3\n\n\nSepsis.Diag\nNo\n3036\n85.5\n1668\n76.4\n\n\n\nYes\n515\n14.5\n516\n23.6\n\n\nTrauma.Diag\nNo\n3533\n99.5\n2150\n98.4\n\n\n\nYes\n18\n0.5\n34\n1.6\n\n\nOrthopedic.Diag\nNo\n3548\n99.9\n2180\n99.8\n\n\n\nYes\n3\n0.1\n4\n0.2\n\n\nrace\nwhite\n2753\n77.5\n1707\n78.2\n\n\n\nblack\n585\n16.5\n335\n15.3\n\n\n\nother\n213\n6.0\n142\n6.5\n\n\nincome\n$11-$25k\n713\n20.1\n452\n20.7\n\n\n\n$25-$50k\n500\n14.1\n393\n18.0\n\n\n\n\n$50k\n\n257\n7.2\n194\n8.9\n\n\n\nUnder $11k\n2081\n58.6\n1145\n52.4\n\n\n\n\n\nExtract correlations between variables with datasummary_correlation:\n\ndatasummary_correlation(ObsData)\n\n\n\n    \n\n\n      \n\n \n                edu\n                DASIndex\n                APACHE.score\n                Glasgow.Coma.Score\n                blood.pressure\n                WBC\n                Heart.rate\n                Respiratory.rate\n                Temperature\n                PaO2vs.FIO2\n                Albumin\n                Hematocrit\n                Bilirubin\n                Creatinine\n                Sodium\n                Potassium\n                PaCo2\n                PH\n                Weight\n                Length.of.Stay\n                Death\n                RHC.use\n              \n\n\nedu\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nDASIndex\n                  .10\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nAPACHE.score\n                  .02\n                  -.06\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nGlasgow.Coma.Score\n                  -.02\n                  .04\n                  .03\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nblood.pressure\n                  -.04\n                  .06\n                  -.40\n                  .02\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nWBC\n                  -.02\n                  .03\n                  .13\n                  .04\n                  -.03\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nHeart.rate\n                  .05\n                  .02\n                  .22\n                  -.11\n                  .06\n                  .03\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nRespiratory.rate\n                  .03\n                  -.00\n                  .26\n                  -.14\n                  .03\n                  .01\n                  .28\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nTemperature\n                  .07\n                  .15\n                  -.04\n                  .05\n                  .01\n                  -.01\n                  .22\n                  .14\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nPaO2vs.FIO2\n                  -.01\n                  -.07\n                  -.23\n                  .09\n                  .07\n                  -.06\n                  -.08\n                  -.10\n                  -.10\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nAlbumin\n                  -.00\n                  -.00\n                  -.29\n                  .02\n                  .13\n                  -.06\n                  -.08\n                  .00\n                  -.01\n                  .07\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nHematocrit\n                  -.05\n                  .03\n                  -.24\n                  .03\n                  .16\n                  -.03\n                  -.03\n                  .01\n                  -.02\n                  -.01\n                  .29\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nBilirubin\n                  .07\n                  -.01\n                  .28\n                  .05\n                  -.05\n                  .01\n                  .03\n                  .01\n                  -.03\n                  .03\n                  -.09\n                  -.15\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nCreatinine\n                  -.02\n                  -.05\n                  .38\n                  .01\n                  -.09\n                  .05\n                  -.06\n                  -.04\n                  -.11\n                  .06\n                  -.00\n                  -.20\n                  .12\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nSodium\n                  -.01\n                  .03\n                  .02\n                  .13\n                  .02\n                  -.04\n                  .03\n                  .02\n                  .05\n                  -.02\n                  .00\n                  .06\n                  .02\n                  -.01\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nPotassium\n                  -.02\n                  -.06\n                  .15\n                  -.01\n                  -.07\n                  .08\n                  -.11\n                  -.01\n                  -.13\n                  .04\n                  .03\n                  -.01\n                  .00\n                  .30\n                  -.10\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nPaCo2\n                  -.05\n                  -.09\n                  -.09\n                  -.12\n                  .04\n                  -.05\n                  -.01\n                  .00\n                  -.04\n                  -.17\n                  .09\n                  .24\n                  -.08\n                  -.13\n                  .06\n                  .04\n                  1\n                  .\n                  .\n                  .\n                  .\n                  .\n                \n\nPH\n                  .04\n                  .05\n                  -.33\n                  .03\n                  .13\n                  -.06\n                  .03\n                  -.01\n                  .14\n                  .11\n                  .04\n                  -.03\n                  .02\n                  -.16\n                  -.02\n                  -.20\n                  -.47\n                  1\n                  .\n                  .\n                  .\n                  .\n                \n\nWeight\n                  -.06\n                  .05\n                  .08\n                  -.08\n                  -.02\n                  -.01\n                  .02\n                  -.01\n                  .02\n                  -.05\n                  -.05\n                  .05\n                  .01\n                  .10\n                  -.00\n                  .06\n                  .03\n                  -.05\n                  1\n                  .\n                  .\n                  .\n                \n\nLength.of.Stay\n                  .02\n                  .04\n                  .02\n                  -.00\n                  -.02\n                  .03\n                  .07\n                  -.01\n                  .09\n                  -.08\n                  -.11\n                  -.09\n                  -.00\n                  .01\n                  .04\n                  -.02\n                  .00\n                  .02\n                  .02\n                  1\n                  .\n                  .\n                \n\nDeath\n                  -.03\n                  -.18\n                  .19\n                  .12\n                  -.10\n                  .03\n                  -.02\n                  .01\n                  -.10\n                  .02\n                  -.03\n                  -.09\n                  .08\n                  .08\n                  .00\n                  .05\n                  -.04\n                  -.04\n                  -.05\n                  -.08\n                  1\n                  .\n                \n\nRHC.use\n                  .04\n                  .03\n                  .24\n                  -.05\n                  -.21\n                  .04\n                  .07\n                  -.08\n                  -.01\n                  -.20\n                  -.12\n                  -.13\n                  .07\n                  .13\n                  -.04\n                  -.01\n                  -.12\n                  -.06\n                  .12\n                  .10\n                  .05\n                  1\n                \n\n\n\n\n\n\nGenerate a contingency table using datasummary_crosstab:\n\ndatasummary_crosstab(age ~ RHC.use, data=ObsData)\n\n\n\n    \n\n\n      \n\nage\n                 \n                0\n                1\n                All\n              \n\n\n[-Inf,50)\n                  N\n                  884\n                  540\n                  1424\n                \n\n\n                  % row\n                  62.1\n                  37.9\n                  100.0\n                \n\n[50,60)\n                  N\n                  546\n                  371\n                  917\n                \n\n\n                  % row\n                  59.5\n                  40.5\n                  100.0\n                \n\n[60,70)\n                  N\n                  812\n                  577\n                  1389\n                \n\n\n                  % row\n                  58.5\n                  41.5\n                  100.0\n                \n\n[70,80)\n                  N\n                  809\n                  529\n                  1338\n                \n\n\n                  % row\n                  60.5\n                  39.5\n                  100.0\n                \n\n[80, Inf)\n                  N\n                  500\n                  167\n                  667\n                \n\n\n                  % row\n                  75.0\n                  25.0\n                  100.0\n                \n\nAll\n                  N\n                  3551\n                  2184\n                  5735\n                \n\n\n                  % row\n                  61.9\n                  38.1\n                  100.0\n                \n\n\n\n\n\n\nThe modelsummary package also allows for professional display of regression models, combining results from multiple models into one table for easy comparison.\n\n# Example: Fit two regression models\nmodel1 &lt;- lm(Length.of.Stay ~ age + sex + APACHE.score, data = ObsData)\nmodel2 &lt;- lm(Length.of.Stay ~ age + sex + APACHE.score + Medical.insurance, data = ObsData)\n\n# Display both models side by side\nmodelsummary(list(model1, model2), \n             statistic = \"p.value\", \n             stars = TRUE)\n\n\n\n    \n\n\n      \n\n \n                (1)\n                (2)\n              \n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n(Intercept)\n                  21.312***\n                  21.200***\n                \n\n\n                  (&lt;0.001)\n                  (&lt;0.001)\n                \n\nage[50,60)\n                  -1.065\n                  -1.116\n                \n\n\n                  (0.330)\n                  (0.310)\n                \n\nage[60,70)\n                  -1.420\n                  -1.201\n                \n\n\n                  (0.145)\n                  (0.255)\n                \n\nage[70,80)\n                  -2.763**\n                  -2.334+\n                \n\n\n                  (0.005)\n                  (0.056)\n                \n\nage[80, Inf)\n                  -6.762***\n                  -6.530***\n                \n\n\n                  (&lt;0.001)\n                  (&lt;0.001)\n                \n\nsexFemale\n                  0.932\n                  1.021\n                \n\n\n                  (0.176)\n                  (0.140)\n                \n\nAPACHE.score\n                  0.033+\n                  0.032+\n                \n\n\n                  (0.057)\n                  (0.063)\n                \n\nMedical.insuranceMedicare\n                  \n                  0.561\n                \n\n\n                  \n                  (0.687)\n                \n\nMedical.insuranceMedicare & Medicaid\n                  \n                  -2.545\n                \n\n\n                  \n                  (0.145)\n                \n\nMedical.insuranceNo insurance\n                  \n                  0.475\n                \n\n\n                  \n                  (0.788)\n                \n\nMedical.insurancePrivate\n                  \n                  0.385\n                \n\n\n                  \n                  (0.748)\n                \n\nMedical.insurancePrivate & Medicare\n                  \n                  -0.817\n                \n\n\n                  \n                  (0.569)\n                \n\nNum.Obs.\n                  5735\n                  5735\n                \n\nR2\n                  0.007\n                  0.008\n                \n\nR2 Adj.\n                  0.006\n                  0.006\n                \n\nAIC\n                  53563.6\n                  53567.9\n                \n\nBIC\n                  53616.8\n                  53654.4\n                \n\nLog.Lik.\n                  -26773.805\n                  -26770.932\n                \n\nF\n                  6.388\n                  4.006\n                \n\nRMSE\n                  25.78\n                  25.77\n                \n\n\n\n\n\n\nAll tables generated by modelsummary can be saved in various formats such as LaTeX, HTML, or Word.\n\n# Save as a LaTeX file\nmodelsummary(model1, output = \"table.tex\")\n\n# Save as an HTML file\nmodelsummary(model1, output = \"table.html\")\n\nstargazer\nThe stargazer package is widely used for outputting regression results in LaTeX, HTML, or plain text formats. It allows you to include various statistics, such as standard errors, t-values, p-values, and confidence intervals.\n\nrequire(stargazer)\n#&gt; Loading required package: stargazer\n#&gt; \n#&gt; Please cite as:\n#&gt;  Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n#&gt;  R package version 5.2.3. https://CRAN.R-project.org/package=stargazer\n\n\n# Example: Fit two regression models\nmodel1 &lt;- lm(Length.of.Stay ~ age + sex + APACHE.score, data = ObsData)\nmodel2 &lt;- lm(Length.of.Stay ~ age + sex + APACHE.score + Medical.insurance, data = ObsData)\n\n# Display the regression results using stargazer\nstargazer(model1, model2, type = \"text\", \n          title = \"Regression Results\",\n          dep.var.labels = \"Length of Stay\",\n          covariate.labels = c(\"Age\", \"Sex\", \"APACHE Score\", \"Medical Insurance\"),\n          out = \"regression_table.txt\")\n#&gt; \n#&gt; Regression Results\n#&gt; ==============================================================================\n#&gt;                                             Dependent variable:               \n#&gt;                               ------------------------------------------------\n#&gt;                                                Length of Stay                 \n#&gt;                                         (1)                     (2)           \n#&gt; ------------------------------------------------------------------------------\n#&gt; Age                                   -1.065                   -1.116         \n#&gt;                                       (1.093)                 (1.099)         \n#&gt;                                                                               \n#&gt; Sex                                   -1.420                   -1.201         \n#&gt;                                       (0.974)                 (1.055)         \n#&gt;                                                                               \n#&gt; APACHE Score                         -2.763***                -2.334*         \n#&gt;                                       (0.982)                 (1.221)         \n#&gt;                                                                               \n#&gt; Medical Insurance                    -6.762***               -6.530***        \n#&gt;                                       (1.213)                 (1.424)         \n#&gt;                                                                               \n#&gt; sexFemale                              0.932                   1.021          \n#&gt;                                       (0.688)                 (0.691)         \n#&gt;                                                                               \n#&gt; APACHE.score                          0.033*                   0.032*         \n#&gt;                                       (0.017)                 (0.017)         \n#&gt;                                                                               \n#&gt; Medical.insuranceMedicare                                      0.561          \n#&gt;                                                               (1.391)         \n#&gt;                                                                               \n#&gt; Medical.insuranceMedicare            Medicaid                                 \n#&gt;                                                               (1.744)         \n#&gt;                                                                               \n#&gt; Medical.insuranceNo insurance                                  0.475          \n#&gt;                                                               (1.763)         \n#&gt;                                                                               \n#&gt; Medical.insurancePrivate                                       0.385          \n#&gt;                                                               (1.199)         \n#&gt;                                                                               \n#&gt; Medical.insurancePrivate             Medicare                                 \n#&gt;                                                               (1.436)         \n#&gt;                                                                               \n#&gt; Constant                             21.312***               21.200***        \n#&gt;                                       (1.220)                 (1.495)         \n#&gt;                                                                               \n#&gt; ------------------------------------------------------------------------------\n#&gt; Observations                           5,735                   5,735          \n#&gt; R2                                     0.007                   0.008          \n#&gt; Adjusted R2                            0.006                   0.006          \n#&gt; Residual Std. Error             25.795 (df = 5728)       25.793 (df = 5723)   \n#&gt; F Statistic                   6.388*** (df = 6; 5728) 4.006*** (df = 11; 5723)\n#&gt; ==============================================================================\n#&gt; Note:                                              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\nbroom\nThe broom package converts regression model outputs into tidy data frames, making it easy to extract and manipulate specific parts of the model for custom summaries.\n\nrequire(broom)\n#&gt; Loading required package: broom\n\n\n# Tidy the model results for model1\ntidy(model1)\n\n\n  \n\n\n\n# Glance at model1 for a quick summary of goodness-of-fit statistics\nglance(model1)\n\n\n  \n\n\n\ntexreg\nThe texreg package allows you to export regression tables into LaTeX, HTML, or Word. It supports outputting multiple regression models side by side for easy comparison.\n\nrequire(texreg)\n#&gt; Loading required package: texreg\n#&gt; Version:  1.39.4\n#&gt; Date:     2024-07-23\n#&gt; Author:   Philip Leifeld (University of Manchester)\n#&gt; \n#&gt; Consider submitting praise using the praise or praise_interactive functions.\n#&gt; Please cite the JSS article in your publications -- see citation(\"texreg\").\n\n\n# Export models into a LaTeX table\ntexreg(list(model1, model2), \n       file = \"regression_table.tex\", \n       label = \"tab:regression\", \n       caption = \"Regression Models for Length of Stay\")\n#&gt; The table was written to the file 'regression_table.tex'.\n\nYou can also customize the output from these packages by specifying different statistics to display, such as including robust standard errors, changing the number of decimal places, or adding significance stars.\n\nstargazer(model1, model2, type = \"text\", \n          se = list(coef(summary(model1))[ , \"Std. Error\"], \n                    coef(summary(model2))[ , \"Std. Error\"]), \n          star.cutoffs = c(0.05, 0.01, 0.001))\n#&gt; \n#&gt; ==============================================================================\n#&gt;                                             Dependent variable:               \n#&gt;                               ------------------------------------------------\n#&gt;                                                Length.of.Stay                 \n#&gt;                                         (1)                     (2)           \n#&gt; ------------------------------------------------------------------------------\n#&gt; age[50,60)                            -1.065                   -1.116         \n#&gt;                                       (1.093)                 (1.099)         \n#&gt;                                                                               \n#&gt; age[60,70)                            -1.420                   -1.201         \n#&gt;                                       (0.974)                 (1.055)         \n#&gt;                                                                               \n#&gt; age[70,80)                           -2.763**                  -2.334         \n#&gt;                                       (0.982)                 (1.221)         \n#&gt;                                                                               \n#&gt; age[80, Inf)                         -6.762***               -6.530***        \n#&gt;                                       (1.213)                 (1.424)         \n#&gt;                                                                               \n#&gt; sexFemale                              0.932                   1.021          \n#&gt;                                       (0.688)                 (0.691)         \n#&gt;                                                                               \n#&gt; APACHE.score                           0.033                   0.032          \n#&gt;                                       (0.017)                 (0.017)         \n#&gt;                                                                               \n#&gt; Medical.insuranceMedicare                                      0.561          \n#&gt;                                                               (1.391)         \n#&gt;                                                                               \n#&gt; Medical.insuranceMedicare            Medicaid                                 \n#&gt;                                                               (1.744)         \n#&gt;                                                                               \n#&gt; Medical.insuranceNo insurance                                  0.475          \n#&gt;                                                               (1.763)         \n#&gt;                                                                               \n#&gt; Medical.insurancePrivate                                       0.385          \n#&gt;                                                               (1.199)         \n#&gt;                                                                               \n#&gt; Medical.insurancePrivate             Medicare                                 \n#&gt;                                                               (1.436)         \n#&gt;                                                                               \n#&gt; Constant                             21.312***               21.200***        \n#&gt;                                       (1.220)                 (1.495)         \n#&gt;                                                                               \n#&gt; ------------------------------------------------------------------------------\n#&gt; Observations                           5,735                   5,735          \n#&gt; R2                                     0.007                   0.008          \n#&gt; Adjusted R2                            0.006                   0.006          \n#&gt; Residual Std. Error             25.795 (df = 5728)       25.793 (df = 5723)   \n#&gt; F Statistic                   6.388*** (df = 6; 5728) 4.006*** (df = 11; 5723)\n#&gt; ==============================================================================\n#&gt; Note:                                            *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n\njanitor\nThe janitor package simplifies data cleaning tasks, such as checking for duplicate records, cleaning column names, and generating cross-tabulations.\n\nrequire(janitor)\n#&gt; Loading required package: janitor\n#&gt; \n#&gt; Attaching package: 'janitor'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     chisq.test, fisher.test\n\n\n# Clean column names to make them syntactically valid\nObsData_clean &lt;- clean_names(ObsData)\n\n# Method 1: Using the data frame name in the tabyl function\ntabyl(ObsData, Disease.category, RHC.use)\n\n\n  \n\n\n\n# Method 2: Using the pipe operator\nObsData %&gt;% tabyl(Disease.category, RHC.use)\n\n\n  \n\n\n\nskimr\nThe skimr package provides a more compact and readable summary compared to the default summary() function. It tailors its output for each variable type.\n\nrequire(skimr)\n#&gt; Loading required package: skimr\n\n\n# Quick summary of the entire dataset\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n30\n\n\nnumeric\n22\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, &gt; $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nDeath\n0\n1\n0.65\n0.48\n0.00\n0.00\n1.00\n1.00\n1.00\n▅▁▁▁▇\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\ncorrplot\nThe corrplot package is useful for visualizing correlation matrices with different styles (circle, color, etc.), making correlations easier to interpret.\n\nrequire(corrplot)\n#&gt; Loading required package: corrplot\n#&gt; corrplot 0.95 loaded\n\n\n# Step 1: Select only numerical variables from ObsData\nObsData_num &lt;- dplyr::select(ObsData, where(is.numeric))\n\n# Step 2: Compute the correlation matrix\ncorr_matrix &lt;- cor(ObsData_num, use = \"complete.obs\")\n\n# Step 3: Plot the correlation matrix using corrplot\ncorrplot::corrplot(corr_matrix, method = \"circle\")\n\n\n\n\n\n\n\nvisdat\nThe visdat package helps visualize missing data patterns, data types, and distributions.\n\nrequire(visdat)\n#&gt; Loading required package: visdat\n\n\n# Visualize missing data patterns\nvis_miss(ObsData)\n\n\n\n\n\n\n\n# Visualize data types and missingness\nvis_dat(ObsData)\n\n\n\n\n\n\n\nnaniar\nThe naniar package provides tools to handle and visualize missing data, helping to explore missingness patterns in the data.\n\nrequire(naniar)\n#&gt; Loading required package: naniar\n#&gt; \n#&gt; Attaching package: 'naniar'\n#&gt; The following object is masked from 'package:skimr':\n#&gt; \n#&gt;     n_complete\n\n\n# Introduce some missing values for demonstration\nObsData_with_NA &lt;- ObsData\nObsData_with_NA$age[sample(1:nrow(ObsData), 10)] &lt;- NA\nObsData_with_NA$sex[sample(1:nrow(ObsData), 10)] &lt;- NA\n\n# Visualize missing data across variables\ngg_miss_var(ObsData_with_NA)\n\n\n\n\n\n\n\n# Plot the missing data upset plot\nnaniar::gg_miss_upset(ObsData_with_NA)\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n#&gt; ℹ The deprecated feature was likely used in the UpSetR package.\n#&gt;   Please report the issue to the authors.\n#&gt; `geom_line()`: Each group consists of only one observation.\n#&gt; ℹ Do you need to adjust the group aesthetic?\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n#&gt; ℹ The deprecated feature was likely used in the UpSetR package.\n#&gt;   Please report the issue to the authors.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Useful Packages"
    ]
  },
  {
    "objectID": "researchquestion.html",
    "href": "researchquestion.html",
    "title": "Research questions",
    "section": "",
    "text": "Background\nWhen we are starting a research project, one of the first steps is to clearly define your research topic or question. We will primarily focus on two types of research questions:",
    "crumbs": [
      "Research questions"
    ]
  },
  {
    "objectID": "researchquestion.html#background",
    "href": "researchquestion.html#background",
    "title": "Research questions",
    "section": "",
    "text": "predictive (predictors predicting one outcome)\nassociational or causal (association between an outcome and an exposure, adjusting for confounders and risk factors for the outcome).\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Research questions"
    ]
  },
  {
    "objectID": "researchquestion.html#overview-of-tutorials",
    "href": "researchquestion.html#overview-of-tutorials",
    "title": "Research questions",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nPredictive questions\n\n\nIn the previous chapter, we learned about how to access external data. In this chapter, we will embark on a journey to understand the nuances of different research questions, laying the groundwork for the topics that lie ahead. As we move forward, the next chapter will delve deeper into the challenges associated with causal questions. We will explore the complexities of causal associations and discuss the optimal types of variables to include in adjustment models for accurate treatment effect estimation. Following that, we will transition to a chapter dedicated entirely to predictive questions, shedding light on their unique attributes and the methodologies best suited for addressing them. Join us as we navigate these intricate terrains of research inquiry.\n\nRHC Data\nThe first tutorial serves to educate the user on how to utilize the RHC dataset to answer a predictive research question: developing a prediction model for the length of stay. The tutorial equips users with the skills to clean and process raw data, transforming it into an analyzable format, and introduces concepts that will be foundational for subsequent analysis.\n\n\nData from NHANES Part 1: prepare data Part 2: work with data\nThe second tutorial (part a for downloading and part b for analyzing) provides an in-depth guide on how to build a predictive model for Diastolic blood pressure using the NHANES dataset for the years 2013-14.\n\n\n\nCausal questions\n\nData from CCHS\nThe third tutorial aims to guide a study on the relationship between Osteoarthritis (OA) and cardiovascular diseases (CVD) among Canadian adults from 2001-2005. Utilizing the Canadian Community Health Survey (CCHS) cycle 1.1-3.1, the study intends to explore whether OA increases (more accurately, whether associated with) the risk of developing CVD.\n\n\nData from NHANES\nThe NHANES dataset was analyzed in this fourth tutorial to explore the relationship between health predictors and cholesterol levels (association/causal). After refining the survey design and handling missing data, regression models were built using varying predictors. Standard error computations and p-values were derived, adjusting for the survey’s unique structure.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou will find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\nReference",
    "crumbs": [
      "Research questions"
    ]
  },
  {
    "objectID": "researchquestion0.html",
    "href": "researchquestion0.html",
    "title": "Concepts (Q)",
    "section": "",
    "text": "PICOT Framework\nAccording to Thabane et al., there is a structured approach to do this, primarily using the PICOT and FINER frameworks.\nThe PICOT framework helps to structure a specific and clear research question by focusing on five key elements:\nElement\nDescription\nExample\n\n\n\nP\nPopulation of Interest: Who is the target group you are studying?\nUS adults\n\n\nI\nIntervention: What is the main action, treatment, or variable you're looking at?\nEffect of having rheumatoid arthritis\n\n\nC\nComparison: Are you comparing the intervention against a control group or usual care?\nPeople without rheumatoid arthritis\n\n\nO\nOutcome of Interest: What specifically do you want to measure?\nRate of cardiovascular diseases\n\n\nT\nTime Frame: Over what time period will your study take place?\n1999–2018",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#picot-framework",
    "href": "researchquestion0.html#picot-framework",
    "title": "Concepts (Q)",
    "section": "",
    "text": "Research Question: “In US adults, does having rheumatoid arthritis, compared to those without rheumatoid arthritis, affect the rate of cardiovascular diseases during 1999–2018?” based on Hossain et al. (2022): DOI: 10.1016/j.annepidem.2022.03.005",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#finer-criteria",
    "href": "researchquestion0.html#finer-criteria",
    "title": "Concepts (Q)",
    "section": "FINER Criteria",
    "text": "FINER Criteria\nOnce we have formulated your research question with the help of the PICOT elements, we should evaluate it using the FINER criteria:\n\n\n\nFINER Criteria\n\nElement\nDescription\n\n\n\nF\nFeasible: Is it possible to conduct this research with available resources?\n\n\nI\nInteresting: Is the research question intriguing to the scientific community?\n\n\nN\nNovel: Is the question original and not already thoroughly researched?\n\n\nE\nEthical: Is the research ethically sound?\n\n\nR\nRelevant: Is the research currently needed or will it fill a gap in existing knowledge?\n\n\n\n\n\nThe key takeaway is: Use the PICOT and FINER frameworks to guide you in framing a compelling, ethical, and achievable research question.",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#sap",
    "href": "researchquestion0.html#sap",
    "title": "Concepts (Q)",
    "section": "SAP",
    "text": "SAP\nA Statistical Analysis Plan (SAP), also referred to as a Data Analysis Plan (DAP) or Reporting Analysis Plan (RAP), is an integral part of research, particularly in randomized controlled trials (RCTs) (Kahan et al. 2020), but also in observational studies (Hiemstra et al. 2019). Here are a few reasons why it is beneficial to pre-plan the SAP for an observational study:\n\nPre-planning an SAP helps define the specific analytical strategies and methods that will be used to answer the research questions. It outlines the techniques for handling data, including\n\n\nthe treatment of missing data, outliers,\nthe use of statistical tests, and\nconfounding adjustment techniques.\n\n\nBy detailing the analysis plan before the data is examined, researchers ensure transparency and reduce the risk of data dredging or p-hacking.\nConfounding is a more pronounced issue in observational studies. Strategies for addressing confounding need to be more elaborate and explicit in observational studies.\n\n\n\nRefer to the ‘Scientific Writing for Health Research’ book chapter for more details and examples for PICOT, FINER and Statistical Analysis Plan (SAP).\n\n\n\n\n\n\nNote\n\n\n\nWe include 2 types of tutorials that emphasize the critical steps of data preparation and analysis tailored to specific research questions, cosidering the PICOT framework. They underscore the importance of refining and cleaning datasets to ensure their suitability for rigorous analytical procedures. The analyses, while rooted in distinct methodologies, converge on the common goal of deriving meaningful insights and ensuring the integrity and validity of the results obtained from the processed analytical data.\n\n\n\n\nData preparation: Merging, reformatting and recategorizing essential variables to create a dataset suitable for analysis, aligning it with the study’s objectives.",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#video-lessons",
    "href": "researchquestion0.html#video-lessons",
    "title": "Concepts (Q)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPICOT and FINER\n\n\n\nWhat is included in this Video Lesson:\n\nReferences 0:53\nHow to get an idea about a Research Question? 1:05\nWhy the question need to be good? 2:41\nA framework for defining a research question 5:17\nThink hard about the ‘Outcome’ 14:40\nIs this research doable? 17:57\nOverall Roadmap 19:57\nOther Reference (optional) 21:27\n\nThe timestamps are also included in the YouTube video description.\n\n\n\n\n\n\n\n\n\n\n\n\nSAP\n\n\n\nWhat is included in this Video Lesson:\n\nSAP 0:03\nSAP example from a RCT 1:31\nSAP example from an observational study 4:40\nCode book 15:35\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#video-lesson-slides",
    "href": "researchquestion0.html#video-lesson-slides",
    "title": "Concepts (Q)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#links",
    "href": "researchquestion0.html#links",
    "title": "Concepts (Q)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion0.html#references",
    "href": "researchquestion0.html#references",
    "title": "Concepts (Q)",
    "section": "References",
    "text": "References\n\n\n\n\nHiemstra, Bart, Frederik Keus, Jørn Wetterslev, Christian Gluud, and Iwan CC van der Horst. 2019. “DEBATE-Statistical Analysis Plans for Observational Studies.” BMC Medical Research Methodology 19 (1): 1–10.\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nKahan, Brennan C, Tahania Ahmad, Gordon Forbes, and Suzie Cro. 2020. “Public Availability and Adherence to Prespecified Statistical Analysis Approaches Was Low in Published Randomized Trials.” Journal of Clinical Epidemiology 128: 29–34.\n\n\nThabane, Lehana, Tara Thomas, Chenglin Ye, and James Paul. 2009. “Posing the Research Question: Not so Simple.” Canadian Journal of Anesthesia/Journal Canadien d’anesthésie 56 (1): 71–79.",
    "crumbs": [
      "Research questions",
      "Concepts (Q)"
    ]
  },
  {
    "objectID": "researchquestion1.html",
    "href": "researchquestion1.html",
    "title": "Predictive question-1",
    "section": "",
    "text": "# Load required packages\nrequire(tableone)\nrequire(Publish)\nrequire(MatchIt)\nrequire(cobalt)\nrequire(ggplot2)\n\nWorking with a Predictive question using RHC\nThis tutorial delves into processing and understanding the right heart catheterization (RHC) dataset, which pertains to patients in the intensive care unit. The dataset is particularly centered around the implications of using RHC in the early phases of care, with a focus on comparing two patient groups: those who received the RHC procedure and those who did not. The key outcome being analyzed is the 30-day survival rate. We will use this as an example to explain how to work with a predictive research question to build the analytic data.\n\n\nLink for the RHC dataset\n(Connors et al. 1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as\n\n\nlength of stay (hospital stay; measured continuously)\n\ndeath within certain period (death at any time up to 180 Days; measured as a binary variable)\n\nThe original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above.\nBut we will use this data as a case study for our prediction modelling. Traditional PICOT framework is designed primarily for clinical questions related to interventions, so when applying it to other areas like predictive modeling, some creative adaptation is needed.\n\n\n\n\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nNot applicable, as we are dealing with a prediction model here\n\n\nC\nNot applicable, as we are dealing with a prediction model here\n\n\nO\nin-hospital mortality\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\n\n\n\nWe are interested in developing a prediction model for the length of stay.\nData download\nData is freely available from Vanderbilt Biostatistics, variable list is available here, and the article is freely available from researchgate.\n\n\nRHC Data amd search for right heart catheterization dataset\n\nVariable list\n\nArticle\n\n\nLet us download the dataset and save it for later use.\n\n# Load the dataset\nObsData &lt;- read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", \n                    header = TRUE)\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhc.RDS\")\n\nCreating analytic dataset\nNow, we show the process of preparing our analytic dataset (i.e., ready to use dataset for our analysis), so that the variables generally match with the way the authors were coded in the original article. Below we show the process of creating the analytic dataset.\nAdd column for outcome: length of stay\n\n# Length of Stay = date of discharge - study admission date\nObsData$Length.of.Stay &lt;- ObsData$dschdte - ObsData$sadmdte\n\n# Length of Stay = date of death - study admission date if date of discharge not available\nObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] &lt;- \n  ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - \n  ObsData$sadmdte[is.na(ObsData$Length.of.Stay)]\n\nRecoding column for outcome: death\n\n\n\n\n\n\nTip\n\n\n\nHere we use the ifelse function to create a categorical variable. Other related functions are cut, car.\n\n\nLet us recode our outcome variable as a binary variable:\n\nObsData$death &lt;- ifelse(ObsData$death == \"Yes\", 1, 0)\n\nRemove unnecessary outcomes\nOur next task is to remove unnecessary outcomes:\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to drop variables from a dataset. E.g., without using any package and using the select function from the dplyr package.\n\n\n\nObsData &lt;- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, \n                            t3d30, dth30, surv2md1))\n\nRemove unnecessary and problematic variables\nNow we will drop unnecessary and problematic variables:\n\nObsData &lt;- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, \n                                     urin1, cat2))\n\nBasic data cleanup\nNow we will do some basic cleanup.\n\n\n\n\n\n\nTip\n\n\n\nWe an use the lapply function to convert all categorical variables to factors at once. Not that a similar function to lapply is sapply. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\n\n# convert all categorical variables to factors\nfactors &lt;- c(\"cat1\", \"ca\", \"death\", \"cardiohx\", \"chfhx\", \n             \"dementhx\", \"psychhx\", \"chrpulhx\", \"renalhx\", \n             \"liverhx\", \"gibledhx\", \"malighx\", \"immunhx\", \n             \"transhx\", \"amihx\", \"sex\", \"dnr1\", \"ninsclas\", \n             \"resp\", \"card\", \"neuro\", \"gastr\", \"renal\", \"meta\", \n             \"hema\", \"seps\", \"trauma\", \"ortho\", \"race\", \n             \"income\")\nObsData[factors] &lt;- lapply(ObsData[factors], as.factor)\n\n# convert RHC.use (RHC vs. No RHC) to a binary variable\nObsData$RHC.use &lt;- ifelse(ObsData$swang1 == \"RHC\", 1, 0)\nObsData &lt;- dplyr::select(ObsData, !swang1)\n\n# Categorize the variables to match with the original paper\nObsData$age &lt;- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf),\n                   right=FALSE)\nObsData$race &lt;- factor(ObsData$race, \n                       levels=c(\"white\",\"black\",\"other\"))\nObsData$sex &lt;- as.factor(ObsData$sex)\nObsData$sex &lt;- relevel(ObsData$sex, ref = \"Male\")\nObsData$cat1 &lt;- as.factor(ObsData$cat1)\nlevels(ObsData$cat1) &lt;- c(\"ARF\",\"CHF\",\"Other\",\"Other\",\"Other\",\n                          \"Other\",\"Other\",\"MOSF\",\"MOSF\")\nObsData$ca &lt;- as.factor(ObsData$ca)\nlevels(ObsData$ca) &lt;- c(\"Metastatic\",\"None\",\"Localized (Yes)\")\nObsData$ca &lt;- factor(ObsData$ca, levels=c(\"None\", \"Localized (Yes)\",\n                                          \"Metastatic\"))\n\nRename variables\n\n# Rename the variables\nnames(ObsData) &lt;- c(\"Disease.category\", \"Cancer\", \"Death\", \n                    \"Cardiovascular\", \"Congestive.HF\", \n                    \"Dementia\", \"Psychiatric\", \"Pulmonary\", \n                    \"Renal\", \"Hepatic\", \"GI.Bleed\", \"Tumor\", \n                    \"Immunosupperssion\", \"Transfer.hx\", \"MI\", \n                    \"age\", \"sex\", \"edu\", \"DASIndex\", \n                    \"APACHE.score\", \"Glasgow.Coma.Score\", \n                    \"blood.pressure\", \"WBC\", \"Heart.rate\",\n                    \"Respiratory.rate\",  \"Temperature\",\n                    \"PaO2vs.FIO2\", \"Albumin\", \"Hematocrit\", \n                    \"Bilirubin\", \"Creatinine\", \"Sodium\", \n                    \"Potassium\", \"PaCo2\",  \"PH\", \"Weight\", \n                    \"DNR.status\", \"Medical.insurance\", \n                    \"Respiratory.Diag\", \"Cardiovascular.Diag\", \n                    \"Neurological.Diag\", \"Gastrointestinal.Diag\",\n                    \"Renal.Diag\", \"Metabolic.Diag\", \n                    \"Hematologic.Diag\", \"Sepsis.Diag\", \n                    \"Trauma.Diag\", \"Orthopedic.Diag\", \n                    \"race\", \"income\", \n                    \"Length.of.Stay\", \"RHC.use\")\n\n# Save the dataset\nsaveRDS(ObsData, file = \"Data/researchquestion/rhcAnalytic.RDS\")\n\nNotations\nlet us introduce with some notations:\n\n\nNotations\nExample in RHC study\n\n\n\n\n\\(Y_1\\): Observed outcome\nlength of stay\n\n\n\n\\(Y_2\\): Observed outcome\ndeath within 3 months\n\n\n\n\\(L\\): Covariates\nSee below\n\n\nBasic data exploration\nDimension\nLet us the how many rows and columns we have:\n\ndim(ObsData)\n#&gt; [1] 5735   52\n\nComprehensive summary\nLet us see the summary statistics of the variables:\n\n\n\n\n\n\nTip\n\n\n\nTo see the comprehensive summary of the variables, we can use the skim function form skimr package or describe function from rms package\n\n\n\nrequire(skimr)\n#&gt; Loading required package: skimr\nskim(ObsData)\n\n\nData summary\n\n\nName\nObsData\n\n\nNumber of rows\n5735\n\n\nNumber of columns\n52\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nDisease.category\n0\n1\nFALSE\n4\nARF: 2490, MOS: 1626, Oth: 1163, CHF: 456\n\n\nCancer\n0\n1\nFALSE\n3\nNon: 4379, Loc: 972, Met: 384\n\n\nDeath\n0\n1\nFALSE\n2\n1: 3722, 0: 2013\n\n\nCardiovascular\n0\n1\nFALSE\n2\n0: 4722, 1: 1013\n\n\nCongestive.HF\n0\n1\nFALSE\n2\n0: 4714, 1: 1021\n\n\nDementia\n0\n1\nFALSE\n2\n0: 5171, 1: 564\n\n\nPsychiatric\n0\n1\nFALSE\n2\n0: 5349, 1: 386\n\n\nPulmonary\n0\n1\nFALSE\n2\n0: 4646, 1: 1089\n\n\nRenal\n0\n1\nFALSE\n2\n0: 5480, 1: 255\n\n\nHepatic\n0\n1\nFALSE\n2\n0: 5334, 1: 401\n\n\nGI.Bleed\n0\n1\nFALSE\n2\n0: 5550, 1: 185\n\n\nTumor\n0\n1\nFALSE\n2\n0: 4419, 1: 1316\n\n\nImmunosupperssion\n0\n1\nFALSE\n2\n0: 4192, 1: 1543\n\n\nTransfer.hx\n0\n1\nFALSE\n2\n0: 5073, 1: 662\n\n\nMI\n0\n1\nFALSE\n2\n0: 5535, 1: 200\n\n\nage\n0\n1\nFALSE\n5\n[-I: 1424, [60: 1389, [70: 1338, [50: 917\n\n\nsex\n0\n1\nFALSE\n2\nMal: 3192, Fem: 2543\n\n\nDNR.status\n0\n1\nFALSE\n2\nNo: 5081, Yes: 654\n\n\nMedical.insurance\n0\n1\nFALSE\n6\nPri: 1698, Med: 1458, Pri: 1236, Med: 647\n\n\nRespiratory.Diag\n0\n1\nFALSE\n2\nNo: 3622, Yes: 2113\n\n\nCardiovascular.Diag\n0\n1\nFALSE\n2\nNo: 3804, Yes: 1931\n\n\nNeurological.Diag\n0\n1\nFALSE\n2\nNo: 5042, Yes: 693\n\n\nGastrointestinal.Diag\n0\n1\nFALSE\n2\nNo: 4793, Yes: 942\n\n\nRenal.Diag\n0\n1\nFALSE\n2\nNo: 5440, Yes: 295\n\n\nMetabolic.Diag\n0\n1\nFALSE\n2\nNo: 5470, Yes: 265\n\n\nHematologic.Diag\n0\n1\nFALSE\n2\nNo: 5381, Yes: 354\n\n\nSepsis.Diag\n0\n1\nFALSE\n2\nNo: 4704, Yes: 1031\n\n\nTrauma.Diag\n0\n1\nFALSE\n2\nNo: 5683, Yes: 52\n\n\nOrthopedic.Diag\n0\n1\nFALSE\n2\nNo: 5728, Yes: 7\n\n\nrace\n0\n1\nFALSE\n3\nwhi: 4460, bla: 920, oth: 355\n\n\nincome\n0\n1\nFALSE\n4\nUnd: 3226, $11: 1165, $25: 893, &gt; $: 451\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedu\n0\n1\n11.68\n3.15\n0.00\n10.00\n12.00\n13.00\n30.00\n▁▇▃▁▁\n\n\nDASIndex\n0\n1\n20.50\n5.32\n11.00\n16.06\n19.75\n23.43\n33.00\n▃▇▆▂▃\n\n\nAPACHE.score\n0\n1\n54.67\n19.96\n3.00\n41.00\n54.00\n67.00\n147.00\n▂▇▅▁▁\n\n\nGlasgow.Coma.Score\n0\n1\n21.00\n30.27\n0.00\n0.00\n0.00\n41.00\n100.00\n▇▂▂▁▁\n\n\nblood.pressure\n0\n1\n78.52\n38.05\n0.00\n50.00\n63.00\n115.00\n259.00\n▆▇▆▁▁\n\n\nWBC\n0\n1\n15.65\n11.87\n0.00\n8.40\n14.10\n20.05\n192.00\n▇▁▁▁▁\n\n\nHeart.rate\n0\n1\n115.18\n41.24\n0.00\n97.00\n124.00\n141.00\n250.00\n▁▂▇▂▁\n\n\nRespiratory.rate\n0\n1\n28.09\n14.08\n0.00\n14.00\n30.00\n38.00\n100.00\n▅▇▂▁▁\n\n\nTemperature\n0\n1\n37.62\n1.77\n27.00\n36.09\n38.09\n39.00\n43.00\n▁▁▅▇▁\n\n\nPaO2vs.FIO2\n0\n1\n222.27\n114.95\n11.60\n133.31\n202.50\n316.62\n937.50\n▇▇▁▁▁\n\n\nAlbumin\n0\n1\n3.09\n0.78\n0.30\n2.60\n3.50\n3.50\n29.00\n▇▁▁▁▁\n\n\nHematocrit\n0\n1\n31.87\n8.36\n2.00\n26.10\n30.00\n36.30\n66.19\n▁▆▇▃▁\n\n\nBilirubin\n0\n1\n2.27\n4.80\n0.10\n0.80\n1.01\n1.40\n58.20\n▇▁▁▁▁\n\n\nCreatinine\n0\n1\n2.13\n2.05\n0.10\n1.00\n1.50\n2.40\n25.10\n▇▁▁▁▁\n\n\nSodium\n0\n1\n136.77\n7.66\n101.00\n132.00\n136.00\n142.00\n178.00\n▁▂▇▁▁\n\n\nPotassium\n0\n1\n4.07\n1.03\n1.10\n3.40\n3.80\n4.60\n11.90\n▂▇▁▁▁\n\n\nPaCo2\n0\n1\n38.75\n13.18\n1.00\n31.00\n37.00\n42.00\n156.00\n▃▇▁▁▁\n\n\nPH\n0\n1\n7.39\n0.11\n6.58\n7.34\n7.40\n7.46\n7.77\n▁▁▂▇▁\n\n\nWeight\n0\n1\n67.83\n29.06\n0.00\n56.30\n70.00\n83.70\n244.00\n▂▇▁▁▁\n\n\nLength.of.Stay\n0\n1\n21.56\n25.87\n2.00\n7.00\n14.00\n25.00\n394.00\n▇▁▁▁▁\n\n\nRHC.use\n0\n1\n0.38\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\n\n\n\nPredictive vs. causal models\nThe focus of current document is predictive models (e.g., predicting a health outcome).\n\n\n\n\n\n\n\n\nThe original article by Connors et al. (1996) focused on the association of\n\n\nConnors et al. (1996)\n\nright heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and\nthe health-outcomes (such as length of stay).\n\n\n\n\n\n\n\n\n\nThen the PICOT table changes as follows:\n\n\nAspect\nDescription\n\n\n\nP\nPatients who are critically ill\n\n\nI\nReceiving a right heart catheterization (RHC)\n\n\nC\nNot receiving a right heart catheterization (RHC)\n\n\nO\nlength of stay\n\n\nT\nBetween 1989 to 1994 (see the JAMA paper)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.",
    "crumbs": [
      "Research questions",
      "Predictive question-1"
    ]
  },
  {
    "objectID": "researchquestion2a.html",
    "href": "researchquestion2a.html",
    "title": "Predictive question-2a",
    "section": "",
    "text": "Working with a Predictive question using NHANES\nPart 1: Identify, download and merge necessary data:\nThe tutorial focuses on building a predictive model for Diastolic blood pressure in the U.S. population for the years 2013-14. It provides a step-by-step guide on how to use R for data manipulation and analysis, covering the initial setup of the R environment, identification of relevant covariates like age, sex, and lifestyle factors, and methods to search and import these variables from the NHANES dataset. Following data importation, subsets of relevant variables are merged into a single analytic dataset, which is then saved for future research. The tutorial also includes an exercise.",
    "crumbs": [
      "Research questions",
      "Predictive question-2a"
    ]
  },
  {
    "objectID": "researchquestion2a.html#saving-data-for-later-use",
    "href": "researchquestion2a.html#saving-data-for-later-use",
    "title": "Predictive question-2a",
    "section": "Saving data for later use",
    "text": "Saving data for later use\nIt’s a good practice to save your data for future reference.\n\nsave(analytic.data, file=\"Data/researchquestion/Analytic2013.RData\")",
    "crumbs": [
      "Research questions",
      "Predictive question-2a"
    ]
  },
  {
    "objectID": "researchquestion2a.html#exercise-try-yourself",
    "href": "researchquestion2a.html#exercise-try-yourself",
    "title": "Predictive question-2a",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nFollow the steps in the exercise section to deepen your understanding and broaden the analysis.\n\nThe following variables were not included in the above analysis, that were included in this paper: try including them and then create the new analytic data:\n\n\neducation level\npoverty income ratio\nSodium intake (mg)\nPotassium intake (mg)\n\n\nDownload the NHANES 2015-2016 and append with the NHANES 2013-2014 analytic data with same variables.",
    "crumbs": [
      "Research questions",
      "Predictive question-2a"
    ]
  },
  {
    "objectID": "researchquestion2a.html#references",
    "href": "researchquestion2a.html#references",
    "title": "Predictive question-2a",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54.",
    "crumbs": [
      "Research questions",
      "Predictive question-2a"
    ]
  },
  {
    "objectID": "researchquestion2b.html",
    "href": "researchquestion2b.html",
    "title": "Predictive question-2b",
    "section": "",
    "text": "Working with a Predictive question using NHANES\nPart 2: Analysis of downloaded data:\nThis tutorial provides a comprehensive guide to NHANES data preparation and initial analysis using R. The tutorial covers topics such as loading dataset, variable recoding, data summary statistics, and various types of regression analyses, including bivariate and multivariate models. It also delves into dealing with missing data, first by omitting NA values for a complete case analysis and then using a simple imputation method. The guide is designed to walk the reader through each step of data manipulation and analysis, with a focus on avoiding common pitfalls in statistical analysis.",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion2b.html#saving-for-further-use",
    "href": "researchquestion2b.html#saving-for-further-use",
    "title": "Predictive question-2b",
    "section": "Saving for further use",
    "text": "Saving for further use\n\nsave(analytic.data1, \n     file = \"Data/researchquestion/NHANESanalytic.Rdata\")",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion2b.html#regression-summary-optional",
    "href": "researchquestion2b.html#regression-summary-optional",
    "title": "Predictive question-2b",
    "section": "Regression summary (Optional)",
    "text": "Regression summary (Optional)\n\n\nThis is optional content for this chapter. Later in confounding and predictive factor chapters, we will learn more about adjustment.\nDifferent Generalized Linear Models (GLMs) are fit for diastolic blood pressure using variables like gender, marital status, etc. Below, we used GLMs with the Gaussian family for the continuous outcome diastolic. Note that gaussian is the default family for the glm function. Some other family options include binomial, poisson, quasibinomial, and so on.\nBivariate Regression summary (missing values included)\n\nfit1g &lt;- glm(diastolic ~ gender, data = analytic.data1)\nsummary(fit1g)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = diastolic ~ gender, data = analytic.data1)\n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   71.5789     0.2352 304.299  &lt; 2e-16 ***\n#&gt; genderFemale  -2.4880     0.3278  -7.591 3.76e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 136.3911)\n#&gt; \n#&gt;     Null deviance: 700862  on 5082  degrees of freedom\n#&gt; Residual deviance: 693003  on 5081  degrees of freedom\n#&gt;   (686 observations deleted due to missingness)\n#&gt; AIC: 39415\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n\n\nfit1m &lt;- glm(diastolic ~ marital, data=analytic.data1)\nsummary(fit1m)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = diastolic ~ marital, data = analytic.data1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                70.7500     0.2138 330.901  &lt; 2e-16 ***\n#&gt; maritalNever married       -1.9116     0.4316  -4.429 9.69e-06 ***\n#&gt; maritalPreviously married  -0.3953     0.4140  -0.955     0.34    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 137.5101)\n#&gt; \n#&gt;     Null deviance: 700840  on 5079  degrees of freedom\n#&gt; Residual deviance: 698139  on 5077  degrees of freedom\n#&gt;   (689 observations deleted due to missingness)\n#&gt; AIC: 39434\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n\n\nstr(analytic.data1)\n#&gt; 'data.frame':    5769 obs. of  14 variables:\n#&gt;  $ id         : num  73557 73558 73559 73561 73562 ...\n#&gt;  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#&gt;  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#&gt;  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#&gt;  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#&gt;  $ systolic   : num  122 156 140 136 160 118 NA 128 140 106 ...\n#&gt;  $ diastolic  : num  72 62 90 86 84 80 NA 74 78 60 ...\n#&gt;  $ race       : Factor w/ 6 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#&gt;  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#&gt;  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#&gt;  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#&gt;  $ alcohol    : num  1 4 NA NA 1 1 NA 1 3 2 ...\n#&gt;  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 NA 3 NA 3 1 1 NA ...\n#&gt;  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nfit13 &lt;- glm(diastolic ~ gender + age.centred + race + \n               marital + systolic + smoke + alcohol, \n             data = analytic.data1)\nsummary(fit13)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = diastolic ~ gender + age.centred + race + marital + \n#&gt;     systolic + smoke + alcohol, data = analytic.data1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                                         Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)                             30.75357    2.44749  12.565  &lt; 2e-16\n#&gt; genderFemale                            -0.25974    0.59910  -0.434 0.664679\n#&gt; age.centred                             -0.13838    0.02142  -6.461  1.4e-10\n#&gt; raceNon-Hispanic Black                   1.40049    1.11141   1.260 0.207828\n#&gt; raceNon-Hispanic White                   0.57654    0.96012   0.600 0.548273\n#&gt; raceOther Hispanic                       1.06047    1.29646   0.818 0.413500\n#&gt; raceOther race                           3.58464    1.43338   2.501 0.012496\n#&gt; raceOther Race - Including Multi-Racial -0.15288    1.60687  -0.095 0.924216\n#&gt; maritalNever married                    -2.91627    0.79034  -3.690 0.000232\n#&gt; maritalPreviously married                0.55582    0.72012   0.772 0.440333\n#&gt; systolic                                 0.31186    0.01762  17.701  &lt; 2e-16\n#&gt; smokeSome days                          -0.44535    0.97748  -0.456 0.648735\n#&gt; smokeNot at all                         -0.09063    0.65288  -0.139 0.889618\n#&gt; alcohol                                  0.18556    0.10998   1.687 0.091752\n#&gt;                                            \n#&gt; (Intercept)                             ***\n#&gt; genderFemale                               \n#&gt; age.centred                             ***\n#&gt; raceNon-Hispanic Black                     \n#&gt; raceNon-Hispanic White                     \n#&gt; raceOther Hispanic                         \n#&gt; raceOther race                          *  \n#&gt; raceOther Race - Including Multi-Racial    \n#&gt; maritalNever married                    ***\n#&gt; maritalPreviously married                  \n#&gt; systolic                                ***\n#&gt; smokeSome days                             \n#&gt; smokeNot at all                            \n#&gt; alcohol                                 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 117.4455)\n#&gt; \n#&gt;     Null deviance: 219477  on 1515  degrees of freedom\n#&gt; Residual deviance: 176403  on 1502  degrees of freedom\n#&gt;   (4253 observations deleted due to missingness)\n#&gt; AIC: 11543\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion2b.html#check-missingness-optional",
    "href": "researchquestion2b.html#check-missingness-optional",
    "title": "Predictive question-2b",
    "section": "Check missingness (optional)",
    "text": "Check missingness (optional)\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nThe plot_missing() function from the DataExplorer package is used to plot missing data.\n\nrequire(DataExplorer)\nplot_missing(analytic.data1)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\n\nrequire(\"tableone\")\nvars = c(\"systolic\", \"smoke\", \"diastolic\", \"race\", \n         \"age.centred\", \"gender\", \"marital\", \"alcohol\")\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#&gt;                                         \n#&gt;                                          Overall       \n#&gt;   n                                        5769        \n#&gt;   systolic (mean (SD))                   123.16 (18.12)\n#&gt;   smoke (%)                                            \n#&gt;      Every day                              965 (16.7) \n#&gt;      Some days                              229 ( 4.0) \n#&gt;      Not at all                            1336 (23.2) \n#&gt;      NA                                    3239 (56.1) \n#&gt;   diastolic (mean (SD))                   70.30 (11.74)\n#&gt;   race (%)                                             \n#&gt;      Mexican American                       767 (13.3) \n#&gt;      Non-Hispanic Black                    1177 (20.4) \n#&gt;      Non-Hispanic White                    2472 (42.8) \n#&gt;      Other Hispanic                         508 ( 8.8) \n#&gt;      Other race                             667 (11.6) \n#&gt;      Other Race - Including Multi-Racial    178 ( 3.1) \n#&gt;   age.centred (mean (SD))                  0.00 (17.56)\n#&gt;   gender = Female (%)                      3011 (52.2) \n#&gt;   marital (%)                                          \n#&gt;      Married                               3382 (58.6) \n#&gt;      Never married                         1112 (19.3) \n#&gt;      Previously married                    1272 (22.0) \n#&gt;      NA                                       3 ( 0.1) \n#&gt;   alcohol (mean (SD))                      2.65 (2.34)\n\nSetting correct variable types\nThe variables are explicitly set to either numeric or factor types.\nNote: In case any of the variables types are wrong, your table 1 output will be wrong. Better to be sure about what type of variable you want them to be (numeric or factor). For example, systolic should be numeric. Is it defined that way?\n\nmode(analytic.data1$systolic)\n#&gt; [1] \"numeric\"\n\nIn case it wasn’t (often they can get converted to character), then here is the solution:\n\n# solution 1: one-by-one\nanalytic.data1$systolic &lt;- \n  as.numeric(as.character(analytic.data1$systolic))\nsummary(analytic.data1$systolic)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    66.0   110.0   120.0   123.2   134.0   228.0     658\n\n\n# solution 2: fixing all variable types at once\nnumeric.names &lt;- c(\"systolic\", \"diastolic\", \n                   \"age.centred\", \"alcohol\")\nfactor.names &lt;- vars[!vars %in% numeric.names]\nfactor.names\n#&gt; [1] \"smoke\"   \"race\"    \"gender\"  \"marital\"\nanalytic.data1[,factor.names] &lt;- \n  lapply(analytic.data1[,factor.names] , factor)\nanalytic.data1[numeric.names] &lt;- \n  apply(X = analytic.data1[numeric.names], MARGIN = 2, \n        FUN =function (x) as.numeric(as.character(x)))\nlevels(analytic.data1$marital)\n#&gt; [1] \"Married\"            \"Never married\"      \"Previously married\"\nCreateTableOne(data = analytic.data1, includeNA = TRUE, \n               vars = vars)\n#&gt;                                         \n#&gt;                                          Overall       \n#&gt;   n                                        5769        \n#&gt;   systolic (mean (SD))                   123.16 (18.12)\n#&gt;   smoke (%)                                            \n#&gt;      Every day                              965 (16.7) \n#&gt;      Some days                              229 ( 4.0) \n#&gt;      Not at all                            1336 (23.2) \n#&gt;      NA                                    3239 (56.1) \n#&gt;   diastolic (mean (SD))                   70.30 (11.74)\n#&gt;   race (%)                                             \n#&gt;      Mexican American                       767 (13.3) \n#&gt;      Non-Hispanic Black                    1177 (20.4) \n#&gt;      Non-Hispanic White                    2472 (42.8) \n#&gt;      Other Hispanic                         508 ( 8.8) \n#&gt;      Other race                             667 (11.6) \n#&gt;      Other Race - Including Multi-Racial    178 ( 3.1) \n#&gt;   age.centred (mean (SD))                  0.00 (17.56)\n#&gt;   gender = Female (%)                      3011 (52.2) \n#&gt;   marital (%)                                          \n#&gt;      Married                               3382 (58.6) \n#&gt;      Never married                         1112 (19.3) \n#&gt;      Previously married                    1272 (22.0) \n#&gt;      NA                                       3 ( 0.1) \n#&gt;   alcohol (mean (SD))                      2.65 (2.34)\n\nComplete case analysis\nRemoves all rows containing NA.\n\ndim(analytic.data1)\n#&gt; [1] 5769   14\nanalytic.data2 &lt;- as.data.frame(na.omit(analytic.data1))\ndim(analytic.data2)\n#&gt; [1] 1516   14\nplot_missing(analytic.data2)\n\n\n\n\n\n\n\n\ntab1 &lt;- CreateTableOne(data = analytic.data2, includeNA = TRUE, \n               vars = vars)\nprint(tab1)\n#&gt;                                         \n#&gt;                                          Overall       \n#&gt;   n                                        1516        \n#&gt;   systolic (mean (SD))                   123.29 (17.58)\n#&gt;   smoke (%)                                            \n#&gt;      Every day                              590 (38.9) \n#&gt;      Some days                              159 (10.5) \n#&gt;      Not at all                             767 (50.6) \n#&gt;   diastolic (mean (SD))                   70.11 (12.04)\n#&gt;   race (%)                                             \n#&gt;      Mexican American                       162 (10.7) \n#&gt;      Non-Hispanic Black                     292 (19.3) \n#&gt;      Non-Hispanic White                     778 (51.3) \n#&gt;      Other Hispanic                         126 ( 8.3) \n#&gt;      Other race                              92 ( 6.1) \n#&gt;      Other Race - Including Multi-Racial     66 ( 4.4) \n#&gt;   age.centred (mean (SD))                 -0.76 (16.71)\n#&gt;   gender = Female (%)                       626 (41.3) \n#&gt;   marital (%)                                          \n#&gt;      Married                                858 (56.6) \n#&gt;      Never married                          300 (19.8) \n#&gt;      Previously married                     358 (23.6) \n#&gt;   alcohol (mean (SD))                      3.15 (2.76)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\nWe can export the tableone to a csv file as follows:\n\ntabl1p &lt;- print(tab1)\n#&gt;                                         \n#&gt;                                          Overall       \n#&gt;   n                                        1516        \n#&gt;   systolic (mean (SD))                   123.29 (17.58)\n#&gt;   smoke (%)                                            \n#&gt;      Every day                              590 (38.9) \n#&gt;      Some days                              159 (10.5) \n#&gt;      Not at all                             767 (50.6) \n#&gt;   diastolic (mean (SD))                   70.11 (12.04)\n#&gt;   race (%)                                             \n#&gt;      Mexican American                       162 (10.7) \n#&gt;      Non-Hispanic Black                     292 (19.3) \n#&gt;      Non-Hispanic White                     778 (51.3) \n#&gt;      Other Hispanic                         126 ( 8.3) \n#&gt;      Other race                              92 ( 6.1) \n#&gt;      Other Race - Including Multi-Racial     66 ( 4.4) \n#&gt;   age.centred (mean (SD))                 -0.76 (16.71)\n#&gt;   gender = Female (%)                       626 (41.3) \n#&gt;   marital (%)                                          \n#&gt;      Married                                858 (56.6) \n#&gt;      Never married                          300 (19.8) \n#&gt;      Previously married                     358 (23.6) \n#&gt;   alcohol (mean (SD))                      3.15 (2.76)\nwrite.csv(tabl1p, file = \"Data/researchquestion/table1.csv\")\n\n\nfit23 &lt;- glm(diastolic ~ gender + age.centred + race + \n               marital + systolic + smoke + alcohol, \n             data = analytic.data2)\nrequire(Publish)\npublish(fit23)\n#&gt;     Variable                               Units Coefficient         CI.95     p-value \n#&gt;  (Intercept)                                           30.75 [25.96;35.55]     &lt; 1e-04 \n#&gt;       gender                                Male         Ref                           \n#&gt;                                           Female       -0.26  [-1.43;0.91]   0.6646785 \n#&gt;  age.centred                                           -0.14 [-0.18;-0.10]     &lt; 1e-04 \n#&gt;         race                    Mexican American         Ref                           \n#&gt;                               Non-Hispanic Black        1.40  [-0.78;3.58]   0.2078278 \n#&gt;                               Non-Hispanic White        0.58  [-1.31;2.46]   0.5482728 \n#&gt;                                   Other Hispanic        1.06  [-1.48;3.60]   0.4134996 \n#&gt;                                       Other race        3.58   [0.78;6.39]   0.0124960 \n#&gt;              Other Race - Including Multi-Racial       -0.15  [-3.30;3.00]   0.9242163 \n#&gt;      marital                             Married         Ref                           \n#&gt;                                    Never married       -2.92 [-4.47;-1.37]   0.0002324 \n#&gt;                               Previously married        0.56  [-0.86;1.97]   0.4403334 \n#&gt;     systolic                                            0.31   [0.28;0.35]     &lt; 1e-04 \n#&gt;        smoke                           Every day         Ref                           \n#&gt;                                        Some days       -0.45  [-2.36;1.47]   0.6487351 \n#&gt;                                       Not at all       -0.09  [-1.37;1.19]   0.8896177 \n#&gt;      alcohol                                            0.19  [-0.03;0.40]   0.0917518\n\nImputed data\nWe will learn about proper missing data analysis at a latter class. Currently, we will do a simple (but rather controversial) single imputation. In here we are simply using a random sampling to impute (probably the worst method, but we are just filling in some gaps for now).\n\nrequire(mice)\nimputation1 &lt;- mice(analytic.data1,\n                   method = \"sample\",  \n                   m = 1, # Number of multiple imputations\n                   maxit = 1 # #iteration; mostly useful for convergence\n                   )\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  systolic  diastolic  marital  alcohol  smoke\n#&gt; Warning: Number of logged events: 5\nanalytic.data.imputation1 &lt;- complete(imputation1)\ndim(analytic.data.imputation1)\n#&gt; [1] 5769   14\nstr(analytic.data.imputation1)\n#&gt; 'data.frame':    5769 obs. of  14 variables:\n#&gt;  $ id         : num  73557 73558 73559 73561 73562 ...\n#&gt;  $ w.all      : num  13281 23682 57215 63710 24978 ...\n#&gt;  $ w.MEC      : num  13481 24472 57193 65542 25345 ...\n#&gt;  $ PSU        : num  1 1 1 2 1 1 2 1 2 2 ...\n#&gt;  $ STRATA     : num  112 108 109 116 111 114 106 112 112 113 ...\n#&gt;  $ systolic   : num  122 156 140 136 160 118 134 128 140 106 ...\n#&gt;  $ diastolic  : num  72 62 90 86 84 80 86 74 78 60 ...\n#&gt;  $ race       : Factor w/ 6 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n#&gt;  $ age.centred: num  19.89 4.89 22.89 23.89 6.89 ...\n#&gt;  $ gender     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n#&gt;  $ marital    : Factor w/ 3 levels \"Married\",\"Never married\",..: 3 1 1 1 3 3 1 3 3 2 ...\n#&gt;  $ alcohol    : num  1 4 2 3 1 1 4 1 3 2 ...\n#&gt;  $ smoke      : Factor w/ 3 levels \"Every day\",\"Some days\",..: 3 2 3 2 3 1 3 1 1 1 ...\n#&gt;  $ age.cat    : Factor w/ 3 levels \"[-Inf,20)\",\"[20,50)\",..: 3 3 3 3 3 3 2 3 3 2 ...\nplot_missing(analytic.data.imputation1)\n\n\n\n\n\n\n\n\nCreateTableOne(data = analytic.data.imputation1, includeNA = TRUE,\n               vars = vars)\n#&gt;                                         \n#&gt;                                          Overall       \n#&gt;   n                                        5769        \n#&gt;   systolic (mean (SD))                   123.13 (18.09)\n#&gt;   smoke (%)                                            \n#&gt;      Every day                             2120 (36.7) \n#&gt;      Some days                              527 ( 9.1) \n#&gt;      Not at all                            3122 (54.1) \n#&gt;   diastolic (mean (SD))                   70.36 (11.76)\n#&gt;   race (%)                                             \n#&gt;      Mexican American                       767 (13.3) \n#&gt;      Non-Hispanic Black                    1177 (20.4) \n#&gt;      Non-Hispanic White                    2472 (42.8) \n#&gt;      Other Hispanic                         508 ( 8.8) \n#&gt;      Other race                             667 (11.6) \n#&gt;      Other Race - Including Multi-Racial    178 ( 3.1) \n#&gt;   age.centred (mean (SD))                  0.00 (17.56)\n#&gt;   gender = Female (%)                      3011 (52.2) \n#&gt;   marital (%)                                          \n#&gt;      Married                               3383 (58.6) \n#&gt;      Never married                         1114 (19.3) \n#&gt;      Previously married                    1272 (22.0) \n#&gt;   alcohol (mean (SD))                      2.65 (2.37)\n# For categorical variables, try to see if \n# any categories have 0% or 100% frequency.\n# If yes, those may create problem in further analysis.\n\n\nfit23i &lt;- glm(diastolic ~ gender + age.centred + race + \n                marital + systolic + smoke + alcohol, \n              data = analytic.data.imputation1)\npublish(fit23i)\n#&gt;     Variable                               Units Coefficient         CI.95  p-value \n#&gt;  (Intercept)                                           39.49 [37.10;41.88]   &lt;1e-04 \n#&gt;       gender                                Male         Ref                        \n#&gt;                                           Female       -1.27 [-1.85;-0.69]   &lt;1e-04 \n#&gt;  age.centred                                           -0.12 [-0.14;-0.10]   &lt;1e-04 \n#&gt;         race                    Mexican American         Ref                        \n#&gt;                               Non-Hispanic Black        0.74  [-0.27;1.75]   0.1520 \n#&gt;                               Non-Hispanic White        0.53  [-0.36;1.42]   0.2460 \n#&gt;                                   Other Hispanic        0.98  [-0.24;2.21]   0.1160 \n#&gt;                                       Other race        2.27   [1.13;3.41]   &lt;1e-04 \n#&gt;              Other Race - Including Multi-Racial       -0.27  [-2.05;1.52]   0.7681 \n#&gt;      marital                             Married         Ref                        \n#&gt;                                    Never married       -2.30 [-3.11;-1.50]   &lt;1e-04 \n#&gt;                               Previously married       -0.09  [-0.84;0.65]   0.8029 \n#&gt;     systolic                                            0.25   [0.24;0.27]   &lt;1e-04 \n#&gt;        smoke                           Every day         Ref                        \n#&gt;                                        Some days       -0.12  [-1.16;0.93]   0.8257 \n#&gt;                                       Not at all       -0.24  [-0.85;0.38]   0.4502 \n#&gt;      alcohol                                            0.04  [-0.08;0.16]   0.4989\n\nWe see some changes in the estimates. After imputing compared to complete case analysis, any changes dramatic (e.g., changing conclusion)?\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\n\nrequire(jtools)\nrequire(ggstance)\nrequire(broom.mixed)\nrequire(huxtable)\nexport_summs(fit23, fit23i)\n\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n(Intercept)\n30.75 ***\n39.49 ***\n\n\n\n(2.45)   \n(1.22)   \n\n\ngenderFemale\n-0.26    \n-1.27 ***\n\n\n\n(0.60)   \n(0.30)   \n\n\nage.centred\n-0.14 ***\n-0.12 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nraceNon-Hispanic Black\n1.40    \n0.74    \n\n\n\n(1.11)   \n(0.51)   \n\n\nraceNon-Hispanic White\n0.58    \n0.53    \n\n\n\n(0.96)   \n(0.45)   \n\n\nraceOther Hispanic\n1.06    \n0.98    \n\n\n\n(1.30)   \n(0.63)   \n\n\nraceOther race\n3.58 *  \n2.27 ***\n\n\n\n(1.43)   \n(0.58)   \n\n\nraceOther Race - Including Multi-Racial\n-0.15    \n-0.27    \n\n\n\n(1.61)   \n(0.91)   \n\n\nmaritalNever married\n-2.92 ***\n-2.30 ***\n\n\n\n(0.79)   \n(0.41)   \n\n\nmaritalPreviously married\n0.56    \n-0.09    \n\n\n\n(0.72)   \n(0.38)   \n\n\nsystolic\n0.31 ***\n0.25 ***\n\n\n\n(0.02)   \n(0.01)   \n\n\nsmokeSome days\n-0.45    \n-0.12    \n\n\n\n(0.98)   \n(0.53)   \n\n\nsmokeNot at all\n-0.09    \n-0.24    \n\n\n\n(0.65)   \n(0.31)   \n\n\nalcohol\n0.19    \n0.04    \n\n\n\n(0.11)   \n(0.06)   \n\n\nN\n1516       \n5769       \n\n\nAIC\n11543.37    \n43964.73    \n\n\nBIC\n11623.23    \n44064.63    \n\n\nPseudo R2\n0.20    \n0.14    \n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\nplot_summs(fit23, fit23i)\n\n\n\n\n\n\n# plot_summs(fit23, fit23i, plot.distributions = TRUE)",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion2b.html#exercise-try-yourself",
    "href": "researchquestion2b.html#exercise-try-yourself",
    "title": "Predictive question-2b",
    "section": "Exercise (try yourself)",
    "text": "Exercise (try yourself)\nIn this lab, we have done multiple steps that could be improved. One of them was single imputation by random sampling. What other ad hoc method you could use to impute the factor variables?",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion2b.html#references",
    "href": "researchquestion2b.html#references",
    "title": "Predictive question-2b",
    "section": "References",
    "text": "References\n\n\n\n\nLi, Meng, Shoumeng Yan, Xing Li, Shan Jiang, Xiaoyu Ma, Hantong Zhao, Jiagen Li, et al. 2020. “Association Between Blood Pressure and Dietary Intakes of Sodium and Potassium Among US Adults Using Quantile Regression Analysis NHANES 2007–2014.” Journal of Human Hypertension 34 (5): 346–54.",
    "crumbs": [
      "Research questions",
      "Predictive question-2b"
    ]
  },
  {
    "objectID": "researchquestion3.html",
    "href": "researchquestion3.html",
    "title": "Causal question-1",
    "section": "",
    "text": "Working with a Predictive question using CCHS",
    "crumbs": [
      "Research questions",
      "Causal question-1"
    ]
  },
  {
    "objectID": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "href": "researchquestion3.html#naive-analysis-of-combined-3-cycles",
    "title": "Causal question-1",
    "section": "Naive Analysis of combined 3 cycles",
    "text": "Naive Analysis of combined 3 cycles\nIn the current analysis, we will simply consider all of the variables under consideration as ‘confounders’, and include in our analysis. Later we will perform a refined analysis.\nSummary of the analytic data\nIncluding missing values\n\ndim(c123sub3)\n#&gt; [1] 241380     17\nanalytic &lt;- c123sub3\ndim(analytic)\n#&gt; [1] 241380     17\n\nrequire(\"tableone\")\n#&gt; Loading required package: tableone\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, includeNA = TRUE)\n#&gt;                       \n#&gt;                        Overall       \n#&gt;   n                    241380        \n#&gt;   CVD = event (%)        7044 ( 2.9) \n#&gt;   age (%)                            \n#&gt;      20-39 years       108161 (44.8) \n#&gt;      40-49 years        59690 (24.7) \n#&gt;      50-59 years        52685 (21.8) \n#&gt;      60-64 years        20844 ( 8.6) \n#&gt;   sex = Male (%)       114104 (47.3) \n#&gt;   income (%)                         \n#&gt;      $29,999 or less    48005 (19.9) \n#&gt;      $30,000-$49,999    49496 (20.5) \n#&gt;      $50,000-$79,999    61093 (25.3) \n#&gt;      $80,000 or more    57056 (23.6) \n#&gt;      NA                 25730 (10.7) \n#&gt;   race (%)                           \n#&gt;      Non-white          25840 (10.7) \n#&gt;      White             210307 (87.1) \n#&gt;      NA                  5233 ( 2.2) \n#&gt;   bmicat (%)                         \n#&gt;      Normal            103378 (42.8) \n#&gt;      Overweight        120423 (49.9) \n#&gt;      Underweight         8964 ( 3.7) \n#&gt;      NA                  8615 ( 3.6) \n#&gt;   phyact (%)                         \n#&gt;      Active             57033 (23.6) \n#&gt;      Inactive          117516 (48.7) \n#&gt;      Moderate           60164 (24.9) \n#&gt;      NA                  6667 ( 2.8) \n#&gt;   smoke (%)                          \n#&gt;      Current smoker     71321 (29.5) \n#&gt;      Former smoker      97845 (40.5) \n#&gt;      Never smoker       71397 (29.6) \n#&gt;      NA                   817 ( 0.3) \n#&gt;   fruit (%)                          \n#&gt;      0-3 daily serving  56256 (23.3) \n#&gt;      4-6 daily serving  96177 (39.8) \n#&gt;      6+ daily serving   45861 (19.0) \n#&gt;      NA                 43086 (17.8) \n#&gt;   painmed (%)                        \n#&gt;      No                 11141 ( 4.6) \n#&gt;      Yes                25743 (10.7) \n#&gt;      NA                204496 (84.7) \n#&gt;   ht (%)                             \n#&gt;      No                213432 (88.4) \n#&gt;      Yes                27592 (11.4) \n#&gt;      NA                   356 ( 0.1) \n#&gt;   copd (%)                           \n#&gt;      No                192608 (79.8) \n#&gt;      Yes                 1353 ( 0.6) \n#&gt;      NA                 47419 (19.6) \n#&gt;   diab (%)                           \n#&gt;      No                232486 (96.3) \n#&gt;      Yes                 8811 ( 3.7) \n#&gt;      NA                    83 ( 0.0) \n#&gt;   edu (%)                            \n#&gt;      &lt; 2ndary           37775 (15.6) \n#&gt;      2nd grad.          44376 (18.4) \n#&gt;      Other 2nd grad.    19273 ( 8.0) \n#&gt;      Post-2nd grad.    136031 (56.4) \n#&gt;      NA                  3925 ( 1.6)\nCreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic, strata = \"OA\", includeNA = TRUE)\n#&gt;                       Stratified by OA\n#&gt;                        Control        OA            p      test\n#&gt;   n                    221029         20351                    \n#&gt;   CVD = event (%)        5429 ( 2.5)   1615 ( 7.9)  &lt;0.001     \n#&gt;   age (%)                                           &lt;0.001     \n#&gt;      20-39 years       106003 (48.0)   2158 (10.6)             \n#&gt;      40-49 years        55569 (25.1)   4121 (20.2)             \n#&gt;      50-59 years        43706 (19.8)   8979 (44.1)             \n#&gt;      60-64 years        15751 ( 7.1)   5093 (25.0)             \n#&gt;   sex = Male (%)       107729 (48.7)   6375 (31.3)  &lt;0.001     \n#&gt;   income (%)                                        &lt;0.001     \n#&gt;      $29,999 or less    42019 (19.0)   5986 (29.4)             \n#&gt;      $30,000-$49,999    45090 (20.4)   4406 (21.7)             \n#&gt;      $50,000-$79,999    56754 (25.7)   4339 (21.3)             \n#&gt;      $80,000 or more    53637 (24.3)   3419 (16.8)             \n#&gt;      NA                 23529 (10.6)   2201 (10.8)             \n#&gt;   race (%)                                          &lt;0.001     \n#&gt;      Non-white          24681 (11.2)   1159 ( 5.7)             \n#&gt;      White             191513 (86.6)  18794 (92.3)             \n#&gt;      NA                  4835 ( 2.2)    398 ( 2.0)             \n#&gt;   bmicat (%)                                        &lt;0.001     \n#&gt;      Normal             96697 (43.7)   6681 (32.8)             \n#&gt;      Overweight        107871 (48.8)  12552 (61.7)             \n#&gt;      Underweight         8490 ( 3.8)    474 ( 2.3)             \n#&gt;      NA                  7971 ( 3.6)    644 ( 3.2)             \n#&gt;   phyact (%)                                        &lt;0.001     \n#&gt;      Active             52942 (24.0)   4091 (20.1)             \n#&gt;      Inactive          106580 (48.2)  10936 (53.7)             \n#&gt;      Moderate           55222 (25.0)   4942 (24.3)             \n#&gt;      NA                  6285 ( 2.8)    382 ( 1.9)             \n#&gt;   smoke (%)                                         &lt;0.001     \n#&gt;      Current smoker     65398 (29.6)   5923 (29.1)             \n#&gt;      Former smoker      88210 (39.9)   9635 (47.3)             \n#&gt;      Never smoker       66663 (30.2)   4734 (23.3)             \n#&gt;      NA                   758 ( 0.3)     59 ( 0.3)             \n#&gt;   fruit (%)                                         &lt;0.001     \n#&gt;      0-3 daily serving  52140 (23.6)   4116 (20.2)             \n#&gt;      4-6 daily serving  87951 (39.8)   8226 (40.4)             \n#&gt;      6+ daily serving   41606 (18.8)   4255 (20.9)             \n#&gt;      NA                 39332 (17.8)   3754 (18.4)             \n#&gt;   painmed (%)                                       &lt;0.001     \n#&gt;      No                 10624 ( 4.8)    517 ( 2.5)             \n#&gt;      Yes                23084 (10.4)   2659 (13.1)             \n#&gt;      NA                187321 (84.7)  17175 (84.4)             \n#&gt;   ht (%)                                            &lt;0.001     \n#&gt;      No                198550 (89.8)  14882 (73.1)             \n#&gt;      Yes                22142 (10.0)   5450 (26.8)             \n#&gt;      NA                   337 ( 0.2)     19 ( 0.1)             \n#&gt;   copd (%)                                          &lt;0.001     \n#&gt;      No                173224 (78.4)  19384 (95.2)             \n#&gt;      Yes                  938 ( 0.4)    415 ( 2.0)             \n#&gt;      NA                 46867 (21.2)    552 ( 2.7)             \n#&gt;   diab (%)                                          &lt;0.001     \n#&gt;      No                213910 (96.8)  18576 (91.3)             \n#&gt;      Yes                 7046 ( 3.2)   1765 ( 8.7)             \n#&gt;      NA                    73 ( 0.0)     10 ( 0.0)             \n#&gt;   edu (%)                                           &lt;0.001     \n#&gt;      &lt; 2ndary           32884 (14.9)   4891 (24.0)             \n#&gt;      2nd grad.          40950 (18.5)   3426 (16.8)             \n#&gt;      Other 2nd grad.    17808 ( 8.1)   1465 ( 7.2)             \n#&gt;      Post-2nd grad.    125772 (56.9)  10259 (50.4)             \n#&gt;      NA                  3615 ( 1.6)    310 ( 1.5)\nrequire(DataExplorer)\n#&gt; Loading required package: DataExplorer\nplot_missing(analytic)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nLet us investigate why pain medication has so much missing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional content respondent (cycle 3.1):\n\n\n\n\n\n\n\n\nIn cycle 2.1, only 21,755 out of 134,072 responded to optional medication component.\nComplete case analysis\n\ndim(c123sub3)\n#&gt; [1] 241380     17\nanalytic2 &lt;- as.data.frame(na.omit(c123sub3))\ndim(analytic2)\n#&gt; [1] 21623    17\n\n\ntab1 &lt;- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\", \n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, includeNA = TRUE)\nprint(tab1, showAllLevels = TRUE)\n#&gt;              \n#&gt;               level             Overall      \n#&gt;   n                             21623        \n#&gt;   CVD (%)     0 event           20917 (96.7) \n#&gt;               event               706 ( 3.3) \n#&gt;   age (%)     20-39 years        7119 (32.9) \n#&gt;               40-49 years        7024 (32.5) \n#&gt;               50-59 years        5457 (25.2) \n#&gt;               60-64 years        2023 ( 9.4) \n#&gt;   sex (%)     Female            10982 (50.8) \n#&gt;               Male              10641 (49.2) \n#&gt;   income (%)  $29,999 or less    4054 (18.7) \n#&gt;               $30,000-$49,999    4461 (20.6) \n#&gt;               $50,000-$79,999    6600 (30.5) \n#&gt;               $80,000 or more    6508 (30.1) \n#&gt;   race (%)    Non-white          2488 (11.5) \n#&gt;               White             19135 (88.5) \n#&gt;   bmicat (%)  Normal             8993 (41.6) \n#&gt;               Overweight        11739 (54.3) \n#&gt;               Underweight         891 ( 4.1) \n#&gt;   phyact (%)  Active             5502 (25.4) \n#&gt;               Inactive          10495 (48.5) \n#&gt;               Moderate           5626 (26.0) \n#&gt;   smoke (%)   Current smoker     5887 (27.2) \n#&gt;               Former smoker      9368 (43.3) \n#&gt;               Never smoker       6368 (29.5) \n#&gt;   fruit (%)   0-3 daily serving  5806 (26.9) \n#&gt;               4-6 daily serving 10730 (49.6) \n#&gt;               6+ daily serving   5087 (23.5) \n#&gt;   painmed (%) No                 6197 (28.7) \n#&gt;               Yes               15426 (71.3) \n#&gt;   ht (%)      No                19014 (87.9) \n#&gt;               Yes                2609 (12.1) \n#&gt;   copd (%)    No                21475 (99.3) \n#&gt;               Yes                 148 ( 0.7) \n#&gt;   diab (%)    No                20760 (96.0) \n#&gt;               Yes                 863 ( 4.0) \n#&gt;   edu (%)     &lt; 2ndary           2998 (13.9) \n#&gt;               2nd grad.          4605 (21.3) \n#&gt;               Other 2nd grad.    1509 ( 7.0) \n#&gt;               Post-2nd grad.    12511 (57.9)\ntab1b &lt;- CreateTableOne(vars = c(\"CVD\", \"age\", \n               \"sex\", \"income\", \"race\",\n               \"bmicat\", \"phyact\", \"smoke\", \"fruit\", \n               \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\"),\n               data = analytic2, strata = \"OA\", includeNA = TRUE)\nprint(tab1b, showAllLevels = TRUE)\n#&gt;              Stratified by OA\n#&gt;               level             Control       OA           p      test\n#&gt;   n                             19459         2164                    \n#&gt;   CVD (%)     0 event           18917 (97.2)  2000 (92.4)  &lt;0.001     \n#&gt;               event               542 ( 2.8)   164 ( 7.6)             \n#&gt;   age (%)     20-39 years        6915 (35.5)   204 ( 9.4)  &lt;0.001     \n#&gt;               40-49 years        6515 (33.5)   509 (23.5)             \n#&gt;               50-59 years        4504 (23.1)   953 (44.0)             \n#&gt;               60-64 years        1525 ( 7.8)   498 (23.0)             \n#&gt;   sex (%)     Female             9521 (48.9)  1461 (67.5)  &lt;0.001     \n#&gt;               Male               9938 (51.1)   703 (32.5)             \n#&gt;   income (%)  $29,999 or less    3413 (17.5)   641 (29.6)  &lt;0.001     \n#&gt;               $30,000-$49,999    3968 (20.4)   493 (22.8)             \n#&gt;               $50,000-$79,999    6023 (31.0)   577 (26.7)             \n#&gt;               $80,000 or more    6055 (31.1)   453 (20.9)             \n#&gt;   race (%)    Non-white          2370 (12.2)   118 ( 5.5)  &lt;0.001     \n#&gt;               White             17089 (87.8)  2046 (94.5)             \n#&gt;   bmicat (%)  Normal             8277 (42.5)   716 (33.1)  &lt;0.001     \n#&gt;               Overweight        10356 (53.2)  1383 (63.9)             \n#&gt;               Underweight         826 ( 4.2)    65 ( 3.0)             \n#&gt;   phyact (%)  Active             4986 (25.6)   516 (23.8)   0.190     \n#&gt;               Inactive           9417 (48.4)  1078 (49.8)             \n#&gt;               Moderate           5056 (26.0)   570 (26.3)             \n#&gt;   smoke (%)   Current smoker     5247 (27.0)   640 (29.6)  &lt;0.001     \n#&gt;               Former smoker      8363 (43.0)  1005 (46.4)             \n#&gt;               Never smoker       5849 (30.1)   519 (24.0)             \n#&gt;   fruit (%)   0-3 daily serving  5290 (27.2)   516 (23.8)  &lt;0.001     \n#&gt;               4-6 daily serving  9686 (49.8)  1044 (48.2)             \n#&gt;               6+ daily serving   4483 (23.0)   604 (27.9)             \n#&gt;   painmed (%) No                 5859 (30.1)   338 (15.6)  &lt;0.001     \n#&gt;               Yes               13600 (69.9)  1826 (84.4)             \n#&gt;   ht (%)      No                17356 (89.2)  1658 (76.6)  &lt;0.001     \n#&gt;               Yes                2103 (10.8)   506 (23.4)             \n#&gt;   copd (%)    No                19359 (99.5)  2116 (97.8)  &lt;0.001     \n#&gt;               Yes                 100 ( 0.5)    48 ( 2.2)             \n#&gt;   diab (%)    No                18751 (96.4)  2009 (92.8)  &lt;0.001     \n#&gt;               Yes                 708 ( 3.6)   155 ( 7.2)             \n#&gt;   edu (%)     &lt; 2ndary           2527 (13.0)   471 (21.8)  &lt;0.001     \n#&gt;               2nd grad.          4173 (21.4)   432 (20.0)             \n#&gt;               Other 2nd grad.    1364 ( 7.0)   145 ( 6.7)             \n#&gt;               Post-2nd grad.    11395 (58.6)  1116 (51.6)",
    "crumbs": [
      "Research questions",
      "Causal question-1"
    ]
  },
  {
    "objectID": "researchquestion3.html#save-data-for-later",
    "href": "researchquestion3.html#save-data-for-later",
    "title": "Causal question-1",
    "section": "Save data for later",
    "text": "Save data for later\n\nsave(analytic, analytic2, cc123a, file = \"Data/researchquestion/OA123CVD.RData\")",
    "crumbs": [
      "Research questions",
      "Causal question-1"
    ]
  },
  {
    "objectID": "researchquestion3.html#references",
    "href": "researchquestion3.html#references",
    "title": "Causal question-1",
    "section": "References",
    "text": "References\n\n\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624.",
    "crumbs": [
      "Research questions",
      "Causal question-1"
    ]
  },
  {
    "objectID": "researchquestion4.html",
    "href": "researchquestion4.html",
    "title": "Causal question-2",
    "section": "",
    "text": "Working with a causal question using NHANES\nWe are interested in exploring the relationship between diabetes (binary exposure variable defined as whether the doctor ever told the participant has diabetes) and cholesterol (binary outcome variable defined as whether total cholesterol is more than 200 mg/dL). Below is the PICOT:\n\n\nPICOT element\nDescription\n\n\n\nP\nUS adults\n\n\nI\nDiabetes\n\n\nC\nNo diabetes\n\n\nO\nTotal cholesterol &gt; 200 mg/dL\n\n\nT\n2017–2018\n\n\n\nFirst, we will prepare the analytic dataset from NHANES 2017–2018.\nSecond, we will work with subset of data to assess the association between diabetes and cholesterol, and to get proper SE and 95% CI for the estimate. We emphasize the correct usage of the survey’s design features (correct handling of survey design elements, such as stratification, clustering, and weighting) to obtain accurate population-level estimates.\n\n# Load required packages\nrequire(SASxport)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(nhanesA)\nrequire(survey)\nrequire(Publish)\nrequire(jtools)\n\nSteps for creating analytic dataset\nWe will combine multiple components (e.g., demographic, blood pressure) using the unique identifier to create our analytic dataset.\n\n\nWithin NHANES datasets in a given cycle, each sampled person has an unique identifier sequence number (variable SEQN).\nDownload and Subsetting to retain only the useful variables\nSearch literature for the relevant variables, and then see if some of them are available in the NHANES data.\n\n\nPeters, Fabian, and Levy (2014)\nAn an example, let us assume that variables listed in the following figures are known to be useful. Then we will try to indentify, in which NHANES component we have these variables.\n\n\nRefer to the earlier chapter to get a more detailed understanding of how we search for variables within NHANES.\n\n\n\n\n\n\n\n\n\n\nNHANES Data Components:\n\nDemographic (variables like age, gender, income, etc.)\nBlood Pressure (Diastolic and Systolic pressure)\nBody Measures (BMI, Waist Circumference, etc.)\nSmoking Status (Current smoker or not)\nCholesterol (Total cholesterol in different units)\nBiochemistry Profile (Triglycerides, Uric acid, etc.)\nPhysical Activity (Vigorous work and recreational activities)\nDiabetes (Whether the respondent has been told by a doctor that they have diabetes)\n\nDemographic component:\n\ndemo &lt;- nhanes('DEMO_J') # Both males and females 0 YEARS - 150 YEARS\ndemo &lt;- demo[c(\"SEQN\", # Respondent sequence number\n                 \"RIAGENDR\", # gender\n                 \"RIDAGEYR\", # Age in years at screening\n                 \"DMDBORN4\", # Country of birth\n                 \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                 \"DMDEDUC3\", # Education level - Children/Youth 6-19\n                 \"DMDEDUC2\", # Education level - Adults 20+\n                 \"DMDMARTL\", # Marital status: 20 YEARS - 150 YEARS\n                 \"INDHHIN2\", # Total household income\n                 \"WTMEC2YR\", \"SDMVPSU\", \"SDMVSTRA\")]\ndemo_vars &lt;- names(demo) # nhanesTableVars('DEMO', 'DEMO_J', namesonly=TRUE)\ndemo1 &lt;- nhanesTranslate('DEMO_J', demo_vars, data=demo)\n#&gt; Translated columns: RIAGENDR DMDBORN4 RIDRETH3 DMDEDUC3 DMDEDUC2 DMDMARTL INDHHIN2\n\nBlood pressure component:\n\nbpx &lt;- nhanes('BPX_J')\nbpx &lt;- bpx[c(\"SEQN\", # Respondent sequence number\n             \"BPXDI1\", #Diastolic: Blood pres (1st rdg) mm Hg\n             \"BPXSY1\" # Systolic: Blood pres (1st rdg) mm Hg\n             )]\nbpx_vars &lt;- names(bpx) \nbpx1 &lt;- nhanesTranslate('BPX_J', bpx_vars, data=bpx)\n#&gt; Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx): No columns were\n#&gt; translated\n\nBody measure component:\n\nbmi &lt;- nhanes('BMX_J')\nbmi &lt;- bmi[c(\"SEQN\", # Respondent sequence number\n               \"BMXWT\", # Weight (kg) \n               \"BMXHT\", # Standing Height (cm)\n               \"BMXBMI\", # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\n               #\"BMDBMIC\", # BMI Category - Children/Youth # 2 YEARS - 19 YEARS\n               \"BMXWAIST\" # Waist Circumference (cm): 2 YEARS - 150 YEARS\n               )]\nbmi_vars &lt;- names(bmi) \nbmi1 &lt;- nhanesTranslate('BMX_J', bmi_vars, data=bmi)\n#&gt; Warning in nhanesTranslate(\"BMX_J\", bmi_vars, data = bmi): No columns were\n#&gt; translated\n\nSmoking component:\n\nsmq &lt;- nhanes('SMQ_J')\nsmq &lt;- smq[c(\"SEQN\", # Respondent sequence number\n               \"SMQ040\" # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\n               )]\nsmq_vars &lt;- names(smq) \nsmq1 &lt;- nhanesTranslate('SMQ_J', smq_vars, data=smq)\n#&gt; Translated columns: SMQ040\n\n\n# alq &lt;- nhanes('ALQ_J')\n# alq &lt;- alq[c(\"SEQN\", # Respondent sequence number\n#                \"ALQ130\" # Avg # alcoholic drinks/day - past 12 mos\n#                # 18 YEARS - 150 YEARS\n#                )]\n# alq_vars &lt;- names(alq) \n# alq1 &lt;- nhanesTranslate('ALQ_J', alq_vars, data=alq)\n\nCholesterol component:\n\nchl &lt;- nhanes('TCHOL_J') # 6 YEARS - 150 YEARS\nchl &lt;- chl[c(\"SEQN\", # Respondent sequence number\n               \"LBXTC\", # Total Cholesterol (mg/dL)\n               \"LBDTCSI\" # Total Cholesterol (mmol/L)\n               )]\nchl_vars &lt;- names(chl) \nchl1 &lt;- nhanesTranslate('TCHOL_J', chl_vars, data=chl)\n#&gt; Warning in nhanesTranslate(\"TCHOL_J\", chl_vars, data = chl): No columns were\n#&gt; translated\n\nBiochemistry Profile component:\n\ntri &lt;- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\ntri &lt;- tri[c(\"SEQN\", # Respondent sequence number\n               \"LBXSTR\", # Triglycerides, refrig serum (mg/dL)\n               \"LBXSUA\", # Uric acid\n               \"LBXSTP\", # total Protein (g/dL)\n               \"LBXSTB\", # Total Bilirubin (mg/dL)\n               \"LBXSPH\", # Phosphorus (mg/dL)\n               \"LBXSNASI\", # Sodium (mmol/L)\n               \"LBXSKSI\", # Potassium (mmol/L)\n               \"LBXSGB\", # Globulin (g/dL)\n               \"LBXSCA\" # Total Calcium (mg/dL)\n               )]\ntri_vars &lt;- names(tri) \ntri1 &lt;- nhanesTranslate('BIOPRO_J', tri_vars, data=tri)\n#&gt; Warning in nhanesTranslate(\"BIOPRO_J\", tri_vars, data = tri): No columns were\n#&gt; translated\n\nPhysical activity component:\n\npaq &lt;- nhanes('PAQ_J')\npaq &lt;- paq[c(\"SEQN\", # Respondent sequence number\n               \"PAQ605\", # Vigorous work activity \n               \"PAQ650\" # Vigorous recreational activities\n               )]\npaq_vars &lt;- names(paq) \npaq1 &lt;- nhanesTranslate('PAQ_J', paq_vars, data=paq)\n#&gt; Translated columns: PAQ605 PAQ650\n\nDiabetes component:\n\ndiq &lt;- nhanes('DIQ_J')\ndiq &lt;- diq[c(\"SEQN\", # Respondent sequence number\n               \"DIQ010\" # Doctor told you have diabetes\n               )]\ndiq_vars &lt;- names(diq) \ndiq1 &lt;- nhanesTranslate('DIQ_J', diq_vars, data=diq)\n#&gt; Translated columns: DIQ010\n\nMerging all the datasets\n\n\n\n\n\n\nTip\n\n\n\nWe can use the merge or Reduce function to combine the datasets\n\n\n\nanalytic.data7 &lt;- Reduce(function(x,y) merge(x,y,by=\"SEQN\",all=TRUE) ,\n       list(demo1,bpx1,bmi1,smq1,chl1,tri1,paq1,diq1))\ndim(analytic.data7)\n#&gt; [1] 9254   33\n\n\n\nAll these datasets are merged into one analytic dataset using the SEQN as the key. This can be done either all at once using the Reduce function or one by one (using merge once at a time).\n\n# Merging one by one\n# analytic.data0 &lt;- merge(demo1, bpx1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data1 &lt;- merge(analytic.data0, bmi1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data2 &lt;- merge(analytic.data1, smq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data3 &lt;- merge(analytic.data2, alq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data4 &lt;- merge(analytic.data3, chl1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data5 &lt;- merge(analytic.data4, tri1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data6 &lt;- merge(analytic.data5, paq1, by = c(\"SEQN\"), all=TRUE)\n# analytic.data7 &lt;- merge(analytic.data6, diq1, by = c(\"SEQN\"), all=TRUE)\n# dim(analytic.data7)\n\nCheck Target population and avoid zero-cell cross-tabulation\n\n\nThe dataset is then filtered to only include adults (20 years and older) and avoid zero-cell cross-tabulation.\nSee that marital status variable was restricted to 20 YEARS - 150 YEARS.\n\nstr(analytic.data7)\n#&gt; 'data.frame':    9254 obs. of  33 variables:\n#&gt;  $ SEQN    : num  93703 93704 93705 93706 93707 ...\n#&gt;  $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 2 1 2 1 1 2 2 2 1 1 ...\n#&gt;  $ RIDAGEYR: num  2 2 66 18 13 66 75 0 56 18 ...\n#&gt;  $ DMDBORN4: Factor w/ 4 levels \"Born in 50 US states or Washington, DC\",..: 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;  $ RIDRETH3: Factor w/ 6 levels \"Mexican American\",..: 5 3 4 5 6 5 4 3 5 1 ...\n#&gt;  $ DMDEDUC3: Factor w/ 17 levels \"Never attended / kindergarten only\",..: NA NA NA 16 7 NA NA NA NA 13 ...\n#&gt;  $ DMDEDUC2: Factor w/ 7 levels \"Less than 9th grade\",..: NA NA 2 NA NA 1 4 NA 5 NA ...\n#&gt;  $ DMDMARTL: Factor w/ 7 levels \"Married\",\"Widowed\",..: NA NA 3 NA NA 1 2 NA 1 NA ...\n#&gt;  $ INDHHIN2: Factor w/ 16 levels \"$ 0 to $ 4,999\",..: 14 14 3 NA 10 6 2 14 14 4 ...\n#&gt;  $ WTMEC2YR: num  8540 42567 8338 8723 7065 ...\n#&gt;  $ SDMVPSU : num  2 1 2 2 1 2 1 1 2 2 ...\n#&gt;  $ SDMVSTRA: num  145 143 145 134 138 138 136 134 134 147 ...\n#&gt;  $ BPXDI1  : num  NA NA NA 74 38 NA 66 NA 68 68 ...\n#&gt;  $ BPXSY1  : num  NA NA NA 112 128 NA 120 NA 108 112 ...\n#&gt;  $ BMXWT   : num  13.7 13.9 79.5 66.3 45.4 53.5 88.8 10.2 62.1 58.9 ...\n#&gt;  $ BMXHT   : num  88.6 94.2 158.3 175.7 158.4 ...\n#&gt;  $ BMXBMI  : num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#&gt;  $ BMXWAIST: num  48.2 50 101.8 79.3 64.1 ...\n#&gt;  $ SMQ040  : Factor w/ 3 levels \"Every day\",\"Some days\",..: NA NA 3 NA NA NA 1 NA NA 2 ...\n#&gt;  $ LBXTC   : num  NA NA 157 148 189 209 176 NA 238 182 ...\n#&gt;  $ LBDTCSI : num  NA NA 4.06 3.83 4.89 5.4 4.55 NA 6.15 4.71 ...\n#&gt;  $ LBXSTR  : num  NA NA 95 92 110 72 132 NA 59 124 ...\n#&gt;  $ LBXSUA  : num  NA NA 5.8 8 5.5 4.5 6.2 NA 4.2 5.8 ...\n#&gt;  $ LBXSTP  : num  NA NA 7.3 7.1 8 7.1 7 NA 7.1 8.1 ...\n#&gt;  $ LBXSTB  : num  NA NA 0.6 0.7 0.7 0.5 0.3 NA 0.3 0.8 ...\n#&gt;  $ LBXSPH  : num  NA NA 4 4 4.3 3.3 3.5 NA 3.4 5.1 ...\n#&gt;  $ LBXSNASI: num  NA NA 141 144 137 144 141 NA 140 141 ...\n#&gt;  $ LBXSKSI : num  NA NA 4 4.4 3.3 4.4 4.1 NA 4.9 4.3 ...\n#&gt;  $ LBXSGB  : num  NA NA 2.9 2.7 2.8 3.2 3.3 NA 3.1 3.3 ...\n#&gt;  $ LBXSCA  : num  NA NA 9.2 9.6 10.1 9.5 9.9 NA 9.4 9.6 ...\n#&gt;  $ PAQ605  : Factor w/ 3 levels \"Yes\",\"No\",\"Don't know\": NA NA 2 2 NA 2 2 NA 2 1 ...\n#&gt;  $ PAQ650  : Factor w/ 2 levels \"Yes\",\"No\": NA NA 2 2 NA 2 2 NA 1 1 ...\n#&gt;  $ DIQ010  : Factor w/ 4 levels \"Yes\",\"No\",\"Borderline\",..: 2 2 2 2 2 3 2 NA 2 2 ...\nhead(analytic.data7)\n\n\n  \n\n\nsummary(analytic.data7$RIDAGEYR)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.00   11.00   31.00   34.33   58.00   80.00\n\n\ndim(analytic.data7)\n#&gt; [1] 9254   33\nanalytic.data8 &lt;- analytic.data7\nanalytic.data8$RIDAGEYR[analytic.data8$RIDAGEYR &lt; 20] &lt;- NA\n#analytic.data8 &lt;- subset(analytic.data7, RIDAGEYR &gt;= 20)\ndim(analytic.data8)\n#&gt; [1] 9254   33\n\nGet rid of variables where target was less than 20 years of age accordingly.\n\nanalytic.data8$DMDEDUC3 &lt;- NULL # not relevant for adults\n#analytic.data8$BMDBMIC &lt;- NULL # not relevant for adults\n\nGet rid of invalid responses\n\n\nVariables that have “Don’t Know” or “Refused” as responses are set to NA, effectively getting rid of invalid responses.\n\nfactor.names &lt;- c(\"RIAGENDR\",\"DMDBORN4\",\"RIDRETH3\",\n                  \"DMDEDUC2\",\"DMDMARTL\",\"INDHHIN2\", \n                  \"SMQ040\", \"PAQ605\", \"PAQ650\", \"DIQ010\")\nnumeric.names &lt;- c(\"SEQN\",\"RIDAGEYR\",\"WTMEC2YR\",\n                   \"SDMVPSU\", \"SDMVSTRA\",\n                   \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"BMXHT\",\n                   \"BMXBMI\", \"BMXWAIST\",\n                   \"ALQ130\", \"LBXTC\", \"LBDTCSI\", \n                   \"LBXSTR\", \"LBXSUA\", \"LBXSTP\", \"LBXSTB\", \n                   \"LBXSPH\", \"LBXSNASI\", \"LBXSKSI\",\n                   \"LBXSGB\",\"LBXSCA\")\nanalytic.data8[factor.names] &lt;- apply(X = analytic.data8[factor.names], \n                                      MARGIN = 2, FUN = as.factor)\n# analytic.data8[numeric.names] &lt;- apply(X = analytic.data8[numeric.names], \n#                                        MARGIN = 2, FUN = \n#                                          function (x) as.numeric(as.character(x)))\n\n\nanalytic.data9 &lt;- analytic.data8\nanalytic.data9$DMDBORN4[analytic.data9$DMDBORN4 == \"Don't Know\"] &lt;- NA\n#analytic.data9 &lt;- subset(analytic.data8, DMDBORN4 != \"Don't Know\")\ndim(analytic.data9)\n#&gt; [1] 9254   32\n\nanalytic.data10 &lt;- analytic.data9\nanalytic.data10$DMDEDUC2[analytic.data10$DMDEDUC2 == \"Don't Know\"] &lt;- NA\n#analytic.data10 &lt;- subset(analytic.data9, DMDEDUC2 != \"Don't Know\")\ndim(analytic.data10)\n#&gt; [1] 9254   32\n\nanalytic.data11 &lt;- analytic.data10\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Don't Know\"] &lt;- NA\nanalytic.data11$DMDMARTL[analytic.data11$DMDMARTL == \"Refused\"] &lt;- NA\n# analytic.data11 &lt;- subset(analytic.data10, DMDMARTL != \"Don't Know\" & DMDMARTL != \"Refused\")\ndim(analytic.data11)\n#&gt; [1] 9254   32\n\n\nanalytic.data12 &lt;- analytic.data11\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Don't Know\"] &lt;- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Refused\"] &lt;- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"Under $20,000\"] &lt;- NA\nanalytic.data12$INDHHIN2[analytic.data12$INDHHIN2 == \"$20,000 and Over\"] &lt;- NA\n# analytic.data12 &lt;- subset(analytic.data11, INDHHIN2 != \"Don't know\" & INDHHIN2 !=  \"Refused\" & INDHHIN2 != \"Under $20,000\" & INDHHIN2 != \"$20,000 and Over\" )\ndim(analytic.data12)\n#&gt; [1] 9254   32\n\n#analytic.data11 &lt;- subset(analytic.data10, ALQ130 != 777 & ALQ130 != 999 )\n#dim(analytic.data11) # this are listed as NA anyway\n\nanalytic.data13 &lt;- analytic.data12\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Don't know\"] &lt;- NA\nanalytic.data13$PAQ605[analytic.data13$PAQ605 == \"Refused\"] &lt;- NA\n# analytic.data13 &lt;- subset(analytic.data12, PAQ605 != \"Don't know\" & PAQ605 != \"Refused\")\ndim(analytic.data13)\n#&gt; [1] 9254   32\n\nanalytic.data14 &lt;- analytic.data13\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Don't know\"] &lt;- NA\nanalytic.data14$PAQ650[analytic.data14$PAQ650 == \"Refused\"] &lt;- NA\n# analytic.data14 &lt;- subset(analytic.data13, PAQ650 != \"Don't Know\" & PAQ650 != \"Refused\")\ndim(analytic.data14)\n#&gt; [1] 9254   32\n\nanalytic.data15 &lt;- analytic.data14\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Don't know\"] &lt;- NA\nanalytic.data15$DIQ010[analytic.data15$DIQ010 == \"Refused\"] &lt;- NA\n# analytic.data15 &lt;- subset(analytic.data14, DIQ010 != \"Don't Know\" & DIQ010 != \"Refused\")\ndim(analytic.data15)\n#&gt; [1] 9254   32\n\n\n# analytic.data15$ALQ130[analytic.data15$ALQ130 &gt; 100] &lt;- NA\n# summary(analytic.data15$ALQ130)\ntable(analytic.data15$SMQ040,useNA = \"always\")\n#&gt; \n#&gt;  Every day Not at all  Some days       &lt;NA&gt; \n#&gt;        805       1338        216       6895\ntable(analytic.data15$PAQ605,useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4461 1389 3404\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4422 1434 3398\ntable(analytic.data15$PAQ650,useNA = \"always\")\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4422 1434 3398\n\nRecode values\nLet us recode the variables using the recode function:\n\nrequire(car)\n#&gt; Loading required package: car\n#&gt; Loading required package: carData\nanalytic.data15$RIDRETH3 &lt;- recode(analytic.data15$RIDRETH3, \n                            \"c('Mexican American','Other Hispanic')='Hispanic'; \n                            'Non-Hispanic White'='White'; \n                            'Non-Hispanic Black'='Black';\n                            c('Non-Hispanic Asian',\n                               'Other Race - Including Multi-Rac')='Other';\n                               else=NA\")\nanalytic.data15$DMDEDUC2 &lt;- recode(analytic.data15$DMDEDUC2, \n                            \"c('Some college or AA degree',\n                             'College graduate or above')='College'; \n                            c('9-11th grade (Includes 12th grad', \n                              'High school graduate/GED or equi')\n                               ='High.School'; \n                            'Less than 9th grade'='School';\n                               else=NA\")\nanalytic.data15$DMDMARTL &lt;- recode(analytic.data15$DMDMARTL, \n                            \"c('Divorced','Separated','Widowed')\n                                ='Previously.married'; \n                            c('Living with partner', 'Married')\n                                ='Married'; \n                            'Never married'='Never.married';\n                               else=NA\")\nanalytic.data15$INDHHIN2 &lt;- recode(analytic.data15$INDHHIN2, \n                            \"c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999', \n                                 '$10,000 to $14,999', '$15,000 to $19,999', \n                                 '$20,000 to $24,999')='&lt;25k';\n                            c('$25,000 to $34,999', '$35,000 to $44,999', \n                                 '$45,000 to $54,999') = 'Between.25kto54k';\n                            c('$55,000 to $64,999', '$65,000 to $74,999',\n                                 '$75,000 to $99,999')='Between.55kto99k';\n                            '$100,000 and Over'= 'Over100k';\n                               else=NA\")\nanalytic.data15$SMQ040 &lt;- recode(analytic.data15$SMQ040, \n                            \"'Every day'='Every.day';\n                            'Not at all'='Not.at.all';\n                            'Some days'='Some.days';\n                               else=NA\")\nanalytic.data15$DIQ010 &lt;- recode(analytic.data15$DIQ010, \n                            \"'No'='No';\n                            c('Yes', 'Borderline')='Yes';\n                               else=NA\")\n\n\n\nData types for various variables are set correctly; for instance, factor variables are converted to factor data types, and numeric variables to numeric data types.\nCheck missingness\n\n\n\n\n\n\nTip\n\n\n\nWe can use the plot_missing function to plot the profile of missing values, e.g., the percentage of missing per variable\n\n\n\nrequire(DataExplorer)\n#&gt; Loading required package: DataExplorer\nplot_missing(analytic.data15)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\n\n\nA subsequent chapter will delve into the additional factors that impact how we handle missing data.\nCheck data summaries\n\nnames(analytic.data15)\n#&gt;  [1] \"SEQN\"     \"RIAGENDR\" \"RIDAGEYR\" \"DMDBORN4\" \"RIDRETH3\" \"DMDEDUC2\"\n#&gt;  [7] \"DMDMARTL\" \"INDHHIN2\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BPXDI1\"  \n#&gt; [13] \"BPXSY1\"   \"BMXWT\"    \"BMXHT\"    \"BMXBMI\"   \"BMXWAIST\" \"SMQ040\"  \n#&gt; [19] \"LBXTC\"    \"LBDTCSI\"  \"LBXSTR\"   \"LBXSUA\"   \"LBXSTP\"   \"LBXSTB\"  \n#&gt; [25] \"LBXSPH\"   \"LBXSNASI\" \"LBXSKSI\"  \"LBXSGB\"   \"LBXSCA\"   \"PAQ605\"  \n#&gt; [31] \"PAQ650\"   \"DIQ010\"\nnames(analytic.data15) &lt;- c(\"ID\", \"gender\", \"age\", \"born\", \"race\", \"education\", \n\"married\", \"income\", \"weight\", \"psu\", \"strata\", \"diastolicBP\", \n\"systolicBP\", \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\", \n\"cholesterol\", \"cholesterolM2\", \"triglycerides\", \n\"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\", \n\"potassium\", \"globulin\", \"calcium\", \"physical.work\", \n\"physical.recreational\",\"diabetes\")\nrequire(\"tableone\")\n#&gt; Loading required package: tableone\nCreateTableOne(data = analytic.data15, includeNA = TRUE)\n#&gt;                                            \n#&gt;                                             Overall            \n#&gt;   n                                             9254           \n#&gt;   ID (mean (SD))                            98329.50 (2671.54) \n#&gt;   gender = Male (%)                             4557 (49.2)    \n#&gt;   age (mean (SD))                              51.50 (17.81)   \n#&gt;   born (%)                                                     \n#&gt;      Born in 50 US states or Washington, DC     7303 (78.9)    \n#&gt;      Don't know                                    1 ( 0.0)    \n#&gt;      Others                                     1948 (21.1)    \n#&gt;      Refused                                       2 ( 0.0)    \n#&gt;   race (%)                                                     \n#&gt;      Black                                      2115 (22.9)    \n#&gt;      Hispanic                                   2187 (23.6)    \n#&gt;      Other                                      1168 (12.6)    \n#&gt;      White                                      3150 (34.0)    \n#&gt;      NA                                          634 ( 6.9)    \n#&gt;   education (%)                                                \n#&gt;      College                                    3114 (33.7)    \n#&gt;      School                                      479 ( 5.2)    \n#&gt;      NA                                         5661 (61.2)    \n#&gt;   married (%)                                                  \n#&gt;      Married                                    3252 (35.1)    \n#&gt;      Never.married                              1006 (10.9)    \n#&gt;      Previously.married                         1305 (14.1)    \n#&gt;      NA                                         3691 (39.9)    \n#&gt;   income (%)                                                   \n#&gt;      &lt;25k                                       1998 (21.6)    \n#&gt;      Between.25kto54k                           2460 (26.6)    \n#&gt;      Between.55kto99k                           1843 (19.9)    \n#&gt;      Over100k                                   1624 (17.5)    \n#&gt;      NA                                         1329 (14.4)    \n#&gt;   weight (mean (SD))                        34670.71 (43344.00)\n#&gt;   psu (mean (SD))                               1.52 (0.50)    \n#&gt;   strata (mean (SD))                          140.97 (4.20)    \n#&gt;   diastolicBP (mean (SD))                      67.84 (16.36)   \n#&gt;   systolicBP (mean (SD))                      121.33 (19.98)   \n#&gt;   bodyweight (mean (SD))                       65.14 (32.89)   \n#&gt;   bodyheight (mean (SD))                      156.59 (22.26)   \n#&gt;   bmi (mean (SD))                              26.58 (8.26)    \n#&gt;   waist (mean (SD))                            89.93 (22.81)   \n#&gt;   smoke (%)                                                    \n#&gt;      Every.day                                   805 ( 8.7)    \n#&gt;      Not.at.all                                 1338 (14.5)    \n#&gt;      Some.days                                   216 ( 2.3)    \n#&gt;      NA                                         6895 (74.5)    \n#&gt;   cholesterol (mean (SD))                     179.89 (40.60)   \n#&gt;   cholesterolM2 (mean (SD))                     4.65 (1.05)    \n#&gt;   triglycerides (mean (SD))                   137.44 (109.13)  \n#&gt;   uric.acid (mean (SD))                         5.40 (1.48)    \n#&gt;   protein (mean (SD))                           7.17 (0.44)    \n#&gt;   bilirubin (mean (SD))                         0.46 (0.28)    \n#&gt;   phosphorus (mean (SD))                        3.66 (0.59)    \n#&gt;   sodium (mean (SD))                          140.32 (2.75)    \n#&gt;   potassium (mean (SD))                         4.09 (0.36)    \n#&gt;   globulin (mean (SD))                          3.09 (0.43)    \n#&gt;   calcium (mean (SD))                           9.32 (0.37)    \n#&gt;   physical.work (%)                                            \n#&gt;      No                                         4461 (48.2)    \n#&gt;      Yes                                        1389 (15.0)    \n#&gt;      NA                                         3404 (36.8)    \n#&gt;   physical.recreational (%)                                    \n#&gt;      No                                         4422 (47.8)    \n#&gt;      Yes                                        1434 (15.5)    \n#&gt;      NA                                         3398 (36.7)    \n#&gt;   diabetes (%)                                                 \n#&gt;      No                                         7816 (84.5)    \n#&gt;      Yes                                        1077 (11.6)    \n#&gt;      NA                                          361 ( 3.9)\n\nCreate complete case data (for now)\n\nanalytic.with.miss &lt;- analytic.data15\nanalytic.with.miss$cholesterol.bin &lt;- ifelse(analytic.with.miss$cholesterol &lt;200, 1,0)\nanalytic &lt;- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic)\n#&gt; [1] 837  33\n\nCreating Table 1 from the complete case data\n\nrequire(\"tableone\")\nCreateTableOne(data = analytic, includeNA = TRUE)\n#&gt;                                  \n#&gt;                                   Overall            \n#&gt;   n                                    837           \n#&gt;   ID (mean (SD))                  98419.74 (2670.63) \n#&gt;   gender = Male (%)                    510 (60.9)    \n#&gt;   age (mean (SD))                    53.44 (16.89)   \n#&gt;   born = Others (%)                    200 (23.9)    \n#&gt;   race (%)                                           \n#&gt;      Black                             179 (21.4)    \n#&gt;      Hispanic                          162 (19.4)    \n#&gt;      Other                              96 (11.5)    \n#&gt;      White                             400 (47.8)    \n#&gt;   education = School (%)                94 (11.2)    \n#&gt;   married (%)                                        \n#&gt;      Married                           511 (61.1)    \n#&gt;      Never.married                     120 (14.3)    \n#&gt;      Previously.married                206 (24.6)    \n#&gt;   income (%)                                         \n#&gt;      &lt;25k                              215 (25.7)    \n#&gt;      Between.25kto54k                  256 (30.6)    \n#&gt;      Between.55kto99k                  202 (24.1)    \n#&gt;      Over100k                          164 (19.6)    \n#&gt;   weight (mean (SD))              47722.58 (54356.70)\n#&gt;   psu (mean (SD))                     1.46 (0.50)    \n#&gt;   strata (mean (SD))                141.19 (4.18)    \n#&gt;   diastolicBP (mean (SD))            71.76 (14.33)   \n#&gt;   systolicBP (mean (SD))            126.02 (18.12)   \n#&gt;   bodyweight (mean (SD))             86.48 (22.60)   \n#&gt;   bodyheight (mean (SD))            169.14 (9.33)    \n#&gt;   bmi (mean (SD))                    30.16 (7.30)    \n#&gt;   waist (mean (SD))                 103.32 (17.17)   \n#&gt;   smoke (%)                                          \n#&gt;      Every.day                         228 (27.2)    \n#&gt;      Not.at.all                        531 (63.4)    \n#&gt;      Some.days                          78 ( 9.3)    \n#&gt;   cholesterol (mean (SD))           190.33 (43.16)   \n#&gt;   cholesterolM2 (mean (SD))           4.92 (1.12)    \n#&gt;   triglycerides (mean (SD))         156.78 (124.67)  \n#&gt;   uric.acid (mean (SD))               5.62 (1.46)    \n#&gt;   protein (mean (SD))                 7.10 (0.42)    \n#&gt;   bilirubin (mean (SD))               0.46 (0.25)    \n#&gt;   phosphorus (mean (SD))              3.53 (0.50)    \n#&gt;   sodium (mean (SD))                140.02 (2.82)    \n#&gt;   potassium (mean (SD))               4.10 (0.38)    \n#&gt;   globulin (mean (SD))                3.01 (0.43)    \n#&gt;   calcium (mean (SD))                 9.31 (0.36)    \n#&gt;   physical.work = Yes (%)              222 (26.5)    \n#&gt;   physical.recreational = Yes (%)      203 (24.3)    \n#&gt;   diabetes = Yes (%)                   179 (21.4)    \n#&gt;   cholesterol.bin (mean (SD))         0.62 (0.49)\n\n\n\nAdditional factors come into play when dealing with complex survey datasets; these will be explored in a subsequent chapter.\nSaving data\n\n# getwd()\nsave(analytic.with.miss, analytic, file=\"Data/researchquestion/NHANES17.RData\")\n\nReferences\n\n\n\n\nPeters, Junenette L, M Patricia Fabian, and Jonathan I Levy. 2014. “Combined Impact of Lead, Cadmium, Polychlorinated Biphenyls and Non-Chemical Risk Factors on Blood Pressure in NHANES.” Environmental Research 132: 93–99.",
    "crumbs": [
      "Research questions",
      "Causal question-2"
    ]
  },
  {
    "objectID": "researchquestionF.html",
    "href": "researchquestionF.html",
    "title": "R functions (Q)",
    "section": "",
    "text": "The list of new R functions introduced in this Research question lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nas.data.frame\nbase\nTo force an object to a data frame\n\n\nas.formula\nbase/stats\nTo specify a model formula, e.g., formula for an outcome model\n\n\nconfint\nbase/stats\nTo estimate the confidence interval for model parameters\n\n\ndegf\nsurvey\nTo see the degrees of freedom for a survey design object\n\n\ndescribe\nDescTools\nTo see the summary statistics of variables\n\n\nexp\nbase\nExponentials\n\n\nglm\nbase/stats\nTo run generalized linear models\n\n\nlapply\nbase\nTo apply a function over a list, e.g., to see the summary of a list of variables or to convert a list of categorical variables to factor variables. A similar function is `sapply`. lapply and sapply have the same functionality. The main difference is that sapply attempts to convert the result into a vector or matrix, while lapply returns a list.\n\n\nlength\nbase\nTo see the length of an object, e.g., number of elements/observations of a variable\n\n\nplot_missing\nDataExplorer\nTo plot the profile of missing values, e.g., the percentage of missing per variable\n\n\npublish\nPublish\nTo show/publish regression tables\n\n\nReduce\nbase\nTo combine multiple objects, e.g., datasets\n\n\nround\nbase\nTo round numeric values\n\n\nsaveRDS\nbase\nTo save a single R object. Similarly, readDRS will read an R object\n\n\nskim\nskimr\nTo see the summary statistics of variables\n\n\nsvydesign\nsurvey\nTo create a design for the survey data analysis\n\n\nsvyglm\nsurvey\nTo run design-adjusted generalized linear models\n\n\nunique\nbase\nTo see the number of unique elements\n\n\nweights\nbase/stats\nTo extract model weights, e.g., see the weights from a pre-specified survey design\n\n\n\n\n\n\nFor more information, visit the resources mentioned earlier.",
    "crumbs": [
      "Research questions",
      "R functions (Q)"
    ]
  },
  {
    "objectID": "researchquestionQ.html",
    "href": "researchquestionQ.html",
    "title": "Quiz (Q)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Research questions",
      "Quiz (Q)"
    ]
  },
  {
    "objectID": "researchquestionQ.html#live-quiz",
    "href": "researchquestionQ.html#live-quiz",
    "title": "Quiz (Q)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Research questions",
      "Quiz (Q)"
    ]
  },
  {
    "objectID": "researchquestionQ.html#download-quiz",
    "href": "researchquestionQ.html#download-quiz",
    "title": "Quiz (Q)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Research questions",
      "Quiz (Q)"
    ]
  },
  {
    "objectID": "researchquestionS.html",
    "href": "researchquestionS.html",
    "title": "App (Q)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset from the prediction question tutorial. Users can generate regression outcomes and generate a Table 1 from it based on selected predictors.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveQ\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, Publish, jtools, ggstance, broom.mixed, huxtable, httr and tableone packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Research questions",
      "App (Q)"
    ]
  },
  {
    "objectID": "Simulation.html",
    "href": "Simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Background\nThis chapter provides a structured introduction to using Monte Carlo simulation for statistical inference and evaluation. They guide the reader through key concepts—beginning with the fundamentals of simulation, followed by methods for assessing estimator performance, and culminating in the estimation of causal effects using simulated data. Each tutorial emphasizes how artificial data generated under known conditions can be used to study the behavior of statistical estimators, especially when theoretical solutions are complex or unavailable. By integrating hands-on examples, the tutorials illustrate how simulation supports robust statistical reasoning, performance evaluation, and causal inference in controlled settings.",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "Simulation.html#overview-of-tutorials",
    "href": "Simulation.html#overview-of-tutorials",
    "title": "Simulation",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nMonte Carlo Simulation\nThis tutorial explains how Monte Carlo simulation is used to estimate probabilities in uncertain scenarios by generating and analyzing many random samples. It begins by defining simulations as experiments that create artificial data to test statistical methods when real data is limited or assumptions are unmet. Monte Carlo methods are then introduced as a way to approximate sampling distributions through repeated random sampling. Two examples are provided: one estimates the probability of getting exactly three heads in five coin flips, and the other estimates the likelihood of rolling a sum of 10 or more at least twice in five paired dice rolls. In both cases, simulation results closely match theoretical probabilities, illustrating how Monte Carlo methods help approximate complex probabilities when analytical solutions are impractical.\n\n\nPerformance Measures\nThis tutorial extends Monte Carlo simulation by introducing performance measures used to assess the accuracy and reliability of simulation-based estimates. It explains how convergence, bias, relative bias, empirical and model-based standard errors, mean squared error, and coverage can quantify how well a simulation captures the true parameter value. The tutorial revisits a coin-flip example to illustrate these metrics, showing how estimates stabilize over iterations and how numerical measures like bias and standard error can validate simulation accuracy. Ultimately, these performance measures help determine whether a simulation provides reliable and valid inference in settings where analytical solutions are impractical.\n\n\nEstimating Causal Effects\nThis tutorial demonstrates how to estimate causal effects using regression-based Monte Carlo simulations under a fully specified data-generating process. A simulated dataset is constructed where age confounds the relationship between diabetes medication and cholesterol levels, and the true treatment effect is set to be known. By repeatedly generating data and fitting regression models, the simulation estimates the average treatment effect and evaluates estimator performance using standard metrics. The results confirm that the estimates converge toward the true effect, with minimal bias, consistent variance, and accurate confidence interval coverage—highlighting how simulation can validate causal inference under controlled conditions.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Coming Next:\nThis chapter on Monte Carlo simulation lays the groundwork for the following chapter on causal roles by introducing key methods for evaluating statistical estimators in controlled settings. Through simulation, it demonstrates how treatment effects can be estimated and validated when the data-generating process is fully known. This foundation directly connects to the causal roles chapter, which builds on these ideas to explore how specific types of variables—such as confounders, mediators, colliders, and instruments—affect causal effect estimation. Together, these chapters form a logical progression: from simulating causal effects under ideal conditions to examining the complexities and biases that arise in real-world causal analysis.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "Simulation1.html",
    "href": "Simulation1.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "This tutorial introduces Monte Carlo simulations, a method for exploring possible outcomes in uncertain scenarios through repeated random sampling.\n\n# Load Required Packages\nrequire(\"ggplot2\")\n\nWhat is Simulation\nSimulations are a type of experiment that generates artificial data using known probability distributions or models (Morris, White, and Crowther 2019; White et al. 2024). They allow researchers to test the performance of statistical methods in a controlled setting, which is especially useful when comparing new or alternative approaches. Simulations are particularly valuable when standard statistical methods may not be appropriate due to issues with data completeness, quality, or incorrect assumptions (Morris, White, and Crowther 2019; White et al. 2024).\nFor example, many statistical methods, such as Ordinary Least Squares regression, rely on specific assumptions. When these assumptions are not met, the results may be biased, inconsistent, or inefficient, making them less reliable for understanding relationships in the data (Mooney 2025). In such cases, simulation studies provide a way to explore different scenarios under controlled conditions. This is especially helpful when practical constraints, such as time, cost, or ethical considerations, make traditional study designs difficult or impossible (Lohmann et al. 2022). Simulations also allow researchers to examine how relationships behave under varying conditions, offering insights into patterns and outcomes that may not be directly observable in real-world data.\nWhat is Monte Carlo Simulation\nMonte Carlo simulation is a method used to estimate the range of possible values a statistic can take by repeatedly generating random samples (Morris, White, and Crowther 2019; White et al. 2024). These random samples help approximate the statistic’s sampling distribution (Morris, White, and Crowther 2019; White et al. 2024). The sampling distribution represents the range of possible values a statistic can take if we repeatedly sampled from the same population (Rossi 2022). For example, If we flip a fair coin 100 times, record the proportion of heads, and repeat this experiment many times, then the distribution of those proportion values is called the sampling distribution.\nSince collecting multiple real-world samples is often impractical, Monte Carlo simulation provides an alternative by analyzing randomly generated samples that reflect real data patterns (Mooney 2025). This approach is especially useful when gathering real data is difficult or costly. By creating artificial data that behaves like real-world data, researchers can study statistical patterns without needing multiple real samples (Mooney 2025).\nThe basic idea behind Monte Carlo simulation is straightforward. To understand how a statistic behaves in actual data, researchers create a simulated version that mimics key characteristics of the real-world scenario. By running multiple experiments on this simulated data, they can observe how a statistical method performs across different conditions, leading to more informed conclusions (Mooney 2025).\nExample 1\nWhen flipping a fair coin, there is a 50% chance (0.5 probability) of getting heads. This type of event is called a Bernoulli trial, where there are only two possible outcomes: heads or tails.\nIf we want to find the probability of getting exactly 3 heads in 5 coin flips, we can use the binomial distribution. This mathematical formula calculates the probability of a certain number of successes (heads) in a fixed number of trials (flips). Using the binomial formula, the probability of getting exactly 3 heads in 5 flips is 0.3125, or about 31%.\n\\[\nP(X = 3) = \\binom{5}{3}0.5^{3}(1-0.5)^{5-3}= 0.3125\n\\] Instead of using a formula, we can also estimate this probability using Monte Carlo simulation. This approach involves simulating 5 coin flips many times and counting how often exactly 3 of them land on heads. By running this experiment repeatedly, we can approximate the probability based on the results from the simulations.\n\n# Simulate flipping a fair coin 5 times and count how often the sum of heads equals 3\nset.seed(123)  # Set seed for reproducibility\n\n# Establish variables prior to simulation\ncount &lt;- 0          \niter &lt;- 100000  # Number of iterations for the simulation    \nsave.sum &lt;- numeric(iter) # Numeric vector to store flip results\nresults &lt;- numeric(iter)  # Numeric vector to store cumulative parameter estimate\n\n# Loop through a specified number of iterations\nfor(n in 1:iter) {  \n  # Generate a sample of 5 values (either 0 or 1), then sum them\n  save.sum[n] &lt;- sum(sample(c(0,1), 5, replace = TRUE))\n  \n  # Check if the sum of the sampled values equals 3\n  if(save.sum[n] == 3){  \n    count = count + 1  # Increment the count if condition is met\n  }\n  \n  # Compute the proportion of times the sum was 3 up to the current iteration\n  results[n] &lt;- count / n  \n}\n\nprint(count/iter)\n#&gt; [1] 0.31034\n\nUsing Monte Carlo simulation, we estimate the probability of getting exactly 3 heads in 5 coin flips. The simulation gives a probability of 0.31034, which is very close to the true probability of 0.3125.\nWe can use a few plots to understand these results more intuitively. First, let’s create a histogram of the sums (the number of heads) obtained from the random samples:\n\n# Plot a histogram of the number of heads in 5 coin flips\nhist_data &lt;- hist(save.sum, \n                  breaks = seq(-0.5, 5.5, by = 1), \n                  plot = FALSE)\n\n# Compute density values manually\ndensities &lt;- hist_data$counts / sum(hist_data$counts) / diff(hist_data$breaks)\n\n# Define custom bin colors (color bins 2,3,4,5 red, others gray)\nbin_colors &lt;- ifelse(hist_data$mids == 3, \"red\", \"gray\")\n\n# Use barplot to display density histogram with custom colors\nbarplot(densities, names.arg = hist_data$mids, col = bin_colors, \n        main = \"Histogram of the Number of Heads in 5 Coin Flips\",\n        xlab = \"Sum of 5 Coin Flips\", ylab = \"Density\", border = \"black\")\n\n\n\n\n\n\n\nFrom this histogram, we observe that the frequency of getting exactly 3 heads hovers around 30%, consistent with our simulation’s estimate.\nTo further explore how our simulation arrives at the probability of getting 3 heads, we can look at a trace plot. A trace plot shows how a parameter (in this case, our running estimate of the probability of getting 3 heads) evolves over many iterations of the algorithm.\nBelow is a trace plot of the first 1000 iterations:\n\n# Trace plot for first 1000 iterations\n# Convert results into a data frame for plotting\ntrace_data &lt;- data.frame(\n  Iteration = 1:iter,\n  ProbabilityEstimate = results \n)\n\n# Create a line plot using ggplot2\nggplot(trace_data[1:1000,], aes(x = Iteration, y = ProbabilityEstimate)) +  \n  \n  # Add a blue line to represent probability estimates over iterations\n  geom_line(color = \"blue\") + \n  \n  # Add a horizontal dashed red line at y = 0.3125, \n  # the true probability of filling 3 heads in 5 flips\n  geom_hline(yintercept = 0.3125, linetype = \"dashed\", color = \"red\") + \n  \n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",  # Plot Title\n    x = \"Iteration Number\",  # x-axis label for the x-axis\n    y = \"Estimated Probability\"  # y-axis label\n  ) +\n  theme_minimal()  #ggplot2 minimal theme for clean appearance\n\n\n\n\n\n\n\nIn the first 100–200 iterations, there is a considerable amount of fluctuation in our estimate. As the number of iterations increases, it settles closer to the red dashed line at 0.3125, which is the true theoretical probability for flipping exactly 3 heads out of 5 coin tosses.\nWe can also construct a trace plot for all iterations to confirm that the estimate remains stable as more samples are drawn:\n\n# Plotting the running estimate as iterations increase\nggplot(trace_data, aes(x = Iteration, y = ProbabilityEstimate)) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = 0.3125, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",\n    x = \"Number of Iterations\",\n    y = \"Estimated Probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nAs we continue running the simulation, the proportion of sums of coin flips equal to 3 settles around the theoretical value (0.3125), demonstrating how Monte Carlo methods approximate true probabilities over many iterations.\nYou can experiment by changing the total number of coin flips or the target number of heads to see how the Monte Carlo simulation behaves under different conditions. This can help illustrate how probabilities shift for various binomial scenarios.\nExample 2\nSuppose we roll two dice together and want to determine how often the total sum is 10 or greater in 5 rolls.\nEach die has six sides, and the possible sums range from 2 (1+1) to 12 (6+6). To roll 10 or more, we need one of the following combinations:\n10: (4,6), (5,5), (6,4)\n11: (5,6), (6,5)\n12: (6,6)\nOut of 36 possible outcomes, there are 6 ways to roll 10 or greater, giving a probability of 1/6.\nTo find the probability of rolling a sum of 10 or greater at least 2 times in 5 rolls, we use the binomial distribution:\n\\[\n\\begin{aligned}\nP(X = 0) & = \\binom{5}{0} (1/6)^{0} (1 - 1/6)^{5}  = 0.4018776 \\\\\nP(X = 1) & = \\binom{5}{1} (1/6)^{1} (1 - 1/6)^{5-1} = (5/6)^5 = 0.4018776 \\\\[20pt]\nP(X \\geq 2) & = 1 - (P(X = 0) + P(X = 1)) = 0.1962449\n\\end{aligned}\n\\]\nWe can also estimate this probability using Monte Carlo simulation. This involves simulating five rolls of two dice and counting how often the total is 10 or greater at least twice. By repeating this experiment many times, we can approximate the probability based on the simulation results.\n\nset.seed(123)\n\ncount &lt;- 0 # Initialize count variable\niter &lt;- 100000 # Number of iterations for the simulation\ncount_10_or_greater &lt;- numeric(iter) # Numeric vector to store the number of dice rolls that are 10 or greater\nresults2 &lt;- numeric(iter) # Numeric vector to store cumulative parameter estimate\n\n# Loop through the simulation\nfor(n in 1:iter){\n   # Simulate rolling two six-sided dice 5 times, summing each pair's result\n  dice_rolls &lt;- replicate(5, sum(sample(1:6, 2, replace = TRUE)))\n  \n  # Count how many of the 5 rolls resulted in a sum of 10 or greater\n  count_10_or_greater[n] &lt;- sum(dice_rolls &gt;= 10)  \n  \n  # Check if at least 2 out of the 5 rolls resulted in a sum ≥ 10\n  if(count_10_or_greater[n] &gt;= 2){\n    count &lt;- count + 1 # Increment the count if condition is met\n    \n  } \n  \n  # Compute the cumulative proportion of times the 10 or greater was rolled at least twice\n  results2[n] &lt;- count / n  \n}\n\nprint(count/iter)\n#&gt; [1] 0.19676\n\nUsing Monte Carlo simulation, we estimate the probability of rolling a sum of 10 or greater in 5 paired dice rolls. The simulation gives an estimated probability of 0.19676, which closely matches the true probability of 0.19624.\nAs we did in the previous example, we can again plot a histogram to visualize the number of rolls (out of 5) where the sum was at least 10. Since we are particularly interested in cases where this happens at least twice, we will highlight the corresponding bins in red:\n\n# Create histogram object without plotting, save it under hist_data\nhist_data &lt;- hist(count_10_or_greater, \n                  breaks = seq(-0.5, 5.5, by = 1), \n                  plot = FALSE)\n\n# Compute density values manually\ndensities &lt;- hist_data$counts / sum(hist_data$counts) / diff(hist_data$breaks)\n\n# Define custom bin colors (color bins 2,3,4,5 red, others gray)\nbin_colors &lt;- ifelse(hist_data$mids %in% c(2,3,4,5), \"red\", \"gray\")\n\n# Use barplot to display density histogram with custom colors\nbarplot(densities, names.arg = hist_data$mids, col = bin_colors, \n        main = \"Histogram of Rolls with Sum ≥10 in 5 Paired Dice Rolls\",\n        xlab = \"Number of Rolls (Out of 5) Where Sum ≥10\", ylab = \"Density\", border = \"black\")\n\n\n\n\n\n\n\nObserving the red bins, we see that their total proportion is approximately 20% of the simulations, aligning with our probability estimate.\nJust as before, we can also plot a trace plot to observe how our probability estimate stabilizes over iterations:\n\n# Convert results into a data frame for plotting\ntrace_data2 &lt;- data.frame(\n  Iteration = 1:iter,\n  ProbabilityEstimate = results2\n)\n \n# Plotting the running estimate as iterations increase\nggplot(trace_data2, aes(x = Iteration, y = ProbabilityEstimate)) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = 0.1962449, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",\n    x = \"Number of Iterations\",\n    y = \"Estimated Probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nAs in the last example, the first 100–200 iterations show significant fluctuation in the estimated probability. However, as the number of iterations increases, the estimate stabilizes around 0.19624, which is the true theoretical probability of rolling a sum of 10 or more at least twice in 5 rolls.\nReferences\n\n\n\n\nLohmann, Anna, Oscar L. O. Astivia, Tim P. Morris, and Rolf H. H. Groenwold. 2022. “It’s Time! Ten Reasons to Start Replicating Simulation Studies.” Frontiers in Epidemiology 2. https://doi.org/10.3389/fepid.2022.973470.\n\n\nMooney, Christopher. 2025. Monte Carlo Simulation. Thousand Oaks, California: SAGE Publications, Inc. https://doi.org/10.4135/9781412985116.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine 38 (11): 2074–2102. https://doi.org/https://doi.org/10.1002/sim.8086.\n\n\nRossi, Richard J. 2022. “Measuring the Reliability of Statistics.” In Applied Biostatistics for the Health Sciences, 186–234. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781119722717.ch5.\n\n\nWhite, Ian R., Tra M. Pham, Matteo Quartagno, and Tim P. Morris. 2024. “How to Check a Simulation Study.” International Journal of Epidemiology 53 (1).",
    "crumbs": [
      "Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "Simulation2.html",
    "href": "Simulation2.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Now that we understand Monte Carlo simulation, how can we evaluate how close our Monte Carlo estimates are to the true parameter value? To do this, we can use numerical techniques called Performance Measures.\nPerformance measures are calculations used to assess how well our simulation method estimates the parameter of interest (Morris, White, and Crowther 2019)\nThere are various ways to evaluate the accuracy of a Monte Carlo simulation. However, the choice of method depends on the aim of the simulation and the parameter being estimated.\n\n# Load Required Packages\nrequire(ggplot2)\nrequire(rsimsum)\n\nConvergence\nOne measure that we have already encountered, without explicitly mentioning it, is convergence! Convergence tells us how quickly our estimate stabilizes at a value close to or equivalent to the true parameter.\nThis can be assessed using a trace plot. If the estimate stabilizes after a relatively small number of iterations, this suggests the method/estimator is producing stable estimates with fewer iterations.\nLet’s take a look again at the trace plot from our coin flip example:\n\n# Simulate flipping a fair coin 5 times and count how often the sum of heads equals 3\nset.seed(123)  # Set seed for reproducibility\n\n# Establish variables prior to simulation\ncount &lt;- 0    # Initialize count variable      \niter &lt;- 100000  # Number of iterations for the simulation    \nsave.sum &lt;- numeric(iter) # Numeric vector to store flip results\nresults &lt;- numeric(iter)  # Numeric vector to store cumulative parameter estimate\n\n# Loop through a specified number of iterations\nfor(n in 1:iter) {  \n  # Generate a sample of 5 values (either 0 or 1), then sum them\n  save.sum[n] &lt;- sum(sample(c(0,1), 5, replace = TRUE))\n  \n  # Check if the sum of the sampled values equals 3\n  if(save.sum[n] == 3){  \n    count = count + 1  # Increment the count if condition is met\n  }\n  \n  # Compute the cumulative proportion of times \n  # the sum was 3 up to the current iteration\n  results[n] &lt;- count / n  \n}\n\n# Convert results into a data frame for plotting\ntrace_data &lt;- data.frame(\n  Iteration = 1:iter,\n  ProbabilityEstimate = results \n)\n\n# Create a line plot using ggplot2\nggplot(trace_data, aes(x = Iteration, y = ProbabilityEstimate)) +  \n  # Add a blue line to represent probability estimates over iterations\n  geom_line(color = \"blue\") + \n  # Add a horizontal dashed red line at y = 0.3125, \n  # the true probability of filling 3 heads in 5 flips\n  geom_hline(yintercept = 0.3125, linetype = \"dashed\", color = \"red\") + \n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",  # Plot Title\n    x = \"Iteration Number\",  # x-axis label for the x-axis\n    y = \"Estimated Probability\"  # y-axis label\n  ) +\n  theme_minimal()  #ggplot2 minimal theme for clean appearance\n\n\n\n\n\n\n\nFrom the plot, we can see that our estimate appears to stabilize around 10,000 iterations, suggesting that the method produces stable estimates of the true probability of flipping exactly 3 heads in 5 coin flips.\n\nIn addition to observing the rate at which an estimate converges to its true value, mathematical measures can also be calculated to assess how well our simulation is performing. The following performance measures and syntax are referenced from the rsimsum package, which we will use to verify our simulations (Gasparini 2018).\nFor the explanations of the different performance measures, the following notation will be used:\n\n\n\\(\\theta\\) : the true population parameter, what we are trying to estimate.\n\n\\(\\hat{\\theta_i}\\) : the estimated value of \\(\\theta\\), for the \\(ith\\) iteration of the simulation\n\n\\(n_{sim}\\) : the number of simulations\n\n\\(i = 1, . . . , n_{sim}\\) : the index of the current iteration\n\nBias\nBias measures the difference between the Monte Carlo Estimatea and the true parameter. It is calculated as the average difference between each estimate and the true parameter value (i.e., summing all differences and dividing by the number of iterations).\n\\[\n\\text{Bias} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} ( \\hat{\\theta_i} - \\theta)\n\\] (Gasparini 2024)\nWe want the bias to be zero or close to zero, as that indicates that our estimate is accurately capturing the true parameter. Bias can be interpreted as follows:\n\\[\n\\begin{aligned}\n\\text{Bias} & \\approx 0 \\to \\text{unbiased} \\\\\n\\text{Bias} & &gt; 0 \\to \\text{Biased, Overestimated} \\\\\n\\text{Bias} & &lt; 0 \\to \\text{Biased, Underestimated}\n\\end{aligned}\n\\]\n(Rossi 2022)\nRelative Bias\nRelative bias measures how much the Monte Carlo estimate differs relative to the true parameter value. While bias provides the absolute difference between the estimate and the true parameter, relative bias expresses this difference as a proportion of the true parameter value.\n\\[\n\\text{Relative Bias} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} \\bigg(\\frac{\\hat{\\theta_i} - \\theta}{\\theta} \\bigg)\n\\]\n(Gasparini 2024)\nEmpirical Standard Error (Empirical SE)\nThe empirical standard error is the empirical standard deviation of the estimates across all simulation iterations. This tells us, on average, how much the Monte Carlo estimates differ from their mean (by summing all the Monte Carlo estimates and dividing by the number of iterations).\n\\[\n\\text{Empirical Standard Error} = \\sqrt{\\frac{1}{n_{sim} - 1} \\sum^{n_{sim}}_{i=1} (\\hat{\\theta_i} - \\bar{\\theta})^2 }\n\\]\n(Gasparini 2024)\nThe term empirical standard deviation generally refers to the standard deviation computed from observed or sampled data, estimating the population standard deviation. The simulation is drawing a finite number of parameter estimates from a theoretical infinite population of estimates. Since we have a sample, we calculate the empirical standard deviation of the estimates, which is used to approximate the variability of an estimator across repeated samples.\nModel-Based Standard Error (Model SE)\nThe Model-Based Standard Error (Model SE) represents the expected variability of an estimator, computed as the average of the estimated standard errors across multiple simulation iterations. In each iteration of a Monte Carlo simulation, we obtain an estimate of the parameter of interest along with an estimated standard error, often derived from a statistical model (e.g., based on likelihood methods or asymptotic approximations). The Model SE is then calculated as the square root of the average of these estimated variances. It essentially reflects how much uncertainty the model predicts for each estimate within a single simulation run.\n\\[\n\\text{Model SE} = \\sqrt{\\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} \\widehat{\\text{Var}}(\\hat\\theta_i)}\n\\]\n(Gasparini 2024)\nWhere \\(\\widehat{\\text{Var}}(\\hat\\theta_i)\\) is the estimated variance of the estimator \\(\\theta\\) for the \\(ith\\) iteration of the simulation.\nIn contrast, the Empirical Standard Error (Empirical SE) measures the actual observed variability of the estimates across all Monte Carlo iterations. It is computed as the standard deviation of the simulated estimates. While Model SE is derived from model-based assumptions at the iteration level, Empirical SE reflects the true variability of the estimator across repeated simulations.\nIdeally we would like both the Empirical and Model-based SEs to be low, as this indicates that either the estimates from each simulation replication have small variation to the overall mean across simulation repetitions, or the individual standard errors from each simulation iteration are small.\nRelative % error in model SE\nThe Relative % Error in Model SE quantifies how much the model-based estimate of standard error deviates from the actual observed variation in estimates. If the relative error is small, it suggests that the model’s standard error assumptions align well with reality. However, a large discrepancy (positive or negative) may indicate issues with model misspecification, biased estimators, or incorrect variance assumptions in the statistical model used for inference.\n\\[\n\\text{Relative % error in Model SE} = 100 \\bigg(\\frac{\\text{Model SE}}{\\text{Empirical SE}} - 1\\bigg)\n\\]\n(Gasparini 2024)\nMean Squared Error (MSE)\nThe Mean Squared Error (MSE) is the average squared differences between the Monte Carlo estimate and the true parameter value (Rossi 2022).\n\\[\n\\text{MSE} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1} (\\hat{\\theta_i} - \\theta)^2\n\\]\n(Gasparini 2024)\nThe MSE combines both the Bias and the variance of the estimator (Rossi 2022).\n\\[\n\\text{MSE}(\\hat\\theta) = \\text{Bias}(\\hat\\theta, \\theta)^2 + \\text{Var}(\\hat\\theta)\n\\] In the event that our estimator is unbiased (i.e., bias = 0), the MSE would be equal to the variance.\nCoverage\nCoverage is the probability that a confidence interval will contain the true parameter, \\(\\theta\\) (Morris, White, and Crowther 2019; Gasparini 2024). In a 95% confidence interval for \\(\\hat\\theta\\), we expect that 95% of all confidence intervals constructed from our estimates will contain the true parameter, \\(\\theta\\).\nA confidence interval provides a range within which the true parameter is expected to fall in with a given probability. Thus, coverage gives the probability that a confidence interval from each simulation iteration contains the true parameter \\(\\theta\\).\n\\[\n\\text{Coverage} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1}I(\\hat\\theta_{i,low} \\le \\theta \\le \\hat\\theta_{i,upp})\n\\] (Gasparini 2024)\nA confidence interval is calculated for each iteration of the simulation, where \\(\\hat\\theta_{i,low}\\), and \\(\\hat\\theta_{i,upp}\\), represent the lower and upper bounds of the \\(ith\\) confidence interval.\n\\(I(\\cdot)\\) is an indicator function that returns 1 if the condition inside it is true (i.e., if \\(\\theta\\) is within \\(\\hat\\theta_{i,low}\\), and \\(\\hat\\theta_{i,upp}\\)), and returns 0 if the condition is false (i.e., \\(\\theta\\) falls outside the interval)\nThus, coverage represents the probability that \\(\\theta\\) is captured within a confidence interval across simulation iterations.\nBias-eliminated coverage\nThe bias-eliminated coverage calculates how often the confidence intervals contain the average estimate \\(\\bar{\\theta}\\) instead of the true parameter \\(\\theta\\). If our estimate is considered bias, using \\(\\bar{\\theta}\\) rather than \\(\\theta\\) eliminates its impact on the coverage calculation (Morris, White, and Crowther 2019; Gasparini 2024).\n\\[\n\\text{Bias-eliminated coverage} = \\frac{1}{n_{sim}}\\sum^{n_{sim}}_{i=1}I(\\hat\\theta_{i,low} \\le \\bar\\theta \\le \\hat\\theta_{i,upp})\n\\]\n(Gasparini 2024)\nPower\nPower is a key performance measure in Monte Carlo simulations, ensuring that our tests detect true effects with high probability. It measures the probability of correctly rejecting the null hypothesis (H0) when the alternative hypothesis (Ha) is true. In other words, power quantifies how effective a statistical test is at detecting a real effect when one exists. If power is too low, we may need to increase the sample size or improve the efficiency of our estimator to avoid missing important findings. Conversely, very high power may indicate that the test is too sensitive, potentially detecting trivial effects.\n\\[\n\\text{Power} = \\frac{1}{n_{sim}} \\sum^{n_{sim}}_{i=1} I\\bigg[| \\hat\\theta_i | \\ge z_{\\alpha/2} \\times \\sqrt{\\widehat{Var}(\\hat\\theta_i)} \\bigg]\n\\]\n(Gasparini 2024)\nExample\nLet’s revisit our first example, where we were interested in flipping exactly 3 heads in 5 tosses of a fair coin. We previously established that the theoretical probability of this event is 0.3125. Let’s explore the performance measures of this simulation using the rsimsum package (Gasparini 2018).\nIn this package we will be using the simsum() function. This function computed the performance measures.\nThe simsum() function computes the performance measures. It requires a data.frame in tidy format (data), the variable name containing the estimates (estvarname), and the true parameter value, which is used to calculate bias, relative bias, coverage, and mean squared error (Morris, White, and Crowther 2019; Gasparini 2018). Additionally, this function can take on an se parameter, which is the variable name containing the standard errors of the estimates for each simulation iteration.\n\n# simsum(\n#   data,\n#   estvarname,\n#   se = NULL,\n#   true = NULL,\n#   methodvar = NULL,\n#   ref = NULL,\n#   by = NULL,\n#   ci.limits = NULL,\n#   df = NULL,\n#   dropbig = FALSE,\n#   x = FALSE,\n#   control = list()\n# )\n\nIn this simulation, instead of saving the number of heads each iteration, we save a binary response indicating whether 3 heads have been flipped or not (1 for exactly 3 heads, 0 otherwise). This converts each iteration from a binomial experiment to a Bernoulli trial, where success (1) represents getting exactly 3 heads in 5 flips, and failure (0) represents any other outcome. The probability of success for this Bernoulli trial is 0.3125.\n\nset.seed(123)  # Set seed for reproducibility\n\n# Simulation\niter &lt;- 100000  # Number of iterations for the simulation\nsave.sum &lt;- numeric(iter)  # Numeric vector to store results\n\n# Loop through each iteration\nfor(n in 1:iter) {  \n  # Generate a random sample of 5 values (either 0 or 1) with replacement\n  # Count how many 1s were drawn and check if the sum equals 3\n  # This returns TRUE (1) if exactly 3 ones are drawn, otherwise FALSE (0)\n  save.sum[n] &lt;- sum(sample(c(0,1), 5, replace = TRUE)) == 3  \n}\n\n# Create data frame for rsimsum package analysis\nsim_data &lt;- data.frame(\n  iteration = 1:iter,  # Iteration index\n  prob = save.sum  # Store whether each iteration resulted in a sum of exactly 3\n)\n\n# Analyze the simulated results using the rsimsum package\nsim_analysis &lt;- simsum(\n  data = sim_data,  # Our simulated dataset\n  estvarname = \"prob\",  # Variable name containing event results (TRUE/FALSE)\n  true = 0.3125  # The theoretical probability of getting exactly 3 ones in 5 trials\n)\n\n# Output summary of the analysis\nsim_analysis$summ\n\n\n  \n\n\n\nThe simsum() output shows that the mean of all estimates is 0.3103, which is close to our theoretical probability of 0.3125. The bias is -0.00216, indicating that our estimated mean slightly underestimates the true probability, though this bias is very small. The relative bias of -0.691% further confirms this minimal bias.\nThe empirical SE is 0.4626. Since we converted our experiment to a Bernoulli trial, we can compare this to the theoretical standard deviation of a Bernoulli random variable with probability 0.3125:\n\\[\n\\text{Standard Deviation} = \\sqrt{(p\\times(1-p))} = \\sqrt{0.3125*(1-0.3125)} = 0.4635\n\\]\nThe close match between our empirical SE (0.4626) and the theoretical standard deviation (0.4635) validates that our simulation is accurately capturing the variability of this Bernoulli process. Note that in larger simulations, the empirical SE will converge to the theoretical SE.\nThe MSE is 0.214, which should approximately equal the variance since MSE = bias² + variance and our bias is very small. We can verify this:\n\\[\n\\text{Variance} = \\text{Standard Deviation}^2 = 0.4635^2 =  0.2148\n\\]\nThe closeness between our MSE (0.214) and the theoretical variance (0.2148) further confirms that our simulation is accurately estimating the properties of this probability experiment.\nReferences\n\n\n\n\nGasparini, Alessandro. 2018. “Rsimsum: Summarise Results from Monte Carlo Simulation Studies.” Journal of Open Source Software 3 (26): 739. https://doi.org/10.21105/joss.00739.\n\n\n———. 2024. “Introduction to Rsimsum.” https://cran.r-project.org/web/packages/rsimsum/vignettes/A-introduction.html.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine 38 (11): 2074–2102. https://doi.org/https://doi.org/10.1002/sim.8086.\n\n\nRossi, Richard J. 2022. “Measuring the Reliability of Statistics.” In Applied Biostatistics for the Health Sciences, 186–234. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781119722717.ch5.",
    "crumbs": [
      "Simulation",
      "Performance Measures"
    ]
  },
  {
    "objectID": "Simulation3.html",
    "href": "Simulation3.html",
    "title": "Estimating Causal Effects",
    "section": "",
    "text": "Dataset Generation\nThis tutorial walks through a hypothetical regression-based Monte Carlo simulation example. Specifically, we simulate many datasets under a known data-generating mechanism where the true treatment effect is 0.7. We then estimate the average treatment effect (ATE) and compare the estimated coefficients from generalized linear models (glm(Y ~ A + L)) with the true treatment effect. Finally, we use the rsimsum package to assess estimator performance based on simulation-based performance measures introduced previously.\nWe are interested in exploring cholesterol levels, and examining how diabetes medication intake affects them while accounting for the potential confounding effect of age.\nWhere the true parameter for \\(\\theta = 0.7\\), indicating that, while adjusting for age, the true difference in cholesterol levels between individuals taking diabetes medication and those who are not is 0.7.\nIn this example, age is considered a confounder. This is because it likely influences both the exposure (diabetes medication intake), and the outcome (cholesterol levels). The risk for developing Type 2 Diabetes increases with age, which increases the likelihood of taking diabetes medication (“Risk Factors for Type 2 Diabetes - NIDDK” n.d.). Additionally, cholesterol levels tend to increase due to changes in metabolism, increasing the risk for developing high blood cholesterol levels (“Blood Cholesterol - Causes and Risk Factors  NHLBI, NIH” 2024).\nThe relationship can be illustrated with a DAG using the simcausal package (Sofrygin, van der Laan, and Neugebauer 2017).\nBased on the above structural causal model, we simulate a hypothetical dataset where each observation represents an individual with a given age (\\(L\\)), diabetes medication status (\\(A\\)), and cholesterol level (\\(Y\\)). We generate counterfactual cholesterol levels under both treatment (\\(A=1\\), denoted as \\(Y_1\\)) and no treatment (\\(A=0\\), denoted as \\(Y_0\\)), enabling us to estimate causal effects. Diabetes medication status is randomly assigned to individuals, and their cholesterol level will be assigned \\(Y=Y_1\\) if cholesterol is treated, and \\(Y=Y_0\\) if cholesterol is not treated.\n# Creating a function for generating data based on the 3 variables, L, A, and Y.\n\nsimulate_data &lt;- function(n = 10, seedx = 123){\n  require(simcausal)\n  set.seed(seedx)\n  D &lt;- DAG.empty()\n  D &lt;- D +\n    node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) +\n    node(\"A\", distr = \"rbern\", prob = plogis(0.4*L)) +\n    node(\"Y\", distr = \"rnorm\", mean = 3 * L + 0.7 * A, sd = 1)\n  Dset &lt;- suppressMessages(set.DAG(D))\n  \n  # Creating the diabetes medication intervention, but not adding more nodes.\n  # Under A1, A will always be 1 (Assigned the intervention)\n  # Under A0, A will always be 0 (Not assigned the intervention).\n  \n  A1 &lt;- node(\"A\", distr = \"rbern\", prob = 1)\n  Dset &lt;- Dset + action(\"A1\", nodes = A1)\n  A0 &lt;- node(\"A\", distr = \"rbern\", prob = 0)\n  Dset &lt;- Dset + action(\"A0\", nodes = A0)\n  \n  # Simulate data based on on the intervention \n  # (Simulate both L and Y for both interventions\n  # (If Diabetes Medication is Given (A = 1) or if is not (A = 0)))\n  \n  Cdat &lt;- suppressMessages(sim(DAG = Dset, actions = c(\"A1\", \"A0\"), n = n, \n                               rndseed = 123))\n  \n  # Round and bind the data to one dataframe\n  generated.data &lt;- round(cbind(Cdat$A1[c(\"ID\", \"L\", \"Y\")],Cdat$A0[c(\"Y\")]),2)\n  names(generated.data) &lt;- c(\"ID\", \"L\", \"Y1\", \"Y0\")\n  \n  # Order data by Age (L) and ID\n  generated.data &lt;- generated.data[order(generated.data$L, generated.data$ID),]\n  \n  # Randomly Assign data either A = 0 or A = 1\n  generated.data$A &lt;- sample(c(0,1),n, replace = TRUE)\n  \n  # If A=0, Y0 is assigned, else Y1 is assigned\n  # Y1 is cholesterol if treated (A=1), Y0 is cholesterol if not treated (A=0) \n  # and Y is the observed cholesterol based on assigned treatment\n  generated.data$Y &lt;- ifelse(generated.data$A==0, generated.data$Y0, generated.data$Y1)\n  \n  # Save and return both the complete (observed) and counterfactual datasets.\n  #\n  # The counterfactual dataset contains cholesterol levels \n  # under both medication statuses.\n  #\n  # The observed dataset contains only the cholesterol level \n  # corresponding to the assigned medication status.\n  \n  counterfactual.dataset&lt;- generated.data[order(generated.data$ID) , ][c(\"ID\",\"L\",\"A\",\"Y1\",\"Y0\")]\n  observed.dataset&lt;- generated.data[order(generated.data$ID) , ][c(\"ID\",\"L\",\"A\",\"Y\")]\n  return(list(counterfactual=counterfactual.dataset,\n              observed=observed.dataset))\n  \n}\nUsing this function, let’s generate 10 random data points.\nresult.data &lt;- simulate_data(n=10)  \nresult.data$observed\nLet’s also observe the counterfactual dataset, and see how cholesterol levels differ based on medication status.\nresult.data$counterfactual$TE &lt;- result.data$counterfactual$Y1- result.data$counterfactual$Y0\nresult.data$counterfactual\nWhen subtracting the cholesterol levels under the two medication statues, we can see that the difference for each individual is equal to the true effect, \\(\\theta=0.7\\), as defined earlier.\nWe established that theoretically \\(Y\\) follows a normal distribution with mean \\(\\mu = 3L + \\theta A\\). Observing the data above, we see that the true effect of diabetes medication on cholesterol levels is 0.7, adjusted for age.\nWe can also observe the distribution of the cholesterol levels for both the treatment and non-treatment groups using a mirrored density plot:\ndf &lt;- result.data$counterfactual\n \n# Create densities manually so we can flip one of them\ndensity_y0 &lt;- density(df$Y0)\ndensity_y1 &lt;- density(df$Y1)\n \n# Create dataframes for plotting\ndf_y0 &lt;- data.frame(x = density_y0$x, y = -density_y0$y, Treatment = \"Y0 (No Medication)\")\ndf_y1 &lt;- data.frame(x = density_y1$x, y =  density_y1$y, Treatment = \"Y1 (Medication)\")\n \n# Combine\ndensity_df &lt;- rbind(df_y0, df_y1)\n \n# Plot\nggplot(density_df, aes(x = x, y = y, fill = Treatment)) +\n  geom_area(alpha = 0.6, color = \"black\") +\n  geom_hline(yintercept = 0, color = \"gray40\") +\n  labs(\n    title = \"Mirrored Cholesterol Densities\",\n    x = \"Cholesterol Level\",\n    y = \"Density\",\n    fill = \"Medication\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = abs) +\n  theme(legend.position = \"bottom\")\nThe mirrored density plot shows the distribution of cholesterol levels for both the medicated and non-medicated groups. The two distributions are similar in shape, however, the distribution for the non-medicated group is slightly shifted to the left compared to the medicated group. This indicate that cholesterol levels are, on average, slightly higher when medication is given.\nUsing the simulated data and Monte Carlo simulation, we can estimate the true effect that \\(A\\), diabetes medication, has on cholesterol levels adjusted for age, based on the theoretical distributions we defined for each variable.\nsim &lt;- 10000 # Number of iterations for the simulation  \ntheta &lt;- 0.7 #True parameter value\n\n# Create a dataframe for the regression estimate for A, and its standard error, for each iteration\nresults &lt;- data.frame(\n  A = numeric(sim),\n  se = numeric(sim)\n)\n\nparam_A &lt;- numeric(sim) # Numeric vector to store cumulative effect estimate\n\n# Simulate 10000 different datasets using the simulate_data() function,\n# and storing the regression parameters for A, and their standard errors\nfor(i in 1:sim){\n  \n  result_data &lt;- simulate_data(n = 1000)\n  results[i,] &lt;- summary(glm(Y ~ A + L, family = \"gaussian\", \n                             data = result_data$observed))$coefficients[\"A\",c(\"Estimate\", \"Std. Error\")]\n\n  param_A[i] &lt;- mean(results[1:i, \"A\"])\n}\n\ntrace_data &lt;- data.frame(\n  Iteration = 1:sim,\n  ProbabilityEstimate_A = param_A\n)\n\n# Create a line plot using ggplot2\nggplot(trace_data, aes(x = Iteration, y = ProbabilityEstimate_A)) +  \n  \n  # Add a blue line to represent probability estimates over iterations\n  geom_line(color = \"blue\") + \n  geom_hline(yintercept = theta, linetype = \"dashed\", color = \"red\") + \n  \n  labs(\n    title = \"Trace Plot of Estimated Probability Over Iterations\",  # Plot Title\n    x = \"Iteration Number\",  # x-axis label for the x-axis\n    y = \"Estimated Probability\"  # y-axis label\n  ) +\n  theme_minimal()  #ggplot2 minimal theme for clean appearance\nFrom the plot, we can see that the Monte Carlo simulation is doing a stable job at estimating the true effect of diabetes medication on cholesterol levels adjusted for age, as the estimate stabilizes.\nAs we did with the last example, we can observe the performance using the simsum() function from the rsimsum package (Gasparini 2018). Since we constructed a glm model, we also have standard errors for our estimates, meaning we can calculate model SE, relative error, coverage, bias-eliminated coverage, and power, in addition to what we calculated in the previous example. Results will be formatted and printed using the knitr package (Xie 2023).\n# Analyze the simulated results using the rsimsum package\nsim_analysis &lt;- simsum(\n  data = results,  # Our simulated dataset\n  estvarname = \"A\",  \n  true = theta,\n  se = \"se\" \n)\n\n# Output summary of the analysis\n# Format 'est' and 'mcse' columns to regular decimal notation with 5 digits\nformatted_summ &lt;- sim_analysis$summ\nformatted_summ$est &lt;- format(formatted_summ$est, scientific = FALSE, digits = 5)\nformatted_summ$mcse &lt;- format(formatted_summ$mcse, scientific = FALSE, digits = 5)\n \n# Show using kable\nkable(formatted_summ, caption = \"Simulation Summary (Non-scientific Notation)\")\n\n\nSimulation Summary (Non-scientific Notation)\n\nstat\nest\nmcse\n\n\n\nnsim\n10000.000000000\nNA\n\n\nthetamean\n0.699914715\nNA\n\n\nthetamedian\n0.699565160\nNA\n\n\nse2mean\n0.004061151\nNA\n\n\nse2median\n0.004059860\nNA\n\n\nbias\n-0.000085285\n0.00063196265\n\n\nrbias\n-0.000121835\n0.00090280378\n\n\nempse\n0.063196265\n0.00044688742\n\n\nmse\n0.003993376\n0.00005621261\n\n\nrelprec\nNA\nNA\n\n\nmodelse\n0.063727161\n0.00000077866\n\n\nrelerror\n0.840075814\n0.71308373374\n\n\ncover\n0.950600000\n0.00216701730\n\n\nbecover\n0.950600000\n0.00216701730\n\n\npower\n1.000000000\n0.00000000000\nFirst, the bias is very small, indicating that the estimate just slightly underestimates the true effect, though this bias is very small. the relative bias confirms that an unbiased estimate is generated from the Monte Carlo simulations. The empirical standard error and model standard error show that the estimates vary little across simulation repetitions, and that the standard errors produced within each simulation are also small. The relative percentage error in the model standard error is very small, indicating that the model SE deviates only slightly from the empirical SE.\nThe MSE should approximately equal the variance since \\(\\text{MSE} = \\text{bias}^2 + \\text{variance}\\) and our bias is approximately zero. Squaring the empirical standard error gives an empirical variance, which aligns with the MSE, highlighting that our estimate is unbiased. Additionally, since the MSE is approximately zero, this confirms that our simulation is accurately estimating the properties of this probability experiment.\nLastly, both the calculated coverage and the bias-corrected coverage indicate that the confidence intervals generated across simulations successfully captured the true effect approximately 95% of the time.\nThis simulation shows accurate estimation of the treatment effect (\\(\\approx 0.7\\)). The approach demonstrates how counterfactuals + Monte Carlo allow causal effect estimation in a fully known data-generating process (DGP).",
    "crumbs": [
      "Simulation",
      "Estimating Causal Effects"
    ]
  },
  {
    "objectID": "Simulation3.html#dataset-generation",
    "href": "Simulation3.html#dataset-generation",
    "title": "Estimating Causal Effects",
    "section": "",
    "text": "Tip\n\n\n\nThis tutorial developed is based on a previously developed external (tutorial)\n\n\nReferences\n\n\n\n\n“Blood Cholesterol - Causes and Risk Factors  NHLBI, NIH.” 2024. https://www.nhlbi.nih.gov/health/blood-cholesterol/causes.\n\n\nGasparini, Alessandro. 2018. “Rsimsum: Summarise Results from Monte Carlo Simulation Studies.” Journal of Open Source Software 3 (26): 739. https://doi.org/10.21105/joss.00739.\n\n\n“Risk Factors for Type 2 Diabetes - NIDDK.” n.d. National Institute of Diabetes and Digestive and Kidney Diseases. Accessed March 21, 2025. https://www.niddk.nih.gov/health-information/diabetes/overview/risk-factors-type-2-diabetes.\n\n\nSofrygin, Oleg, Mark J. van der Laan, and Romain Neugebauer. 2017. “simcausal R Package: Conducting Transparent and Reproducible Simulation Studies of Causal Effect Estimation with Complex Longitudinal Data.” Journal of Statistical Software 81 (2): 1–47. https://doi.org/10.18637/jss.v081.i02.\n\n\nXie, Yihui. 2023. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.",
    "crumbs": [
      "Simulation",
      "Estimating Causal Effects"
    ]
  },
  {
    "objectID": "confounding.html",
    "href": "confounding.html",
    "title": "Causal roles",
    "section": "",
    "text": "Background\nThis chapter delves deep into the intricate issues surrounding causal associations, particularly confounding, mediation, and other related biases. In this comprehensive series of tutorials, various aspects of confounding and bias are explored through the lens of Directed Acyclic Graphs (DAGs). Initially, the tutorials guide you through the process of generating large datasets based on these DAGs. They then delve into how the inclusion of different types of variables in adjustment models can skew estimates of treatment effects. We use the R package simcausal in these tutorials to derive empirical estimates from a large dataset.",
    "crumbs": [
      "Causal roles"
    ]
  },
  {
    "objectID": "confounding.html#background",
    "href": "confounding.html#background",
    "title": "Causal roles",
    "section": "",
    "text": "In the preceding chapter, we delved into the various types of research questions, distinguishing between causal and predictive inquiries. This current chapter focuses primarily on the challenges and intricacies associated with causal questions. Specifically, we will explore which types of variables are most appropriate to incorporate into adjustment models when aiming to estimate treatment effects accurately. In contrast, the subsequent chapter will shift its emphasis towards predictive questions, providing insights into their unique characteristics and considerations.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nWe use the R package simcausal in these tutorials to derive empirical estimates from a large simulated dataset. The simulation is based on data generation based on specified DAGs.\n\n\n\n\nDirected Acyclic Graphs (DAGs) are powerful tools in the realm of causal inference (see the concepts page). They are a type of graphical representation used to depict causal relationships between variables. This visual representation of the causal structure among variables makes it easier to understand and communicate complex causal relationships. They provide a visual framework to understand, represent, and analyze complex causal relationships, ensuring that researchers make informed decisions when trying to answer causal questions.",
    "crumbs": [
      "Causal roles"
    ]
  },
  {
    "objectID": "confounding.html#overview-of-tutorials",
    "href": "confounding.html#overview-of-tutorials",
    "title": "Causal roles",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nConfounding\nThe first tutorial provides a thorough exploration of confounding, with a particular focus on its impact on treatment effect estimates in large datasets. It emphasizes the importance of properly adjusting for confounders to arrive at accurate estimates.\n\n\nMediator\nThis tutorial focuses on the role of mediator variables in estimating treatment effects. It assesses how adjusting for the mediator influences the estimated treatment effect, exploring both scenarios where the true treatment effect is either non-null or null.\n\n\nCollider\nThis tutorial serves as a practical guide for understanding how the inclusion of colliders can affect the estimation of treatment effects in causal models.\n\n\nSimpson’s Paradox\nThis tutorial explores Simpson’s Paradox, a phenomenon where the direction of an association between two variables changes when a third variable is included in the model. It demonstrates how this paradox can lead to misleading conclusions in causal inference and provides a step-by-step guide to understanding and addressing it.\n\n\nZ-bias\nThis tutorial explores the concept of Z-bias, a phenomenon that can lead to misleading estimates of treatment effects in observational studies. It demonstrates how failing to properly adjust or not adjust for instrumental variables can result in biased estimates and compares these with the true treatment effect.\n\n\nCollapsibility\nThis tutorial provides a detailed guide on calculating marginal probabilities and measures of association, including Risk Difference (RD), Risk Ratio (RR), and Odds Ratio (OR). It examines the impact of adjusting for various covariates on these measures, highlighting the concept of “collapsibility.”\n\n\nChange-in-estimate\nThis tutorial focuses on the “Change-in-estimate” concept to understand the impact of various variables on measures of effect. For both continuous and binary outcomes, the tutorial reveals that adding a confounder to the model alters the true treatment effect estimate. Conversely, including a variable that is not a confounder but is a pure risk factor can either change or not change the effect estimate, depending on the type of outcome involved. This nuanced approach aids in understanding how different roles of variables can influence results and interpretations in causal inference.\n\n\nInteraction\nWe compare two saturated parameterizations for interaction with a binary outcome analyzed via logistic regression: a joint categorical variable versus main effects with a product interaction term. Using NHANES 2009–2012 survey data, we study hypertension at 130 mmHg with two interacting factors: Race1 and ObeseStatus, adjusting for Age. Under saturation the models are equivalent. Joint effects relative to a single reference are obtained directly from the joint-variable model and by post-estimation transformation from the interaction model. Simple (stratum-specific) effects can be obtained from either model. Joint effects relative to a single reference are often the most intuitive.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Causal roles"
    ]
  },
  {
    "objectID": "confounding0.html",
    "href": "confounding0.html",
    "title": "Concepts (R)",
    "section": "",
    "text": "Confounding\nConfounding is a pervasive concern in epidemiology, especially in observational studies focusing on causality. Epidemiologists need to carefully select confounders to avoid biased results due to third factors affecting the relationship between exposure and outcome. Commonly used methods for selecting confounders, such as change-in-estimator or solely relying on p-value-based statistical methods, may be inadequate or even problematic.\nEpidemiologists need a more formalized system for confounder selection, incorporating causal diagrams (Greenland, Pearl, and Robins 1999; Tennant et al. 2021) and counterfactual reasoning. This includes an understanding of the underlying causal relationships and the potential impacts of different variables on the observed association. Understanding the temporal order and causal pathways is crucial for accurate confounder control.\nHowever, it is possible that epidemiologists may lack comprehensive knowledge about the causal roles of all variables and hence may need to resort to empirical criteria (VanderWeele 2019) such as the disjunctive cause criterion, or other variable selection methods such as machine learning approaches. While these methods can provide more sophisticated analyses and help address the high dimensionality and complex structures of modern epidemiological data, epidemiologists need to understand how these approaches function, along with their benefits and limitations, to avoid introducing additional bias into the analysis.",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#effect-modifier",
    "href": "confounding0.html#effect-modifier",
    "title": "Concepts (R)",
    "section": "Effect modifier",
    "text": "Effect modifier\nEffect modification and interaction are two distinct concepts in epidemiology (VanderWeele 2009; Bours 2021). Effect modification occurs when the causal effect of an exposure (A) on an outcome (Y) varies based on the levels of a third factor (B).\nIn this scenario, the association between the exposure and the outcome differs within the strata of a second exposure, which acts as the effect modifier. For instance, the impact of alcohol (A) on oral cancer (Y) might differ based on tobacco smoking (B).\nOn the other hand, interaction refers to the joint causal effect of two exposures (A and B) on an outcome (Y). It examines how the combination of multiple exposures influences the outcome, such as the combined effect of alcohol (A) and tobacco smoking (B) on oral cancer (Y).\nIn essence, while effect modification looks at how a third factor influences the relationship between an exposure and an outcome, interaction focuses on the combined effect of two exposures on the outcome.",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#table-2-fallacy",
    "href": "confounding0.html#table-2-fallacy",
    "title": "Concepts (R)",
    "section": "Table 2 fallacy",
    "text": "Table 2 fallacy\nThe “Table 2 Fallacy” in epidemiology refers to the misleading practice of presenting multiple adjusted effect estimates from a single statistical model in one table, often resulting in misinterpretation. This occurs when researchers report both the primary exposure’s effects and secondary exposures’ (often an adjustment variable for the primary exposure) effects without adequately distinguishing between the types of effects or considering the causal relationships among variables.\nThis idea highlights the potential for misunderstanding in interpreting the effects of various exposures on an outcome when they are reported together, leading to confusion over the nature and magnitude of the relationships and possibly influencing the design and interpretation of further studies (Westreich and Greenland 2013). The fallacy demonstrates the need for careful consideration of the types of effects estimated and reported in statistical models, urging researchers to be clear about the distinctions and implications of controlled direct effects, total effects, and the presence of confounding or mediating variables.",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#reading-list",
    "href": "confounding0.html#reading-list",
    "title": "Concepts (R)",
    "section": "Reading list",
    "text": "Reading list\nConfounding key reference: (VanderWeele 2019; Tennant et al. 2021)\nEffect modification key reference: (VanderWeele 2009; Bours 2021)\nTable 2 fallacy key reference: (Westreich and Greenland 2013)\nOptional reading:\n\n(Greenland, Pearl, and Robins 1999)\n(Lederer et al. 2019)\n(Etminan, Collins, and Mansournia 2020)\n(Heinze, Wallisch, and Dunkler 2018)",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#video-lessons",
    "href": "confounding0.html#video-lessons",
    "title": "Concepts (R)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nThe Epistemological Divide: Explanatory versus Predictive Modeling\n\n\n\nBefore dissecting specific confounder selection techniques, it is crucial to establish the epistemological distinction that governs variable selection: the divergence between predictive and causal inference goals. This distinction is frequently conflated in practice, leading to the misapplication of algorithms designed for one purpose to the problems of the other.\nThe Goal of Prediction\nIn predictive modeling, the objective is to minimize the expected loss (e.g., mean squared error) between the predicted and observed outcome values. In this context, a “good” variable is one that is strongly correlated with the outcome, regardless of the direction of causality. A variable that is a consequence of the outcome (a proxy) or a mediator of the exposure can be an excellent predictor. Variable selection methods in this domain, such as standard stepwise regression, Akaike Information Criterion (AIC) minimization, or standard Lasso regularization, are designed to identify a parsimonious set of correlates that maximize model fit and reduce prediction error.\nThe Goal of Causal Explanation\nIn causal inference, the objective is to isolate the specific marginal effect of an intervention (exposure) on an outcome. Here, the correlation is only useful if it reflects a structural cause-effect relationship. Including a mediator in the model will increase the \\(R^2\\) (predictive power) but will bias the estimation of the total causal effect toward the null.\nConsequently, variable selection methods optimized for prediction are often mathematically antagonistic to causal inference. Techniques that rely on “goodness-of-fit” or statistical significance can inadvertently select colliders (inducing bias) or drop weak confounders that are critical for validity. The failure to distinguish these goals is a primary source of methodological error in the medical literature, motivating the need for distinct, causally-grounded selection strategies.\nThe Counterfactual Framework for Defining Causality\nDefining the Causal Effect: Potential Outcomes\nTo understand causality, one must first be able to imagine a world that does not exist. The potential outcomes framework formalizes this by defining the causal effect of an exposure in terms of what would have happened under different exposure scenarios. Let us define the key notations:\n\n\nA: The exposure status of an individual (e.g., \\(A=1\\) if a smoker, \\(A=0\\) if a non-smoker).\n\nY: The outcome of interest (e.g., hypertension).\n\nL: A measured covariate or potential confounder.\n\nU: An unmeasured variable.\n\nFor any individual, we can define two potential outcomes:\n\n\nY(A=1): The outcome that would be observed if the individual were a smoker.\n\nY(A=0): The outcome that would be observed if that same individual were a non-smoker at the same point in time.\n\nThe Individual Treatment Effect (TE) is the difference between these two potential outcomes for a single person: \\(TE = Y(A=1) - Y(A=0)\\). For example, if a patient named John smokes (\\(A=1\\)) and develops hypertension, while he would not have developed hypertension had he not smoked (\\(A=0\\)), the causal effect of smoking for John is present.\nThe Fundamental Problem of Causal Inference\nThe definition of the individual TE immediately presents a profound challenge. For any given individual, we can only ever observe one of their potential outcomes. If John smokes, we observe \\(Y(A=1)\\), but his counterfactual outcome, \\(Y(A=0)\\), remains unobserved forever. This is known as the fundamental problem of causal inference; it is a problem of missing data where half the data is always missing for every subject.\nBecause the individual TE is unobservable, the goal of epidemiology shifts from the individual to the population. We instead seek to estimate the Average Treatment Effect (ATE), defined as the average of the individual effects across all subjects in a population: \\(ATE = E\\).\nFrom Association to Causation: The Role of Confounding\nIn the real world, we cannot directly observe both potential outcomes for a population. Instead, we observe outcomes in two different groups of people: those who happened to be exposed (smokers) and those who were not (non-smokers). We can calculate the associational difference between these groups: \\(E - E\\). A critical error is to assume this associational difference is equal to the causal ATE.\nThis difference arises because of confounding. The groups of smokers and non-smokers may differ systematically on factors that also affect the outcome. For instance, individuals with lower socioeconomic status may be more likely to smoke and also have a higher underlying risk of hypertension for reasons unrelated to smoking (e.g., diet, stress). In this case, the observed difference in outcomes is a mixture of the true treatment effect and these pre-existing, systematic differences between the groups.\nThe Observational Study Solution: Conditional Exchangeability\nRandomized Controlled Trials (RCTs) are the gold standard for causal inference because the process of randomization, with a large enough sample size, ensures that the exposed and unexposed groups are, on average, identical (“exchangeable”) on all baseline characteristics, both measured and unmeasured. In an RCT, any systematic differences are eliminated, making the associational difference a valid estimate of the causal ATE.\nIn observational studies, where randomization is not possible, we cannot achieve this level of exchangeability. Instead, we strive for conditional exchangeability. This is the assumption that, within strata of the measured confounders, the exposed and unexposed groups are exchangeable. By estimating the effect of smoking separately within each level of the confounder(s) \\(L\\) (e.g., estimating the effect of smoking separately for different age groups) and then averaging these stratum-specific effects, we can aim to reconstruct the causal ATE. This process of stratification, or “adjustment,” is the conceptual basis for controlling for confounding in observational research. However, its validity rests entirely on the critical and untestable assumption that we have successfully identified and measured all important common causes of the exposure and the outcome.\nWhat is included in this Video Lesson:\n\n0:00 Introduction\n0:16 Notations\n2:40 Treatment Effect\n6:13 Real-world Problem of the counterfactual definition\n9:44 Real-world Solution in Observational Setting\n\nThe timestamps are also included in the YouTube video description.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructural and Knowledge-Based Selection Techniques\n\n\n\nTo properly address confounding, researchers need a tool to translate their subject-matter knowledge and assumptions about the world into a formal structure. Directed Acyclic Graphs (DAGs) serve this purpose, providing a visual language and a set of rigorous rules for identifying sources of bias and guiding statistical analysis.\nThe Grammar of Causal Diagrams\nA DAG is a graphical model of causal relationships between variables. Its components follow a simple grammar:\n\n\nNodes: Represent variables (e.g., smoking, hypertension, age).\n\nArrows (Directed Edges): Represent a direct causal effect from one variable to another.\n\nDirected: The arrows have a single head, indicating the assumed direction of causality.\n\nAcyclic: A path of arrows cannot form a closed loop. This enforces the principle of temporality: a variable cannot be its own cause.\n\nCrucially, the most powerful assumptions in a DAG are the absent arrows. The absence of an arrow between two variables represents a strong claim of no direct causal effect.\nPaths: Causal and Non-Causal\nA path is any sequence of arrows connecting two variables, regardless of the direction of the arrowheads. When assessing the relationship between an exposure like smoking (\\(A\\)) and an outcome like hypertension (\\(Y\\)), paths can be categorized into two critical types:\n\n\nCausal Paths (Front-door paths): These are paths that begin with an arrow originating from \\(A\\) and moving toward \\(Y\\) (e.g., \\(A \\rightarrow \\text{Stress} \\rightarrow Y\\)). These paths transmit the causal effect of \\(A\\) on \\(Y\\) that we wish to estimate.\n\nNon-Causal Paths (Back-door paths): These are paths between \\(A\\) and \\(Y\\) that begin with an arrow pointing into \\(A\\) (e.g., \\(A \\leftarrow \\text{Age} \\rightarrow Y\\)). These paths are sources of non-causal association (confounding) that can bias our estimate. The goal of adjustment is to “block” these backdoor paths.\nThe Three Elementary Causal Structures\nAll complex DAGs are composed of three fundamental building blocks. Understanding how information flows through these structures is the key to using DAGs to identify and control for bias.\n\n\nThe Fork (Confounding): The structure is \\(A \\leftarrow L \\rightarrow Y\\). Here, \\(L\\) is a common cause of both the exposure \\(A\\) and the outcome \\(Y\\).\n\n\nExample: Age (\\(L\\)) is a common cause of both smoking habits (\\(A\\)) and hypertension (\\(Y\\)).\n\nRule: The backdoor path through a common cause is open by default, creating a spurious association. To remove this confounding, one must condition on the confounder \\(L\\), which blocks the path.\n\n\n\nThe Chain (Mediation): The structure is \\(A \\rightarrow M \\rightarrow Y\\). Here, \\(M\\) is a mediator that lies on the causal pathway.\n\n\nExample: Smoking (\\(A\\)) causes chronic inflammation (\\(M\\)), which in turn causes hypertension (\\(Y\\)).\n\nRule: The causal path through a mediator is open by default. To estimate the total effect of \\(A\\) on \\(Y\\), one must not condition on the mediator \\(M\\). Doing so would block this part of the causal effect.\n\n\n\nThe Collider (Selection/Collider Bias): The structure is \\(A \\rightarrow L \\leftarrow Y\\). Here, \\(L\\) is a common effect of both \\(A\\) and \\(Y\\).\n\n\nExample: Both smoking (\\(A\\)) and a genetic predisposition (\\(Y\\)) can lead to a specific biomarker level (\\(L\\)).\n\nRule: The path through a collider is blocked by default. However, conditioning on the collider \\(L\\) opens the path, inducing a spurious, non-causal association between \\(A\\) and \\(Y\\). Adjusting for a collider is a critical error that introduces bias.\n\n\nApplying the Rules with Dagitty\nIn practice, causal systems can be highly complex. Software such as Dagitty.net automates the application of these path-blocking rules. Given a user-drawn DAG, Dagitty can identify all open backdoor paths and determine the minimal sufficient adjustment sets: the smallest set of covariates that, if conditioned on, will block all backdoor paths and allow for an unbiased estimation of the total causal effect.\nThe video lesson split into 3 parts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDAG codes:\n\n\n\nExample DAG codes can be accessed from this GitHub repository folder\n\n\n\n\n\n\n\n\nEmpirical Criteria for DAG-Deficient Scenarios\n\n\n\nIn many practical epidemiological investigations, particularly those involving novel exposures or complex metabolic pathways, the full causal structure is unknown. The uncertainty regarding the presence or direction of arrows makes the strict construction of a DAG impossible. In such “DAG-deficient” scenarios, epidemiologists must resort to pragmatic heuristics or empirical criteria that aim to approximate the Backdoor Criterion with less stringent assumptions.\nIn the absence of a fully specified DAG, researchers can rely on a set of empirical criteria that require less stringent assumptions.\n\n\n\n\n\n\nPre-treatment Criterion\n\n\n\nOne of the simplest and most intuitive heuristics is the Pre-treatment Criterion, which dictates adjusting for all covariates measured chronologically before the exposure was administered or assigned.\nRationale: The logic is grounded in temporal causality; a variable occurring before the exposure cannot be a downstream effect (mediator) of the exposure. Therefore, adjusting for pre-treatment variables avoids the error of overadjustment via mediation.\nCritique:\n\nWhile this criterion successfully avoids adjusting for mediators, it fails to protect against M-bias. A pre-treatment variable can still be a collider if it is caused by two unobserved latent variables—one linked to the exposure and one to the outcome. Adjusting for such a pre-treatment collider introduces bias.\nThis “kitchen sink” approach often leads to the inclusion of Instrumental Variables (IVs)—pre-treatment variables that cause the exposure but have no independent effect on the outcome. As discussed later, adjusting for IVs inflates the variance of the estimator and can amplify bias due to residual unmeasured confounding (Z-bias).\n\nThus, while the Pre-treatment Criterion is a helpful starting point, it is often too crude for high-stakes causal inference.\n\n\n\n\n\n\n\n\nCommon Cause Criterion\n\n\n\nThe Common Cause Criterion refines the selection process by narrowing the adjustment set to variables known (or suspected) to be causes of both the exposure and the outcome.\nRationale: This criterion targets the classical epidemiological definition of a confounder. By restricting selection to common causes, it theoretically avoids colliders (which are effects) and instruments (which are causes of exposure only).\nCritique: The major limitation of this approach is its reliance on definitive knowledge. If a researcher is unsure whether a variable causes the outcome, the strict application of this criterion would lead to its exclusion. However, standard bias analysis suggests that omitting a true confounder (due to uncertainty) generally introduces more bias than including a non-confounder. Therefore, the Common Cause Criterion is often viewed as overly conservative, potentially leading to residual confounding in the pursuit of parsimony.\n\n\n\n\n\n\n\n\nDisjunctive Cause Criterion\n\n\n\nTo address the limitations of the Common Cause Criterion, the Disjunctive Cause Criterion is proposed as a pragmatic strategy for confounder selection (VanderWeele 2019).\nThe Rule: Control for any pre-exposure covariate that is\n\na cause of the exposure, OR\n\na cause of the outcome, OR\n\nboth.\n\nMechanism: This union-based approach ensures that all common causes (confounders) are included, as they satisfy the condition of being a cause of both. By including variables that are only causes of the outcome, the method improves the precision of the estimate (reducing standard error) without introducing bias. By including variables that are only causes of the exposure (potential instruments), it risks some variance inflation, but this is often considered an acceptable trade-off to ensure no confounders are missed.\nStrength: The primary strength of the Disjunctive Cause Criterion is its robustness to uncertainty regarding the full causal structure. The researcher does not need to know if a variable affects both exposure and outcome; knowing it affects at least one is sufficient for inclusion. This effectively minimizes the risk of unadjusted confounding while generally avoiding colliders (which are effects, not causes).\n\n\n\n\n\n\n\n\nModified Disjunctive Cause Criterion\n\n\n\nRefining the Disjunctive Cause Criterion further, the Modified Disjunctive Cause Criterion incorporates specific exclusions and inclusions to optimize both validity and efficiency.\nExclude IVs: Recognizing the variance inflation and Z-bias risks associated with instruments, the modified criterion explicitly removes variables known to affect the exposure but not the outcome. This requires some structural knowledge but yields a more efficient estimator.\nInclude Proxies: Acknowledging that true confounders are often unmeasured, the modified criterion mandates the inclusion of measured variables that serve as proxies for the unmeasured common causes. Even if a proxy is not a direct cause, adjusting for it partially blocks the backdoor path transmitted through the unobserved parent variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelling criteria for variable selection\n\n\n\nStatistical methods can also be used for variable selection, but their application requires careful consideration of the research goal: prediction versus causal inference.\n\n\n\n\n\n\nChange-in-Estimate\n\n\n\nThe Change-in-Estimate (CIE) method represents an operationalization of the definition of confounding: if a variable is a confounder, adjusting for it should change the estimated effect of the exposure.\nThe Procedure: The researcher begins with a “crude” model containing only the exposure and outcome. Potential confounders are added to the model one by one (or removed from a full model). If the regression coefficient for the exposure changes by more than a specified percentage (commonly 10%), the variable is deemed a confounder and retained in the model.\nThe Non-Collapsibility Trap: A critical flaw of the CIE method arises when using non-collapsible effect measures, such as the OR or HR. In logistic regression, the addition of a covariate that is strongly associated with the outcome (but independent of the exposure) will increase the magnitude of the exposure’s OR—driving it further from the null. This occurs not because of confounding bias, but because of a mathematical property known as non-collapsibility. A CIE algorithm would interpret this change as evidence of confounding and select the variable, potentially leading to over-adjustment or misinterpretation of the effect measure modification. Thus, CIE is safer for RDs or RRs but hazardous for ORs.\n\n\n\n\n\n\n\n\nStatistical Significance (Stepwise Selection)\n\n\n\nStepwise selection algorithms (forward selection, backward elimination, or bidirectional search) rely on statistical significance (p-values) to determine variable inclusion.\nThe Procedure: Variables are added to the model if their association with the outcome yields a p-value below a certain threshold (e.g., 0.05) or removed if the p-value exceeds it.\nThe Confounding vs. Significance Fallacy: The most fundamental critique of this approach is that “confounding is not a significance test.” A variable can be a strong confounder—systematically biasing the effect estimate—even if its association with the outcome fails to reach statistical significance in a specific sample, particularly in small studies. Relying on p-values often leads to under-adjustment and residual confounding.\nPost-Selection Inference: Stepwise selection invalidates the statistical theory behind confidence intervals. The final model treats the selected variables as if they were specified a priori, ignoring the immense “data dredging” and multiple testing that occurred during the selection process. This results in standard errors that are systematically too small and confidence intervals that are too narrow, creating a false sense of precision.\nPrediction vs. Causation: Ultimately, stepwise algorithms are designed to maximize model fit (prediction). They will happily select a collider or a mediator if it is strongly correlated with the outcome, thereby maximizing \\(R^2\\) while destroying the validity of the causal coefficient.\n\n\n\n\n\n\n\n\nPurposeful Selection of Covariates\n\n\n\nRecognizing the limitations of purely mechanical stepwise regression, the “Purposeful Selection” algorithm, a hybrid approach was proposed (Hosmer, Lemeshow, and Sturdivant 2013; Bursac et al. 2008)that combines statistical criteria with researcher judgment and confounding checks.\nThe Algorithm:\n\n\nUnivariate Screening:\n\nEvaluate all covariates individually.\nRetain any variable with a univariate p-value \\(&lt; 0.25\\). This relaxed threshold is crucial; it aims to capture potential confounders that may be weak individually but strong jointly, or whose effects are masked in univariate analysis.\n\n\n\nMultivariable Model:\n\nFit a model with all candidates identified in step 1.\nRemove variables that are not significant at traditional levels (e.g., \\(p &lt; 0.05\\)).\n\n\n\nConfounding Check: This is the distinguishing feature.\n\nBefore permanently discarding a variable, the analyst must check if its removal induces a major change (\\(&gt;15-20\\%\\)) in the coefficients of the remaining variables.\nIf it does, the variable is added back into the model as a confounder, regardless of its statistical significance.\n\n\n\nRefinement and Interactions: Excluded variables are added back one by one to check for residual significance. Finally, the model is checked for plausible interactions.\n\nInsight: Purposeful Selection is widely cited in epidemiology because it operationalizes the definition of confounding within the selection process. Unlike rigid stepwise regression, it prioritizes the stability of the exposure coefficient over the parsimony of the outcome model. It forces the analyst to examine the data at each step, acting as a safeguard against the automation of causal errors.\nCriticism: Purposeful Selection is now considered outdated and flawed by modern causal inference standards. Its fundamental weakness is that it remains entirely driven by statistical associations within the data rather than by a priori causal structure. The “confounding check” (Step 3), its distinguishing feature, is ironically its most critical flaw. This change-in-estimator (CIE) criterion cannot distinguish true confounders from colliders or mediators. In the case of a collider, adjusting for it induces a spurious association (bias), which causes a large change in the exposure’s coefficient. The algorithm misinterprets this induced bias as a sign of confounding and therefore retains the collider, leading to a biased final estimate. Because it is “causally blind,” it is not a safeguard against causal errors and is superseded by methods like those based on DAGs.\n\n\n\n\n\n\n\n\nMachine Learning (ML)\n\n\n\nAlgorithms such as LASSO and Random Forests are excellent for high-dimensional prediction. Their primary role in causal inference is in developing propensity score (PS) models, which is a prediction task for the exposure model (Karim and Lei 2025). The goal is to create a score that balances measured covariates between the exposed and unexposed groups, mimicking randomization.\nCriticism: The variance estimation can be poor depending on the machine learning method used to do the variable selection, often resulting in poor coverage.\n\n\n\n\n\n\n\n\nAdvanced Causal Inference Methods, often incorporating ML\n\n\n\n\nHigh-Dimensional Propensity Score (hdPS) (Schneeweiss et al. 2009; Karim et al. 2025): designed for healthcare databases. It algorithmically scans thousands of proxy variables (e.g., prior diagnoses, medications) and selects those that are most likely to be confounders to include in the propensity score model.\nMachine learning versions of hdPS (Karim 2025; Karim, Pang, and Platt 2018): These models are excellent at capturing complex, non-linear relationships and interactions among covariates. See external workshop materials here.\nPost-double-selection method (Belloni, Chernozhukov, and Hansen 2014): It formally recognizes that a confounder must be related to both the exposure and the outcome. It use a machine learning method (e.g., LASSO) to select all covariates that are predictive of the outcome, and then again uses LASSO to select all covariates that are predictive of the exposure. The final set of confounders to adjust for is the union (all variables from both lists). This algorithmically mimics the “Disjunctive Cause Criterion” (adjust for causes of Exposure or Outcome). It is robust and avoids the biases of selecting based only on the outcome. Runs a simple (non-penalized) regression for the final estimate, adjusting for the union set.\nOutcome-Adaptive Lasso (Shortreed and Ertefaie 2017; Baldé, Yang, and Lefebvre 2023): This is a variation of LASSO that essentially performs “double selection” in a single step. It’s a penalized regression (LASSO) for the outcome model, but the penalty for each covariate is adapted (weighted). Covariates that are strongly predictive of the exposure are given a smaller penalty, making them more likely to be kept in the final outcome model, regardless of their association with the outcome.\nCollaborative Targeted Maximum Likelihood Estimation (C-TMLE) (Laan and Gruber 2010): It uses machine learning (often a “Super Learner” that combines many ML algorithms) to build the best possible outcome model. Then, it collaboratively uses information from that model to decide which covariates also need to go into the propensity score model to minimize bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollapsibility and the Choice of Effect Measure\n\n\n\nA crucial, and often overlooked, aspect of statistical adjustment is the concept of collapsibility. An effect measure is said to be collapsible if the marginal (crude) measure of association is equal to a weighted average of the stratum-specific measures of association after conditioning on another variable. This property has profound implications for how we interpret adjusted estimates.\nIn the absence of confounding, some effect measures, like the Risk Difference (RD) and Risk Ratio (RR), are collapsible. This means that if a variable is not a confounder, adjusting for it will not change the effect estimate. However, other common measures, most notably the Odds Ratio (OR), are non-collapsible.\nThe non-collapsibility of the odds ratio is a mathematical property stemming from the non-linearity of the logistic model’s link function. It means that the adjusted OR can be different from the crude OR even when there is no confounding. This phenomenon, where an association in a population differs from the association within its subgroups, is also known as Simpson’s Paradox (in the absence of confounding). This is precisely why the change-in-estimate criterion for confounder selection is invalid when using odds ratios—a change in the OR upon adjustment does not necessarily signal the presence of confounding.\n\n\n\n\n\n\n\n\nSimpson’s Paradox: A Case Study in Bias\n\n\n\nSimpson’s Paradox is a statistical phenomenon where an association observed in a population is different from—and often in the opposite direction of—the associations observed in all of its subgroups. This paradox is a powerful illustration of how failing to account for a key third variable (a confounder or a collider) can lead to completely erroneous conclusions.\nA famous example is the “Birthweight Paradox,” where maternal smoking appeared to be protective against infant mortality among low-birthweight infants, a finding that contradicted the known harms of smoking. This occurred because birthweight acted as a collider. Adjusting for it induced a spurious association between smoking and other unmeasured causes of mortality (e.g., birth defects).\n\n\n\n\n\n\n\n\nUnpacking Effect Heterogeneity: Interaction vs. Effect Modification\n\n\n\nThe effect of an exposure may not be uniform across a population. A third variable can alter the exposure-outcome relationship, a phenomenon that leads to frequent confusion between two distinct concepts: interaction and effect modification.\nFormal Definitions\nWhile often used interchangeably, these terms address different causal questions:\n\n\nEffect Modification: This occurs when the causal effect of a single exposure (e.g., smoking) on an outcome (hypertension) differs across strata of a second variable (e.g., education level). The question is: “Is the effect of smoking different for people with high education versus people with low education?” This involves only one intervention (on smoking). The variable ‘education’ is treated as a baseline characteristic defining subgroups.\n\nInteraction: This refers to the joint causal effect of two exposures (e.g., smoking and low education) on an outcome (hypertension). The question is: “Is the effect of intervening on both smoking and education greater than the sum of the effects of intervening on each one alone?” This involves two distinct interventions and assesses synergy or antagonism.\nImplications for Confounding Control\nThe distinction is critical for analytical strategy:\n\n\nTo assess Effect Modification: When investigating if education modifies the effect of smoking on hypertension, a researcher only needs to control for the set of confounders of the smoking -&gt; hypertension relationship.\n\nTo assess Interaction: When investigating the causal interaction between smoking and education, a researcher must control for all confounders of the smoking -&gt; hypertension relationship AND all confounders of the education -&gt; hypertension relationship. This is a much more demanding requirement.\nThe Role of the Scale: Effect Measure Modification\nWhether modification is detected can depend on the statistical scale used (e.g., additive scale for Risk Difference vs. multiplicative scale for Risk Ratio). For this reason, the more precise term is effect measure modification. A statistical finding of interaction is a property of the chosen model and does not necessarily correspond to a specific biological mechanism.\nReporting guideline\nSee Knol and VanderWeele (2012)\n\n\n\n\n\n\nComparison of Reporting Guidelines (Knol & Vanderweele, 2012)\n\n\n\n\n\n\n\n\n\nRecommendations from Knol & Vanderweele (2012)\n\n\n\nReporting Component\nGuideline for Effect Modification\nGuideline for Interaction\n\n\n\n\nPurpose\nTo show how the effect of one primary exposure (A) is modified by the strata of another factor (X).\nTo show the causal, joint effect of two distinct exposures (A and B) acting together.\n\n\nStep 1: Joint Effects\nRequired. (e.g., ORs for all A/X combinations vs. a single reference, A=0, X=0).\nRequired. (e.g., ORs for all A/B combinations vs. a single reference, A=0, B=0).\n\n\nStep 2: Stratum-Specific Effects\nRequired (SUBSET). Show only the effect of A within each stratum of X.\nRequired (FULL). Show the effect of A within each stratum of B... ...AND... ...the effect of B within each stratum of A.\n\n\nStep 3: Interaction Measures\nRequired. Report measures for both additive (e.g., RERI) and multiplicative (e.g., ROR) scales, with CIs and p-values.\nRequired. Report measures for both additive (e.g., RERI) and multiplicative (e.g., ROR) scales, with CIs and p-values.\n\n\nStep 4: Confounder Adjustment\nRequired. Adjust for confounders of the primary A-D relationship.\nRequired. Adjust for confounders of *both* the A-D relationship *and* the B-D relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo revisit or deepen your grasp of these two concepts, consider reviewing this external tutorial.\n\n\n\n\n\n\n\n\nAvoiding Misinterpretation: The Table 2 Fallacy\n\n\n\nOne of the most common errors in reporting observational research is the Table 2 Fallacy. This fallacy is the practice of presenting a single multivariable regression model and interpreting the coefficients for all variables—the primary exposure and all adjustment covariates—as if they are equally valid estimates of the total causal effect of each variable on the outcome.\nWhy A Single Model Fails: A DAG-Based Explanation\nA multivariable regression model is built to answer a single, specific causal question. The adjustment set required to estimate the causal effect of one variable is often different from the set required to estimate the effect of another.\nConsider a DAG for the effects of smoking, age, and hypertension:\n\n\nCausal Question 1: What is the total effect of Smoking on Hypertension?\n\nAssume Age is a common cause of both Smoking and Hypertension. To get an unbiased estimate of the total effect of Smoking, one must adjust for Age. The appropriate model is: Hypertension ~ Smoking + Age. The coefficient for Smoking can be interpreted as the total causal effect.\n\n\n\nCausal Question 2: What is the total effect of Age on Hypertension?\n\nIn this same DAG, Smoking may be a mediator of the effect of Age (i.e., Age -&gt; Smoking -&gt; Hypertension). To estimate the total effect of Age, one must not adjust for the mediator, Smoking. The model built for Question 1 does adjust for Smoking. Therefore, the coefficient for Age in that first model is not an estimate of the total effect; it is an estimate of the controlled direct effect—the effect of Age on Hypertension that does not operate through the Smoking pathway.\n\n\nBest Practices for Reporting\nTo avoid the Table 2 Fallacy, analysis and reporting must be driven by a “one exposure, one model” principle:\n\n\nBe Explicit: Clearly state the single primary exposure of interest for each model.\n\nUse Multiple Models: If causal effects are desired for multiple variables, fit a separate, correctly specified model for each one.\n\nStructure Tables Clearly: The primary results table should only show the effect estimate for the main exposure of interest. The covariates used for adjustment should be listed in a footnote, not in the table with their own effect estimates.",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#video-lesson-slides",
    "href": "confounding0.html#video-lesson-slides",
    "title": "Concepts (R)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nConfounding\n\n\n\n\nEffect modification\n\n\nTable 2 fallacy",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#links",
    "href": "confounding0.html#links",
    "title": "Concepts (R)",
    "section": "Links",
    "text": "Links\nConfounding\n\nGoogle Slides\nPDF Slides\n\nEffect modification\n\nGoogle Slides\nPDF Slides\n\nTable 2 fallacy\n\nGoogle Slides\nPDF Slides\nExternal link from dagitty",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding0.html#references",
    "href": "confounding0.html#references",
    "title": "Concepts (R)",
    "section": "References",
    "text": "References\n\n\n\n\nBaldé, Ismaila, Yi Archer Yang, and Geneviève Lefebvre. 2023. “Reader Reaction to ‘Outcome-Adaptive Lasso: Variable Selection for Causal Inference’ by Shortreed and Ertefaie (2017).” Biometrics 79 (1): 514–20.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls.” Review of Economic Studies 81 (2): 608–50.\n\n\nBours, Martijn JL. 2021. “Tutorial: A Nontechnical Explanation of the Counterfactual Definition of Effect Modification and Interaction.” Journal of Clinical Epidemiology 134: 113–24.\n\n\nBursac, Zoran, C Heath Gauss, David Keith Williams, and David W Hosmer. 2008. “Purposeful Selection of Variables in Logistic Regression.” Source Code for Biology and Medicine 3 (1): 17.\n\n\nEtminan, Mahyar, Gary S Collins, and Mohammad Ali Mansournia. 2020. “Using Causal Diagrams to Improve the Design and Interpretation of Medical Research.” Chest 158 (1): S21–28.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal Diagrams for Epidemiologic Research.” Epidemiology, 37–48.\n\n\nHeinze, Georg, Christine Wallisch, and Daniela Dunkler. 2018. “Variable Selection–a Review and Recommendations for the Practicing Statistician.” Biometrical Journal 60 (3): 431–49.\n\n\nHosmer, Jr., David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression, 3rd Edition. Hoboken, NJ: John Wiley & Sons.\n\n\nKarim, Mohammad Ehsanul. 2025. “High-Dimensional Propensity Score and Its Machine Learning Extensions in Residual Confounding Control.” The American Statistician 79 (1): 72–90.\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, Huah Shin Ng, Feng Zhu, Hanna A Frank, and Helen Tremlett. 2025. “Evaluating the Role of High-Dimensional Proxy Data in Confounding Adjustment in Multiple Sclerosis Research: A Case Study.” Pharmacoepidemiology and Drug Safety 34 (2): e70112.\n\n\nKarim, Mohammad Ehsanul, and Yang Lei. 2025. “How Effective Are Machine Learning and Doubly Robust Estimators in Incorporating High-Dimensional Proxies to Reduce Residual Confounding?” Pharmacoepidemiology and Drug Safety 34 (5): e70155.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nKnol, Mirjam J, and Tyler J VanderWeele. 2012. “Recommendations for Presenting Analyses of Effect Modification and Interaction.” International Journal of Epidemiology 41 (2): 514–20.\n\n\nLaan, Mark J van der, and Susan Gruber. 2010. “Collaborative Double Robust Targeted Maximum Likelihood Estimation.” The International Journal of Biostatistics 6 (1): 17.\n\n\nLederer, David J, Scott C Bell, Richard D Branson, James D Chalmers, Rebecca Marshall, David M Maslove, Peter W Stewart, et al. 2019. “Control of Confounding and Reporting of Results in Causal Inference Studies: Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals.” Annals of the American Thoracic Society 16 (1): 22–28.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology 20 (4): 512–22.\n\n\nShortreed, Susan M, and Ashkan Ertefaie. 2017. “Outcome-Adaptive Lasso: Variable Selection for Causal Inference.” Biometrics 73 (4): 1111–22.\n\n\nTennant, Paul W, Elizabeth J Murray, Kathryn F Arnold, Leigh Berrie, Matthew P Fox, Samantha C Gadd, and George TH Ellison. 2021. “Use of Directed Acyclic Graphs (DAGs) to Identify Confounders in Applied Health Research: Review and Recommendations.” International Journal of Epidemiology 50 (2): 620–32.\n\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\n\n———. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34 (3): 211–19.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98.",
    "crumbs": [
      "Causal roles",
      "Concepts (R)"
    ]
  },
  {
    "objectID": "confounding1.html",
    "href": "confounding1.html",
    "title": "Confounding",
    "section": "",
    "text": "This tutorial aims to delve into the role of confounding variables in data analysis, especially in the context of big data. We will examine each of these using simulations built on Directed Acyclic Graphs (DAGs). The objective is to understand whether a simple regression adjusting for the confounder variable can correctly estimate treatment effects in such a large sample.\n\n# devtools::install_github('osofr/simcausal')\nrequire(simcausal)\n\nBig data: What if we had 1,000,000 (one million) observations? Would that give us true result? Let’s try to answer that using DAGs.\nLet us consider\n\nL is continuous variable\nA is binary treatment\nY is continuous outcome\n\nAn example of A could be receiving the right heart catheterization (RHC) procedure or not, Y could be the length of hospital stay, and L could be age (see here).\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\nTo perform the lab, we’ll need the simcausal R package. This package may not be available on CRAN but can be installed from the author’s GitHub page.\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 1.3 * A, sd = 0.1)\nDset &lt;- set.DAG(D)\n\nNote that rnorm and rbern generate random samples from Normal and Bernoulli distributions, respectively. In the above code chuck, we set the parameters to generate L from Normal distribution with a mean of 10 and a standard deviation of 1. Similarly, we set the parameters to generate A from the Bernoulli distribution with the probability of logit of (-10 + 1.1 \\(\\times\\) L). Finally, we set the parameters to generate Y from the Normal distribution with a mean of (0.5 \\(\\times\\) L + 1.3 \\(\\times\\) A) and a standard deviation of 0.1.\nGenerate DAG\nLet us draw a directed acyclic graph (DAG). Below we use the plotDAG function from the simcausal package. However, we can draw a DAG using DAGitty.\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nAs per the DAG, L is a confounder. When exploring the relationship between A and Y, we need to adjust our model for L to get an unbiased estimate of A on Y.\nGenerate Data\nNow, let us simulate the data using the defined parameters and the DAG. Below, we generate data for 1,000,000 participants. We set a seed 123 so that one can reproduce the same dataset.\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\nNow we will fit the generalized linear model (glm), without and with adjusting for L.\n\n# Not adjusted for L\nfit0 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        4.69        1.75\n\n# Adjusted for L\nfit &lt;- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           L \n#&gt;         0.0         1.3         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, our true treatment effect is 1.3. When we estimate the relationship between A and Y without adjusting for L, we obtain an estimated effect of 1.75. However, this is not the true effect. The true treatment effect of 1.3 is recovered when we adjust for L.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nLet us see the results when there is no treatment effect, i.s., the true treatment effect is zero.\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"L\", distr = \"rnorm\", mean = 10, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10 + 1.1*L)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * L + 0 * A, sd = .1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 &lt;- glm(Y ~ A, family = \"gaussian\", data = Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        4.69        0.45\n\n# Adjusted for L\nfit &lt;- glm(Y ~ A + L, family = \"gaussian\", data = Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           L \n#&gt;         0.0         0.0         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this second scenario, the true treatment effect is zero. There is no arrow from A to Y in the DAG, but L remains a common cause for both. Upon analyzing the data without adjusting for L, we observe an induced correlation between A and Y. This correlation disappears, confirming the true null effect, when we adjust for L.\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Causal roles",
      "Confounding"
    ]
  },
  {
    "objectID": "confounding2.html",
    "href": "confounding2.html",
    "title": "Mediator",
    "section": "",
    "text": "Mediators play a crucial role in understanding how a treatment variable affects an outcome. A mediator variable lies in the pathway between the treatment and outcome, essentially transmitting or explaining the effect of the treatment variable. In this expanded tutorial, we’ll delve into more details based on the lecture, specifically focusing on the true direct and indirect effects when a mediator is present.\n\n# Load required packages\nlibrary(simcausal)\n\nLet us consider\n\nM is continuous variable\nA is binary treatment\nY is continuous outcome\n\nAn example of A could be receiving the right heart catheterization (RHC) procedure or not, Y could be the length of hospital stay, and M could be the number of comorbidities.\nNon-null effect\n\nTrue treatment effect = 1.3\n\nOur true treatment effect is 1.3, and the mediator variable’s effect on the outcome Y is 0.5. It’s important to differentiate between these effects.\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nAs we can see, M is a mediator (mediates the effect of A on Y). When exploring the total effect of A on Y, we should not adjust our model for M.\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for M\nfit0 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        5.00        1.69\n\n# Adjusted for M\nfit &lt;- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nYou might notice a total effect that could differ from the true effects. In the lecture example, a crude association showed an effect of 1.69, which is the total effect combining both direct and indirect pathways.\nUpon adjusting for M, the coefficients will show you the direct effect of A on Y and the indirect effect through M. These should align closely with our true effects: a direct effect of 1.3 and an indirect effect of 0.5.\nIn this expanded tutorial, we’ve shown how essential it is to consider mediator variables when estimating treatment effects. We’ve also illustrated how adjusting for mediators allows you to differentiate between true direct and indirect effects, thereby reducing the risk of drawing incorrect conclusions from your data.\nDetailed on the mediation analysis can be found in the Mediation analysis chapter.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"M\", distr = \"rnorm\", mean = 10 + 0.9 * A, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M, sd = .1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for M\nfit0 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        5.00        0.39\n\n# Adjusted for M\nfit &lt;- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         0.0         0.5\n\n\n\n\n\n\n\nImportant\n\n\n\nTotal Effect: If you want to measure the “total effect” of a treatment on an outcome, then you typically don’t adjust for the mediator. The reason is that the total effect captures both the direct effect of the treatment on the outcome and the indirect effect through the mediator.\nDirect and Indirect Effects: If you want to separate out the direct and indirect effects, then you would adjust for the mediator. In essence, when you control for the mediator, what remains is the direct effect of the treatment on the outcome.\nLinearity and Decomposition: In linear models with continuous outcomes, it is more straightforward to decompose total effects into direct and indirect effects. The mathematics get more complicated in non-linear models or when dealing with non-continuous outcomes.",
    "crumbs": [
      "Causal roles",
      "Mediator"
    ]
  },
  {
    "objectID": "confounding3.html",
    "href": "confounding3.html",
    "title": "Collider",
    "section": "",
    "text": "In causal inference, understanding the role of colliders is crucial. A collider is a variable that is a common effect of two or more variables. Adjusting for a collider can introduce bias into your estimates.\n\n# Load required packages\nlibrary(simcausal)\n\nIn a DAG, a collider is a variable influenced by two or more other variables. In our case, L is a collider because it is affected by both A (the treatment) and Y (the outcome). When you adjust for a collider like L, you could introduce bias into your estimates, as demonstrated in the examples below.\nLet us consider\n\nL is continuous variable\nA is binary treatment/exposure\nY is continuous outcome\n\nAn example of A could be a genetic variable (e.g., skin color), Y could be an environmental variable (e.g., indoor air pollution), and L could be disease conditions (e.g., number of comorbidities) (detailed expamles here).\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 1.3 * A, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.29\n\n# Adjusted for L\nfit &lt;- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           L \n#&gt;        0.00        0.58        0.05\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen not adjusting for L, we recover the true effect close to 1.3. Adjusting for L introduces bias, making the estimate unreliable.\n\n\nNull effect\n\nTrue treatment effect = 0\n\nData generating process\n\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rbern\", prob = plogis(-10)) +\n  node(\"Y\", distr = \"rnorm\", mean = 0, sd = .1) +\n  node(\"L\", distr = \"rnorm\", mean = 10 * Y + 1.3 * A, sd = 1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# Not adjusted for L\nfit0 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A \n#&gt;        0.00       -0.01\n\n# Adjusted for L\nfit &lt;- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           L \n#&gt;        0.00       -0.07        0.05\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen the true effect is null, not adjusting for L shows an estimate close to zero. Adjusting for L moves the estimate away from the null value, introducing bias.\n\n\nEven 1,000,000 observations were not enough to recover true treatment effect! But we are close enough.",
    "crumbs": [
      "Causal roles",
      "Collider"
    ]
  },
  {
    "objectID": "confounding3b.html",
    "href": "confounding3b.html",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Introduction\nIn this tutorial, we will use simcausal in R to simulate data and demonstrate how Simpson’s Paradox arises in a birthweight example similar to the “Birthweight Paradox” discussed by Hernández-Díaz et al. (2006). The paradox arises when adjusting for birthweight, leading to misleading conclusions about the relationship between maternal smoking and infant mortality. We will show how improper adjustment for birthweight, a collider in this context, can lead to biased estimates, and how to correct for this.\nBirth Weight Paradox\nStudies showed that while maternal smoking increased the risk of low birth weight (LBW) and infant mortality, among LBW infants, those born to smokers had lower infant mortality than LBW infants born to non-smokers. This seemingly paradoxical finding suggests that smoking could be “protective” for LBW infants, contradicting the established harmful effects of smoking during pregnancy.\nThis is a classic example of Simpson’s Paradox. The statistical reversal happens upon stratification, and the reason in this specific causal structure is that birthweight acts as a collider. Understanding this causal role is the only way to determine whether the stratified or unstratified result is the correct one for our research question.\nData Generation\nA Directed Acyclic Graph (DAG) is more than a mere illustration; it is a formal representation of a set of non-parametric causal assumptions about the data-generating process. The DAG provided in the tutorial encapsulates the hypothesized relationships between maternal smoking, an unmeasured factor, low birthweight, and infant mortality.\nSetup\nWe will use the following R packages:\n\nlibrary(simcausal)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(knitr)\nlibrary(kableExtra)\n\nData Generation\nWe will generate data to simulate the birthweight paradox. We assume that:\n\n\nSmoking influences Birthweight (low birth weight).\n\nBirthweight influences Infant Mortality.\nThere are unmeasured factors (e.g., birth defects) that also influence both birthweight and infant mortality.\n\nWe will set up this data generating process using simcausal.\n\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"Smoking\", distr = \"rbern\", prob = 0.3) +\n  node(\"Unmeasured\", distr = \"rbern\", prob = 0.2) +\n  node(\"LowBirthweight\", distr = \"rbern\", prob = plogis(-1 + 1.5*Smoking + 2*Unmeasured)) +\n  node(\"Mortality\", distr = \"rbern\", prob = plogis(-2 + 4*LowBirthweight + 4*Unmeasured))\n\nDset &lt;- set.DAG(D)\n\nVisualizing the DAG\nLet’s plot the DAG to see how these variables are related:\n\nplotDAG(Dset, xjitter = 0.1, yjitter = 0.1, \n        edge_attrs = list(width = 1, arrow.width = 1, arrow.size = 1),\n        vertex_attrs = list(size = 15, label.cex = 0.8))\n\n\n\n\n\n\n\nThis DAG represents the relationships between maternal smoking, low birth weight, and mortality, with an unmeasured confounder affecting both birthweight and mortality. Low birth weight is a collider because it has two incoming arrows from both Smoking and Birth Defect (Unmeasured).\n\n\nDAG Plot\n\nThe Dual Role of Low Birthweight: Mediator and Collider\nThe key to this puzzle is that LowBirthweight plays two different and conflicting roles depending on which causal path you examine.\n\nMediator: On the path Smoking → LowBirthweight → Mortality, LBW is a mediator. It is a mechanism through which smoking causes death. Smoking leads to LBW, which in turn leads to mortality. The purpose of mediation analysis is often to isolate the direct effect from this indirect, mediated path.\nCollider: On the path Smoking → LowBirthweight ← Unmeasured, LBW is a collider. A collider is a variable that is a common effect of two other variables (arrows “collide” into it). The fundamental rule of DAGs concerning colliders is that one must not condition on them or their descendants. Conditioning on a collider opens the non-causal path between its parents, inducing a spurious statistical association where none may have existed before. Conditioning on a collider (by adjusting for it in a model or stratifying by it) is a major analytical error.\n\nThis dual role creates an analytical trap. The statistical act of adjusting for LowBirthweight—which you might do to block its mediating effect—simultaneously conditions on it as a collider. This opens a spurious “backdoor” path between Smoking and the Unmeasured factors, introducing severe bias and leading to the paradoxical conclusion.\nSimulate Data\nWe now simulate data based on the DAG structure.\n\nsim_data &lt;- sim(Dset, n = 10000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(sim_data) %&gt;%\n  kable(\"html\", caption = \"First 6 Rows of Simulated Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nFirst 6 Rows of Simulated Data\n\nID\nSmoking\nUnmeasured\nLowBirthweight\nMortality\n\n\n\n1\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n1\n\n\n3\n0\n1\n1\n1\n\n\n4\n1\n0\n1\n1\n\n\n5\n1\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n\n\n\n\n\n\nVisualizing Hypothetical Distributions\n\n# Simulate continuous birth weight based on binary LowBirthweight variable\nset.seed(123)\nsim_data$BirthWeight &lt;- ifelse(sim_data$LowBirthweight == 1, \n                               rnorm(n = sum(sim_data$LowBirthweight == 1), \n                                     mean = 2200, sd = 300),  # Low birthweight\n                               rnorm(n = sum(sim_data$LowBirthweight == 0), \n                                     mean = 3300, sd = 500))  # Normal birthweight\nsim_data$Smoking &lt;- factor(sim_data$Smoking, \n                           levels = c(0, 1), \n                           labels = c(\"Non-smoking\", \"Smoking\"))\nsim_data$LowBirthweight &lt;- factor(sim_data$LowBirthweight, \n                                  levels = c(0, 1), \n                                  labels = c(\"Normal Birthweight\", \"Low Birthweight\"))\n# Create a density plot for birth weight distribution by smoking status\nggplot(sim_data, aes(x = BirthWeight, fill = Smoking, color = Smoking)) +\n  geom_density(alpha = 0.3, size = 1.2) +\n  geom_vline(xintercept = 2500, linetype = \"solid\", color = \"black\", size = 1) +\n  scale_fill_manual(values = c(\"Smoking\" = \"red\", \"Non-smoking\" = \"blue\")) +\n  scale_color_manual(values = c(\"Smoking\" = \"red\", \"Non-smoking\" = \"blue\")) +\n  labs(title = \"Distribution of Birth Weights by Smoking Status\",\n    subtitle = \"Low Birth Weight (LBW) is typically defined as a birth weight of less than 2,500 grams\",\n       x = \"Birth Weight (g)\", \n       y = \"Density\",\n       fill = \"Smoking Status\",\n       color = \"Smoking Status\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nAnalysis\nCrude Bivariate Relationships\n\n# Calculate the summary statistics (mean birth weight, LBW prevalence, and mortality prevalence) by smoking status\nsummary_stats &lt;- sim_data %&gt;%\n  group_by(Smoking) %&gt;%\n  summarise(\n    mean_birthweight = mean(BirthWeight, na.rm = TRUE),  # Mean birth weight\n    prevalence_LBW = mean(LowBirthweight == \"Low Birthweight\") * 100,  # Prevalence of LBW in percentage\n    prevalence_mortality = mean(Mortality) * 100  # Prevalence of mortality in percentage\n  ) %&gt;%\n  mutate(\n    mean_birthweight = format(round(mean_birthweight, 1), big.mark = \",\"),  # Round and format birth weight\n    prevalence_LBW = round(prevalence_LBW, 1),  # Round LBW prevalence to 1 decimal place\n    prevalence_mortality = round(prevalence_mortality, 1)  # Round mortality prevalence to 1 decimal place\n  )\n\n# Display the formatted table with kable\nsummary_stats %&gt;%\n  kable(\"html\", caption = \"Summary Statistics: Mean Birth Weight, LowBirthWeight (LBW) Prevalence, and Mortality Prevalence\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nSummary Statistics: Mean Birth Weight, LowBirthWeight (LBW) Prevalence, and Mortality Prevalence\n\nSmoking\nmean_birthweight\nprevalence_LBW\nprevalence_mortality\n\n\n\nNon-smoking\n2,895.1\n36.6\n45.8\n\n\nSmoking\n2,552.5\n67.9\n66.2\n\n\n\n\n\n\nAnalysis Part 1: Attempting to Estimate the Direct Effect\nWe now show what happens when we adjust for LowBirthweight, which acts as a collider.\n\n# Model adjusted for LowBirthweight (incorrect adjustment)\nfit_incorrect &lt;- glm(Mortality ~ Smoking + LowBirthweight, family = binomial(), data = sim_data)\npublish(fit_incorrect, print = FALSE)$regressionTable %&gt;%\n  kable(\"html\", caption = \"Adjusted Model Regression Table\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nAdjusted Model Regression Table\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nSmoking\nNon-smoking\nRef\n\n\n\n\n\nSmoking\n0.73\n[0.63;0.84]\n&lt;1e-04\n\n\nLowBirthweight\nNormal Birthweight\nRef\n\n\n\n\n\nLow Birthweight\n59.40\n[51.75;68.18]\n&lt;1e-04\n\n\n\n\n\n\nExplanation: To estimate the direct effect of smoking (the effect not mediated through birthweight), the standard approach would be to adjust for the mediator, LowBirthweight. However, in this causal structure, this adjustment is incorrect because it simultaneously conditions on a collider. This opens a spurious backdoor path between Smoking and Unmeasured, introducing collider bias. The resulting odds ratio of 0.73 is not a valid estimate of the direct effect; it is a hopelessly biased number.\nStratification\nWe will now stratify the data by Low Birthweight and calculate the effect of smoking on mortality separately for low birthweight and normal birthweight infants.\n\n# Subset for low birthweight\nfit_stratified_LBW &lt;- glm(Mortality ~ Smoking, family = binomial(), data = subset(sim_data, LowBirthweight == \"Low Birthweight\"))\npublish(fit_stratified_LBW, print = FALSE)$regressionTable %&gt;%\n  kable(\"html\", caption = \"Stratified Model Regression Table\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nStratified Model Regression Table\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nSmoking\nNon-smoking\nRef\n\n\n\n\n\nSmoking\n0.69\n[0.55;0.85]\n0.000565\n\n\n\n\n\n\n# Subset for normal birthweight\nfit_stratified_NBW &lt;- glm(Mortality ~ Smoking, family = binomial(), data = subset(sim_data, LowBirthweight == \"Normal Birthweight\"))\npublish(fit_stratified_NBW, print = FALSE)$regressionTable %&gt;%\n  kable(\"html\", caption = \"Stratified Model Regression Table\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nStratified Model Regression Table\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nSmoking\nNon-smoking\nRef\n\n\n\n\n\nSmoking\n0.77\n[0.63;0.93]\n0.007758\n\n\n\n\n\n\nInteraction Approach\nWe include an interaction term between Smoking and LowBirthweight to test whether the effect of smoking on mortality differs by birthweight status.\n\nfit_interaction &lt;- glm(Mortality ~ Smoking * LowBirthweight, family = binomial(), data = sim_data)\npublish(fit_interaction, print = FALSE)$regressionTable %&gt;%\n  kable(\"html\", caption = \"Interaction Model Regression Table\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nInteraction Model Regression Table\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nSmoking(Non-smoking): LowBirthweight(Low Birthweight vs Normal Birthweight)\n\n61.73\n[52.00;73.28]\n&lt; 1e-04\n\n\nSmoking(Smoking): LowBirthweight(Low Birthweight vs Normal Birthweight)\n\n55.21\n[43.70;69.74]\n&lt; 1e-04\n\n\nLowBirthweight(Normal Birthweight): Smoking(Smoking vs Non-smoking)\n\n0.77\n[0.63;0.93]\n0.007758\n\n\nLowBirthweight(Low Birthweight): Smoking(Smoking vs Non-smoking)\n\n0.69\n[0.55;0.85]\n0.000565\n\n\n\n\n\n\nCorrect Approach: No Adjustment for LowBirthweight\nTo estimate the total causal effect of smoking on mortality (the net effect through all pathways), we must not adjust for the mediator, LowBirthweight. Our DAG shows no other open backdoor paths that need blocking. Therefore, the unadjusted model correctly estimates the total causal effect.\n\n# Correct model (no adjustment for LowBirthweight)\nfit_correct &lt;- glm(Mortality ~ Smoking, family = binomial(), data = sim_data)\npublish(fit_correct, print = FALSE)$regressionTable %&gt;%\n  kable(\"html\", caption = \"Unadjusted Model Regression Table\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\nUnadjusted Model Regression Table\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nSmoking\nNon-smoking\nRef\n\n\n\n\n\nSmoking\n2.31\n[2.12;2.53]\n&lt;1e-04\n\n\n\n\n\n\nComparing Estimates\n\n# Extract ORs from publish output for each model\nor_incorrect &lt;- publish(fit_incorrect, print = FALSE)$regressionTable[2, \"OddsRatio\"]\nor_stratified_LBW &lt;- publish(fit_stratified_LBW, print = FALSE)$regressionTable[2, \"OddsRatio\"]\nor_stratified_NBW &lt;- publish(fit_stratified_NBW, print = FALSE)$regressionTable[2, \"OddsRatio\"]\nor_interaction1 &lt;- publish(fit_interaction, print = FALSE)$regressionTable[3, \"OddsRatio\"]\nor_interaction2 &lt;- publish(fit_interaction, print = FALSE)$regressionTable[4, \"OddsRatio\"]\n\n\n\nDAG plot with incorrect adjustment\n\n\n# Extract ORs from publish output for correct model\nor_correct &lt;- publish(fit_correct, print = FALSE)$regressionTable[2, \"OddsRatio\"]\n\n\n\nDAG plot with no adjustment\n\nNow we compare the estimates from:\n\nThe incorrect model (adjusted for LowBirthweight), with an OR of 0.73.\nThe stratified model\n\nfor low birthweight infants, with an OR of 0.69.\nfor normal birthweight infants, with an OR of 0.77.\n\n\nThe interaction model, where the OR for smokers is\n\n0.69 (Low Birthweight),\n0.77 (Normal Birthweight).\n\n\nThe correct model (unadjusted for LowBirthweight), with an OR of 2.31.\n\nThese comparisons highlight how adjusting for a collider (Low Birthweight) introduces bias, while avoiding this adjustment gives an unbiased estimate of the effect of Smoking on Mortality.\nVisualizing Estimates\n\n# Helper function to clean and convert ORs and CIs\nextract_or_ci &lt;- function(or_str, ci_str) {\n  or_val &lt;- ifelse(or_str == \"Ref\", NA, as.numeric(or_str))  # Convert OR to numeric, handle Ref\n  \n  # Split the CI string, handle empty or malformed CI strings\n  if (ci_str != \"\" && grepl(\"\\\\[\", ci_str)) {\n    ci_vals &lt;- as.numeric(gsub(\"\\\\[|\\\\]\", \"\", unlist(strsplit(ci_str, \";\"))))  # Split and convert CI\n  } else {\n    ci_vals &lt;- c(NA, NA)  # Handle missing CIs\n  }\n  \n  return(list(OR = or_val, CI.lower = ci_vals[1], CI.upper = ci_vals[2]))\n}\n\n# Extract ORs and CIs for each model using the helper function\nor_ci_incorrect &lt;- extract_or_ci(publish(fit_incorrect, print = FALSE)$regressionTable[2, \"OddsRatio\"],\n                                 publish(fit_incorrect, print = FALSE)$regressionTable[2, \"CI.95\"])\n\nor_ci_stratified_LBW &lt;- extract_or_ci(publish(fit_stratified_LBW, print = FALSE)$regressionTable[2, \"OddsRatio\"],\n                                      publish(fit_stratified_LBW, print = FALSE)$regressionTable[2, \"CI.95\"])\n\nor_ci_stratified_NBW &lt;- extract_or_ci(publish(fit_stratified_NBW, print = FALSE)$regressionTable[2, \"OddsRatio\"],\n                                      publish(fit_stratified_NBW, print = FALSE)$regressionTable[2, \"CI.95\"])\n\nor_ci_interaction1 &lt;- extract_or_ci(publish(fit_interaction, print = FALSE)$regressionTable[3, \"OddsRatio\"],\n                                    publish(fit_interaction, print = FALSE)$regressionTable[3, \"CI.95\"])\n\nor_ci_interaction2 &lt;- extract_or_ci(publish(fit_interaction, print = FALSE)$regressionTable[4, \"OddsRatio\"],\n                                    publish(fit_interaction, print = FALSE)$regressionTable[4, \"CI.95\"])\n\nor_ci_correct &lt;- extract_or_ci(publish(fit_correct, print = FALSE)$regressionTable[2, \"OddsRatio\"],\n                               publish(fit_correct, print = FALSE)$regressionTable[2, \"CI.95\"])\n\n\n# Step 2: Create a data frame for the forest plot\nforest_data &lt;- data.frame(\n  Model = c(\"Incorrect (Adjusted for LBW)\",\n            \"Stratified (Low Birthweight)\",\n            \"Stratified (Normal Birthweight)\",\n            \"Interaction (Low Birthweight)\",\n            \"Interaction (Normal Birthweight)\",\n            \"Correct (Unadjusted)\"),\n  OR = c(or_ci_incorrect$OR, or_ci_stratified_LBW$OR, or_ci_stratified_NBW$OR, or_ci_interaction2$OR, or_ci_interaction1$OR, or_ci_correct$OR),\n  CI.lower = c(or_ci_incorrect$CI.lower, or_ci_stratified_LBW$CI.lower, or_ci_stratified_NBW$CI.lower, or_ci_interaction2$CI.lower, or_ci_interaction1$CI.lower, or_ci_correct$CI.lower),\n  CI.upper = c(or_ci_incorrect$CI.upper, or_ci_stratified_LBW$CI.upper, or_ci_stratified_NBW$CI.upper, or_ci_interaction2$CI.upper, or_ci_interaction1$CI.upper, or_ci_correct$CI.upper)\n)\n\n# Display the data frame to ensure it's correct\nprint(forest_data)\n#&gt;                              Model   OR CI.lower CI.upper\n#&gt; 1     Incorrect (Adjusted for LBW) 0.73     0.63     0.84\n#&gt; 2     Stratified (Low Birthweight) 0.69     0.55     0.85\n#&gt; 3  Stratified (Normal Birthweight) 0.77     0.63     0.93\n#&gt; 4    Interaction (Low Birthweight) 0.69     0.55     0.85\n#&gt; 5 Interaction (Normal Birthweight) 0.77     0.63     0.93\n#&gt; 6             Correct (Unadjusted) 2.31     2.12     2.53\n\n\n# Create the forest plot\nggplot(forest_data, aes(x = OR, y = Model)) +\n  geom_point(size = 3) +\n  geom_errorbarh(aes(xmin = CI.lower, xmax = CI.upper), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\") +  # Reference line at OR = 1\n  labs(title = \"Forest Plot of OR\",\n       x = \"Odds Ratio (OR) and Confidence Intervals\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 12),\n        axis.title.x = element_text(size = 14),\n        plot.title = element_text(size = 16, face = \"bold\"))\n#&gt; Warning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\n#&gt; ℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n#&gt; `height` was translated to `width`.\n\n\n\n\n\n\n\nClarifying the Target: Total Effect vs. Direct Effect\nA frequent source of confusion in causal analysis is the failure to distinguish between different causal questions, or “estimands.” The tutorial correctly demonstrates how to estimate the total causal effect of smoking, while the user’s query focuses on the direct effect. These are distinct quantities that require different analytical strategies.\n\nTotal Causal Effect: This estimand represents the overall impact of the exposure on the outcome, encompassing all causal pathways. It answers the question: “What is the net effect of a mother smoking versus not smoking on the risk of infant mortality?” To estimate the TCE, an analyst must block all non-causal “back-door” paths between the exposure and outcome. In the provided DAG, there are no open back-door paths from Smoking to Mortality. The path through LowBirthweight is a front-door (causal) path, and the path involving the Unmeasured variable is blocked by the collider LowBirthweight. Therefore, as the tutorial’s “Correct Approach” shows, the total causal effect is correctly estimated by a simple unadjusted regression of Mortality on Smoking, which yields an Odds Ratio (OR) of 2.31.\nDirect Effect: This estimand represents the effect of the exposure on the outcome that does not operate through a specific mediator. It answers the question: “What is the effect of a mother smoking on infant mortality, independent of its effect on the baby’s birth weight?” As noted above, the conventional (though in this case, incorrect) approach to isolating this effect is to include the mediator as a covariate in a regression model. The tutorial’s “Incorrect Adjustment” section performs exactly this, regressing Mortality on both Smoking and LowBirthweight.\nCausal Questions Dictate Correct Methods\nThe Birthweight Paradox is a powerful lesson in causal inference. It demonstrates that a naive statistical adjustment, without regard for the underlying causal structure, can produce results that are not just wrong, but dangerously misleading.\n\nThe “paradox” is resolved by causal reasoning: The reversal is caused by conditioning on LowBirthweight, a variable that is simultaneously a mediator of the effect we want to study and a collider that introduces bias from an unmeasured variable.\n\nThe analysis must match the question:\n\nTo estimate the total effect of smoking, we must not adjust for the mediator/collider. The unadjusted OR of 2.31 is the correct estimate for this question.\nEstimating the direct effect is not possible with simple adjustment in this scenario. The adjusted OR of 0.73 is a biased artifact and does not represent a real causal effect.\n\n\n\n\n\n\n\n\n\nThe Quest for the Direct Effect: A Fundamental Problem of Identifiability\n\n\n\nEstimating the direct effect (“What is the effect of smoking on mortality that is not mediated by birthweight?”) is “not possible with simple adjustment” because the necessary information to disentangle the direct effect from the confounding effect of the Unmeasured variable is simply not present in the observable data (Smoking, LowBirthweight, Mortality). No matter how sophisticated a regression model one fits using only these observed variables, it cannot recover an unbiased estimate of the (natural) direct effect. The problem is not with the statistical tool, but with the fundamental limits of what the data can reveal given the causal reality depicted by the DAG. The language of potential outcomes makes this challenge explicit, as it requires estimating “cross-world” counterfactuals (e.g., the outcome under smoking with the mediator at its non-smoking level), which is impossible when an unmeasured confounder links the worlds of the mediator and the outcome.\nAdvanced methods such as instrumental variable analysis represents a powerful theoretical alternative for achieving identification in the face of unmeasured confounding. However, it should not be seen as a simple or universally applicable solution. It requires a fundamental shift in the “burden of belief” from the assumption of no unmeasured confounding (sequential ignorability) to a different, and often more stringent, set of untestable assumptions about the existence of a valid instrument. The choice between these analytical frameworks is not a statistical one, but a scientific one, based on which set of assumptions is most defensible in a specific research context.\n\n\n\n\nKey Insight: The most important step in an analysis is to draw a DAG based on subject-matter knowledge. This causal map is essential for identifying which variables to adjust for and which to leave alone to answer your specific research question correctly.",
    "crumbs": [
      "Causal roles",
      "Simpson's Paradox"
    ]
  },
  {
    "objectID": "confounding4.html",
    "href": "confounding4.html",
    "title": "Z-bias",
    "section": "",
    "text": "Z-bias occurs in the context of causal inference, specifically when using instrumental variables to estimate causal effects. Instrumental variables (IVs) are used to isolate the variation in the treatment variable that is unrelated to the confounding factors, thus providing a pathway to estimate causal effects.\n\n# Load required packages\nlibrary(simcausal)\n\nContinuous Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is continuous outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rnorm\", mean = -1 + 3 * U + 1.3 * A, sd = 0.1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# True data generating mechanism (unattainable as U is unmeasured)\nfit0 &lt;- glm(Y ~ A + U, family=\"gaussian\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A           U \n#&gt;        -1.0         1.3         3.0\n\n# Unadjusted effect (Z not controlled)\nfit1 &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit1),2)\n#&gt; (Intercept)           A \n#&gt;        0.79        5.59\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#&gt;      A \n#&gt; 4.2935\n\n# Adjusted effect (Z  controlled)\nfit2 &lt;- glm(Y ~ A + Z, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           Z \n#&gt;        0.86        5.77       -0.12\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#&gt;        A \n#&gt; 4.465787\n\nBinary Y\n\nU is unmeasured continuous variable\nZ is an instrumental variable\nA is binary treatment\nY is binary outcome\n\nNon-null effect\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"U\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"Z\", distr = \"rnorm\", mean = 2, sd = 1) + \n  node(\"A\", distr = \"rbern\", prob = plogis(-1 + 2*U + 2*Z)) +\n  node(\"Y\", distr = \"rbern\", prob = plogis(-1 + 3 * U + 1.3 * A))\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect\n\n# True data generating mechanism (unattainable as U is unmeasured)\nfit0 &lt;- glm(Y ~ A + U, family=\"binomial\", data=Obs.Data)\nround(coef(fit0),2)\n#&gt; (Intercept)           A           U \n#&gt;       -0.99        1.30        3.01\n\n# Unadjusted effect (Z not controlled)\nfit1 &lt;- glm(Y ~ A, family=\"binomial\", data=Obs.Data)\nround(coef(fit1),2)\n#&gt; (Intercept)           A \n#&gt;        0.40        3.02\n\n# Bias fit 1\ncoef(fit1)[\"A\"] - 1.3\n#&gt;        A \n#&gt; 1.716482\n\n# Adjusted effect (Z  controlled)\nfit2 &lt;- glm(Y ~ A + Z, family=\"binomial\", data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           Z \n#&gt;        0.51        3.29       -0.18\n\n# Bias from fit 2\ncoef(fit2)[\"A\"] - 1.3\n#&gt;        A \n#&gt; 1.991396",
    "crumbs": [
      "Causal roles",
      "Z-bias"
    ]
  },
  {
    "objectID": "confounding5.html",
    "href": "confounding5.html",
    "title": "Collapsibility",
    "section": "",
    "text": "Let us assume we have a regression of hypertension (\\(Y\\)), smoking (\\(A\\)) and a risk factor for outcome, gender (\\(L\\)). Then let us set up 2 regression models:\n\n1st regression model is \\(Y \\sim \\beta \\times A + \\alpha \\times L\\). Here we are conditioning on gender (\\(L\\)).\n2nd regression model is \\(Y \\sim \\beta' \\times A\\)\n\n\nThen regression is collapsible for \\(\\beta\\) over \\(L\\) if \\(\\beta = \\beta'\\) from the 2nd regression omitting \\(L\\). \\(\\beta \\ne \\beta'\\) would mean non-collapsibility. A measure of association (say, risk difference) is collapsible if the marginal measure of association is equal to a weighted average of the stratum-specific measures of association. Non-collapsibility is also knows as Simpson’s Paradox (in absence of confounding of course): a statistical phenomenon where an association between two factors (say, hypertension and smoking) in a population (we are talking about marginal estimate here) is different than the associations of same relationship in subpopulations (conditional on some other factor, say, age; hence talking about conditional estimates). Risk ratio and risk difference are collapsible.\nOdds ratio can be non-collapsible. It can produce different treatment effect estimate for different covariate adjustment sets (see example below of when adjusting form age and sex vs. when adjusting none). This is true even in the absence of confounding. However, according to our definition here, OR is collapsible when we consider gender in the adjustment set.\nNote that, OR non-collapsibility is a consequence of the fact that it is estimated via a logit link function (nonlinearity of the logistic transformation).\nRef:\n\n(Greenland, Pearl, and Robins 1999)\n(Mansournia and Greenland 2015)\n\n\n# Load required packages\nlibrary(simcausal)\nlibrary(tableone)\nlibrary(Publish)\nlibrary(lawstat)\n\nData generating process\nLet us generate the data with smoking being the exposure and hypertension being the outcome.\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"gender\", distr = \"rbern\", prob = 0.7) +\n  node(\"age\", distr = \"rnorm\", mean = 2, sd = 4) +\n  node(\"smoking\", distr = \"rbern\", prob = plogis(.1)) +\n  node(\"hypertension\", distr = \"rbern\", \n       prob = plogis(1 + log(3.5) * smoking + log(.1) * gender + log(7) * age))\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 100000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nBalance check\n\nrequire(tableone)\nCreateTableOne(data = Obs.Data, strata = \"smoking\", vars = c(\"gender\", \"age\"))\n#&gt;                     Stratified by smoking\n#&gt;                      0            1            p      test\n#&gt;   n                  47720        52280                   \n#&gt;   gender (mean (SD))  0.70 (0.46)  0.70 (0.46)  0.403     \n#&gt;   age (mean (SD))     2.02 (4.02)  2.01 (4.00)  0.690\n\nConditional and crude risk difference (RD)\nSince RD is a collapsible measure, the marginal and conditional estimate should be approximately the same.\nFull list of risk factors for outcome (2 variables)\n\n## RD\nrequire(Publish)\nfitx0 &lt;- glm(hypertension ~ smoking + gender + age, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator …robust variance estimator (or bootstrap) should be used to obtain valid standard errors.”)\nStrtatum specific (2 variables)\n\nfitx1 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age &lt; 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nfitx2 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age &lt; 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nfitx3 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1 & age &gt;= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nfitx4 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0 & age &gt;= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nround(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"],\n             coef(fitx3)[\"smoking\"],\n             coef(fitx4)[\"smoking\"])),2)\n#&gt; [1] 0.05\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 &lt;- glm(hypertension ~ smoking + gender, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nfitx2 &lt;- glm(hypertension ~ smoking, family=gaussian(link = \"identity\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nround(mean(c(coef(fitx1)[\"smoking\"],\n             coef(fitx2)[\"smoking\"])),2)\n#&gt; [1] 0.05\n\nCrude (in absence of confounding)\n\nfitx0 &lt;- glm(hypertension ~ smoking, \n             family=gaussian(link = \"identity\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[2,]\n\n\n  \n\n\n\nConditional and crude risk ratio (RR)\nRef:\n\n\n(Naimi and Whitcomb 2020) (“For the risk ratio, one may use a GLM with a Poisson distribution and log link function …. one should use the robust (or sandwich) variance estimator to obtain valid standard errors (the bootstrap can also be used)”).\n\nFull list of risk factors for outcome (2 variables)\n\nfitx0 &lt;- glm(hypertension ~ smoking + gender + age, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nThe estimate we see as a hazard ratio is basically a risk ratio.\nStrtatum specific (2 variables)\n\nfitx1 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age &lt; 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx2 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age &lt; 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx3 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1 & age &gt;= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx4 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0 & age &gt;= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#&gt; [1] 1.156387\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 &lt;- glm(hypertension ~ smoking + gender, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\nfitx2 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#&gt; [1] 1.077402\n\nCrude (in absence of confounding)\n\nfitx0 &lt;- glm(hypertension ~ smoking, \n             family=poisson(link = \"log\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nSince the marginal and conditional estimate are approximately the same, RD is a collapsible measure.\nConditional and crude OR\nFull list of risk factors for outcome (2 variables)\n\nfitx0 &lt;- glm(hypertension ~ smoking + gender + age, family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nStrtatum specific (2 variables)\n\nfitx1 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age &lt; 2))\npublish(fitx1, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx2 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age &lt; 2))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx3 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1 & age &gt;= 2))\npublish(fitx3, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx4 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0 & age &gt;= 2))\npublish(fitx4, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 2 variables)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"],\n           coef(fitx3)[\"smoking\"],\n           coef(fitx4)[\"smoking\"])))\n#&gt; [1] 2.180804\n\nPartial list of risk factors for outcome (1 variable)\n\nfitx0 &lt;- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nStrtatum specific (1 variable)\n\nfitx1 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 1))\npublish(fitx1, print = FALSE, confint.method = \"robust\",\n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nfitx2 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), \n             data=subset(Obs.Data, gender == 0))\npublish(fitx2, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nUnweighted mean from all strata (for simplicity, 1 variable)\n\nmean(exp(c(coef(fitx1)[\"smoking\"],\n           coef(fitx2)[\"smoking\"])))\n#&gt; [1] 1.290429\n\nMantel-Haenszel adjusted ORs with 1 variable\n\ntabx &lt;- xtabs( ~ hypertension + smoking + gender, data = Obs.Data)\nftable(tabx)    \n#&gt;                      gender     0     1\n#&gt; hypertension smoking                   \n#&gt; 0            0               3788 12401\n#&gt;              1               3400 11504\n#&gt; 1            0              10547 20984\n#&gt;              1              12178 25198\n# require(samplesizeCMH)\n# apply(tabx, 3, odds.ratio)\n\nlibrary(lawstat)\ncmh.test(tabx)\n#&gt; \n#&gt;  Cochran-Mantel-Haenszel Chi-square Test\n#&gt; \n#&gt; data:  tabx\n#&gt; CMH statistic = NA, df = 1.0000, p-value = NA, MH Estimate = 1.2924,\n#&gt; Pooled Odd Ratio = 1.2876, Odd Ratio of level 1 = 1.2864, Odd Ratio of\n#&gt; level 2 = 1.2945\n# mantelhaen.test(tabx, exact = TRUE)\n\nCrude (in absence of confounding)\n\nfitx0 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\npublish(fitx0, print = FALSE, confint.method = \"robust\", \n        pvalue.method = \"robust\")$regressionTable[1,]\n\n\n  \n\n\n\nMarginal RD, RR and OR\nBelow we show a procedure for calculating marginal probabilities \\(p_1\\) (for treated) and \\(p_0\\) (for untreated).\nAdjustment of 2 variables\n\nfitx3 &lt;- glm(hypertension ~ smoking + gender + age, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx &lt;- Obs.Data\nObs.Data.all.tx$smoking &lt;- 1\np1 &lt;- mean(predict(fitx3, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx &lt;- Obs.Data\nObs.Data.all.utx$smoking &lt;- 0\np0 &lt;- mean(predict(fitx3, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm &lt;- p1 - p0\nRRm &lt;- p1 / p0\nORm &lt;- (p1 / (1-p1)) / (p0 / (1-p0))\nORc &lt;- as.numeric(exp(coef(fitx3)[\"smoking\"]))\nRRz &lt;- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#&gt; RD marginal =  0.05 \n#&gt; RR marginal =  1.08 \n#&gt; OR marginal =  1.29 \n#&gt; OR conditional =  3.37 \n#&gt; RR (ZY)=  1.08\n\nAdjustment of 1 variable\n\nfitx2 &lt;- glm(hypertension ~ smoking + gender, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx &lt;- Obs.Data\nObs.Data.all.tx$smoking &lt;- 1\np1 &lt;- mean(predict(fitx2, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx &lt;- Obs.Data\nObs.Data.all.utx$smoking &lt;- 0\np0 &lt;- mean(predict(fitx2, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm &lt;- p1 - p0\nRRm &lt;- p1 / p0\nORm &lt;- (p1 / (1-p1)) / (p0 / (1-p0))\nORc &lt;- as.numeric(exp(coef(fitx2)[\"smoking\"]))\nRRz &lt;- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#&gt; RD marginal =  0.05 \n#&gt; RR marginal =  1.08 \n#&gt; OR marginal =  1.29 \n#&gt; OR conditional =  1.29 \n#&gt; RR (ZY)=  1.08\n\nNo adjustment\n\nfitx1 &lt;- glm(hypertension ~ smoking, \n             family=binomial(link = \"logit\"), data=Obs.Data)\nObs.Data.all.tx &lt;- Obs.Data\nObs.Data.all.tx$smoking &lt;- 1\np1 &lt;- mean(predict(fitx0, \n                   newdata = Obs.Data.all.tx, type = \"response\"))\nObs.Data.all.utx &lt;- Obs.Data\nObs.Data.all.utx$smoking &lt;- 0\np0 &lt;- mean(predict(fitx0, \n                   newdata = Obs.Data.all.utx, type = \"response\"))\n\nRDm &lt;- p1 - p0\nRRm &lt;- p1 / p0\nORm &lt;- (p1 / (1-p1)) / (p0 / (1-p0))\nORc &lt;- as.numeric(exp(coef(fitx1)[\"smoking\"]))\nRRz &lt;- ORm / ((1-p0) + p0 * ORm) # Zhang and Yu (1998)\ncat(\"RD marginal = \", round(RDm,2), \n    \"\\nRR marginal = \", round(RRm,2), \n    \"\\nOR marginal = \", round(ORm,2), \n    \"\\nOR conditional = \", round(ORc,2), \n    \"\\nRR (ZY)= \", round(RRz,2))\n#&gt; RD marginal =  0.05 \n#&gt; RR marginal =  1.08 \n#&gt; OR marginal =  1.29 \n#&gt; OR conditional =  1.29 \n#&gt; RR (ZY)=  1.08\n\nBootstrap could be used to estimate confidence intervals.\nRef:\n\n\n(Kleinman and Norton 2009) (“this paper demonstrates how to move from a nonlinear model to estimates of marginal effects that are quantified as the adjusted risk ratio or adjusted risk difference”)\n\n(Austin 2010) (“clinically meaningful measures of treatment effect using logistic regression model”)\n\n(Luijken et al. 2022) (“marginal odds ratio”)\n\n(Muller and MacLehose 2014) (“marginal standardization”)\n\n(Greenland 2004) (“standardized / population-averaged”)\n\n(Bieler et al. 2010) (“standardized /population-averaged risk from the logistic model”)\nSummary\nHere are the summary of the results based on a scenario where confounding was absent:\n\n\n\n\n\n\n\n\nModelling strategy\nRD (conditional)\nRR (conditional)\nOR (conditional)\n\n\n\nage + gender in regression\n0.06 [0.05;0.06]\n1.08 [1.08;1.09]\n3.37 [3.17;3.58]\n\n\nstratified by age and gender (mean)\n0.05 (0.11, 0.1,0.01,0)\n1.16 (1.41, 1.21, 1.01,1)\n2.18 (unweighted; 1.65, 1.49, 3.45, 2.14)\n\n\ngender in regression\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.26;1.33]\n\n\nstratified by gender (mean)\n0.05 (0.6,0.5)\n1.08 (1.09, 1.06)\n1.29 (1.29, 1.29; M-H 1.29)\n\n\nMarginal estimates\n\n\n\n\n\ncrude\n0.05 [0.05;0.06]\n1.08 [1.07;1.09]\n1.29 [1.25;1.32]\n\n\nBased on marginal probabilities (any variable combination)\n0.05\n1.08\n1.29\n\n\n\nAs we can see, marginal estimate = conditional estimate for RD and RR but not for OR. Hence, RD and RR are collapsible measures, while OR is a non-collapsible measure.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walk through, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nAustin, Peter C. 2010. “Absolute Risk Reductions, Relative Risks, Relative Risk Reductions, and Numbers Needed to Treat Can Be Obtained from a Logistic Regression Model.” Journal of Clinical Epidemiology 63 (1): 2–6.\n\n\nBieler, Gayle S, G Gordon Brown, Rick L Williams, and Donna J Brogan. 2010. “Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios from Complex Survey Data.” American Journal of Epidemiology 171 (5): 618–23.\n\n\nGreenland, Sander. 2004. “Model-Based Estimation of Relative Risks and Other Epidemiologic Measures in Studies of Common Outcomes and in Case-Control Studies.” American Journal of Epidemiology 160 (4): 301–5.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Confounding and Collapsibility in Causal Inference.” Statistical Science 14 (1): 29–46.\n\n\nKleinman, Lawrence C, and Edward C Norton. 2009. “What’s the Risk? A Simple Approach for Estimating Adjusted Risk Measures from Nonlinear Models Including Logistic Regression.” Health Services Research 44 (1): 288–302.\n\n\nLuijken, Kim, Rolf HH Groenwold, Maarten van Smeden, Susanne Strohmaier, and Georg Heinze. 2022. “A Comparison of Full Model Specification and Backward Elimination of Potential Confounders When Estimating Marginal and Conditional Causal Effects on Binary Outcomes from Observational Data.” Biometrical Journal.\n\n\nMansournia, Mohammad Ali, and Sander Greenland. 2015. “The Relation of Collapsibility and Confounding to Faithfulness and Stability.” Epidemiology 26 (4): 466–72.\n\n\nMuller, Clemma J, and Richard F MacLehose. 2014. “Estimating Predicted Probabilities from Logistic Regression: Different Methods Correspond to Different Target Populations.” International Journal of Epidemiology 43 (3): 962–70.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10.",
    "crumbs": [
      "Causal roles",
      "Collapsibility"
    ]
  },
  {
    "objectID": "confounding6.html",
    "href": "confounding6.html",
    "title": "Change-in-estimate",
    "section": "",
    "text": "Adjusting for a variable that is a confounder\nThe change-in-estimate criterion is one of the purposeful variable selection criteria, where a relative change is used to determine whether a variable should be included in the model or omitted from the model. This change can be measured by using the relative change in the regression coefficient or relative change by adjusting the effect of the variance estimate. Eliminating a variable using this technique can bias the estimates. In particular, this method is not appropriate when the estimate is non-collapsible. In this chapter, we will see the use of change-in-estimate for various measures of effects (RD and OR, and impact of collapsibility vs non-collapsibility in absence of confounding).\nFor a collapsible measure (e.g., beta coefficient from a linear model with a continuous outcome, risk difference or risk ratio from a generalized linear model with a binary outcome), our estimate could be the same when we adjust our model or not for a variable that is not a confounder. However, a non-collapsible measure (e.g., odds ratio) could be different whether or not we adjust our model for a variable that is not a confounder.",
    "crumbs": [
      "Causal roles",
      "Change-in-estimate"
    ]
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-a-confounder",
    "title": "Change-in-estimate",
    "section": "",
    "text": "Continuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.85\n\nfit2 &lt;- glm(Y ~ A + L, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           L \n#&gt;         0.0         1.3         1.1\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"L\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"A\", distr = \"rnorm\", mean = 0 + L, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis( 1.1 * L + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node L, order:1\n#&gt; node A, order:2\n#&gt; node P, order:3\n#&gt; node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.68\n\nfit2 &lt;- glm(Y ~ A + L, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           L \n#&gt;         0.0         1.3         1.1\n\nIncluding a variable that is a confounder (L) in the model changes effect estimate (1.3).",
    "crumbs": [
      "Causal roles",
      "Change-in-estimate"
    ]
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-simplified",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (simplified)",
    "text": "Adjusting for a variable that is not a confounder (simplified)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * R + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node P, order:2\n#&gt; node R, order:3\n#&gt; node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         1.3\n\nfit2 &lt;- glm(Y ~ A + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           R \n#&gt;         0.0         1.3         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  # node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(1.1 * R + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node P, order:2\n#&gt; node R, order:3\n#&gt; node Y, order:4\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.06\n\nfit2 &lt;- glm(Y ~ A + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           R \n#&gt;         0.0         1.3         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3).",
    "crumbs": [
      "Causal roles",
      "Change-in-estimate"
    ]
  },
  {
    "objectID": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "href": "confounding6.html#adjusting-for-a-variable-that-is-not-a-confounder-complex",
    "title": "Change-in-estimate",
    "section": "Adjusting for a variable that is not a confounder (Complex)",
    "text": "Adjusting for a variable that is not a confounder (Complex)\nContinuous Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.1 * R + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node P, order:2\n#&gt; node M, order:3\n#&gt; node R, order:4\n#&gt; node Y, order:5\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\n\nfit2 &lt;- glm(Y ~ A + M + R, family=\"gaussian\", data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           M           R \n#&gt;         0.0         1.3         0.5         1.1\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model does not change effect estimate (1.3).\nBinary Outcome\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"P\", distr = \"rbern\", prob = plogis(-10)) + \n  node(\"M\", distr = \"rnorm\", mean = P + 0.5 * A, sd = 1) + \n  node(\"R\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.1 * R + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node P, order:2\n#&gt; node M, order:3\n#&gt; node R, order:4\n#&gt; node Y, order:5\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nrequire(simcausal)\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (OR)\n\nfit &lt;- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A           M \n#&gt;        0.00        1.06        0.41\n\nfit2 &lt;- glm(Y ~ A + M + R, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit2),2)\n#&gt; (Intercept)           A           M           R \n#&gt;        0.00        1.29        0.50        1.10\n\nIncluding a variable that is not a confounder (R is a pure risk factor for the outcome Y) in the model changes effect estimate (1.3).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Causal roles",
      "Change-in-estimate"
    ]
  },
  {
    "objectID": "confounding9.html",
    "href": "confounding9.html",
    "title": "Interaction",
    "section": "",
    "text": "Data and variables\nInteraction is commonly assessed by either\nWhen the specification is saturated, these parameterizations are mathematically equivalent and encode the same set of contrasts on the log-odds scale.\nFor clear presentation, it is often helpful to report the full set of joint effects relative to a single reference, and selected simple effects within strata, alongside additive interaction measures (Knol and VanderWeele 2012).\nThe motivating example in this document uses publicly available NHANES 2009–2012 data, studying hypertension defined by systolic blood pressure ≥ 130 mmHg and examining whether the effect of obesity differs across levels of self reported race or ethnicity.\nWe will use interaction analysis functions (jointeffects, inteffects, addint, addintlist from svyTable1 package) to compute and present joint effects, simple effects, and additive interaction measures with appropriate survey adjustments from saturated logistic regression models.\nWe use NHANES 2009–2012 adults (Age ≥ 20) with survey design variables. The binary outcome is Hypertension_130, defined from average systolic BP. The two interacting exposures are Race1 and ObeseStatus derived from BMI ≥ 30, with \"White\" and \"Not Obese\" as reference levels. We adjust for Age.\nlibrary(survey)\nlibrary(dplyr)\nlibrary(NHANES)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(kableExtra)\nlibrary(svyTable1)\ndata(NHANESraw, package = \"NHANES\")\n\nnhanes_adults &lt;- NHANESraw %&gt;%\n  filter(Age &gt;= 20) %&gt;%\n  mutate(\n    ObeseStatus = factor(ifelse(BMI &gt;= 30, \"Obese\", \"Not Obese\"),\n                         levels = c(\"Not Obese\", \"Obese\")),\n    Hypertension_130 = factor(ifelse(BPSysAve &gt;= 130, \"Yes\", \"No\"),\n                              levels = c(\"No\", \"Yes\")),\n    Race1 = relevel(as.factor(Race1), ref = \"White\")\n  ) %&gt;%\n  select(Age, Race1, BPSysAve, BMI, ObeseStatus, Hypertension_130,\n         SDMVPSU, SDMVSTRA, WTMEC2YR) %&gt;%\n  drop_na()\n\nadult_design &lt;- svydesign(\n  id = ~SDMVPSU,\n  strata = ~SDMVSTRA,\n  weights = ~WTMEC2YR,\n  nest = TRUE,\n  data = nhanes_adults\n)\n\nf1_levels &lt;- levels(adult_design$variables$Race1)\nkable(f1_levels)\n\n\n\nx\n\n\n\nWhite\n\n\nBlack\n\n\nHispanic\n\n\nMexican\n\n\nOther\n\n\n\n\nf2_levels &lt;- levels(adult_design$variables$ObeseStatus)\nkable(f2_levels)\n\n\n\nx\n\n\n\nNot Obese\n\n\nObese",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#data-and-variables",
    "href": "confounding9.html#data-and-variables",
    "title": "Interaction",
    "section": "Data and variables",
    "text": "Data and variables\nWe use NHANES 2009–2012 adults (Age ≥ 20) with survey design variables. The binary outcome is Hypertension_130, defined from average systolic BP. The two interacting exposures are Race1 and ObeseStatus derived from BMI ≥ 30, with \"White\" and \"Not Obese\" as reference levels. We adjust for Age.\n\nlibrary(survey)\nlibrary(dplyr)\nlibrary(NHANES)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(kableExtra)\nlibrary(svyTable1)\n\n\ndata(NHANESraw, package = \"NHANES\")\n\nnhanes_adults &lt;- NHANESraw %&gt;%\n  filter(Age &gt;= 20) %&gt;%\n  mutate(\n    ObeseStatus = factor(ifelse(BMI &gt;= 30, \"Obese\", \"Not Obese\"),\n                         levels = c(\"Not Obese\", \"Obese\")),\n    Hypertension_130 = factor(ifelse(BPSysAve &gt;= 130, \"Yes\", \"No\"),\n                              levels = c(\"No\", \"Yes\")),\n    Race1 = relevel(as.factor(Race1), ref = \"White\")\n  ) %&gt;%\n  select(Age, Race1, BPSysAve, BMI, ObeseStatus, Hypertension_130,\n         SDMVPSU, SDMVSTRA, WTMEC2YR) %&gt;%\n  drop_na()\n\nadult_design &lt;- svydesign(\n  id = ~SDMVPSU,\n  strata = ~SDMVSTRA,\n  weights = ~WTMEC2YR,\n  nest = TRUE,\n  data = nhanes_adults\n)\n\nf1_levels &lt;- levels(adult_design$variables$Race1)\nf1_levels\n#&gt; [1] \"White\"    \"Black\"    \"Hispanic\" \"Mexican\"  \"Other\"\nf2_levels &lt;- levels(adult_design$variables$ObeseStatus)\nf2_levels\n#&gt; [1] \"Not Obese\" \"Obese\"",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#models",
    "href": "confounding9.html#models",
    "title": "Interaction",
    "section": "Models",
    "text": "Models\nModel 1. Joint variable model\nCreate one factor (variable) for all combinations of Race1 × ObeseStatus with \"White_Not Obese\" as the reference, and fit a survey-weighted logistic regression adjusting for Age.\n\n# Create joint exposure factor inside the data\nnhanes_adults &lt;- nhanes_adults %&gt;%\n  mutate(\n    Race1_ObeseStatus = interaction(Race1, ObeseStatus, sep = \"_\"),\n    Race1_ObeseStatus = relevel(Race1_ObeseStatus, ref = \"White_Not Obese\")\n  )\nkable(levels(nhanes_adults$Race1_ObeseStatus))\n\n\n\nx\n\n\n\nWhite_Not Obese\n\n\nBlack_Not Obese\n\n\nHispanic_Not Obese\n\n\nMexican_Not Obese\n\n\nOther_Not Obese\n\n\nWhite_Obese\n\n\nBlack_Obese\n\n\nHispanic_Obese\n\n\nMexican_Obese\n\n\nOther_Obese\n\n\n\n\n\n# Recreate survey design with the new variable included\nadult_design_joint &lt;- svydesign(\n  id = ~SDMVPSU,\n  strata = ~SDMVSTRA,\n  weights = ~WTMEC2YR,\n  nest = TRUE,\n  data = nhanes_adults\n)\n\n# Fit joint model using the explicitly named variable\njoint_model &lt;- svyglm(\n  Hypertension_130 ~ Race1_ObeseStatus + Age,\n  design = adult_design_joint,\n  family = binomial()\n)\nm1 &lt;- publish(joint_model) \n#&gt;           Variable              Units OddsRatio       CI.95     p-value \n#&gt;  Race1_ObeseStatus    White_Not Obese       Ref                         \n#&gt;                       Black_Not Obese      2.11 [1.73;2.58]     &lt; 1e-04 \n#&gt;                    Hispanic_Not Obese      0.99 [0.77;1.27]   0.9374741 \n#&gt;                     Mexican_Not Obese      1.17 [0.95;1.43]   0.1510628 \n#&gt;                       Other_Not Obese      1.05 [0.87;1.28]   0.6050711 \n#&gt;                           White_Obese      1.40 [1.21;1.61]   0.0001361 \n#&gt;                           Black_Obese      2.53 [1.98;3.24]     &lt; 1e-04 \n#&gt;                        Hispanic_Obese      1.65 [1.27;2.13]   0.0009516 \n#&gt;                         Mexican_Obese      1.92 [1.55;2.39]     &lt; 1e-04 \n#&gt;                           Other_Obese      1.15 [0.74;1.80]   0.5437189 \n#&gt;                Age                         1.06 [1.06;1.06]     &lt; 1e-04\n\nModel 2. Interaction term model\nInclude main effects and their product term for Race1 and ObeseStatus, adjusting for Age. This is the saturated parameterization equivalent to the joint-variable model.\n\ninteraction_model &lt;- survey::svyglm(\n  Hypertension_130 ~ Race1 * ObeseStatus + Age,\n  design = adult_design,\n  family = binomial()\n)\nm2 &lt;- publish(interaction_model) \n#&gt;                                          Variable Units OddsRatio       CI.95     p-value \n#&gt;                                               Age            1.06 [1.06;1.06]     &lt; 1e-04 \n#&gt;     Race1(White): ObeseStatus(Obese vs Not Obese)            1.40 [1.21;1.61]     &lt; 1e-04 \n#&gt;     Race1(Black): ObeseStatus(Obese vs Not Obese)            1.20 [0.95;1.52]   0.1302843 \n#&gt;  Race1(Hispanic): ObeseStatus(Obese vs Not Obese)            1.66 [1.25;2.20]   0.0004009 \n#&gt;   Race1(Mexican): ObeseStatus(Obese vs Not Obese)            1.65 [1.30;2.10]     &lt; 1e-04 \n#&gt;     Race1(Other): ObeseStatus(Obese vs Not Obese)            1.09 [0.68;1.77]   0.7152699 \n#&gt;     ObeseStatus(Not Obese): Race1(Black vs White)            2.11 [1.73;2.58]     &lt; 1e-04 \n#&gt;  ObeseStatus(Not Obese): Race1(Hispanic vs White)            0.99 [0.77;1.27]   0.9367881 \n#&gt;   ObeseStatus(Not Obese): Race1(Mexican vs White)            1.17 [0.95;1.43]   0.1374845 \n#&gt;     ObeseStatus(Not Obese): Race1(Other vs White)            1.05 [0.87;1.28]   0.6000542 \n#&gt;         ObeseStatus(Obese): Race1(Black vs White)            1.81 [1.41;2.33]     &lt; 1e-04 \n#&gt;      ObeseStatus(Obese): Race1(Hispanic vs White)            1.18 [0.92;1.51]   0.1960934 \n#&gt;       ObeseStatus(Obese): Race1(Mexican vs White)            1.38 [1.03;1.84]   0.0296074 \n#&gt;         ObeseStatus(Obese): Race1(Other vs White)            0.82 [0.52;1.31]   0.4136666",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#joint-effects-from-either-parameterization",
    "href": "confounding9.html#joint-effects-from-either-parameterization",
    "title": "Interaction",
    "section": "Joint effects from either parameterization",
    "text": "Joint effects from either parameterization\nRetrieve the joint effects for each Race1 × ObeseStatus combination relative to \"White & Not Obese\". From the interaction model we obtain these by post-estimation transformation using the function jointeffects. From the joint model they correspond directly to exponentiated coefficients for the non-reference levels.\njoint_from_interaction &lt;- jointeffects(\n  interaction_model = interaction_model,\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  scale = \"ratio\",\n  digits = 2\n)\n\nkable(joint_from_interaction,\n      caption = \"Table 1. Joint effects (OR) for Race1 × ObeseStatus vs White & Not Obese\") %&gt;%\n  kable_styling(full_width = FALSE)\n\nTable 1. Joint effects (OR) for Race1 × ObeseStatus vs White & Not Obese\n\nLevel1\nLevel2\nEstimate\nSE\nCI.low\nCI.upp\n\n\n\nWhite\nNot Obese\n1.00\n0.00\n1.00\n1.00\n\n\nBlack\nNot Obese\n2.11\n0.21\n1.73\n2.58\n\n\nHispanic\nNot Obese\n0.99\n0.12\n0.77\n1.27\n\n\nMexican\nNot Obese\n1.17\n0.12\n0.95\n1.43\n\n\nOther\nNot Obese\n1.05\n0.10\n0.87\n1.28\n\n\nWhite\nObese\n1.40\n0.10\n1.21\n1.61\n\n\nBlack\nObese\n2.53\n0.32\n1.98\n3.24\n\n\nHispanic\nObese\n1.65\n0.22\n1.27\n2.13\n\n\nMexican\nObese\n1.92\n0.21\n1.55\n2.39\n\n\nOther\nObese\n1.15\n0.26\n0.74\n1.80\n\n\n\nCheck equivalence of results obtained from joint variable model\nkable(m1$regressionTable)\n\n\n\n\n\n\n\n\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nRace1_ObeseStatus\nWhite_Not Obese\nRef\n\n\n\n\n\nBlack_Not Obese\n2.11\n[1.73;2.58]\n&lt; 1e-04\n\n\n\nHispanic_Not Obese\n0.99\n[0.77;1.27]\n0.9374741\n\n\n\nMexican_Not Obese\n1.17\n[0.95;1.43]\n0.1510628\n\n\n\nOther_Not Obese\n1.05\n[0.87;1.28]\n0.6050711\n\n\n\nWhite_Obese\n1.40\n[1.21;1.61]\n0.0001361\n\n\n\nBlack_Obese\n2.53\n[1.98;3.24]\n&lt; 1e-04\n\n\n\nHispanic_Obese\n1.65\n[1.27;2.13]\n0.0009516\n\n\n\nMexican_Obese\n1.92\n[1.55;2.39]\n&lt; 1e-04\n\n\n\nOther_Obese\n1.15\n[0.74;1.80]\n0.5437189\n\n\nAge\n\n1.06\n[1.06;1.06]\n&lt; 1e-04",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#simple-effects-within-strata",
    "href": "confounding9.html#simple-effects-within-strata",
    "title": "Interaction",
    "section": "Simple effects within strata",
    "text": "Simple effects within strata\nFrom the joint model we compute simple effects such as Obese vs Not Obese within each Race1 level using inteffects function.\nsimple_from_joint &lt;- inteffects(\n  joint_model = joint_model,\n  joint_var_name &lt;- \"Race1_ObeseStatus\",\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  factor1_levels = f1_levels,\n  factor2_levels = f2_levels,\n  level_separator = \"_\", \n  scale = \"ratio\",\n  digits = 2\n)\n\nkable(simple_from_joint,\n      caption = \"Table 2. Simple effects: Obese vs Not Obese within Race1 strata\") %&gt;%\n  kable_styling(full_width = FALSE)\n\nTable 2. Simple effects: Obese vs Not Obese within Race1 strata\n\nComparison\nEstimate\nSE\nCI.low\nCI.upp\np-value\n\n\n\nObeseStatus(Obese vs Not Obese): Race1(White)\n1.40\n0.10\n1.21\n1.61\n0.0000049\n\n\nObeseStatus(Obese vs Not Obese): Race1(Black)\n1.20\n0.14\n0.95\n1.52\n0.1302843\n\n\nObeseStatus(Obese vs Not Obese): Race1(Hispanic)\n1.66\n0.24\n1.25\n2.20\n0.0004009\n\n\nObeseStatus(Obese vs Not Obese): Race1(Mexican)\n1.65\n0.20\n1.30\n2.10\n0.0000415\n\n\nObeseStatus(Obese vs Not Obese): Race1(Other)\n1.09\n0.27\n0.68\n1.77\n0.7152699\n\n\nRace1(Black vs White): ObeseStatus(Not Obese)\n2.11\n0.21\n1.73\n2.58\n0.0000000\n\n\nRace1(Hispanic vs White): ObeseStatus(Not Obese)\n0.99\n0.12\n0.77\n1.27\n0.9367881\n\n\nRace1(Mexican vs White): ObeseStatus(Not Obese)\n1.17\n0.12\n0.95\n1.43\n0.1374845\n\n\nRace1(Other vs White): ObeseStatus(Not Obese)\n1.05\n0.10\n0.87\n1.28\n0.6000542\n\n\nRace1(Black vs White): ObeseStatus(Obese)\n1.81\n0.23\n1.41\n2.33\n0.0000037\n\n\nRace1(Hispanic vs White): ObeseStatus(Obese)\n1.18\n0.15\n0.92\n1.51\n0.1960934\n\n\nRace1(Mexican vs White): ObeseStatus(Obese)\n1.38\n0.20\n1.03\n1.84\n0.0296074\n\n\nRace1(Other vs White): ObeseStatus(Obese)\n0.82\n0.19\n0.52\n1.31\n0.4136666\n\n\n\nCheck equivalence of results obtained from interaction model\nkable(m2$regressionTable)\n\n\n\n\n\n\n\n\n\nVariable\nUnits\nOddsRatio\nCI.95\np-value\n\n\n\nAge\n\n1.06\n[1.06;1.06]\n&lt; 1e-04\n\n\nRace1(White): ObeseStatus(Obese vs Not Obese)\n\n1.40\n[1.21;1.61]\n&lt; 1e-04\n\n\nRace1(Black): ObeseStatus(Obese vs Not Obese)\n\n1.20\n[0.95;1.52]\n0.1302843\n\n\nRace1(Hispanic): ObeseStatus(Obese vs Not Obese)\n\n1.66\n[1.25;2.20]\n0.0004009\n\n\nRace1(Mexican): ObeseStatus(Obese vs Not Obese)\n\n1.65\n[1.30;2.10]\n&lt; 1e-04\n\n\nRace1(Other): ObeseStatus(Obese vs Not Obese)\n\n1.09\n[0.68;1.77]\n0.7152699\n\n\nObeseStatus(Not Obese): Race1(Black vs White)\n\n2.11\n[1.73;2.58]\n&lt; 1e-04\n\n\nObeseStatus(Not Obese): Race1(Hispanic vs White)\n\n0.99\n[0.77;1.27]\n0.9367881\n\n\nObeseStatus(Not Obese): Race1(Mexican vs White)\n\n1.17\n[0.95;1.43]\n0.1374845\n\n\nObeseStatus(Not Obese): Race1(Other vs White)\n\n1.05\n[0.87;1.28]\n0.6000542\n\n\nObeseStatus(Obese): Race1(Black vs White)\n\n1.81\n[1.41;2.33]\n&lt; 1e-04\n\n\nObeseStatus(Obese): Race1(Hispanic vs White)\n\n1.18\n[0.92;1.51]\n0.1960934\n\n\nObeseStatus(Obese): Race1(Mexican vs White)\n\n1.38\n[1.03;1.84]\n0.0296074\n\n\nObeseStatus(Obese): Race1(Other vs White)\n\n0.82\n[0.52;1.31]\n0.4136666",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#additive-interaction-measures",
    "href": "confounding9.html#additive-interaction-measures",
    "title": "Interaction",
    "section": "Additive interaction measures",
    "text": "Additive interaction measures\nWe also summarize additive interaction using addintlist to report RERI, AP, and S, with \"White & Not Obese\" as the joint reference.\nadd_tab &lt;- addintlist(\n  model = interaction_model,\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  measures = \"all\",\n  digits = 3\n)\n\nkable(add_tab,\n      caption = \"Table 3. Additive interaction measures by Race1 with obesity as the binary factor\") %&gt;%\n  kable_styling(full_width = FALSE)\n\nTable 3. Additive interaction measures by Race1 with obesity as the binary factor\n\nFactor1\nLevel1\nFactor2\nLevel2\nMeasure\nEstimate\nSE\nCI_low\nCI_upp\n\n\n\nRace1\nBlack\nObeseStatus\nObese\nRERI\n0.025\n0.312\n-0.587\n0.637\n\n\nRace1\nBlack\nObeseStatus\nObese\nAP\n0.010\n0.122\n-0.230\n0.250\n\n\nRace1\nBlack\nObeseStatus\nObese\nS\n1.016\n0.205\n0.680\n1.518\n\n\nRace1\nHispanic\nObeseStatus\nObese\nRERI\n0.258\n0.194\n-0.123\n0.639\n\n\nRace1\nHispanic\nObeseStatus\nObese\nAP\n0.157\n0.105\n-0.048\n0.362\n\n\nRace1\nHispanic\nObeseStatus\nObese\nS\n1.667\n0.382\n0.788\n3.525\n\n\nRace1\nMexican\nObeseStatus\nObese\nRERI\n0.360\n0.253\n-0.136\n0.857\n\n\nRace1\nMexican\nObeseStatus\nObese\nAP\n0.187\n0.116\n-0.041\n0.415\n\n\nRace1\nMexican\nObeseStatus\nObese\nS\n1.639\n0.348\n0.829\n3.241\n\n\nRace1\nOther\nObeseStatus\nObese\nRERI\n-0.299\n0.305\n-0.896\n0.299\n\n\nRace1\nOther\nObeseStatus\nObese\nAP\n-0.259\n0.316\n-0.878\n0.360\n\n\nRace1\nOther\nObeseStatus\nObese\nS\n0.337\n1.758\n0.011\n10.567",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#minimal-code-to-reproduce-tables",
    "href": "confounding9.html#minimal-code-to-reproduce-tables",
    "title": "Interaction",
    "section": "Minimal code to reproduce tables",
    "text": "Minimal code to reproduce tables\n\n# Joint effects from interaction model\njoint_from_interaction &lt;- jointeffects(\n  interaction_model = interaction_model,\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  scale = \"ratio\",\n  digits = 2\n)\n\n# Simple effects from joint model\nsimple_from_joint &lt;- inteffects(\n  joint_model = joint_model,\n  joint_var_name = \"Race1_ObeseStatus\",\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  factor1_levels = f1_levels,\n  factor2_levels = f2_levels,\n  level_separator = \"-\",\n  scale = \"ratio\",\n  digits = 2\n)\n\n# Additive interaction measures\nadd_tab &lt;- addintlist(\n  model = interaction_model,\n  factor1_name = \"Race1\",\n  factor2_name = \"ObeseStatus\",\n  measures = \"all\",\n  digits = 3\n)\n\nlist(\n  joint_effects = head(joint_from_interaction, 4),\n  simple_effects = head(simple_from_joint, 4),\n  additive = head(add_tab, 4)\n)",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confoundingF.html",
    "href": "confoundingF.html",
    "title": "R functions (R)",
    "section": "",
    "text": "The list of new R functions introduced in this Confounding and bias lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\ncmh.test\nlawstat\nTo conduct the Mantel-Haenszel Chi-square test\n\n\nDAG.empty\nsimcausal\nTo initialize an empty DAG\n\n\nftable\nbase/stats\nTo create a flat contingency table\n\n\nplotDAG\nsimcausal\nTo visualize a DAG\n\n\nset.DAG\nsimcausal\nTo create a DAG\n\n\nsim\nsimcausal\nTo simulate data using a DAG",
    "crumbs": [
      "Causal roles",
      "R functions (R)"
    ]
  },
  {
    "objectID": "confoundingQ.html",
    "href": "confoundingQ.html",
    "title": "Quiz (R)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Causal roles",
      "Quiz (R)"
    ]
  },
  {
    "objectID": "confoundingQ.html#live-quiz",
    "href": "confoundingQ.html#live-quiz",
    "title": "Quiz (R)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Causal roles",
      "Quiz (R)"
    ]
  },
  {
    "objectID": "confoundingQ.html#download-quiz",
    "href": "confoundingQ.html#download-quiz",
    "title": "Quiz (R)",
    "section": "Download Quiz",
    "text": "Download Quiz",
    "crumbs": [
      "Causal roles",
      "Quiz (R)"
    ]
  },
  {
    "objectID": "confoundingQ.html#quiz-r",
    "href": "confoundingQ.html#quiz-r",
    "title": "Quiz (R)",
    "section": "Quiz (R)",
    "text": "Quiz (R)\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Causal roles",
      "Quiz (R)"
    ]
  },
  {
    "objectID": "confoundingE.html",
    "href": "confoundingE.html",
    "title": "Exercise 1 (R)",
    "section": "",
    "text": "Problem Statement\nThe following lab builds upon the RHC data set used in a previous Lab Assignment and explores the marginal and conditional effects of right-heart-catheteriziation treatment (swang1) on mortality (death). We begin by loading the RHC data set that we have seen earlier.\n# Import the rhc data set\ndf &lt;- read.csv('Data/rhc.csv', header = TRUE)\nWe then apply the following data-wrangling procedures, which we implemented previously:\n# Generate age.cat\ndf$age.cat &lt;- cut(df$age, breaks = c(0, 50, 60, 70, 80, Inf), \n                  include.lowest = TRUE, right = FALSE)\n\n# Factor `race`\ndf$race &lt;- factor(x = df$race, levels = c('white', 'black', 'other'))\n\n# Factor `sex`\ndf$sex &lt;- factor(x = df$sex, levels = c('Male', 'Female'))\n\n# Factor `cat1`\ndf$cat1 &lt;- factor(df$cat1)\nlevels(df$cat1) = list(ARF = 'ARF',\n                       CHF = 'CHF',\n                       MOSF = c('MOSF w/Malignancy', 'MOSF w/Sepsis'),\n                       Other = c('Cirrhosis', 'Colon Cancer', 'Coma', \n                                 'COPD', 'Lung Cancer'))\n\n# Factoring `ca`\ndf$ca &lt;- factor(x = df$ca, \n                levels = c('No', 'Yes', 'Metastatic'),\n                labels = c('None', 'Localized (Yes)', 'Metastatic'))\n\n# Generate n.comorbiditis\ndf$n.comorbidities &lt;- rowSums(x = subset(df, select = cardiohx:amihx), na.rm = TRUE)\n\n# Factor `swang1`\ndf$swang1 &lt;- factor(x = df$swang1, levels = c('No RHC', 'RHC'))\n\n# Convert `death` into a logical vector\ndf$death &lt;- factor(df$death, levels = c('No', 'Yes'), labels = c('FALSE', 'TRUE'))\ndf$death &lt;- as.logical(df$death)",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#problem-statement",
    "href": "confoundingE.html#problem-statement",
    "title": "Exercise 1 (R)",
    "section": "",
    "text": "Generate a variable age.cat that converts age into a factor with levels [0, 50), [50, 60), [60, 70), [70, 80), and [80, Inf).\nConvert race into a factor with levels white, black, and other.\nConvert sex into a factor with levels Male and Female.\nConvert cat1 into a factor with levels ARF, CHF, MOSF, and Other.\nConvert ca into a factor with levels None, Localized (Yes), and Metastatic.\nGenerate a variable called n.comorbidities that counts the total number of the following comorbidities per person: cardiohx, chfhx, dementhx, psychhx, chrpulhx, renalhx, liverhx, gibledhx, malighx, immunhx, transhx, amihx.\nConvert swang1 into a factor with levels No RHC and RHC.\nConvert death into a logical vector with FALSE/TRUE values.",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#a-generating-the-analytic-dataset",
    "href": "confoundingE.html#a-generating-the-analytic-dataset",
    "title": "Exercise 1 (R)",
    "section": "1(a): Generating the analytic dataset",
    "text": "1(a): Generating the analytic dataset\nGenerate an analytic dataset called df.analytic according to the following parameters:\n\nContains the variables age.cat, sex, race, cat1, ca, dnr1, aps1, surv2md1, n.comorbidities, adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21, ph1, crea1, alb1, scoma1, swang1, death\n\nSubset to complete case observations\n\nNOTE. The resulting analytic dataset should have 24 columns and 1439 rows.\n\n# Generate df.analytic\n# df.analytic &lt;- ... # Complete the code here\n\n# Verify the number of columns and rows within df.analytic\n#",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#b-generating-a-descriptive-table-1.-20-grade",
    "href": "confoundingE.html#b-generating-a-descriptive-table-1.-20-grade",
    "title": "Exercise 1 (R)",
    "section": "1(b): Generating a descriptive Table 1. [20% grade]",
    "text": "1(b): Generating a descriptive Table 1. [20% grade]\nUsing df.analytic, produce a descriptive Table 1 that matches the following sample table:\n\n\n\n\n\n\n\n\n\nOverall\nFALSE\nTRUE\n\n\n\nn\n1439\n733\n706\n\n\nadld3p (mean (SD))\n1.18 (1.82)\n0.96 (1.68)\n1.41 (1.93)\n\n\nage.cat (%)\n\n\n\n\n\n   [0,50)\n377 (26.2)\n253 (34.5)\n124 (17.6)\n\n\n   [50,60)\n245 (17.0)\n115 (15.7)\n130 (18.4)\n\n\n   [60,70)\n360 (25.0)\n158 (21.6)\n202 (28.6)\n\n\n   [70,80)\n308 (21.4)\n144 (19.6)\n164 (23.2)\n\n\n   [80,Inf]\n149 (10.4)\n63 ( 8.6)\n86 (12.2)\n\n\nalb1 (mean (SD))\n3.24 (0.64)\n3.22 (0.66)\n3.25 (0.62)\n\n\naps1 (mean (SD))\n48.63 (17.32)\n47.51 (17.24)\n49.80 (17.32)\n\n\nca (%)\n\n\n\n\n\n   None\n1121 (77.9)\n629 (85.8)\n492 (69.7)\n\n\n   Localized (Yes)\n217 (15.1)\n88 (12.0)\n129 (18.3)\n\n\n   Metastatic\n101 ( 7.0)\n16 ( 2.2)\n85 (12.0)\n\n\ncat1 (%)\n\n\n\n\n\n   ARF\n556 (38.6)\n331 (45.2)\n225 (31.9)\n\n\n   CHF\n303 (21.1)\n134 (18.3)\n169 (23.9)\n\n\n   MOSF\n290 (20.2)\n136 (18.6)\n154 (21.8)\n\n\n   Other\n290 (20.2)\n132 (18.0)\n158 (22.4)\n\n\ncrea1 (mean (SD))\n2.08 (2.21)\n1.96 (2.08)\n2.21 (2.34)\n\n\ndas2d3pc (mean (SD))\n20.36 (7.19)\n21.75 (7.63)\n18.91 (6.39)\n\n\ndnr1 = Yes (%)\n98 ( 6.8)\n26 ( 3.5)\n72 (10.2)\n\n\nhrt1 (mean (SD))\n111.26 (38.50)\n112.14 (38.18)\n110.36 (38.84)\n\n\nmeanbp1 (mean (SD))\n82.90 (37.49)\n86.51 (39.21)\n79.15 (35.25)\n\n\nn.comorbidities (mean (SD))\n1.75 (1.22)\n1.51 (1.20)\n2.00 (1.19)\n\n\npaco21 (mean (SD))\n40.52 (13.60)\n40.72 (14.47)\n40.31 (12.64)\n\n\npafi1 (mean (SD))\n247.64 (110.40)\n237.35 (109.56)\n258.33 (110.34)\n\n\nph1 (mean (SD))\n7.39 (0.10)\n7.39 (0.10)\n7.39 (0.09)\n\n\nrace (%)\n\n\n\n\n\n   white\n1110 (77.1)\n564 (76.9)\n546 (77.3)\n\n\n   black\n243 (16.9)\n129 (17.6)\n114 (16.1)\n\n\n   other\n86 ( 6.0)\n40 ( 5.5)\n46 ( 6.5)\n\n\nresp1 (mean (SD))\n29.03 (12.17)\n29.08 (12.34)\n28.98 (11.99)\n\n\nscoma1 (mean (SD))\n5.60 (16.22)\n6.30 (17.77)\n4.87 (14.41)\n\n\nsex = Female (%)\n617 (42.9)\n336 (45.8)\n281 (39.8)\n\n\nsurv2md1 (mean (SD))\n0.70 (0.16)\n0.73 (0.13)\n0.66 (0.17)\n\n\nswang1 = RHC (%)\n390 (27.1)\n196 (26.7)\n194 (27.5)\n\n\ntemp1 (mean (SD))\n37.32 (1.65)\n37.52 (1.67)\n37.11 (1.60)\n\n\nwblc1 (mean (SD))\n14.54 (11.71)\n14.36 (8.45)\n14.72 (14.33)\n\n\n\nNOTE. Ensure that the order of all variables and the levels of all factors matches the sample table.\n\n# Load the `tableone` package into the library\nlibrary(tableone)\n\n# Generate the Table 1\n#",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#a-crude-models-30-grade",
    "href": "confoundingE.html#a-crude-models-30-grade",
    "title": "Exercise 1 (R)",
    "section": "2(a): Crude Models [30% grade]",
    "text": "2(a): Crude Models [30% grade]\nUsing df.analytic, estimate the crude odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions, and round your estimates to 3 decimal places.\n\nWhen estimating crude ORs, use a logistic model.\n\n\n# Estimate & print crude odds ratio, \n# and associated confidence interval\n\n\nWhen estimating crude RRs, use a Poisson model with robust SEs.\n\n\n# Estimate & print crude risk ratio,\n# and associated confidence interval\n\n\nWhen estimating crude RDs, use a Gaussian model with robust SEs.\n\n\n# Estimate & print crude risk difference, and \n# associated confidence interval",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#b-conditional-models-20-grade",
    "href": "confoundingE.html#b-conditional-models-20-grade",
    "title": "Exercise 1 (R)",
    "section": "2(b): Conditional Models [20% grade]",
    "text": "2(b): Conditional Models [20% grade]\nUsing df.analytic, estimate the conditional odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions, and round your estimates to 3 decimal places. Adjust for all covariates found in df.analytic.\n\nWhen estimating conditional odds ratios, use a logistic model.\n\n\n# Estimate & print conditional odds ratio, and \n# associated confidence interval\n\n\nWhen estimating conditional risk ratios, use a Poisson model with robust SEs (i.e., modified Poisson regression).\n\n\n# Estimate & print conditional risk ratio, \n# and associated confidence interval\n\n\nWhen estimating conditional risk differences, use a Gaussian model with an identity link and robust SEs (i.e., linear regression with robust SEs).\n\n\n# Estimate & print conditional risk difference, \n# and associated confidence interval",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#c-marginal-models-30-grade",
    "href": "confoundingE.html#c-marginal-models-30-grade",
    "title": "Exercise 1 (R)",
    "section": "2(c): Marginal Models [30% grade]",
    "text": "2(c): Marginal Models [30% grade]\nUsing df.analytic, estimate the marginal odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions: Round your estimates to 3 decimal places. Adjust for all covariates found in df.analytic. Bootstrap could be used to estimate confidence intervals, but we won’t be calculating confidence intervals for the marginal models.\n\nWhen estimating marginal odds ratios, use a logistic model.\n\n\n# Estimate & print marginal odds ratio\n\n\nWhen estimating marginal risk ratios, use a Poisson model with robust SEs (i.e., modified Poisson regression).\n\n\n# Estimate & print marginal risk ratio\n\n\nWhen estimating marginal risk differences, use a Gaussian model with an identity link and robust SEs (i.e., linear regression with robust SEs).\n\n\n# Estimate & print marginal risk difference",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#d-summarizing-models",
    "href": "confoundingE.html#d-summarizing-models",
    "title": "Exercise 1 (R)",
    "section": "2(d): Summarizing Models",
    "text": "2(d): Summarizing Models\nBased upon the results from 2(a) - 2(c), complete the following table by replacing 9.999 with estimates from the corresponding models (edit if needed/your results are different):\n\n\n\n\n\n\n\n\nModeling Strategy\nOR (95% CI)\nRR (95% CI)\nRD (95% CI)\n\n\n\nCrude Est.\n\n\n\n\n\n   swang1 [RHC]\n1.038 (0.823, 1.310)\n1.019 (0.906, 1.146)\n0.009 (-0.049, 0.067)\n\n\nConditional Est.\n\n\n\n\n\n   swang1 [RHC]\n1.072 (0.805, 1.427)\n1.030 (0.913, 1.161)\n0.013 (-0.044, 0.071)\n\n\nMarginal Est.\n\n\n\n\n\n   swang1 [RHC]\n1.058\n1.03\n0.013",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingE.html#e-interpreting-optional",
    "href": "confoundingE.html#e-interpreting-optional",
    "title": "Exercise 1 (R)",
    "section": "2(e): Interpreting (optional)",
    "text": "2(e): Interpreting (optional)\nWhich of the three measure of effects are collapsible and why?\nConditional estimates are adjusted for covariates and show the effect of the exposure (RHC) accounting for confounders. Marginal estimates reflect the overall effect of the exposure across the entire population without considering stratification by covariates, providing an “average” effect.\n\nThe conditional OR (1.072) and the marginal OR (1.058) are different. This difference illustrates the non-collapsibility of the odds ratio. Even after adjusting for covariates (conditional), the OR changes when calculating the marginal estimate across the population, because the odds ratio is sensitive to how covariates interact with the exposure and outcome.\nThe conditional RR (1.030) and marginal RR (1.030) are identical. This indicates that the RR is collapsible because adjusting for covariates (conditional model) does not change the overall effect when you average it across the population (marginal model).\nThe conditional RD (0.013) and marginal RD (0.013) are also identical, further supporting that the RD is collapsible. Whether you adjust for covariates or average across the entire population, the absolute difference in risk remains the same.",
    "crumbs": [
      "Causal roles",
      "Exercise 1 (R)"
    ]
  },
  {
    "objectID": "confoundingEsolution.html",
    "href": "confoundingEsolution.html",
    "title": "Exercise 1 Solution (R)",
    "section": "",
    "text": "Assignment Setup\nThe following lab builds upon the RHC data set used in Lab Assignment #1 and explores the marginal and conditional effects of right-heart-catheteriziation treatment (swang1) on mortality (death).\nWe begin by loading the RHC data set from Lab #1…\n# Import the rhc data set\n# read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE)\ndf &lt;- read.csv('Data/confounding/rhc.csv', header = TRUE)\nWe then apply the following data-wrangling procedures, which we implemented in Lab #1:\n# Generate age.cat\ndf$age.cat &lt;- cut(df$age, breaks = c(0, 50, 60, 70, 80, Inf), \n                  include.lowest = TRUE, right = FALSE)\n\n# Factor `race`\ndf$race &lt;- factor(x = df$race, levels = c('white', 'black', 'other'))\n\n# Factor `sex`\ndf$sex &lt;- factor(x = df$sex, levels = c('Male', 'Female'))\n\n# Factor `cat1`\ndf$cat1 &lt;- factor(df$cat1)\nlevels(df$cat1) = list(ARF = 'ARF',\n                       CHF = 'CHF',\n                       MOSF = c('MOSF w/Malignancy', 'MOSF w/Sepsis'),\n                       Other = c('Cirrhosis', 'Colon Cancer', 'Coma', \n                                 'COPD', 'Lung Cancer'))\n\n# Factoring `ca`\ndf$ca &lt;- factor(x = df$ca, \n                levels = c('No', 'Yes', 'Metastatic'),\n                labels = c('None', 'Localized (Yes)', 'Metastatic'))\n\n# Generate n.comorbiditis\ndf$n.comorbidities &lt;- rowSums(x = subset(df, select = cardiohx:amihx), na.rm = TRUE)\n\n# Factor `swang1`\ndf$swang1 &lt;- factor(x = df$swang1, levels = c('No RHC', 'RHC'))\n\n# Convert `death` into a logical vector\ndf$death &lt;- factor(df$death, levels = c('No', 'Yes'), labels = c('FALSE', 'TRUE'))\ndf$death &lt;- as.logical(df$death)",
    "crumbs": [
      "Causal roles",
      "Exercise 1 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingEsolution.html#assignment-setup",
    "href": "confoundingEsolution.html#assignment-setup",
    "title": "Exercise 1 Solution (R)",
    "section": "",
    "text": "Generate a variable age.cat that converts age into a factor with levels [0, 50), [50, 60), [60, 70), [70, 80), and [80, Inf).\nConvert race into a factor with levels white, black, and other.\nConvert sex into a factor with levels Male and Female.\nConvert cat1 into a factor with levels ARF, CHF, MOSF, and Other.\nConvert ca into a factor with levels None, Localized (Yes), and Metastatic.\nGenerate a variable called n.comorbidities that counts the total number of the following comorbidities per person: cardiohx, chfhx, dementhx, psychhx, chrpulhx, renalhx, liverhx, gibledhx, malighx, immunhx, transhx, amihx.\nConvert swang1 into a factor with levels No RHC and RHC.\nConvert death into a logical vector with FALSE/TRUE values.",
    "crumbs": [
      "Causal roles",
      "Exercise 1 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingEsolution.html#problem-1-summarizing-analytic-dataset",
    "href": "confoundingEsolution.html#problem-1-summarizing-analytic-dataset",
    "title": "Exercise 1 Solution (R)",
    "section": "Problem 1: Summarizing Analytic Dataset",
    "text": "Problem 1: Summarizing Analytic Dataset\n1(a): Generating the analytic dataset\nGenerate an analytic dataset called df.analytic according to the following parameters:\n\nContains the variables age.cat, sex, race, cat1, ca, dnr1, aps1, surv2md1, n.comorbidities, adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21, ph1, crea1, alb1, scoma1, swang1, death\n\nSubset to complete case observations\n\nNOTE. The resulting analytic dataset should have 24 columns and 1439 rows.\n\n# Generate df.analytic\ndf.analytic &lt;- \n  df |&gt;\n  subset(select = c(age.cat, sex, race, cat1, ca, dnr1, aps1, surv2md1, n.comorbidities,\n                    adld3p, das2d3pc, temp1, hrt1, meanbp1, resp1, wblc1, pafi1, paco21,\n                    ph1, crea1, alb1, scoma1, swang1, death)) |&gt;\n  na.omit()\n\n# Verify the number of columns and rows within df.analytic\ndim(df.analytic)\n#&gt; [1] 1439   24\n\n1(b): Generating a descriptive Table 1.\nUsing df.analytic, produce a descriptive Table 1 that matches the following sample table:\n\n\n\n\n\n\n\n\n\nOverall\nFALSE\nTRUE\n\n\n\nn\n1439\n733\n706\n\n\nadld3p (mean (SD))\n1.18 (1.82)\n0.96 (1.68)\n1.41 (1.93)\n\n\nage.cat (%)\n\n\n\n\n\n   [0,50)\n377 (26.2)\n253 (34.5)\n124 (17.6)\n\n\n   [50,60)\n245 (17.0)\n115 (15.7)\n130 (18.4)\n\n\n   [60,70)\n360 (25.0)\n158 (21.6)\n202 (28.6)\n\n\n   [70,80)\n308 (21.4)\n144 (19.6)\n164 (23.2)\n\n\n   [80,Inf]\n149 (10.4)\n63 ( 8.6)\n86 (12.2)\n\n\nalb1 (mean (SD))\n3.24 (0.64)\n3.22 (0.66)\n3.25 (0.62)\n\n\naps1 (mean (SD))\n48.63 (17.32)\n47.51 (17.24)\n49.80 (17.32)\n\n\nca (%)\n\n\n\n\n\n   None\n1121 (77.9)\n629 (85.8)\n492 (69.7)\n\n\n   Localized (Yes)\n217 (15.1)\n88 (12.0)\n129 (18.3)\n\n\n   Metastatic\n101 ( 7.0)\n16 ( 2.2)\n85 (12.0)\n\n\ncat1 (%)\n\n\n\n\n\n   ARF\n556 (38.6)\n331 (45.2)\n225 (31.9)\n\n\n   CHF\n303 (21.1)\n134 (18.3)\n169 (23.9)\n\n\n   MOSF\n290 (20.2)\n136 (18.6)\n154 (21.8)\n\n\n   Other\n290 (20.2)\n132 (18.0)\n158 (22.4)\n\n\ncrea1 (mean (SD))\n2.08 (2.21)\n1.96 (2.08)\n2.21 (2.34)\n\n\ndas2d3pc (mean (SD))\n20.36 (7.19)\n21.75 (7.63)\n18.91 (6.39)\n\n\ndnr1 = Yes (%)\n98 ( 6.8)\n26 ( 3.5)\n72 (10.2)\n\n\nhrt1 (mean (SD))\n111.26 (38.50)\n112.14 (38.18)\n110.36 (38.84)\n\n\nmeanbp1 (mean (SD))\n82.90 (37.49)\n86.51 (39.21)\n79.15 (35.25)\n\n\nn.comorbidities (mean (SD))\n1.75 (1.22)\n1.51 (1.20)\n2.00 (1.19)\n\n\npaco21 (mean (SD))\n40.52 (13.60)\n40.72 (14.47)\n40.31 (12.64)\n\n\npafi1 (mean (SD))\n247.64 (110.40)\n237.35 (109.56)\n258.33 (110.34)\n\n\nph1 (mean (SD))\n7.39 (0.10)\n7.39 (0.10)\n7.39 (0.09)\n\n\nrace (%)\n\n\n\n\n\n   white\n1110 (77.1)\n564 (76.9)\n546 (77.3)\n\n\n   black\n243 (16.9)\n129 (17.6)\n114 (16.1)\n\n\n   other\n86 ( 6.0)\n40 ( 5.5)\n46 ( 6.5)\n\n\nresp1 (mean (SD))\n29.03 (12.17)\n29.08 (12.34)\n28.98 (11.99)\n\n\nscoma1 (mean (SD))\n5.60 (16.22)\n6.30 (17.77)\n4.87 (14.41)\n\n\nsex = Female (%)\n617 (42.9)\n336 (45.8)\n281 (39.8)\n\n\nsurv2md1 (mean (SD))\n0.70 (0.16)\n0.73 (0.13)\n0.66 (0.17)\n\n\nswang1 = RHC (%)\n390 (27.1)\n196 (26.7)\n194 (27.5)\n\n\ntemp1 (mean (SD))\n37.32 (1.65)\n37.52 (1.67)\n37.11 (1.60)\n\n\nwblc1 (mean (SD))\n14.54 (11.71)\n14.36 (8.45)\n14.72 (14.33)\n\n\n\nNOTE. Ensure that the order of all variables and the levels of all factors matches the sample table.\n\n# Load the `tableone` package into the library\nlibrary(tableone)\n\n# Generate the Table 1\n.vars &lt;- sort(names(df.analytic))[-9]\nCreateTableOne(vars = .vars,\n               strata = 'death',\n               data = df.analytic,\n               includeNA = TRUE,\n               test = FALSE,\n               addOverall = TRUE)\n#&gt;                              Stratified by death\n#&gt;                               Overall         FALSE           TRUE           \n#&gt;   n                             1439             733             706         \n#&gt;   adld3p (mean (SD))            1.18 (1.82)     0.96 (1.68)     1.41 (1.93)  \n#&gt;   age.cat (%)                                                                \n#&gt;      [0,50)                      377 (26.2)      253 (34.5)      124 (17.6)  \n#&gt;      [50,60)                     245 (17.0)      115 (15.7)      130 (18.4)  \n#&gt;      [60,70)                     360 (25.0)      158 (21.6)      202 (28.6)  \n#&gt;      [70,80)                     308 (21.4)      144 (19.6)      164 (23.2)  \n#&gt;      [80,Inf]                    149 (10.4)       63 ( 8.6)       86 (12.2)  \n#&gt;   alb1 (mean (SD))              3.24 (0.64)     3.22 (0.66)     3.25 (0.62)  \n#&gt;   aps1 (mean (SD))             48.63 (17.32)   47.51 (17.24)   49.80 (17.32) \n#&gt;   ca (%)                                                                     \n#&gt;      None                       1121 (77.9)      629 (85.8)      492 (69.7)  \n#&gt;      Localized (Yes)             217 (15.1)       88 (12.0)      129 (18.3)  \n#&gt;      Metastatic                  101 ( 7.0)       16 ( 2.2)       85 (12.0)  \n#&gt;   cat1 (%)                                                                   \n#&gt;      ARF                         556 (38.6)      331 (45.2)      225 (31.9)  \n#&gt;      CHF                         303 (21.1)      134 (18.3)      169 (23.9)  \n#&gt;      MOSF                        290 (20.2)      136 (18.6)      154 (21.8)  \n#&gt;      Other                       290 (20.2)      132 (18.0)      158 (22.4)  \n#&gt;   crea1 (mean (SD))             2.08 (2.21)     1.96 (2.08)     2.21 (2.34)  \n#&gt;   das2d3pc (mean (SD))         20.36 (7.19)    21.75 (7.63)    18.91 (6.39)  \n#&gt;   dnr1 = Yes (%)                  98 ( 6.8)       26 ( 3.5)       72 (10.2)  \n#&gt;   hrt1 (mean (SD))            111.26 (38.50)  112.14 (38.18)  110.36 (38.84) \n#&gt;   meanbp1 (mean (SD))          82.90 (37.49)   86.51 (39.21)   79.15 (35.25) \n#&gt;   n.comorbidities (mean (SD))   1.75 (1.22)     1.51 (1.20)     2.00 (1.19)  \n#&gt;   paco21 (mean (SD))           40.52 (13.60)   40.72 (14.47)   40.31 (12.64) \n#&gt;   pafi1 (mean (SD))           247.64 (110.40) 237.35 (109.56) 258.33 (110.34)\n#&gt;   ph1 (mean (SD))               7.39 (0.10)     7.39 (0.10)     7.39 (0.09)  \n#&gt;   race (%)                                                                   \n#&gt;      white                      1110 (77.1)      564 (76.9)      546 (77.3)  \n#&gt;      black                       243 (16.9)      129 (17.6)      114 (16.1)  \n#&gt;      other                        86 ( 6.0)       40 ( 5.5)       46 ( 6.5)  \n#&gt;   resp1 (mean (SD))            29.03 (12.17)   29.08 (12.34)   28.98 (11.99) \n#&gt;   scoma1 (mean (SD))            5.60 (16.22)    6.30 (17.77)    4.87 (14.41) \n#&gt;   sex = Female (%)               617 (42.9)      336 (45.8)      281 (39.8)  \n#&gt;   surv2md1 (mean (SD))          0.70 (0.16)     0.73 (0.13)     0.66 (0.17)  \n#&gt;   swang1 = RHC (%)               390 (27.1)      196 (26.7)      194 (27.5)  \n#&gt;   temp1 (mean (SD))            37.32 (1.65)    37.52 (1.67)    37.11 (1.60)  \n#&gt;   wblc1 (mean (SD))            14.54 (11.71)   14.36 (8.45)    14.72 (14.33)",
    "crumbs": [
      "Causal roles",
      "Exercise 1 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingEsolution.html#problem-2-crude-conditional-and-marginal-regression-models",
    "href": "confoundingEsolution.html#problem-2-crude-conditional-and-marginal-regression-models",
    "title": "Exercise 1 Solution (R)",
    "section": "Problem 2: Crude, Conditional and Marginal Regression Models",
    "text": "Problem 2: Crude, Conditional and Marginal Regression Models\nFor this next section, refer to the following examples in the Advanced Epi Methods text. Additionally, you may find it useful to review Naimi & Whitcomb (2020) as a primer on how to estimate odds ratios, risk ratios, and risk differences via generalized linear models (see Table 2 of the article).\n2(a): Crude Models\nUsing df.analytic, estimate the crude odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions, and round your estimates to 3 decimal places.\n\nWhen estimating crude ORs, use a logistic model.\n\n\n# Estimate & print crude odds ratio & summarizing model\nmod.2a.1 &lt;-\nglm(death ~ swang1, \n    family = binomial(link = 'logit'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, digits = 3)\n\nmod.2a.1$regressionTable[1:2,]\n\n\n  \n\n\n\n\nWhen estimating crude RRs, use a Poisson model with robust SEs.\n\n\n# Estimate & print crude risk ratio\nmod.2a.2 &lt;-\nglm(death ~ swang1, \n    family = poisson(link = 'log'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, confint.method = 'robust', digits = 3)\n\nmod.2a.2$regressionTable[1:2,]\n\n\n  \n\n\n\n\nWhen estimating crude RDs, use a Gaussian model with robust SEs.\n\n\n# Estimate & print crude risk difference\nmod.2a.3 &lt;-\nglm(death ~ swang1, \n    family = gaussian(link = 'identity'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, confint.method = 'robust', digits = 3)\n\nmod.2a.3$regressionTable[2:3,]\n\n\n  \n\n\n\n2(b): Conditional Models\nUsing df.analytic, estimate the conditional odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions, and round your estimates to 3 decimal places. Adjust for all covariates found in df.analytic.\n\nWhen estimating conditional odds ratios, use a logistic model.\n\n\n# Estimate & print conditional odds ratio\nmod.2b.1 &lt;-\nglm(death ~ swang1 + ., \n    family = binomial(link = 'logit'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, digits = 3)\n\nmod.2b.1$regressionTable[1:2,]\n\n\n  \n\n\n\n\nWhen estimating conditional risk ratios, use a Poisson model with robust SEs (i.e., modified Poisson regression).\n\n\n# Estimate & print conditional risk ratio\nmod.2b.2 &lt;-\nglm(death ~ swang1 + ., \n    family = poisson(link = 'log'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, confint.method = 'robust', digits = 3)\n\nmod.2b.2$regressionTable[1:2,]\n\n\n  \n\n\n\n\nWhen estimating conditional risk differences, use a Gaussian model with an identity link and robust SEs (i.e., linear regression with robust SEs).\n\n\n# Estimate & print conditional risk difference\nmod.2b.3 &lt;-\nglm(death ~ swang1 + ., \n    family = gaussian(link = 'identity'),\n    data = df.analytic) |&gt;\n  Publish::publish(print = FALSE, confint.method = 'robust', digits = 3)\n\nmod.2b.3$regressionTable[2:3,]\n\n\n  \n\n\n\n2(c): Marginal Models\nUsing df.analytic, estimate the marginal odds ratio, risk ratio, and risk difference for the effect of swang1 (exposure) on death (outcome). Please adhere to the following instructions: Round your estimates to 3 decimal places. Adjust for all covariates found in df.analytic. Bootstrap could be used to estimate confidence intervals, but we won’t be calculating confidence intervals for the marginal models.\n\nWhen estimating marginal odds ratios, use a logistic model.\n\n\n# Logistic regression model\nmod_marg_or &lt;- glm(death ~ swang1 + ., family = binomial(link = \"logit\"), \n                   data = df.analytic)\n\n# Create a new dataset where everyone receives the treatment (swang1 = 1)\ndf.analytic_rhc &lt;- df.analytic\ndf.analytic_rhc$swang1 &lt;- \"RHC\"\np1 &lt;- mean(predict(mod_marg_or, newdata = df.analytic_rhc, type = \"response\"))\n\n# Create a new dataset where no one receives the treatment (swang1 = 0)\ndf.analytic_rhc$swang1 &lt;- \"No RHC\"\np0 &lt;- mean(predict(mod_marg_or, newdata = df.analytic_rhc, type = \"response\"))\n\n# Calculate the marginal odds ratio\nORm &lt;- (p1 / (1 - p1)) / (p0 / (1 - p0))\nORm\n#&gt; [1] 1.057679\nround(ORm, 3)\n#&gt; [1] 1.058\n\n\nWhen estimating marginal risk ratios, use a Poisson model with robust SEs (i.e., modified Poisson regression).\n\n\n# Poisson regression model for risk ratio\nmod_marg_rr &lt;- glm(death ~ swang1 + ., family = poisson(link = \"log\"), \n                   data = df.analytic)\n\n# Set swang1 = 1 for everyone\ndf.analytic_rhc &lt;- df.analytic\ndf.analytic_rhc$swang1 &lt;- \"RHC\"\np1 &lt;- mean(predict(mod_marg_rr, newdata = df.analytic_rhc, type = \"response\"))\n\n# Set swang1 = 0 for everyone\ndf.analytic_rhc$swang1 &lt;- \"No RHC\"\np0 &lt;- mean(predict(mod_marg_rr, newdata = df.analytic_rhc, type = \"response\"))\n\n# Calculate the marginal risk ratio\nRRm &lt;- p1 / p0\nRRm\n#&gt; [1] 1.029778\nround(RRm, 3)\n#&gt; [1] 1.03\n\n\nWhen estimating marginal risk differences, use a Gaussian model with an identity link and robust SEs (i.e., linear regression with robust SEs).\n\n\n# Gaussian model for risk difference\nmod_marg_rd &lt;- glm(death ~ swang1 + ., family = gaussian(link = \"identity\"), \n                   data = df.analytic)\n\n# Set swang1 = 1 for everyone\ndf.analytic_rhc &lt;- df.analytic\ndf.analytic_rhc$swang1 &lt;- \"RHC\"\np1 &lt;- mean(predict(mod_marg_rd, newdata = df.analytic_rhc, type = \"response\"))\n\n# Set swang1 = 0 for everyone\ndf.analytic_rhc$swang1 &lt;- \"No RHC\"\np0 &lt;- mean(predict(mod_marg_rd, newdata = df.analytic_rhc, type = \"response\"))\n\n# Calculate the marginal risk difference\nRDm &lt;- p1 - p0\nRDm\n#&gt; [1] 0.01323736\nround(RDm, 3)\n#&gt; [1] 0.013\n\n2(d): Summarizing Models\nBased upon the results from 2(a) - 2(c), complete the following table by replacing 9.999 with estimates from the corresponding models:\n\n\n\n\n\n\n\n\nModeling Strategy\nOR (95% CI)\nRR (95% CI)\nRD (95% CI)\n\n\n\nCrude Est.\n\n\n\n\n\n   swang1 [RHC]\n1.038 (0.823, 1.310)\n1.019 (0.906, 1.146)\n0.009 (-0.049, 0.067)\n\n\nConditional Est.\n\n\n\n\n\n   swang1 [RHC]\n1.072 (0.805, 1.427)\n1.030 (0.913, 1.161)\n0.013 (-0.044, 0.071)\n\n\nMarginal Est.\n\n\n\n\n\n   swang1 [RHC]\n1.058\n1.03\n0.013",
    "crumbs": [
      "Causal roles",
      "Exercise 1 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingE2.html",
    "href": "confoundingE2.html",
    "title": "Exercise 2 (R)",
    "section": "",
    "text": "About the data\nThe following lab builds upon the RHC data used previously.\n# Create \"data\" directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")\n}\n\n# Importing and transforming RHC data\nrhc_data &lt;- \n  read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE) |&gt;\n\n  # Generating `age_category`\n  transform(age_category = cut(age,\n                          breaks = c(0, 50, 60, 70, 80, Inf),\n                          include.lowest = TRUE, right = FALSE)) |&gt;\n  \n  # Factoring `race`\n  transform(race = factor(race,\n                          levels = c('white', 'black', 'other'),\n                          labels = c('White', 'Black', 'Other'))) |&gt;\n  \n  # Factoring `sex`\n  transform(sex = factor(sex, levels = c('Male', 'Female'))) |&gt;\n  \n  # Factoring `primary_diagnosis`\n  transform(primary_diagnosis = factor(cat1)) |&gt;\n  within(levels(primary_diagnosis) &lt;- list(ARF = 'ARF',\n                                           CHF = 'CHF',\n                                           MOSF = c('MOSF w/Malignancy', 'MOSF w/Sepsis'),\n                                           Other = c('Cirrhosis', 'Colon Cancer', 'Coma', \n                                                     'COPD', 'Lung Cancer'))) |&gt;\n  \n  # Factoring `cancer_status`\n  transform(cancer_status = factor(ca,\n                                   levels = c('No', 'Yes', 'Metastatic'),\n                                   labels = c('None', 'Localized (Yes)', 'Metastatic'))) |&gt;\n  \n  # Generating `num_comorbidities`\n  {\\(.x) transform(.x, num_comorbidities = rowSums(subset(.x, select = \n                                                            cardiohx:amihx)))}() |&gt;\n  \n  # Factoring `rhc_status`\n  transform(rhc_status = factor(swang1, levels = c('No RHC', 'RHC'))) |&gt;\n  \n  # Converting `death` to logical\n  transform(death_status = ifelse(death == 'Yes', TRUE, FALSE)) |&gt;\n  \n  # Renaming other variables to be more meaningful\n  rename(dnr_status = dnr1,\n         aps_score = aps1,\n         two_month_survival_probability = surv2md1,\n         activities_of_daily_living = adld3p,\n         duke_activity_status_index = das2d3pc,\n         temperature = temp1,\n         heart_rate = hrt1,\n         mean_blood_pressure = meanbp1,\n         respiratory_rate = resp1,\n         white_blood_cell_count = wblc1,\n         pafi_ratio = pafi1,\n         partial_pressure_of_co2 = paco21,\n         blood_ph = ph1,\n         creatinine = crea1,\n         albumin = alb1,\n         glasgow_coma_scale = scoma1) |&gt;\n  \n  # Subsetting relevant columns\n  subset(select = c(age_category, sex, race, primary_diagnosis, \n                    cancer_status, dnr_status, aps_score, \n                    two_month_survival_probability, activities_of_daily_living, \n                    duke_activity_status_index, temperature, heart_rate, \n                    mean_blood_pressure, respiratory_rate, white_blood_cell_count, \n                    pafi_ratio, partial_pressure_of_co2, blood_ph, creatinine, \n                    albumin, glasgow_coma_scale, \n                    rhc_status, death_status, num_comorbidities)) |&gt;\n  \n  # Subsetting complete case observations\n  na.omit()\n\n# Save the transformed dataset\nsaveRDS(rhc_data, \"data/rhc_data.rds\")\nWe begin by importing the RHC data, which has already been prepared for this assignment. What is different in this exercise is that we are changing the reference levels of RHC and DNR variables to No RHC and No, respectively.\n# Loading the RHC data\nrhc_data &lt;- readRDS(\"data/rhc_data.rds\")\n# Convert 'rhc_status' to binary (1 for 'No RHC', 0 for 'RHC')\nrhc_data$no_rhc_status &lt;- ifelse(rhc_data$rhc_status == \"No RHC\", 1, 0)\n# Convert 'dnr_status' to binary (1 for 'No', 0 for 'Yes')\nrhc_data$no_dnr_status &lt;- ifelse(rhc_data$dnr_status == \"No\", 1, 0)\nThe variables included within this data extract are:\nstr(rhc_data)\nFor information regarding the variables included in the data, review the variable descriptions here and here.\nThe Duke Activity Status Index (DASI) is a self-administered questionnaire that measures a patient’s functional capacity or physical activity level. It is commonly used in cardiology to assess a patient’s ability to perform daily activities and is predictive of cardiovascular outcomes. The DASI score ranges from 0 to 58.2, with higher scores indicating better physical functioning and higher activity levels. The DASI score in this dataset ranges from 11 to 33. The median value of 19 and the mean value of 20.36 suggest that the typical person in this dataset has a moderate activity level. In this exercise, we will only work with the subset of the data where the DASI score is greater than 20.\n# Subset the data\nhigh_dasi_data &lt;- rhc_data[rhc_data$duke_activity_status_index &gt; 20, ]",
    "crumbs": [
      "Causal roles",
      "Exercise 2 (R)"
    ]
  },
  {
    "objectID": "confoundingE2.html#about-the-assignment",
    "href": "confoundingE2.html#about-the-assignment",
    "title": "Exercise 2 (R)",
    "section": "About the assignment",
    "text": "About the assignment\nEffect Modification refers to how the effect of an exposure on an outcome differs across levels of a third variable (the effect modifier). It focuses on how the association between the exposure and the outcome changes based on this modifying factor. This lab adapts exercises from the following tutorial on “Interaction in Epidemiology”.\nFor Problem Set #1, we will explore in the high_dasi_data dataset whether the association between right-heart-catheterization (no_rhc_status; treatment or exposure) on mortality (death_status; outcome) is modified by whether a participant signed a do-not-resuscitate order (no_dnr_status; potential effect modifier).\nNote that we will assume that confounders for RHC and Mortality are the following:\n\nAge Category (age_category)\nAPS Score (aps_score)\nCancer Status (cancer_status)\nNumber of Comorbidities (num_comorbidities)\nGlasgow Coma Scale (glasgow_coma_scale)\nHeart Rate (heart_rate)\nMean Blood Pressure (mean_blood_pressure)\nPrimary Diagnosis (primary_diagnosis)\nSex (sex)\nRespiratory Rate (respiratory_rate)\nWhite Blood Cell Count (white_blood_cell_count)\n\nInteraction, particularly in the statistical context, often refers to a departure from multiplicativity when using models like logistic regression. It examines how the combined effect of two exposures differs from what would be expected if their effects were simply multiplied. Interaction is typically tested on a multiplicative scale (e.g., odds ratios) and is interpreted through synergy (super-multiplicative) or antagonism (sub-multiplicative).\nFor Problem Set #2, we will explore in the high_dasi_data dataset whether binary DNR status (“Do Not Resuscitate”: no_dnr_status) interacts with right-heart-catheterization (no_rhc_status) in its association with mortality (death_status).\nNote that we will assume that confounders for DNR Status and Mortality are the following:\n\nAge Category (age_category)\nAPS Score (aps_score)\nCancer Status (cancer_status)\nNumber of Comorbidities (num_comorbidities)\nGlasgow Coma Scale (glasgow_coma_scale)\nHeart Rate (heart_rate)\nMean Blood Pressure (mean_blood_pressure)\nPrimary Diagnosis (primary_diagnosis)\nWhite Blood Cell Count (white_blood_cell_count)\nActivities of Daily Living (activities_of_daily_living)",
    "crumbs": [
      "Causal roles",
      "Exercise 2 (R)"
    ]
  },
  {
    "objectID": "confoundingE2.html#problem-1-exploring-effect-modification-50",
    "href": "confoundingE2.html#problem-1-exploring-effect-modification-50",
    "title": "Exercise 2 (R)",
    "section": "Problem #1: Exploring Effect Modification [50%]",
    "text": "Problem #1: Exploring Effect Modification [50%]\n1(a) Fit the effect modification model [15%]\nGenerate a multivariable logistic regression model (effect_modification_model), adhering to the following specifications:\n\nSet the outcome as death.\nSet the exposure as no RHC.\nInclude a multiplicative interaction term of the exposure by no DNR status.\nAdjust for the stated confounders in the relationship (no RHC vs death) in the model.\n\n\n# Logistic regression with interaction between no_rhc_status and no_dnr_status in the subset data\n# effect_modification_model &lt;- glm(...\n\n1(b) Calculate Relative Excess Risk due to Interaction (RERI) [20%]\nEffect modification looks at how the effect of one exposure on an outcome changes based on the level of another exposure or factor (the effect modifier). RERI is useful in this context because it quantifies the additive interaction (whether the combined effect is larger or smaller than expected based on individual effects).\n\n# Extract the coefficients\n\n# Calculate the Odds Ratios\n\n# Calculate RERI (Relative Excess Risk due to Interaction)\n\n# # Output the results\n# cat(\"Relative Excess Risk due to Interaction (RERI):\", RERI, \"\\n\")\n\nInterpretation of RERI in Effect Modification:\n\n\nRERI &gt; 0: Indicates a positive additive interaction, meaning the combined effect of two exposures (e.g., no RHC and no DNR) is greater than the sum of their individual effects. In other words, there is an excess risk attributable to the interaction between the two exposures.\n\nRERI = 0: Indicates no additive interaction. The combined effect of the two exposures is equal to the sum of their individual effects. There is no additional risk due to their interaction.\n\nRERI &lt; 0: Indicates a negative additive interaction (also known as antagonism), meaning the combined effect of the two exposures is less than the sum of their individual effects. This suggests that the exposures may be protective when combined, compared to when considered separately.\n1(c) Exploring Effect Modification using interactionR [15%] and Publish (optional)\nBased on the output of the above regression, implement the following:\n\nUse interactionR::interactionR() to obtain a complete reporting of effect modification estimates.\n\n\n# Load the necessary package\nlibrary(interactionR)\n\n# Obtain the interaction effect modification estimates\n# interaction_result &lt;- \n\n# Display the necessary effect modification report\n# interaction_result$...\n\n\nUse Publish::publish() to obtain stratum-specific estimates of the main effect. NOTE: Only return those quantities requested (i.e., not the full regression table).\n\n\n# # Load the necessary package\n# library(Publish)\n# \n# # Recode variables for better interpretation\n# high_dasi_data$no_dnr_factor &lt;- factor(high_dasi_data$no_dnr_status, levels = c(1, 0), labels = c(\"No\", \"Yes\"))\n# high_dasi_data$no_dnr_factor &lt;- relevel(high_dasi_data$no_dnr_factor, ref = \"Yes\")\n# high_dasi_data$no_rhc_factor &lt;- factor(high_dasi_data$no_rhc_status, levels = c(1, 0), labels = c(\"No RHC\", \"RHC\"))\n# high_dasi_data$no_rhc_factor &lt;- relevel(high_dasi_data$no_rhc_factor, ref = \"RHC\")\n# \n# # Fit the model with recoded factors\n# effect_modification_model2 &lt;- glm(...\n# \n# # Obtain stratum-specific estimates from the model\n# stratum_estimates &lt;- publish(effect_modification_model2)\n# \n# # Print only the relevant stratum-specific estimates (for no_rhc_status and no_dnr_status)\n# tail(stratum_estimates$rawTable,2)\n\nNote: Effect modification occurs when the effect of an exposure on an outcome differs across levels of a third variable (the effect modifier). It does not require the interaction term (no_rhc_status:no_dnr_status or the multiplicative scale p-value) to be statistically significant; rather, we are looking at whether the association between the primary exposure (RHC) and the outcome (death) changes depending on the level of the effect modifier (DNR).",
    "crumbs": [
      "Causal roles",
      "Exercise 2 (R)"
    ]
  },
  {
    "objectID": "confoundingE2.html#problem-2-exploring-interaction-50",
    "href": "confoundingE2.html#problem-2-exploring-interaction-50",
    "title": "Exercise 2 (R)",
    "section": "Problem #2: Exploring Interaction [50%]",
    "text": "Problem #2: Exploring Interaction [50%]\n2(a) Interaction Model [15%]\nGenerate a multivariable logistic regression model, adhering to the following specifications:\n\nSet the outcome as death.\nSet two exposures as RHC and DNR status.\nInclude a multiplicative interaction term of the exposures.\nAdjust for the stated confounders in the relationship (no RHC vs death, as well as no DNR vs death) in the model. That means the union of the two confounder sets.\n\n\n# Logistic regression with interaction between no_rhc_status and no_dnr_status in the subset data\n# interaction_model &lt;- glm(...\n\nNote: Check if the interaction term (no_rhc_status:no_dnr_status) has a low p-value. If this p-value is below the acceptable cutpoint (commonly used significance threshold of 0.05 or something reasonable/justifiable), we could conclude that the interaction between RHC and DNR status on the outcome (death) is statistically significant. This could suggest that the association between RHC status and death depends on DNR status.\n2(b) Exploring Interaction using interactionR [15%]\nBased on the output of the above regression, implement the following:\n\nUse interactionR::interactionR() to obtain a complete reporting of interaction estimates.\n\n\n# Load the necessary package\nlibrary(interactionR)\n\n# Obtain the interaction estimates\n# interaction_result &lt;- ...\n\n2(c) Calculate RERI, AP, and SI [20%]\nBased on the output of the above regression, calculate RERI, the attributable proportion due to interaction (AP), and the synergy index (SI).\n\n# Extract the coefficients\n\n# Calculate the odds ratios\n\n\n# Calculate RERI\n\n# Calculate AP\n\n\n# Calculate SI\n\n\n# Output the results\n# cat(\"Relative Excess Risk due to Interaction (RERI):\", RERI, \"\\n\")\n# cat(\"Attributable Proportion (AP):\", AP, \"\\n\")\n# cat(\"Synergy Index (SI):\", SI, \"\\n\")\n\nInterpretation of Results:\n\n\nRERI: A positive RERI indicates an excess risk due to the interaction between RHC and DNR. A negative RERI suggests antagonism.\n\nAP: Represents the proportion of the risk due to the interaction.\n\nAP&gt;0: A positive proportion of the risk is due to the interaction.\nAP=0: No additional risk is due to the interaction.\nAP&lt;0: A negative interaction, indicating antagonism between the two exposures.\n\n\n\nSI: A synergy index greater than 1 indicates a synergistic effect, while an SI less than 1 indicates antagonism.\n\nSI&gt;1: Synergistic effect. The combined effect of the two exposures is greater than expected based on multiplicative effects.\nSI=1: No multiplicative interaction.\nSI&lt;1: Antagonism. The combined effect of the two exposures is less than expected based on multiplicative effects.\n\n\n\nNote: If the p-values for the interaction term (or the multiplicative scale), RERI, AP, and SI are all greater than a reasonable cut-point (e.g., 0.05 is one example), that indicates no statistically significant interaction. In that case, this would mean we do not have strong evidence to claim that the combined effects of RHC and DNR on mortality are significantly different from their individual effects.",
    "crumbs": [
      "Causal roles",
      "Exercise 2 (R)"
    ]
  },
  {
    "objectID": "confoundingE2.html#knit-your-file",
    "href": "confoundingE2.html#knit-your-file",
    "title": "Exercise 2 (R)",
    "section": "Knit your file",
    "text": "Knit your file\nPlease knit your file once you finished and submit the knitted PDF or doc file. Please also fill-up the group member names.",
    "crumbs": [
      "Causal roles",
      "Exercise 2 (R)"
    ]
  },
  {
    "objectID": "confoundingE2solution.html",
    "href": "confoundingE2solution.html",
    "title": "Exercise 2 Solution (R)",
    "section": "",
    "text": "About the data\nThe following lab builds upon the RHC data used previously.\n# Create nested directories if they don't exist\nif (!dir.exists(\"data/confounding\")) {\n  dir.create(\"data/confounding\", recursive = TRUE)\n}\n\n# Importing and transforming RHC data\nrhc_data &lt;- \n  read.csv(\"https://hbiostat.org/data/repo/rhc.csv\", header = TRUE) |&gt;\n\n  # Generating `age_category`\n  transform(age_category = cut(age,\n                          breaks = c(0, 50, 60, 70, 80, Inf),\n                          include.lowest = TRUE, right = FALSE)) |&gt;\n  \n  # Factoring `race`\n  transform(race = factor(race,\n                          levels = c('white', 'black', 'other'),\n                          labels = c('White', 'Black', 'Other'))) |&gt;\n  \n  # Factoring `sex`\n  transform(sex = factor(sex, levels = c('Male', 'Female'))) |&gt;\n  \n  # Factoring `primary_diagnosis`\n  transform(primary_diagnosis = factor(cat1)) |&gt;\n  within(levels(primary_diagnosis) &lt;- list(ARF = 'ARF',\n                                           CHF = 'CHF',\n                                           MOSF = c('MOSF w/Malignancy', 'MOSF w/Sepsis'),\n                                           Other = c('Cirrhosis', 'Colon Cancer', 'Coma', \n                                                     'COPD', 'Lung Cancer'))) |&gt;\n  \n  # Factoring `cancer_status`\n  transform(cancer_status = factor(ca,\n                                   levels = c('No', 'Yes', 'Metastatic'),\n                                   labels = c('None', 'Localized (Yes)', 'Metastatic'))) |&gt;\n  \n  # Generating `num_comorbidities`\n  {\\(.x) transform(.x, num_comorbidities = rowSums(subset(.x, select = \n                                                            cardiohx:amihx)))}() |&gt;\n  \n  # Factoring `rhc_status`\n  transform(rhc_status = factor(swang1, levels = c('No RHC', 'RHC'))) |&gt;\n  \n  # Converting `death` to logical\n  transform(death_status = ifelse(death == 'Yes', TRUE, FALSE)) |&gt;\n  \n  # Renaming other variables to be more meaningful\n  rename(dnr_status = dnr1,\n         aps_score = aps1,\n         two_month_survival_probability = surv2md1,\n         activities_of_daily_living = adld3p,\n         duke_activity_status_index = das2d3pc,\n         temperature = temp1,\n         heart_rate = hrt1,\n         mean_blood_pressure = meanbp1,\n         respiratory_rate = resp1,\n         white_blood_cell_count = wblc1,\n         pafi_ratio = pafi1,\n         partial_pressure_of_co2 = paco21,\n         blood_ph = ph1,\n         creatinine = crea1,\n         albumin = alb1,\n         glasgow_coma_scale = scoma1) |&gt;\n  \n  # Subsetting relevant columns\n  subset(select = c(age_category, sex, race, primary_diagnosis, \n                    cancer_status, dnr_status, aps_score, \n                    two_month_survival_probability, activities_of_daily_living, \n                    duke_activity_status_index, temperature, heart_rate, \n                    mean_blood_pressure, respiratory_rate, white_blood_cell_count, \n                    pafi_ratio, partial_pressure_of_co2, blood_ph, creatinine, \n                    albumin, glasgow_coma_scale, \n                    rhc_status, death_status, num_comorbidities)) |&gt;\n  \n  # Subsetting complete case observations\n  na.omit()\n\n# Save the transformed dataset\nsaveRDS(rhc_data, \"data/confounding/rhc_data.rds\")\nWe begin by importing the RHC data, which has already been prepared for this assignment. What is different in this exercise is that we are changing the reference levels of RHC and DNR variables to No RHC and No, respectively.\n# Loading the RHC data\nrhc_data &lt;- readRDS(\"data/confounding/rhc_data.rds\")\n# Convert 'rhc_status' to binary (1 for 'No RHC', 0 for 'RHC')\nrhc_data$no_rhc_status &lt;- ifelse(rhc_data$rhc_status == \"No RHC\", 1, 0)\n# Convert 'dnr_status' to binary (1 for 'No', 0 for 'Yes')\nrhc_data$no_dnr_status &lt;- ifelse(rhc_data$dnr_status == \"No\", 1, 0)\nThe variables included within this data extract are:\nstr(rhc_data)\n#&gt; 'data.frame':    1439 obs. of  26 variables:\n#&gt;  $ age_category                  : Factor w/ 5 levels \"[0,50)\",\"[50,60)\",..: 4 5 1 1 1 3 1 4 3 2 ...\n#&gt;  $ sex                           : Factor w/ 2 levels \"Male\",\"Female\": 1 2 2 1 1 1 2 1 2 1 ...\n#&gt;  $ race                          : Factor w/ 3 levels \"White\",\"Black\",..: 1 1 1 2 3 1 2 1 1 1 ...\n#&gt;  $ primary_diagnosis             : Factor w/ 4 levels \"ARF\",\"CHF\",\"MOSF\",..: 4 4 3 3 2 2 1 2 3 1 ...\n#&gt;  $ cancer_status                 : Factor w/ 3 levels \"None\",\"Localized (Yes)\",..: 2 1 1 1 1 1 1 1 1 2 ...\n#&gt;  $ dnr_status                    : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ aps_score                     : int  46 38 55 60 30 26 46 35 48 103 ...\n#&gt;  $ two_month_survival_probability: num  0.641 0.665 0.672 0.777 0.889 ...\n#&gt;  $ activities_of_daily_living    : int  0 0 0 2 0 2 0 5 0 3 ...\n#&gt;  $ duke_activity_status_index    : num  23.5 17.5 13.5 21 22 13 29 11 17 12 ...\n#&gt;  $ temperature                   : num  38.7 39.2 35.7 39 36.9 ...\n#&gt;  $ heart_rate                    : int  124 134 110 130 122 115 104 54 65 170 ...\n#&gt;  $ mean_blood_pressure           : num  41 115 77 53 47 63 144 77 41 35 ...\n#&gt;  $ respiratory_rate              : num  10 36 40 12 20 22 8 42 20 42 ...\n#&gt;  $ white_blood_cell_count        : num  22.1 18 7.3 5.4 10.1 ...\n#&gt;  $ pafi_ratio                    : num  68 184 171 390 333 ...\n#&gt;  $ partial_pressure_of_co2       : num  40 68 25 31 40 40 39 42 40 35 ...\n#&gt;  $ blood_ph                      : num  7.36 7.3 7.47 7.32 7.4 ...\n#&gt;  $ creatinine                    : num  1.2 1.4 1.9 15 1.3 ...\n#&gt;  $ albumin                       : num  3.5 3.1 3.5 3.4 4.5 ...\n#&gt;  $ glasgow_coma_scale            : int  0 0 37 9 0 0 0 0 0 9 ...\n#&gt;  $ rhc_status                    : Factor w/ 2 levels \"No RHC\",\"RHC\": 1 1 1 1 2 1 2 1 2 1 ...\n#&gt;  $ death_status                  : logi  FALSE FALSE TRUE FALSE TRUE TRUE ...\n#&gt;  $ num_comorbidities             : num  2 2 1 1 1 2 0 2 2 2 ...\n#&gt;  $ no_rhc_status                 : num  1 1 1 1 0 1 0 1 0 1 ...\n#&gt;  $ no_dnr_status                 : num  1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  - attr(*, \"na.action\")= 'omit' Named int [1:4296] 2 3 4 5 7 8 9 10 11 12 ...\n#&gt;   ..- attr(*, \"names\")= chr [1:4296] \"2\" \"3\" \"4\" \"5\" ...\nFor information regarding the variables included in the data, review the variable descriptions here and here.\nThe Duke Activity Status Index (DASI) is a self-administered questionnaire that measures a patient’s functional capacity or physical activity level. It is commonly used in cardiology to assess a patient’s ability to perform daily activities and is predictive of cardiovascular outcomes. The DASI score ranges from 0 to 58.2, with higher scores indicating better physical functioning and higher activity levels. The DASI score in this dataset ranges from 11 to 33. The median value of 19 and the mean value of 20.36 suggest that the typical person in this dataset has a moderate activity level. In this exercise, we will only work with the subset of the data where the DASI score is greater than 20.\n# Subset the data\nhigh_dasi_data &lt;- rhc_data[rhc_data$duke_activity_status_index &gt; 20, ]",
    "crumbs": [
      "Causal roles",
      "Exercise 2 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingE2solution.html#about-the-assignment",
    "href": "confoundingE2solution.html#about-the-assignment",
    "title": "Exercise 2 Solution (R)",
    "section": "About the assignment",
    "text": "About the assignment\nEffect Modification refers to how the effect of an exposure on an outcome differs across levels of a third variable (the effect modifier). It focuses on how the association between the exposure and the outcome changes based on this modifying factor. This lab adapts exercises from the following tutorial on “Interaction in Epidemiology”.\nFor Problem Set #1, we will explore in the high_dasi_data dataset whether the association between right-heart-catheterization (no_rhc_status; treatment or exposure) on mortality (death_status; outcome) is modified by whether a participant signed a do-not-resuscitate order (no_dnr_status; potential effect modifier).\nNote that we will assume that confounders for RHC and Mortality are the following:\n\nAge Category (age_category)\nAPS Score (aps_score)\nCancer Status (cancer_status)\nNumber of Comorbidities (num_comorbidities)\nGlasgow Coma Scale (glasgow_coma_scale)\nHeart Rate (heart_rate)\nMean Blood Pressure (mean_blood_pressure)\nPrimary Diagnosis (primary_diagnosis)\nSex (sex)\nRespiratory Rate (respiratory_rate)\nWhite Blood Cell Count (white_blood_cell_count)\n\nInteraction, particularly in the statistical context, often refers to a departure from multiplicativity when using models like logistic regression. It examines how the combined effect of two exposures differs from what would be expected if their effects were simply multiplied. Interaction is typically tested on a multiplicative scale (e.g., odds ratios) and is interpreted through synergy (super-multiplicative) or antagonism (sub-multiplicative).\nFor Problem Set #2, we will explore in the high_dasi_data dataset whether binary DNR status (“Do Not Resuscitate”: no_dnr_status) interacts with right-heart-catheterization (no_rhc_status) in its association with mortality (death_status).\nNote that we will assume that confounders for DNR Status and Mortality are the following:\n\nAge Category (age_category)\nAPS Score (aps_score)\nCancer Status (cancer_status)\nNumber of Comorbidities (num_comorbidities)\nGlasgow Coma Scale (glasgow_coma_scale)\nHeart Rate (heart_rate)\nMean Blood Pressure (mean_blood_pressure)\nPrimary Diagnosis (primary_diagnosis)\nWhite Blood Cell Count (white_blood_cell_count)\nActivities of Daily Living (activities_of_daily_living)",
    "crumbs": [
      "Causal roles",
      "Exercise 2 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingE2solution.html#problem-1-exploring-effect-modification",
    "href": "confoundingE2solution.html#problem-1-exploring-effect-modification",
    "title": "Exercise 2 Solution (R)",
    "section": "Problem #1: Exploring Effect Modification",
    "text": "Problem #1: Exploring Effect Modification\n1(a) Fit the effect modification model\nGenerate a multivariable logistic regression model (effect_modification_model), adhering to the following specifications:\n\nSet the outcome as death.\nSet the exposure as no RHC.\nInclude a multiplicative interaction term of the exposure by no DNR status.\nAdjust for the stated confounders in the relationship (no RHC vs death) in the model.\n\n\n# Logistic regression with interaction between no_rhc_status and no_dnr_status in the subset data\neffect_modification_model &lt;- glm(death_status ~ no_rhc_status * no_dnr_status + \n                                  age_category + aps_score + cancer_status +\n                                  num_comorbidities + glasgow_coma_scale + \n                                  heart_rate + mean_blood_pressure + \n                                  primary_diagnosis + sex + \n                                  respiratory_rate + white_blood_cell_count, \n                                data = high_dasi_data, \n                                family = binomial)\nsummary(effect_modification_model)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death_status ~ no_rhc_status * no_dnr_status + \n#&gt;     age_category + aps_score + cancer_status + num_comorbidities + \n#&gt;     glasgow_coma_scale + heart_rate + mean_blood_pressure + primary_diagnosis + \n#&gt;     sex + respiratory_rate + white_blood_cell_count, family = binomial, \n#&gt;     data = high_dasi_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                                Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)                  -2.3712376  1.4019207  -1.691 0.090756 .  \n#&gt; no_rhc_status                 3.4969691  1.6575046   2.110 0.034877 *  \n#&gt; no_dnr_status                 0.6228925  1.3008762   0.479 0.632063    \n#&gt; age_category[50,60)           0.5615264  0.3007867   1.867 0.061921 .  \n#&gt; age_category[60,70)           1.0150295  0.2615213   3.881 0.000104 ***\n#&gt; age_category[70,80)           0.6505600  0.2780222   2.340 0.019286 *  \n#&gt; age_category[80,Inf]          0.8609830  0.3498095   2.461 0.013844 *  \n#&gt; aps_score                    -0.0032209  0.0070546  -0.457 0.647977    \n#&gt; cancer_statusLocalized (Yes)  0.6516821  0.2753557   2.367 0.017948 *  \n#&gt; cancer_statusMetastatic       1.8068081  0.4031984   4.481 7.42e-06 ***\n#&gt; num_comorbidities             0.2526729  0.0888800   2.843 0.004471 ** \n#&gt; glasgow_coma_scale           -0.0006699  0.0053385  -0.125 0.900136    \n#&gt; heart_rate                   -0.0030546  0.0026816  -1.139 0.254659    \n#&gt; mean_blood_pressure          -0.0049564  0.0027455  -1.805 0.071034 .  \n#&gt; primary_diagnosisCHF          0.7685691  0.3003703   2.559 0.010505 *  \n#&gt; primary_diagnosisMOSF         0.4425051  0.2653825   1.667 0.095430 .  \n#&gt; primary_diagnosisOther        0.6229054  0.2796315   2.228 0.025908 *  \n#&gt; sexFemale                    -0.1207658  0.1982125  -0.609 0.542343    \n#&gt; respiratory_rate              0.0119828  0.0085488   1.402 0.161005    \n#&gt; white_blood_cell_count        0.0153771  0.0081258   1.892 0.058442 .  \n#&gt; no_rhc_status:no_dnr_status  -3.2992008  1.6709649  -1.974 0.048333 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 839.22  on 622  degrees of freedom\n#&gt; Residual deviance: 712.15  on 602  degrees of freedom\n#&gt; AIC: 754.15\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\n1(b) Calculate Relative Excess Risk due to Interaction (RERI)\nEffect modification looks at how the effect of one exposure on an outcome changes based on the level of another exposure or factor (the effect modifier). RERI is useful in this context because it quantifies the additive interaction (whether the combined effect is larger or smaller than expected based on individual effects).\n\n# Extract the coefficients\nbeta_rhc &lt;- as.numeric(effect_modification_model$coefficients['no_rhc_status'])   # Coefficient for no_rhc_status\nbeta_dnr &lt;- as.numeric(effect_modification_model$coefficients['no_dnr_status'])   # Coefficient for no_dnr_status\nbeta_interaction &lt;- as.numeric(effect_modification_model$coefficients['no_rhc_status:no_dnr_status'])  # Interaction term\n\n# Calculate the Odds Ratios\nOR_rhc_only &lt;- exp(beta_rhc)         # OR when no_rhc_status = 1, no_dnr_status = 0\nOR_dnr_only &lt;- exp(beta_dnr)         # OR when no_rhc_status = 0, no_dnr_status = 1\nOR_both &lt;- exp(beta_rhc + beta_dnr + beta_interaction)  \n# OR when both no_rhc_status and no_dnr_status are 1\n\n# Calculate RERI (Relative Excess Risk due to Interaction)\nRERI &lt;- OR_both - OR_rhc_only - OR_dnr_only + 1\n\n# Output the results\ncat(\"OR for no RHC only (no_rhc_status = 1, no_dnr_status = 0):\", OR_rhc_only, \"\\n\")\n#&gt; OR for no RHC only (no_rhc_status = 1, no_dnr_status = 0): 33.01524\ncat(\"OR for no DNR only (no_rhc_status = 0, no_dnr_status = 1):\", OR_dnr_only, \"\\n\")\n#&gt; OR for no DNR only (no_rhc_status = 0, no_dnr_status = 1): 1.864313\ncat(\"OR for both no RHC and no DNR (no_rhc_status = 1, no_dnr_status = 1):\", OR_both, \"\\n\")\n#&gt; OR for both no RHC and no DNR (no_rhc_status = 1, no_dnr_status = 1): 2.272001\ncat(\"Relative Excess Risk due to Interaction (RERI):\", RERI, \"\\n\")\n#&gt; Relative Excess Risk due to Interaction (RERI): -31.60755\n\nInterpretation of RERI in Effect Modification:\n\n\nRERI &gt; 0: Indicates a positive additive interaction, meaning the combined effect of two exposures (e.g., no RHC and no DNR) is greater than the sum of their individual effects. In other words, there is an excess risk attributable to the interaction between the two exposures.\n\nRERI = 0: Indicates no additive interaction. The combined effect of the two exposures is equal to the sum of their individual effects. There is no additional risk due to their interaction.\n\nRERI &lt; 0: Indicates a negative additive interaction (also known as antagonism), meaning the combined effect of the two exposures is less than the sum of their individual effects. This suggests that the exposures may be protective when combined, compared to when considered separately.\n1(c) Exploring Effect Modification using interactionR [15%] and Publish (optional)\nBased on the output of the above regression, implement the following:\n\nUse interactionR::interactionR() to obtain a complete reporting of effect modification estimates.\n\n\n# Load the necessary package\nlibrary(interactionR)\n\n# Obtain the interaction effect modification estimates\ninteraction_result &lt;- interactionR(effect_modification_model, \n                                   exposure_names = c(\"no_rhc_status\", \"no_dnr_status\"), \n                                   ci.type = \"delta\",  # Confidence interval method\n                                   em = TRUE,          # Effect modification estimates\n                                   recode = FALSE)     # Do not recode the exposure variables\n# Display the complete effect modification report\ninteraction_result$dframe\n\n\n  \n\n\n\n\nUse Publish::publish() to obtain stratum-specific estimates of the main effect. NOTE: Only return those quantities requested (i.e., not the full regression table).\n\n\n# Load the necessary package\nlibrary(Publish)\n\n# Recode variables for better interpretation\nhigh_dasi_data$no_dnr_factor &lt;- factor(high_dasi_data$no_dnr_status, levels = c(1, 0), labels = c(\"No\", \"Yes\"))\nhigh_dasi_data$no_dnr_factor &lt;- relevel(high_dasi_data$no_dnr_factor, ref = \"Yes\")\nhigh_dasi_data$no_rhc_factor &lt;- factor(high_dasi_data$no_rhc_status, levels = c(1, 0), labels = c(\"No RHC\", \"RHC\"))\nhigh_dasi_data$no_rhc_factor &lt;- relevel(high_dasi_data$no_rhc_factor, ref = \"RHC\")\n\n# Fit the model with recoded factors\neffect_modification_model2 &lt;- glm(death_status ~ no_rhc_factor * no_dnr_factor + \n                                  age_category + aps_score + cancer_status +\n                                  num_comorbidities + glasgow_coma_scale + \n                                  heart_rate + mean_blood_pressure + \n                                  primary_diagnosis + sex + \n                                  respiratory_rate + white_blood_cell_count, \n                                data = high_dasi_data, \n                                family = binomial)\n\n# Obtain stratum-specific estimates from the model\nstratum_estimates &lt;- publish(effect_modification_model2)\n#&gt;                                          Variable           Units OddsRatio         CI.95     p-value \n#&gt;                                      age_category          [0,50)       Ref                           \n#&gt;                                                           [50,60)      1.75   [0.97;3.16]   0.0619213 \n#&gt;                                                           [60,70)      2.76   [1.65;4.61]   0.0001039 \n#&gt;                                                           [70,80)      1.92   [1.11;3.31]   0.0192860 \n#&gt;                                                          [80,Inf]      2.37   [1.19;4.70]   0.0138438 \n#&gt;                                         aps_score                      1.00   [0.98;1.01]   0.6479771 \n#&gt;                                     cancer_status            None       Ref                           \n#&gt;                                                   Localized (Yes)      1.92   [1.12;3.29]   0.0179479 \n#&gt;                                                        Metastatic      6.09  [2.76;13.42]     &lt; 1e-04 \n#&gt;                                 num_comorbidities                      1.29   [1.08;1.53]   0.0044712 \n#&gt;                                glasgow_coma_scale                      1.00   [0.99;1.01]   0.9001364 \n#&gt;                                        heart_rate                      1.00   [0.99;1.00]   0.2546595 \n#&gt;                               mean_blood_pressure                      1.00   [0.99;1.00]   0.0710337 \n#&gt;                                 primary_diagnosis             ARF       Ref                           \n#&gt;                                                               CHF      2.16   [1.20;3.89]   0.0105053 \n#&gt;                                                              MOSF      1.56   [0.93;2.62]   0.0954302 \n#&gt;                                                             Other      1.86   [1.08;3.23]   0.0259076 \n#&gt;                                               sex            Male       Ref                           \n#&gt;                                                            Female      0.89   [0.60;1.31]   0.5423427 \n#&gt;                                  respiratory_rate                      1.01   [1.00;1.03]   0.1610054 \n#&gt;                            white_blood_cell_count                      1.02   [1.00;1.03]   0.0584416 \n#&gt;      no_rhc_factor(RHC): no_dnr_factor(No vs Yes)                      1.86  [0.15;23.87]   0.6320628 \n#&gt;   no_rhc_factor(No RHC): no_dnr_factor(No vs Yes)                      0.07   [0.01;0.55]   0.0113453 \n#&gt;  no_dnr_factor(Yes): no_rhc_factor(No RHC vs RHC)                     33.02 [1.28;850.32]   0.0348774 \n#&gt;   no_dnr_factor(No): no_rhc_factor(No RHC vs RHC)                      1.22   [0.79;1.87]   0.3682362\n\n# Print only the relevant stratum-specific estimates (for no_rhc_status and no_dnr_status)\ntail(stratum_estimates$rawTable,2)\n\n\n  \n\n\n\nNote: Effect modification occurs when the effect of an exposure on an outcome differs across levels of a third variable (the effect modifier). It does not require the interaction term (no_rhc_status:no_dnr_status) to be statistically significant; rather, we are looking at whether the association between the primary exposure (RHC) and the outcome (death) changes depending on the level of the effect modifier (DNR).",
    "crumbs": [
      "Causal roles",
      "Exercise 2 Solution (R)"
    ]
  },
  {
    "objectID": "confoundingE2solution.html#problem-2-exploring-interaction",
    "href": "confoundingE2solution.html#problem-2-exploring-interaction",
    "title": "Exercise 2 Solution (R)",
    "section": "Problem #2: Exploring Interaction",
    "text": "Problem #2: Exploring Interaction\n2(a) Interaction Model\nGenerate a multivariable logistic regression model, adhering to the following specifications:\n\nSet the outcome as death.\nSet two exposures as RHC and DNR status.\nInclude a multiplicative interaction term of the exposures.\nAdjust for the stated confounders in the relationship (no RHC vs death, as well as no DNR vs death) in the model. That means the union of the two confounder sets.\n\n\n# Logistic regression with interaction between no_rhc_status and no_dnr_status in the subset data\ninteraction_model &lt;- glm(death_status ~ no_rhc_status * no_dnr_status + \n                         age_category + aps_score + cancer_status +\n                         num_comorbidities + glasgow_coma_scale + \n                         heart_rate + mean_blood_pressure + \n                         primary_diagnosis + sex + \n                         respiratory_rate + white_blood_cell_count + \n                         activities_of_daily_living, \n                       data = high_dasi_data, \n                       family = binomial)\nsummary(interaction_model)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death_status ~ no_rhc_status * no_dnr_status + \n#&gt;     age_category + aps_score + cancer_status + num_comorbidities + \n#&gt;     glasgow_coma_scale + heart_rate + mean_blood_pressure + primary_diagnosis + \n#&gt;     sex + respiratory_rate + white_blood_cell_count + activities_of_daily_living, \n#&gt;     family = binomial, data = high_dasi_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                                Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)                  -2.2800128  1.4080440  -1.619 0.105388    \n#&gt; no_rhc_status                 3.3958648  1.6658528   2.039 0.041499 *  \n#&gt; no_dnr_status                 0.5238144  1.3076850   0.401 0.688740    \n#&gt; age_category[50,60)           0.5760202  0.3013259   1.912 0.055925 .  \n#&gt; age_category[60,70)           1.0173738  0.2632735   3.864 0.000111 ***\n#&gt; age_category[70,80)           0.6498421  0.2793077   2.327 0.019986 *  \n#&gt; age_category[80,Inf]          0.8040125  0.3523068   2.282 0.022481 *  \n#&gt; aps_score                    -0.0037222  0.0070865  -0.525 0.599411    \n#&gt; cancer_statusLocalized (Yes)  0.6443974  0.2770866   2.326 0.020039 *  \n#&gt; cancer_statusMetastatic       1.8888507  0.4064679   4.647 3.37e-06 ***\n#&gt; num_comorbidities             0.2408454  0.0895108   2.691 0.007131 ** \n#&gt; glasgow_coma_scale           -0.0008445  0.0053673  -0.157 0.874971    \n#&gt; heart_rate                   -0.0032735  0.0026971  -1.214 0.224856    \n#&gt; mean_blood_pressure          -0.0050871  0.0027627  -1.841 0.065575 .  \n#&gt; primary_diagnosisCHF          0.7932514  0.3007341   2.638 0.008347 ** \n#&gt; primary_diagnosisMOSF         0.3831509  0.2685658   1.427 0.153679    \n#&gt; primary_diagnosisOther        0.5884193  0.2812664   2.092 0.036435 *  \n#&gt; sexFemale                    -0.1391314  0.1993885  -0.698 0.485308    \n#&gt; respiratory_rate              0.0123729  0.0086003   1.439 0.150246    \n#&gt; white_blood_cell_count        0.0159359  0.0081423   1.957 0.050326 .  \n#&gt; activities_of_daily_living    0.2249761  0.1006831   2.234 0.025450 *  \n#&gt; no_rhc_status:no_dnr_status  -3.2074335  1.6793158  -1.910 0.056138 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 839.22  on 622  degrees of freedom\n#&gt; Residual deviance: 707.11  on 601  degrees of freedom\n#&gt; AIC: 751.11\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\nNote: Check if the interaction term (no_rhc_status:no_dnr_status) has a low p-value. If this p-value is below the acceptable cutpoint (commonly used significance threshold of 0.05 or something reasonable/justifiable), we could conclude that the interaction between RHC and DNR status on the outcome (death) is statistically significant. This could suggest that the association between RHC status and death depends on DNR status.\n2(b) Exploring Interaction using interactionR\n\nBased on the output of the above regression, implement the following:\n\nUse interactionR::interactionR() to obtain a complete reporting of interaction estimates.\n\n\n# Load the necessary package\nlibrary(interactionR)\n\n# Obtain the interaction estimates\ninteraction_result &lt;- interactionR(interaction_model, \n                                   exposure_names = c(\"no_rhc_status\", \"no_dnr_status\"), \n                                   ci.type = \"delta\",  # Confidence interval method\n                                   em = FALSE,         # Interaction estimates\n                                   recode = FALSE)     # Do not recode the exposure variables\n# Display the complete interaction report\ninteraction_result$dframe\n\n\n  \n\n\n\n2(c) Calculate RERI, AP, and SI\nBased on the output of the above regression, calculate RERI, the attributable proportion due to interaction (AP), and the synergy index (SI).\n\n# Extract the coefficients\nbeta_rhc &lt;- coef(interaction_model)[\"no_rhc_status\"]\nbeta_dnr &lt;- coef(interaction_model)[\"no_dnr_status\"]\nbeta_interaction &lt;- coef(interaction_model)[\"no_rhc_status:no_dnr_status\"]\n\n# Calculate the odds ratios\nOR_rhc_only &lt;- exp(beta_rhc)  # OR when no_rhc_status = 1, no_dnr_status = 0\nOR_dnr_only &lt;- exp(beta_dnr)  # OR when no_rhc_status = 0, no_dnr_status = 1\nOR_both &lt;- exp(beta_rhc + beta_dnr + beta_interaction)  \n# OR when both no_rhc_status and no_dnr_status are 1\n\n# Calculate RERI\nRERI &lt;- OR_both - OR_rhc_only - OR_dnr_only + 1\n\n# Calculate AP\nAP &lt;- RERI / OR_both\n\n# Calculate SI\nSI &lt;- (OR_both - 1) / ((OR_rhc_only - 1) + (OR_dnr_only - 1))\n\n# Output the results\ncat(\"Relative Excess Risk due to Interaction (RERI):\", RERI, \"\\n\")\n#&gt; Relative Excess Risk due to Interaction (RERI): -28.49034\ncat(\"Attributable Proportion (AP):\", AP, \"\\n\")\n#&gt; Attributable Proportion (AP): -13.97569\ncat(\"Synergy Index (SI):\", SI, \"\\n\")\n#&gt; Synergy Index (SI): 0.03517111\n\nInterpretation of Results:\n\n\nRERI: A positive RERI indicates an excess risk due to the interaction between RHC and DNR. A negative RERI suggests antagonism.\n\nAP: Represents the proportion of the risk due to the interaction.\n\nAP&gt;0: A positive proportion of the risk is due to the interaction.\nAP=0: No additional risk is due to the interaction.\nAP&lt;0: A negative interaction, indicating antagonism between the two exposures.\n\n\n\nSI: A synergy index greater than 1 indicates a synergistic effect, while an SI less than 1 indicates antagonism.\n\nSI&gt;1: Synergistic effect. The combined effect of the two exposures is greater than expected based on multiplicative effects.\nSI=1: No multiplicative interaction.\nSI&lt;1: Antagonism. The combined effect of the two exposures is less than expected based on multiplicative effects.\n\n\n\nNote: If the p-values for the interaction term, RERI, AP, and SI are all greater than a reasonable cut-point (e.g., 0.05 is one example), that indicates no statistically significant interaction. In that case, this would mean we do not have strong evidence to claim that the combined effects of RHC and DNR on mortality are significantly different from their individual effects.",
    "crumbs": [
      "Causal roles",
      "Exercise 2 Solution (R)"
    ]
  },
  {
    "objectID": "predictivefactors.html",
    "href": "predictivefactors.html",
    "title": "Prediction ideas",
    "section": "",
    "text": "Background\nThe chapter provides a comprehensive guide to prediction modeling for cholesterol levels, focusing on the challenges and solutions involved in building robust prediction models. It begins by addressing the issue of collinearity among predictors and progresses to cover the intricacies of modeling both continuous and binary outcomes. Special attention is given to diagnosing and preventing model overfitting through various techniques such as data splitting and cross-validation. Advanced topics in model validation like bootstrapping are also explored. The overarching theme is to equip data analysts with the tools and methods needed to build, assess, and improve predictive models while addressing common challenges like collinearity and overfitting.",
    "crumbs": [
      "Prediction ideas"
    ]
  },
  {
    "objectID": "predictivefactors.html#background",
    "href": "predictivefactors.html#background",
    "title": "Prediction ideas",
    "section": "",
    "text": "As we’ve journeyed through the previous chapters, we’ve gained a comprehensive understanding of various research questions, particularly distinguishing between causal and predictive inquiries. While the prior chapter delved into the intricacies of causal questions and the challenges they present, this chapter shifts the spotlight to the realm of prediction. Predictive questions have their own set of complexities and methodologies, distinct from those of causal inquiries. Here, we’ll explore the art and science of making accurate predictions, understanding the factors that influence them, and the tools and techniques best suited for predictive analysis.\nFurthermore, this chapter serves as a precursor to our upcoming exploration of machine learning. While prediction provides the foundation, machine learning offers advanced tools and algorithms to refine and enhance our predictive capabilities. By building on the foundational knowledge from the preceding chapters and setting the stage for the machine learning chapter, we aim to provide a holistic view of how prediction and machine learning intertwine in the broader landscape of research inquiry.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Prediction ideas"
    ]
  },
  {
    "objectID": "predictivefactors.html#overview-of-tutorials",
    "href": "predictivefactors.html#overview-of-tutorials",
    "title": "Prediction ideas",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nIdentify collinear predictors\nThis tutorial focuses on identifying collinear predictors in a dataset related to cholesterol levels from the NHANES 2015 collection. The tutorial guides you through summarizing its structure, and applying methods for variable clustering to detect collinear predictors. The tutorial is practical for data analysts aiming to improve model accuracy by identifying and addressing redundant variables.\n\n\nExplore relationships for continuous outcome variable\nThis comprehensive tutorial walks you through the process of analyzing a dataset on cholesterol levels, focusing on exploring relationships for a continuous outcome variable. It starts by generating a correlation plot. Multiple methods for examining descriptive associations are provided, including stratification by key predictors. The tutorial also covers linear regression modeling, diagnosing data issues like outliers and leverage, and refitting the model after cleaning the data. Additionally, the tutorial delves into more complex modeling techniques like polynomial regression and multiple covariates, and addresses issues of collinearity using Variance Inflation Factors (VIF).\n\n\nExplore relationships for binary outcome variable\nA binary outcome variable is created to classify cholesterol levels as ‘healthy’ or ‘unhealthy’. This transformed variable is then modeled using logistic regression. Various predictors including demographic variables, vital statistics, and other health parameters are considered in the model. The performance of the model is evaluated using Variance Inflation Factor (VIF) for multicollinearity and Area Under the Curve (AUC) for classification accuracy. Two models are fitted, and their respective AUCs are calculated to assess the predictive power.\n\n\nOverfitting and performance\nThe tutorial focus is on addressing overfitting and assessing model performance. A linear regression model is fitted using a comprehensive set of predictors. Various statistical metrics such as the design matrix dimensions, Sum of Squares for Error (SSE), Total Sum of Squares (SST), R-squared (R2), Root Mean Square Error (RMSE), and Adjusted R2 are calculated to evaluate the model’s predictive power and fit. Functions are also created to streamline the calculation of these metrics, allowing for more dynamic and customizable performance assessment. One such function, perform, encapsulates the entire process, outputting key performance indicators including R2, adjusted R2, and RMSE, and it can be applied to new data sets for validation.\n\n\nData spliting\nThe tutorial focuses on splitting data into training and testing sets to prevent model overfitting. We allocate approximately 70% of the data to the training set and the remaining 30% to the test set. The linear regression model is then fitted using the training data. Performance metrics are extracted using the previously defined perform function, which is applied not only to the training and test sets but also to the entire dataset for comprehensive performance evaluation. This data splitting approach allows for more robust model validation by assessing how well the model generalizes to unseen data.\n\n\nCross-vaildation\nThe tutorial outlines the process of implementing k-fold cross-validation to validate a linear regression model’s performance, aiming to predict cholesterol levels. The dataset is divided into 5 folds, by turn used as training (to fit the model), and test sets (used for prediction and performance evaluation). Performance metrics such as R-squared are calculated for each fold. The process can also be automated , which helps in fitting the model across all folds and summarizing the results, including calculating the mean and standard deviation of the R-squared values to understand the model’s consistency and reliability.\n\n\nBootstrap\nThe tutorial outlines methods for implementing various bootstrapping techniques in statistical analysis. It demonstrates resampling methods using vectors and matrices. The idea of bootstrapping is emphasized as a useful technique for estimating the standard deviation (SD) of a statistic (e.g., mean), when the distribution of the data is unknown. This SD is then used to calculate confidence intervals. Different variations of bootstrap methods, such as “boot,” “boot632,” and “Optimism corrected bootstrap,” are demonstrated for linear regression and logistic regression models. They are used to obtain performance metrics like R-squared for regression models and the Receiver Operating Characteristic (ROC) curve for classification models. The tutorial also includes an example of calculating the Brier Score. The examples aim to offer various strategies for model evaluation, from the basics of resampling a vector to applying complex methods like ‘Optimism corrected bootstrap’ on real-world data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Prediction ideas"
    ]
  },
  {
    "objectID": "predictivefactors0.html",
    "href": "predictivefactors0.html",
    "title": "Concepts (P)",
    "section": "",
    "text": "Research goals\nEpidemiologists identify four types of inferential goals in research:",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#research-goals",
    "href": "predictivefactors0.html#research-goals",
    "title": "Concepts (P)",
    "section": "",
    "text": "Prediction: Establishing models to predict future outcomes based on current or past information.\nEvaluating an exposure of primary interest: Focusing on assessing the impact or significance of a specific exposure variable within a model.\nIdentifying the important independent predictors of an outcome: Determining which variables most significantly affect the outcome and understanding the strength and nature of these relationships.\nDescriptive: A possible emphasis on describing the data and relationships within it.",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#reading-list",
    "href": "predictivefactors0.html#reading-list",
    "title": "Concepts (P)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Vittinghoff et al. 2011)\nOptional reading:\n\n(Greenland and Pearce 2015)\n(Kuhn and Johnson 2013) (chapter 4)\n\nExercise:\nWhich type of goal does this article have?\n\n(Williamson et al. 2020)",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#video-lessons",
    "href": "predictivefactors0.html#video-lessons",
    "title": "Concepts (P)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nInferential goals in an epidemiological study\n\n\n\nPrediction, causal, important predictors, descriptive\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 1: Prediction model\n\n\n\ndiscrimination, calibration, overfitting, validation, model selection\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 2: Causal exploration\n\n\n\noutcome vs. exposure of primary interest\n\n\n\n\n\n\n\n\n\n\n\n\nGoal 3: Outcome vs. multiple exposures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering and scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing reference level\n\n\n\nCriteria to determine the appropriate reference level for a categorical covariate:\n\nIf the covariate possesses at least an ordinal nature and serves primarily as an adjustment variable, it is advisable to select either the lowest or highest category as the reference level. This choice can be particularly useful in uncovering potential dose-response relationships within the data.\nConsider the specific aspect you wish to emphasize in your interpretation. For instance, if you aim to shed light on the concept of an unhealthy diet within the context, opting for the “healthy” category as the reference level can align with the typical causal motivation behind the choice.\nIn general, it is advisable to designate the category with the highest frequency as the reference level. This selection carries a statistical advantage, especially when dealing with imbalanced categories. Avoid choosing a low-frequency category as the reference level, as regression estimates may become highly unstable under such circumstances.",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#video-lesson-slides",
    "href": "predictivefactors0.html#video-lesson-slides",
    "title": "Concepts (P)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#links",
    "href": "predictivefactors0.html#links",
    "title": "Concepts (P)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors0.html#references",
    "href": "predictivefactors0.html#references",
    "title": "Concepts (P)",
    "section": "References",
    "text": "References\n\n\n\n\nGreenland, Sander, and Neil Pearce. 2015. “Statistical Foundations for Model-Based Adjustments.” Annual Review of Public Health 36: 89–108. https://doi.org/10.1146/annurev-publhealth-031914-122531.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Vol. 26. Springer.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2011. “Predictor Selection.” In Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Springer.\n\n\nWilliamson, Elizabeth J, Alex J Walker, Krishnan Bhaskaran, Seb Bacon, Chris Bates, Caroline E Morton, Helen J Curtis, et al. 2020. “Factors Associated with COVID-19-Related Death Using OpenSAFELY.” Nature 584 (7821): 430–36.",
    "crumbs": [
      "Prediction ideas",
      "Concepts (P)"
    ]
  },
  {
    "objectID": "predictivefactors1.html",
    "href": "predictivefactors1.html",
    "title": "Collinear predictors",
    "section": "",
    "text": "In this tutorial, we’ll continue with the data analysis. We’ll focus on an analysis of an NHANES data. This data contains over 1200 observations and 33 variables. These variables come in various types: numeric, categorical, and binary. Our primary goal is to fit a linear regression model to predict cholesterol levels.\n\n# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\n\nLoad data\nLet us load the dataset and see structure of the variables:\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n#head(analytic)\nstr(analytic)\n#&gt; 'data.frame':    1267 obs. of  33 variables:\n#&gt;  $ ID                   : num  83732 83733 83741 83747 83750 ...\n#&gt;  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n#&gt;  $ age                  : num  62 53 22 46 45 30 60 69 24 70 ...\n#&gt;  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Others\" ...\n#&gt;  $ race                 : chr  \"White\" \"White\" \"Black\" \"White\" ...\n#&gt;  $ education            : chr  \"College\" \"High.School\" \"College\" \"College\" ...\n#&gt;  $ married              : chr  \"Married\" \"Previously.married\" \"Never.married\" \"Married\" ...\n#&gt;  $ income               : chr  \"Between.55kto99k\" \"&lt;25k\" \"Between.25kto54k\" \"&lt;25k\" ...\n#&gt;  $ weight               : num  135630 25282 39353 35674 97002 ...\n#&gt;  $ psu                  : num  1 1 2 1 1 1 1 2 1 2 ...\n#&gt;  $ strata               : num  125 125 128 121 125 124 128 120 130 132 ...\n#&gt;  $ diastolicBP          : num  70 88 70 94 70 50 74 70 72 54 ...\n#&gt;  $ systolicBP           : num  128 146 110 144 116 104 142 146 126 144 ...\n#&gt;  $ bodyweight           : num  94.8 90.4 76.6 86.2 76.2 71.2 75.6 84 89.2 81.7 ...\n#&gt;  $ bodyheight           : num  184 171 165 177 178 ...\n#&gt;  $ bmi                  : num  27.8 30.8 28 27.6 24.1 26.6 35.9 31 26.9 27 ...\n#&gt;  $ waist                : num  101.1 107.9 86.6 104.3 90.1 ...\n#&gt;  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Some.days\" \"Every.day\" ...\n#&gt;  $ alcohol              : num  1 6 8 1 3 2 1 1 2 2 ...\n#&gt;  $ cholesterol          : num  173 265 164 242 181 184 205 287 126 192 ...\n#&gt;  $ cholesterolM2        : num  4.47 6.85 4.24 6.26 4.68 4.76 5.3 7.42 3.26 4.97 ...\n#&gt;  $ triglycerides        : num  158 170 77 497 63 62 169 245 95 64 ...\n#&gt;  $ uric.acid            : num  4.2 7 6 6.5 5.4 5.5 5.1 4.3 7.6 7.1 ...\n#&gt;  $ protein              : num  7.5 7.4 7.4 6.8 7.4 6.7 7.4 6.8 7.3 7.2 ...\n#&gt;  $ bilirubin            : num  0.5 0.6 0.2 0.5 0.7 0.8 0.4 0.6 1.2 1.2 ...\n#&gt;  $ phosphorus           : num  4.7 4.4 5.3 3.6 3.9 3.4 3.9 4.4 3.2 3 ...\n#&gt;  $ sodium               : num  136 140 139 138 138 136 139 140 140 139 ...\n#&gt;  $ potassium            : num  4.3 4.55 4.16 4.27 3.91 3.97 3.99 4.25 3.8 4.63 ...\n#&gt;  $ globulin             : num  2.9 2.9 3 2.6 2.8 2.5 3.2 2.3 2.7 2.6 ...\n#&gt;  $ calcium              : num  9.8 9.8 9.3 9.3 9.3 9.4 9.6 9.6 9.6 9.6 ...\n#&gt;  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ physical.recreational: chr  \"No\" \"No\" \"Yes\" \"No\" ...\n#&gt;  $ diabetes             : chr  \"Yes\" \"No\" \"No\" \"No\" ...\n#&gt;  - attr(*, \"na.action\")= 'omit' Named int [1:3739] 3 4 5 6 8 9 13 14 15 16 ...\n#&gt;   ..- attr(*, \"names\")= chr [1:3739] \"3\" \"4\" \"5\" \"6\" ...\n\nDescribe the data\n\nrequire(rms)\ndescribe(analytic) \n#&gt; analytic \n#&gt; \n#&gt;  33  Variables      1267  Observations\n#&gt; --------------------------------------------------------------------------------\n#&gt; ID \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0     1267        1    88660    88660     3366    84250 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    84687    86019    88692    91252    92670    93089 \n#&gt; \n#&gt; lowest : 83732 83733 83741 83747 83750, highest: 93617 93633 93643 93659 93685\n#&gt; --------------------------------------------------------------------------------\n#&gt; gender \n#&gt;        n  missing distinct \n#&gt;     1267        0        2 \n#&gt;                         \n#&gt; Value      Female   Male\n#&gt; Frequency     496    771\n#&gt; Proportion  0.391  0.609\n#&gt; --------------------------------------------------------------------------------\n#&gt; age \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       61        1    49.91       50    19.18       24 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;       27       36       51       63       72       78 \n#&gt; \n#&gt; lowest : 20 21 22 23 24, highest: 76 77 78 79 80\n#&gt; --------------------------------------------------------------------------------\n#&gt; born \n#&gt;        n  missing distinct \n#&gt;     1267        0        2 \n#&gt;                                                                             \n#&gt; Value      Born in 50 US states or Washingt                           Others\n#&gt; Frequency                               991                              276\n#&gt; Proportion                            0.782                            0.218\n#&gt; --------------------------------------------------------------------------------\n#&gt; race \n#&gt;        n  missing distinct \n#&gt;     1267        0        4 \n#&gt;                                               \n#&gt; Value         Black Hispanic    Other    White\n#&gt; Frequency       246      337      132      552\n#&gt; Proportion    0.194    0.266    0.104    0.436\n#&gt; --------------------------------------------------------------------------------\n#&gt; education \n#&gt;        n  missing distinct \n#&gt;     1267        0        3 \n#&gt;                                               \n#&gt; Value          College High.School      School\n#&gt; Frequency          648         523          96\n#&gt; Proportion       0.511       0.413       0.076\n#&gt; --------------------------------------------------------------------------------\n#&gt; married \n#&gt;        n  missing distinct \n#&gt;     1267        0        3 \n#&gt;                                                                    \n#&gt; Value                 Married      Never.married Previously.married\n#&gt; Frequency                 751                226                290\n#&gt; Proportion              0.593              0.178              0.229\n#&gt; --------------------------------------------------------------------------------\n#&gt; income \n#&gt;        n  missing distinct \n#&gt;     1267        0        4 \n#&gt;                                                                               \n#&gt; Value                  &lt;25k Between.25kto54k Between.55kto99k         Over100k\n#&gt; Frequency               344              435              297              191\n#&gt; Proportion            0.272            0.343            0.234            0.151\n#&gt; --------------------------------------------------------------------------------\n#&gt; weight \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0     1184        1    48904    38496    44337     9158 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    11549    19540    30335    63822   121803   151546 \n#&gt; \n#&gt; lowest : 5470.04 5948.95 6197.66 6480.95 6703.84\n#&gt; highest: 203563  207197  213611  218139  224892 \n#&gt; --------------------------------------------------------------------------------\n#&gt; psu \n#&gt;        n  missing distinct     Info     Mean \n#&gt;     1267        0        2     0.75    1.493 \n#&gt;                       \n#&gt; Value          1     2\n#&gt; Frequency    642   625\n#&gt; Proportion 0.507 0.493\n#&gt; --------------------------------------------------------------------------------\n#&gt; strata \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       15    0.994    126.3      126    4.792      120 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      121      123      126      130      132      133 \n#&gt;                                                                             \n#&gt; Value        119   120   121   122   123   124   125   126   127   128   129\n#&gt; Frequency     47    74   118    63    77    66   114   104   107    65    53\n#&gt; Proportion 0.037 0.058 0.093 0.050 0.061 0.052 0.090 0.082 0.084 0.051 0.042\n#&gt;                                   \n#&gt; Value        130   131   132   133\n#&gt; Frequency     99   120    95    65\n#&gt; Proportion 0.078 0.095 0.075 0.051\n#&gt; \n#&gt; For the frequency table, variable is rounded to the nearest 0\n#&gt; --------------------------------------------------------------------------------\n#&gt; diastolicBP \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       41    0.997    70.37       70    13.99       52 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;       54       62       70       78       86       92 \n#&gt; \n#&gt; lowest :   0  26  34  38  40, highest: 104 106 108 110 112\n#&gt; --------------------------------------------------------------------------------\n#&gt; systolicBP \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       56    0.998    126.5      125     19.3    102.0 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    106.0    114.0    124.0    136.0    148.8    160.0 \n#&gt; \n#&gt; lowest :  84  88  90  92  94, highest: 194 196 206 218 236\n#&gt; --------------------------------------------------------------------------------\n#&gt; bodyweight \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      615        1    84.95    83.15    23.56    56.29 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    61.10    69.70    81.40    97.00   113.44   127.47 \n#&gt; \n#&gt; lowest : 39.7  39.8  40.7  42.6  42.7 , highest: 161.9 166.3 175.7 175.9 178.4\n#&gt; --------------------------------------------------------------------------------\n#&gt; bodyheight \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      376        1    169.2    169.2    10.66    153.8 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    157.0    162.6    169.3    176.2    181.1    184.2 \n#&gt; \n#&gt; lowest : 143.8 144.2 145.2 145.9 146.2, highest: 194.6 195.1 195.6 198.4 201  \n#&gt; --------------------------------------------------------------------------------\n#&gt; bmi \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      284        1    29.58       29    7.403    20.60 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    22.06    24.80    28.60    33.30    38.24    42.00 \n#&gt; \n#&gt; lowest : 16.3 17.5 17.6 17.7 17.9, highest: 57.2 57.6 59.4 60.7 64.5\n#&gt; --------------------------------------------------------------------------------\n#&gt; waist \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      544        1    101.8      101    18.47     77.1 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;     81.4     90.5    100.3    111.2    122.8    132.5 \n#&gt; \n#&gt; lowest : 65    65.5  66.5  68.2  68.7 , highest: 159.2 159.8 160.2 160.5 161.5\n#&gt; --------------------------------------------------------------------------------\n#&gt; smoke \n#&gt;        n  missing distinct \n#&gt;     1267        0        3 \n#&gt;                                            \n#&gt; Value       Every.day Not.at.all  Some.days\n#&gt; Frequency         448        665        154\n#&gt; Proportion      0.354      0.525      0.122\n#&gt; --------------------------------------------------------------------------------\n#&gt; alcohol \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       14    0.952    3.109      2.5    2.419        1 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;        1        1        2        4        6        8 \n#&gt;                                                                             \n#&gt; Value          1     2     3     4     5     6     7     8     9    10    11\n#&gt; Frequency    336   371   189   106    79    95    10    26     4    20     1\n#&gt; Proportion 0.265 0.293 0.149 0.084 0.062 0.075 0.008 0.021 0.003 0.016 0.001\n#&gt;                             \n#&gt; Value         12    14    15\n#&gt; Frequency     23     1     6\n#&gt; Proportion 0.018 0.001 0.005\n#&gt; \n#&gt; For the frequency table, variable is rounded to the nearest 0\n#&gt; --------------------------------------------------------------------------------\n#&gt; cholesterol \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      203        1    193.1      191    47.47    132.0 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    142.0    162.5    191.0    217.0    248.0    268.0 \n#&gt; \n#&gt; lowest :  81  93  97 100 101, highest: 345 348 349 358 545\n#&gt; --------------------------------------------------------------------------------\n#&gt; cholesterolM2 \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      203        1    4.994     4.94    1.228    3.410 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;    3.670    4.205    4.940    5.610    6.410    6.930 \n#&gt; \n#&gt; lowest : 2.09  2.4   2.51  2.59  2.61 , highest: 8.92  9     9.03  9.26  14.09\n#&gt; --------------------------------------------------------------------------------\n#&gt; triglycerides \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      361        1    165.8    141.5    124.1     48.0 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;     59.0     84.0    127.0    201.5    309.0    396.6 \n#&gt; \n#&gt; lowest :   18   21   24   25   31, highest:  964 1020 1157 1253 3061\n#&gt; --------------------------------------------------------------------------------\n#&gt; uric.acid \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       84        1    5.598     5.55    1.626     3.43 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;     3.80     4.60     5.50     6.50     7.40     8.00 \n#&gt; \n#&gt; lowest : 1.6  2.2  2.3  2.4  2.5 , highest: 10.2 10.3 11.7 12.2 18  \n#&gt; --------------------------------------------------------------------------------\n#&gt; protein \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       32    0.995    7.126      7.1   0.5095      6.4 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      6.6      6.8      7.1      7.4      7.7      7.9 \n#&gt; \n#&gt; lowest : 5.7 5.8 5.9 6   6.1, highest: 8.4 8.5 8.6 8.8 9  \n#&gt; --------------------------------------------------------------------------------\n#&gt; bilirubin \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       31    0.984   0.5467      0.5   0.2949      0.2 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      0.2      0.4      0.5      0.7      0.9      1.0 \n#&gt; \n#&gt; lowest : 0    0.01 0.02 0.03 0.04, highest: 1.8  2    2.1  2.6  3.3 \n#&gt; --------------------------------------------------------------------------------\n#&gt; phosphorus \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       37    0.996    3.642     3.65    0.593      2.8 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      3.0      3.3      3.6      4.0      4.3      4.5 \n#&gt; \n#&gt; lowest : 1.8 2   2.2 2.3 2.4, highest: 5.2 5.3 5.4 5.6 6.1\n#&gt; --------------------------------------------------------------------------------\n#&gt; sodium \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       20    0.977    138.5    138.5    2.383      135 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      136      137      139      140      141      142 \n#&gt;                                                                             \n#&gt; Value        124   126   129   130   131   132   133   134   135   136   137\n#&gt; Frequency      1     1     1     1     5     4    11    23    46    93   176\n#&gt; Proportion 0.001 0.001 0.001 0.001 0.004 0.003 0.009 0.018 0.036 0.073 0.139\n#&gt;                                                                 \n#&gt; Value        138   139   140   141   142   143   144   146   148\n#&gt; Frequency    235   260   206   112    55    29     6     1     1\n#&gt; Proportion 0.185 0.205 0.163 0.088 0.043 0.023 0.005 0.001 0.001\n#&gt; \n#&gt; For the frequency table, variable is rounded to the nearest 0\n#&gt; --------------------------------------------------------------------------------\n#&gt; potassium \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0      175        1    3.985     3.98   0.3725     3.45 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;     3.57     3.78     3.98     4.19     4.40     4.54 \n#&gt; \n#&gt; lowest : 2.6  2.92 2.96 3.07 3.09, highest: 5.15 5.21 5.36 5.37 5.51\n#&gt; --------------------------------------------------------------------------------\n#&gt; globulin \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       29    0.994    2.799      2.8   0.4536      2.2 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      2.3      2.5      2.8      3.0      3.3      3.5 \n#&gt; \n#&gt; lowest : 1.6 1.8 1.9 2   2.1, highest: 4.1 4.2 4.3 4.5 5.5\n#&gt; --------------------------------------------------------------------------------\n#&gt; calcium \n#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n#&gt;     1267        0       25    0.991    9.335     9.35   0.3786      8.8 \n#&gt;      .10      .25      .50      .75      .90      .95 \n#&gt;      8.9      9.1      9.3      9.6      9.7      9.9 \n#&gt; \n#&gt; lowest : 8.4  8.5  8.6  8.7  8.8 , highest: 10.4 10.5 10.7 11   11.1\n#&gt; --------------------------------------------------------------------------------\n#&gt; physical.work \n#&gt;        n  missing distinct \n#&gt;     1267        0        2 \n#&gt;                       \n#&gt; Value         No   Yes\n#&gt; Frequency    895   372\n#&gt; Proportion 0.706 0.294\n#&gt; --------------------------------------------------------------------------------\n#&gt; physical.recreational \n#&gt;        n  missing distinct \n#&gt;     1267        0        2 \n#&gt;                       \n#&gt; Value         No   Yes\n#&gt; Frequency   1002   265\n#&gt; Proportion 0.791 0.209\n#&gt; --------------------------------------------------------------------------------\n#&gt; diabetes \n#&gt;        n  missing distinct \n#&gt;     1267        0        2 \n#&gt;                     \n#&gt; Value        No  Yes\n#&gt; Frequency  1064  203\n#&gt; Proportion 0.84 0.16\n#&gt; --------------------------------------------------------------------------------\n\nCollinearity\nAvoiding collinear variables can result in a more interpretable, stable, and efficient predictive model. Collinearity refers to a situation in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with substantial accuracy. Collinearity poses several issues for predictive analysis:\nReduced Interpretability: When predictor variables are highly correlated, it becomes challenging to isolate the impact of individual predictors on the response variable. In other words, it is difficult to determine which predictor is genuinely influential in explaining variance in the response variable. This reduces the interpretability of the model.\nUnstable Coefficients: Collinearity can lead to inflated standard errors of the regression coefficients. This means that the coefficients can be very sensitive to small changes in the data, making the model unstable and less generalizable to new, unseen data.\nOverfitting: When predictors are collinear, the model is more likely to fit to the noise in the data rather than the actual signal. This is a manifestation of overfitting, where the model becomes too complex and captures random noise. Overfitted models will perform poorly on new, unseen data.\nInefficiency: Including redundant variables (collinear variables) does not add additional information to the model. This could be inefficient, especially when dealing with large datasets, as it increases computational costs without improving model performance.\nMulticollinearity Diagnostics\nSeveral techniques are available for diagnosing multicollinearity, including:\n\nVariance Inflation Factor (VIF): The VIF is a measure of multicollinearity, which measures how much the variance of a regression coefficient is increased because of multicollinearity. A general rule of thumb is that if VIF exceeds 4 or 5, multicollinearity is present.\nEigenvalues and Eigenvectors of the correlation/covariance matrix: The eigenvalues of the correlation matrix can also be used to measure the presence of multicollinearity, with one or more eigenvalues close to zero indicating multicollinearity.\nPairwise correlation matrices: Large correlation coefficients in the correlation matrix of predictors indicate the possibility of multicollinearity.\nRemedies\nSome common ways to handle collinearity include:\n\nRemoving one of the correlated predictors\nCombining correlated predictors into a single composite predictor\nUsing regularization techniques like Ridge Regression, which can help handle collinearity by adding a penalty term. Ridge regression adds a small amount of bias to the regression estimates, which in turn reduces the standard error, addresses the collinearity issue to some extent, and makes the results more reliable.\nIdentify collinear predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can also use hclust and varclus or variable clustering, i.e., to identify collinear predictors\n\n\n\n\nhclust is the hierarchical clustering function where default is squared Spearman correlation coefficients to detect monotonic but nonlinear relationships.\n\nrequire(Hmisc)\nsel.names &lt;- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \n               \"married\", \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \"smoke\",\n               \"alcohol\",  \"cholesterol\", \"triglycerides\", \n               \"uric.acid\", \"protein\", \"bilirubin\", \"phosphorus\",\n               \"sodium\", \"potassium\", \"globulin\", \"calcium\", \n               \"physical.work\", \"physical.recreational\", \n               \"diabetes\")\nvar.cluster &lt;- varclus(~., data = analytic[sel.names])\n# var.cluster\nplot(var.cluster)\n\n\n\n\n\n\n\nIn this plot, Spearman correlation or Spearman’s \\(\\rho\\)(rho) assesses the monotonic but nonlinear relationships predictors. We can also specify linear relationships such as Pearson correlation. A high correlation coefficient indicates predictors are highly correlated. For example, Spearman’s \\(\\rho\\) is more than 0.8 for bodyweight, BMI, and waist, indicating high collinearity between these predictors.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Prediction ideas",
      "Collinear predictors"
    ]
  },
  {
    "objectID": "predictivefactors2.html",
    "href": "predictivefactors2.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Let us focus on issues related to predictive modeling for continuous outcomes in 4 steps:\n\nDiagnosis Phase: Identifies outliers, leverage points, and residuals that could affect the model.\nCleaning Phase: Deletes problematic data based on predefined conditions.\nModeling Phase: Various models are fitted including polynomial models and a comprehensive model with multiple predictors.\nColinearity Check: A rule of thumb is used to check for multicollinearity in the comprehensive model, and problematic variables are flagged.\n\nExplore relationships for continuous outcome variable\nFirst, load several R packages for statistical modeling, data manipulation, and visualization.\n\n# Load required packages\nlibrary(rms)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(Publish)\nlibrary(car)\nlibrary(corrplot)\nlibrary(olsrr)\n\nLoad data\nHere, a dataset is loaded into the R environment from an RData file.\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15.RData\")\n\nCorrelation plot\n\n\n\n\n\n\nTip\n\n\n\nWe can use the cor function to see the correlation between numeric variables and then use the corrplot function to plot the cor object. The plot helps in understanding the linear or nonlinear relationships between different numerical variables.\n\n\n\nrequire(corrplot)\nnumeric.names &lt;- c(\"age\", \"diastolicBP\", \"systolicBP\", \"bodyweight\", \n                   \"bodyheight\", \"bmi\", \"waist\", \"alcohol\", \n                   \"cholesterol\", \"triglycerides\", \"uric.acid\", \n                   \"protein\", \"bilirubin\", \"phosphorus\", \"sodium\",\n                   \"potassium\", \"globulin\", \"calcium\")\ncorrelationMatrix &lt;- cor(analytic[numeric.names])\nmat.num &lt;- round(correlationMatrix,2)\nmat.num[mat.num&gt;0.8 & mat.num &lt; 1]\n#&gt; [1] 0.89 0.90 0.89 0.91 0.90 0.91\ncorrplot(correlationMatrix, method=\"number\", type=\"upper\")\n\n\n\n\n\n\n\nAs we can see, the correlation between bmi and waist is 0.91, suggesting a high correlation between these two predictors. Similarly, bodyweight is highly correlated with waist (correlation 0.90), bodyweight is highly correlated with bmi (correlation 0.89). Also, there is no linear relationship between age and bmi (correlation -0.02).\nExamine descriptive associations\nLet us examine the descriptive associations with the dependent variable by stratifying separately by key predictors\n\n\n\n\n\n\nTip\n\n\n\nThere are multiple ways to examine the descriptive associations by strata/groups, e.g., summarize, aggregate, describeBy, tapply, summary\n\n\nThe code calculates and explores various ways to describe the cholesterol levels, stratified by key predictors such as gender.\n\nmean(analytic$cholesterol)\n#&gt; [1] 193.1002\n\n# Process 1\nmean(analytic$cholesterol[analytic$gender == \"Male\"])\n#&gt; [1] 190.7626\nmean(analytic$cholesterol[analytic$gender == \"Female\"])\n#&gt; [1] 196.7339\n\n# Process 2\nlibrary(dplyr)\nanalytic %&gt;%\n  group_by(gender) %&gt;%\n  dplyr::summarize(mean.ch=mean(cholesterol), .groups = 'drop') \n\n\n  \n\n\n\n# process 3\nwith(analytic, aggregate( analytic$cholesterol, by=list(gender), \n                          FUN=summary))\n\n\n  \n\n\n\n# process 4\npsych::describeBy(analytic$cholesterol, analytic$gender)\n#&gt; \n#&gt;  Descriptive statistics by group \n#&gt; group: Female\n#&gt;    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#&gt; X1    1 496 196.73 43.26  194.5  194.44 40.77 100 358   258 0.57     0.56 1.94\n#&gt; ------------------------------------------------------------ \n#&gt; group: Male\n#&gt;    vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\n#&gt; X1    1 771 190.76 43.06    188  188.54 40.03  81 545   464  1.1     5.76 1.55\n\n# process 5\ntapply(analytic$cholesterol, analytic$gender, summary)\n#&gt; $Female\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   100.0   166.8   194.5   196.7   220.2   358.0 \n#&gt; \n#&gt; $Male\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    81.0   161.0   188.0   190.8   215.0   545.0\n\n# A general process\nsel.names &lt;- c(\"gender\", \"age\", \"born\", \"race\", \"education\", \n               \"married\", \"income\", \"diastolicBP\", \"systolicBP\", \n               \"bodyweight\", \"bodyheight\", \"bmi\", \"waist\", \n               \"smoke\", \"alcohol\", \"cholesterol\", \n               \"triglycerides\", \"uric.acid\", \"protein\", \n               \"bilirubin\", \"phosphorus\", \"sodium\", \"potassium\", \n               \"globulin\", \"calcium\", \"physical.work\",\n               \"physical.recreational\", \"diabetes\")\nvar.summ &lt;- summary(cholesterol~ ., data = analytic[sel.names])\nvar.summ\n#&gt; cholesterol      N= 1267  \n#&gt; \n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                     |                                |   N|cholesterol|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |               gender|                          Female| 496|   196.7339|\n#&gt; |                     |                            Male| 771|   190.7626|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                  age|                         [20,37)| 342|   182.4854|\n#&gt; |                     |                         [37,52)| 313|   200.1661|\n#&gt; |                     |                         [52,64)| 315|   199.7873|\n#&gt; |                     |                         [64,80]| 297|   190.7845|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                 born|Born in 50 US states or Washingt| 991|   190.9253|\n#&gt; |                     |                          Others| 276|   200.9094|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                 race|                           Black| 246|   187.3740|\n#&gt; |                     |                        Hispanic| 337|   193.5490|\n#&gt; |                     |                           Other| 132|   191.8561|\n#&gt; |                     |                           White| 552|   195.6757|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |            education|                         College| 648|   192.5478|\n#&gt; |                     |                     High.School| 523|   193.4532|\n#&gt; |                     |                          School|  96|   194.9062|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |              married|                         Married| 751|   194.0306|\n#&gt; |                     |                   Never.married| 226|   182.8761|\n#&gt; |                     |              Previously.married| 290|   198.6586|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |               income|                            &lt;25k| 344|   191.9564|\n#&gt; |                     |                Between.25kto54k| 435|   191.9310|\n#&gt; |                     |                Between.55kto99k| 297|   195.7508|\n#&gt; |                     |                        Over100k| 191|   193.7016|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |          diastolicBP|                        [ 0, 64)| 336|   186.7649|\n#&gt; |                     |                        [64, 72)| 321|   189.3458|\n#&gt; |                     |                        [72, 80)| 319|   195.7085|\n#&gt; |                     |                        [80,112]| 291|   201.6976|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |           systolicBP|                       [ 84,116)| 340|   186.2765|\n#&gt; |                     |                       [116,126)| 317|   190.6372|\n#&gt; |                     |                       [126,138)| 335|   196.9881|\n#&gt; |                     |                       [138,236]| 275|   199.6400|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |           bodyweight|                    [39.7, 69.8)| 319|   193.8903|\n#&gt; |                     |                    [69.8, 81.5)| 316|   197.1424|\n#&gt; |                     |                    [81.5, 97.2)| 317|   192.4984|\n#&gt; |                     |                    [97.2,178.4]| 315|   188.8508|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |           bodyheight|                       [144,163)| 317|   198.7003|\n#&gt; |                     |                       [163,169)| 320|   193.7750|\n#&gt; |                     |                       [169,176)| 314|   189.8790|\n#&gt; |                     |                       [176,201]| 316|   190.0000|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                  bmi|                     [16.3,24.9)| 322|   188.8043|\n#&gt; |                     |                     [24.9,28.7)| 315|   198.5016|\n#&gt; |                     |                     [28.7,33.4)| 317|   197.5016|\n#&gt; |                     |                     [33.4,64.5]| 313|   187.6262|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                waist|                   [ 65.0, 90.7)| 320|   188.9688|\n#&gt; |                     |                   [ 90.7,100.4)| 315|   199.6413|\n#&gt; |                     |                   [100.4,111.3)| 316|   197.3892|\n#&gt; |                     |                   [111.3,161.5]| 316|   186.4747|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |                smoke|                       Every.day| 448|   191.5938|\n#&gt; |                     |                      Not.at.all| 665|   194.6451|\n#&gt; |                     |                       Some.days| 154|   190.8117|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |              alcohol|                               1| 336|   191.0387|\n#&gt; |                     |                               2| 371|   192.0809|\n#&gt; |                     |                          [3, 5)| 295|   195.9356|\n#&gt; |                     |                          [5,15]| 265|   193.9849|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |        triglycerides|                      [ 18,  85)| 320|   172.2344|\n#&gt; |                     |                      [ 85, 128)| 319|   185.6834|\n#&gt; |                     |                      [128, 203)| 314|   199.4140|\n#&gt; |                     |                      [203,3061]| 314|   215.5860|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |            uric.acid|                      [1.6, 4.7)| 348|   188.9310|\n#&gt; |                     |                      [4.7, 5.6)| 305|   191.8033|\n#&gt; |                     |                      [5.6, 6.6)| 307|   195.7720|\n#&gt; |                     |                      [6.6,18.0]| 307|   196.4430|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |              protein|                       [5.7,6.9)| 336|   189.8631|\n#&gt; |                     |                       [6.9,7.2)| 328|   192.3201|\n#&gt; |                     |                       [7.2,7.5)| 310|   193.4258|\n#&gt; |                     |                       [7.5,9.0]| 293|   197.3413|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |            bilirubin|                       [0.0,0.5)| 506|   195.7391|\n#&gt; |                     |                             0.5| 212|   192.2264|\n#&gt; |                     |                       [0.6,0.8)| 310|   192.0645|\n#&gt; |                     |                       [0.8,3.3]| 239|   189.6318|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |           phosphorus|                       [1.8,3.4)| 362|   188.0387|\n#&gt; |                     |                       [3.4,3.7)| 309|   192.5405|\n#&gt; |                     |                       [3.7,4.1)| 323|   195.5542|\n#&gt; |                     |                       [4.1,6.1]| 273|   197.5421|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |               sodium|                       [124,138)| 362|   191.9420|\n#&gt; |                     |                       [138,140)| 495|   194.2929|\n#&gt; |                     |                             140| 206|   191.7864|\n#&gt; |                     |                       [141,148]| 204|   193.5882|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |            potassium|                     [2.60,3.79)| 320|   191.5375|\n#&gt; |                     |                     [3.79,3.99)| 328|   192.3628|\n#&gt; |                     |                     [3.99,4.20)| 308|   196.9643|\n#&gt; |                     |                     [4.20,5.51]| 311|   191.6592|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |             globulin|                       [1.6,2.6)| 350|   189.9429|\n#&gt; |                     |                       [2.6,2.9)| 388|   199.0052|\n#&gt; |                     |                       [2.9,3.1)| 230|   193.3783|\n#&gt; |                     |                       [3.1,5.5]| 299|   188.9197|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |              calcium|                      [8.4, 9.2)| 371|   186.0323|\n#&gt; |                     |                      [9.2, 9.4)| 294|   188.3605|\n#&gt; |                     |                      [9.4, 9.7)| 395|   197.4430|\n#&gt; |                     |                      [9.7,11.1]| 207|   204.2126|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |        physical.work|                              No| 895|   194.0078|\n#&gt; |                     |                             Yes| 372|   190.9167|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |physical.recreational|                              No|1002|   193.5359|\n#&gt; |                     |                             Yes| 265|   191.4528|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |             diabetes|                              No|1064|   194.8036|\n#&gt; |                     |                             Yes| 203|   184.1724|\n#&gt; +---------------------+--------------------------------+----+-----------+\n#&gt; |              Overall|                                |1267|   193.1002|\n#&gt; +---------------------+--------------------------------+----+-----------+\nplot(var.summ)\n\n\n\n\n\n\n\nsummary(analytic$diastolicBP)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.00   62.00   70.00   70.37   78.00  112.00\n\nanalytic$diastolicBP[analytic$diastolicBP == 0] &lt;- NA\n\n# Bivariate Summaries Computed Separately by a Series of Predictors\nvar.summ2 &lt;- spearman2(cholesterol~ ., data = analytic[sel.names])\nplot(var.summ2)\n\n\n\n\n\n\n\nRegression: Linear regression\nA linear regression model is fitted to explore the association between cholesterol levels and triglycerides. Various summary statistics are also generated for the model.\n\n\n\n\n\n\nTip\n\n\n\nWe use lm function to fit the linear regression\n\n\n\n# set up formula with just 1 variable\nformula0 &lt;- as.formula(\"cholesterol~triglycerides\")\n\n# fitting regression on the analytic2 data\nfit0 &lt;- lm(formula0,data = analytic2)\n\n# extract results\nsummary(fit0)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = formula0, data = analytic2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -111.651  -26.157   -2.661   22.549  166.752 \n#&gt; \n#&gt; Coefficients:\n#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.716e+02  1.127e+00  152.23   &lt;2e-16 ***\n#&gt; triglycerides 1.275e-01  5.456e-03   23.37   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 37.38 on 2632 degrees of freedom\n#&gt; Multiple R-squared:  0.1718, Adjusted R-squared:  0.1715 \n#&gt; F-statistic:   546 on 1 and 2632 DF,  p-value: &lt; 2.2e-16\n\n# extract just the coefficients/estimates\ncoef(fit0)\n#&gt;   (Intercept) triglycerides \n#&gt;   171.6147531     0.1274909\n\n# extract confidence intervals\nconfint(fit0)\n#&gt;                     2.5 %      97.5 %\n#&gt; (Intercept)   169.4042284 173.8252779\n#&gt; triglycerides   0.1167919   0.1381899\n\n# residual plots\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\n\n\n\nDiagnosis\nIdentifying problematic data\nOutliers: We can begin by plotting cholesterol against triglycerides to visualize any potential outliers. We can then identify data points where triglycerides are high.\nLeverage: It calculates and plots leverage points. Leverage points that have values greater than 0.05 are isolated for inspection.\nResiduals: Studentized residuals are computed for each data point to identify potential outliers. Those with values less than -5 are identified.\n\nrequire(olsrr)\n# Outlier\nplot(cholesterol ~ triglycerides, data = analytic2)\n\n\n\n\n\n\nsubset(analytic2, triglycerides &gt; 1500)\n\n\n  \n\n\n\n# leverage\nols_plot_resid_lev(fit0)\n\n\n\n\n\n\nanalytic2$lev &lt;- hat(model.matrix(fit0))\nplot(analytic2$lev)\n\n\n\n\n\n\nsummary(analytic2$lev)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; 0.0003797 0.0004062 0.0004773 0.0007593 0.0005831 0.1800021\nwhich(analytic2$lev &gt; 0.05)\n#&gt; [1] 1102\nsubset(analytic2, lev &gt; 0.05)\n\n\n  \n\n\n\n# Residual\nanalytic2$rstudent.values &lt;- rstudent(fit0)\nplot(analytic2$rstudent.values)\n\n\n\n\n\n\nwhich(analytic2$rstudent.values &lt; -5)\n#&gt; integer(0)\n# Heteroskedasticity: Test for constant variance\n#ols_test_breusch_pagan(fit0, rhs = TRUE)\n\nDeleting suspicious data\nWe then delete observations based on two conditions: triglycerides &gt; 1500 and leverage &gt; 0.05.\n\n# condition 1: triglycerides above 1500 needs deleting\nanalytic2b &lt;- subset(analytic2, triglycerides &lt; 1500)\ndim(analytic2b)\n#&gt; [1] 2632   34\n\n# condition 2: leverage above 0.05 needs deleting\nanalytic3 &lt;- subset(analytic2b, lev &lt; 0.05)\ndim(analytic3)\n#&gt; [1] 2632   34\n\n# Check how many observations are deleted\nnrow(analytic2)-nrow(analytic3)\n#&gt; [1] 2\n\nRefitting in cleaned data\nWe refit the linear model on this cleaned data, and diagnostic plots are generated.\n\n### Re-fit in data analytic3 (without problematic data)\nformula0\n#&gt; cholesterol ~ triglycerides\nfit0 &lt;- lm(formula0,data = analytic3)\n\nrequire(Publish)\npublish(fit0)\n#&gt;       Variable Units Coefficient           CI.95 p-value \n#&gt;    (Intercept)            171.74 [169.37;174.11] &lt; 1e-04 \n#&gt;  triglycerides              0.13     [0.11;0.14] &lt; 1e-04\nlayout(matrix(1:6, byrow = T, ncol = 3))\nplot(fit0, which = 1:6)\n\n\n\n\n\n\n\nrequire(car)\n# component+residual plot or partial-residual plot\ncrPlots(fit0)\n\n\n\n\n\n\n\nPolynomial order 2\nWe fit polynomial models of orders 2 and 3 to explore non-linear relationships between cholesterol and triglycerides.\n\nformula1 &lt;- as.formula(\"cholesterol~poly(triglycerides,2)\")\nformula1 &lt;- as.formula(\"cholesterol~triglycerides^2\")\nfit1 &lt;- lm(formula1,data = analytic3)\npublish(fit1)\n#&gt;       Variable Units Coefficient           CI.95 p-value \n#&gt;    (Intercept)            171.74 [169.37;174.11] &lt; 1e-04 \n#&gt;  triglycerides              0.13     [0.11;0.14] &lt; 1e-04\n\n# Partial Residual Plots\ncrPlots(fit1)\n\n\n\n\n\n\n\nIt seems triglyceride has a quadratic effect on cholesterol. We can compare models fit0 and fit1 using the anova function:\n\n# compare fit0 and fit1 models\nres0 &lt;- anova(fit0,fit1)\nprint(res0)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: cholesterol ~ triglycerides\n#&gt; Model 2: cholesterol ~ triglycerides^2\n#&gt;   Res.Df     RSS Df Sum of Sq F Pr(&gt;F)\n#&gt; 1   2630 3673461                      \n#&gt; 2   2630 3673461  0         0\n\nPolynomial order 3\n\n# Fit a polynomial of order 3\nformula2 &lt;- as.formula(\"cholesterol~poly(triglycerides,3)\")\nformula2 &lt;- as.formula(\"cholesterol~triglycerides^3\")\nfit2 &lt;- lm(formula2,data = analytic3)\npublish(fit2)\n#&gt;       Variable Units Coefficient           CI.95 p-value \n#&gt;    (Intercept)            171.74 [169.37;174.11] &lt; 1e-04 \n#&gt;  triglycerides              0.13     [0.11;0.14] &lt; 1e-04\n\n# Partial Residual Plots\ncrPlots(fit2)\n\n\n\n\n\n\n\n\n# compare fit1 and fit2 models\nres1 &lt;- anova(fit1,fit2)\nprint(res1)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: cholesterol ~ triglycerides^2\n#&gt; Model 2: cholesterol ~ triglycerides^3\n#&gt;   Res.Df     RSS Df Sum of Sq F Pr(&gt;F)\n#&gt; 1   2630 3673461                      \n#&gt; 2   2630 3673461  0         0\n\nMultiple covariates\nWe add more covariates.\n\n# include everything!\nformula3 &lt;- as.formula(\"cholesterol ~ gender + age + born + race + \n                       education + married + income + diastolicBP + \n                       systolicBP + bmi + bodyweight + bodyheight + \n                       waist +  triglycerides + uric.acid + protein +\n                       bilirubin + phosphorus + sodium + potassium + \n                       globulin + calcium + physical.work + \n                       physical.recreational + diabetes\")\nfit3 &lt;- lm(formula3, data = analytic3)\npublish(fit3)\n#&gt;               Variable                            Units Coefficient           CI.95     p-value \n#&gt;            (Intercept)                                       280.02 [133.42;426.62]   0.0001852 \n#&gt;                 gender                           Female         Ref                             \n#&gt;                                                    Male      -11.99  [-16.41;-7.57]     &lt; 1e-04 \n#&gt;                    age                                         0.35     [0.23;0.47]     &lt; 1e-04 \n#&gt;                   born Born in 50 US states or Washingt         Ref                             \n#&gt;                                                  Others        7.52    [3.68;11.36]   0.0001270 \n#&gt;                   race                            Black         Ref                             \n#&gt;                                                Hispanic       -6.15  [-10.87;-1.44]   0.0106253 \n#&gt;                                                   Other       -5.37   [-10.92;0.18]   0.0579281 \n#&gt;                                                   White       -0.95    [-5.21;3.30]   0.6603698 \n#&gt;              education                          College         Ref                             \n#&gt;                                             High.School        2.90    [-0.28;6.08]   0.0743132 \n#&gt;                                                  School       -2.54    [-8.61;3.54]   0.4134016 \n#&gt;                married                          Married         Ref                             \n#&gt;                                           Never.married       -5.72   [-9.63;-1.81]   0.0041887 \n#&gt;                                      Previously.married        0.31    [-3.54;4.17]   0.8730460 \n#&gt;                 income                             &lt;25k         Ref                             \n#&gt;                                        Between.25kto54k       -0.97    [-4.87;2.93]   0.6261315 \n#&gt;                                        Between.55kto99k        2.29    [-1.98;6.56]   0.2928564 \n#&gt;                                                Over100k        2.44    [-2.27;7.14]   0.3099380 \n#&gt;            diastolicBP                                         0.38     [0.25;0.50]     &lt; 1e-04 \n#&gt;             systolicBP                                         0.02    [-0.08;0.12]   0.6668119 \n#&gt;                    bmi                                        -2.55   [-4.29;-0.81]   0.0041392 \n#&gt;             bodyweight                                         0.82     [0.19;1.45]   0.0105518 \n#&gt;             bodyheight                                        -0.89   [-1.55;-0.24]   0.0074286 \n#&gt;                  waist                                        -0.02    [-0.29;0.26]   0.9020424 \n#&gt;          triglycerides                                         0.12     [0.11;0.14]     &lt; 1e-04 \n#&gt;              uric.acid                                         1.27     [0.08;2.47]   0.0369190 \n#&gt;                protein                                         4.99   [-0.77;10.74]   0.0897748 \n#&gt;              bilirubin                                        -5.43  [-10.53;-0.33]   0.0370512 \n#&gt;             phosphorus                                        -0.18    [-2.81;2.45]   0.8939361 \n#&gt;                 sodium                                        -0.97   [-1.66;-0.29]   0.0052516 \n#&gt;              potassium                                         1.04    [-3.44;5.52]   0.6487979 \n#&gt;               globulin                                        -2.25    [-8.22;3.71]   0.4591138 \n#&gt;                calcium                                        12.02    [6.98;17.07]     &lt; 1e-04 \n#&gt;          physical.work                               No         Ref                             \n#&gt;                                                     Yes       -0.45    [-3.68;2.79]   0.7858787 \n#&gt;  physical.recreational                               No         Ref                             \n#&gt;                                                     Yes        1.35    [-1.94;4.65]   0.4210703 \n#&gt;               diabetes                               No         Ref                             \n#&gt;                                                     Yes      -19.11 [-23.37;-14.85]     &lt; 1e-04\n\nColinearity\nWe finally check for multicollinearity among predictors using the Variance Inflation Factor (VIF).\n\n\n\n\n\n\nNote\n\n\n\nRule of thumb: variables with VIF &gt; 4 needs further investigation\n\n\n\ncar::vif(fit3)\n#&gt;                             GVIF Df GVIF^(1/(2*Df))\n#&gt; gender                  2.694171  1        1.641393\n#&gt; age                     2.164388  1        1.471186\n#&gt; born                    1.611478  1        1.269440\n#&gt; race                    2.463445  3        1.162137\n#&gt; education               1.435876  2        1.094660\n#&gt; married                 1.481141  2        1.103187\n#&gt; income                  1.402249  3        1.057964\n#&gt; diastolicBP             1.271126  1        1.127442\n#&gt; systolicBP              1.594986  1        1.262928\n#&gt; bmi                    81.811969  1        9.044997\n#&gt; bodyweight            101.102349  1       10.054966\n#&gt; bodyheight             21.863188  1        4.675809\n#&gt; waist                  11.913719  1        3.451626\n#&gt; triglycerides           1.219331  1        1.104233\n#&gt; uric.acid               1.603290  1        1.266211\n#&gt; protein                 3.622385  1        1.903256\n#&gt; bilirubin               1.185035  1        1.088593\n#&gt; phosphorus              1.116982  1        1.056874\n#&gt; sodium                  1.120920  1        1.058735\n#&gt; potassium               1.178381  1        1.085533\n#&gt; globulin                3.371211  1        1.836086\n#&gt; calcium                 1.591677  1        1.261617\n#&gt; physical.work           1.087315  1        1.042744\n#&gt; physical.recreational   1.226830  1        1.107624\n#&gt; diabetes                1.210715  1        1.100325\ncollinearity &lt;- ols_vif_tol(fit3)\ncollinearity\n\n\n  \n\n\n\n# VIF &gt; 4\ncollinearity[collinearity$VIF&gt;4,]\n\n\n  \n\n\n\n\nformula4 &lt;- as.formula(\"cholesterol ~ gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + # bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit4 &lt;- lm(formula4, data = analytic3)\npublish(fit4)\n#&gt;               Variable                            Units Coefficient           CI.95    p-value \n#&gt;            (Intercept)                                       136.87  [34.96;238.79]   0.008533 \n#&gt;                 gender                           Female         Ref                            \n#&gt;                                                    Male      -13.06  [-16.60;-9.53]    &lt; 1e-04 \n#&gt;                    age                                         0.35     [0.24;0.46]    &lt; 1e-04 \n#&gt;                   born Born in 50 US states or Washingt         Ref                            \n#&gt;                                                  Others        7.88    [4.06;11.69]    &lt; 1e-04 \n#&gt;                   race                            Black         Ref                            \n#&gt;                                                Hispanic       -5.79  [-10.34;-1.24]   0.012740 \n#&gt;                                                   Other       -4.88   [-10.33;0.57]   0.079497 \n#&gt;                                                   White       -0.85    [-5.02;3.33]   0.690720 \n#&gt;              education                          College         Ref                            \n#&gt;                                             High.School        2.85    [-0.32;6.02]   0.078008 \n#&gt;                                                  School       -2.45    [-8.49;3.60]   0.427694 \n#&gt;                married                          Married         Ref                            \n#&gt;                                           Never.married       -5.74   [-9.65;-1.83]   0.004088 \n#&gt;                                      Previously.married        0.34    [-3.52;4.20]   0.861981 \n#&gt;                 income                             &lt;25k         Ref                            \n#&gt;                                        Between.25kto54k       -0.87    [-4.77;3.03]   0.663123 \n#&gt;                                        Between.55kto99k        2.46    [-1.79;6.71]   0.256585 \n#&gt;                                                Over100k        2.63    [-2.07;7.32]   0.272886 \n#&gt;            diastolicBP                                         0.37     [0.25;0.50]    &lt; 1e-04 \n#&gt;             systolicBP                                         0.03    [-0.07;0.13]   0.544971 \n#&gt;                    bmi                                        -0.31   [-0.54;-0.08]   0.009302 \n#&gt;          triglycerides                                         0.12     [0.11;0.14]    &lt; 1e-04 \n#&gt;              uric.acid                                         1.36     [0.16;2.55]   0.025926 \n#&gt;                protein                                         4.77   [-0.98;10.51]   0.104059 \n#&gt;              bilirubin                                        -6.06  [-11.14;-0.98]   0.019519 \n#&gt;             phosphorus                                        -0.08    [-2.71;2.55]   0.954561 \n#&gt;                 sodium                                        -1.03   [-1.71;-0.35]   0.003175 \n#&gt;              potassium                                         0.89    [-3.58;5.37]   0.695615 \n#&gt;               globulin                                        -2.20    [-8.15;3.75]   0.469150 \n#&gt;                calcium                                        12.20    [7.16;17.25]    &lt; 1e-04 \n#&gt;          physical.work                               No         Ref                            \n#&gt;                                                     Yes       -0.44    [-3.68;2.80]   0.790297 \n#&gt;  physical.recreational                               No         Ref                            \n#&gt;                                                     Yes        1.24    [-2.03;4.51]   0.457666 \n#&gt;               diabetes                               No         Ref                            \n#&gt;                                                     Yes      -19.03 [-23.26;-14.80]    &lt; 1e-04\n\n# check if there is still any problematic variable\n# with high collinearity problem\ncollinearity &lt;- ols_vif_tol(fit4)\ncollinearity[collinearity$VIF&gt;4,]\n\n\n  \n\n\n\nSave data\n\nsave.image(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")",
    "crumbs": [
      "Prediction ideas",
      "Continuous outcome"
    ]
  },
  {
    "objectID": "predictivefactors3.html",
    "href": "predictivefactors3.html",
    "title": "Binary outcome",
    "section": "",
    "text": "We focus on statistical analysis and modeling of a binary outcome (cholesterol level) that is categorized as either “healthy” or “unhealthy.”\nExplore relationships for binary outcome variable\n\n#&gt; Loading required package: Hmisc\n#&gt; \n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     format.pval, units\n#&gt; Loading required package: prodlim\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; The following objects are masked from 'package:rms':\n#&gt; \n#&gt;     Predict, vif\n#&gt; Type 'citation(\"pROC\")' for a citation.\n#&gt; \n#&gt; Attaching package: 'pROC'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cov, smooth, var\n\nLoad data\n\nload(file = \"Data/predictivefactors/cholesterolNHANES15part1.RData\")\n\nCreating binary variable\nBinary categorization: The cholesterol variable is converted into a binary outcome (“healthy” or “unhealthy”) using the ifelse function based on a threshold value of 200.\nRe-leveling: The reference category for the binary variable is changed to “unhealthy.”\n\n# Binary variable\nanalytic3$cholesterol.bin &lt;- ifelse(analytic3$cholesterol &lt; 200, \"healthy\", \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#&gt; \n#&gt;   healthy unhealthy \n#&gt;      1586      1046\n\n# Changing the reference category\nanalytic3$cholesterol.bin &lt;- as.factor(analytic3$cholesterol.bin)\nanalytic3$cholesterol.bin &lt;- relevel(analytic3$cholesterol.bin, ref = \"unhealthy\")\ntable(analytic3$cholesterol.bin)\n#&gt; \n#&gt; unhealthy   healthy \n#&gt;      1046      1586\n\nModelling data\nA logistic regression is fitted to predict the binary cholesterol outcome from multiple predictor variables.\n\n\n\n\n\n\nTip\n\n\n\nWe use the glm function to run generalized linear models. The default family is gaussian with identity link. Setting binomial family with logit link (logit link is default for binomial family) means fitting logistic regression.\n\n\n\n# Regression model\nformula5x &lt;- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + bodyweight + bodyheight + waist +  \n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\n\n# Summary\nfit5x &lt;- glm(formula5x, family = binomial(), data = analytic3)\npublish(fit5x)\n#&gt;               Variable                            Units OddsRatio       CI.95    p-value \n#&gt;                 gender                           Female       Ref                        \n#&gt;                                                    Male      1.68 [1.27;2.23]   0.000313 \n#&gt;                    age                                       0.97 [0.97;0.98]    &lt; 1e-04 \n#&gt;                   born Born in 50 US states or Washingt       Ref                        \n#&gt;                                                  Others      0.69 [0.54;0.88]   0.002636 \n#&gt;                   race                            Black       Ref                        \n#&gt;                                                Hispanic      1.29 [0.95;1.75]   0.107104 \n#&gt;                                                   Other      1.24 [0.87;1.79]   0.234062 \n#&gt;                                                   White      1.10 [0.83;1.44]   0.505147 \n#&gt;              education                          College       Ref                        \n#&gt;                                             High.School      0.84 [0.68;1.02]   0.082168 \n#&gt;                                                  School      1.23 [0.84;1.82]   0.292070 \n#&gt;                married                          Married       Ref                        \n#&gt;                                           Never.married      1.29 [1.00;1.67]   0.052969 \n#&gt;                                      Previously.married      0.89 [0.70;1.13]   0.345688 \n#&gt;                 income                             &lt;25k       Ref                        \n#&gt;                                        Between.25kto54k      1.01 [0.78;1.29]   0.957537 \n#&gt;                                        Between.55kto99k      0.90 [0.69;1.19]   0.462854 \n#&gt;                                                Over100k      0.90 [0.66;1.21]   0.472137 \n#&gt;            diastolicBP                                       0.98 [0.97;0.98]    &lt; 1e-04 \n#&gt;             systolicBP                                       1.01 [1.00;1.01]   0.029513 \n#&gt;                    bmi                                       1.11 [0.99;1.24]   0.065627 \n#&gt;             bodyweight                                       0.96 [0.92;1.00]   0.045338 \n#&gt;             bodyheight                                       1.05 [1.00;1.09]   0.030995 \n#&gt;                  waist                                       1.01 [0.99;1.02]   0.464825 \n#&gt;          triglycerides                                       0.99 [0.99;0.99]    &lt; 1e-04 \n#&gt;              uric.acid                                       0.96 [0.89;1.03]   0.273792 \n#&gt;                protein                                       0.61 [0.42;0.89]   0.009192 \n#&gt;              bilirubin                                       1.19 [0.86;1.66]   0.292632 \n#&gt;             phosphorus                                       0.96 [0.81;1.13]   0.610931 \n#&gt;                 sodium                                       1.06 [1.02;1.11]   0.007980 \n#&gt;              potassium                                       0.95 [0.71;1.26]   0.729218 \n#&gt;               globulin                                       1.38 [0.94;2.01]   0.101667 \n#&gt;                calcium                                       0.64 [0.47;0.89]   0.007026 \n#&gt;          physical.work                               No       Ref                        \n#&gt;                                                     Yes      0.91 [0.74;1.12]   0.392539 \n#&gt;  physical.recreational                               No       Ref                        \n#&gt;                                                     Yes      1.05 [0.85;1.29]   0.681388 \n#&gt;               diabetes                               No       Ref                        \n#&gt;                                                     Yes      2.68 [2.02;3.56]    &lt; 1e-04\n\n# VIF\ncar::vif(fit5x)\n#&gt;                             GVIF Df GVIF^(1/(2*Df))\n#&gt; gender                  2.735258  1        1.653862\n#&gt; age                     2.121098  1        1.456399\n#&gt; born                    1.664094  1        1.289998\n#&gt; race                    2.585539  3        1.171544\n#&gt; education               1.458430  2        1.098933\n#&gt; married                 1.432595  2        1.094034\n#&gt; income                  1.426911  3        1.061043\n#&gt; diastolicBP             1.297308  1        1.138994\n#&gt; systolicBP              1.614374  1        1.270580\n#&gt; bmi                    81.928815  1        9.051454\n#&gt; bodyweight            103.125772  1       10.155086\n#&gt; bodyheight             22.647853  1        4.758976\n#&gt; waist                  11.493710  1        3.390237\n#&gt; triglycerides           1.258340  1        1.121758\n#&gt; uric.acid               1.636512  1        1.279262\n#&gt; protein                 3.684816  1        1.919587\n#&gt; bilirubin               1.186181  1        1.089119\n#&gt; phosphorus              1.117915  1        1.057315\n#&gt; sodium                  1.123193  1        1.059808\n#&gt; potassium               1.181358  1        1.086903\n#&gt; globulin                3.427401  1        1.851324\n#&gt; calcium                 1.543019  1        1.242183\n#&gt; physical.work           1.090958  1        1.044490\n#&gt; physical.recreational   1.218558  1        1.103883\n#&gt; diabetes                1.212365  1        1.101074\n\nAUC\nThe Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) is calculated to assess the model’s predictive performance in terms of discrimination. The AUC would tell how much the model is capable of distinguishing between healthy and unhealthy levels.\nLet us measure the accuracy for classification models fit5x.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the roc function to build a ROC curve and auc function to calculate the AUC (are under the ROC curve) value.\n\n\n\nrequire(pROC)\npred.y &lt;- predict(fit5x, type = \"response\")\nrocobj &lt;- roc(analytic3$cholesterol.bin, pred.y)\n#&gt; Setting levels: control = unhealthy, case = healthy\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#&gt; \n#&gt; Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) &lt; 1586 cases (analytic3$cholesterol.bin healthy).\n#&gt; Area under the curve: 0.7411\n\nauc(rocobj)\n#&gt; Area under the curve: 0.7411\n\nRe-modelling\nLet us re-fit the model and measure the AUC. VIF is calculated again for this new model.\n\nformula5 &lt;- as.formula(\"cholesterol.bin~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi +\n             triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + physical.recreational + \n             diabetes\")\nfit5 &lt;- glm(formula5, family = binomial(), data = analytic3)\npublish(fit5)\n#&gt;               Variable                            Units OddsRatio       CI.95    p-value \n#&gt;                 gender                           Female       Ref                        \n#&gt;                                                    Male      1.86 [1.48;2.33]    &lt; 1e-04 \n#&gt;                    age                                       0.97 [0.97;0.98]    &lt; 1e-04 \n#&gt;                   born Born in 50 US states or Washingt       Ref                        \n#&gt;                                                  Others      0.67 [0.52;0.85]   0.001233 \n#&gt;                   race                            Black       Ref                        \n#&gt;                                                Hispanic      1.26 [0.94;1.69]   0.128165 \n#&gt;                                                   Other      1.21 [0.85;1.73]   0.284083 \n#&gt;                                                   White      1.11 [0.85;1.45]   0.460652 \n#&gt;              education                          College       Ref                        \n#&gt;                                             High.School      0.84 [0.68;1.02]   0.083806 \n#&gt;                                                  School      1.22 [0.83;1.80]   0.305514 \n#&gt;                married                          Married       Ref                        \n#&gt;                                           Never.married      1.29 [1.00;1.68]   0.049983 \n#&gt;                                      Previously.married      0.89 [0.70;1.13]   0.339401 \n#&gt;                 income                             &lt;25k       Ref                        \n#&gt;                                        Between.25kto54k      1.00 [0.78;1.28]   0.999445 \n#&gt;                                        Between.55kto99k      0.90 [0.68;1.17]   0.425427 \n#&gt;                                                Over100k      0.89 [0.66;1.20]   0.447012 \n#&gt;            diastolicBP                                       0.98 [0.97;0.99]    &lt; 1e-04 \n#&gt;             systolicBP                                       1.01 [1.00;1.01]   0.042769 \n#&gt;                    bmi                                       1.01 [0.99;1.02]   0.496430 \n#&gt;          triglycerides                                       0.99 [0.99;0.99]    &lt; 1e-04 \n#&gt;              uric.acid                                       0.96 [0.89;1.03]   0.242942 \n#&gt;                protein                                       0.62 [0.43;0.89]   0.010343 \n#&gt;              bilirubin                                       1.24 [0.89;1.72]   0.203993 \n#&gt;             phosphorus                                       0.95 [0.80;1.12]   0.539847 \n#&gt;                 sodium                                       1.06 [1.02;1.11]   0.006777 \n#&gt;              potassium                                       0.96 [0.72;1.28]   0.790080 \n#&gt;               globulin                                       1.37 [0.94;2.00]   0.102430 \n#&gt;                calcium                                       0.64 [0.46;0.88]   0.005772 \n#&gt;          physical.work                               No       Ref                        \n#&gt;                                                     Yes      0.91 [0.74;1.12]   0.382281 \n#&gt;  physical.recreational                               No       Ref                        \n#&gt;                                                     Yes      1.04 [0.85;1.29]   0.682962 \n#&gt;               diabetes                               No       Ref                        \n#&gt;                                                     Yes      2.69 [2.03;3.57]    &lt; 1e-04\n\n# VIF\ncar::vif(fit5)\n#&gt;                           GVIF Df GVIF^(1/(2*Df))\n#&gt; gender                1.749947  1        1.322856\n#&gt; age                   1.850160  1        1.360206\n#&gt; born                  1.640947  1        1.280994\n#&gt; race                  2.345460  3        1.152669\n#&gt; education             1.430721  2        1.093676\n#&gt; married               1.432015  2        1.093923\n#&gt; income                1.409064  3        1.058819\n#&gt; diastolicBP           1.289411  1        1.135523\n#&gt; systolicBP            1.605248  1        1.266984\n#&gt; bmi                   1.477795  1        1.215646\n#&gt; triglycerides         1.246395  1        1.116421\n#&gt; uric.acid             1.624039  1        1.274378\n#&gt; protein               3.648367  1        1.910070\n#&gt; bilirubin             1.177643  1        1.085193\n#&gt; phosphorus            1.114298  1        1.055603\n#&gt; sodium                1.117463  1        1.057101\n#&gt; potassium             1.176914  1        1.084857\n#&gt; globulin              3.395946  1        1.842809\n#&gt; calcium               1.542486  1        1.241969\n#&gt; physical.work         1.089742  1        1.043907\n#&gt; physical.recreational 1.197719  1        1.094404\n#&gt; diabetes              1.200402  1        1.095629\n\nThe AUC for this new model is also calculated.\n\n#### AUC\npred.y &lt;- predict(fit5, type = \"response\")\nrocobj &lt;- roc(analytic3$cholesterol.bin, pred.y)\n#&gt; Setting levels: control = unhealthy, case = healthy\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = analytic3$cholesterol.bin, predictor = pred.y)\n#&gt; \n#&gt; Data: pred.y in 1046 controls (analytic3$cholesterol.bin unhealthy) &lt; 1586 cases (analytic3$cholesterol.bin healthy).\n#&gt; Area under the curve: 0.7406\nauc(rocobj)\n#&gt; Area under the curve: 0.7406\n\nSave data\n\nsave.image(file = \"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nReferences",
    "crumbs": [
      "Prediction ideas",
      "Binary outcome"
    ]
  },
  {
    "objectID": "predictivefactors4.html",
    "href": "predictivefactors4.html",
    "title": "Overfitting and performance",
    "section": "",
    "text": "The following tutorial extends the work from the previous lab and focuses on understanding overfitting, evaluating performance, and function writing in the context of linear modeling for a continuous outcome variable, cholesterol levels.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nNow we will fit the final model that we decided at the end of previous part of the lab.\n\nformula4 &lt;- as.formula(\"cholesterol~gender + age + born + \n             race + education + married + \n             income + diastolicBP + systolicBP + \n             bmi + triglycerides + uric.acid + \n             protein + bilirubin + phosphorus + sodium + potassium + \n             globulin + calcium + physical.work + \n             physical.recreational + diabetes\")\nformula4\n#&gt; cholesterol ~ gender + age + born + race + education + married + \n#&gt;     income + diastolicBP + systolicBP + bmi + triglycerides + \n#&gt;     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium + physical.work + physical.recreational + \n#&gt;     diabetes\nfit4 &lt;- lm(formula4, data = analytic3)\nsummary(fit4)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = formula4, data = analytic3)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -115.465  -23.695   -2.598   20.017  177.264 \n#&gt; \n#&gt; Coefficients:\n#&gt;                             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               136.871606  51.998527   2.632  0.00853 ** \n#&gt; genderMale                -13.064857   1.802099  -7.250 5.48e-13 ***\n#&gt; age                         0.351838   0.056116   6.270 4.22e-10 ***\n#&gt; bornOthers                  7.877420   1.947498   4.045 5.39e-05 ***\n#&gt; raceHispanic               -5.790547   2.323010  -2.493  0.01274 *  \n#&gt; raceOther                  -4.879882   2.781673  -1.754  0.07950 .  \n#&gt; raceWhite                  -0.847635   2.130149  -0.398  0.69072    \n#&gt; educationHigh.School        2.851633   1.617435   1.763  0.07801 .  \n#&gt; educationSchool            -2.446765   3.084409  -0.793  0.42769    \n#&gt; marriedNever.married       -5.739509   1.997152  -2.874  0.00409 ** \n#&gt; marriedPreviously.married   0.342206   1.968165   0.174  0.86198    \n#&gt; incomeBetween.25kto54k     -0.867063   1.990253  -0.436  0.66312    \n#&gt; incomeBetween.55kto99k      2.462130   2.169757   1.135  0.25658    \n#&gt; incomeOver100k              2.626046   2.394560   1.097  0.27289    \n#&gt; diastolicBP                 0.374971   0.062238   6.025 1.93e-09 ***\n#&gt; systolicBP                  0.029976   0.049515   0.605  0.54497    \n#&gt; bmi                        -0.309530   0.118927  -2.603  0.00930 ** \n#&gt; triglycerides               0.124806   0.006427  19.419  &lt; 2e-16 ***\n#&gt; uric.acid                   1.357242   0.609012   2.229  0.02593 *  \n#&gt; protein                     4.767008   2.931636   1.626  0.10406    \n#&gt; bilirubin                  -6.060791   2.593508  -2.337  0.01952 *  \n#&gt; phosphorus                 -0.076472   1.341957  -0.057  0.95456    \n#&gt; sodium                     -1.026686   0.347679  -2.953  0.00318 ** \n#&gt; potassium                   0.893507   2.283488   0.391  0.69561    \n#&gt; globulin                   -2.198037   3.036091  -0.724  0.46915    \n#&gt; calcium                    12.202366   2.574400   4.740 2.25e-06 ***\n#&gt; physical.workYes           -0.439108   1.651078  -0.266  0.79030    \n#&gt; physical.recreationalYes    1.238756   1.667670   0.743  0.45767    \n#&gt; diabetesYes               -19.032748   2.158825  -8.816  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 35.22 on 2603 degrees of freedom\n#&gt; Multiple R-squared:  0.2415, Adjusted R-squared:  0.2334 \n#&gt; F-statistic: 29.61 on 28 and 2603 DF,  p-value: &lt; 2.2e-16\n\nDesign Matrix\nExpands factors to a set of dummy variables.\n\n\n\n\n\n\nTip\n\n\n\nWe can use the model.matrix function to construct a design/model matrix, such as expand factor variables to a matrix of dummy variable\n\n\nThe dimensions of the model matrix are obtained, and the total number of model parameters (p) is calculated.\n\nhead(model.matrix(fit4))\n#&gt;    (Intercept) genderMale age bornOthers raceHispanic raceOther raceWhite\n#&gt; 1            1          1  62          0            0         0         1\n#&gt; 2            1          1  53          1            0         0         1\n#&gt; 4            1          0  56          0            0         0         1\n#&gt; 5            1          0  42          0            0         0         0\n#&gt; 10           1          1  22          0            0         0         0\n#&gt; 11           1          0  32          1            1         0         0\n#&gt;    educationHigh.School educationSchool marriedNever.married\n#&gt; 1                     0               0                    0\n#&gt; 2                     1               0                    0\n#&gt; 4                     0               0                    0\n#&gt; 5                     0               0                    0\n#&gt; 10                    0               0                    1\n#&gt; 11                    0               0                    0\n#&gt;    marriedPreviously.married incomeBetween.25kto54k incomeBetween.55kto99k\n#&gt; 1                          0                      0                      1\n#&gt; 2                          1                      0                      0\n#&gt; 4                          0                      0                      1\n#&gt; 5                          1                      1                      0\n#&gt; 10                         0                      1                      0\n#&gt; 11                         0                      1                      0\n#&gt;    incomeOver100k diastolicBP systolicBP  bmi triglycerides uric.acid protein\n#&gt; 1               0          70        128 27.8           158       4.2     7.5\n#&gt; 2               0          88        146 30.8           170       7.0     7.4\n#&gt; 4               0          72        132 42.4            93       5.4     6.1\n#&gt; 5               0          70        100 20.3            52       3.3     7.7\n#&gt; 10              0          70        110 28.0            77       6.0     7.4\n#&gt; 11              0          70        120 28.2           295       5.2     7.4\n#&gt;    bilirubin phosphorus sodium potassium globulin calcium physical.workYes\n#&gt; 1        0.5        4.7    136      4.30      2.9     9.8                0\n#&gt; 2        0.6        4.4    140      4.55      2.9     9.8                0\n#&gt; 4        0.3        3.8    141      4.08      2.3     8.9                0\n#&gt; 5        0.3        3.2    136      3.50      3.4     9.3                0\n#&gt; 10       0.2        5.3    139      4.16      3.0     9.3                0\n#&gt; 11       0.4        3.1    138      4.31      2.9    10.3                0\n#&gt;    physical.recreationalYes diabetesYes\n#&gt; 1                         0           1\n#&gt; 2                         0           0\n#&gt; 4                         0           0\n#&gt; 5                         0           0\n#&gt; 10                        1           0\n#&gt; 11                        0           0\n\n# Dimension of the model matrix\ndim(model.matrix(fit4))\n#&gt; [1] 2632   29\n\n# Number of parameters = intercept + slopes\np &lt;- dim(model.matrix(fit4))[2] \np\n#&gt; [1] 29\n\nCheck prediction\nThe observed and predicted cholesterol values are summarized.\n\nobs.y &lt;- analytic3$cholesterol\nsummary(obs.y)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    81.0   163.0   189.0   191.5   216.0   362.0\n\n# Predict the above fit on analytic3 data\npred.y &lt;- predict(fit4, analytic3)\nsummary(pred.y)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   136.3   178.2   189.4   191.5   202.4   337.6\nn &lt;- length(pred.y)\nn\n#&gt; [1] 2632\nplot(obs.y,pred.y)\nlines(lowess(obs.y,pred.y), col = \"red\")\n\n\n\n\n\n\n\n# Prediction on a new data: fictitious.data\nstr(fictitious.data)\n#&gt; 'data.frame':    4121 obs. of  33 variables:\n#&gt;  $ ID                   : num  83732 83733 83734 83735 83736 ...\n#&gt;  $ gender               : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n#&gt;  $ age                  : num  62 53 78 56 42 72 22 32 56 46 ...\n#&gt;  $ born                 : chr  \"Born in 50 US states or Washingt\" \"Others\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;  $ race                 : chr  \"White\" \"White\" \"White\" \"White\" ...\n#&gt;  $ education            : chr  \"College\" \"High.School\" \"High.School\" \"College\" ...\n#&gt;  $ married              : chr  \"Married\" \"Previously.married\" \"Married\" \"Married\" ...\n#&gt;  $ income               : chr  \"Between.55kto99k\" \"&lt;25k\" \"&lt;25k\" \"Between.55kto99k\" ...\n#&gt;  $ weight               : num  135630 25282 12576 102079 18235 ...\n#&gt;  $ psu                  : num  1 1 1 1 2 1 2 1 2 1 ...\n#&gt;  $ strata               : num  125 125 131 131 126 128 128 125 126 121 ...\n#&gt;  $ diastolicBP          : num  70 88 46 72 70 58 70 70 116 94 ...\n#&gt;  $ systolicBP           : num  128 146 138 132 100 116 110 120 178 144 ...\n#&gt;  $ bodyweight           : num  94.8 90.4 83.4 109.8 55.2 ...\n#&gt;  $ bodyheight           : num  184 171 170 161 165 ...\n#&gt;  $ bmi                  : num  27.8 30.8 28.8 42.4 20.3 28.6 28 28.2 33.6 27.6 ...\n#&gt;  $ waist                : num  101.1 107.9 116.5 110.1 80.4 ...\n#&gt;  $ smoke                : chr  \"Not.at.all\" \"Every.day\" \"Not.at.all\" \"Not.at.all\" ...\n#&gt;  $ alcohol              : num  1 6 0 1 1 0 8 1 0 1 ...\n#&gt;  $ cholesterol          : num  173 265 229 174 204 190 164 190 145 242 ...\n#&gt;  $ cholesterolM2        : num  4.47 6.85 5.92 4.5 5.28 4.91 4.24 4.91 3.75 6.26 ...\n#&gt;  $ triglycerides        : num  158 170 299 93 52 52 77 295 121 497 ...\n#&gt;  $ uric.acid            : num  4.2 7 7.3 5.4 3.3 4.9 6 5.2 4.8 6.5 ...\n#&gt;  $ protein              : num  7.5 7.4 7.3 6.1 7.7 7.1 7.4 7.4 6.9 6.8 ...\n#&gt;  $ bilirubin            : num  0.5 0.6 0.5 0.3 0.3 0.5 0.2 0.4 0.4 0.5 ...\n#&gt;  $ phosphorus           : num  4.7 4.4 3.6 3.8 3.2 3.7 5.3 3.1 4.1 3.6 ...\n#&gt;  $ sodium               : num  136 140 140 141 136 140 139 138 140 138 ...\n#&gt;  $ potassium            : num  4.3 4.55 4.7 4.08 3.5 4.2 4.16 4.31 4.5 4.27 ...\n#&gt;  $ globulin             : num  2.9 2.9 2.8 2.3 3.4 3 3 2.9 2.9 2.6 ...\n#&gt;  $ calcium              : num  9.8 9.8 9.7 8.9 9.3 9.3 9.3 10.3 9.5 9.3 ...\n#&gt;  $ physical.work        : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ physical.recreational: chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ diabetes             : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n#&gt;  - attr(*, \"na.action\")= 'omit' Named int [1:885] 16 30 39 48 50 58 61 65 67 68 ...\n#&gt;   ..- attr(*, \"names\")= chr [1:885] \"27\" \"68\" \"90\" \"112\" ...\npred.y.new1 &lt;- predict(fit4, fictitious.data)\nsummary(pred.y.new1)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   128.7   178.9   190.6   192.5   203.3   557.4\n\nMeasuring prediction error\nContinuous outcomes\nR2\nThe Sum of Squares of Errors (SSE) and the Total Sum of Squares (SST) are calculated. The proportion of variance explained by the model is then calculated as R2.\n\n\nSee Wikipedia (2023a)\n\n# Find SSE\nSSE &lt;- sum( (obs.y - pred.y)^2 )\nSSE\n#&gt; [1] 3228460\n\n# Find SST\nmean.obs.y &lt;- mean(obs.y)\nSST &lt;- sum( (obs.y - mean.obs.y)^2 )\nSST\n#&gt; [1] 4256586\n\n# Find R2\nR.2 &lt;- 1- SSE/SST\nR.2\n#&gt; [1] 0.2415378\n\nrequire(caret)\nR2(pred.y, obs.y)\n#&gt; [1] 0.2415378\n\nRMSE\nThe Root Mean Square Error is calculated to measure the average magnitude of the errors between predicted and observed values.\n\n\nSee Wikipedia (2023b)\n\n# Find RMSE\nRmse &lt;- sqrt(SSE/(n-p)) \nRmse\n#&gt; [1] 35.21767\n\nRMSE(pred.y, obs.y)\n#&gt; [1] 35.02311\n\nAdj R2\nIt provides a measure of how well the model generalizes and adjusts R2 based on the number of predictors.\n\n\nSee Wikipedia (2023a)\n\n# Find adj R2\nadjR2 &lt;- 1-(1-R.2)*((n-1)/(n-p))\nadjR2\n#&gt; [1] 0.2333791\n\nWriting function\nSyntax for Writing Functions\n\nfunc_name &lt;- function (argument) {\n  A statement or multiple lines of statements\n  return(output)\n}\n\nExample of a simple function\n\nf1 &lt;- function(a,b){\n  result &lt;- a + b\n  return(result)\n}\nf1(a=1,b=3)\n#&gt; [1] 4\nf1(a=1,b=6)\n#&gt; [1] 7\n# setting default values\nf1 &lt;- function(a=1,b=1){\n  result &lt;- a + b\n  return(result)\n}\nf1()\n#&gt; [1] 2\nf1(b = 10)\n#&gt; [1] 11\n\nA bit more complicated\n\n# one argument\nmodel.fit &lt;- function(data.for.fitting){\n  formulax &lt;- as.formula(\"cholesterol~gender + age + born\")\n  fitx &lt;- lm(formulax, data = data.for.fitting)\n  result &lt;- coef(fitx)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#&gt; (Intercept)  genderMale         age  bornOthers \n#&gt; 184.3131838  -7.8095595   0.2225745  11.1557140\nmodel.fit(data.for.fitting=analytic3)\n#&gt; (Intercept)  genderMale         age  bornOthers \n#&gt; 176.1286576  -4.8256829   0.3375009   7.7186190\n\n\n# adding one more argument: digits\nmodel.fit &lt;- function(data.for.fitting, digits=2){\n  formulax &lt;- as.formula(\"cholesterol~gender + age + born\")\n  fitx &lt;- lm(formulax, data = data.for.fitting)\n  result &lt;- coef(fitx)\n  result &lt;- round(result,digits)\n  return(result)\n}\nmodel.fit(data.for.fitting=analytic)\n#&gt; (Intercept)  genderMale         age  bornOthers \n#&gt;      184.31       -7.81        0.22       11.16\nmodel.fit(data.for.fitting=analytic3)\n#&gt; (Intercept)  genderMale         age  bornOthers \n#&gt;      176.13       -4.83        0.34        7.72\n\nFunction that gives performance measures\nlet us create a function that will give us the performance measures:\n\nperform &lt;- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p &lt;- dim(model.matrix(model.fit))[2]\n  \n  # predicted value\n  pred.y &lt;- predict(model.fit, new.data)\n  \n  # sample size\n  n &lt;- length(pred.y)\n  \n  # outcome\n  new.data.y &lt;- as.numeric(new.data[,y.name])\n  \n  # R2\n  R2 &lt;- caret:::R2(pred.y, new.data.y)\n  \n  # adj R2 using alternate formula\n  df.residual &lt;- n-p\n  adjR2 &lt;- 1-(1-R2)*((n-1)/df.residual)\n  \n  # RMSE\n  RMSE &lt;-  caret:::RMSE(pred.y, new.data.y)\n  \n  # combine all of the results\n  res &lt;- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  \n  # returning object\n  return(res)\n}\nperform(new.data = analytic3, y.name = \"cholesterol\", model.fit = fit4)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 2632 29 0.242 0.233 35.023\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance.",
    "crumbs": [
      "Prediction ideas",
      "Overfitting and performance"
    ]
  },
  {
    "objectID": "predictivefactors5.html",
    "href": "predictivefactors5.html",
    "title": "Data Splitting",
    "section": "",
    "text": "This tutorial is focused on a crucial aspect of model building: splitting your data into training and test sets to avoid overfitting. Overfitting occurs when your model learns the noise in the data rather than the underlying trend. As a result, the model performs well on the training data but poorly on new, unseen data. To mitigate this, you often split your data.\nLoad data anf files\nInitially, several libraries are loaded to facilitate data manipulation and analysis.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nThen, previously saved dataset related to cholesterol and other factors is loaded for further use.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nData splitting to avoid model overfitting\nYou start by setting a random seed to ensure that the random splitting of data is reproducible. A specified function is then used to partition the data, taking as arguments the outcome variable (cholesterol level in this case) and the percentage of data that you want to allocate to the training set (70% in this example).\n\n\nKDnuggets (2023)\n\nKuhn (2023a)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use the createDataPartition function to split a dataset into training and testing datasets. The function will return the row indices that should go into the training set. These indices are stored in a variable, and its dimensions are displayed to provide an understanding of the size of the training set that will be created. Additionally, you can calculate what 70% of your entire dataset would look like to verify the approximation of the training data size, as well as what the remaining 30% (for the test set) would look like.\n\n\n\n# Using a seed to randomize in a reproducible way \nset.seed(123)\nsplit &lt;- createDataPartition(y = analytic3$cholesterol, p = 0.7, list = FALSE)\nstr(split)\n#&gt;  int [1:1844, 1] 3 4 5 8 9 13 14 16 20 21 ...\n#&gt;  - attr(*, \"dimnames\")=List of 2\n#&gt;   ..$ : NULL\n#&gt;   ..$ : chr \"Resample1\"\ndim(split)\n#&gt; [1] 1844    1\n\n# Approximate train data\ndim(analytic3)*.7 \n#&gt; [1] 1842.4   24.5\n\n# Approximate test data\ndim(analytic3)*(1-.7) \n#&gt; [1] 789.6  10.5\n\nSplit the data\nAfter determining how to partition the data, the next step is actually creating the training and test datasets. The indices are used to subset the original dataset into these two new datasets. The dimensions of each dataset are displayed to confirm their sizes.\n\n# Create train data\ntrain.data &lt;- analytic3[split,]\ndim(train.data)\n#&gt; [1] 1844   35\n\n# Create test data\ntest.data &lt;- analytic3[-split,]\ndim(test.data)\n#&gt; [1] 788  35\n\nOur next task is to fit the model (e.g., linear regression) on the training set and evaluate the performance on the test set.\nTrain the model\nOnce the training dataset is created, you can proceed to train the model using the training data. A previously defined formula containing the predictor variables is used in a linear regression model. After fitting the model, a summary is generated to display key statistics that help in evaluating the model’s performance.\n\nformula4\n#&gt; cholesterol ~ gender + age + born + race + education + married + \n#&gt;     income + diastolicBP + systolicBP + bmi + triglycerides + \n#&gt;     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium + physical.work + physical.recreational + \n#&gt;     diabetes\nfit4.train1 &lt;- lm(formula4, data = train.data)\nsummary(fit4.train1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = formula4, data = train.data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -91.973 -23.719  -1.563  20.586 178.542 \n#&gt; \n#&gt; Coefficients:\n#&gt;                             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                72.716792  59.916086   1.214  0.22504    \n#&gt; genderMale                -11.293629   2.136545  -5.286 1.40e-07 ***\n#&gt; age                         0.306235   0.066376   4.614 4.23e-06 ***\n#&gt; bornOthers                  7.220858   2.300658   3.139  0.00172 ** \n#&gt; raceHispanic               -6.727473   2.709718  -2.483  0.01313 *  \n#&gt; raceOther                  -4.865771   3.237066  -1.503  0.13298    \n#&gt; raceWhite                  -1.468522   2.494981  -0.589  0.55621    \n#&gt; educationHigh.School        1.626097   1.920289   0.847  0.39722    \n#&gt; educationSchool            -4.853095   3.585185  -1.354  0.17602    \n#&gt; marriedNever.married       -5.298265   2.332033  -2.272  0.02321 *  \n#&gt; marriedPreviously.married   1.202448   2.305191   0.522  0.60199    \n#&gt; incomeBetween.25kto54k     -1.736495   2.360385  -0.736  0.46202    \n#&gt; incomeBetween.55kto99k      0.170505   2.565896   0.066  0.94703    \n#&gt; incomeOver100k              1.712359   2.860226   0.599  0.54946    \n#&gt; diastolicBP                 0.355813   0.074380   4.784 1.86e-06 ***\n#&gt; systolicBP                  0.037464   0.059848   0.626  0.53140    \n#&gt; bmi                        -0.282881   0.139160  -2.033  0.04222 *  \n#&gt; triglycerides               0.123797   0.007613  16.261  &lt; 2e-16 ***\n#&gt; uric.acid                   1.006499   0.712871   1.412  0.15815    \n#&gt; protein                     1.721623   3.468969   0.496  0.61975    \n#&gt; bilirubin                  -6.143411   3.006858  -2.043  0.04118 *  \n#&gt; phosphorus                  0.093824   1.575489   0.060  0.95252    \n#&gt; sodium                     -0.604286   0.400694  -1.508  0.13170    \n#&gt; potassium                  -0.583525   2.715189  -0.215  0.82986    \n#&gt; globulin                   -0.278970   3.614404  -0.077  0.93849    \n#&gt; calcium                    15.679677   3.054968   5.133 3.17e-07 ***\n#&gt; physical.workYes           -1.099540   1.960321  -0.561  0.57494    \n#&gt; physical.recreationalYes    0.834737   1.953960   0.427  0.66928    \n#&gt; diabetesYes               -19.932101   2.580138  -7.725 1.83e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 34.68 on 1815 degrees of freedom\n#&gt; Multiple R-squared:  0.2433, Adjusted R-squared:  0.2316 \n#&gt; F-statistic: 20.84 on 28 and 1815 DF,  p-value: &lt; 2.2e-16\n\nExtract performance measures\nYou can use a saved function to measure the performance of the trained model. The function will return performance metrics like R-squared, RMSE, etc. This function is applied not just to the training data but also to the test data, the full dataset, and a separate, fictitious dataset.\n\n\n\n\n\n\nTip\n\n\n\nBelow we use the perform function that we saved to evaluate the model performances\n\n\n\nperform(new.data = train.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#&gt;         n  p df.residual     SSE     SST    R2 adjR2  sigma   logLik      AIC\n#&gt; [1,] 1844 29        1815 2182509 2884109 0.243 0.232 34.677 -9140.98 18341.96\n#&gt;           BIC\n#&gt; [1,] 18507.55\nperform(new.data = test.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#&gt;        n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#&gt; [1,] 788 29         759 1057454 1372214 0.229 0.201 37.326 -3955.936 7971.873\n#&gt;           BIC\n#&gt; [1,] 8111.958\nperform(new.data = analytic3,y.name = \"cholesterol\", model.fit = fit4.train1)\n#&gt;         n  p df.residual     SSE     SST    R2 adjR2 sigma    logLik      AIC\n#&gt; [1,] 2632 29        2603 3239962 4256586 0.239 0.231 35.28 -13098.82 26257.64\n#&gt;           BIC\n#&gt; [1,] 26433.91\nperform(new.data = fictitious.data,y.name = \"cholesterol\", model.fit = fit4.train1)\n#&gt;         n  p df.residual     SSE     SST    R2 adjR2  sigma    logLik      AIC\n#&gt; [1,] 4121 29        4092 5306559 6912485 0.232 0.227 36.011 -20601.92 41263.84\n#&gt;           BIC\n#&gt; [1,] 41453.55\n\nEvaluating the model’s performance on the test data provides insights into how well the model will generalize to new, unseen data. Comparing the performance metrics across different datasets can give you a robust view of your model’s predictive power and reliability.\n\n\nFor more on model training and tuning, see Kuhn (2023b)\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023a. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.\n\n\n———. 2023b. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.",
    "crumbs": [
      "Prediction ideas",
      "Data Splitting"
    ]
  },
  {
    "objectID": "predictivefactors6.html",
    "href": "predictivefactors6.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Cross-validation is another important technique used to assess the performance of machine learning models and mitigate the risk of overfitting. This tutorial focuses on k-fold cross-validation as a strategy to obtain a more generalized and robust assessment of the model’s performance. It shows both manual calculations for individual folds and an automated approach using the caret package. This ensures that you aren’t simply fitting your model well to a specific subset of your data but are achieving good performance in a general sense.\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nk-fold cross-vaildation\n\n\nSee Wikipedia (2023)\nWe can set the number of folds to 5 (k = 5). A random seed is used for reproducibility. We use the function createFolds to create the folds. The data is divided based on the cholesterol levels, with each fold having approximately equal numbers of data points. The resulting structure contains training indices for each fold.\nWe can also examine the approximate size of training and test sets for each fold. The dimensions are displayed to understand the partitioning, and you can examine the length of indices in each fold to confirm the size of the training sets.\n\nk = 5\ndim(analytic3)\n#&gt; [1] 2632   35\nset.seed(567)\n\n# Create folds (based on the outcome)\nfolds &lt;- createFolds(analytic3$cholesterol, k = k, list = TRUE, \n                     returnTrain = TRUE)\nmode(folds)\n#&gt; [1] \"list\"\n\n# Approximate training data size\ndim(analytic3)*4/5\n#&gt; [1] 2105.6   28.0\n\n# Approximate test data size\ndim(analytic3)/5  \n#&gt; [1] 526.4   7.0\n\nlength(folds[[1]])\n#&gt; [1] 2105\nlength(folds[[2]])\n#&gt; [1] 2107\nlength(folds[[3]])\n#&gt; [1] 2106\nlength(folds[[4]])\n#&gt; [1] 2105\nlength(folds[[5]])\n#&gt; [1] 2105\n\nstr(folds[[1]])\n#&gt;  int [1:2105] 1 3 5 6 8 10 11 12 13 14 ...\nstr(folds[[2]])\n#&gt;  int [1:2107] 1 2 3 4 5 6 7 8 9 12 ...\nstr(folds[[3]])\n#&gt;  int [1:2106] 2 4 5 7 8 9 10 11 12 14 ...\nstr(folds[[4]])\n#&gt;  int [1:2105] 1 2 3 4 6 7 8 9 10 11 ...\nstr(folds[[5]])\n#&gt;  int [1:2105] 1 2 3 4 5 6 7 9 10 11 ...\n\nCalculation for Fold 1\nThe first fold is used as an example. The indices for the training data in the first fold are extracted and used to subset the main data set into training and test sets for that fold. Then a linear regression model is fitted using the training data, and predictions are made on the test set. The model’s performance is evaluated using the same performance function as before.\n\nfold.index &lt;- 1\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1]  1  3  5  6  8 10\n\nfold1.train &lt;- analytic3[fold1.train.ids,]\nfold1.test &lt;- analytic3[-fold1.train.ids,]\nformula4\n#&gt; cholesterol ~ gender + age + born + race + education + married + \n#&gt;     income + diastolicBP + systolicBP + bmi + triglycerides + \n#&gt;     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium + physical.work + physical.recreational + \n#&gt;     diabetes\n\nmodel.fit &lt;- lm(formula4, data = fold1.train)\npredictions &lt;- predict(model.fit, newdata = fold1.test)\n\nperform(new.data=fold1.test, y.name = \"cholesterol\", \n        model.fit = model.fit)\n#&gt;        n  p df.residual      SSE      SST    R2 adjR2  sigma    logLik      AIC\n#&gt; [1,] 527 29         498 637317.5 830983.2 0.233  0.19 35.774 -2618.471 5296.942\n#&gt;           BIC\n#&gt; [1,] 5424.958\n\nCalculation for Fold 2\nThe same process is repeated for the second fold. This way, you can manually evaluate how the model performs on different subsets of the data, making the performance assessment more robust.\n\nfold.index &lt;- 2\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1] 1 2 3 4 5 6\n\nfold1.train &lt;- analytic3[fold1.train.ids,]\nfold1.test &lt;- analytic3[-fold1.train.ids,]\n\nmodel.fit &lt;- lm(formula4, data = fold1.train)\n\npredictions &lt;- predict(model.fit, newdata = fold1.test)\nperform(new.data=fold1.test, y.name = \"cholesterol\", \n        model.fit = model.fit)\n#&gt;        n  p df.residual    SSE      SST    R2 adjR2  sigma    logLik      AIC\n#&gt; [1,] 525 29         496 615243 785326.6 0.217 0.172 35.219 -2600.282 5260.564\n#&gt;           BIC\n#&gt; [1,] 5388.466\n\nUsing caret package to automate\n\n\nSee Kuhn (2023)\nInstead of manually running the process for each fold, the caret package can be used to automate k-fold cross-validation. A control object is set up specifying that 5-fold cross-validation should be used. Then, the train function from the caret package can be used to fit the linear regression model on each fold.\nAfter fitting, you can access summary results for each fold in the resampling results. This summary provides performance metrics such as R-squared for each fold. You can calculate the mean and standard deviation of these metrics to get an overall sense of the model’s performance.\nAdditionally, an adjusted R-squared can be calculated to consider the number of predictors in the model, giving a more accurate sense of the model’s explanatory power when you have multiple predictors.\n\n# Using Caret package\nset.seed(567)\n\n# make a 5-fold CV\nctrl&lt;-trainControl(method = \"cv\",number = 5)\n\n# fit the model with formula = formula4\n# use training method lm\nfit4.cv&lt;-train(formula4, trControl = ctrl,\n               data = analytic3, method = \"lm\")\nfit4.cv\n#&gt; Linear Regression \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 2106, 2105, 2106, 2105, 2106 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE     \n#&gt;   35.62758  0.2194187  27.85731\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# extract results from each test data \nsummary.res &lt;- fit4.cv$resample\nsummary.res\n\n\n  \n\n\nmean(fit4.cv$resample$Rsquared)\n#&gt; [1] 0.2194187\nsd(fit4.cv$resample$Rsquared)\n#&gt; [1] 0.02755561\n\n# # extract adj R2\n# k &lt;- 5\n# p &lt;- 2\n# n &lt;- round(nrow(analytic3)/k)\n# summary.res$adjR2 &lt;- 1-(1-fit4.cv$resample$Rsquared)*\n#  ((n-1)/(n-p))\n# summary.res\n\nReferences\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics).",
    "crumbs": [
      "Prediction ideas",
      "Cross-validation"
    ]
  },
  {
    "objectID": "predictivefactors7.html",
    "href": "predictivefactors7.html",
    "title": "Bootstrap",
    "section": "",
    "text": "The tutorial is on bootstrapping methods, mainly using R. Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling, with replacement (after a data point is chosen randomly from the original dataset and included in the sample, it is “replaced” back into the original dataset, making it possible for that same data point to be picked again in the same sampling process), from the observed data points. It is a way to quantify the uncertainty associated with a given estimator or statistical measure, such as the mean, median, variance, or correlation coefficient, among others. Bootstrapping is widely applicable and very straightforward to implement, which has made it a popular choice for statistical inference when analytical solutions are not available or are difficult to derive.\n\n\n\n\n\n\nImportant\n\n\n\nBootstrapping is a powerful statistical tool for making inferences by empirically estimating the sampling distribution of a statistic. It is especially useful when the underlying distribution is unknown or when an analytical solution is difficult to obtain.\n\n\n\n# Load required packages\nlibrary(caret)\nlibrary(knitr)\nlibrary(Publish)\nlibrary(car)\nlibrary(DescTools)\n\nLoad data\nLoad the data saved at the end of previous part of the lab.\n\nload(file=\"Data/predictivefactors/cholesterolNHANES15part2.RData\")\n\nResampling a vector\nHere, the document introduces basic resampling of a simple vector. The code creates a new sample using the sample function with replacement. It also discusses “out-of-bag” samples which are the samples not chosen during the resampling.\n\nfake.data &lt;- 1:5\nfake.data\n#&gt; [1] 1 2 3 4 5\n\n\nresampled.fake.data &lt;- sample(fake.data, size = length(fake.data), \n                              replace = TRUE)\nresampled.fake.data\n#&gt; [1] 2 1 1 3 3\n\nselected.fake.data &lt;- unique(resampled.fake.data)\nselected.fake.data\n#&gt; [1] 2 1 3\n\nfake.data[!(fake.data %in% selected.fake.data)]\n#&gt; [1] 4 5\n\nThe samples not selected are known as the out-of-bag samples\n\nB &lt;- 10\nfor (i in 1:B){\n  new.boot.sample &lt;- sample(fake.data, size = length(fake.data), \n                            replace = TRUE)\n  print(new.boot.sample)\n}\n#&gt; [1] 4 3 2 3 3\n#&gt; [1] 1 2 2 1 3\n#&gt; [1] 1 4 4 1 5\n#&gt; [1] 2 2 4 3 5\n#&gt; [1] 4 3 3 5 5\n#&gt; [1] 3 5 1 4 1\n#&gt; [1] 4 2 2 2 5\n#&gt; [1] 5 4 5 4 3\n#&gt; [1] 4 1 3 4 4\n#&gt; [1] 3 2 4 4 2\n\nCalculating SD of a statistics\nWe introduce the concept of calculating confidence intervals (CIs) using bootstrapping when the distribution of data is not known. It uses resampling to create multiple bootstrap samples, then calculates means and standard deviations (SD) for those samples.\nIdea:\n\nNot sure about what distribution is appropriate to make inference?\nIf that is the case, calculating CI is hard.\nresample and get a new bootstrap sample\ncalculate a statistic (say, mean) from that sample\nfind SD of those statistic (say, means)\nUse those SD to calculate CI\n\n\nmean(fake.data)\n#&gt; [1] 3\nB &lt;- 5\nresamples &lt;- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\nstr(resamples)\n#&gt; List of 5\n#&gt;  $ : int [1:5] 4 5 5 5 1\n#&gt;  $ : int [1:5] 5 2 5 1 3\n#&gt;  $ : int [1:5] 3 2 4 1 4\n#&gt;  $ : int [1:5] 4 3 5 5 5\n#&gt;  $ : int [1:5] 5 2 1 4 5\n\nB.means &lt;- sapply(resamples, mean)\nB.means\n#&gt; [1] 4.0 3.2 2.8 4.4 3.4\nmean(B.means)\n#&gt; [1] 3.56\n\n# SD of the distribution of means\nsd(B.means)\n#&gt; [1] 0.6387488\n\n\nmean(fake.data)\n#&gt; [1] 3\nB &lt;- 200\nresamples &lt;- lapply(1:B, function(i) sample(fake.data, \n                                            replace = TRUE))\n# str(resamples)\n\nB.means &lt;- sapply(resamples, mean)\nB.medians &lt;- sapply(resamples, median)\nmean(B.means)\n#&gt; [1] 3.053\n\n# SD of the distribution of means\nsd(B.means)\n#&gt; [1] 0.6155367\nmean(B.medians)\n#&gt; [1] 3.075\nhist(B.means)\n\n\n\n\n\n\n\n# SD of the distribution of medians\nsd(B.medians)\n#&gt; [1] 1.012175\nhist(B.medians)\n\n\n\n\n\n\n\nResampling a data or matrix\nWe show how to resample a data frame or a matrix, and how to identify which rows have been selected and which haven’t, introducing the concept of “out-of-bag samples” for matrices.\n\nanalytic.mini &lt;- head(analytic)\nkable(analytic.mini[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n1\n83732\nMale\n62\n\n\n2\n83733\nMale\n53\n\n\n10\n83741\nMale\n22\n\n\n16\n83747\nMale\n46\n\n\n19\n83750\nMale\n45\n\n\n21\n83752\nFemale\n30\n\n\n\n\n\n\nanalytic.boot &lt;- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n21\n83752\nFemale\n30\n\n\n2\n83733\nMale\n53\n\n\n21.1\n83752\nFemale\n30\n\n\n21.2\n83752\nFemale\n30\n\n\n21.3\n83752\nFemale\n30\n\n\n2.1\n83733\nMale\n53\n\n\n\n\nselected.subjects &lt;- unique(analytic.boot$ID)\nselected.subjects\n#&gt; [1] 83752 83733\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#&gt; [1] 83732 83741 83747 83750\n\n\nanalytic.boot &lt;- analytic.mini[sample(x = 1:nrow(analytic.mini), \n                                      size = nrow(analytic.mini), \n                                      replace = TRUE), ]\nkable(analytic.boot[,1:3])\n\n\n\n\nID\ngender\nage\n\n\n\n2\n83733\nMale\n53\n\n\n1\n83732\nMale\n62\n\n\n16\n83747\nMale\n46\n\n\n1.1\n83732\nMale\n62\n\n\n16.1\n83747\nMale\n46\n\n\n16.2\n83747\nMale\n46\n\n\n\n\nselected.subjects &lt;- unique(analytic.boot$ID)\nselected.subjects\n#&gt; [1] 83733 83732 83747\n\n# out-of-bag samples\nanalytic.mini$ID[!(analytic.mini$ID %in% selected.subjects)]\n#&gt; [1] 83741 83750 83752\n\nThe caret package / boot\nUsually B = 200 or 500 is recommended, but we will do 50 for the lab (to save time). We introduce the trainControl and train functions from the caret package. It sets up a linear model and demonstrates how bootstrapping can be done to estimate the variability in R-squared, a measure of goodness-of-fit for the model.\n\nset.seed(234)\nctrl&lt;-trainControl(method = \"boot\", number = 50)\nfit4.boot2&lt;-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2\n#&gt; Linear Regression \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Bootstrapped (50 reps) \n#&gt; Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared  MAE     \n#&gt;   35.58231  0.22375   27.77634\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2$resample)\n\n\n  \n\n\nmean(fit4.boot2$resample$Rsquared)\n#&gt; [1] 0.22375\nsd(fit4.boot2$resample$Rsquared)\n#&gt; [1] 0.01693917\n\nMethod boot632\nA specific bootstrapping method called “boot632”, which aims to reduce bias but can provide unstable results if the sample size is small. Compared to the original bootstrap method, boot632 addresses the bias that is due to this the sampling with replacement.\n\n\nSee Raschka (2023)\n\nctrl&lt;-trainControl(method = \"boot632\", number = 50)\nfit4.boot2b&lt;-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lm\")\nfit4.boot2b\n#&gt; Linear Regression \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Bootstrapped (50 reps) \n#&gt; Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE     \n#&gt;   35.33279  0.2277843  27.58945\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n\nhead(fit4.boot2b$resample)\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.2197801\nsd(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.02259778\n\nMethod boot632 for stepwise\nWe discuss the use of stepwise regression models in conjunction with the “boot632” method. It highlights the trade-offs and explains that models could be unstable depending on the data.\nA stable model\n\n\nSee Kuhn (2023)\nBias is reduced with 632 bootstrap, but may provide unstable results with a small samples size.\n\nctrl &lt;- trainControl(method = \"boot632\", number = 50)\nfit4.boot2b&lt;-train(formula4, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#&gt; Linear Regression with Stepwise Selection \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Bootstrapped (50 reps) \n#&gt; Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE     \n#&gt;   35.34494  0.2293058  27.65063\n\nhead(fit4.boot2b$resample)\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.2226174\nsd(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.01922833\n\nAn unstable model\n\nctrl&lt;-trainControl(method = \"boot632\", number = 50)\n\n# formula3 includes collinear variables\nfit4.boot2b&lt;-train(formula3, trControl = ctrl,\n                  data = analytic3, method = \"lmStepAIC\", \n                  trace = 0)\nfit4.boot2b\n#&gt; Linear Regression with Stepwise Selection \n#&gt; \n#&gt; 2632 samples\n#&gt;   25 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Bootstrapped (50 reps) \n#&gt; Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE    \n#&gt;   35.39802  0.2287758  27.6471\n\nhead(fit4.boot2b$resample)\n\n\n  \n\n\nmean(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.2205909\nsd(fit4.boot2b$resample$Rsquared)\n#&gt; [1] 0.0176326\n\nNote that SD should be higher for larger B.\nOptimism corrected bootstrap\nWe discuss a specific type of bootstrap called the “Optimism corrected bootstrap”. It’s a way to adjust performance metrics for the optimism that is often present when a model is tested on the data used to create it.\n\n\nSee Bondarenko and Consulting (2023)\nSteps:\n\nFit a model M to entire data D and estimate predictive ability R2.\nIterate from b=1 to B:\n\nTake a resample from the original data, and name it D.star\nFit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\nUse the bootstrap model M.star to get predictive ability on D, R2.fullData\n\n\nOptimism Opt is calculated as mean(R2.boot - R2.fullData)\nCalculate optimism corrected performance as R2-Opt.\n\n\nR2.opt &lt;- function(data, fit, B, y.name = \"cholesterol\"){\n  D &lt;- data\n  y.index &lt;- which(names(D)==y.name)\n  \n  # M is the model fit to entire data D\n  M &lt;- fit\n  pred.y &lt;- predict(M, D)\n  n &lt;- length(pred.y)\n  y &lt;- as.numeric(D[,y.index])\n  \n  # estimate predictive ability R2.\n  R2.app &lt;- caret:::R2(pred.y, y)\n  \n  # create blank vectors to save results\n  R2.boot &lt;- vector (mode = \"numeric\", length = B)\n  R2.fullData &lt;- vector (mode = \"numeric\", length = B)\n  opt &lt;- vector (mode = \"numeric\", length = B)\n  \n  # Iterate from b=1 to B\n  for(i in 1:B){    \n    # Take a resample from the original data, and name it D.star\n    boot.index &lt;- sample(x=rownames(D), size=nrow(D), replace=TRUE)\n    D.star &lt;- D[boot.index,]\n    M.star &lt;- lm(formula(M), data = D.star)\n    \n    # Fit the bootstrap model M.stat to D.star and get predictive ability, R2.boot\n    D.star$pred.y &lt;- predict(M.star, new.data = D.star)\n    y.index &lt;- which(names(D.star)==y.name)\n    D.star$y &lt;- as.numeric(D.star[,y.index])\n    R2.boot[i] &lt;- caret:::R2(D.star$pred.y, D.star$y)\n    \n    # Use the bootstrap model M.star to get predictive ability on D, R2_fullData\n    D$pred.y &lt;- predict(M.star, newdata=D)\n    R2.fullData[i] &lt;- caret:::R2(D$pred.y, y)\n    \n    # Optimism Opt is calculated as R2.boot - R2.fullData\n    opt[i] &lt;- R2.boot[i] - R2.fullData[i]\n  }\n  boot.res &lt;- round(cbind(R2.boot, R2.fullData,opt),2)\n  # Calculate optimism corrected performance as R2- mean(Opt).\n  R2.oc &lt;- R2.app - (sum(opt)/B)\n  return(list(R2.oc=R2.oc,R2.app=R2.app, boot.res = boot.res))\n}\n\nR2x &lt;- R2.opt(data = analytic3, fit4, B=50)\nR2x\n#&gt; $R2.oc\n#&gt; [1] 0.2238703\n#&gt; \n#&gt; $R2.app\n#&gt; [1] 0.2415378\n#&gt; \n#&gt; $boot.res\n#&gt;       R2.boot R2.fullData   opt\n#&gt;  [1,]    0.23        0.24 -0.01\n#&gt;  [2,]    0.24        0.23  0.01\n#&gt;  [3,]    0.26        0.24  0.03\n#&gt;  [4,]    0.25        0.23  0.02\n#&gt;  [5,]    0.26        0.24  0.02\n#&gt;  [6,]    0.26        0.23  0.03\n#&gt;  [7,]    0.21        0.24 -0.03\n#&gt;  [8,]    0.25        0.23  0.02\n#&gt;  [9,]    0.24        0.23  0.01\n#&gt; [10,]    0.27        0.23  0.03\n#&gt; [11,]    0.25        0.23  0.01\n#&gt; [12,]    0.24        0.23  0.01\n#&gt; [13,]    0.26        0.23  0.03\n#&gt; [14,]    0.25        0.24  0.02\n#&gt; [15,]    0.25        0.23  0.02\n#&gt; [16,]    0.24        0.23  0.00\n#&gt; [17,]    0.25        0.23  0.02\n#&gt; [18,]    0.26        0.24  0.03\n#&gt; [19,]    0.24        0.24  0.01\n#&gt; [20,]    0.27        0.24  0.03\n#&gt; [21,]    0.27        0.24  0.04\n#&gt; [22,]    0.26        0.23  0.02\n#&gt; [23,]    0.23        0.23  0.00\n#&gt; [24,]    0.23        0.23  0.00\n#&gt; [25,]    0.26        0.23  0.03\n#&gt; [26,]    0.26        0.23  0.03\n#&gt; [27,]    0.27        0.23  0.04\n#&gt; [28,]    0.27        0.24  0.03\n#&gt; [29,]    0.27        0.23  0.04\n#&gt; [30,]    0.24        0.23  0.00\n#&gt; [31,]    0.25        0.23  0.02\n#&gt; [32,]    0.25        0.24  0.02\n#&gt; [33,]    0.26        0.24  0.02\n#&gt; [34,]    0.23        0.24  0.00\n#&gt; [35,]    0.25        0.23  0.02\n#&gt; [36,]    0.26        0.23  0.03\n#&gt; [37,]    0.26        0.23  0.03\n#&gt; [38,]    0.23        0.24  0.00\n#&gt; [39,]    0.26        0.23  0.03\n#&gt; [40,]    0.27        0.23  0.03\n#&gt; [41,]    0.24        0.23  0.01\n#&gt; [42,]    0.24        0.24  0.00\n#&gt; [43,]    0.28        0.23  0.04\n#&gt; [44,]    0.25        0.24  0.02\n#&gt; [45,]    0.25        0.23  0.02\n#&gt; [46,]    0.26        0.24  0.02\n#&gt; [47,]    0.25        0.23  0.02\n#&gt; [48,]    0.25        0.23  0.02\n#&gt; [49,]    0.25        0.24  0.02\n#&gt; [50,]    0.23        0.23 -0.01\n\nBinary outcome\nHere, bootstrapping and cross-validation are used for a logistic regression model. It calculates the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), a measure for the performance of classification models.\n\nAUC from Receiver Operating Characteristic (ROC) = Measure of accuracy for classification models.\nAUC = 1 (perfect classification)\nAUC = 0.5 (random classification such as a coin toss)\n\n\nset.seed(234)\nformula5\n#&gt; cholesterol.bin ~ gender + age + born + race + education + married + \n#&gt;     income + diastolicBP + systolicBP + bmi + triglycerides + \n#&gt;     uric.acid + protein + bilirubin + phosphorus + sodium + potassium + \n#&gt;     globulin + calcium + physical.work + physical.recreational + \n#&gt;     diabetes\n\n# Bootstrap\nctrl&lt;-trainControl(method = \"boot\", \n                   number = 50, \n                   classProbs=TRUE,\n                   summaryFunction = twoClassSummary)\n\nfit5.boot&lt;-caret::train(formula5, \n                        trControl = ctrl,\n                        data = analytic3, \n                        method = \"glm\", \n                        family=\"binomial\",\n                        metric=\"ROC\")\nfit5.boot\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt;    2 classes: 'unhealthy', 'healthy' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Bootstrapped (50 reps) \n#&gt; Summary of sample sizes: 2632, 2632, 2632, 2632, 2632, 2632, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7238856  0.4563417  0.8201976\nmean(fit5.boot$resample$ROC)\n#&gt; [1] 0.7238856\nsd(fit5.boot$resample$ROC)\n#&gt; [1] 0.01166374\n\n# CV\nctrl &lt;- trainControl(method = \"cv\",\n                   number = 5,\n                   classProbs = TRUE, \n                   summaryFunction = twoClassSummary)\n\nfit5.cv &lt;- train(formula5, \n               trControl = ctrl,\n               data = analytic3, \n               method = \"glm\", \n               family=\"binomial\",\n               metric=\"ROC\")\nfit5.cv\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 2632 samples\n#&gt;   22 predictor\n#&gt;    2 classes: 'unhealthy', 'healthy' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 2106, 2106, 2105, 2105, 2106 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7291594  0.4512144  0.8253358\nfit5.cv$resample\n\n\n  \n\n\nmean(fit5.cv$resample$ROC)\n#&gt; [1] 0.7291594\nsd(fit5.cv$resample$ROC)\n#&gt; [1] 0.02683386\n\nBrier Score is another metric for evaluating the performance of binary classification models. Brier Score is equivalent to the mean squared error, which we calculate for a continuous outcome. A Brier score of 0 indicates perfect accuracy and a score of 1 indicates perfect inaccuracy.\n\nrequire(DescTools)\nfit5 &lt;- glm(formula5, family = binomial(), data = analytic3)\nBrierScore(fit5)\n#&gt; [1] 0.1998676\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nBondarenko, Vadim, and FI Consulting. 2023. “The Bootstrap Approach to Managing Model Uncertainty.” https://rstudio-pubs-static.s3.amazonaws.com/90467_c70206f3dc864d53bf36072207ee011d.html.\n\n\nKuhn, Max. 2023. “Available Models.” https://topepo.github.io/caret/available-models.html.\n\n\nRaschka, Sebastian. 2023. “Bootstrap_point632_score: The .632 and .632+ Boostrap for Classifier Evaluation.” 2023. https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/.",
    "crumbs": [
      "Prediction ideas",
      "Bootstrap"
    ]
  },
  {
    "objectID": "predictivefactorsF.html",
    "href": "predictivefactorsF.html",
    "title": "R functions (P)",
    "section": "",
    "text": "The list of new R functions introduced in this Predictive factors lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\naggregate\nbase/stats\nTo see summary by groups, e.g., by gender\n\n\nanova\nbase/stats\nTo compare models\n\n\nauc\npROC\nTo compute the AUC (area under the ROC curve) value\n\n\nBrierScore\nDescTools\nTo calculate the Brier score\n\n\ncoef\nbase/stats\nTo see the coefficients of a fitted model\n\n\ncor\nbase/stats\nTo see the correlation between numeric variables\n\n\ncorrplot\ncorrplot\nTo visualize a correlation matrix\n\n\ncreateDataPartition\ncaret\nTo split a dataset into training and testing sets\n\n\ncreateFolds\ncaret\nTo create k folds based on the outcome variable\n\n\ncrPlots\ncar\nTo see partial residual plot\n\n\ndescribeBy\npsych\nTo see summary by groups, e.g., by gender\n\n\nglm\nbase/stats\nTo run generalized linear models\n\n\ngroup_by\ndplyr\nTo group by variables\n\n\nhat\nbase/stats\nTo return a hat matrix\n\n\nifelse\nbase\nTo set an condition, e.g., creating a categorical variable from a numerical variable based on a condition\n\n\nkable\nknitr\nTo create a nice table\n\n\nlayout\nbase/graphics\nTo specify plot arrangement\n\n\nlines\nbase/graphics\nTo draw a line graph\n\n\nlm\nbase/stats\nTo fit a linear regression\n\n\nlowess\nbase/stats\nTo smooth a scatter plot\n\n\nmodel.matrix\nbase/stats\nTo construct a design/model matrix, e.g., a matrix with covariate values\n\n\nols_plot_resid_lev\nolsrr\nTo visualize the residuals vs leverage plot\n\n\nols_vif_tol\nolsrr\nTo calculate tolerance and variance inflation factor\n\n\npredict\nbase/stats\n`predict` is a generic function that is used for prediction, e.g., predicting probability of an event from a model\n\n\nR2\ncaret\nTo calculate the R-squared value\n\n\nRMSE\ncaret\nTo calculate the RMSE value\n\n\nroc\npROC\nTo build a ROC curve\n\n\nsample\nbase\nTo take/draw random samples with or without replacement\n\n\nsave.image\nbase\nTo save an R object\n\n\nspearman2\nHmisc\nTo compute the square of Spearman's rank correlation\n\n\nsummarize\ndplyr\nTo see summary\n\n\ntapply\nbase\nTo apply a function over an array, e.g., to see the summary of a variable by gender\n\n\ntrain\ncaret\nTo fit the model with tuning hyperparameters\n\n\ntrainControl\ncaret\nTo tune the hyperparameters, i.e., controlling the parameters to train the model\n\n\nvarclus\nHmisc\nWe use the `varclus` function to identify collinear predictors with cluster analysis\n\n\nvif\ncar\nTo calculate variance inflation factor\n\n\nwhich\nbase\nTo see which indices are TRUE",
    "crumbs": [
      "Prediction ideas",
      "R functions (P)"
    ]
  },
  {
    "objectID": "predictivefactorsQ.html",
    "href": "predictivefactorsQ.html",
    "title": "Quiz (P)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Prediction ideas",
      "Quiz (P)"
    ]
  },
  {
    "objectID": "predictivefactorsQ.html#live-quiz",
    "href": "predictivefactorsQ.html#live-quiz",
    "title": "Quiz (P)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Prediction ideas",
      "Quiz (P)"
    ]
  },
  {
    "objectID": "predictivefactorsQ.html#download-quiz",
    "href": "predictivefactorsQ.html#download-quiz",
    "title": "Quiz (P)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Prediction ideas",
      "Quiz (P)"
    ]
  },
  {
    "objectID": "surveydata.html",
    "href": "surveydata.html",
    "title": "Survey data analysis",
    "section": "",
    "text": "Background\nThe chapter consists of a series of tutorials focused on conducting rigorous analyses of complex survey data, mainly using Canadian Community Health Survey (CCHS) and National Health and Nutrition Examination Survey (NHANES) datasets. The tutorials guide users through various stages of survey data analysis: formulating research questions via the PICOT framework, data preparation, quality assessment, and handling missing data. They cover both bivariate and multivariable statistical methods, such as logistic and linear regressions, emphasizing the need to account for complex survey design elements like weights, strata, and clusters to avoid biased estimates. Advanced statistical techniques like backward elimination and interaction effect assessments are also discussed. Predictive model performance is evaluated using metrics like AIC, pseudo R-squared, and ROC curves, along with specialized tests like Archer and Lemeshow Goodness of Fit. The tutorials serve as a comprehensive guide for anyone looking to delve deep into the intricacies of analyzing complex survey data effectively.",
    "crumbs": [
      "Survey data analysis"
    ]
  },
  {
    "objectID": "surveydata.html#background",
    "href": "surveydata.html#background",
    "title": "Survey data analysis",
    "section": "",
    "text": "The foundation we’ve built in understanding various research questions, especially the distinction between causal and predictive inquiries, will be instrumental in our next phase. Survey data, with its rich and diverse information, often presents opportunities to address both causal and predictive questions. By leveraging the knowledge we’ve garnered about the intricacies of causality and the methodologies of prediction, we’ll be better equipped to extract meaningful insights from nationally representative survey data. This holistic approach ensures that we not only comprehend the underlying theories but also effectively apply them in real-world contexts, making our analysis robust, relevant, and impactful.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Survey data analysis"
    ]
  },
  {
    "objectID": "surveydata.html#overview-of-tutorials",
    "href": "surveydata.html#overview-of-tutorials",
    "title": "Survey data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCCHS: Revisiting PICOT\nThe tutorial focuses on revisiting a research question concerning the relationship between osteoarthritis (OA) and cardiovascular disease (CVD) in Canadian adults, utilizing data from the Canadian Community Health Survey (CCHS) spanning from 2001 to 2005. The approach follows the PICOT framework, which specifies the target population, outcome, exposure and control groups, and timeline. The tutorial provides detailed steps for data preparation and analysis, from loading the necessary R packages to subsetting data based on a comprehensive set of variables like age, sex, marital status, and income among others. The variables are recoded into broader categories for easier analysis. The tutorial then combines different cycles of CCHS data into one comprehensive dataset. Potential confounders are also identified to better understand the relationship between OA and CVD.\n\n\nCCHS: Assessing data\nThis tutorial provides a comprehensive guide to data preparation and quality assessment. It emphasizes the importance of checking for missing data and visualizing it, creating summary tables to look for zero-cells in variables, and generating frequency tables for various variables to examine data distribution. Specific attention is given to the presence of problematic variables. Data dictionaries from different cycles are also consulted to ensure variable compatibility. After identifying and modifying problematic data, the tutorial also explains how to set appropriate reference levels for factors in the dataset and offers an option to create a new dataset that excludes missing values (although this is not generally recommended).\n\n\nCCHS: Bivariate analysis\nThis tutorial outlines how to examine relationships between two variables using R. The tutorial covers data preparation steps such as accumulating survey weights across cycles. It also highlights the handling of missing data and survey design specifications for weighted analyses. Descriptive weighted statistics are generated in tables, stratified by exposure and outcome, to provide insights for survey weighted logistic regression analysis. Additionally, proportions and design effects are calculated to account for the complex survey design’s impact on statistical estimates. The tutorial employs specialized chi-square tests, such as, Rao-Scott and Thomas-Rao modifications, to assess associations between variables, accounting for the survey’s complex design.\n\n\nCCHS: Regression analysis\nThis tutorial offers a comprehensive guide on conducting complex regression analyses on survey data using R. The tutorial starts by conducting basic data checks. It then performs both simple and multivariable logistic regression to explore the relationship between cardiovascular disease and osteoarthritis. Model fit is assessed using Akaike Information Criterion (AIC) and pseudo R-squared metrics. Variable selection techniques such as backward elimination and stepwise regression guided by AIC are applied to hone the model. The tutorial also delves into assessing interaction effects among variables like age, sex, and diabetes, incorporating significant interactions into the final model.\n\n\nCCHS: Model performance\nThe tutorial guides users through the process of evaluating logistic regression models fitted to complex survey data in R, focusing primarily on the Receiver Operating Characteristic (ROC) curves and Archer and Lemeshow Goodness of Fit tests. It introduces a specialized function for plotting ROC curves and calculating the Area Under the Curve (AUC) to gauge the model’s predictive accuracy, while taking survey weights into account. Grading guidelines for AUC values are provided for model discrimination quality. For model fit, the Archer and Lemeshow test is used. The tutorial also covers additional functionalities for dealing with strata and clusters in the survey data.\n\n\nNHANES: Predicting blood pressure\nThe tutorial provides a comprehensive guide for analyzing health survey data with a focus on how demographic factors like race, age, gender, and marital status relate to blood pressure levels using NHANES dataset. The tutorial constructs both bivariate and multivariate regression models. Additionally, the tutorial incorporates complex survey designs by creating a new survey design object that factors in sampling weight, strata, and clusters. It also generates box plots and summary statistics to visualize variations in blood pressure across different demographic groups, considering survey design. The tutorial emphasizes the importance of accounting for survey design features to avoid biased estimates and discusses the challenges of model overfitting and optimism when shifting from inference to prediction, recommending optimism-correction techniques.\n\n\nNHANES: Predicting cholesterol level\nIn the study using NHANES data, the goal was to predict cholesterol levels in adults based on various predictors such as gender, country of birth, race, education, marital status, income, BMI, and diabetes. The data was filtered to include only adults 18 years and older, and multiple statistical tests were conducted. Linear regression and logistic regression models were fitted, with results suggesting an association between gender and cholesterol level. Various statistical tests, including Wald tests and backward elimination, were employed to optimize the model. The study found that income was not a significant predictor for cholesterol levels, and interaction terms did not improve the model. Despite utilizing survey design features, the model had poor discriminatory power. However, Archer-Lemeshow Goodness of Fit test showed that the model fits the data well. The inclusion of age as an additional predictor led to different odds ratios, and the AIC value suggested that adding age improved the model.\n\n\nNHANES: Properly subsetting a design object\nThe tutorial provides a comprehensive guide on how to handle and analyze a subset of complex survey data from the NHANES study using R. It begins by checking for missing data. The focus is on subsetting data based on complete information, emphasizing the importance of accounting for the full complex survey design to obtain unbiased variance estimates. Logistic regression is then run on this subset, with the tutorial explicitly differentiating between correct and incorrect approaches to consider the survey design. Finally, variable selection methods like backward elimination are discussed to determine significant predictors, emphasizing the retention of variables deemed important based on prior research.\n\n\nNHANES: Reliability Standards\nThis tutorial guides users in replicating key tables from a published article using NHANES data, emphasizing NCHS/CDC reliability standards for complex survey analyses. At first, it created a survey design object with weights, strata, and clusters. For Table 1, the svytable1 package generates stratified descriptive summaries in “mixed mode” (unweighted N alongside weighted percentages), stratified by race/ethnicity for overall, male, and female subgroups, with automatic suppression (*) of unreliable estimates based on established rules, ensuring transparency on precision and design effects. Table 3 is reproduced via gender-stratified svyglm logistic regressions, incorporating diagnostics from svyglmdiag() (SE, p-values, CIs) and VIF checks for multicollinearity; results are formatted into a publication-ready table showing odds ratios and 95% CIs.\n\n\nNHANES: Model performance\nThis tutorial demonstrates how to evaluate a survey-weighted logistic regression model in R using NHANES data. It guides the user through preparing the data, fitting a design-based logistic regression model to predict obesity from demographic variables, and then assessing its performance. The evaluation uses two primary methods: calculating a design-correct Area Under the Curve (AUC) to measure predictive accuracy and running an Archer-Lemeshow test to assess goodness-of-fit.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Survey data analysis"
    ]
  },
  {
    "objectID": "surveydata0.html",
    "href": "surveydata0.html",
    "title": "Concepts (D)",
    "section": "",
    "text": "Survey Data Analysis\nDesign-based analysis differs from model-based analysis in its approach to handling survey data. Design-based analysis emphasizes the importance of the survey’s sampling method and structure, focusing on representativeness and accurate variance estimation according to how the data was collected. It accounts for the complexities of the sampling design, e.g., stratification and clustering, to ensure that results are representative of the entire population. On the other hand, model-based analysis uses statistical models to understand relationships and patterns, assuming data come from a specific distribution and often relying on random sampling.\nUnderstanding survey features such as weights, strata, and clusters is crucial in complex survey data analysis. Survey weights adjust for unequal probabilities of selection and nonresponse, ensuring that the sample represents the population accurately. Stratification improves precision and representation of subgroups, while clustering, often used for practicality and cost considerations, must be accounted for to avoid underestimating standard errors. These features are vital in design-based analysis to provide unbiased, reliable estimates and are what fundamentally distinguish it from model-based approaches, which may not reflect the difficulties of complex survey structures. NHANES is used an an example to explain these ideas.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#reading-list",
    "href": "surveydata0.html#reading-list",
    "title": "Concepts (D)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Steven G. Heeringa, West, and Berglund 2017) (chapters 2 and 3)\nOptional reading: (Steven G. Heeringa, West, and Berglund 2014)\nTheoretical references (optional):\n\nF/chi-squared statistic with the Rao-Scott second-order correction (Rao and Scott 1984; Koch, Freeman Jr, and Freeman 1975; Thomas and Rao 1987)\nAIC and BIC for modeling with complex survey data (Lumley and Scott 2015)\nPseudo-R2 statistics under complex sampling (Lumley 2017)\nTests for regression models fitted to survey data (Lumley and Scott 2014)\nGoodness-of-fit test for a logistic regression model fitted using survey sample data (Archer and Lemeshow 2006)",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#video-lessons",
    "href": "surveydata0.html#video-lessons",
    "title": "Concepts (D)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nSurvey Data Analysis\n\n\n\nLarge-scale health and social surveys are invaluable resources for research, providing nationally representative data that informs public policy and scientific discovery. However, their sophisticated, multi-stage sampling designs render them unsuitable for analysis with standard statistical methods that assume a simple random sample (SRS). Treating complex survey data as an SRS is a critical error that can lead to profoundly incorrect conclusions, such as underestimated standard errors and deceptively small p-values, producing a distorted and unreliable picture of the population under study.\nThis guide serves as a comprehensive, theoretical roadmap for students and researchers navigating the unique landscape of complex survey data analysis. It is designed as a self-contained tutorial, focusing on the conceptual underpinnings required for valid analysis, with a special emphasis on the U.S. National Health and Nutrition Examination Survey (NHANES) as a practical example. The material is structured to build knowledge progressively, from the foundational theory of survey design to the application of advanced statistical models. By understanding that the design of the survey must be integrated into every step of the analysis, the reader will be equipped to produce scientifically rigorous and defensible conclusions.\n\n\nThe Landscape of Survey Data\n\n1.1. Examples of Major Complex Surveys\nIn public health, epidemiology, and the social sciences, our understanding of population-level phenomena is often built upon data from large-scale surveys. Prominent examples include :\n\nNational Health and Nutrition Examination Survey (NHANES): A program of studies to assess the health and nutritional status of adults and children in the United States, unique for combining interviews with physical examinations.\nCanadian Community Health Survey (CCHS): A cross-sectional survey that collects information related to health status, health care utilization, and health determinants for the Canadian population.\nBehavioral Risk Factor Surveillance System (BRFSS): The world’s largest, continuously conducted telephone health survey system, tracking health conditions and risk behaviors in the United States.\nHealth and Retirement Study (HRS): A longitudinal panel study that surveys a representative sample of Americans over the age of 50 to inform research on aging.\nEuropean Social Survey (ESS): A cross-national survey conducted across Europe that measures attitudes, beliefs, and behavior patterns of diverse populations.\n\nEach of these surveys employs a complex design, making the principles discussed in this guide broadly applicable. We will focus on NHANES to illustrate these concepts in detail.\n\n\n1.2. The NHANES Sampling Procedure\nThe National Health and Nutrition Examination Survey (NHANES) does not use a simple random sample. Instead, it employs a complex, multistage, probability sampling design to select participants who are representative of the civilian, non-institutionalized U.S. population. This multi-stage process is a practical necessity to balance logistical efficiency with statistical rigor. The procedure involves four main stages :\n\nStage 1: Primary Sampling Units (PSUs): The U.S. is divided into geographically defined PSUs, which are typically counties or groups of contiguous counties. These PSUs are grouped into strata based on characteristics like geography and minority population density. From each stratum, a sample of PSUs is selected, usually with a probability proportional to its population size.\nStage 2: Segments: Each selected PSU is further divided into smaller geographic areas called segments (e.g., city blocks). A sample of these segments is then drawn, again often with probability proportional to size.\nStage 3: Households: Within each selected segment, a list of all housing units is compiled, and a sample of households is randomly selected.\nStage 4: Individuals: Finally, within each selected household, individuals are randomly chosen from a list of all household members based on specific age, sex, and race/ethnicity screening criteria.\n\nThis hierarchical process means that data collection is concentrated in a limited number of geographic areas (the sampled PSUs), which is far more cost-effective than traveling to households scattered randomly across the entire nation.\n\n\n1.3. The Three Pillars of Complex Sampling\nAs discussed previously, Complex survey designs are built upon three core methodological pillars: stratification, clustering, and weighting. Each serves a distinct purpose, and together they form the intricate architecture that must be understood and accounted for in any valid analysis.\n\nStratification: This is the process of partitioning the target population into mutually exclusive subgroups (strata) before sampling begins. Sampling is then performed independently within each stratum. The primary purpose of stratification is to increase the statistical precision of survey estimates and to ensure that key subgroups of interest are adequately represented in the sample.\n\nApplication in NHANES: NHANES stratifies its primary sampling units (PSUs) based on geography and the proportions of minority populations to ensure that the sample accurately reflects the nation’s demographic diversity and allows for reliable subgroup analyses.\n\nClustering: This is a sampling technique used primarily to improve logistical efficiency and reduce data collection costs. Instead of sampling individuals directly, the process involves sampling natural groupings (clusters) first, such as counties or city blocks.\n\nApplication in NHANES: The first two stages of NHANES sampling (selecting counties and then segments within those counties) are examples of clustering. This design makes it feasible to deploy the Mobile Examination Centers (MECs) where physical examinations are conducted.\n\nWeighting: In a complex survey, individuals have unequal probabilities of selection. Survey weights are created to compensate for this. A respondent’s final survey weight can be conceptually understood as the number of people in the target population that he or she represents.\n\nApplication in NHANES: NHANES weights are crucial for generating unbiased national estimates. They are constructed in a multi-step process to account for the complex design :\n\nBase Weight: This is the inverse of the individual’s probability of selection, accounting for all stages of the sampling design. This step is necessary because of the deliberate oversampling of certain subgroups to increase the reliability of estimates for these groups. For example, NHANES often oversamples older adults, low-income individuals, and specific racial/ethnic minorities like African Americans and Mexican Americans.\nNonresponse Adjustment: The weights are adjusted to account for individuals who were selected but did not participate in the survey (either the interview or the exam). This reduces potential bias if non-respondents differ systematically from respondents.\nPost-stratification: The weights are further adjusted so that the weighted sample’s demographic totals (e.g., by age, sex, and race/ethnicity) match known population totals from an independent source like the U.S. Census Bureau. This final calibration helps correct for any remaining discrepancies between the sample and the target population.\n\n\n\n\n\n1.4. The Design Effect (DEFF): Why It Still Matters\nThe cumulative statistical impact of stratification, clustering, and unequal weighting is captured by the Design Effect (DEFF). It is formally defined as the ratio of the variance of an estimate obtained from the complex survey to the variance of the same estimate that would have been obtained from a simple random sample (SRS) of the exact same size.\n\\[DEFF = \\frac{Var(\\hat{\\theta})_{complex}}{Var(\\hat{\\theta})_{srs}}\\]\nEven with modern software that directly calculates correct variances, the DEFF remains a valuable concept for several reasons:\n\nInterpretation: The DEFF provides a simple, intuitive metric for understanding the efficiency of the survey design for a particular estimate. A DEFF of 2.0, for instance, implies that the variance of the estimate is twice as large as it would have been under SRS, meaning the confidence interval is about 41% wider (\\(\\sqrt{2} \\approx 1.41\\)).\nEffective Sample Size: The DEFF allows you to calculate the “effective sample size” (\\(n_{eff} = n / DEFF\\)), which is the size of an SRS that would yield the same level of precision. This is a powerful tool for communicating the statistical power of a study.\nSurvey Planning: In the design phase of a new survey, estimated DEFFs from previous, similar surveys are crucial for calculating the required sample size to achieve a desired level of precision.\n\nIn essence, while software performs the complex calculations, the DEFF provides the conceptual understanding and interpretive context for the results.\n\nTLDR\n\nComplex surveys like NHANES do not use simple random sampling. They use a multi-stage design involving stratification (dividing the population into groups) and clustering (sampling geographic areas) for efficiency. This creates unequal selection probabilities, which are corrected by survey weights. The Design Effect (DEFF) is a key metric that quantifies how much the complex design increases the variance of an estimate compared to a simple random sample.\n\n\n\n\n\n\nThe Perils of Naive Analysis\n\n2.1. The Breakdown of the I.I.D. Assumption\nStandard statistical methods assume that observations are Independent and Identically Distributed (I.I.D.). Complex survey designs systematically violate both components of this assumption.\n\nViolation of Independence: Cluster sampling directly violates independence. People living in the same neighborhood (cluster) are more similar to each other than randomly selected individuals, a phenomenon known as intra-cluster correlation. Standard formulas fail to account for this redundancy and therefore underestimate the true variability in the population.\nViolation of Identical Distribution: Stratification and the deliberate oversampling of certain groups mean that individuals have unequal probabilities of selection. Therefore, observations are not “identically distributed.” Survey weights are introduced to correct for this, but their very existence signals a violation of the I.I.D. assumption.\n\n\n\n2.2. Inappropriate Analysis in the Literature\nThe availability of public-use survey datasets to researchers who may lack adequate training contributes to methodological issues and analytic errors in published literature. A 2013 study reviewing articles that used the Korean National Health and Nutrition Examination Survey (KNHANES) found that a substantial portion of publications failed to properly account for the complex survey design, potentially leading to biased results and incorrect conclusions. This problem is not unique to one survey. The issue is widespread enough that checklists, such as the proposed Preferred Reporting Items for Complex Sample Survey Analysis (PRICSSA) (Seidenberg, Moser, and West 2023), have been developed to guide researchers and improve the quality of reporting. The root cause is often simple ignorance of the correct methods, perpetuated by the publication of flawed examples.\n\nCommon Pitfall: Ignoring the Survey Design The most frequent and dangerous error is to analyze complex survey data as if it were from a simple random sample. This ignores the clustering, stratification, and weighting, leading to incorrect point estimates and, most critically, underestimated standard errors. The result is an analysis prone to overstated significance and an increased risk of Type I errors (false positives). Researchers may report significant associations that are, in reality, merely artifacts of incorrect statistical analysis.\n\n\nTLDR\n\nStandard statistical tests assume data are I.I.D. (independent and identically distributed). Complex surveys violate this assumption through clustering (violating independence) and unequal selection probabilities (violating identical distribution). Ignoring the survey design leads to biased results and underestimated standard errors, a common problem in published research that can produce false-positive findings.\n\n\n\n\n\n\nThe Toolkit for Valid Inference\n\n3.1. The Theory of Variance Estimation for Complex Surveys\nBecause standard formulas for variance are invalid for complex survey data, analysts must use specialized techniques. There are two major families of methods for this purpose: Taylor Series Linearization and Replication Methods.\n\nTaylor Series Linearization (TSL): Also known as the delta method, TSL is an analytical approach that approximates a complex, non-linear statistic (like a regression coefficient) with a simpler linear function. The variance of this linear approximation, which correctly accounts for stratification and clustering, serves as an estimate for the variance of the original statistic. This is the default method in many software packages and requires the strata and cluster (PSU) identifiers for each observation to be specified.\n\nApplication in NHANES: To use TSL with NHANES data, an analyst must specify the masked variance pseudo-stratum (SDMVSTRA) and pseudo-PSU (SDMVPSU) variables provided in the public-use data files.\n\nReplication Methods: These methods take an empirical approach. The idea is to repeatedly draw subsamples (“replicates”) from the full sample in a way that mimics the original design. The statistic of interest is calculated for each replicate, and the variance of the full-sample estimate is determined by the variability of the estimates across these replicates. This approach requires a set of pre-calculated “replicate weights” instead of the strata and PSU variables. Common replication techniques include :\n\nBalanced Repeated Replication (BRR): Typically used for designs with two PSUs per stratum. It creates replicates by selecting one of the two PSUs from each stratum according to a balanced scheme.\nJackknife Repeated Replication (JRR): Creates replicates by successively deleting one PSU at a time from the sample and up-weighting the remaining PSUs in that stratum.\nFay’s BRR: A modification of BRR that retains all cases in each replicate but perturbs the weights by a factor k (where 0 &lt; k &lt; 1) instead of dropping cases entirely. This method can be more stable, especially for statistics like quantiles.\nBootstrap Methods: Survey-specific bootstrap methods, such as the Rao-Wu bootstrap, create replicates by resampling PSUs within each stratum.\n\n\n\n\n3.2. Hypothesis Testing in the Design-Based Framework\nHypothesis testing procedures must also be adjusted for complex survey data.\n\nDesign-Adjusted t-tests: The standard t-statistic formula is used, but the “Estimate” is calculated using survey weights, and the “Standard Error” in the denominator is a design-based SE from TSL or replication. Furthermore, the degrees of freedom are based on the number of PSUs and strata, not the number of individuals, which appropriately reflects the reduced amount of independent information in a clustered sample.\nThe Rao-Scott Chi-Square Test: The standard Pearson’s chi-squared test is invalid for complex survey data because it does not follow a standard chi-squared distribution. The Rao-Scott chi-square test was developed to correct this problem by adjusting the standard Pearson’s statistic to account for the survey’s design effect. An F-test version of this correction is often preferred as it provides a more accurate adjustment, especially when design effects vary across the cells of a contingency table.\n\n\nTLDR\n\nTo get correct standard errors, use specialized variance estimation methods. Taylor Series Linearization (TSL) is an analytical approach requiring strata and PSU variables. Replication Methods (like BRR, JRR, and Bootstrap) are an empirical approach requiring pre-calculated replicate weights. Standard hypothesis tests like the t-test and chi-square test must be replaced with their design-adjusted counterparts (e.g., Rao-Scott test).\n\n\n\n\n\n\nMultivariable Regression with Survey Data\n\n4.1. Principles of Fitting Regression Models\nWhen fitting regression models to complex survey data, the estimation procedures must be adapted. This is typically achieved through pseudo-maximum likelihood estimation (PMLE), which incorporates the survey weights into the likelihood function. This ensures that the resulting coefficient estimates are representative of the target population.\nWhile weights are used to obtain unbiased point estimates, the most critical adjustment is in the estimation of their variance. After the coefficients are estimated, a design-based method (TSL or replication) is used to calculate the variance-covariance matrix. From this correctly calculated matrix, valid standard errors, confidence intervals, and hypothesis tests (e.g., Wald tests) for each predictor are derived.\n\n\n4.2. A Word of Caution About “Weights”!\nWhile there is universal agreement that weights are necessary for descriptive statistics (e.g., population means or prevalences), their use in regression models is more nuanced and has been a subject of debate.\n\nThe Argument for Using Weights: The primary goal of most public health research using national survey data is to make inferences about the target population. In this case, weights must be used to obtain unbiased estimates of the population-level associations. Failing to do so can lead to biased coefficients if the sampling probabilities are related to the outcome variable.\nThe Argument Against Using Weights: Some argue that using weights can reduce precision (i.e., inflate standard errors), especially if the weights are highly variable. They contend that if a regression model is perfectly specified (includes all relevant predictors), the unweighted estimates may be more efficient.\nPractical Recommendation: In practice, one can never be certain that a model is perfectly specified. Therefore, the widely accepted best practice is to use the survey weights in regression models as the default approach. This prioritizes the avoidance of bias. A prudent strategy is to fit both weighted and unweighted models as a sensitivity analysis. If the coefficients differ substantially, it suggests the weights are correcting for significant bias, and the weighted results should be considered primary.\n\n\n\n4.3. Assessing Model Fit and Performance\nTools for assessing model performance must also be adapted for complex survey data.\n\nGoodness-of-Fit: For logistic regression, the standard Hosmer-Lemeshow test is invalid. The Archer-Lemeshow goodness-of-fit test is the design-based analogue that correctly accounts for the survey design. A non-significant p-value from this test suggests the model fits the data adequately.\nPredictive Performance: Measures like pseudo-R² should be calculated using survey-weighted versions. For assessing a model’s ability to discriminate between outcomes, the Area Under the ROC Curve (AUC) should be calculated as a weighted AUC to ensure the measure is representative of the model’s performance in the target population.\n\n\nTLDR\n\nWhen running regressions, use pseudo-maximum likelihood estimation (PMLE) to incorporate weights for unbiased coefficients, and use design-based methods for valid standard errors. While there is some debate, the best practice is to always use weights in regression to avoid bias. Model fit should be assessed with specialized tools like the Archer-Lemeshow test and weighted AUC.\n\n\n\n\n\n\nPractical Considerations for NHANES Data\n\n5.1. Analyzing Subpopulations: The Correct Approach\nA frequent task in survey analysis is to focus on a specific subgroup of the population, such as analyzing health outcomes only among women.\n\nCommon Pitfall: Incorrectly Subsetting the Data A critical mistake is to simply filter the dataset to keep only the individuals in the subpopulation of interest before specifying the survey design. This approach is incorrect because it provides the software with incomplete information about the total number of strata and PSUs in the original sample, leading to biased standard errors and invalid inference.\n\nThe correct procedure involves two steps : 1. Define the full survey design object first. The analyst must specify the survey design to the statistical software using the entire sample dataset, including all strata, PSUs, and weights. 2. Use a subpopulation or subset command. Once the full design object is created, the analyst should use a specific command within the survey analysis software to restrict subsequent analyses to the desired subpopulation. This command performs calculations only on the subgroup but does so while using the variance estimation structure of the full sample design.\n\n\n5.2. Selecting and Combining NHANES Weights\nThe practical application of survey weights requires careful attention to selecting the correct weight for a given analysis and adjusting weights when combining survey cycles.\n\nSelecting the Correct Weight\nNHANES data files often provide multiple survey weights because different components of the survey are administered to different subsets of the sample. For example, NHANES provides an interview weight (wtint2yr), a Mobile Examination Center (MEC) exam weight (wtmec2yr), and even more specific subsample weights, such as a fasting subsample weight.\nThe analyst must choose the correct weight based on the variables included in the analysis. A good rule of thumb is to use the weight corresponding to the “least common denominator”—that is, the weight that applies to the smallest, most specific subsample required for the analysis. For example, if an analysis includes variables from both the interview and the MEC exam, the MEC exam weight must be used, and the analysis must be restricted to only those participants who have a MEC weight.\n\n\nCombining Survey Cycles\nResearchers often combine data from multiple two-year cycles of NHANES to increase sample size. When doing so, the survey weights must be adjusted.\n\nStandard Cycles (2001-onward): The standard rule is to divide the original two-year sample weight by the number of cycles being combined. For example, if combining three two-year cycles (a total of six years of data), the new multi-year weight would be the original two-year weight divided by three.\nPandemic-Era Data (2017–March 2020): Due to the COVID-19 pandemic, data collection for the 2019–2020 cycle was halted prematurely. To create a nationally representative dataset, the partial 2019–March 2020 data were combined with the full 2017–2018 data. This created a 3.2-year file, and combining it with other 2-year cycles requires special weighting formulas provided by NCHS. For example, to combine 2015-2016 (2 years) and 2017-March 2020 (3.2 years), the new weight (MEC52Y) would be calculated as:\n\nMEC52Y = (2 / 5.2) * WTMEC2YR for the 2015-2016 respondents.\nMEC52Y = (3.2 / 5.2) * WTMECPRP for the 2017-March 2020 respondents.\n\n\n\n\n\n5.3. Preferred reporting items for complex sample survey analysis (PRICSSA)\nWe apply preferred reporting items for complex sample survey analysis (PRICSSA) (Seidenberg, Moser, and West 2023) [link] on an example article (Karim, Hossain, and Zheng 2025) [link].\n\n\n\nPRICSSA Item\nDescription\nExample Text from/for the Article\n\n\n\n\n1.1 Data collection dates\nState the specific start and end dates of the survey to provide historical context.\n“For this study, the authors utilized data from 10 aggregated NHANES cycles spanning from 1999–2000 to 2017–2018.”\n\n\n1.2 Data collection mode(s)\nDescribe how the data was gathered (e.g., in-person interview, phone, web), as this can influence responses.\n“Data collection was primarily done through interviews.”\n\n\n1.3 Target population\nClearly define the population the survey is intended to represent.\nThe analysis focused on the “noninstitutionalized U.S. civilian population” aged “between 20 and 79 years.”\n\n\n1.4 Sample design\nExplain the survey’s sampling methodology, such as stratification and clustering.\n“The survey employs a multistage, stratified cluster sampling design.”\n\n\n1.5 Survey response rate(s)\nReport the survey’s response rate and the calculation method to inform potential nonresponse bias.\n✍️ This was not reported. An example would be: “The NHANES response rates vary by cycle. For the cycles included, the unweighted interview response rates ranged from approximately 84% (1999-2000) to 52% (2017-2018).” [link]\n\n\n2.1 Missingness rates\nReport the rate of missing data for key variables and describe how it was handled (e.g., complete case analysis, imputation).\n“In total, 275 observations (about 0.5% of the entire sample size) were discarded owing to having missing exposure or outcome.” “There were no missing values for the covariates considered in the main analysis.”\n\n\n2.2 Observation deletion\nState if any observations were deleted and provide a justification. Best practice is to use subpopulation commands instead of deleting.\nThe authors state they “discarded” 275 observations due to missing data and “excluded participants who were aged either below 20 or above 79 years.”\n\n\n2.3 Sample sizes\nInclude unweighted counts (n) for all weighted estimates to show the actual number of participants underlying the results.\n“The analytic dataset comprised a sample size of 50,549.” The appendix tables provide detailed unweighted counts for all subgroups, such as the N=80 females who started smoking before age 10 (Appendix Table 2).\n\n\n2.4 Confidence intervals/standard errors\nReport 95% confidence intervals or standard errors for all estimates to convey their precision.\n“…estimated hazard ratios (HRs) along with their associated 95% CIs…” All results in Figure 2 include 95% CIs.\n\n\n2.5 Weighting\nState which analyses were weighted and specify the exact weight variables used.\n“The design was created on the entire data using the design features: interview weights, clusters, and strata.”\n\n\n2.6 Variance estimation\nDescribe the method used to calculate design-adjusted variances and specify the design variables (e.g., PSU, strata).\nThe authors estimated “variances using the Taylor series linearization method.”\n\n\n2.7 Subpopulation analysis\nExplain the correct statistical procedure for analyzing subgroups (e.g., using a subpop or domain command).\n“Subsequently, the authors subset the design to focus on eligible patients…” This describes the correct procedural step for subpopulation analysis.\n\n\n2.8 Suppression rules\nState whether a rule was followed to suppress unreliable estimates (e.g., based on small sample size or large relative standard error).\n✍️ This was not reported as suppression was not done. An example would be: “Estimates based on an unweighted sample size of fewer than 30 participants were considered potentially unstable and are noted in the text.”\n\n\n2.9 Software and code\nState the statistical software and version used, and make the analysis code available for reproducibility.\n“All analyses were conducted using R, Version 4.2.2.” The code is “available from the corresponding author upon reasonable request.”\n\n\n2.10 Singleton problem (as needed)\nIf using Taylor Series Linearization, describe how any strata with only one PSU were handled during analysis.\n✍️ This was not mentioned. As this is an “as needed” item, it is appropriate to omit if the problem was not encountered during the analysis.\n\n\n2.11 Public/restricted data (as needed)\nSpecify whether the public-use or a restricted-use version of the dataset was analyzed.\nThe authors state that “NHANES data are publicly accessible” and that they used “public-use linked mortality files”, indicating public-use data was analyzed.\n\n\n2.12 Embedded experiments (as needed)\nIf the survey included an experiment, describe it and how it was handled in the analysis.\nThis item is not applicable to this study, as the analysis did not involve an embedded experiment within the NHANES data.\n\n\n\nNote\n\nReporting of Tables:\n\nExpanding on item 2.3 “Sample sizes”, when presenting a descriptive statistics table (a “Table 1”), the recommended best practice is to report two pieces of information for each variable :\n\nThe unweighted sample size (n) for each category.\nThe weighted percentage (%) or mean for each category.\n\nThis dual presentation provides a complete picture of both the sample that was actually collected (the ‘n’) and the population it is intended to represent (the weighted estimate). This transparency allows readers to immediately see the effects of the survey design, such as the oversampling of certain groups.\n\n\n\n\n\n\nWarning\n\n\n\nTo apply these principles in R, the svyTable1 package can be used to generate a “Table 1” from complex survey data. This package is provided “as is” and used at the reader’s discretion.\n\n\n\nReporting of lonely/singleton PSU:\n\nThe “lonely PSU” problem, also known as the “singleton PSU” problem is a technical issue that can arise when analyzing complex survey data. The Taylor Series Linearization (TSL) method for variance estimation works by measuring the variability between Primary Sampling Units (PSUs) within each stratum. To calculate variance (which is a measure of spread), you need at least two points to compare. This problem most often occurs during subpopulation analysis. While the full NHANES sample is designed to have at least two PSUs in every stratum, when you analyze a very specific subgroup (e.g., non-Hispanic Black participants who started smoking before age 10), it’s possible that your subgroup of interest exists in only one PSU within a particular stratum. Solutions are to [i] Centering at the grand mean (conservative approach), [ii] The stratum with the lonely PSU is merged with another, similar stratum, or [iii] use Replication methods.\n\nReporting of reliability of estimates:\n\n\ndesign effect (deff): A value &gt;1 indicates the complex design increases variance (e.g., 1.234 means ~23% inflation vs. SRS). Report it in footnotes or Methods for transparency.\nRelative standard error (RSE) or %RSE = (Standard error of estimate / Estimate) * 100: Should be &lt;30% for “stable” estimates per CDC guidelines; suppress or flag unstable ones (e.g., wide CIs).\n\n\nTLDR\n\nFor subpopulation analysis, always define the survey design on the full dataset first, then use a subset command. When using NHANES, select the correct weight based on the “least common denominator” of your variables (e.g., interview vs. exam weight). When combining survey cycles, divide the 2-year weights by the number of cycles, but use special formulas for the pandemic-era data. For descriptive tables, always report both unweighted counts (n) and weighted percentages (%).\n\n\n\n\n5.4 The Sex and Gender Equity in Research (SAGER) guidelines\nFor reporting sex- and gender-based analyses (not necessarily part of survey data analysis), we recommend SAGER guidelines (Heidari et al. 2016). We apply SAGER checklist (Van Epps et al. 2022) [link] on an example article (Karim, Hossain, and Zheng 2025) [link].\n\n\n\n\n\n\n\n\nSAGER Item\nWhat the Manuscript Said\nWhat Should Have Been Said (to fully comply)\n\n\n\n\n1. General: Use terms sex/gender appropriately.\nThe article consistently uses “sex” when referring to the demographic variable from NHANES and discussing biological differences.\nThis is correct. The article appropriately uses “sex” when referring to the demographic variable from NHANES, which is consistent with the data source.\n\n\n3b. Abstract: Describe study population with sex/gender breakdown.\nThe abstract states the analysis explored “effect modification by race/ethnicity and sex” but does not provide the numerical breakdown of participants.\nThe abstract should have included the numbers: “Results: The analysis included 50,549 participants (48.5% male). The authors found that early smoking initiation…”\n\n\n4a. Introduction: Cite previous studies on sex/gender differences.\nThe introduction cites literature on how “biological differences in nicotine metabolism… vary across sexes” and discusses disparities related to sex.\nThis is correct. The introduction properly cites existing literature to establish the rationale for investigating sex as a variable.\n\n\n5a. Methods: State the method used to define sex/gender.\nThe Methods section lists “sex (male, female)” as a variable but does not specify how it was collected or defined by the survey.\nThe Methods section should have specified the data collection method: “Sex (male, female) was based on participant self-report as recorded in the NHANES demographic files.”\n\n\n6a. Results: Provide a complete sex/gender breakdown.\nAppendix Table 1 provides the complete unweighted and weighted breakdown: “Male 24,391 (48.54%)” and “Female 26,158 (51.46%)”.\nThis is correct. The article provides a full and appropriate breakdown of the study population by sex in an appendix table.\n\n\n6b. Results: Present data disaggregated by sex/gender.\nFigure 2 presents hazard ratios stratified by “Male” and “Female”. Appendix Figure 2 shows smoking duration disaggregated by sex.\nThis is correct. The results are clearly and appropriately disaggregated by sex throughout the article and appendix.\n\n\n7a. Discussion: Discuss the implications of sex/gender on the results.\nThe Discussion section analyzes the findings: “Effect modification by sex resulted in slightly higher HR estimates for the female subpopulation…”.\nThis is correct. The manuscript properly discusses and interprets the sex-specific findings.\n\n\n\n\n\n\nGlossary of Terms\n\nArcher-Lemeshow Test: A goodness-of-fit test for logistic regression models that has been adapted for use with complex survey data.\nBalanced Repeated Replication (BRR): A replication method for variance estimation, typically used for designs with two PSUs per stratum.\nBootstrap: A replication method for variance estimation that involves resampling PSUs within each stratum.\nClustering: A sampling technique where natural groups (e.g., counties, schools) are sampled first, followed by sampling of units within the selected groups. This increases variance.\nDesign Effect (DEFF): The ratio of the variance of an estimate from a complex survey to the variance of the same estimate from a simple random sample of the same size. It measures the impact of the design on precision.\nDesign-Based Inference: A statistical framework where inference is based on the known random process of sample selection from a fixed, finite population.\nFay’s BRR: A modification of BRR that perturbs weights rather than deleting cases, which can improve stability for certain estimates.\nI.I.D. (Independent and Identically Distributed): A core assumption of classical statistics that observations are independent of one another and all drawn from the same distribution. This is violated by complex surveys.\nJackknife Repeated Replication (JRR): A replication method for variance estimation that involves creating replicates by successively deleting one PSU at a time.\nModel-Based Inference: A statistical framework where inference is based on an assumed statistical model that generates the data.\nPrimary Sampling Unit (PSU): The first-stage sampling unit in a multi-stage design, often a geographic area like a county.\nPseudo-Maximum Likelihood Estimation (PMLE): An estimation method for regression models that incorporates survey weights into the likelihood function.\nRao-Scott Chi-Square Test: A design-adjusted version of the Pearson chi-square test used to assess association between categorical variables in complex survey data.\nReplication Methods: A family of variance estimation techniques that use the variability across multiple subsamples (replicates) to estimate the variance of an estimate.\nSimple Random Sample (SRS): A basic sampling method where every individual in the population has an equal and independent chance of being selected.\nStrata: Mutually exclusive subgroups of a population from which independent samples are drawn. Stratification generally decreases variance.\nTaylor Series Linearization (TSL): An analytical method for variance estimation that uses a linear approximation of a statistic. It is the most common default method.\nWeighting: The process of assigning a weight to each respondent to adjust for unequal probabilities of selection, nonresponse, and deviations from population totals.\n\n\nWhat is included in this Video Lesson:\n\nreference 00:38\ndesign-based 1:28\nexamples 3:33\nNHANES and sampling 4:54\nweights and other survey features 9:05\nestimate of interest 12:55\ndesign effect 15:52\nVariance estimation 18:13\ndesign-based analysis 25:11\nHow to make inference 29:33\ninappropriate analysis 32:08\nhow useful are sampling weights 36:15\nhow useful are psu/cluster info 37:42\nsubpopulation / subsetting 38:57\nmissingness collected to weights? 40:45\nDealing with subpopulation 41:38\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#examples-of-major-complex-surveys",
    "href": "surveydata0.html#examples-of-major-complex-surveys",
    "title": "Concepts (D)",
    "section": "1.1. Examples of Major Complex Surveys",
    "text": "1.1. Examples of Major Complex Surveys\nIn public health, epidemiology, and the social sciences, our understanding of population-level phenomena is often built upon data from large-scale surveys. Prominent examples include :\n\nNational Health and Nutrition Examination Survey (NHANES): A program of studies to assess the health and nutritional status of adults and children in the United States, unique for combining interviews with physical examinations.\nCanadian Community Health Survey (CCHS): A cross-sectional survey that collects information related to health status, health care utilization, and health determinants for the Canadian population.\nBehavioral Risk Factor Surveillance System (BRFSS): The world’s largest, continuously conducted telephone health survey system, tracking health conditions and risk behaviors in the United States.\nHealth and Retirement Study (HRS): A longitudinal panel study that surveys a representative sample of Americans over the age of 50 to inform research on aging.\nEuropean Social Survey (ESS): A cross-national survey conducted across Europe that measures attitudes, beliefs, and behavior patterns of diverse populations.\n\nEach of these surveys employs a complex design, making the principles discussed in this guide broadly applicable. We will focus on NHANES to illustrate these concepts in detail.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-nhanes-sampling-procedure",
    "href": "surveydata0.html#the-nhanes-sampling-procedure",
    "title": "Concepts (D)",
    "section": "1.2. The NHANES Sampling Procedure",
    "text": "1.2. The NHANES Sampling Procedure\nThe National Health and Nutrition Examination Survey (NHANES) does not use a simple random sample. Instead, it employs a complex, multistage, probability sampling design to select participants who are representative of the civilian, non-institutionalized U.S. population. This multi-stage process is a practical necessity to balance logistical efficiency with statistical rigor. The procedure involves four main stages :\n\nStage 1: Primary Sampling Units (PSUs): The U.S. is divided into geographically defined PSUs, which are typically counties or groups of contiguous counties. These PSUs are grouped into strata based on characteristics like geography and minority population density. From each stratum, a sample of PSUs is selected, usually with a probability proportional to its population size.\nStage 2: Segments: Each selected PSU is further divided into smaller geographic areas called segments (e.g., city blocks). A sample of these segments is then drawn, again often with probability proportional to size.\nStage 3: Households: Within each selected segment, a list of all housing units is compiled, and a sample of households is randomly selected.\nStage 4: Individuals: Finally, within each selected household, individuals are randomly chosen from a list of all household members based on specific age, sex, and race/ethnicity screening criteria.\n\nThis hierarchical process means that data collection is concentrated in a limited number of geographic areas (the sampled PSUs), which is far more cost-effective than traveling to households scattered randomly across the entire nation.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-three-pillars-of-complex-sampling",
    "href": "surveydata0.html#the-three-pillars-of-complex-sampling",
    "title": "Concepts (D)",
    "section": "1.3. The Three Pillars of Complex Sampling",
    "text": "1.3. The Three Pillars of Complex Sampling\nAs discussed previously, Complex survey designs are built upon three core methodological pillars: stratification, clustering, and weighting. Each serves a distinct purpose, and together they form the intricate architecture that must be understood and accounted for in any valid analysis.\n\nStratification: This is the process of partitioning the target population into mutually exclusive subgroups (strata) before sampling begins. Sampling is then performed independently within each stratum. The primary purpose of stratification is to increase the statistical precision of survey estimates and to ensure that key subgroups of interest are adequately represented in the sample.\n\nApplication in NHANES: NHANES stratifies its primary sampling units (PSUs) based on geography and the proportions of minority populations to ensure that the sample accurately reflects the nation’s demographic diversity and allows for reliable subgroup analyses.\n\nClustering: This is a sampling technique used primarily to improve logistical efficiency and reduce data collection costs. Instead of sampling individuals directly, the process involves sampling natural groupings (clusters) first, such as counties or city blocks.\n\nApplication in NHANES: The first two stages of NHANES sampling (selecting counties and then segments within those counties) are examples of clustering. This design makes it feasible to deploy the Mobile Examination Centers (MECs) where physical examinations are conducted.\n\nWeighting: In a complex survey, individuals have unequal probabilities of selection. Survey weights are created to compensate for this. A respondent’s final survey weight can be conceptually understood as the number of people in the target population that he or she represents.\n\nApplication in NHANES: NHANES weights are crucial for generating unbiased national estimates. They are constructed in a multi-step process to account for the complex design :\n\nBase Weight: This is the inverse of the individual’s probability of selection, accounting for all stages of the sampling design. This step is necessary because of the deliberate oversampling of certain subgroups to increase the reliability of estimates for these groups. For example, NHANES often oversamples older adults, low-income individuals, and specific racial/ethnic minorities like African Americans and Mexican Americans.\nNonresponse Adjustment: The weights are adjusted to account for individuals who were selected but did not participate in the survey (either the interview or the exam). This reduces potential bias if non-respondents differ systematically from respondents.\nPost-stratification: The weights are further adjusted so that the weighted sample’s demographic totals (e.g., by age, sex, and race/ethnicity) match known population totals from an independent source like the U.S. Census Bureau. This final calibration helps correct for any remaining discrepancies between the sample and the target population.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-design-effect-deff-why-it-still-matters",
    "href": "surveydata0.html#the-design-effect-deff-why-it-still-matters",
    "title": "Concepts (D)",
    "section": "1.4. The Design Effect (DEFF): Why It Still Matters",
    "text": "1.4. The Design Effect (DEFF): Why It Still Matters\nThe cumulative statistical impact of stratification, clustering, and unequal weighting is captured by the Design Effect (DEFF). It is formally defined as the ratio of the variance of an estimate obtained from the complex survey to the variance of the same estimate that would have been obtained from a simple random sample (SRS) of the exact same size.\n\\[DEFF = \\frac{Var(\\hat{\\theta})_{complex}}{Var(\\hat{\\theta})_{srs}}\\]\nEven with modern software that directly calculates correct variances, the DEFF remains a valuable concept for several reasons:\n\nInterpretation: The DEFF provides a simple, intuitive metric for understanding the efficiency of the survey design for a particular estimate. A DEFF of 2.0, for instance, implies that the variance of the estimate is twice as large as it would have been under SRS, meaning the confidence interval is about 41% wider (\\(\\sqrt{2} \\approx 1.41\\)).\nEffective Sample Size: The DEFF allows you to calculate the “effective sample size” (\\(n_{eff} = n / DEFF\\)), which is the size of an SRS that would yield the same level of precision. This is a powerful tool for communicating the statistical power of a study.\nSurvey Planning: In the design phase of a new survey, estimated DEFFs from previous, similar surveys are crucial for calculating the required sample size to achieve a desired level of precision.\n\nIn essence, while software performs the complex calculations, the DEFF provides the conceptual understanding and interpretive context for the results.\n\nTLDR\n\nComplex surveys like NHANES do not use simple random sampling. They use a multi-stage design involving stratification (dividing the population into groups) and clustering (sampling geographic areas) for efficiency. This creates unequal selection probabilities, which are corrected by survey weights. The Design Effect (DEFF) is a key metric that quantifies how much the complex design increases the variance of an estimate compared to a simple random sample.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-breakdown-of-the-i.i.d.-assumption",
    "href": "surveydata0.html#the-breakdown-of-the-i.i.d.-assumption",
    "title": "Concepts (D)",
    "section": "2.1. The Breakdown of the I.I.D. Assumption",
    "text": "2.1. The Breakdown of the I.I.D. Assumption\nStandard statistical methods assume that observations are Independent and Identically Distributed (I.I.D.). Complex survey designs systematically violate both components of this assumption.\n\nViolation of Independence: Cluster sampling directly violates independence. People living in the same neighborhood (cluster) are more similar to each other than randomly selected individuals, a phenomenon known as intra-cluster correlation. Standard formulas fail to account for this redundancy and therefore underestimate the true variability in the population.\nViolation of Identical Distribution: Stratification and the deliberate oversampling of certain groups mean that individuals have unequal probabilities of selection. Therefore, observations are not “identically distributed.” Survey weights are introduced to correct for this, but their very existence signals a violation of the I.I.D. assumption.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#inappropriate-analysis-in-the-literature",
    "href": "surveydata0.html#inappropriate-analysis-in-the-literature",
    "title": "Concepts (D)",
    "section": "2.2. Inappropriate Analysis in the Literature",
    "text": "2.2. Inappropriate Analysis in the Literature\nThe availability of public-use survey datasets to researchers who may lack adequate training contributes to methodological issues and analytic errors in published literature. A 2013 study reviewing articles that used the Korean National Health and Nutrition Examination Survey (KNHANES) found that a substantial portion of publications failed to properly account for the complex survey design, potentially leading to biased results and incorrect conclusions. This problem is not unique to one survey. The issue is widespread enough that checklists, such as the proposed Preferred Reporting Items for Complex Sample Survey Analysis (PRICSSA) (Seidenberg, Moser, and West 2023), have been developed to guide researchers and improve the quality of reporting. The root cause is often simple ignorance of the correct methods, perpetuated by the publication of flawed examples.\n\nCommon Pitfall: Ignoring the Survey Design The most frequent and dangerous error is to analyze complex survey data as if it were from a simple random sample. This ignores the clustering, stratification, and weighting, leading to incorrect point estimates and, most critically, underestimated standard errors. The result is an analysis prone to overstated significance and an increased risk of Type I errors (false positives). Researchers may report significant associations that are, in reality, merely artifacts of incorrect statistical analysis.\n\n\nTLDR\n\nStandard statistical tests assume data are I.I.D. (independent and identically distributed). Complex surveys violate this assumption through clustering (violating independence) and unequal selection probabilities (violating identical distribution). Ignoring the survey design leads to biased results and underestimated standard errors, a common problem in published research that can produce false-positive findings.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-theory-of-variance-estimation-for-complex-surveys",
    "href": "surveydata0.html#the-theory-of-variance-estimation-for-complex-surveys",
    "title": "Concepts (D)",
    "section": "3.1. The Theory of Variance Estimation for Complex Surveys",
    "text": "3.1. The Theory of Variance Estimation for Complex Surveys\nBecause standard formulas for variance are invalid for complex survey data, analysts must use specialized techniques. There are two major families of methods for this purpose: Taylor Series Linearization and Replication Methods.\n\nTaylor Series Linearization (TSL): Also known as the delta method, TSL is an analytical approach that approximates a complex, non-linear statistic (like a regression coefficient) with a simpler linear function. The variance of this linear approximation, which correctly accounts for stratification and clustering, serves as an estimate for the variance of the original statistic. This is the default method in many software packages and requires the strata and cluster (PSU) identifiers for each observation to be specified.\n\nApplication in NHANES: To use TSL with NHANES data, an analyst must specify the masked variance pseudo-stratum (SDMVSTRA) and pseudo-PSU (SDMVPSU) variables provided in the public-use data files.\n\nReplication Methods: These methods take an empirical approach. The idea is to repeatedly draw subsamples (“replicates”) from the full sample in a way that mimics the original design. The statistic of interest is calculated for each replicate, and the variance of the full-sample estimate is determined by the variability of the estimates across these replicates. This approach requires a set of pre-calculated “replicate weights” instead of the strata and PSU variables. Common replication techniques include :\n\nBalanced Repeated Replication (BRR): Typically used for designs with two PSUs per stratum. It creates replicates by selecting one of the two PSUs from each stratum according to a balanced scheme.\nJackknife Repeated Replication (JRR): Creates replicates by successively deleting one PSU at a time from the sample and up-weighting the remaining PSUs in that stratum.\nFay’s BRR: A modification of BRR that retains all cases in each replicate but perturbs the weights by a factor k (where 0 &lt; k &lt; 1) instead of dropping cases entirely. This method can be more stable, especially for statistics like quantiles.\nBootstrap Methods: Survey-specific bootstrap methods, such as the Rao-Wu bootstrap, create replicates by resampling PSUs within each stratum.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#hypothesis-testing-in-the-design-based-framework",
    "href": "surveydata0.html#hypothesis-testing-in-the-design-based-framework",
    "title": "Concepts (D)",
    "section": "3.2. Hypothesis Testing in the Design-Based Framework",
    "text": "3.2. Hypothesis Testing in the Design-Based Framework\nHypothesis testing procedures must also be adjusted for complex survey data.\n\nDesign-Adjusted t-tests: The standard t-statistic formula is used, but the “Estimate” is calculated using survey weights, and the “Standard Error” in the denominator is a design-based SE from TSL or replication. Furthermore, the degrees of freedom are based on the number of PSUs and strata, not the number of individuals, which appropriately reflects the reduced amount of independent information in a clustered sample.\nThe Rao-Scott Chi-Square Test: The standard Pearson’s chi-squared test is invalid for complex survey data because it does not follow a standard chi-squared distribution. The Rao-Scott chi-square test was developed to correct this problem by adjusting the standard Pearson’s statistic to account for the survey’s design effect. An F-test version of this correction is often preferred as it provides a more accurate adjustment, especially when design effects vary across the cells of a contingency table.\n\n\nTLDR\n\nTo get correct standard errors, use specialized variance estimation methods. Taylor Series Linearization (TSL) is an analytical approach requiring strata and PSU variables. Replication Methods (like BRR, JRR, and Bootstrap) are an empirical approach requiring pre-calculated replicate weights. Standard hypothesis tests like the t-test and chi-square test must be replaced with their design-adjusted counterparts (e.g., Rao-Scott test).",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#principles-of-fitting-regression-models",
    "href": "surveydata0.html#principles-of-fitting-regression-models",
    "title": "Concepts (D)",
    "section": "4.1. Principles of Fitting Regression Models",
    "text": "4.1. Principles of Fitting Regression Models\nWhen fitting regression models to complex survey data, the estimation procedures must be adapted. This is typically achieved through pseudo-maximum likelihood estimation (PMLE), which incorporates the survey weights into the likelihood function. This ensures that the resulting coefficient estimates are representative of the target population.\nWhile weights are used to obtain unbiased point estimates, the most critical adjustment is in the estimation of their variance. After the coefficients are estimated, a design-based method (TSL or replication) is used to calculate the variance-covariance matrix. From this correctly calculated matrix, valid standard errors, confidence intervals, and hypothesis tests (e.g., Wald tests) for each predictor are derived.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#a-word-of-caution-about-weights",
    "href": "surveydata0.html#a-word-of-caution-about-weights",
    "title": "Concepts (D)",
    "section": "4.2. A Word of Caution About “Weights”!",
    "text": "4.2. A Word of Caution About “Weights”!\nWhile there is universal agreement that weights are necessary for descriptive statistics (e.g., population means or prevalences), their use in regression models is more nuanced and has been a subject of debate.\n\nThe Argument for Using Weights: The primary goal of most public health research using national survey data is to make inferences about the target population. In this case, weights must be used to obtain unbiased estimates of the population-level associations. Failing to do so can lead to biased coefficients if the sampling probabilities are related to the outcome variable.\nThe Argument Against Using Weights: Some argue that using weights can reduce precision (i.e., inflate standard errors), especially if the weights are highly variable. They contend that if a regression model is perfectly specified (includes all relevant predictors), the unweighted estimates may be more efficient.\nPractical Recommendation: In practice, one can never be certain that a model is perfectly specified. Therefore, the widely accepted best practice is to use the survey weights in regression models as the default approach. This prioritizes the avoidance of bias. A prudent strategy is to fit both weighted and unweighted models as a sensitivity analysis. If the coefficients differ substantially, it suggests the weights are correcting for significant bias, and the weighted results should be considered primary.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#assessing-model-fit-and-performance",
    "href": "surveydata0.html#assessing-model-fit-and-performance",
    "title": "Concepts (D)",
    "section": "4.3. Assessing Model Fit and Performance",
    "text": "4.3. Assessing Model Fit and Performance\nTools for assessing model performance must also be adapted for complex survey data.\n\nGoodness-of-Fit: For logistic regression, the standard Hosmer-Lemeshow test is invalid. The Archer-Lemeshow goodness-of-fit test is the design-based analogue that correctly accounts for the survey design. A non-significant p-value from this test suggests the model fits the data adequately.\nPredictive Performance: Measures like pseudo-R² should be calculated using survey-weighted versions. For assessing a model’s ability to discriminate between outcomes, the Area Under the ROC Curve (AUC) should be calculated as a weighted AUC to ensure the measure is representative of the model’s performance in the target population.\n\n\nTLDR\n\nWhen running regressions, use pseudo-maximum likelihood estimation (PMLE) to incorporate weights for unbiased coefficients, and use design-based methods for valid standard errors. While there is some debate, the best practice is to always use weights in regression to avoid bias. Model fit should be assessed with specialized tools like the Archer-Lemeshow test and weighted AUC.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#analyzing-subpopulations-the-correct-approach",
    "href": "surveydata0.html#analyzing-subpopulations-the-correct-approach",
    "title": "Concepts (D)",
    "section": "5.1. Analyzing Subpopulations: The Correct Approach",
    "text": "5.1. Analyzing Subpopulations: The Correct Approach\nA frequent task in survey analysis is to focus on a specific subgroup of the population, such as analyzing health outcomes only among women.\n\nCommon Pitfall: Incorrectly Subsetting the Data A critical mistake is to simply filter the dataset to keep only the individuals in the subpopulation of interest before specifying the survey design. This approach is incorrect because it provides the software with incomplete information about the total number of strata and PSUs in the original sample, leading to biased standard errors and invalid inference.\n\nThe correct procedure involves two steps : 1. Define the full survey design object first. The analyst must specify the survey design to the statistical software using the entire sample dataset, including all strata, PSUs, and weights. 2. Use a subpopulation or subset command. Once the full design object is created, the analyst should use a specific command within the survey analysis software to restrict subsequent analyses to the desired subpopulation. This command performs calculations only on the subgroup but does so while using the variance estimation structure of the full sample design.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#selecting-and-combining-nhanes-weights",
    "href": "surveydata0.html#selecting-and-combining-nhanes-weights",
    "title": "Concepts (D)",
    "section": "5.2. Selecting and Combining NHANES Weights",
    "text": "5.2. Selecting and Combining NHANES Weights\nThe practical application of survey weights requires careful attention to selecting the correct weight for a given analysis and adjusting weights when combining survey cycles.\n\nSelecting the Correct Weight\nNHANES data files often provide multiple survey weights because different components of the survey are administered to different subsets of the sample. For example, NHANES provides an interview weight (wtint2yr), a Mobile Examination Center (MEC) exam weight (wtmec2yr), and even more specific subsample weights, such as a fasting subsample weight.\nThe analyst must choose the correct weight based on the variables included in the analysis. A good rule of thumb is to use the weight corresponding to the “least common denominator”—that is, the weight that applies to the smallest, most specific subsample required for the analysis. For example, if an analysis includes variables from both the interview and the MEC exam, the MEC exam weight must be used, and the analysis must be restricted to only those participants who have a MEC weight.\n\n\nCombining Survey Cycles\nResearchers often combine data from multiple two-year cycles of NHANES to increase sample size. When doing so, the survey weights must be adjusted.\n\nStandard Cycles (2001-onward): The standard rule is to divide the original two-year sample weight by the number of cycles being combined. For example, if combining three two-year cycles (a total of six years of data), the new multi-year weight would be the original two-year weight divided by three.\nPandemic-Era Data (2017–March 2020): Due to the COVID-19 pandemic, data collection for the 2019–2020 cycle was halted prematurely. To create a nationally representative dataset, the partial 2019–March 2020 data were combined with the full 2017–2018 data. This created a 3.2-year file, and combining it with other 2-year cycles requires special weighting formulas provided by NCHS. For example, to combine 2015-2016 (2 years) and 2017-March 2020 (3.2 years), the new weight (MEC52Y) would be calculated as:\n\nMEC52Y = (2 / 5.2) * WTMEC2YR for the 2015-2016 respondents.\nMEC52Y = (3.2 / 5.2) * WTMECPRP for the 2017-March 2020 respondents.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#preferred-reporting-items-for-complex-sample-survey-analysis-pricssa",
    "href": "surveydata0.html#preferred-reporting-items-for-complex-sample-survey-analysis-pricssa",
    "title": "Concepts (D)",
    "section": "5.3. Preferred reporting items for complex sample survey analysis (PRICSSA)",
    "text": "5.3. Preferred reporting items for complex sample survey analysis (PRICSSA)\nWe apply preferred reporting items for complex sample survey analysis (PRICSSA) (Seidenberg, Moser, and West 2023) [link] on an example article (Karim, Hossain, and Zheng 2025) [link].\n\n\n\nPRICSSA Item\nDescription\nExample Text from/for the Article\n\n\n\n\n1.1 Data collection dates\nState the specific start and end dates of the survey to provide historical context.\n“For this study, the authors utilized data from 10 aggregated NHANES cycles spanning from 1999–2000 to 2017–2018.”\n\n\n1.2 Data collection mode(s)\nDescribe how the data was gathered (e.g., in-person interview, phone, web), as this can influence responses.\n“Data collection was primarily done through interviews.”\n\n\n1.3 Target population\nClearly define the population the survey is intended to represent.\nThe analysis focused on the “noninstitutionalized U.S. civilian population” aged “between 20 and 79 years.”\n\n\n1.4 Sample design\nExplain the survey’s sampling methodology, such as stratification and clustering.\n“The survey employs a multistage, stratified cluster sampling design.”\n\n\n1.5 Survey response rate(s)\nReport the survey’s response rate and the calculation method to inform potential nonresponse bias.\n✍️ This was not reported. An example would be: “The NHANES response rates vary by cycle. For the cycles included, the unweighted interview response rates ranged from approximately 84% (1999-2000) to 52% (2017-2018).” [link]\n\n\n2.1 Missingness rates\nReport the rate of missing data for key variables and describe how it was handled (e.g., complete case analysis, imputation).\n“In total, 275 observations (about 0.5% of the entire sample size) were discarded owing to having missing exposure or outcome.” “There were no missing values for the covariates considered in the main analysis.”\n\n\n2.2 Observation deletion\nState if any observations were deleted and provide a justification. Best practice is to use subpopulation commands instead of deleting.\nThe authors state they “discarded” 275 observations due to missing data and “excluded participants who were aged either below 20 or above 79 years.”\n\n\n2.3 Sample sizes\nInclude unweighted counts (n) for all weighted estimates to show the actual number of participants underlying the results.\n“The analytic dataset comprised a sample size of 50,549.” The appendix tables provide detailed unweighted counts for all subgroups, such as the N=80 females who started smoking before age 10 (Appendix Table 2).\n\n\n2.4 Confidence intervals/standard errors\nReport 95% confidence intervals or standard errors for all estimates to convey their precision.\n“…estimated hazard ratios (HRs) along with their associated 95% CIs…” All results in Figure 2 include 95% CIs.\n\n\n2.5 Weighting\nState which analyses were weighted and specify the exact weight variables used.\n“The design was created on the entire data using the design features: interview weights, clusters, and strata.”\n\n\n2.6 Variance estimation\nDescribe the method used to calculate design-adjusted variances and specify the design variables (e.g., PSU, strata).\nThe authors estimated “variances using the Taylor series linearization method.”\n\n\n2.7 Subpopulation analysis\nExplain the correct statistical procedure for analyzing subgroups (e.g., using a subpop or domain command).\n“Subsequently, the authors subset the design to focus on eligible patients…” This describes the correct procedural step for subpopulation analysis.\n\n\n2.8 Suppression rules\nState whether a rule was followed to suppress unreliable estimates (e.g., based on small sample size or large relative standard error).\n✍️ This was not reported as suppression was not done. An example would be: “Estimates based on an unweighted sample size of fewer than 30 participants were considered potentially unstable and are noted in the text.”\n\n\n2.9 Software and code\nState the statistical software and version used, and make the analysis code available for reproducibility.\n“All analyses were conducted using R, Version 4.2.2.” The code is “available from the corresponding author upon reasonable request.”\n\n\n2.10 Singleton problem (as needed)\nIf using Taylor Series Linearization, describe how any strata with only one PSU were handled during analysis.\n✍️ This was not mentioned. As this is an “as needed” item, it is appropriate to omit if the problem was not encountered during the analysis.\n\n\n2.11 Public/restricted data (as needed)\nSpecify whether the public-use or a restricted-use version of the dataset was analyzed.\nThe authors state that “NHANES data are publicly accessible” and that they used “public-use linked mortality files”, indicating public-use data was analyzed.\n\n\n2.12 Embedded experiments (as needed)\nIf the survey included an experiment, describe it and how it was handled in the analysis.\nThis item is not applicable to this study, as the analysis did not involve an embedded experiment within the NHANES data.\n\n\n\nNote\n\nReporting of Tables:\n\nExpanding on item 2.3 “Sample sizes”, when presenting a descriptive statistics table (a “Table 1”), the recommended best practice is to report two pieces of information for each variable :\n\nThe unweighted sample size (n) for each category.\nThe weighted percentage (%) or mean for each category.\n\nThis dual presentation provides a complete picture of both the sample that was actually collected (the ‘n’) and the population it is intended to represent (the weighted estimate). This transparency allows readers to immediately see the effects of the survey design, such as the oversampling of certain groups.\n\n\n\n\n\n\nWarning\n\n\n\nTo apply these principles in R, the svyTable1 package can be used to generate a “Table 1” from complex survey data. This package is provided “as is” and used at the reader’s discretion.\n\n\n\nReporting of lonely/singleton PSU:\n\nThe “lonely PSU” problem, also known as the “singleton PSU” problem is a technical issue that can arise when analyzing complex survey data. The Taylor Series Linearization (TSL) method for variance estimation works by measuring the variability between Primary Sampling Units (PSUs) within each stratum. To calculate variance (which is a measure of spread), you need at least two points to compare. This problem most often occurs during subpopulation analysis. While the full NHANES sample is designed to have at least two PSUs in every stratum, when you analyze a very specific subgroup (e.g., non-Hispanic Black participants who started smoking before age 10), it’s possible that your subgroup of interest exists in only one PSU within a particular stratum. Solutions are to [i] Centering at the grand mean (conservative approach), [ii] The stratum with the lonely PSU is merged with another, similar stratum, or [iii] use Replication methods.\n\nReporting of reliability of estimates:\n\n\ndesign effect (deff): A value &gt;1 indicates the complex design increases variance (e.g., 1.234 means ~23% inflation vs. SRS). Report it in footnotes or Methods for transparency.\nRelative standard error (RSE) or %RSE = (Standard error of estimate / Estimate) * 100: Should be &lt;30% for “stable” estimates per CDC guidelines; suppress or flag unstable ones (e.g., wide CIs).\n\n\nTLDR\n\nFor subpopulation analysis, always define the survey design on the full dataset first, then use a subset command. When using NHANES, select the correct weight based on the “least common denominator” of your variables (e.g., interview vs. exam weight). When combining survey cycles, divide the 2-year weights by the number of cycles, but use special formulas for the pandemic-era data. For descriptive tables, always report both unweighted counts (n) and weighted percentages (%).",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#the-sex-and-gender-equity-in-research-sager-guidelines",
    "href": "surveydata0.html#the-sex-and-gender-equity-in-research-sager-guidelines",
    "title": "Concepts (D)",
    "section": "5.4 The Sex and Gender Equity in Research (SAGER) guidelines",
    "text": "5.4 The Sex and Gender Equity in Research (SAGER) guidelines\nFor reporting sex- and gender-based analyses (not necessarily part of survey data analysis), we recommend SAGER guidelines (Heidari et al. 2016). We apply SAGER checklist (Van Epps et al. 2022) [link] on an example article (Karim, Hossain, and Zheng 2025) [link].\n\n\n\n\n\n\n\n\nSAGER Item\nWhat the Manuscript Said\nWhat Should Have Been Said (to fully comply)\n\n\n\n\n1. General: Use terms sex/gender appropriately.\nThe article consistently uses “sex” when referring to the demographic variable from NHANES and discussing biological differences.\nThis is correct. The article appropriately uses “sex” when referring to the demographic variable from NHANES, which is consistent with the data source.\n\n\n3b. Abstract: Describe study population with sex/gender breakdown.\nThe abstract states the analysis explored “effect modification by race/ethnicity and sex” but does not provide the numerical breakdown of participants.\nThe abstract should have included the numbers: “Results: The analysis included 50,549 participants (48.5% male). The authors found that early smoking initiation…”\n\n\n4a. Introduction: Cite previous studies on sex/gender differences.\nThe introduction cites literature on how “biological differences in nicotine metabolism… vary across sexes” and discusses disparities related to sex.\nThis is correct. The introduction properly cites existing literature to establish the rationale for investigating sex as a variable.\n\n\n5a. Methods: State the method used to define sex/gender.\nThe Methods section lists “sex (male, female)” as a variable but does not specify how it was collected or defined by the survey.\nThe Methods section should have specified the data collection method: “Sex (male, female) was based on participant self-report as recorded in the NHANES demographic files.”\n\n\n6a. Results: Provide a complete sex/gender breakdown.\nAppendix Table 1 provides the complete unweighted and weighted breakdown: “Male 24,391 (48.54%)” and “Female 26,158 (51.46%)”.\nThis is correct. The article provides a full and appropriate breakdown of the study population by sex in an appendix table.\n\n\n6b. Results: Present data disaggregated by sex/gender.\nFigure 2 presents hazard ratios stratified by “Male” and “Female”. Appendix Figure 2 shows smoking duration disaggregated by sex.\nThis is correct. The results are clearly and appropriately disaggregated by sex throughout the article and appendix.\n\n\n7a. Discussion: Discuss the implications of sex/gender on the results.\nThe Discussion section analyzes the findings: “Effect modification by sex resulted in slightly higher HR estimates for the female subpopulation…”.\nThis is correct. The manuscript properly discusses and interprets the sex-specific findings.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#glossary-of-terms",
    "href": "surveydata0.html#glossary-of-terms",
    "title": "Concepts (D)",
    "section": "Glossary of Terms",
    "text": "Glossary of Terms\n\nArcher-Lemeshow Test: A goodness-of-fit test for logistic regression models that has been adapted for use with complex survey data.\nBalanced Repeated Replication (BRR): A replication method for variance estimation, typically used for designs with two PSUs per stratum.\nBootstrap: A replication method for variance estimation that involves resampling PSUs within each stratum.\nClustering: A sampling technique where natural groups (e.g., counties, schools) are sampled first, followed by sampling of units within the selected groups. This increases variance.\nDesign Effect (DEFF): The ratio of the variance of an estimate from a complex survey to the variance of the same estimate from a simple random sample of the same size. It measures the impact of the design on precision.\nDesign-Based Inference: A statistical framework where inference is based on the known random process of sample selection from a fixed, finite population.\nFay’s BRR: A modification of BRR that perturbs weights rather than deleting cases, which can improve stability for certain estimates.\nI.I.D. (Independent and Identically Distributed): A core assumption of classical statistics that observations are independent of one another and all drawn from the same distribution. This is violated by complex surveys.\nJackknife Repeated Replication (JRR): A replication method for variance estimation that involves creating replicates by successively deleting one PSU at a time.\nModel-Based Inference: A statistical framework where inference is based on an assumed statistical model that generates the data.\nPrimary Sampling Unit (PSU): The first-stage sampling unit in a multi-stage design, often a geographic area like a county.\nPseudo-Maximum Likelihood Estimation (PMLE): An estimation method for regression models that incorporates survey weights into the likelihood function.\nRao-Scott Chi-Square Test: A design-adjusted version of the Pearson chi-square test used to assess association between categorical variables in complex survey data.\nReplication Methods: A family of variance estimation techniques that use the variability across multiple subsamples (replicates) to estimate the variance of an estimate.\nSimple Random Sample (SRS): A basic sampling method where every individual in the population has an equal and independent chance of being selected.\nStrata: Mutually exclusive subgroups of a population from which independent samples are drawn. Stratification generally decreases variance.\nTaylor Series Linearization (TSL): An analytical method for variance estimation that uses a linear approximation of a statistic. It is the most common default method.\nWeighting: The process of assigning a weight to each respondent to adjust for unequal probabilities of selection, nonresponse, and deviations from population totals.\n\n\nWhat is included in this Video Lesson:\n\nreference 00:38\ndesign-based 1:28\nexamples 3:33\nNHANES and sampling 4:54\nweights and other survey features 9:05\nestimate of interest 12:55\ndesign effect 15:52\nVariance estimation 18:13\ndesign-based analysis 25:11\nHow to make inference 29:33\ninappropriate analysis 32:08\nhow useful are sampling weights 36:15\nhow useful are psu/cluster info 37:42\nsubpopulation / subsetting 38:57\nmissingness collected to weights? 40:45\nDealing with subpopulation 41:38\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#video-lesson-slides",
    "href": "surveydata0.html#video-lesson-slides",
    "title": "Concepts (D)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#links",
    "href": "surveydata0.html#links",
    "title": "Concepts (D)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata0.html#references",
    "href": "surveydata0.html#references",
    "title": "Concepts (D)",
    "section": "References",
    "text": "References\n\n\n\n\nArcher, K. J., and S. Lemeshow. 2006. “Goodness-of-Fit Test for a Logistic Regression Model Fitted Using Survey Sample Data.” The Stata Journal 6 (1): 97–105.\n\n\nHeeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2014. “Regression with Complex Samples.” In The SAGE Handbook of Regression Analysis and Causal Inference, edited by Henning Best and Christof Wolf. SAGE Publications.\n\n\nHeeringa, Steven G, Brady T West, and Patricia A Berglund. 2017. Applied Survey Data Analysis. Chapman; Hall/CRC.\n\n\nHeidari, Shirin, Thomas F Babor, Paola De Castro, Sera Tort, and Mirjam Curno. 2016. “Sex and Gender Equity in Research: Rationale for the SAGER Guidelines and Recommended Use.” Research Integrity and Peer Review 1 (1): 2.\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, and Chuyi Zheng. 2025. “Examining the Role of Race/Ethnicity and Sex in Modifying the Association Between Early Smoking Initiation and Mortality: A 20-Year NHANES Analysis.” AJPM Focus 4 (2): 100282.\n\n\nKoch, G. G., D. H. Freeman Jr, and J. L. Freeman. 1975. “Strategies in the Multivariate Analysis of Data from Complex Surveys.” International Statistical Review/Revue Internationale de Statistique, 59–78.\n\n\nLumley, Thomas. 2017. “Pseudo-R2 Statistics Under Complex Sampling.” Australian & New Zealand Journal of Statistics 59 (2): 187–94.\n\n\nLumley, Thomas, and Alan Scott. 2014. “Tests for Regression Models Fitted to Survey Data.” Australian & New Zealand Journal of Statistics 56 (1): 1–14.\n\n\n———. 2015. “AIC and BIC for Modeling with Complex Survey Data.” Journal of Survey Statistics and Methodology 3 (1): 1–18.\n\n\nRao, J. N. K., and A. J. Scott. 1984. “On Chi-Squared Tests for Multiway Contingency Tables with Cell Proportions Estimated from Survey Data.” The Annals of Statistics, 46–60.\n\n\nSeidenberg, Andrew B, Richard P Moser, and Brady T West. 2023. “Preferred Reporting Items for Complex Sample Survey Analysis (PRICSSA).” Journal of Survey Statistics and Methodology 11 (4): 743–57.\n\n\nThomas, D. R., and J. N. K. Rao. 1987. “Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics Under Cluster Sampling.” Journal of the American Statistical Association 82 (398): 630–36.\n\n\nVan Epps, Heather, Olaya Astudillo, Yaiza Del Pozo Martin, and Joan Marsh. 2022. “The Sex and Gender Equity in Research (SAGER) Guidelines: Implementation and Checklist Development.” European Science Editing 48: e86910.",
    "crumbs": [
      "Survey data analysis",
      "Concepts (D)"
    ]
  },
  {
    "objectID": "surveydata1.html",
    "href": "surveydata1.html",
    "title": "CCHS: Revisiting PICOT",
    "section": "",
    "text": "Remembering PICOT\nWelcome to this tutorial where we will examine the same research question presented in the Causal question-1 tutorial. Our approach will be enriched this time by working with a more comprehensive set of covariates. We will follow the guidelines from the research article by Rahman et al. (2013) (DOI:10.1136/bmjopen-2013-002624). We will also will work properly with survey feature variables (e.g., sampling weights).\nBefore diving into the data, let’s clarify and remember the research parameters using the PICOT framework:\nIn addition, we’ll identify potential confounders based on literature to better understand the relationship between exposure and outcome.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Revisiting PICOT"
    ]
  },
  {
    "objectID": "surveydata1.html#references",
    "href": "surveydata1.html#references",
    "title": "CCHS: Revisiting PICOT",
    "section": "References",
    "text": "References\n\n\n\n\nCanada, Statistics. 2005. “Canadian Community Health Survey (CCHS), Cycle 3.1.” Author Ottawa.\n\n\nKarim, Ehsan. 2023. “Case Study 2: Risk of Cardiovascular Disease Among Osteoarthritis Patients.” https://ssc.ca/en/case-study/case-study-2-risk-cardiovascular-disease-among-osteoarthritis-patients.\n\n\nRahman, M Mushfiqur, Jacek A Kopec, Jolanda Cibere, Charlie H Goldsmith, and Aslam H Anis. 2013. “The Relationship Between Osteoarthritis and Cardiovascular Disease in a Population Health Survey: A Cross-Sectional Study.” BMJ Open 3 (5): e002624.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Revisiting PICOT"
    ]
  },
  {
    "objectID": "surveydata2.html",
    "href": "surveydata2.html",
    "title": "CCHS: Assessing data",
    "section": "",
    "text": "Let us load all the necessary packages for data manipulation, statistical analysis, and plotting.\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\n\nLoad data\nData loading that we saved earlier:\n\nload(\"Data/surveydata/cchs123.RData\")\nls()\n#&gt; [1] \"analytic\" \"cc123a\"\n\nChecking\nCheck the data for missingness\nChecks the dimensions of the data and runs functions to explore missing data, stratifying by some variables. Additionally, it plots the missing data for visualization.\n\ndim(analytic)\n#&gt; [1] 397173     24\nrequire(\"tableone\")\n#CreateTableOne(data = analytic, includeNA = TRUE)\nCreateTableOne(data = analytic, strata = \"CVD\", includeNA = TRUE)\n#&gt;                       Stratified by CVD\n#&gt;                        event                 no event              p      test\n#&gt;   n                        25524                371121                        \n#&gt;   CVD (%)                                                             NaN     \n#&gt;      event                 25524 (100.0)             0 (  0.0)                \n#&gt;      no event                  0 (  0.0)        371121 (100.0)                \n#&gt;      NA                        0 (  0.0)             0 (  0.0)                \n#&gt;   age (%)                                                          &lt;0.001     \n#&gt;      20-29 years             330 (  1.3)         48293 ( 13.0)                \n#&gt;      30-39 years             580 (  2.3)         63194 ( 17.0)                \n#&gt;      40-49 years            1498 (  5.9)         63549 ( 17.1)                \n#&gt;      50-59 years            3635 ( 14.2)         57300 ( 15.4)                \n#&gt;      60-64 years            2720 ( 10.7)         22497 (  6.1)                \n#&gt;      65 years and over     16496 ( 64.6)         64198 ( 17.3)                \n#&gt;      teen                    265 (  1.0)         52090 ( 14.0)                \n#&gt;   sex = Male (%)           12506 ( 49.0)        169776 ( 45.7)     &lt;0.001     \n#&gt;   married (%)                                                      &lt;0.001     \n#&gt;      not single            13287 ( 52.1)        188687 ( 50.8)                \n#&gt;      single                12207 ( 47.8)        181811 ( 49.0)                \n#&gt;      NA                       30 (  0.1)           623 (  0.2)                \n#&gt;   race (%)                                                         &lt;0.001     \n#&gt;      Non-white              1276 (  5.0)         37323 ( 10.1)                \n#&gt;      White                 23629 ( 92.6)        325178 ( 87.6)                \n#&gt;      NA                      619 (  2.4)          8620 (  2.3)                \n#&gt;   edu (%)                                                          &lt;0.001     \n#&gt;      &lt; 2ndary              11547 ( 45.2)        112678 ( 30.4)                \n#&gt;      2nd grad.              3310 ( 13.0)         61355 ( 16.5)                \n#&gt;      Other 2nd grad.        1323 (  5.2)         27643 (  7.4)                \n#&gt;      Post-2nd grad.         8744 ( 34.3)        163052 ( 43.9)                \n#&gt;      NA                      600 (  2.4)          6393 (  1.7)                \n#&gt;   income (%)                                                       &lt;0.001     \n#&gt;      $29,999 or less       11664 ( 45.7)         89506 ( 24.1)                \n#&gt;      $30,000-$49,999        4871 ( 19.1)         72994 ( 19.7)                \n#&gt;      $50,000-$79,999        3193 ( 12.5)         81861 ( 22.1)                \n#&gt;      $80,000 or more        1905 (  7.5)         73768 ( 19.9)                \n#&gt;      NA                     3891 ( 15.2)         52992 ( 14.3)                \n#&gt;   bmi (%)                                                          &lt;0.001     \n#&gt;      Underweight             504 (  2.0)          9600 (  2.6)                \n#&gt;      healthy weight         7176 ( 28.1)        141200 ( 38.0)                \n#&gt;      Overweight            12104 ( 47.4)        153887 ( 41.5)                \n#&gt;      NA                     5740 ( 22.5)         66434 ( 17.9)                \n#&gt;   phyact (%)                                                       &lt;0.001     \n#&gt;      Active                 3642 ( 14.3)         94844 ( 25.6)                \n#&gt;      Inactive              15494 ( 60.7)        174976 ( 47.1)                \n#&gt;      Moderate               4928 ( 19.3)         88480 ( 23.8)                \n#&gt;      NA                     1460 (  5.7)         12821 (  3.5)                \n#&gt;   doctor (%)                                                       &lt;0.001     \n#&gt;      No                     1134 (  4.4)         57425 ( 15.5)                \n#&gt;      Yes                   24384 ( 95.5)        313282 ( 84.4)                \n#&gt;      NA                        6 (  0.0)           414 (  0.1)                \n#&gt;   stress (%)                                                       &lt;0.001     \n#&gt;      Not too stressed      20041 ( 78.5)        266358 ( 71.8)                \n#&gt;      stressed               5184 ( 20.3)         76986 ( 20.7)                \n#&gt;      NA                      299 (  1.2)         27777 (  7.5)                \n#&gt;   smoke (%)                                                        &lt;0.001     \n#&gt;      Current smoker         4481 ( 17.6)         93253 ( 25.1)                \n#&gt;      Former smoker         13927 ( 54.6)        143421 ( 38.6)                \n#&gt;      Never smoker           6981 ( 27.4)        132891 ( 35.8)                \n#&gt;      NA                      135 (  0.5)          1556 (  0.4)                \n#&gt;   drink (%)                                                        &lt;0.001     \n#&gt;      Current drinker       15852 ( 62.1)        279583 ( 75.3)                \n#&gt;      Former driker          6820 ( 26.7)         48373 ( 13.0)                \n#&gt;      Never drank            2421 (  9.5)         38195 ( 10.3)                \n#&gt;      NA                      431 (  1.7)          4970 (  1.3)                \n#&gt;   fruit (%)                                                        &lt;0.001     \n#&gt;      0-3 daily serving      4284 ( 16.8)         79088 ( 21.3)                \n#&gt;      4-6 daily serving     10527 ( 41.2)        148684 ( 40.1)                \n#&gt;      6+ daily serving       5047 ( 19.8)         73729 ( 19.9)                \n#&gt;      NA                     5666 ( 22.2)         69620 ( 18.8)                \n#&gt;   bp (%)                                                           &lt;0.001     \n#&gt;      No                    12611 ( 49.4)        315344 ( 85.0)                \n#&gt;      Yes                   12857 ( 50.4)         55037 ( 14.8)                \n#&gt;      NA                       56 (  0.2)           740 (  0.2)                \n#&gt;   copd (%)                                                         &lt;0.001     \n#&gt;      No                    23378 ( 91.6)        267481 ( 72.1)                \n#&gt;      Yes                    1449 (  5.7)          3043 (  0.8)                \n#&gt;      NA                      697 (  2.7)        100597 ( 27.1)                \n#&gt;   diab (%)                                                         &lt;0.001     \n#&gt;      No                    20461 ( 80.2)        353817 ( 95.3)                \n#&gt;      Yes                    5038 ( 19.7)         17138 (  4.6)                \n#&gt;      NA                       25 (  0.1)           166 (  0.0)                \n#&gt;   province = South (%)     25271 ( 99.0)        363659 ( 98.0)     &lt;0.001     \n#&gt;   weight (mean (SD))      152.58 (181.69)       203.40 (244.28)    &lt;0.001     \n#&gt;   cycle (%)                                                        &lt;0.001     \n#&gt;      11                     7968 ( 31.2)        122798 ( 33.1)                \n#&gt;      21                     9027 ( 35.4)        124838 ( 33.6)                \n#&gt;      31                     8529 ( 33.4)        123485 ( 33.3)                \n#&gt;   ID (mean (SD))       199839.07 (114705.35) 198466.74 (114661.51)  0.064     \n#&gt;   OA (%)                                                           &lt;0.001     \n#&gt;      Control               12655 ( 49.6)        301675 ( 81.3)                \n#&gt;      OA                     6522 ( 25.6)         34346 (  9.3)                \n#&gt;      NA                     6347 ( 24.9)         35100 (  9.5)                \n#&gt;   immigrate (%)                                                    &lt;0.001     \n#&gt;      &gt; 10 years             2295 (  9.0)         24409 (  6.6)                \n#&gt;      not immigrant         21342 ( 83.6)        316353 ( 85.2)                \n#&gt;      recent                  159 (  0.6)         10476 (  2.8)                \n#&gt;      NA                     1728 (  6.8)         19883 (  5.4)                \n#&gt;   province.check (%)                                                  NaN     \n#&gt;      NEWFOUNDLAND            519 (  2.0)          7398 (  2.0)                \n#&gt;      PEI                     567 (  2.2)          7172 (  1.9)                \n#&gt;      NOVA SCOTIA            1308 (  5.1)         14015 (  3.8)                \n#&gt;      NEW BRUNSWICK          1223 (  4.8)         13786 (  3.7)                \n#&gt;      QU\\xc9BEC              1380 (  5.4)         20625 (  5.6)                \n#&gt;      ONTARIO                8596 ( 33.7)        115053 ( 31.0)                \n#&gt;      MANITOBA               1339 (  5.2)         22074 (  5.9)                \n#&gt;      SASKATCHEWAN           1542 (  6.0)         21782 (  5.9)                \n#&gt;      ALBERTA                1837 (  7.2)         38238 ( 10.3)                \n#&gt;      BRITISH COLUMBIA       2847 ( 11.2)         46834 ( 12.6)                \n#&gt;      YUKON/NWT/NUNAVT        173 (  0.7)          4884 (  1.3)                \n#&gt;      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#&gt;      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#&gt;      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#&gt;      NOT STATED                0 (  0.0)             0 (  0.0)                \n#&gt;      QUEBEC                 3839 ( 15.0)         52850 ( 14.2)                \n#&gt;      NFLD & LAB.             274 (  1.1)          3832 (  1.0)                \n#&gt;      YUKON/NWT/NUNA.          80 (  0.3)          2578 (  0.7)\nCreateTableOne(data = analytic, strata = \"OA\", includeNA = TRUE)\n#&gt;                       Stratified by OA\n#&gt;                        Control               OA                    p      test\n#&gt;   n                       314542                 40943                        \n#&gt;   CVD (%)                                                          &lt;0.001     \n#&gt;      event                 12655 (  4.0)          6522 ( 15.9)                \n#&gt;      no event             301675 ( 95.9)         34346 ( 83.9)                \n#&gt;      NA                      212 (  0.1)            75 (  0.2)                \n#&gt;   age (%)                                                          &lt;0.001     \n#&gt;      20-29 years           46805 ( 14.9)           537 (  1.3)                \n#&gt;      30-39 years           59233 ( 18.8)          1622 (  4.0)                \n#&gt;      40-49 years           55598 ( 17.7)          4128 ( 10.1)                \n#&gt;      50-59 years           43746 ( 13.9)          8994 ( 22.0)                \n#&gt;      60-64 years           15772 (  5.0)          5100 ( 12.5)                \n#&gt;      65 years and over     41661 ( 13.2)         20436 ( 49.9)                \n#&gt;      teen                  51727 ( 16.4)           126 (  0.3)                \n#&gt;   sex = Male (%)          153889 ( 48.9)         11627 ( 28.4)     &lt;0.001     \n#&gt;   married (%)                                                      &lt;0.001     \n#&gt;      not single           158065 ( 50.3)         21794 ( 53.2)                \n#&gt;      single               155952 ( 49.6)         19099 ( 46.6)                \n#&gt;      NA                      525 (  0.2)            50 (  0.1)                \n#&gt;   race (%)                                                         &lt;0.001     \n#&gt;      Non-white             34028 ( 10.8)          1803 (  4.4)                \n#&gt;      White                273378 ( 86.9)         38241 ( 93.4)                \n#&gt;      NA                     7136 (  2.3)           899 (  2.2)                \n#&gt;   edu (%)                                                          &lt;0.001     \n#&gt;      &lt; 2ndary              92831 ( 29.5)         14539 ( 35.5)                \n#&gt;      2nd grad.             52077 ( 16.6)          6291 ( 15.4)                \n#&gt;      Other 2nd grad.       24099 (  7.7)          2484 (  6.1)                \n#&gt;      Post-2nd grad.       140400 ( 44.6)         16887 ( 41.2)                \n#&gt;      NA                     5135 (  1.6)           742 (  1.8)                \n#&gt;   income (%)                                                       &lt;0.001     \n#&gt;      $29,999 or less       68530 ( 21.8)         16233 ( 39.6)                \n#&gt;      $30,000-$49,999       61697 ( 19.6)          8360 ( 20.4)                \n#&gt;      $50,000-$79,999       72657 ( 23.1)          6348 ( 15.5)                \n#&gt;      $80,000 or more       67458 ( 21.4)          4191 ( 10.2)                \n#&gt;      NA                    44200 ( 14.1)          5811 ( 14.2)                \n#&gt;   bmi (%)                                                          &lt;0.001     \n#&gt;      Underweight            8660 (  2.8)           715 (  1.7)                \n#&gt;      healthy weight       123416 ( 39.2)         12631 ( 30.9)                \n#&gt;      Overweight           123898 ( 39.4)         20715 ( 50.6)                \n#&gt;      NA                    58568 ( 18.6)          6882 ( 16.8)                \n#&gt;   phyact (%)                                                       &lt;0.001     \n#&gt;      Active                84269 ( 26.8)          6968 ( 17.0)                \n#&gt;      Inactive             143058 ( 45.5)         23604 ( 57.7)                \n#&gt;      Moderate              75703 ( 24.1)          9176 ( 22.4)                \n#&gt;      NA                    11512 (  3.7)          1195 (  2.9)                \n#&gt;   doctor (%)                                                       &lt;0.001     \n#&gt;      No                    53335 ( 17.0)          2221 (  5.4)                \n#&gt;      Yes                  260802 ( 82.9)         38717 ( 94.6)                \n#&gt;      NA                      405 (  0.1)             5 (  0.0)                \n#&gt;   stress (%)                                                       &lt;0.001     \n#&gt;      Not too stressed     223212 ( 71.0)         31769 ( 77.6)                \n#&gt;      stressed              63923 ( 20.3)          8998 ( 22.0)                \n#&gt;      NA                    27407 (  8.7)           176 (  0.4)                \n#&gt;   smoke (%)                                                        &lt;0.001     \n#&gt;      Current smoker        79521 ( 25.3)          8087 ( 19.8)                \n#&gt;      Former smoker        117745 ( 37.4)         20267 ( 49.5)                \n#&gt;      Never smoker         116006 ( 36.9)         12428 ( 30.4)                \n#&gt;      NA                     1270 (  0.4)           161 (  0.4)                \n#&gt;   drink (%)                                                        &lt;0.001     \n#&gt;      Current drinker      239223 ( 76.1)         28622 ( 69.9)                \n#&gt;      Former driker         37042 ( 11.8)          8668 ( 21.2)                \n#&gt;      Never drank           34185 ( 10.9)          3128 (  7.6)                \n#&gt;      NA                     4092 (  1.3)           525 (  1.3)                \n#&gt;   fruit (%)                                                        &lt;0.001     \n#&gt;      0-3 daily serving     68629 ( 21.8)          6571 ( 16.0)                \n#&gt;      4-6 daily serving    125177 ( 39.8)         17214 ( 42.0)                \n#&gt;      6+ daily serving      62121 ( 19.7)          9123 ( 22.3)                \n#&gt;      NA                    58615 ( 18.6)          8035 ( 19.6)                \n#&gt;   bp (%)                                                           &lt;0.001     \n#&gt;      No                   275443 ( 87.6)         25551 ( 62.4)                \n#&gt;      Yes                   38442 ( 12.2)         15341 ( 37.5)                \n#&gt;      NA                      657 (  0.2)            51 (  0.1)                \n#&gt;   copd (%)                                                         &lt;0.001     \n#&gt;      No                   213719 ( 67.9)         39007 ( 95.3)                \n#&gt;      Yes                    2131 (  0.7)          1214 (  3.0)                \n#&gt;      NA                    98692 ( 31.4)           722 (  1.8)                \n#&gt;   diab (%)                                                         &lt;0.001     \n#&gt;      No                   301943 ( 96.0)         36211 ( 88.4)                \n#&gt;      Yes                   12442 (  4.0)          4705 ( 11.5)                \n#&gt;      NA                      157 (  0.0)            27 (  0.1)                \n#&gt;   province = South (%)    307761 ( 97.8)         40507 ( 98.9)     &lt;0.001     \n#&gt;   weight (mean (SD))      211.50 (251.46)       159.00 (188.84)    &lt;0.001     \n#&gt;   cycle (%)                                                        &lt;0.001     \n#&gt;      11                   106231 ( 33.8)         12052 ( 29.4)                \n#&gt;      21                   104530 ( 33.2)         14750 ( 36.0)                \n#&gt;      31                   103781 ( 33.0)         14141 ( 34.5)                \n#&gt;   ID (mean (SD))       197003.20 (115147.95) 204459.43 (113014.25) &lt;0.001     \n#&gt;   OA (%)                                                              NaN     \n#&gt;      Control              314542 (100.0)             0 (  0.0)                \n#&gt;      OA                        0 (  0.0)         40943 (100.0)                \n#&gt;      NA                        0 (  0.0)             0 (  0.0)                \n#&gt;   immigrate (%)                                                    &lt;0.001     \n#&gt;      &gt; 10 years            19385 (  6.2)          3622 (  8.8)                \n#&gt;      not immigrant        268962 ( 85.5)         34509 ( 84.3)                \n#&gt;      recent                10187 (  3.2)           151 (  0.4)                \n#&gt;      NA                    16008 (  5.1)          2661 (  6.5)                \n#&gt;   province.check (%)                                                  NaN     \n#&gt;      NEWFOUNDLAND           6315 (  2.0)           725 (  1.8)                \n#&gt;      PEI                    5892 (  1.9)           817 (  2.0)                \n#&gt;      NOVA SCOTIA           11081 (  3.5)          1880 (  4.6)                \n#&gt;      NEW BRUNSWICK         11517 (  3.7)          1693 (  4.1)                \n#&gt;      QU\\xc9BEC             19111 (  6.1)          2035 (  5.0)                \n#&gt;      ONTARIO               95651 ( 30.4)         13669 ( 33.4)                \n#&gt;      MANITOBA              18050 (  5.7)          2272 (  5.5)                \n#&gt;      SASKATCHEWAN          17941 (  5.7)          2166 (  5.3)                \n#&gt;      ALBERTA               32207 ( 10.2)          3608 (  8.8)                \n#&gt;      BRITISH COLUMBIA      40034 ( 12.7)          4873 ( 11.9)                \n#&gt;      YUKON/NWT/NUNAVT       4446 (  1.4)           273 (  0.7)                \n#&gt;      NOT APPLICABLE            0 (  0.0)             0 (  0.0)                \n#&gt;      DON'T KNOW                0 (  0.0)             0 (  0.0)                \n#&gt;      REFUSAL                   0 (  0.0)             0 (  0.0)                \n#&gt;      NOT STATED                0 (  0.0)             0 (  0.0)                \n#&gt;      QUEBEC                46817 ( 14.9)          6366 ( 15.5)                \n#&gt;      NFLD & LAB.            3145 (  1.0)           403 (  1.0)                \n#&gt;      YUKON/NWT/NUNA.        2335 (  0.7)           163 (  0.4)\n\nrequire(DataExplorer)\nplot_missing(analytic)\n\n\n\n\n\n\n\nLook for zero-cells\nCreates two new variables based on age groups and generates summary tables. It also comments on the presence of ‘zero cells’ in one of the variables, which might require further handling.\n\nanalytic$age.65p &lt;- analytic$age.teen &lt;- 0\nanalytic$age.teen[analytic$age == \"teen\"] &lt;- 1\nanalytic$age.65p[analytic$age == \"65 years and over\"] &lt;- 1\nCreateTableOne(data = analytic, strata = \"age.teen\", includeNA = TRUE)\n#&gt;                       Stratified by age.teen\n#&gt;                        0                     1                     p      test\n#&gt;   n                       344786                 52387                        \n#&gt;   CVD (%)                                                          &lt;0.001     \n#&gt;      event                 25259 ( 7.3)            265 (  0.5)                \n#&gt;      no event             319031 (92.5)          52090 ( 99.4)                \n#&gt;      NA                      496 ( 0.1)             32 (  0.1)                \n#&gt;   age (%)                                                          &lt;0.001     \n#&gt;      20-29 years           48652 (14.1)              0 (  0.0)                \n#&gt;      30-39 years           63810 (18.5)              0 (  0.0)                \n#&gt;      40-49 years           65111 (18.9)              0 (  0.0)                \n#&gt;      50-59 years           61035 (17.7)              0 (  0.0)                \n#&gt;      60-64 years           25265 ( 7.3)              0 (  0.0)                \n#&gt;      65 years and over     80913 (23.5)              0 (  0.0)                \n#&gt;      teen                      0 ( 0.0)          52387 (100.0)                \n#&gt;   sex = Male (%)          155980 (45.2)          26543 ( 50.7)     &lt;0.001     \n#&gt;   married (%)                                                      &lt;0.001     \n#&gt;      not single           201528 (58.5)            685 (  1.3)                \n#&gt;      single               142639 (41.4)          51660 ( 98.6)                \n#&gt;      NA                      619 ( 0.2)             42 (  0.1)                \n#&gt;   race (%)                                                         &lt;0.001     \n#&gt;      Non-white             31107 ( 9.0)           7534 ( 14.4)                \n#&gt;      White                305497 (88.6)          43725 ( 83.5)                \n#&gt;      NA                     8182 ( 2.4)           1128 (  2.2)                \n#&gt;   edu (%)                                                          &lt;0.001     \n#&gt;      &lt; 2ndary              83649 (24.3)          40776 ( 77.8)                \n#&gt;      2nd grad.             59205 (17.2)           5548 ( 10.6)                \n#&gt;      Other 2nd grad.       24580 ( 7.1)           4420 (  8.4)                \n#&gt;      Post-2nd grad.       170707 (49.5)           1265 (  2.4)                \n#&gt;      NA                     6645 ( 1.9)            378 (  0.7)                \n#&gt;   income (%)                                                       &lt;0.001     \n#&gt;      $29,999 or less       93630 (27.2)           7701 ( 14.7)                \n#&gt;      $30,000-$49,999       69798 (20.2)           8142 ( 15.5)                \n#&gt;      $50,000-$79,999       73596 (21.3)          11512 ( 22.0)                \n#&gt;      $80,000 or more       63697 (18.5)          12018 ( 22.9)                \n#&gt;      NA                    44065 (12.8)          13014 ( 24.8)                \n#&gt;   bmi (%)                                                          &lt;0.001     \n#&gt;      Underweight            7277 ( 2.1)           2839 (  5.4)                \n#&gt;      healthy weight       138611 (40.2)           9922 ( 18.9)                \n#&gt;      Overweight           163701 (47.5)           2520 (  4.8)                \n#&gt;      NA                    35197 (10.2)          37106 ( 70.8)                \n#&gt;   phyact (%)                                                       &lt;0.001     \n#&gt;      Active                74738 (21.7)          23833 ( 45.5)                \n#&gt;      Inactive             176573 (51.2)          14166 ( 27.0)                \n#&gt;      Moderate              82158 (23.8)          11349 ( 21.7)                \n#&gt;      NA                    11317 ( 3.3)           3039 (  5.8)                \n#&gt;   doctor (%)                                                       &lt;0.001     \n#&gt;      No                    49874 (14.5)           8749 ( 16.7)                \n#&gt;      Yes                  294763 (85.5)          43342 ( 82.7)                \n#&gt;      NA                      149 ( 0.0)            296 (  0.6)                \n#&gt;   stress (%)                                                       &lt;0.001     \n#&gt;      Not too stressed     265391 (77.0)          21353 ( 40.8)                \n#&gt;      stressed              78044 (22.6)           4253 (  8.1)                \n#&gt;      NA                     1351 ( 0.4)          26781 ( 51.1)                \n#&gt;   smoke (%)                                                        &lt;0.001     \n#&gt;      Current smoker        88986 (25.8)           8866 ( 16.9)                \n#&gt;      Former smoker        150004 (43.5)           7566 ( 14.4)                \n#&gt;      Never smoker         104332 (30.3)          35685 ( 68.1)                \n#&gt;      NA                     1464 ( 0.4)            270 (  0.5)                \n#&gt;   drink (%)                                                        &lt;0.001     \n#&gt;      Current drinker      268269 (77.8)          27464 ( 52.4)                \n#&gt;      Former driker         50929 (14.8)           4370 (  8.3)                \n#&gt;      Never drank           20754 ( 6.0)          19916 ( 38.0)                \n#&gt;      NA                     4834 ( 1.4)            637 (  1.2)                \n#&gt;   fruit (%)                                                        &lt;0.001     \n#&gt;      0-3 daily serving     72392 (21.0)          11049 ( 21.1)                \n#&gt;      4-6 daily serving    139753 (40.5)          19627 ( 37.5)                \n#&gt;      6+ daily serving      66831 (19.4)          12026 ( 23.0)                \n#&gt;      NA                    65810 (19.1)           9685 ( 18.5)                \n#&gt;   bp (%)                                                           &lt;0.001     \n#&gt;      No                   276318 (80.1)          51846 ( 99.0)                \n#&gt;      Yes                   67763 (19.7)            308 (  0.6)                \n#&gt;      NA                      705 ( 0.2)            233 (  0.4)                \n#&gt;   copd (%)                                                         &lt;0.001     \n#&gt;      No                   291191 (84.5)              0 (  0.0)                \n#&gt;      Yes                    4508 ( 1.3)              0 (  0.0)                \n#&gt;      NA                    49087 (14.2)          52387 (100.0)                \n#&gt;   diab (%)                                                         &lt;0.001     \n#&gt;      No                   322448 (93.5)          52141 ( 99.5)                \n#&gt;      Yes                   22032 ( 6.4)            199 (  0.4)                \n#&gt;      NA                      306 ( 0.1)             47 (  0.1)                \n#&gt;   province = South (%)    338450 (98.2)          51001 ( 97.4)     &lt;0.001     \n#&gt;   weight (mean (SD))      201.76 (245.97)       189.09 (205.24)    &lt;0.001     \n#&gt;   cycle (%)                                                        &lt;0.001     \n#&gt;      11                   113323 (32.9)          17557 ( 33.5)                \n#&gt;      21                   115548 (33.5)          18524 ( 35.4)                \n#&gt;      31                   115915 (33.6)          16306 ( 31.1)                \n#&gt;   ID (mean (SD))       199143.77 (114810.36) 194922.59 (113553.38) &lt;0.001     \n#&gt;   OA (%)                                                           &lt;0.001     \n#&gt;      Control              262815 (76.2)          51727 ( 98.7)                \n#&gt;      OA                    40817 (11.8)            126 (  0.2)                \n#&gt;      NA                    41154 (11.9)            534 (  1.0)                \n#&gt;   immigrate (%)                                                    &lt;0.001     \n#&gt;      &gt; 10 years            25976 ( 7.5)            770 (  1.5)                \n#&gt;      not immigrant        289651 (84.0)          48427 ( 92.4)                \n#&gt;      recent                 8710 ( 2.5)           1934 (  3.7)                \n#&gt;      NA                    20449 ( 5.9)           1256 (  2.4)                \n#&gt;   province.check (%)                                                  NaN     \n#&gt;      NEWFOUNDLAND           6646 ( 1.9)           1278 (  2.4)                \n#&gt;      PEI                    6802 ( 2.0)            942 (  1.8)                \n#&gt;      NOVA SCOTIA           13337 ( 3.9)           2004 (  3.8)                \n#&gt;      NEW BRUNSWICK         13057 ( 3.8)           1968 (  3.8)                \n#&gt;      QU\\xc9BEC             19186 ( 5.6)           2826 (  5.4)                \n#&gt;      ONTARIO              107768 (31.3)          16053 ( 30.6)                \n#&gt;      MANITOBA              20362 ( 5.9)           3092 (  5.9)                \n#&gt;      SASKATCHEWAN          20160 ( 5.8)           3201 (  6.1)                \n#&gt;      ALBERTA               34293 ( 9.9)           5834 ( 11.1)                \n#&gt;      BRITISH COLUMBIA      43431 (12.6)           6336 ( 12.1)                \n#&gt;      YUKON/NWT/NUNAVT       4170 ( 1.2)            894 (  1.7)                \n#&gt;      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#&gt;      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#&gt;      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#&gt;      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#&gt;      QUEBEC                49806 (14.4)           6958 ( 13.3)                \n#&gt;      NFLD & LAB.            3602 ( 1.0)            509 (  1.0)                \n#&gt;      YUKON/NWT/NUNA.        2166 ( 0.6)            492 (  0.9)                \n#&gt;   age.teen (mean (SD))      0.00 (0.00)           1.00 (0.00)      &lt;0.001     \n#&gt;   age.65p (mean (SD))       0.23 (0.42)           0.00 (0.00)      &lt;0.001\n# copd has zero cells\n# analytic$age[analytic$age == 'teen'] &lt;- NA (will set this if we use copd)\n\n\nCreateTableOne(data = analytic, strata = \"age.65p\", includeNA = TRUE)\n#&gt;                       Stratified by age.65p\n#&gt;                        0                     1                     p      test\n#&gt;   n                       316260                 80913                        \n#&gt;   CVD (%)                                                          &lt;0.001     \n#&gt;      event                  9028 ( 2.9)          16496 ( 20.4)                \n#&gt;      no event             306923 (97.0)          64198 ( 79.3)                \n#&gt;      NA                      309 ( 0.1)            219 (  0.3)                \n#&gt;   age (%)                                                          &lt;0.001     \n#&gt;      20-29 years           48652 (15.4)              0 (  0.0)                \n#&gt;      30-39 years           63810 (20.2)              0 (  0.0)                \n#&gt;      40-49 years           65111 (20.6)              0 (  0.0)                \n#&gt;      50-59 years           61035 (19.3)              0 (  0.0)                \n#&gt;      60-64 years           25265 ( 8.0)              0 (  0.0)                \n#&gt;      65 years and over         0 ( 0.0)          80913 (100.0)                \n#&gt;      teen                  52387 (16.6)              0 (  0.0)                \n#&gt;   sex = Male (%)          150152 (47.5)          32371 ( 40.0)     &lt;0.001     \n#&gt;   married (%)                                                      &lt;0.001     \n#&gt;      not single           163660 (51.7)          38553 ( 47.6)                \n#&gt;      single               152077 (48.1)          42222 ( 52.2)                \n#&gt;      NA                      523 ( 0.2)            138 (  0.2)                \n#&gt;   race (%)                                                         &lt;0.001     \n#&gt;      Non-white             35329 (11.2)           3312 (  4.1)                \n#&gt;      White                274000 (86.6)          75222 ( 93.0)                \n#&gt;      NA                     6931 ( 2.2)           2379 (  2.9)                \n#&gt;   edu (%)                                                          &lt;0.001     \n#&gt;      &lt; 2ndary              84832 (26.8)          39593 ( 48.9)                \n#&gt;      2nd grad.             53974 (17.1)          10779 ( 13.3)                \n#&gt;      Other 2nd grad.       25305 ( 8.0)           3695 (  4.6)                \n#&gt;      Post-2nd grad.       147385 (46.6)          24587 ( 30.4)                \n#&gt;      NA                     4764 ( 1.5)           2259 (  2.8)                \n#&gt;   income (%)                                                       &lt;0.001     \n#&gt;      $29,999 or less       62513 (19.8)          38818 ( 48.0)                \n#&gt;      $30,000-$49,999       62296 (19.7)          15644 ( 19.3)                \n#&gt;      $50,000-$79,999       77283 (24.4)           7825 (  9.7)                \n#&gt;      $80,000 or more       72566 (22.9)           3149 (  3.9)                \n#&gt;      NA                    41602 (13.2)          15477 ( 19.1)                \n#&gt;   bmi (%)                                                          &lt;0.001     \n#&gt;      Underweight            8588 ( 2.7)           1528 (  1.9)                \n#&gt;      healthy weight       124932 (39.5)          23601 ( 29.2)                \n#&gt;      Overweight           136225 (43.1)          29996 ( 37.1)                \n#&gt;      NA                    46515 (14.7)          25788 ( 31.9)                \n#&gt;   phyact (%)                                                       &lt;0.001     \n#&gt;      Active                85384 (27.0)          13187 ( 16.3)                \n#&gt;      Inactive             144019 (45.5)          46720 ( 57.7)                \n#&gt;      Moderate              76602 (24.2)          16905 ( 20.9)                \n#&gt;      NA                    10255 ( 3.2)           4101 (  5.1)                \n#&gt;   doctor (%)                                                       &lt;0.001     \n#&gt;      No                    53972 (17.1)           4651 (  5.7)                \n#&gt;      Yes                  261866 (82.8)          76239 ( 94.2)                \n#&gt;      NA                      422 ( 0.1)             23 (  0.0)                \n#&gt;   stress (%)                                                       &lt;0.001     \n#&gt;      Not too stressed     215454 (68.1)          71290 ( 88.1)                \n#&gt;      stressed              73402 (23.2)           8895 ( 11.0)                \n#&gt;      NA                    27404 ( 8.7)            728 (  0.9)                \n#&gt;   smoke (%)                                                        &lt;0.001     \n#&gt;      Current smoker        88068 (27.8)           9784 ( 12.1)                \n#&gt;      Former smoker        115111 (36.4)          42459 ( 52.5)                \n#&gt;      Never smoker         111879 (35.4)          28138 ( 34.8)                \n#&gt;      NA                     1202 ( 0.4)            532 (  0.7)                \n#&gt;   drink (%)                                                        &lt;0.001     \n#&gt;      Current drinker      245156 (77.5)          50577 ( 62.5)                \n#&gt;      Former driker         35401 (11.2)          19898 ( 24.6)                \n#&gt;      Never drank           31888 (10.1)           8782 ( 10.9)                \n#&gt;      NA                     3815 ( 1.2)           1656 (  2.0)                \n#&gt;   fruit (%)                                                        &lt;0.001     \n#&gt;      0-3 daily serving     72908 (23.1)          10533 ( 13.0)                \n#&gt;      4-6 daily serving    124621 (39.4)          34759 ( 43.0)                \n#&gt;      6+ daily serving      61855 (19.6)          17002 ( 21.0)                \n#&gt;      NA                    56876 (18.0)          18619 ( 23.0)                \n#&gt;   bp (%)                                                           &lt;0.001     \n#&gt;      No                   282174 (89.2)          45990 ( 56.8)                \n#&gt;      Yes                   33346 (10.5)          34725 ( 42.9)                \n#&gt;      NA                      740 ( 0.2)            198 (  0.2)                \n#&gt;   copd (%)                                                         &lt;0.001     \n#&gt;      No                   213221 (67.4)          77970 ( 96.4)                \n#&gt;      Yes                    1791 ( 0.6)           2717 (  3.4)                \n#&gt;      NA                   101248 (32.0)            226 (  0.3)                \n#&gt;   diab (%)                                                         &lt;0.001     \n#&gt;      No                   305027 (96.4)          69562 ( 86.0)                \n#&gt;      Yes                   10974 ( 3.5)          11257 ( 13.9)                \n#&gt;      NA                      259 ( 0.1)             94 (  0.1)                \n#&gt;   province = South (%)    309016 (97.7)          80435 ( 99.4)     &lt;0.001     \n#&gt;   weight (mean (SD))      215.36 (255.33)       140.38 (160.88)    &lt;0.001     \n#&gt;   cycle (%)                                                        &lt;0.001     \n#&gt;      11                   106647 (33.7)          24233 ( 29.9)                \n#&gt;      21                   105506 (33.4)          28566 ( 35.3)                \n#&gt;      31                   104107 (32.9)          28114 ( 34.7)                \n#&gt;   ID (mean (SD))       197072.98 (115035.66) 204504.77 (112956.66) &lt;0.001     \n#&gt;   OA (%)                                                           &lt;0.001     \n#&gt;      Control              272881 (86.3)          41661 ( 51.5)                \n#&gt;      OA                    20507 ( 6.5)          20436 ( 25.3)                \n#&gt;      NA                    22872 ( 7.2)          18816 ( 23.3)                \n#&gt;   immigrate (%)                                                    &lt;0.001     \n#&gt;      &gt; 10 years            17607 ( 5.6)           9139 ( 11.3)                \n#&gt;      not immigrant        273622 (86.5)          64456 ( 79.7)                \n#&gt;      recent                10325 ( 3.3)            319 (  0.4)                \n#&gt;      NA                    14706 ( 4.6)           6999 (  8.7)                \n#&gt;   province.check (%)                                                  NaN     \n#&gt;      NEWFOUNDLAND           6665 ( 2.1)           1259 (  1.6)                \n#&gt;      PEI                    5993 ( 1.9)           1751 (  2.2)                \n#&gt;      NOVA SCOTIA           11896 ( 3.8)           3445 (  4.3)                \n#&gt;      NEW BRUNSWICK         11856 ( 3.7)           3169 (  3.9)                \n#&gt;      QU\\xc9BEC             18128 ( 5.7)           3884 (  4.8)                \n#&gt;      ONTARIO               97660 (30.9)          26161 ( 32.3)                \n#&gt;      MANITOBA              17967 ( 5.7)           5487 (  6.8)                \n#&gt;      SASKATCHEWAN          17507 ( 5.5)           5854 (  7.2)                \n#&gt;      ALBERTA               33445 (10.6)           6682 (  8.3)                \n#&gt;      BRITISH COLUMBIA      39394 (12.5)          10373 ( 12.8)                \n#&gt;      YUKON/NWT/NUNAVT       4765 ( 1.5)            299 (  0.4)                \n#&gt;      NOT APPLICABLE            0 ( 0.0)              0 (  0.0)                \n#&gt;      DON'T KNOW                0 ( 0.0)              0 (  0.0)                \n#&gt;      REFUSAL                   0 ( 0.0)              0 (  0.0)                \n#&gt;      NOT STATED                0 ( 0.0)              0 (  0.0)                \n#&gt;      QUEBEC                45226 (14.3)          11538 ( 14.3)                \n#&gt;      NFLD & LAB.            3279 ( 1.0)            832 (  1.0)                \n#&gt;      YUKON/NWT/NUNA.        2479 ( 0.8)            179 (  0.2)                \n#&gt;   age.teen (mean (SD))      0.17 (0.37)           0.00 (0.00)      &lt;0.001     \n#&gt;   age.65p (mean (SD))       0.00 (0.00)           1.00 (0.00)      &lt;0.001\nanalytic$age.65p &lt;- analytic$age.teen &lt;- NULL\n\nProduces frequency tables for multiple variable combinations to check the distribution of the data and identify issues.\n\ntable(analytic$province.check,analytic$fruit)\n#&gt;                   \n#&gt;                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#&gt;   NEWFOUNDLAND                  2991              3401             1237\n#&gt;   PEI                           2280              3654             1506\n#&gt;   NOVA SCOTIA                   3089              4804             1974\n#&gt;   NEW BRUNSWICK                 2989              4730             1880\n#&gt;   QU\\xc9BEC                     5568             10502             5786\n#&gt;   ONTARIO                      28752             59466            30746\n#&gt;   MANITOBA                      4561              7669             3095\n#&gt;   SASKATCHEWAN                  4173              7390             3003\n#&gt;   ALBERTA                      10828             18901             8251\n#&gt;   BRITISH COLUMBIA             10726             24422            12390\n#&gt;   YUKON/NWT/NUNAVT              1829              2045             1023\n#&gt;   NOT APPLICABLE                   0                 0                0\n#&gt;   DON'T KNOW                       0                 0                0\n#&gt;   REFUSAL                          0                 0                0\n#&gt;   NOT STATED                       0                 0                0\n#&gt;   QUEBEC                        5655             12396             7966\n#&gt;   NFLD & LAB.                      0                 0                0\n#&gt;   YUKON/NWT/NUNA.                  0                 0                0\ntable(analytic$age)\n#&gt; \n#&gt;       20-29 years       30-39 years       40-49 years       50-59 years \n#&gt;             48652             63810             65111             61035 \n#&gt;       60-64 years 65 years and over              teen \n#&gt;             25265             80913             52387\ntable(analytic$copd, analytic$age)\n#&gt;      \n#&gt;       20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#&gt;   No            0       63645       64735       60203       24638\n#&gt;   Yes           0         117         320         768         586\n#&gt;      \n#&gt;       65 years and over  teen\n#&gt;   No              77970     0\n#&gt;   Yes              2717     0\ntable(analytic$stress, analytic$age) \n#&gt;                   \n#&gt;                    20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#&gt;   Not too stressed       37117       45494       45316       45226       20948\n#&gt;   stressed               11472       18197       19639       15623        4218\n#&gt;                   \n#&gt;                    65 years and over  teen\n#&gt;   Not too stressed             71290 21353\n#&gt;   stressed                      8895  4253\n\n\nuniverse 15 + is not an issue for stress as age starts from 20\n\ncopd is problematic!\n\nCreates tables to look at the distribution of a specific variable across different cycles (time periods) of the survey. Notes differences and issues.\n\n\nfruit variable measured in an optional component (not available in all cycles)\n\n\ntable(analytic$province.check[analytic$cycle==11],\n      analytic$fruit[analytic$cycle==11])\n#&gt;                   \n#&gt;                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#&gt;   NEWFOUNDLAND                  1512              1643              689\n#&gt;   PEI                           1084              1738              773\n#&gt;   NOVA SCOTIA                   1732              2500             1036\n#&gt;   NEW BRUNSWICK                 1663              2363              934\n#&gt;   QU\\xc9BEC                     5568             10502             5786\n#&gt;   ONTARIO                      10437             19478             8809\n#&gt;   MANITOBA                      2604              4214             1526\n#&gt;   SASKATCHEWAN                  2386              3957             1387\n#&gt;   ALBERTA                       4391              7050             2664\n#&gt;   BRITISH COLUMBIA              4321              9350             4278\n#&gt;   YUKON/NWT/NUNAVT               999              1014              448\n#&gt;   NOT APPLICABLE                   0                 0                0\n#&gt;   DON'T KNOW                       0                 0                0\n#&gt;   REFUSAL                          0                 0                0\n#&gt;   NOT STATED                       0                 0                0\n#&gt;   QUEBEC                           0                 0                0\n#&gt;   NFLD & LAB.                      0                 0                0\n#&gt;   YUKON/NWT/NUNA.                  0                 0                0\n\n\ntable(analytic$province.check[analytic$cycle==21],\n      analytic$fruit[analytic$cycle==21])\n#&gt;                   \n#&gt;                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#&gt;   NEWFOUNDLAND                  1479              1758              548\n#&gt;   PEI                            615               947              344\n#&gt;   NOVA SCOTIA                   1357              2304              938\n#&gt;   NEW BRUNSWICK                 1326              2367              946\n#&gt;   QU\\xc9BEC                        0                 0                0\n#&gt;   ONTARIO                       9365             20356            10933\n#&gt;   MANITOBA                      1957              3455             1569\n#&gt;   SASKATCHEWAN                  1787              3433             1616\n#&gt;   ALBERTA                       3326              6376             3046\n#&gt;   BRITISH COLUMBIA              3186              7727             4224\n#&gt;   YUKON/NWT/NUNAVT               830              1031              575\n#&gt;   NOT APPLICABLE                   0                 0                0\n#&gt;   DON'T KNOW                       0                 0                0\n#&gt;   REFUSAL                          0                 0                0\n#&gt;   NOT STATED                       0                 0                0\n#&gt;   QUEBEC                        5655             12396             7966\n#&gt;   NFLD & LAB.                      0                 0                0\n#&gt;   YUKON/NWT/NUNA.                  0                 0                0\n# a different QUEBEC spelling used\n\n\ntable(analytic$province.check[analytic$cycle==31],\n      analytic$fruit[analytic$cycle==31])\n#&gt;                   \n#&gt;                    0-3 daily serving 4-6 daily serving 6+ daily serving\n#&gt;   NEWFOUNDLAND                     0                 0                0\n#&gt;   PEI                            581               969              389\n#&gt;   NOVA SCOTIA                      0                 0                0\n#&gt;   NEW BRUNSWICK                    0                 0                0\n#&gt;   QU\\xc9BEC                        0                 0                0\n#&gt;   ONTARIO                       8950             19632            11004\n#&gt;   MANITOBA                         0                 0                0\n#&gt;   SASKATCHEWAN                     0                 0                0\n#&gt;   ALBERTA                       3111              5475             2541\n#&gt;   BRITISH COLUMBIA              3219              7345             3888\n#&gt;   YUKON/NWT/NUNAVT                 0                 0                0\n#&gt;   NOT APPLICABLE                   0                 0                0\n#&gt;   DON'T KNOW                       0                 0                0\n#&gt;   REFUSAL                          0                 0                0\n#&gt;   NOT STATED                       0                 0                0\n#&gt;   QUEBEC                           0                 0                0\n#&gt;   NFLD & LAB.                      0                 0                0\n#&gt;   YUKON/NWT/NUNA.                  0                 0                0\n# The real problem!\n\n\nLook at data dictionaries in all cycles\n\ncycle 1.1 FVCADTOT Universe: All respondents\ncycle 2.1 FVCCDTOT Universe: All respondents\ncycle 3.1 FVCEDTOT Universe: Respondents with FVCEFOPT = 1\n\n\n\nBelow we delete or modify problematic data, and removes unnecessary variables. Checks the dimensions before and after data cleanup.\n\ndim(analytic)\n#&gt; [1] 397173     24\nanalytic1 &lt;- analytic\n# analytic1$South[analytic1$province.check == \"NFLD & LAB.\"] &lt;- NA\n# analytic1$South[analytic1$province.check == \"YUKON/NWT/NUNA.\"] &lt;- NA\n# analytic1 &lt;- subset(analytic, province.check != \"NFLD & LAB.\" & \n#                       province.check != \"YUKON/NWT/NUNA.\" )\ndim(analytic1)\n#&gt; [1] 397173     24\n\nanalytic1$copd &lt;- NULL # will bring this later for missing data analysis\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n# analytic1 &lt;- droplevels.data.frame(analytic1)\nanalytic1$province.check &lt;- NULL # we already have simplified province variable\n# CreateTableOne(data = analytic1, strata = \"OA\", includeNA = TRUE)\n\nSet appropriate reference\nSave the original data (with missing values)!\n\nanalytic.miss &lt;- analytic1\n\nRelevels factors in the dataset so that a specific level is set as the reference level. This is often needed for statistical analysis.\n\nanalytic.miss$smoke &lt;- relevel(as.factor(analytic.miss$smoke), ref='Never smoker')\nanalytic.miss$drink &lt;- relevel(as.factor(analytic.miss$drink), ref='Never drank')\nanalytic.miss$province &lt;- relevel(as.factor(analytic.miss$province), ref='South')\nanalytic.miss$immigrate &lt;- relevel(as.factor(analytic.miss$immigrate), ref='not immigrant')\n\nComplete data options\nCreates a new dataset that omits all rows containing any missing values. This is generally not recommended for most data analysis, as it can introduce bias.\n\n# Wrong thing to do for survey data analysis!!\nanalytic2 &lt;- as.data.frame(na.omit(analytic1)) \ndim(analytic2)\n#&gt; [1] 185613     22\n# tab1 &lt;- CreateTableOne(data = analytic2, strata = \"OA\", includeNA = TRUE)\n# print(tab1, test=FALSE, showAllLevels = TRUE)\n\nSaving dataset\nLet us check the dimensions of multiple data objects and then save them to a file for future use.\n\ndim(cc123a)\n#&gt; [1] 397173     25\ndim(analytic)\n#&gt; [1] 397173     24\ndim(analytic.miss)\n#&gt; [1] 397173     22\ndim(analytic2)\n#&gt; [1] 185613     22\nsave(analytic.miss, analytic2, file = \"Data/surveydata/cchs123b.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Assessing data"
    ]
  },
  {
    "objectID": "surveydata3.html",
    "href": "surveydata3.html",
    "title": "CCHS: Bivariate analysis",
    "section": "",
    "text": "The following tutorial is performing bivariate analysis on our CCHS analytic dataset to examine relationships between two variables (association question).\nWe load several R packages required for bivariate analysis, statistical tests, and data visualization.\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\n\nLoad data\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/surveydata/cchs123b.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"\n\nPreparing data\nWeights\nHere, the weights of survey respondents are accumulated, to account for the combination of different cycles of the data.\n\nanalytic.miss$weight &lt;- analytic.miss$weight/3 # 3 cycles combined\n\nFixing variable types\nWe convert several variables to categorical or “factor” types, which are better suited for some statistical analysis when variables have categories.\n\nvar.names &lt;- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"OA\", \"immigrate\")\nanalytic.miss[var.names] &lt;- lapply(analytic.miss[var.names] , factor)\nstr(analytic.miss)\n#&gt; 'data.frame':    397173 obs. of  22 variables:\n#&gt;  $ CVD      : Factor w/ 2 levels \"event\",\"no event\": 1 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ age      : Factor w/ 7 levels \"20-29 years\",..: 6 6 2 6 1 6 3 7 1 1 ...\n#&gt;  $ sex      : Factor w/ 2 levels \"Female\",\"Male\": 1 1 2 1 1 2 2 2 1 2 ...\n#&gt;  $ married  : Factor w/ 2 levels \"not single\",\"single\": 2 2 1 2 2 1 1 2 2 2 ...\n#&gt;  $ race     : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ edu      : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 2 4 4 4 4 4 4 1 4 4 ...\n#&gt;  $ income   : Factor w/ 4 levels \"$29,999 or less\",..: 1 1 4 1 2 2 1 1 NA 4 ...\n#&gt;  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#&gt;  $ phyact   : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 2 2 2 2 2 1 1 2 3 ...\n#&gt;  $ doctor   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ stress   : Factor w/ 2 levels \"Not too stressed\",..: 1 1 2 1 1 1 1 NA 1 1 ...\n#&gt;  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#&gt;  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#&gt;  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#&gt;  $ bp       : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ diab     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ weight   : num  47.6 23.8 56.1 23.8 65.4 ...\n#&gt;  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ OA       : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\nThe code identifies rows where data is missing and labels them for later analyses.\n\nanalytic.miss$miss &lt;- 1\nhead(analytic.miss$ID) # full data\n#&gt; [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#&gt; [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#&gt; [1]  3  5  7 10 11 13\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] &lt;- 0\ntable(analytic.miss$miss)\n#&gt; \n#&gt;      0      1 \n#&gt; 185613 211560\n\nSetting Design\nThe code sets up the survey design, specifying weights (but no specific clustering and stratification, as they are unavailable for CCHS public access data), for use in survey-weighted analyses.\n\nrequire(survey)\nsummary(analytic.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#&gt; [1] 80.34263\n\nThis creates a subset of the data where there are no missing values. Note that subset was done to the design object w.design0, not the data analytic.miss.\n\nw.design &lt;- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   23.85   45.98   71.54   87.30 2384.98\nsd(weights(w.design))\n#&gt; [1] 84.97819\n\nBivariate analysis\nTable 1 (weighted)\nStratified by exposure\nThese tables contain descriptive statistics, stratified by different categories. They can be useful for understanding how variables relate to the exposure or outcome in the data.\n\nrequire(tableone)\nvar.names &lt;- c(\"CVD\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"OA\"\n# tab1 &lt;- CreateTableOne(var = var.names, strata= \"OA\", data=analytic.miss, test = TRUE)\n# print(tab1)\ntab2 &lt;- svyCreateTableOne(var = var.names, strata= \"OA\", \n                          data=w.design, test = TRUE)\nprint(tab2)\n#&gt;                        Stratified by OA\n#&gt;                         Control            OA                p      test\n#&gt;   n                     12124961.5         1153392.2                    \n#&gt;   CVD = no event (%)    11786450.6 (97.2)  1020965.5 (88.5)  &lt;0.001     \n#&gt;   age (%)                                                    &lt;0.001     \n#&gt;      20-29 years         2668880.8 (22.0)    28317.5 ( 2.5)             \n#&gt;      30-39 years         3009426.7 (24.8)    77159.3 ( 6.7)             \n#&gt;      40-49 years         3108300.9 (25.6)   211515.1 (18.3)             \n#&gt;      50-59 years         1900845.3 (15.7)   350264.7 (30.4)             \n#&gt;      60-64 years          546041.8 ( 4.5)   163724.7 (14.2)             \n#&gt;      65 years and over    624706.7 ( 5.2)   322033.4 (27.9)             \n#&gt;      teen                 266759.3 ( 2.2)      377.4 ( 0.0)             \n#&gt;   sex = Male (%)         6374765.5 (52.6)   379850.5 (32.9)  &lt;0.001     \n#&gt;   married = single (%)   4120611.0 (34.0)   367647.7 (31.9)  &lt;0.001     \n#&gt;   race = White (%)      10312228.2 (85.0)  1081778.6 (93.8)  &lt;0.001     \n#&gt;   edu (%)                                                    &lt;0.001     \n#&gt;      &lt; 2ndary            1752318.3 (14.5)   309652.8 (26.8)             \n#&gt;      2nd grad.           2314713.1 (19.1)   203437.5 (17.6)             \n#&gt;      Other 2nd grad.     1078645.2 ( 8.9)    79255.1 ( 6.9)             \n#&gt;      Post-2nd grad.      6979284.9 (57.6)   561046.8 (48.6)             \n#&gt;   income (%)                                                 &lt;0.001     \n#&gt;      $29,999 or less     2051640.6 (16.9)   353862.9 (30.7)             \n#&gt;      $30,000-$49,999     2436063.7 (20.1)   272484.1 (23.6)             \n#&gt;      $50,000-$79,999     3495902.5 (28.8)   275115.8 (23.9)             \n#&gt;      $80,000 or more     4141354.6 (34.2)   251929.4 (21.8)             \n#&gt;   bmi (%)                                                    &lt;0.001     \n#&gt;      Underweight          346004.9 ( 2.9)    22064.6 ( 1.9)             \n#&gt;      healthy weight      6019004.1 (49.6)   431570.2 (37.4)             \n#&gt;      Overweight          5759952.5 (47.5)   699757.4 (60.7)             \n#&gt;   phyact (%)                                                 &lt;0.001     \n#&gt;      Active              3037314.2 (25.1)   216879.5 (18.8)             \n#&gt;      Inactive            5982492.3 (49.3)   647856.2 (56.2)             \n#&gt;      Moderate            3105154.9 (25.6)   288656.5 (25.0)             \n#&gt;   doctor = Yes (%)      10087473.8 (83.2)  1090763.9 (94.6)  &lt;0.001     \n#&gt;   stress = stressed (%)  3123770.9 (25.8)   301895.2 (26.2)   0.420     \n#&gt;   smoke (%)                                                  &lt;0.001     \n#&gt;      Never smoker        4043479.9 (33.3)   320323.7 (27.8)             \n#&gt;      Current smoker      3219168.6 (26.5)   275835.7 (23.9)             \n#&gt;      Former smoker       4862313.0 (40.1)   557232.7 (48.3)             \n#&gt;   drink (%)                                                  &lt;0.001     \n#&gt;      Never drank          678435.7 ( 5.6)    66085.0 ( 5.7)             \n#&gt;      Current drinker    10297713.4 (84.9)   887808.2 (77.0)             \n#&gt;      Former driker       1148812.3 ( 9.5)   199498.9 (17.3)             \n#&gt;   fruit (%)                                                  &lt;0.001     \n#&gt;      0-3 daily serving   3214156.0 (26.5)   236483.8 (20.5)             \n#&gt;      4-6 daily serving   6001124.3 (49.5)   588323.8 (51.0)             \n#&gt;      6+ daily serving    2909681.1 (24.0)   328584.5 (28.5)             \n#&gt;   bp = Yes (%)           1212548.2 (10.0)   347269.8 (30.1)  &lt;0.001     \n#&gt;   diab = Yes (%)          377876.2 ( 3.1)   104541.0 ( 9.1)  &lt;0.001     \n#&gt;   province = North (%)     27124.3 ( 0.2)     1825.1 ( 0.2)  &lt;0.001     \n#&gt;   immigrate (%)                                              &lt;0.001     \n#&gt;      not immigrant       9898636.6 (81.6)   994682.5 (86.2)             \n#&gt;      &gt; 10 years          1384672.6 (11.4)   146879.8 (12.7)             \n#&gt;      recent               841652.3 ( 6.9)    11829.8 ( 1.0)\n\nStratified by outcome\nThis table is generally useful for logistic regression analysis\n\nvar.names &lt;- c(\"OA\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \n               \"phyact\", \"doctor\", \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \n               \"diab\", \"province\", \"immigrate\") # exclude \"CVD\"\ntab3 &lt;- svyCreateTableOne(var = var.names, strata= \"CVD\", data=w.design, test = TRUE)\nprint(tab3)\n#&gt;                        Stratified by CVD\n#&gt;                         event            no event           p      test\n#&gt;   n                     470937.5         12807416.1                    \n#&gt;   OA = OA (%)           132426.7 (28.1)   1020965.5 ( 8.0)  &lt;0.001     \n#&gt;   age (%)                                                   &lt;0.001     \n#&gt;      20-29 years         14966.0 ( 3.2)   2682232.3 (20.9)             \n#&gt;      30-39 years         24105.5 ( 5.1)   3062480.5 (23.9)             \n#&gt;      40-49 years         63520.1 (13.5)   3256296.0 (25.4)             \n#&gt;      50-59 years        122613.0 (26.0)   2128497.0 (16.6)             \n#&gt;      60-64 years         72328.3 (15.4)    637438.1 ( 5.0)             \n#&gt;      65 years and over  172742.2 (36.7)    773997.8 ( 6.0)             \n#&gt;      teen                  662.3 ( 0.1)    266474.4 ( 2.1)             \n#&gt;   sex = Male (%)        267743.8 (56.9)   6486872.2 (50.6)  &lt;0.001     \n#&gt;   married = single (%)  143747.0 (30.5)   4344511.8 (33.9)  &lt;0.001     \n#&gt;   race = White (%)      434591.6 (92.3)  10959415.2 (85.6)  &lt;0.001     \n#&gt;   edu (%)                                                   &lt;0.001     \n#&gt;      &lt; 2ndary           147338.4 (31.3)   1914632.6 (14.9)             \n#&gt;      2nd grad.           77705.6 (16.5)   2440445.0 (19.1)             \n#&gt;      Other 2nd grad.     30921.3 ( 6.6)   1126979.0 ( 8.8)             \n#&gt;      Post-2nd grad.     214972.2 (45.6)   7325359.4 (57.2)             \n#&gt;   income (%)                                                &lt;0.001     \n#&gt;      $29,999 or less    164929.4 (35.0)   2240574.2 (17.5)             \n#&gt;      $30,000-$49,999    109988.2 (23.4)   2598559.6 (20.3)             \n#&gt;      $50,000-$79,999    103091.1 (21.9)   3667927.1 (28.6)             \n#&gt;      $80,000 or more     92928.7 (19.7)   4300355.3 (33.6)             \n#&gt;   bmi (%)                                                   &lt;0.001     \n#&gt;      Underweight          8844.4 ( 1.9)    359225.0 ( 2.8)             \n#&gt;      healthy weight     173475.1 (36.8)   6277099.2 (49.0)             \n#&gt;      Overweight         288617.9 (61.3)   6171091.9 (48.2)             \n#&gt;   phyact (%)                                                &lt;0.001     \n#&gt;      Active              85140.3 (18.1)   3169053.4 (24.7)             \n#&gt;      Inactive           274968.8 (58.4)   6355379.7 (49.6)             \n#&gt;      Moderate           110828.4 (23.5)   3282983.0 (25.6)             \n#&gt;   doctor = Yes (%)      445493.3 (94.6)  10732744.5 (83.8)  &lt;0.001     \n#&gt;   stress = stressed (%) 113282.5 (24.1)   3312383.7 (25.9)   0.023     \n#&gt;   smoke (%)                                                 &lt;0.001     \n#&gt;      Never smoker       119434.6 (25.4)   4244368.9 (33.1)             \n#&gt;      Current smoker      97328.0 (20.7)   3397676.3 (26.5)             \n#&gt;      Former smoker      254174.9 (54.0)   5165370.9 (40.3)             \n#&gt;   drink (%)                                                 &lt;0.001     \n#&gt;      Never drank         29444.3 ( 6.3)    715076.4 ( 5.6)             \n#&gt;      Current drinker    344405.1 (73.1)  10841116.6 (84.6)             \n#&gt;      Former driker       97088.1 (20.6)   1251223.1 ( 9.8)             \n#&gt;   fruit (%)                                                  0.001     \n#&gt;      0-3 daily serving  111803.5 (23.7)   3338836.3 (26.1)             \n#&gt;      4-6 daily serving  233403.5 (49.6)   6356044.7 (49.6)             \n#&gt;      6+ daily serving   125730.4 (26.7)   3112535.1 (24.3)             \n#&gt;   bp = Yes (%)          209257.0 (44.4)   1350561.0 (10.5)  &lt;0.001     \n#&gt;   diab = Yes (%)         78762.9 (16.7)    403654.4 ( 3.2)  &lt;0.001     \n#&gt;   province = North (%)     702.8 ( 0.1)     28246.6 ( 0.2)   0.005     \n#&gt;   immigrate (%)                                             &lt;0.001     \n#&gt;      not immigrant      389553.0 (82.7)  10503766.2 (82.0)             \n#&gt;      &gt; 10 years          69008.0 (14.7)   1462544.4 (11.4)             \n#&gt;      recent              12376.5 ( 2.6)    841105.5 ( 6.6)\n\nHow did they calculate the p-values? Hint: svychisq (see below).\nProportions and Design Effect\nThis part computes proportions and design effects, which help understand the influence of the sampling design on the estimated statistics.\n\nrequire(survey)\n# Computing survey statistics on subsets of a survey defined by factor(s).\nfit0a &lt;- svyby(~CVD,~OA,design=w.design, svymean,deff=TRUE)\nfit0a\n\n\n  \n\n\nconfint(fit0a)\n#&gt;                          2.5 %    97.5 %\n#&gt; Control:CVDevent    0.02681661 0.0290204\n#&gt; OA:CVDevent         0.10847000 0.1211599\n#&gt; Control:CVDno event 0.97097960 0.9731834\n#&gt; OA:CVDno event      0.87884010 0.8915300\n# 7.45% OA patients estimated to have CVD event.\n# 95% CI:  (0.067, 0.0816)\n\nLet\n\n\n\\(\\theta\\) = parameter (population slope) and\n\n\n\\(\\hat(\\theta)\\) = statistic (estimated slope).\n\n\\(b = \\frac{\\sum[w (y_i-\\bar{y}) (x_i-\\bar{x})]}{\\sum[w (x_i-\\bar{x})^2]}\\)\nDE = Effect of complex survey on the SEs, relative to a SRS of equal size.\n\n\\(D^2(\\hat{\\theta}) = \\frac{Var(\\hat{\\theta})_{Complex Survey}}{Var(\\hat{\\theta})_{SRS}}\\)\n\\(D^2(\\hat{\\theta}) = \\frac{SE(\\hat{\\theta})^2_{Complex Survey}}{SE(\\hat{\\theta})^2_{SRS}}\\)\n\nNote:\n\nSE increases as value of weight increases (CCHS).\nNHANES has more things to worry about (strata, PSU)\n\nDEFF = 2 means that the variance of the sample proportion, when choosing the sample by complex survey sampling, is nearly 2 times as large as the variance of the same estimator under simple random sampling/SRS.\n\nfit0b &lt;- svyby(~CVD,~diab,design=w.design, svymean,deff=TRUE)\nfit0b\n\n\n  \n\n\nconfint(fit0b)\n#&gt;                     2.5 %     97.5 %\n#&gt; No:CVDevent     0.0295633 0.03173344\n#&gt; Yes:CVDevent    0.1505917 0.17594247\n#&gt; No:CVDno event  0.9682666 0.97043670\n#&gt; Yes:CVDno event 0.8240575 0.84940825\n\nTesting association\nHere, Chi-square tests are conducted to test the association between different variables. Two variants of the test are used: Rao-Scott and Thomas-Rao modifications. These adaptations are used when the data come from a complex survey design.\n\nTests for hypothesis\n\nRao-Scott modifications (chi-sq)\nThomas-Rao modifications (F)\n\n\n\n\n# Rao-Scott modifications (chi-sq)\nsvychisq(~CVD+OA,design=w.design, statistic=\"Chisq\")\n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~CVD + OA, design = w.design, statistic = \"Chisq\")\n#&gt; X-squared = 3249.7, df = 1, p-value &lt; 2.2e-16\n\n# Thomas-Rao modifications (F)\nsvychisq(~CVD+OA,design=w.design, statistic=\"F\") \n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~CVD + OA, design = w.design, statistic = \"F\")\n#&gt; F = 1863, ndf = 1, ddf = 185612, p-value &lt; 2.2e-16\n\n# Both provide strong evidence to reject the null hypothesis.\n# Conclusion: there is a significant (at 5%) association \n# between CVD prevalence and OA.\nsvychisq(~CVD+fruit,design=w.design, statistic=\"F\") \n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~CVD + fruit, design = w.design, statistic = \"F\")\n#&gt; F = 7.1241, ndf = 1.9758e+00, ddf = 3.6673e+05, p-value = 0.0008503\nsvychisq(~CVD+province,design=w.design, statistic=\"Chisq\") \n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~CVD + province, design = w.design, statistic = \"Chisq\")\n#&gt; X-squared = 1.4848, df = 1, p-value = 0.00492\n\nSaving data\nFinally, the dataset, along with any new variables or subsets created during the analysis, is saved for future use.\n\nsave(w.design, analytic.miss, analytic2, file = \"Data/surveydata/cchs123w.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Bivariate analysis"
    ]
  },
  {
    "objectID": "surveydata4.html",
    "href": "surveydata4.html",
    "title": "CCHS: Regression",
    "section": "",
    "text": "This tutorial is for a complex data analysis, specifically using regression techniques to analyze survey data.\nLet us load necessary R packages for the analysis:\n\n# Load required packages\nlibrary(survey)\nlibrary(knitr)\nlibrary(car)\nlibrary(tableone)\nlibrary(DataExplorer)\nlibrary(Publish)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(MASS)\n\nLoad data\nLoads a dataset and provides some quick data checks, like the dimensions and summary of weights.\n\nload(\"Data/surveydata/cchs123w.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"     \"w.design\"\ndim(analytic.miss)\n#&gt; [1] 397173     23\ndim(analytic2)\n#&gt; [1] 185613     22\nsummary(weights(w.design))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   23.85   45.98   71.54   87.30 2384.98\n\nLogistic for complex survey\nPerforms a simple logistic regression using the complex survey data, focusing on the relationship between cardiovascular disease and osteoarthritis.\n\nformula0 &lt;- as.formula(I(CVD==\"event\") ~ OA)\n\n## Crude regression\nfit2 &lt;- svyglm(formula0, \n              design = w.design, \n              family = binomial(logit))\nrequire(Publish)\npublish(fit2)\n#&gt;  Variable   Units OddsRatio       CI.95 p-value \n#&gt;        OA Control       Ref                     \n#&gt;                OA      4.52 [4.19;4.87]  &lt;1e-04\n\nMultivariable analysis\nRuns a more complex logistic regression model, adding multiple covariates to better understand the relationship.\n\nformula1 &lt;- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race + \n              edu + income + bmi + phyact + doctor + stress + \n              smoke + drink + fruit + bp + diab + province + immigrate)\n\nfit3 &lt;- svyglm(formula1, \n              design = w.design, \n              family = binomial(logit))\npublish(fit3)\n#&gt;   Variable             Units OddsRatio         CI.95     p-value \n#&gt;         OA           Control       Ref                           \n#&gt;                           OA      1.52   [1.40;1.66]     &lt; 1e-04 \n#&gt;        age       20-29 years       Ref                           \n#&gt;                  30-39 years      1.29   [0.98;1.69]   0.0707636 \n#&gt;                  40-49 years      2.74   [2.17;3.47]     &lt; 1e-04 \n#&gt;                  50-59 years      6.24   [4.97;7.83]     &lt; 1e-04 \n#&gt;                  60-64 years      9.71  [7.68;12.29]     &lt; 1e-04 \n#&gt;            65 years and over     15.85 [12.57;20.00]     &lt; 1e-04 \n#&gt;                         teen      0.46   [0.20;1.07]   0.0707295 \n#&gt;        sex            Female       Ref                           \n#&gt;                         Male      1.73   [1.60;1.88]     &lt; 1e-04 \n#&gt;    married        not single       Ref                           \n#&gt;                       single      0.97   [0.90;1.05]   0.5209444 \n#&gt;       race         Non-white       Ref                           \n#&gt;                        White      1.44   [1.19;1.75]   0.0002219 \n#&gt;        edu          &lt; 2ndary       Ref                           \n#&gt;                    2nd grad.      0.90   [0.80;1.00]   0.0512330 \n#&gt;              Other 2nd grad.      0.97   [0.83;1.13]   0.6737665 \n#&gt;               Post-2nd grad.      0.93   [0.85;1.02]   0.1016562 \n#&gt;     income   $29,999 or less       Ref                           \n#&gt;              $30,000-$49,999      0.74   [0.67;0.81]     &lt; 1e-04 \n#&gt;              $50,000-$79,999      0.64   [0.58;0.72]     &lt; 1e-04 \n#&gt;              $80,000 or more      0.58   [0.51;0.66]     &lt; 1e-04 \n#&gt;        bmi       Underweight       Ref                           \n#&gt;               healthy weight      0.86   [0.67;1.10]   0.2350526 \n#&gt;                   Overweight      0.88   [0.69;1.12]   0.3033213 \n#&gt;     phyact            Active       Ref                           \n#&gt;                     Inactive      1.21   [1.10;1.34]   0.0001345 \n#&gt;                     Moderate      1.08   [0.97;1.21]   0.1771985 \n#&gt;     doctor                No       Ref                           \n#&gt;                          Yes      1.75   [1.49;2.06]     &lt; 1e-04 \n#&gt;     stress  Not too stressed       Ref                           \n#&gt;                     stressed      1.30   [1.18;1.42]     &lt; 1e-04 \n#&gt;      smoke      Never smoker       Ref                           \n#&gt;               Current smoker      1.18   [1.05;1.32]   0.0050518 \n#&gt;                Former smoker      1.21   [1.11;1.33]     &lt; 1e-04 \n#&gt;      drink       Never drank       Ref                           \n#&gt;              Current drinker      0.82   [0.68;0.98]   0.0290605 \n#&gt;                Former driker      1.13   [0.93;1.36]   0.2133779 \n#&gt;      fruit 0-3 daily serving       Ref                           \n#&gt;            4-6 daily serving      0.94   [0.86;1.03]   0.1758214 \n#&gt;             6+ daily serving      1.09   [0.97;1.23]   0.1311029 \n#&gt;         bp                No       Ref                           \n#&gt;                          Yes      2.35   [2.16;2.55]     &lt; 1e-04 \n#&gt;       diab                No       Ref                           \n#&gt;                          Yes      1.86   [1.66;2.07]     &lt; 1e-04 \n#&gt;   province             South       Ref                           \n#&gt;                        North      1.21   [0.90;1.62]   0.2103030 \n#&gt;  immigrate     not immigrant       Ref                           \n#&gt;                   &gt; 10 years      1.02   [0.89;1.16]   0.8057243 \n#&gt;                       recent      1.06   [0.73;1.53]   0.7651069\n\nModel fit assessment\nVariability explained\nPseudo-R-square values indicate how much of the total variability in the outcomes is explainable by the fitted model (analogous to R-square). For a continuous outcome, we can compute R-square, while R-square cannot be calculated when the outcome variable is categorical, nominal or ordinal. We use pseudo-R-squared measures when the dependent variable is not continuous but a likelihood function is used to fit a model. Popular Pseudo-R-square measures are:\n\n\nCox/Snell (never reaches max 1)\n\nNagelkerke R-square (scaled to max 1)\n\n\nThe larger Cox & Snell estimate is the better the model.\nThese Pseudo-R-square values should be interpreted with caution (if not ignored).\nThey offer little confidence in interpreting the model fit.\nSurvey weighted version of them are available.\nNot trivial to decide which statistic to use under complex surveys.\n\nEvaluates the model fit using Akaike Information Criterion (AIC) and pseudo R-squared metrics.\n\nfit3 &lt;- svyglm(formula1, \n              design = w.design, \n              family = quasibinomial(logit)) # publish does not work\nAIC(fit3) \n#&gt;        eff.p          AIC     deltabar \n#&gt;    67.752064 45362.706032     2.053093\n\n# AIC for survey weighted regressions\npsrsq(fit3, method = \"Cox-Snell\")\n#&gt; [1] 0.06091896\npsrsq(fit3, method = \"Nagelkerke\")\n#&gt; [1] 0.2307586\n# Nagelkerke and Cox-Snell pseudo-rsquared statistics\n\nBackward Elimination\n\nModel comparisons\n\nLRT-aprroximation\nWald-based\n\n\n\nChecking one by one\nChecks the significance of each variable one by one and removes those that are not statistically significant.\n\nround(sort(summary(fit3)$coef[,\"Pr(&gt;|t|)\"]),2)\n#&gt;            (Intercept)   age65 years and over                  bpYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age60-64 years         age50-59 years                sexMale \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;                diabYes                   OAOA  income$80,000 or more \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age40-49 years  income$50,000-$79,999              doctorYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         phyactInactive              raceWhite    smokeCurrent smoker \n#&gt;                   0.00                   0.00                   0.01 \n#&gt;   drinkCurrent drinker           edu2nd grad.                ageteen \n#&gt;                   0.03                   0.05                   0.07 \n#&gt;         age30-39 years      eduPost-2nd grad.  fruit6+ daily serving \n#&gt;                   0.07                   0.10                   0.13 \n#&gt; fruit4-6 daily serving         phyactModerate          provinceNorth \n#&gt;                   0.18                   0.18                   0.21 \n#&gt;     drinkFormer driker      bmihealthy weight          bmiOverweight \n#&gt;                   0.21                   0.24                   0.30 \n#&gt;          marriedsingle     eduOther 2nd grad.        immigraterecent \n#&gt;                   0.52                   0.67                   0.77 \n#&gt;    immigrate&gt; 10 years \n#&gt;                   0.81\n# bmiOverweight is associated with largest p-value\n# but what about other categories?\n\nregTermTest(fit3,~bmi) # coef of all bmi cat = 0\n#&gt; Wald test for bmi\n#&gt;  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#&gt; F =  0.7591291  on  2  and  185579  df: p= 0.46808\nfit4 &lt;- update(fit3, .~. -bmi) \n\nanova(fit3, fit4)\n#&gt; Working (Rao-Scott+F) LRT for bmi\n#&gt;  in svyglm(formula = formula1, design = w.design, family = quasibinomial(logit))\n#&gt; Working 2logLR =  1.424634 p= 0.49071 \n#&gt; (scale factors:  1.1 0.93 );  denominator df= 185579\n# high p-value (in both wald and Anova) makes it more likely that you should exclude bmi\nAIC(fit3,fit4) \n#&gt;         eff.p      AIC deltabar\n#&gt; [1,] 67.75206 45362.71 2.053093\n#&gt; [2,] 64.30460 45358.26 2.074342\nround(sort(summary(fit4)$coef[,\"Pr(&gt;|t|)\"]),2)\n#&gt;            (Intercept)   age65 years and over                  bpYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age60-64 years         age50-59 years                sexMale \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;                diabYes                   OAOA  income$80,000 or more \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age40-49 years  income$50,000-$79,999              doctorYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         phyactInactive              raceWhite    smokeCurrent smoker \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#&gt;                   0.03                   0.05                   0.07 \n#&gt;                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#&gt;                   0.07                   0.10                   0.13 \n#&gt; fruit4-6 daily serving         phyactModerate          provinceNorth \n#&gt;                   0.17                   0.17                   0.21 \n#&gt;     drinkFormer driker          marriedsingle     eduOther 2nd grad. \n#&gt;                   0.21                   0.53                   0.67 \n#&gt;        immigraterecent    immigrate&gt; 10 years \n#&gt;                   0.76                   0.81\n\nUsing AIC to automate\nUses stepwise regression guided by AIC to automatically select the most important variables.\n\nrequire(MASS)\nformula1b &lt;- as.formula(I(CVD==\"event\") ~ OA + age + sex)\nfit1b &lt;- svyglm(formula1b, \n              design = w.design, \n              family = binomial(logit))\nfit5 &lt;- stepAIC(fit1b, direction = \"backward\")\n#&gt; Start:  AIC=47384.51\n#&gt; I(CVD == \"event\") ~ OA + age + sex\n#&gt;        Df Deviance   AIC\n#&gt; &lt;none&gt;       47353 47385\n#&gt; - sex   1    47634 47658\n#&gt; - OA    1    47679 47702\n#&gt; - age   6    54414 54291\n\n\npublish(fit5)\n#&gt;  Variable             Units OddsRatio         CI.95   p-value \n#&gt;        OA           Control       Ref                         \n#&gt;                          OA      1.81   [1.66;1.97]   &lt; 1e-04 \n#&gt;       age       20-29 years       Ref                         \n#&gt;                 30-39 years      1.40   [1.07;1.84]   0.01374 \n#&gt;                 40-49 years      3.39   [2.69;4.27]   &lt; 1e-04 \n#&gt;                 50-59 years      9.42  [7.55;11.76]   &lt; 1e-04 \n#&gt;                 60-64 years     17.78 [14.22;22.23]   &lt; 1e-04 \n#&gt;           65 years and over     33.82 [27.26;41.97]   &lt; 1e-04 \n#&gt;                        teen      0.45   [0.20;1.02]   0.05462 \n#&gt;       sex            Female       Ref                         \n#&gt;                        Male      1.57   [1.46;1.69]   &lt; 1e-04\nround(sort(summary(fit5)$coef[,\"Pr(&gt;|t|)\"]),2)\n#&gt;          (Intercept) age65 years and over       age60-64 years \n#&gt;                 0.00                 0.00                 0.00 \n#&gt;       age50-59 years                 OAOA              sexMale \n#&gt;                 0.00                 0.00                 0.00 \n#&gt;       age40-49 years       age30-39 years              ageteen \n#&gt;                 0.00                 0.01                 0.05\n\nUsing AIC, but keeping importants\nSimilar to the previous step but ensures certain important variables remain in the model.\n\nformula1c &lt;- as.formula(I(CVD==\"event\") ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate)\nscope &lt;- list(upper = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab + \n                         doctor + stress + smoke + drink + province + immigrate,\n              lower = ~ OA + age + sex + married + race +\n                         edu + income + bmi + phyact + fruit + bp + diab)\n\nfit1c &lt;- svyglm(formula1c, design = w.design, family = binomial(logit))\n\nfitstep &lt;- step(fit1c, scope = scope, trace = FALSE, k = 2, direction = \"backward\")\n# k = 2 gives the genuine AIC\n\n\npublish(fitstep)\n#&gt;  Variable             Units OddsRatio         CI.95     p-value \n#&gt;        OA           Control       Ref                           \n#&gt;                          OA      1.52   [1.40;1.66]     &lt; 1e-04 \n#&gt;       age       20-29 years       Ref                           \n#&gt;                 30-39 years      1.29   [0.98;1.70]   0.0697699 \n#&gt;                 40-49 years      2.74   [2.17;3.47]     &lt; 1e-04 \n#&gt;                 50-59 years      6.24   [4.97;7.83]     &lt; 1e-04 \n#&gt;                 60-64 years      9.71  [7.68;12.28]     &lt; 1e-04 \n#&gt;           65 years and over     15.85 [12.57;19.98]     &lt; 1e-04 \n#&gt;                        teen      0.46   [0.20;1.07]   0.0705660 \n#&gt;       sex            Female       Ref                           \n#&gt;                        Male      1.73   [1.60;1.88]     &lt; 1e-04 \n#&gt;   married        not single       Ref                           \n#&gt;                      single      0.97   [0.90;1.05]   0.5042496 \n#&gt;      race         Non-white       Ref                           \n#&gt;                       White      1.41   [1.18;1.70]   0.0001986 \n#&gt;       edu          &lt; 2ndary       Ref                           \n#&gt;                   2nd grad.      0.90   [0.80;1.00]   0.0524940 \n#&gt;             Other 2nd grad.      0.97   [0.83;1.13]   0.6732446 \n#&gt;              Post-2nd grad.      0.93   [0.85;1.02]   0.1045975 \n#&gt;    income   $29,999 or less       Ref                           \n#&gt;             $30,000-$49,999      0.74   [0.67;0.81]     &lt; 1e-04 \n#&gt;             $50,000-$79,999      0.64   [0.58;0.72]     &lt; 1e-04 \n#&gt;             $80,000 or more      0.58   [0.51;0.66]     &lt; 1e-04 \n#&gt;       bmi       Underweight       Ref                           \n#&gt;              healthy weight      0.86   [0.67;1.10]   0.2316915 \n#&gt;                  Overweight      0.88   [0.69;1.12]   0.2982852 \n#&gt;    phyact            Active       Ref                           \n#&gt;                    Inactive      1.22   [1.10;1.34]   0.0001227 \n#&gt;                    Moderate      1.08   [0.97;1.21]   0.1754422 \n#&gt;     fruit 0-3 daily serving       Ref                           \n#&gt;           4-6 daily serving      0.94   [0.86;1.03]   0.1807666 \n#&gt;            6+ daily serving      1.09   [0.97;1.23]   0.1295281 \n#&gt;        bp                No       Ref                           \n#&gt;                         Yes      2.35   [2.16;2.55]     &lt; 1e-04 \n#&gt;      diab                No       Ref                           \n#&gt;                         Yes      1.85   [1.66;2.07]     &lt; 1e-04 \n#&gt;    doctor                No       Ref                           \n#&gt;                         Yes      1.75   [1.49;2.05]     &lt; 1e-04 \n#&gt;    stress  Not too stressed       Ref                           \n#&gt;                    stressed      1.30   [1.18;1.42]     &lt; 1e-04 \n#&gt;     smoke      Never smoker       Ref                           \n#&gt;              Current smoker      1.17   [1.05;1.31]   0.0053412 \n#&gt;               Former smoker      1.21   [1.10;1.33]     &lt; 1e-04 \n#&gt;     drink       Never drank       Ref                           \n#&gt;             Current drinker      0.82   [0.68;0.98]   0.0254942 \n#&gt;               Former driker      1.12   [0.93;1.36]   0.2205315\nround(sort(summary(fitstep)$coef[,\"Pr(&gt;|t|)\"]),2)\n#&gt;            (Intercept)   age65 years and over                  bpYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age60-64 years         age50-59 years                sexMale \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;                diabYes                   OAOA  income$80,000 or more \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         age40-49 years  income$50,000-$79,999              doctorYes \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;  income$30,000-$49,999         stressstressed     smokeFormer smoker \n#&gt;                   0.00                   0.00                   0.00 \n#&gt;         phyactInactive              raceWhite    smokeCurrent smoker \n#&gt;                   0.00                   0.00                   0.01 \n#&gt;   drinkCurrent drinker           edu2nd grad.         age30-39 years \n#&gt;                   0.03                   0.05                   0.07 \n#&gt;                ageteen      eduPost-2nd grad.  fruit6+ daily serving \n#&gt;                   0.07                   0.10                   0.13 \n#&gt;         phyactModerate fruit4-6 daily serving     drinkFormer driker \n#&gt;                   0.18                   0.18                   0.22 \n#&gt;      bmihealthy weight          bmiOverweight          marriedsingle \n#&gt;                   0.23                   0.30                   0.50 \n#&gt;     eduOther 2nd grad. \n#&gt;                   0.67\n\nAssess interactions\nCheck biologically interesting ones.\nCheck one by one\nChecks if there is a significant interaction effect between ‘age’ and ‘sex’.\n\nfit8a &lt;- update(fitstep, .~. + interaction(age,sex))\nanova(fitstep, fit8a) # keep interaction\n#&gt; Working (Rao-Scott+F) LRT for interaction(age, sex)\n#&gt;  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#&gt;     race + edu + income + bmi + phyact + fruit + bp + diab + \n#&gt;     doctor + stress + smoke + drink + interaction(age, sex), \n#&gt;     design = w.design, family = binomial(logit))\n#&gt; Working 2logLR =  40.16528 p= 1.2167e-06 \n#&gt; (scale factors:  1.3 1.2 1.2 0.93 0.78 0.71 );  denominator df= 185576\n\nChecks if there is a significant interaction effect between ‘sex’ and ‘diabetes’.\n\nfit8b &lt;- update(fitstep, .~. + interaction(sex,diab))\nanova(fitstep, fit8b) # Do not keep this interaction\n#&gt; Working (Rao-Scott+F) LRT for interaction(sex, diab)\n#&gt;  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#&gt;     race + edu + income + bmi + phyact + fruit + bp + diab + \n#&gt;     doctor + stress + smoke + drink + interaction(sex, diab), \n#&gt;     design = w.design, family = binomial(logit))\n#&gt; Working 2logLR =  0.4591456 p= 0.49597 \n#&gt; df=1;  denominator df= 185581\n\nChecks if there is a significant interaction effect between ‘BMI’ and ‘diabetes’.\n\nfit8c &lt;- update(fitstep, .~. + interaction(bmi,diab))\nanova(fitstep, fit8c) # keep this interaction\n#&gt; Working (Rao-Scott+F) LRT for interaction(bmi, diab)\n#&gt;  in svyglm(formula = I(CVD == \"event\") ~ OA + age + sex + married + \n#&gt;     race + edu + income + bmi + phyact + fruit + bp + diab + \n#&gt;     doctor + stress + smoke + drink + interaction(bmi, diab), \n#&gt;     design = w.design, family = binomial(logit))\n#&gt; Working 2logLR =  7.92727 p= 0.02533 \n#&gt; (scale factors:  1.4 0.6 );  denominator df= 185580\n\nAdd all significant interactions in 1 model\nUpdates the model to include significant interaction terms.\nNote that we have 0 effect modifier, 2 interactions\n\nfit9 &lt;- update(fitstep, .~. + interaction(age,sex) + interaction(bmi,diab))\nrequire(jtools)\nsumm(fit9, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n185613\n\n\nDependent variable\nI(CVD == \"event\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.233\n\n\nPseudo-R² (McFadden)\n0.207\n\n\nAIC\n41548.160\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n-5.590\n-6.046\n-5.135\n-24.069\n0.000\n\n\nOAOA\n0.438\n0.352\n0.523\n10.043\n0.000\n\n\nage30-39 years\n0.299\n-0.118\n0.717\n1.405\n0.160\n\n\nage40-49 years\n1.158\n0.794\n1.522\n6.236\n0.000\n\n\nage50-59 years\n2.181\n1.831\n2.531\n12.212\n0.000\n\n\nage60-64 years\n2.619\n2.263\n2.976\n14.395\n0.000\n\n\nage65 years and over\n3.033\n2.680\n3.387\n16.833\n0.000\n\n\nageteen\n-0.695\n-1.704\n0.314\n-1.350\n0.177\n\n\nsexMale\n-0.028\n-0.447\n0.390\n-0.133\n0.895\n\n\nmarriedsingle\n-0.016\n-0.096\n0.064\n-0.382\n0.703\n\n\nraceWhite\n0.348\n0.165\n0.531\n3.724\n0.000\n\n\nedu2nd grad.\n-0.107\n-0.217\n0.003\n-1.906\n0.057\n\n\neduOther 2nd grad.\n-0.039\n-0.192\n0.114\n-0.498\n0.618\n\n\neduPost-2nd grad.\n-0.081\n-0.173\n0.010\n-1.746\n0.081\n\n\nincome$30,000-$49,999\n-0.304\n-0.400\n-0.208\n-6.215\n0.000\n\n\nincome$50,000-$79,999\n-0.444\n-0.552\n-0.336\n-8.034\n0.000\n\n\nincome$80,000 or more\n-0.555\n-0.684\n-0.426\n-8.424\n0.000\n\n\nbmihealthy weight\n-1.406\n-2.677\n-0.135\n-2.167\n0.030\n\n\nbmiOverweight\n-1.110\n-2.375\n0.154\n-1.721\n0.085\n\n\nphyactInactive\n0.194\n0.094\n0.293\n3.817\n0.000\n\n\nphyactModerate\n0.077\n-0.034\n0.187\n1.353\n0.176\n\n\nfruit4-6 daily serving\n-0.062\n-0.155\n0.031\n-1.313\n0.189\n\n\nfruit6+ daily serving\n0.094\n-0.023\n0.211\n1.579\n0.114\n\n\nbpYes\n0.861\n0.778\n0.944\n20.354\n0.000\n\n\ndiabYes\n1.745\n0.462\n3.027\n2.667\n0.008\n\n\ndoctorYes\n0.535\n0.372\n0.697\n6.447\n0.000\n\n\nstressstressed\n0.256\n0.164\n0.347\n5.483\n0.000\n\n\nsmokeCurrent smoker\n0.152\n0.039\n0.266\n2.628\n0.009\n\n\nsmokeFormer smoker\n0.177\n0.082\n0.272\n3.654\n0.000\n\n\ndrinkCurrent drinker\n-0.205\n-0.381\n-0.029\n-2.277\n0.023\n\n\ndrinkFormer driker\n0.116\n-0.072\n0.303\n1.210\n0.226\n\n\ninteraction(age, sex)30-39 years.Female\n-0.083\n-0.622\n0.457\n-0.300\n0.764\n\n\ninteraction(age, sex)40-49 years.Female\n-0.302\n-0.766\n0.161\n-1.278\n0.201\n\n\ninteraction(age, sex)50-59 years.Female\n-0.810\n-1.258\n-0.362\n-3.543\n0.000\n\n\ninteraction(age, sex)60-64 years.Female\n-0.787\n-1.237\n-0.337\n-3.428\n0.001\n\n\ninteraction(age, sex)65 years and over.Female\n-0.603\n-1.035\n-0.170\n-2.733\n0.006\n\n\ninteraction(age, sex)teen.Female\n-0.129\n-1.806\n1.548\n-0.151\n0.880\n\n\ninteraction(bmi, diab)healthy weight.No\n1.380\n0.085\n2.675\n2.089\n0.037\n\n\ninteraction(bmi, diab)Overweight.No\n1.085\n-0.204\n2.373\n1.650\n0.099\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n\nfit9 &lt;- update(fitstep, .~. + age:sex + bmi:diab)\npublish(fit9)\n#&gt;                                            Variable             Units OddsRatio         CI.95     p-value \n#&gt;                                                  OA           Control       Ref                           \n#&gt;                                                                    OA      1.55   [1.42;1.69]     &lt; 1e-04 \n#&gt;                                             married        not single       Ref                           \n#&gt;                                                                single      0.98   [0.91;1.07]   0.7026897 \n#&gt;                                                race         Non-white       Ref                           \n#&gt;                                                                 White      1.42   [1.18;1.70]   0.0001960 \n#&gt;                                                 edu          &lt; 2ndary       Ref                           \n#&gt;                                                             2nd grad.      0.90   [0.81;1.00]   0.0566536 \n#&gt;                                                       Other 2nd grad.      0.96   [0.83;1.12]   0.6183383 \n#&gt;                                                        Post-2nd grad.      0.92   [0.84;1.01]   0.0807393 \n#&gt;                                              income   $29,999 or less       Ref                           \n#&gt;                                                       $30,000-$49,999      0.74   [0.67;0.81]     &lt; 1e-04 \n#&gt;                                                       $50,000-$79,999      0.64   [0.58;0.71]     &lt; 1e-04 \n#&gt;                                                       $80,000 or more      0.57   [0.50;0.65]     &lt; 1e-04 \n#&gt;                                              phyact            Active       Ref                           \n#&gt;                                                              Inactive      1.21   [1.10;1.34]   0.0001349 \n#&gt;                                                              Moderate      1.08   [0.97;1.21]   0.1760803 \n#&gt;                                               fruit 0-3 daily serving       Ref                           \n#&gt;                                                     4-6 daily serving      0.94   [0.86;1.03]   0.1892549 \n#&gt;                                                      6+ daily serving      1.10   [0.98;1.23]   0.1142759 \n#&gt;                                                  bp                No       Ref                           \n#&gt;                                                                   Yes      2.37   [2.18;2.57]     &lt; 1e-04 \n#&gt;                                              doctor                No       Ref                           \n#&gt;                                                                   Yes      1.71   [1.45;2.01]     &lt; 1e-04 \n#&gt;                                              stress  Not too stressed       Ref                           \n#&gt;                                                              stressed      1.29   [1.18;1.42]     &lt; 1e-04 \n#&gt;                                               smoke      Never smoker       Ref                           \n#&gt;                                                        Current smoker      1.16   [1.04;1.30]   0.0085960 \n#&gt;                                                         Former smoker      1.19   [1.09;1.31]   0.0002579 \n#&gt;                                               drink       Never drank       Ref                           \n#&gt;                                                       Current drinker      0.81   [0.68;0.97]   0.0227934 \n#&gt;                                                         Former driker      1.12   [0.93;1.35]   0.2264702 \n#&gt;               age(20-29 years): sex(Male vs Female)                        0.97   [0.64;1.48]   0.8945322 \n#&gt;               age(30-39 years): sex(Male vs Female)                        1.06   [0.75;1.49]   0.7583565 \n#&gt;               age(40-49 years): sex(Male vs Female)                        1.32   [1.07;1.62]   0.0091527 \n#&gt;               age(50-59 years): sex(Male vs Female)                        2.18   [1.85;2.58]     &lt; 1e-04 \n#&gt;               age(60-64 years): sex(Male vs Female)                        2.14   [1.80;2.53]     &lt; 1e-04 \n#&gt;         age(65 years and over): sex(Male vs Female)                        1.78   [1.58;1.99]     &lt; 1e-04 \n#&gt;                      age(teen): sex(Male vs Female)                        1.11   [0.22;5.62]   0.9033924 \n#&gt;        sex(Female): age(30-39 years vs 20-29 years)                        1.24   [0.87;1.77]   0.2276609 \n#&gt;        sex(Female): age(40-49 years vs 20-29 years)                        2.35   [1.75;3.16]     &lt; 1e-04 \n#&gt;        sex(Female): age(50-59 years vs 20-29 years)                        3.94   [2.95;5.27]     &lt; 1e-04 \n#&gt;        sex(Female): age(60-64 years vs 20-29 years)                        6.25   [4.66;8.39]     &lt; 1e-04 \n#&gt;  sex(Female): age(65 years and over vs 20-29 years)                       11.37  [8.60;15.02]     &lt; 1e-04 \n#&gt;               sex(Female): age(teen vs 20-29 years)                        0.44   [0.11;1.69]   0.2320840 \n#&gt;          sex(Male): age(30-39 years vs 20-29 years)                        1.35   [0.89;2.05]   0.1599938 \n#&gt;          sex(Male): age(40-49 years vs 20-29 years)                        3.18   [2.21;4.58]     &lt; 1e-04 \n#&gt;          sex(Male): age(50-59 years vs 20-29 years)                        8.85  [6.24;12.56]     &lt; 1e-04 \n#&gt;          sex(Male): age(60-64 years vs 20-29 years)                       13.73  [9.61;19.61]     &lt; 1e-04 \n#&gt;    sex(Male): age(65 years and over vs 20-29 years)                       20.77 [14.59;29.57]     &lt; 1e-04 \n#&gt;                 sex(Male): age(teen vs 20-29 years)                        0.50   [0.18;1.37]   0.1769155 \n#&gt;                   bmi(Underweight): diab(Yes vs No)                        5.72  [1.59;20.63]   0.0076561 \n#&gt;                bmi(healthy weight): diab(Yes vs No)                        1.44   [1.19;1.75]   0.0002221 \n#&gt;                    bmi(Overweight): diab(Yes vs No)                        1.93   [1.70;2.20]     &lt; 1e-04 \n#&gt;        diab(No): bmi(healthy weight vs Underweight)                        0.97   [0.76;1.25]   0.8394972 \n#&gt;            diab(No): bmi(Overweight vs Underweight)                        0.97   [0.76;1.25]   0.8400722 \n#&gt;       diab(Yes): bmi(healthy weight vs Underweight)                        0.25   [0.07;0.87]   0.0302066 \n#&gt;           diab(Yes): bmi(Overweight vs Underweight)                        0.33   [0.09;1.17]   0.0852377\n\n\nbasic.model &lt;- eval(fit5$call[[2]])\nbasic.model\n#&gt; I(CVD == \"event\") ~ OA + age + sex\n#&gt; attr(,\"variables\")\n#&gt; list(I(CVD == \"event\"), OA, age, sex)\n#&gt; attr(,\"factors\")\n#&gt;                   OA age sex\n#&gt; I(CVD == \"event\")  0   0   0\n#&gt; OA                 1   0   0\n#&gt; age                0   1   0\n#&gt; sex                0   0   1\n#&gt; attr(,\"term.labels\")\n#&gt; [1] \"OA\"  \"age\" \"sex\"\n#&gt; attr(,\"order\")\n#&gt; [1] 1 1 1\n#&gt; attr(,\"intercept\")\n#&gt; [1] 1\n#&gt; attr(,\"response\")\n#&gt; [1] 1\n#&gt; attr(,\".Environment\")\n#&gt; &lt;environment: R_GlobalEnv&gt;\n#&gt; attr(,\"predvars\")\n#&gt; list(I(CVD == \"event\"), OA, age, sex)\n#&gt; attr(,\"dataClasses\")\n#&gt; I(CVD == \"event\")                OA               age               sex \n#&gt;         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#&gt;         (weights) \n#&gt;         \"numeric\"\n\naic.int.model &lt;- eval(fit9$call[[2]])\naic.int.model\n#&gt; I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#&gt;     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#&gt;     drink + age:sex + bmi:diab\n\nSaving data\nSaves the final regression models for future use.\n\nsave(basic.model, aic.int.model, file = \"Data/surveydata/cchs123w2.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Regression"
    ]
  },
  {
    "objectID": "surveydata5.html",
    "href": "surveydata5.html",
    "title": "CCHS: Performance",
    "section": "",
    "text": "The tutorial outlines the process for evaluating the performance of logistic regression models fitted to complex survey data using R. It focuses on two major aspects: creating Receiver Operating Characteristic (ROC) curves and conducting Archer and Lemeshow Goodness of Fit tests. Here AUC is a measure to evaluate the predictive accuracy of the model, and Archer and Lemeshow test is a statistical test to evaluate how well your model fits the observed data.\n\n\n\n\n\n\nImportant\n\n\n\nSee an updated version of this tutorial here that uses svyTable1 package on an NHANES data analysis.\n\n\nWe start by importing the required R packages.\n\n# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\nLoad data\nIt loads two datasets from the specified paths.\n\nload(\"Data/surveydata/cchs123w.RData\")\nload(\"Data/surveydata/cchs123w2.RData\")\nls()\n#&gt; [1] \"aic.int.model\" \"analytic.miss\" \"analytic2\"     \"basic.model\"  \n#&gt; [5] \"w.design\"\ndim(analytic.miss)\n#&gt; [1] 397173     23\ndim(analytic2)\n#&gt; [1] 185613     22\n\nThree different logistic regression models are fitted to the data:\n\nSimple model: Model with only OA as a predictor\nBasic model: Model with OA, age and sex\nComplex model: Model with many predictors and some interaction terms\n\n\n# Formula for Simple model\nsimple.model &lt;- as.formula(I(CVD==\"event\") ~ OA)\nsimple.model\n#&gt; I(CVD == \"event\") ~ OA\n\n# Formula for Basic model\nbasic.model\n#&gt; I(CVD == \"event\") ~ OA + age + sex\n#&gt; attr(,\"variables\")\n#&gt; list(I(CVD == \"event\"), OA, age, sex)\n#&gt; attr(,\"factors\")\n#&gt;                   OA age sex\n#&gt; I(CVD == \"event\")  0   0   0\n#&gt; OA                 1   0   0\n#&gt; age                0   1   0\n#&gt; sex                0   0   1\n#&gt; attr(,\"term.labels\")\n#&gt; [1] \"OA\"  \"age\" \"sex\"\n#&gt; attr(,\"order\")\n#&gt; [1] 1 1 1\n#&gt; attr(,\"intercept\")\n#&gt; [1] 1\n#&gt; attr(,\"response\")\n#&gt; [1] 1\n#&gt; attr(,\".Environment\")\n#&gt; &lt;environment: R_GlobalEnv&gt;\n#&gt; attr(,\"predvars\")\n#&gt; list(I(CVD == \"event\"), OA, age, sex)\n#&gt; attr(,\"dataClasses\")\n#&gt; I(CVD == \"event\")                OA               age               sex \n#&gt;         \"logical\"          \"factor\"          \"factor\"          \"factor\" \n#&gt;         (weights) \n#&gt;         \"numeric\"\n\n# Formula for the Complex model with interactions\naic.int.model\n#&gt; I(CVD == \"event\") ~ OA + age + sex + married + race + edu + income + \n#&gt;     bmi + phyact + fruit + bp + diab + doctor + stress + smoke + \n#&gt;     drink + age:sex + bmi:diab\n\n\nlibrary(survey)\n\n# Simple model\nfit0 &lt;- svyglm(simple.model,\n              design = w.design,\n              family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# Basic model\nfit5 &lt;- svyglm(basic.model,\n              design = w.design,\n              family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# Complex model with interactions\nfit9 &lt;- svyglm(aic.int.model,\n              design = w.design,\n              family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nModel performance\nROC curve\nThis section defines a function, svyROCw, to plot the ROC curves and calculate the area under the curve (AUC). The function can handle both weighted and unweighted survey data.\n\nThe appropriateness of the fitted logistic regression model needs to be examined before it is accepted for use.\nPlotting the pairs of - sensitivities vs - 1-specificities on a scatter plot provides a Receiver Operating Characteristic (ROC) curve.\nThe area under the ROC curve = AUC / C-statistic.\nROC/AUC should consider weights for complex surveys.\n\nGrading Guidelines for AUC values:\n\n0.90-1.0 excellent discrimination (unusual)\n0.80-0.90 good discrimination\n0.70-0.80 fair discrimination\n0.60-0.70 poor discrimination\n0.50-0.60 failed discrimination\n\n\nrequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw &lt;- function(fit=fit,outcome=analytic2$CVD==\"event\", weight = NULL){\n  # ROC curve for\n  # Survey Data with Logistic Regression\n  if (is.null(weight)){ # require(ROCR)\n    prob &lt;- predict(fit, type = \"response\")\n  pred &lt;- prediction(as.vector(prob), outcome)\n  perf &lt;- performance(pred, \"tpr\", \"fpr\")\n  auc &lt;- performance(pred, measure = \"auc\")\n  auc &lt;- auc@y.values[[1]]\n  roc.data &lt;- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { # library(WeightedROC)\n    outcome &lt;- as.numeric(outcome)\n  pred &lt;- predict(fit, type = \"response\")\n  tp.fp &lt;- WeightedROC(pred, outcome, weight)\n  auc &lt;- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\n\n\nsummary(analytic2$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   71.56  137.95  214.61  261.91 7154.95\nanalytic2$corrected.weight &lt;- weights(w.design)\nsummary(analytic2$corrected.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   23.85   45.98   71.54   87.30 2384.98\nsvyROCw(fit=fit0,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\n\n\n\nsvyROCw(fit=fit5,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\n\n\n\nsvyROCw(fit=fit9,outcome=analytic2$CVD==\"event\", weight = analytic2$corrected.weight)\n\n\n\n\n\n\n# This function does not take in to account of strata/cluster\n\nArcher and Lemeshow test\nThis test helps to evaluate how well the model fits the data. A Goodness of Fit (GOF) function AL.gof is defined. If the p-value from this test is greater than a certain threshold (e.g., 0.05), the model fit is considered acceptable.\n\nHosmer Lemeshow-type tests are most useful as a very crude way to screen for fit problems, and should not be taken as a definitive diagnostic of a ‘good’ fit.\n\nproblem in small sample size\nDependent on G (group)\n\n\nArcher and Lemeshow (2006) extended the standard Hosmer and Lemeshow GOF test for complex surveys.\nAfter fitting the survey weighted logistic regression, the F-adjusted mean residual goodness-of-fit test could suggest\n\nno evidence of lack of fit (if P-value &gt; a reasonable cut-point, e.g., 0.05)\nevidence of lack of fit (if P-value &lt; a reasonable cut-point, e.g., 0.05)\n\n\n\n\nAL.gof &lt;- function(fit=fit, data = analytic2, \n                   weight = \"corrected.weight\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r &lt;- residuals(fit, type=\"response\") \n  f&lt;-fitted(fit) \n  breaks.g &lt;- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g &lt;- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g&lt;- cut(f, breaks.g)\n  data2g &lt;- cbind(data,r,g)\n  newdesign &lt;- svydesign(id=~1, \n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g)\n  decilemodel&lt;- svyglm(r~g, design=newdesign) \n  res &lt;- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\n\nAL.gof(fit0, analytic2, weight =\"corrected.weight\")\n#&gt; Wald test for g\n#&gt;  in svyglm(formula = r ~ g, design = newdesign)\n#&gt; F =  2.20807e-22  on  1  and  185611  df: p= 1\nAL.gof(fit5, analytic2, weight =\"corrected.weight\")\n#&gt; Wald test for g\n#&gt;  in svyglm(formula = r ~ g, design = newdesign)\n#&gt; F =  2.795204  on  8  and  185604  df: p= 0.0042898\nAL.gof(fit9, analytic2, weight = \"corrected.weight\")\n#&gt; Wald test for g\n#&gt;  in svyglm(formula = r ~ g, design = newdesign)\n#&gt; F =  2.650332  on  9  and  185603  df: p= 0.0045417\n\nAdditional function\nIf the survey data contains strata and cluster, then the following function will be useful:\n\nAL.gof2 &lt;- function(fit=fit7, data = analytic, \n                   weight = \"corrected.weight\", psu = \"psu\", strata= \"strata\"){\n  # Archer-Lemeshow Goodness of Fit Test for\n  # Survey Data with Logistic Regression\n  r &lt;- residuals(fit, type=\"response\") \n  f&lt;-fitted(fit) \n  breaks.g &lt;- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g &lt;- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g&lt;- cut(f, breaks.g)\n  data2g &lt;- cbind(data,r,g)\n  newdesign &lt;- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  decilemodel&lt;- svyglm(r~g, design=newdesign) \n  res &lt;- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "CCHS: Performance"
    ]
  },
  {
    "objectID": "surveydata6.html",
    "href": "surveydata6.html",
    "title": "NHANES: Blood Pressure",
    "section": "",
    "text": "Load data\nThe tutorial aims to guide the user through the process of analyzing health survey data, focusing specifically on the relationship between various demographic factors and blood pressure levels.\nRequired packages are imported for data manipulation and statistical analysis.\nNHANES survey data is loaded into the workspace.\nload(file = \"Data/surveydata/NHANESsample.Rdata\")\nls()\n#&gt; [1] \"analytic.data\"",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Blood Pressure"
    ]
  },
  {
    "objectID": "surveydata6.html#a-note-about-predictive-models",
    "href": "surveydata6.html#a-note-about-predictive-models",
    "title": "NHANES: Blood Pressure",
    "section": "A note about Predictive models",
    "text": "A note about Predictive models\nIn statistical analyses involving survey data, it’s crucial to account for the survey’s design features. These features can include sampling weights, stratification, and clustering, among others. Ignoring these could lead to biased estimates and incorrect conclusions. Considering such survey design features make the analysis more robust and reliable in terms of inference.\nHowever, when the goal shifts from inference to prediction, additional challenges come into play. Specifically, the model may perform well on the data used to fit it (the “training” data) but not generalize well to new, unseen data. This discrepancy between training performance and generalization to new data is often referred to as “overfitting,” and the optimism of the model refers to the extent to which it overestimates its predictive performance on new data based on its performance on the training data.\nOptimism-correction techniques are methodologies designed to address this issue. They allow you to evaluate how well your model is likely to perform on new data, not just the data you used to build it. Methods for optimism correction often involve techniques like cross-validation, bootstrapping, or specialized types of model validation that help in estimating the ‘true’ predictive performance of the model. Some of these techniques were discussed in the predictive modelling chapter.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Blood Pressure"
    ]
  },
  {
    "objectID": "surveydata7.html",
    "href": "surveydata7.html",
    "title": "NHANES: Cholesterol",
    "section": "",
    "text": "# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(tableone)\nlibrary(ROCR)\nlibrary(WeightedROC)\nlibrary(jtools)\nlibrary(dplyr)\n\nPreprocessing\nAnalytic data set\nWe will use cholesterolNHANES15part1.RData in this prediction goal question (predicting cholesterol in adults).\nFor this exercise, we are assuming that:\n\noutcome: cholesterol\n\npredictors:\n\ngender\nwhether born in US\nrace\neducation\nwhether married\nincome level\nBMI\nwhether has diabetes\n\n\n\nsurvey features:\n\nsurvey weights\nstrata\ncluster/PSU; where strata is nested within clusters\n\n– restrict to those participants who are of 18 years of age or older\n\n\n\nload(\"Data/surveydata/cholesterolNHANES15part1.rdata\") #Loading the dataset\nls()\n#&gt;  [1] \"analytic\"           \"analytic.with.miss\" \"analytic1\"         \n#&gt;  [4] \"analytic2\"          \"analytic2b\"         \"analytic3\"         \n#&gt;  [7] \"collinearity\"       \"correlationMatrix\"  \"diff.boot\"         \n#&gt; [10] \"extract.boot.fun\"   \"extract.fit\"        \"extract.lm.fun\"    \n#&gt; [13] \"fictitious.data\"    \"fit0\"               \"fit1\"              \n#&gt; [16] \"fit2\"               \"fit3\"               \"fit4\"              \n#&gt; [19] \"fit5\"               \"formula0\"           \"formula1\"          \n#&gt; [22] \"formula2\"           \"formula3\"           \"formula4\"          \n#&gt; [25] \"formula5\"           \"k.folds\"            \"numeric.names\"     \n#&gt; [28] \"perform\"            \"pred.y\"             \"rocobj\"            \n#&gt; [31] \"sel.names\"          \"var.cluster\"        \"var.summ\"          \n#&gt; [34] \"var.summ2\"\n\nRetaining only useful variables\n\n# Data dimensions\ndim(analytic)\n#&gt; [1] 1267   33\n\n# Variable names\nnames(analytic)\n#&gt;  [1] \"ID\"                    \"gender\"                \"age\"                  \n#&gt;  [4] \"born\"                  \"race\"                  \"education\"            \n#&gt;  [7] \"married\"               \"income\"                \"weight\"               \n#&gt; [10] \"psu\"                   \"strata\"                \"diastolicBP\"          \n#&gt; [13] \"systolicBP\"            \"bodyweight\"            \"bodyheight\"           \n#&gt; [16] \"bmi\"                   \"waist\"                 \"smoke\"                \n#&gt; [19] \"alcohol\"               \"cholesterol\"           \"cholesterolM2\"        \n#&gt; [22] \"triglycerides\"         \"uric.acid\"             \"protein\"              \n#&gt; [25] \"bilirubin\"             \"phosphorus\"            \"sodium\"               \n#&gt; [28] \"potassium\"             \"globulin\"              \"calcium\"              \n#&gt; [31] \"physical.work\"         \"physical.recreational\" \"diabetes\"\n\n#Subsetting dataset with variables needed:\nrequire(dplyr)\nanadata &lt;- select(analytic, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\n# new data sizes\ndim(anadata)\n#&gt; [1] 1267   13\n\n# retained variable names\nnames(anadata)\n#&gt;  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#&gt;  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#&gt; [11] \"weight\"      \"psu\"         \"strata\"\n\n#Restricting to participants who are 18 or older\nsummary(anadata$age) #The age range is already 20-80\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   20.00   36.00   51.00   49.91   63.00   80.00\n\n#Recoding the born variable\ntable(anadata$born, useNA = \"always\")\n#&gt; \n#&gt; Born in 50 US states or Washingt                           Others \n#&gt;                              991                              276 \n#&gt;                             &lt;NA&gt; \n#&gt;                                0\nlevels(anadata$born)\n#&gt; NULL\nanadata$born &lt;- car::recode(anadata$born,\n                            \"'Born in 50 US states or Washingt' = 'Born.in.US';\n                            'Others' = 'Others';\n                            else=NA\")\ntable(anadata$born, useNA = \"always\")\n#&gt; \n#&gt; Born.in.US     Others       &lt;NA&gt; \n#&gt;        991        276          0\n\nChecking the data for missing\n\nrequire(DataExplorer)\nplot_missing(anadata) #no missing data\n\n\n\n\n\n\n\nPreparing factor and continuous variables appropriately\n\nvars = c(\"cholesterol\", \"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\nnumeric.names &lt;- c(\"cholesterol\", \"bmi\")\nfactor.names &lt;- vars[!vars %in% numeric.names] \n\nanadata[factor.names] &lt;- apply(X = anadata[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanadata[numeric.names] &lt;- apply(X = anadata[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\n\nTable 1\n\nlibrary(tableone)\ntab1 &lt;- CreateTableOne(data = anadata, includeNA = TRUE, vars = vars)\nprint(tab1, showAllLevels = TRUE,  varLabels = TRUE)\n#&gt;                          \n#&gt;                           level              Overall       \n#&gt;   n                                            1267        \n#&gt;   cholesterol (mean (SD))                    193.10 (43.22)\n#&gt;   gender (%)              Female                496 (39.1) \n#&gt;                           Male                  771 (60.9) \n#&gt;   born (%)                Born.in.US            991 (78.2) \n#&gt;                           Others                276 (21.8) \n#&gt;   race (%)                Black                 246 (19.4) \n#&gt;                           Hispanic              337 (26.6) \n#&gt;                           Other                 132 (10.4) \n#&gt;                           White                 552 (43.6) \n#&gt;   education (%)           College               648 (51.1) \n#&gt;                           High.School           523 (41.3) \n#&gt;                           School                 96 ( 7.6) \n#&gt;   married (%)             Married               751 (59.3) \n#&gt;                           Never.married         226 (17.8) \n#&gt;                           Previously.married    290 (22.9) \n#&gt;   income (%)              &lt;25k                  344 (27.2) \n#&gt;                           Between.25kto54k      435 (34.3) \n#&gt;                           Between.55kto99k      297 (23.4) \n#&gt;                           Over100k              191 (15.1) \n#&gt;   bmi (mean (SD))                             29.58 (6.84) \n#&gt;   diabetes (%)            No                   1064 (84.0) \n#&gt;                           Yes                   203 (16.0)\n\nLinear regression when cholesterol is continuous\nFit a linear regression, and report the VIFs.\n\n#Fitting initial regression\n\nfit0 &lt;- lm(cholesterol ~ gender + born + race + education +\n              married + income + bmi + diabetes,\n            data = anadata)\n\nlibrary(Publish)\npublish(fit0)\n#&gt;     Variable              Units Coefficient           CI.95    p-value \n#&gt;  (Intercept)                         198.90 [184.82;212.97]    &lt; 1e-04 \n#&gt;       gender             Female         Ref                            \n#&gt;                            Male       -6.82  [-11.76;-1.89]   0.006854 \n#&gt;         born         Born.in.US         Ref                            \n#&gt;                          Others       15.65    [8.54;22.75]    &lt; 1e-04 \n#&gt;         race              Black         Ref                            \n#&gt;                        Hispanic       -2.75   [-10.61;5.10]   0.492333 \n#&gt;                           Other       -3.95   [-13.61;5.72]   0.423740 \n#&gt;                           White        5.36   [-1.20;11.92]   0.109403 \n#&gt;    education            College         Ref                            \n#&gt;                     High.School        3.51    [-1.61;8.63]   0.179871 \n#&gt;                          School        0.31   [-9.63;10.24]   0.951841 \n#&gt;      married            Married         Ref                            \n#&gt;                   Never.married      -11.05  [-17.67;-4.44]   0.001082 \n#&gt;              Previously.married        4.72   [-1.43;10.86]   0.132468 \n#&gt;       income               &lt;25k         Ref                            \n#&gt;                Between.25kto54k       -0.48    [-6.72;5.75]   0.879480 \n#&gt;                Between.55kto99k        3.41   [-3.60;10.43]   0.340491 \n#&gt;                        Over100k        2.24   [-6.02;10.51]   0.595131 \n#&gt;          bmi                          -0.21    [-0.56;0.15]   0.257105 \n#&gt;     diabetes                 No         Ref                            \n#&gt;                             Yes      -10.61  [-17.21;-4.02]   0.001652\n\n#Checking VIFs\ncar::vif(fit0) \n#&gt;               GVIF Df GVIF^(1/(2*Df))\n#&gt; gender    1.065810  1        1.032381\n#&gt; born      1.578258  1        1.256288\n#&gt; race      1.684064  3        1.090753\n#&gt; education 1.280113  2        1.063683\n#&gt; married   1.225520  2        1.052156\n#&gt; income    1.277005  3        1.041595\n#&gt; bmi       1.086953  1        1.042570\n#&gt; diabetes  1.073619  1        1.036156\n\nAll VIFs are small.\nTest of association when cholesterol is binary\nDichotomize the outcome such that cholesterol&lt;200 is labeled as ‘healthy’; otherwise label it as ‘unhealthy’, and name it ‘cholesterol.bin’. Test the association between this binary variable and gender.\n\n#Creating binary variable for cholesterol\nanadata$cholesterol.bin &lt;- ifelse(anadata$cholesterol &lt;200, \"healthy\", \"unhealthy\")\n#If cholesterol is &lt;200, then \"healthy\", if not, \"unhealthy\"\n\ntable(anadata$cholesterol.bin)\n#&gt; \n#&gt;   healthy unhealthy \n#&gt;       738       529\nanadata$cholesterol.bin &lt;- as.factor(anadata$cholesterol.bin)\nanadata$cholesterol.bin &lt;- relevel(anadata$cholesterol.bin, ref = \"unhealthy\")\n\nTest of association between cholesterol and gender (no survey features)\n\n# Simple Chi-square testing\nchisq.chol.gen &lt;- chisq.test(anadata$cholesterol.bin, anadata$gender)\nchisq.chol.gen\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  anadata$cholesterol.bin and anadata$gender\n#&gt; X-squared = 5.1321, df = 1, p-value = 0.02349\n\nSetting up survey design\n\nrequire(survey)\nsummary(anadata$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    5470   19540   30335   48904   63822  224892\nw.design &lt;- svydesign(id = ~psu, weights = ~weight, strata = ~strata,\n                      nest = TRUE, data = anadata)\nsummary(weights(w.design))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    5470   19540   30335   48904   63822  224892\n\nTest of association accounting for survey design\n\n#Rao-Scott modifications (chi-sq)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"Chisq\")\n#&gt; X-squared = 11.092, df = 1, p-value = 0.02365\n\n#Thomas-Rao modifications (F)\nsvychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\") \n#&gt; \n#&gt;  Pearson's X^2: Rao & Scott adjustment\n#&gt; \n#&gt; data:  svychisq(~cholesterol.bin + gender, design = w.design, statistic = \"F\")\n#&gt; F = 5.1205, ndf = 1, ddf = 15, p-value = 0.03891\n\nAll three tests indicate strong evidence to reject the H0. There seems to be an association between gender and cholesterol level (healthy/unhealthy)\nTable 1\nCreate a Table 1 (summarizing the covariates) stratified by the binary outcome: cholesterol.bin, utilizing the above survey features.\n\n# Creating Table 1 stratified by binary outcome (cholesterol)\n# Using the survey features\n\nvars2 = c(\"gender\", \"born\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\", \"diabetes\")\n\n\nkableone &lt;- function(x, ...) {\n  capture.output(x &lt;- print(x, showAllLevels= TRUE, padColnames = TRUE, insertLevel = TRUE))\n  knitr::kable(x, ...)\n}\nkableone(svyCreateTableOne(var = vars2, strata= \"cholesterol.bin\", data=w.design, test = TRUE)) \n\n\n\n\n\n\n\n\n\n\n\n\nlevel\nunhealthy\nhealthy\np\ntest\n\n\n\nn\n\n27369732.3\n34591444.0\n\n\n\n\ngender (%)\nFemale\n13573865.5 (49.6)\n13917447.5 (40.2)\n0.039\n\n\n\n\nMale\n13795866.8 (50.4)\n20673996.5 (59.8)\n\n\n\n\nborn (%)\nBorn.in.US\n23772751.7 (86.9)\n31532673.3 (91.2)\n0.028\n\n\n\n\nOthers\n3596980.6 (13.1)\n3058770.7 ( 8.8)\n\n\n\n\nrace (%)\nBlack\n1832118.3 ( 6.7)\n3696893.4 (10.7)\n0.015\n\n\n\n\nHispanic\n3263992.3 (11.9)\n3921344.6 (11.3)\n\n\n\n\n\nOther\n1887156.6 ( 6.9)\n2601870.3 ( 7.5)\n\n\n\n\n\nWhite\n20386465.2 (74.5)\n24371335.7 (70.5)\n\n\n\n\neducation (%)\nCollege\n15855712.5 (57.9)\n20945710.7 (60.6)\n0.522\n\n\n\n\nHigh.School\n10615218.7 (38.8)\n12434827.2 (35.9)\n\n\n\n\n\nSchool\n898801.1 ( 3.3)\n1210906.1 ( 3.5)\n\n\n\n\nmarried (%)\nMarried\n17489306.2 (63.9)\n21170020.0 (61.2)\n0.005\n\n\n\n\nNever.married\n3086474.4 (11.3)\n7175237.2 (20.7)\n\n\n\n\n\nPreviously.married\n6793951.8 (24.8)\n6246186.8 (18.1)\n\n\n\n\nincome (%)\n&lt;25k\n4760281.8 (17.4)\n6364208.6 (18.4)\n0.915\n\n\n\n\nBetween.25kto54k\n8682481.6 (31.7)\n10786198.6 (31.2)\n\n\n\n\n\nBetween.55kto99k\n6939847.0 (25.4)\n9190388.2 (26.6)\n\n\n\n\n\nOver100k\n6987121.9 (25.5)\n8250648.6 (23.9)\n\n\n\n\nbmi (mean (SD))\n\n29.35 (6.13)\n29.64 (7.05)\n0.593\n\n\n\ndiabetes (%)\nNo\n25080412.0 (91.6)\n30006523.6 (86.7)\n0.012\n\n\n\n\nYes\n2289320.3 ( 8.4)\n4584920.4 (13.3)\n\n\n\n\n\n\n\nLogistic regression model\nRun a logistic regression model using the same variables, utilizing the survey features. Report the corresponding odds ratios and the 95% confidence intervals.\n\nformula1 &lt;- as.formula(I(cholesterol.bin==\"unhealthy\") ~ gender + born +\n                         race + education + married + income + bmi +\n                         diabetes)\n\nfit1 &lt;- svyglm(formula1,\n               design = w.design, \n               family = binomial(link = \"logit\"))\n\npublish(fit1)\n#&gt;   Variable              Units OddsRatio       CI.95  p-value \n#&gt;     gender             Female       Ref                      \n#&gt;                          Male      0.70 [0.49;0.98]   0.2866 \n#&gt;       born         Born.in.US       Ref                      \n#&gt;                        Others      2.10 [1.41;3.13]   0.1707 \n#&gt;       race              Black       Ref                      \n#&gt;                      Hispanic      1.15 [0.80;1.67]   0.5871 \n#&gt;                         Other      1.11 [0.69;1.80]   0.7406 \n#&gt;                         White      1.46 [1.00;2.14]   0.3003 \n#&gt;  education            College       Ref                      \n#&gt;                   High.School      1.21 [0.96;1.52]   0.3563 \n#&gt;                        School      0.86 [0.52;1.43]   0.6712 \n#&gt;    married            Married       Ref                      \n#&gt;                 Never.married      0.54 [0.32;0.90]   0.2526 \n#&gt;            Previously.married      1.31 [0.92;1.87]   0.3704 \n#&gt;     income               &lt;25k       Ref                      \n#&gt;              Between.25kto54k      1.03 [0.61;1.73]   0.9408 \n#&gt;              Between.55kto99k      1.02 [0.66;1.56]   0.9525 \n#&gt;                      Over100k      1.12 [0.73;1.72]   0.6920 \n#&gt;        bmi                         1.00 [0.97;1.03]   0.9361 \n#&gt;   diabetes                 No       Ref                      \n#&gt;                           Yes      0.62 [0.41;0.95]   0.2720\n\nWald test (survey version)\nPerform a Wald test (survey version) to test the null hypothesis that all coefficients associated with the income variable are zero, and interpret.\n\n#Testing the H0 that all coefficients associated with the income variable are zero\nregTermTest(fit1, ~income)\n#&gt; Wald test for income\n#&gt;  in svyglm(formula = formula1, design = w.design, family = binomial(link = \"logit\"))\n#&gt; F =  0.1050099  on  3  and  1  df: p= 0.94611\n\nThe Wald test here gives a large p-value; We do not have evidence to reject the H0 of coefficient being 0. If the coefficient for income variable is 0, this means that the outcome in the model (cholesterol) is not affected by income. This suggests that removing income from the model does not statistically improve the model fit. So we can remove income variable from the model.\nBackward elimination\nRun a backward elimination (using the AIC criteria) on the above logistic regression fit (keeping important variables gender, race, bmi, diabetes in the model), and report the odds ratios and the 95% confidence intervals from the resulting final logistic regression fit.\n\n#Running backward elimination based on AIC\nrequire(MASS)\nscope &lt;- list(upper = ~ gender + born + race + education + \n                married + income + bmi + diabetes,\n              lower = ~ gender + race + bmi + diabetes)\n\nfit3 &lt;- step(fit1, scope = scope, trace = FALSE,\n                k = 2, direction = \"backward\")\n\n#Odds Ratios\npublish(fit3)\n#&gt;  Variable              Units OddsRatio       CI.95   p-value \n#&gt;    gender             Female       Ref                       \n#&gt;                         Male      0.71 [0.51;0.98]   0.08558 \n#&gt;      born         Born.in.US       Ref                       \n#&gt;                       Others      2.01 [1.37;2.96]   0.01184 \n#&gt;      race              Black       Ref                       \n#&gt;                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#&gt;                        Other      1.11 [0.68;1.81]   0.69539 \n#&gt;                        White      1.46 [0.99;2.17]   0.10469 \n#&gt;   married            Married       Ref                       \n#&gt;                Never.married      0.54 [0.32;0.90]   0.05770 \n#&gt;           Previously.married      1.30 [0.93;1.80]   0.17125 \n#&gt;       bmi                         1.00 [0.97;1.03]   0.95146 \n#&gt;  diabetes                 No       Ref                       \n#&gt;                          Yes      0.61 [0.40;0.91]   0.05445\n\nBorn and married are also found to be useful on top of gender + race + bmi + diabetes.\nInteraction terms\nChecking interaction terms\n– gender and whether married\n– gender and whether born in the US\n– gender and diabetes\n– whether married and diabetes\n\n#gender and married\nfit4 &lt;- update(fit3, .~. + interaction(gender, married))\nanova(fit3, fit4)\n#&gt; Working (Rao-Scott+F) LRT for interaction(gender, married)\n#&gt;  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#&gt;     born + race + married + bmi + diabetes + interaction(gender, \n#&gt;     married), design = w.design, family = binomial(link = \"logit\"))\n#&gt; Working 2logLR =  0.7461308 p= 0.70903 \n#&gt; (scale factors:  1.1 0.93 );  denominator df= 4\n\nDo not include interaction term\n\n#gender and born in us\nfit5 &lt;- update(fit3, .~. + interaction(gender, born))\nanova(fit3, fit5)\n#&gt; Working (Rao-Scott+F) LRT for interaction(gender, born)\n#&gt;  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#&gt;     born + race + married + bmi + diabetes + interaction(gender, \n#&gt;     born), design = w.design, family = binomial(link = \"logit\"))\n#&gt; Working 2logLR =  0.4635299 p= 0.52441 \n#&gt; df=1;  denominator df= 5\n\nDo not include interaction term\n\n#gender and diabetes\nfit6 &lt;- update(fit3, .~. + interaction(gender, diabetes))\nanova(fit3, fit6)\n#&gt; Working (Rao-Scott+F) LRT for interaction(gender, diabetes)\n#&gt;  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#&gt;     born + race + married + bmi + diabetes + interaction(gender, \n#&gt;     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#&gt; Working 2logLR =  1.222596 p= 0.32211 \n#&gt; df=1;  denominator df= 5\n\nDo not include interaction term\n\n#married and diabetes\nfit7 &lt;- update(fit3, .~. + interaction(married, diabetes))\nanova(fit3, fit7)\n#&gt; Working (Rao-Scott+F) LRT for interaction(married, diabetes)\n#&gt;  in svyglm(formula = I(cholesterol.bin == \"unhealthy\") ~ gender + \n#&gt;     born + race + married + bmi + diabetes + interaction(married, \n#&gt;     diabetes), design = w.design, family = binomial(link = \"logit\"))\n#&gt; Working 2logLR =  0.3207507 p= 0.84547 \n#&gt; (scale factors:  1.4 0.62 );  denominator df= 4\n\nDo not include interaction term\nNone of the interaction terms are improving the model fit.\nAUC\nReport AUC of the final model (only using weight argument) and interpret.\nAUC of the final model (only using weight argument) and interpret\n\nrequire(ROCR)\n# WeightedROC may not be on cran for all R versions\n# devtools::install_github(\"tdhock/WeightedROC\")\n\nlibrary(WeightedROC)\nsvyROCw &lt;- function(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight){\n  if (is.null(weight)){ # require(ROCR)\n    prob &lt;- predict(fit, type = \"response\")\n  pred &lt;- prediction(as.vector(prob), outcome)\n  perf &lt;- performance(pred, \"tpr\", \"fpr\")\n  auc &lt;- performance(pred, measure = \"auc\")\n  auc &lt;- auc@y.values[[1]]\n  roc.data &lt;- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n  with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  } else { \n    outcome &lt;- as.numeric(outcome)\n  pred &lt;- predict(fit, type = \"response\")\n  tp.fp &lt;- WeightedROC(pred, outcome, weight)\n  auc &lt;- WeightedAUC(tp.fp)\n  with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n}\nsvyROCw(fit = fit3, outcome = anadata$cholesterol.bin == \"unhealthy\", weight = anadata$weight)\n\n\n\n\n\n\n\nThe area under the curve in the final model is 0.611, using the survey weighted ROC. The AUC of 0.611 indicates that this model has poor discrimination.\nArcher-Lemeshow Goodness of fit\nReport Archer-Lemeshow Goodness of fit test and interpret (utilizing all the survey features).\n\n#Archer-Lemeshow Goodness of fit test utilizing all survey features\nAL.gof2 &lt;- function(fit = fit3, data = anadata, \n                   weight = \"weight\", psu = \"psu\", strata = \"strata\"){\n  r &lt;- residuals(fit, type = \"response\") \n  f&lt;-fitted(fit) \n  breaks.g &lt;- c(-Inf, quantile(f, (1:9)/10), Inf)\n  breaks.g &lt;- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g&lt;- cut(f, breaks.g)\n  data2g &lt;- cbind(data,r,g)\n  newdesign &lt;- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                         data=data2g, nest = TRUE)\n  decilemodel &lt;- svyglm(r~g, design=newdesign) \n  res &lt;- regTermTest(decilemodel, ~g)\n  return(res) \n}\n\nAL.gof2(fit3, anadata, weight = \"weight\", psu = \"psu\", strata = \"strata\")\n#&gt; Wald test for g\n#&gt;  in svyglm(formula = r ~ g, design = newdesign)\n#&gt; F =  0.7569326  on  9  and  6  df: p= 0.66075\n\nArcher and Lemeshow GoF test was used to test the fit of this model. The p-value of 0.3043, which is greater than 0.05. This means that there is no evidence of lack of fit to this model.\nAdd age as a predictor for linear regression\nFit another logistic regression (similar to Q1) with the above-mentioned predictors (as obtained in Q7) and age, utilizing the survey features. What difference do you see from the previous fit results?\n\naic.int.model &lt;- eval(fit3$call[[2]])\naic.int.model\n#&gt; I(cholesterol.bin == \"unhealthy\") ~ gender + born + race + married + \n#&gt;     bmi + diabetes\n\nformula3 &lt;- as.formula(cholesterol.bin ~ gender + born + race + married + bmi + diabetes + age)\nfit9 &lt;- svyglm(formula3,\n               design = w.design,\n               family = binomial(link=\"logit\"))\nsummary(fit9)\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = formula3, design = w.design, family = binomial(link = \"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; svydesign(id = ~psu, weights = ~weight, strata = ~strata, nest = TRUE, \n#&gt;     data = anadata)\n#&gt; \n#&gt; Coefficients:\n#&gt;                             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)                1.0657919  0.6260889   1.702   0.1494  \n#&gt; genderMale                 0.3821902  0.1703394   2.244   0.0749 .\n#&gt; bornOthers                -0.6912102  0.2026407  -3.411   0.0190 *\n#&gt; raceHispanic              -0.2442019  0.1823190  -1.339   0.2381  \n#&gt; raceOther                 -0.1570271  0.2306208  -0.681   0.5262  \n#&gt; raceWhite                 -0.3638735  0.2029676  -1.793   0.1330  \n#&gt; marriedNever.married       0.4029107  0.2637962   1.527   0.1872  \n#&gt; marriedPreviously.married -0.2096009  0.1620478  -1.293   0.2524  \n#&gt; bmi                       -0.0002237  0.0134117  -0.017   0.9873  \n#&gt; diabetesYes                0.6534019  0.2456333   2.660   0.0449 *\n#&gt; age                       -0.0151364  0.0042038  -3.601   0.0155 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1.000456)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\npublish(fit9)\n#&gt;  Variable              Units OddsRatio       CI.95   p-value \n#&gt;    gender             Female       Ref                       \n#&gt;                         Male      1.47 [1.05;2.05]   0.07487 \n#&gt;      born         Born.in.US       Ref                       \n#&gt;                       Others      0.50 [0.34;0.75]   0.01902 \n#&gt;      race              Black       Ref                       \n#&gt;                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#&gt;                        Other      0.85 [0.54;1.34]   0.52619 \n#&gt;                        White      0.69 [0.47;1.03]   0.13299 \n#&gt;   married            Married       Ref                       \n#&gt;                Never.married      1.50 [0.89;2.51]   0.18721 \n#&gt;           Previously.married      0.81 [0.59;1.11]   0.25238 \n#&gt;       bmi                         1.00 [0.97;1.03]   0.98734 \n#&gt;  diabetes                 No       Ref                       \n#&gt;                          Yes      1.92 [1.19;3.11]   0.04488 \n#&gt;       age                         0.98 [0.98;0.99]   0.01553\n\nComparing with previous model fit\n\npublish(fit3)\n#&gt;  Variable              Units OddsRatio       CI.95   p-value \n#&gt;    gender             Female       Ref                       \n#&gt;                         Male      0.71 [0.51;0.98]   0.08558 \n#&gt;      born         Born.in.US       Ref                       \n#&gt;                       Others      2.01 [1.37;2.96]   0.01184 \n#&gt;      race              Black       Ref                       \n#&gt;                     Hispanic      1.15 [0.81;1.65]   0.46785 \n#&gt;                        Other      1.11 [0.68;1.81]   0.69539 \n#&gt;                        White      1.46 [0.99;2.17]   0.10469 \n#&gt;   married            Married       Ref                       \n#&gt;                Never.married      0.54 [0.32;0.90]   0.05770 \n#&gt;           Previously.married      1.30 [0.93;1.80]   0.17125 \n#&gt;       bmi                         1.00 [0.97;1.03]   0.95146 \n#&gt;  diabetes                 No       Ref                       \n#&gt;                          Yes      0.61 [0.40;0.91]   0.05445\npublish(fit9)\n#&gt;  Variable              Units OddsRatio       CI.95   p-value \n#&gt;    gender             Female       Ref                       \n#&gt;                         Male      1.47 [1.05;2.05]   0.07487 \n#&gt;      born         Born.in.US       Ref                       \n#&gt;                       Others      0.50 [0.34;0.75]   0.01902 \n#&gt;      race              Black       Ref                       \n#&gt;                     Hispanic      0.78 [0.55;1.12]   0.23809 \n#&gt;                        Other      0.85 [0.54;1.34]   0.52619 \n#&gt;                        White      0.69 [0.47;1.03]   0.13299 \n#&gt;   married            Married       Ref                       \n#&gt;                Never.married      1.50 [0.89;2.51]   0.18721 \n#&gt;           Previously.married      0.81 [0.59;1.11]   0.25238 \n#&gt;       bmi                         1.00 [0.97;1.03]   0.98734 \n#&gt;  diabetes                 No       Ref                       \n#&gt;                          Yes      1.92 [1.19;3.11]   0.04488 \n#&gt;       age                         0.98 [0.98;0.99]   0.01553\nAIC(fit3)\n#&gt;       eff.p         AIC    deltabar \n#&gt;   11.121795 1706.792543    1.235755\nAIC(fit9)\n#&gt;      eff.p        AIC   deltabar \n#&gt;   12.14310 1694.15974    1.21431\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Cholesterol"
    ]
  },
  {
    "objectID": "surveydata8.html",
    "href": "surveydata8.html",
    "title": "NHANES: Subsetting",
    "section": "",
    "text": "The tutorial demonstrates how to work with subset of complex survey data, specifically focusing on an NHANES example.\nThe required packages are loaded.\n\n# Load required packages\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\n\nLoad data\nSurvey data is loaded into the R environment.\n\nload(\"Data/surveydata/NHANES17.RData\")\nls()\n#&gt; [1] \"analytic\"           \"analytic.with.miss\"\n\nCheck missingness\nA subset of variables is selected, and the presence of missing data is visualized.\n\nVars &lt;- c(\"ID\", \n          \"weight\", \n          \"psu\", \n          \"strata\", \n          \"gender\", \n          \"born\", \n          \"race\", \n          \"bmi\", \n          \"cholesterol\", \n          \"diabetes\")\nanalytic.full.data &lt;- analytic.with.miss[,Vars]\n\nA new variable is also created to categorize cholesterol levels as “healthy” or “unhealthy.”\n\nanalytic.full.data$cholesterol.bin &lt;- ifelse(analytic.full.data$cholesterol &lt;200, \"healthy\", \"unhealthy\")\nanalytic.full.data$cholesterol &lt;- NULL\n\nrequire(DataExplorer)\nplot_missing(analytic.full.data)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nSubsetting Complex Survey data\nWe are subsetting based on whether the subjects have missing observation (e.g., only retaining those with complete information). This is often an eligibility criteria in studies. In missing data analysis, we will learn more about more appropriate approaches.\n\ndim(analytic.full.data)\n#&gt; [1] 9254   10\nhead(analytic.full.data$ID) # full data\n#&gt; [1] 93703 93704 93705 93706 93707 93708\nanalytic.complete.case.only &lt;- as.data.frame(na.omit(analytic.full.data))\ndim(analytic.complete.case.only)\n#&gt; [1] 6636   10\nhead(analytic.complete.case.only$ID) # complete case\n#&gt; [1] 93705 93706 93707 93708 93709 93711\nhead(analytic.full.data$ID[analytic.full.data$ID %in% analytic.complete.case.only$ID])\n#&gt; [1] 93705 93706 93707 93708 93709 93711\n\nBelow we show how to identify who has missing observations vs not based on full (analytic.full.data) and complete case (analytic.complete.case.only) data. See Heeringa et al (2010) book page 114 (section 4.5.3 “Preparation for Subclass analyses”) and also page 218 (section 7.5.4 “appropriate analysis: incorporating all Sample Design Features”). This is done for 2 reasons:\n\nfull complex survey design structure is taken into account, so that variance estimation is done correctly. If one or more PSU were excluded because none of the complete cases were observed in those PSU, the sub-population (complete cases) will not have complete information of how many PSU were actually present in the original complex design. Then in the population, a reduced number of PSUs would be used to calculate variance (number of SPU is a component of the variance calculation formula, see equation (5.2) in Heeringa et al (2010) textbook. Same is true for strata.), and will result in a wrong/biased variance estimate. Also see West et al. doi: 10.1177/1536867X0800800404\nsize of sub-population (here, those with complete cases) is recognized as a random variable; not just a fixed size.\n\n\n# assign missing indicator\nanalytic.full.data$miss &lt;- 1 \n# assign missing indicator = 0 if the observation is available\nanalytic.full.data$miss[analytic.full.data$ID %in% analytic.complete.case.only$ID] &lt;- 0\n\n\ntable(analytic.full.data$miss)\n#&gt; \n#&gt;    0    1 \n#&gt; 6636 2618\n# IDs not in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==1])\n#&gt; [1] 93703 93704 93710 93720 93724 93725\n# IDs in complete case data\nhead(analytic.full.data$ID[analytic.full.data$miss==0])\n#&gt; [1] 93705 93706 93707 93708 93709 93711\n\nLogistic regression on sub-population\nA logistic regression model is run on the subset of data that has no missing values. Here, it distinguishes between correct and incorrect approaches to account for the complex survey design.\n\nrequire(survey)\nrequire(Publish)\nmodel.formula &lt;- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nWrong approach\n\nw.design.wrong &lt;- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.complete.case.only, #wrong!!\n                       nest = TRUE)\n\nCorrect approach\n\nw.design0 &lt;- svydesign(ids=~psu, \n                       weights=~weight, \n                       strata=~strata,\n                       data = analytic.full.data, \n                       nest = TRUE)\n\n# retain only those that have complete observation / no missing\nw.design &lt;- subset(w.design0, miss == 0)# this is the subset design\n\nFull model\n\nfit &lt;- svyglm(model.formula, family = quasibinomial, \n              design = w.design) # subset design\npublish(fit)\n#&gt;  Variable                            Units Coefficient           CI.95     p-value \n#&gt;  diabetes                               No         Ref                             \n#&gt;                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#&gt;    gender                           Female         Ref                             \n#&gt;                                       Male        0.22     [0.03;0.40]   0.0568343 \n#&gt;      born Born in 50 US states or Washingt         Ref                             \n#&gt;                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#&gt;                                    Refused      -12.26 [-13.65;-10.88]     &lt; 1e-04 \n#&gt;      race                            Black         Ref                             \n#&gt;                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#&gt;                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#&gt;                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#&gt;       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\nVariable selection\nFinally, we discuss variable selection methods. We employ backward elimination to determine which variables are significant predictors while retaining an important variable in the model. If unsure about usefulness of some (gender, born, race, bmi) variables in predicting the outcome, check via backward elimination while keeping important variable (diabetes, say, that has been established in the literature) in the model\n\nmodel.formula &lt;- as.formula(\"I(cholesterol.bin=='healthy')~\n                            diabetes+gender+born+race+bmi\")\n\nscope &lt;- list(upper = ~ diabetes+gender+born+race+bmi, lower = ~ diabetes)\n\nfit &lt;- svyglm(model.formula, design=w.design, # subset design\n              family=quasibinomial)\n\nfitstep &lt;- step(fit,  scope = scope, trace = FALSE, direction = \"backward\")\npublish(fitstep) # final model\n#&gt;  Variable                            Units Coefficient           CI.95     p-value \n#&gt;  diabetes                               No         Ref                             \n#&gt;                                        Yes        0.38     [0.20;0.57]   0.0049202 \n#&gt;    gender                           Female         Ref                             \n#&gt;                                       Male        0.22     [0.03;0.40]   0.0568343 \n#&gt;      born Born in 50 US states or Washingt         Ref                             \n#&gt;                                     Others       -0.66   [-0.84;-0.47]   0.0002304 \n#&gt;                                    Refused      -12.26 [-13.65;-10.88]     &lt; 1e-04 \n#&gt;      race                            Black         Ref                             \n#&gt;                                   Hispanic        0.20    [-0.08;0.47]   0.2075536 \n#&gt;                                      Other       -0.17    [-0.38;0.03]   0.1439474 \n#&gt;                                      White       -0.37   [-0.66;-0.09]   0.0355030 \n#&gt;       bmi                                        -0.04   [-0.05;-0.02]   0.0007697\n\nAlso see (Stata 2023) for further details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nStata. 2023. “How Can i Analyze a Subpopulation of My Survey Data in Stata?” https://stats.oarc.ucla.edu/stata/faq/how-can-i-analyze-a-subpopulation-of-my-survey-data-in-stata/.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Subsetting"
    ]
  },
  {
    "objectID": "surveydata9.html",
    "href": "surveydata9.html",
    "title": "NHANES: Reliability Standards",
    "section": "",
    "text": "Introduction\nThis tutorial reproduces the key tables from the Flegal et al. (2016) article (Flegal et al. 2016). The analysis uses the same NHANES data and aims to replicate the unweighted sample size counts from Table 1 and the weighted logistic regression models from Table 3. We incorporate NCHS/CDC reliability standards to ensure estimates are statistically defensible (Disease Control and Prevention 2025).",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Reliability Standards"
    ]
  },
  {
    "objectID": "surveydata9.html#setup-and-data-preparation",
    "href": "surveydata9.html#setup-and-data-preparation",
    "title": "NHANES: Reliability Standards",
    "section": "Setup and Data Preparation",
    "text": "Setup and Data Preparation\nThis first section prepares the data for analysis. The key steps are:\n\n\nLoad Data: The Flegal2016.RData file containing the necessary NHANES data is loaded.\n\nDefine Analytic Sample: An analytic_design object is created. This sample of N=5,455 matches the total number of participants reported in the paper for the 2013–2014 cycle.\n\nRecode Variables: All variables needed for the analysis (e.g., Age, race, obese, smoking, education) are created and categorized to match the definitions used by Flegal et al.\n\nCreate Survey Design Object: A svydesign object is created to account for survey design (weights, strata, clusters).\n\n\n# --- Load Libraries ---\nlibrary(survey)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(car)\nlibrary(tidyr)\nlibrary(Publish)\nlibrary(stringr)\nlibrary(kableExtra) \n# install.packages(\"devtools\")\n# devtools::install_github(\"ehsanx/svyTable1\", \n#                          build_vignettes = TRUE, \n#                          dependencies = TRUE)\nlibrary(svyTable1)\n\n# --- Load Data ---\nload(\"Data/surveydata/Flegal2016.RData\")\n\n# --- Create Analytic Sample (N=5,455) ---\ndat.analytic &lt;- subset(dat.full, RIDAGEYR &gt;= 20)\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI))\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \"Yes, positive lab pregnancy test\")\n\n# --- Create ALL Analysis Variables in the Full Dataset ---\ndat.full &lt;- dat.full %&gt;%\n  mutate(\n    Age = cut(RIDAGEYR,\n              breaks = c(20, 40, 60, Inf),\n              right = FALSE,\n              labels = c(\"20-39\", \"40-59\", \"&gt;=60\")),\n    \n    race = case_when(\n      RIDRETH3 == \"Non-Hispanic White\" ~ \"Non-Hispanic white\",\n      RIDRETH3 == \"Non-Hispanic Black\" ~ \"Non-Hispanic black\",\n      RIDRETH3 == \"Non-Hispanic Asian\" ~ \"Non-Hispanic Asian\",\n      RIDRETH3 %in% c(\"Mexican American\", \"Other Hispanic\") ~ \"Hispanic\",\n      RIDRETH3 == \"Other Race - Including Multi-Rac\" ~ \"Other\",\n      TRUE ~ NA_character_\n    ),\n    race = factor(race, levels = c(\"Non-Hispanic white\", \"Non-Hispanic black\", \"Non-Hispanic Asian\", \"Hispanic\", \"Other\")),\n    \n    gender = factor(RIAGENDR, labels = c(\"Male\", \"Female\")),\n    \n    obese = factor(ifelse(BMXBMI &gt;= 30, \"Yes\", \"No\"), levels = c(\"No\", \"Yes\")),\n    obese_class3 = factor(ifelse(BMXBMI &gt;= 40, \"Yes\", \"No\"), levels = c(\"No\", \"Yes\")),\n    \n    smoking = case_when(\n      SMQ020 == \"No\" ~ \"Never smoker\",\n      SMQ020 == \"Yes\" & SMQ040 == \"Not at all\" ~ \"Former smoker\",\n      SMQ020 == \"Yes\" ~ \"Current smoker\",\n      TRUE ~ NA_character_\n    ),\n    smoking = factor(smoking, levels = c(\"Never smoker\", \"Former smoker\", \"Current smoker\")),\n\n    education = case_when(\n        DMDEDUC2 %in% c(\"Less than 9th grade\", \"9-11th grade (Includes 12th grad)\") ~ \"&lt;High school\",\n        DMDEDUC2 == \"High school graduate/GED or equi\" ~ \"High school\",\n        DMDEDUC2 %in% c(\"Some college or AA degree\", \"College graduate or above\") ~ \"&gt;High school\",\n        TRUE ~ NA_character_\n    ),\n    education = factor(education, levels = c(\"High school\", \"&lt;High school\", \"&gt;High school\"))\n  )\n\n# --- Create Survey Design Object ---\noptions(survey.lonely.psu = \"adjust\")\nsvy.design.full &lt;- svydesign(id = ~SDMVPSU, \n                             strata = ~SDMVSTRA, \n                             weights = ~WTINT2YR, \n                             nest = TRUE, \n                             data = dat.full)\nanalytic_design &lt;- subset(svy.design.full, SEQN %in% dat.analytic$SEQN)",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Reliability Standards"
    ]
  },
  {
    "objectID": "surveydata9.html#reproducing-table-1",
    "href": "surveydata9.html#reproducing-table-1",
    "title": "NHANES: Reliability Standards",
    "section": "Reproducing Table 1",
    "text": "Reproducing Table 1\nThis section reproduces the unweighted sample sizes shown in Flegal et al.’s Table 1. The code first generates separate summary tables for all participants, men, and women. It then performs several formatting steps to combine these into a single table.\nWhat svytable1 Does\nThe svytable1 function creates a descriptive summary table—commonly referred to as a “Table 1”—from complex survey data (Karim 2025). It is specifically designed to produce publication-ready results that align with NCHS Data Presentation Standards for reliability.\nKey svytable1 Operations\nWhen we call svytable1 (link), it performs the following steps for each analysis (for example, for all participants, men, and women):\n\nCalculates Proportions\nIt summarizes categorical variables (like Age) by calculating the proportion of participants in each category (e.g., 20–39, 40–59, ≥60).\nStratifies by a Grouping Variable\nResults are calculated separately by levels of the strata_var (for example, race), creating side-by-side columns.\n\nDisplays Mixed Mode Results\nBecause mode = \"mixed\", each cell shows both:\n\nThe unweighted sample count (N)\n\nThe weighted percentage (%), which represents the population estimate.\n\n\nPerforms Reliability Checks\nWhen reliability_checks = TRUE, the function evaluates each estimate against NCHS Data Presentation Standards. These checks prevent publication of unstable or statistically unreliable estimates.\nWhat the Asterisk (*) Means\nAn asterisk (*) in these tables output indicate suppression: the estimate was determined to be statistically unreliable. The function hides the unreliable value to avoid misinterpretation.\nNCHS reliability rules (for proportions)\nEstimates are suppressed if they fail one or more NCHS reliability rules (for proportions):\n\n\nfail_n_30: The unweighted sample size (n) is fewer than 30 participants.\n\n\nfail_eff_n_30: The effective sample size (adjusted for design effects) is less than 30.\n\n\nfail_df_8: The design degrees of freedom (df) are fewer than 8.\n\n\nfail_ciw_30: The absolute confidence interval width is ≥ 30 percentage points.\n\n\nfail_rciw_130: The relative CI width is greater than 130%.\nEach of these flags indicates limited precision or instability in the estimate.\n\nIn the output, the asterisks appear in the “Other” race column for certain age groups (such as “40–59” and “≥60”).\nThis happens because the number of participants in those cells is very small, producing unstable or wide confidence intervals. Thus, the function correctly replaces the unreliable estimates with *, ensuring the published results remain statistically defensible and transparent.\nReliability Metrics Table\nIn addition to the detailed checks for proportions, the svytable1 function also assesses the reliability of means for numeric variables. For these estimates, it applies the standard NCHS recommendation, which uses the Relative Standard Error (RSE). If a mean’s RSE is 30% or greater, it is considered statistically unreliable and will be suppressed with an asterisk (*) in the formatted table.\nThe $reliability_metrics table will be printed with the output if we select return_metrics = TRUE which will include rows for each mean, reporting the calculated RSE and the outcome of this check in the fail_rse_30 column.\n\n# View reliability_metrics\ntable1_svy &lt;- svytable1(\n  design = analytic_design, strata_var = \"race\", \n  table_vars = \"Age\", mode = \"mixed\",\n  reliability_checks = TRUE,\n  return_metrics = TRUE\n)\ntable1_svy$reliability_metrics[table1_svy$reliability_metrics$suppressed == TRUE, ]\n\n\n  \n\n\n\nSummary tables for each group\n\n# --- Create the summary tables for each group ---\ntable1_svy_all &lt;- svytable1(\n  design = analytic_design, strata_var = \"race\", \n  table_vars = \"Age\", mode = \"mixed\",\n  reliability_checks = TRUE\n) %&gt;%\n  mutate(Variable = dplyr::recode(Variable, \"Age\" = \"Age Groups\"))\nkable(table1_svy_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLevel\nOverall\nNon-Hispanic white\nNon-Hispanic black\nNon-Hispanic Asian\nHispanic\nOther\n\n\n\nn\n\n5,455\n2,343\n1,115\n623\n1,214\n160\n\n\nAge Groups\n\n\n\n\n\n\n\n\n\n\n20-39\n1,810 (35.5%)\n734 (30.8%)\n362 (38.9%)\n216 (40.2%)\n412 (49.6%)\n86 (49.2%)\n\n\n\n40-59\n1,896 (37.5%)\n759 (37.5%)\n383 (39.3%)\n251 (38.5%)\n449 (35.6%)\n*\n\n\n\n&gt;=60\n1,749 (27.0%)\n850 (31.7%)\n370 (21.8%)\n156 (21.2%)\n353 (14.8%)\n*\n\n\n\n\n\nmale_design &lt;- subset(analytic_design, gender == \"Male\")\ntable1_svy_men &lt;- svytable1(\n  design = male_design, strata_var = \"race\", \n  table_vars = \"Age\", mode = \"mixed\",\n  reliability_checks = TRUE\n) %&gt;%\n  mutate(Variable = dplyr::recode(Variable, \"Age\" = \"Age group\"))\nkable(table1_svy_men)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLevel\nOverall\nNon-Hispanic white\nNon-Hispanic black\nNon-Hispanic Asian\nHispanic\nOther\n\n\n\nn\n\n2,638\n1,130\n556\n300\n573\n79\n\n\nAge group\n\n\n\n\n\n\n\n\n\n\n20-39\n909 (37.2%)\n386 (32.3%)\n182 (41.3%)\n106 (41.7%)\n189 (51.6%)\n46 (52.9%)\n\n\n\n40-59\n897 (37.6%)\n360 (37.9%)\n179 (39.0%)\n120 (38.5%)\n215 (35.1%)\n*\n\n\n\n&gt;=60\n832 (25.1%)\n384 (29.8%)\n195 (19.7%)\n74 (19.8%)\n169 (13.2%)\n*\n\n\n\n\n\nfemale_design &lt;- subset(analytic_design, gender == \"Female\")\ntable1_svy_women &lt;- svytable1(\n  design = female_design, strata_var = \"race\", \n  table_vars = \"Age\", mode = \"mixed\",\n  reliability_checks = TRUE\n) %&gt;%\n  mutate(Variable = dplyr::recode(Variable, \"Age\" = \"Age group\"))\nkable(table1_svy_women)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLevel\nOverall\nNon-Hispanic white\nNon-Hispanic black\nNon-Hispanic Asian\nHispanic\nOther\n\n\n\nn\n\n2,817\n1,213\n559\n323\n641\n81\n\n\nAge group\n\n\n\n\n\n\n\n\n\n\n20-39\n901 (33.8%)\n348 (29.4%)\n180 (36.9%)\n110 (39.0%)\n223 (47.6%)\n*\n\n\n\n40-59\n999 (37.4%)\n399 (37.1%)\n204 (39.5%)\n131 (38.6%)\n234 (36.0%)\n*\n\n\n\n&gt;=60\n917 (28.8%)\n466 (33.5%)\n175 (23.6%)\n82 (22.5%)\n184 (16.4%)\n*\n\n\n\n\n\nFormat and combine the tables\n\n# --- Format and combine the tables ---\ntable_all_formatted &lt;- table1_svy_all %&gt;%\n  select(\n    `Age Groups,y` = Level, `All Groups` = Overall,\n    `White` = `Non-Hispanic white`, `Black` = `Non-Hispanic black`,\n    `Asian` = `Non-Hispanic Asian`, `Hispanic`\n  ) %&gt;%\n  slice(3:5) \n\ntable_men_formatted &lt;- table1_svy_men %&gt;%\n  select(\n    `Age Groups,y` = Level, `All Groups` = Overall,\n    `White` = `Non-Hispanic white`, `Black` = `Non-Hispanic black`,\n    `Asian` = `Non-Hispanic Asian`, `Hispanic`\n  ) %&gt;%\n  slice(3:5)\n\ntable_women_formatted &lt;- table1_svy_women %&gt;%\n  select(\n    `Age Groups,y` = Level, `All Groups` = Overall,\n    `White` = `Non-Hispanic white`, `Black` = `Non-Hispanic black`,\n    `Asian` = `Non-Hispanic Asian`, `Hispanic`\n  ) %&gt;%\n  slice(3:5) \n\nfinal_table_data &lt;- bind_rows(\n  table_all_formatted,\n  table_men_formatted,\n  table_women_formatted\n)\n\n# --- Render the final, publication-quality table ---\nfinal_table_data %&gt;%\n  kable(caption = \"Characteristics by Group and Race/Hispanic Origin\", align = \"lrrrrr\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\"), full_width = FALSE) %&gt;%\n  add_header_above(c(\" \" = 1, \"All Groups\" = 1, \"Race/Ethnicity\" = 4)) %&gt;%\n  kableExtra::group_rows(index = c(\"All participants\" = 3,\n                                   \"Men\" = 3,\n                                   \"Women\" = 3))\n\n\nCharacteristics by Group and Race/Hispanic Origin\n\n\n\n\n\n\n\n\n\n\n\n\nAll Groups\n\n\nRace/Ethnicity\n\n\n\nAge Groups,y\nAll Groups\nWhite\nBlack\nAsian\nHispanic\n\n\n\n\nAll participants\n\n\n20-39\n1,810 (35.5%)\n734 (30.8%)\n362 (38.9%)\n216 (40.2%)\n412 (49.6%)\n\n\n40-59\n1,896 (37.5%)\n759 (37.5%)\n383 (39.3%)\n251 (38.5%)\n449 (35.6%)\n\n\n&gt;=60\n1,749 (27.0%)\n850 (31.7%)\n370 (21.8%)\n156 (21.2%)\n353 (14.8%)\n\n\nMen\n\n\n20-39\n909 (37.2%)\n386 (32.3%)\n182 (41.3%)\n106 (41.7%)\n189 (51.6%)\n\n\n40-59\n897 (37.6%)\n360 (37.9%)\n179 (39.0%)\n120 (38.5%)\n215 (35.1%)\n\n\n&gt;=60\n832 (25.1%)\n384 (29.8%)\n195 (19.7%)\n74 (19.8%)\n169 (13.2%)\n\n\nWomen\n\n\n20-39\n901 (33.8%)\n348 (29.4%)\n180 (36.9%)\n110 (39.0%)\n223 (47.6%)\n\n\n40-59\n999 (37.4%)\n399 (37.1%)\n204 (39.5%)\n131 (38.6%)\n234 (36.0%)\n\n\n&gt;=60\n917 (28.8%)\n466 (33.5%)\n175 (23.6%)\n82 (22.5%)\n184 (16.4%)",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Reliability Standards"
    ]
  },
  {
    "objectID": "surveydata9.html#reproducing-table-3",
    "href": "surveydata9.html#reproducing-table-3",
    "title": "NHANES: Reliability Standards",
    "section": "Reproducing Table 3",
    "text": "Reproducing Table 3\nThis section reproduces the weighted logistic regression models from Flegal et al.’s Table 3. Four separate models are fit: one for obesity and one for class 3 obesity, each stratified by gender. We extend the replication by incorporating NCHS/CDC reliability checks using relative standard error (RSE) for regression coefficients. Per NCHS practices, coefficients with RSE ≥ 30% are considered unreliable and flagged with an asterisk (*).\n\n# --- Helper function to format the output of publish() ---\nformat_publish_output &lt;- function(publish_object, new_col_name) {\n  \n  # Extract the clean regression table from the publish object\n  df &lt;- publish_object$regressionTable\n  \n  # Combine OR and CI into one string, handling reference groups\n  df$result &lt;- ifelse(\n    df$OddsRatio == \"Ref\", \n    \"1 [Reference]\",\n    # Combine the OR and CI, removing the brackets from the CI string and replacing the semicolon\n    paste0(df$OddsRatio, \" (\", str_replace(str_remove_all(df$CI.95, \"\\\\[|\\\\]\"), \";\", \"-\"), \")\")\n  )\n  \n  # Return a clean tibble with just the variable level and the formatted result\n  tibble(term = df$Units, !!new_col_name := df$result)\n}\n\n# --- Create complete-case survey designs for each gender ---\nmale_design_complete &lt;- subset(analytic_design, gender == \"Male\" & !is.na(smoking) & !is.na(education))\nfemale_design_complete &lt;- subset(analytic_design, gender == \"Female\" & !is.na(smoking) & !is.na(education))\n\nFit the Models\n\n# --- Fit all four models ---\nfit_men_obese &lt;- svyglm(I(obese == \"Yes\") ~ Age + race + smoking + education, design = male_design_complete, family = binomial())\n# print(summary(fit_men_obese))\nfit_men_obese3 &lt;- svyglm(I(obese_class3 == \"Yes\") ~ Age + race + smoking + education, design = male_design_complete, family = binomial())\nfit_women_obese &lt;- svyglm(I(obese == \"Yes\") ~ Age + race + smoking + education, design = female_design_complete, family = binomial())\nfit_women_obese3 &lt;- svyglm(I(obese_class3 == \"Yes\") ~ Age + race + smoking + education, design = female_design_complete, family = binomial())\n\nReliability check for regression coefficients\nThe National Center for Health Statistics (NCHS) and the Centers for Disease Control and Prevention (CDC) do not recommend using the relative standard error (RSE) as a primary reliability check for regression coefficients. Their guidelines focus on other types of estimates.\n\nFor means and totals, the RSE is a commonly used metric.\nFor proportions, while the RSE was used in the past, the NCHS has transitioned to a more sophisticated, multi-step approach. This newer method considers a minimum number of events or a minimum denominator sample size, and the width of the confidence interval around the proportion. This change was made because the RSE can be misleading for proportions, especially those close to 0% or 100%.\nThere are no well-established NCHS or CDC guidelines that advocate for the use of RSE to determine the reliability of regression coefficients. The statistical properties of regression coefficients are more complex than those of means or proportions. The reliability of a regression coefficient is influenced by a variety of factors, including the sample size, the variance of the independent variable, and the overall fit of the model. Given this complexity, a simple RSE threshold is not considered an adequate measure of reliability for regression coefficients. Instead, we assess reliability by examining several key metrics that, taken together, give us a complete picture of a coefficient’s stability and precision. The main tool for this is the svydiag() functions in svyTable1 package, which we’ll use to look at:\n\n\nThe Standard Error (SE): A direct measure of the coefficient’s precision. A smaller SE relative to its coefficient suggests a more reliable estimate.\nThe p-value: Tells if the coefficient is statistically distinguishable from zero. A non-significant p-value (e.g., p &gt; 0.05) means we cannot be confident the predictor has any association with the outcome.\nThe Confidence Interval (CI): Provides a plausible range for the true value of the coefficient. A very wide CI indicates a high degree of uncertainty and, therefore, low reliability. For logistic regression, if the CI for the odds ratio contains 1.0, the result is not statistically significant.\n\nWe will also calculate the RSE to demonstrate why it can be misleading. Finally, we’ll run a quick check for multicollinearity using the Variance Inflation Factor (VIF), as this is a common cause of unstable (unreliable) coefficients.\n\n# Let's examine one model in detail: fit_men_obese\n# The same process applies to the other three models.\n# Get the standard model summary and confidence intervals\ndiagnostics_table &lt;- svydiag(fit_men_obese)\nknitr::kable(\n  diagnostics_table,\n  caption = \"Table 3: Reliability Diagnostics for Obesity Model Coefficients\",\n  digits = 3\n)\n\n\nTable 3: Reliability Diagnostics for Obesity Model Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\np.value\nis_significant\nCI_Lower\nCI_Upper\nCI_Width\nRSE_percent\nis_rse_high\n\n\n\n(Intercept)\n-0.745\n0.167\n0.007\nTRUE\n-1.175\n-0.315\n0.860\n22.451\nFALSE\n\n\nAge40-59\n0.204\n0.174\n0.294\nFALSE\n-0.243\n0.651\n0.895\n85.311\nTRUE\n\n\nAge&gt;=60\n0.159\n0.218\n0.498\nFALSE\n-0.401\n0.719\n1.119\n136.779\nTRUE\n\n\nraceNon-Hispanic black\n0.279\n0.136\n0.096\nFALSE\n-0.071\n0.630\n0.701\n48.796\nTRUE\n\n\nraceNon-Hispanic Asian\n-1.277\n0.164\n0.001\nTRUE\n-1.697\n-0.856\n0.841\n12.811\nFALSE\n\n\nraceHispanic\n0.264\n0.190\n0.224\nFALSE\n-0.225\n0.752\n0.977\n72.060\nTRUE\n\n\nraceOther\n0.158\n0.322\n0.644\nFALSE\n-0.670\n0.987\n1.657\n203.734\nTRUE\n\n\nsmokingFormer smoker\n0.207\n0.140\n0.198\nFALSE\n-0.152\n0.567\n0.719\n67.436\nTRUE\n\n\nsmokingCurrent smoker\n-0.250\n0.149\n0.153\nFALSE\n-0.632\n0.132\n0.764\n59.346\nTRUE\n\n\neducation&lt;High school\n-0.528\n0.154\n0.019\nTRUE\n-0.924\n-0.132\n0.792\n29.164\nFALSE\n\n\neducation&gt;High school\n-0.013\n0.147\n0.932\nFALSE\n-0.390\n0.364\n0.754\n1109.146\nTRUE\n\n\n\n\n\nGenerally, the model shows limited reliability and predictive power. Most of the predictor variables, such as Age and smoking status, are not statistically significant (their p.value is high). This indicates that, for men in this dataset, these factors don’t have a clear, reliable association with obesity.\nThe few significant predictors are raceNon-Hispanic Asian and education\\&lt;High school. These coefficients are considered stable and reliable. The unreliability of the other terms is not caused by the variables being correlated with each other, as the multicollinearity check shows.\nThe RSE Can Be Misleading for Regression: Notice that some statistically insignificant coefficients (like Age40-59 and raceHispanic) have high RSEs, which is expected. However, the education\\&gt;High school coefficient is highly insignificant: p-value of 0.932 correctly tells that this coefficient is not statistically significant and is not reliably different from zero. However its RSE is flagged as “TRUE” for being unreliable. The RSE is calculated as (0.147 / -0.013) * 100 = 1109%. Here, the extremely high RSE here is not a result of a large standard error, but of the coefficient estimate being very close to zero. An inflated RSE doesn’t provide any new or more accurate information than the p-value; it simply reflects that the coefficient itself is minuscule. This is a great example of why RSE isn’t a primary tool for regression coefficients: it can be inflated by estimates close to zero, regardless of their precision.\nCheck for Multicollinearity\nMulticollinearity occurs when predictor variables in a model are highly correlated with each other. This can inflate the standard errors and make the coefficient estimates unstable. The VIF is used to detect this issue.\nGVIF^(1/(2*Df)) is a scaled version of the Generalized Variance Inflation Factor (GVIF) used to assess multicollinearity in regression models with categorical predictors (Fox and Monette 1992). It adjusts for the number of dummy variables created for a categorical variable, making its value directly comparable to the traditional VIF used for continuous predictors within the same model.\nWhy GVIF1/(2×Df) is Necessary for Categorical Variables\nA categorical variable with (k) levels (e.g., race) is typically represented in a regression model by (k - 1) dummy variables. Dummy variables are inherently correlated because they all describe the same categorical feature. This intrinsic relationship would lead to very high — but misleading — GVIF scores if the overall GVIF were interpreted directly.\nThe adjustment GVIF1/(2×Df) standardizes the GVIF value. It reduces the measure from a hypervolume of confidence for multiple coefficients down to a linear measure, making it comparable to the single VIF value used for continuous predictors. Here, Df is the degrees of freedom for the categorical term, which equals (k - 1), the number of dummy variables.\nAcceptable Ranges and Interpretation\nThe interpretation of GVIF1/(2×Df) follows the same guidelines as the standard VIF:\n\n\n\n\n\n\nGVIF1/(2×Df) Range\nInterpretation\n\n\n\n1\nNo correlation among predictors.\n\n\n1 – 2.5\nLow to moderate correlation — generally acceptable (typical for most well-specified models).\n\n\n2.5 – 5\nModerate to high correlation — may warrant further investigation.\n\n\n&gt; 5\nPotentially severe multicollinearity. The predictor may have a strong overlap with others, obscuring its effect on the outcome.\n\n\n\nA more conservative cutoff of 3 is sometimes used. The scaled GVIF, \\(GVIF^{1/(2·df)}\\), is designed to be comparable to the square root of the VIF, which explains the use of cutoffs like \\(\\sqrt{5}\\) (≈ 2.24) and \\(\\sqrt{10}\\) (≈ 3.16) (Nahhas 2024). Larger than \\(\\sqrt{20}\\) (≈ 4.47) is therefore the case of severe multicollinearity.\n\nvif_values &lt;- vif(fit_men_obese)\nprint(vif_values)\n#&gt;                GVIF Df GVIF^(1/(2*Df))\n#&gt; Age        8.137621  2        1.688979\n#&gt; race      16.391999  4        1.418499\n#&gt; smoking    3.435829  2        1.361469\n#&gt; education  6.381028  2        1.589361\n\nThe key values in the GVIF\\^(1/(2\\*Df)) column are all low (below 2.5). This confirms that the predictor variables are independent enough from one another and are not artificially inflating each other’s standard errors. The lack of precision in the model comes from other sources, not from multicollinearity.\nFormatting the Table\n\n# --- Use the helper function to format results from each model ---\nmen_obese_res &lt;- format_publish_output(publish(fit_men_obese),\n                                       \"Men, Obese, All Grades\")\n#&gt;   Variable              Units OddsRatio       CI.95    p-value \n#&gt;        Age              20-39       Ref                        \n#&gt;                         40-59      1.23 [0.87;1.72]   0.293928 \n#&gt;                          &gt;=60      1.17 [0.77;1.80]   0.497521 \n#&gt;       race Non-Hispanic white       Ref                        \n#&gt;            Non-Hispanic black      1.32 [1.01;1.73]   0.095724 \n#&gt;            Non-Hispanic Asian      0.28 [0.20;0.38]   0.000553 \n#&gt;                      Hispanic      1.30 [0.90;1.89]   0.223882 \n#&gt;                         Other      1.17 [0.62;2.20]   0.644324 \n#&gt;    smoking       Never smoker       Ref                        \n#&gt;                 Former smoker      1.23 [0.94;1.62]   0.198208 \n#&gt;                Current smoker      0.78 [0.58;1.04]   0.152799 \n#&gt;  education        High school       Ref                        \n#&gt;                  &lt;High school      0.59 [0.44;0.80]   0.018658 \n#&gt;                  &gt;High school      0.99 [0.74;1.32]   0.931661\nmen_obese3_res &lt;- format_publish_output(publish(fit_men_obese3),\n                                        \"Men, Class 3 Obesity\")\n#&gt;   Variable              Units OddsRatio       CI.95   p-value \n#&gt;        Age              20-39       Ref                       \n#&gt;                         40-59      0.85 [0.54;1.35]   0.52065 \n#&gt;                          &gt;=60      0.88 [0.33;2.31]   0.80102 \n#&gt;       race Non-Hispanic white       Ref                       \n#&gt;            Non-Hispanic black      1.52 [0.85;2.72]   0.21824 \n#&gt;            Non-Hispanic Asian      0.08 [0.01;0.53]   0.04797 \n#&gt;                      Hispanic      1.32 [0.57;3.06]   0.54578 \n#&gt;                         Other      0.72 [0.28;1.82]   0.51598 \n#&gt;    smoking       Never smoker       Ref                       \n#&gt;                 Former smoker      1.08 [0.63;1.85]   0.78547 \n#&gt;                Current smoker      0.92 [0.47;1.79]   0.82227 \n#&gt;  education        High school       Ref                       \n#&gt;                  &lt;High school      0.52 [0.27;1.03]   0.11800 \n#&gt;                  &gt;High school      0.93 [0.65;1.33]   0.70887\nwomen_obese_res &lt;- format_publish_output(publish(fit_women_obese), \n                                         \"Women, Obese, All Grades\")\n#&gt;   Variable              Units OddsRatio       CI.95     p-value \n#&gt;        Age              20-39       Ref                         \n#&gt;                         40-59      1.39 [1.08;1.78]   0.0486143 \n#&gt;                          &gt;=60      1.08 [0.86;1.36]   0.5252155 \n#&gt;       race Non-Hispanic white       Ref                         \n#&gt;            Non-Hispanic black      2.17 [1.88;2.50]   0.0001332 \n#&gt;            Non-Hispanic Asian      0.23 [0.16;0.35]   0.0008139 \n#&gt;                      Hispanic      1.30 [0.92;1.84]   0.2016538 \n#&gt;                         Other      0.97 [0.57;1.65]   0.9169417 \n#&gt;    smoking       Never smoker       Ref                         \n#&gt;                 Former smoker      1.33 [1.00;1.76]   0.1067193 \n#&gt;                Current smoker      0.94 [0.59;1.49]   0.7987621 \n#&gt;  education        High school       Ref                         \n#&gt;                  &lt;High school      0.96 [0.61;1.51]   0.8623856 \n#&gt;                  &gt;High school      0.68 [0.55;0.86]   0.0215212\nwomen_obese3_res &lt;- format_publish_output(publish(fit_women_obese3), \n                                          \"Women, Class 3 Obesity\")\n#&gt;   Variable              Units OddsRatio       CI.95   p-value \n#&gt;        Age              20-39       Ref                       \n#&gt;                         40-59      1.23 [0.80;1.91]   0.38944 \n#&gt;                          &gt;=60      0.57 [0.39;0.85]   0.03880 \n#&gt;       race Non-Hispanic white       Ref                       \n#&gt;            Non-Hispanic black      1.92 [1.39;2.64]   0.01054 \n#&gt;            Non-Hispanic Asian      0.02 [0.00;0.16]   0.01235 \n#&gt;                      Hispanic      1.01 [0.56;1.82]   0.98433 \n#&gt;                         Other      1.65 [0.89;3.05]   0.17137 \n#&gt;    smoking       Never smoker       Ref                       \n#&gt;                 Former smoker      1.57 [1.05;2.35]   0.07838 \n#&gt;                Current smoker      1.02 [0.78;1.33]   0.88571 \n#&gt;  education        High school       Ref                       \n#&gt;                  &lt;High school      1.25 [0.62;2.50]   0.55965 \n#&gt;                  &gt;High school      0.89 [0.59;1.35]   0.61174\n\n# --- Create a template for the final table structure ---\nfinal_table_structure &lt;- tibble(\n  Group = c(\n    \"Age, y\", \"\", \"\",\n    \"Race\", \"\", \"\", \"\", \"\",\n    \"Smoking\", \"\", \"\",\n    \"Education\", \"\", \"\"\n  ),\n  term = c(\n    \"20-39\", \"40-59\", \"&gt;=60\",\n    \"Non-Hispanic white\", \"Non-Hispanic black\", \"Non-Hispanic Asian\", \"Hispanic\", \"Other\",\n    \"Never smoker\", \"Former smoker\", \"Current smoker\",\n    \"High school\", \"&lt;High school\", \"&gt;High school\"\n  )\n)\n\n# --- Join all formatted results onto the template ---\nfinal_table &lt;- final_table_structure %&gt;%\n  left_join(men_obese_res, by = \"term\") %&gt;%\n  left_join(men_obese3_res, by = \"term\") %&gt;%\n  left_join(women_obese_res, by = \"term\") %&gt;%\n  left_join(women_obese3_res, by = \"term\")\n\n# --- Display the final, publication-quality table ---\nkable(final_table, caption = \"Weighted Logistic Regression Models for Obesity\", \n      col.names = c(\"\", \"\", \n                    \"Men, Obese, All Grades\", \n                    \"Men, Class 3 Obesity\", \n                    \"Women, Obese, All Grades\", \n                    \"Women, Class 3 Obesity\"))\n\n\nWeighted Logistic Regression Models for Obesity\n\n\n\n\n\n\n\n\n\n\n\nMen, Obese, All Grades\nMen, Class 3 Obesity\nWomen, Obese, All Grades\nWomen, Class 3 Obesity\n\n\n\nAge, y\n20-39\n1 [Reference]\n1 [Reference]\n1 [Reference]\n1 [Reference]\n\n\n\n40-59\n1.23 (0.87-1.72)\n0.85 (0.54-1.35)\n1.39 (1.08-1.78)\n1.23 (0.80-1.91)\n\n\n\n&gt;=60\n1.17 (0.77-1.80)\n0.88 (0.33-2.31)\n1.08 (0.86-1.36)\n0.57 (0.39-0.85)\n\n\nRace\nNon-Hispanic white\n1 [Reference]\n1 [Reference]\n1 [Reference]\n1 [Reference]\n\n\n\nNon-Hispanic black\n1.32 (1.01-1.73)\n1.52 (0.85-2.72)\n2.17 (1.88-2.50)\n1.92 (1.39-2.64)\n\n\n\nNon-Hispanic Asian\n0.28 (0.20-0.38)\n0.08 (0.01-0.53)\n0.23 (0.16-0.35)\n0.02 (0.00-0.16)\n\n\n\nHispanic\n1.30 (0.90-1.89)\n1.32 (0.57-3.06)\n1.30 (0.92-1.84)\n1.01 (0.56-1.82)\n\n\n\nOther\n1.17 (0.62-2.20)\n0.72 (0.28-1.82)\n0.97 (0.57-1.65)\n1.65 (0.89-3.05)\n\n\nSmoking\nNever smoker\n1 [Reference]\n1 [Reference]\n1 [Reference]\n1 [Reference]\n\n\n\nFormer smoker\n1.23 (0.94-1.62)\n1.08 (0.63-1.85)\n1.33 (1.00-1.76)\n1.57 (1.05-2.35)\n\n\n\nCurrent smoker\n0.78 (0.58-1.04)\n0.92 (0.47-1.79)\n0.94 (0.59-1.49)\n1.02 (0.78-1.33)\n\n\nEducation\nHigh school\n1 [Reference]\n1 [Reference]\n1 [Reference]\n1 [Reference]\n\n\n\n&lt;High school\n0.59 (0.44-0.80)\n0.52 (0.27-1.03)\n0.96 (0.61-1.51)\n1.25 (0.62-2.50)\n\n\n\n&gt;High school\n0.99 (0.74-1.32)\n0.93 (0.65-1.33)\n0.68 (0.55-0.86)\n0.89 (0.59-1.35)\n\n\n\n\n\nDifferences from the Original Paper\nWhile the ‘Statistical Analyses’ section of Flegal et al. (2016) details their models, it does not explicitly state the method used to handle missing data for covariates. Our replication employs a complete-case analysis, which excludes participants with missing smoking or education data from the models. This difference is the most likely reason for the minor discrepancies between our results and those published in the original paper.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Reliability Standards"
    ]
  },
  {
    "objectID": "surveydata9.html#references",
    "href": "surveydata9.html#references",
    "title": "NHANES: Reliability Standards",
    "section": "References",
    "text": "References\n\n\n\n\nDisease Control, Centers for, and Prevention. 2025. “NHANES Tutorials: Reliability of Estimates Module.” National Center for Health Statistics. https://wwwn.cdc.gov/nchs/nhanes/tutorials/reliabilityofestimates.aspx.\n\n\nFlegal, Katherine M, Deanna Kruszon-Moran, Margaret D Carroll, Cheryl D Fryar, and Cynthia L Ogden. 2016. “Trends in Obesity Among Adults in the United States, 2005 to 2014.” Jama 315 (21): 2284–91.\n\n\nFox, John, and Georges Monette. 1992. “Generalized Collinearity Diagnostics.” Journal of the American Statistical Association 87 (417): 178–83.\n\n\nKarim, Mohammad Ehsanul. 2025. “svyTable1: Table 1 and Diagnostics from Complex Survey Designs.” https://github.com/ehsanx/svyTable1.\n\n\nNahhas, Ramzi W. 2024. Introduction to Regression Methods for Public Health Using r. CRC Press.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Reliability Standards"
    ]
  },
  {
    "objectID": "surveydata9c.html",
    "href": "surveydata9c.html",
    "title": "NHANES: Performance",
    "section": "",
    "text": "1. Data Preparation ⚙️\nThis tutorial outlines how to evaluate the performance of a logistic regression model fitted to complex survey data using R, focusing on the NHANES dataset.\nWe will assess the model’s predictive accuracy using a design-correct Area Under the Curve (AUC) and evaluate its fit with the Archer-Lemeshow test.\nFirst, we need to load the necessary R packages and prepare the NHANES data. The code below loads data from the Flegal2016.RData file, recodes variables like age, race, smoking, and education, and creates the final survey design object (analytic_design) for our analysis.\n# --- Load Libraries ---\nlibrary(survey)\nlibrary(dplyr)\n# install.packages(\"devtools\")\n# devtools::install_github(\"ehsanx/svyTable1\", \n#                          build_vignettes = TRUE, \n#                          dependencies = TRUE)\nlibrary(svyTable1)\nlibrary(ggplot2)\n\n# --- Load Data ---\nload(\"Data/surveydata/Flegal2016.RData\") \n\n# --- Create Analytic Sample (N=5,455) ---\ndat.analytic &lt;- subset(dat.full, RIDAGEYR &gt;= 20)\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI))\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \"Yes, positive lab pregnancy test\")\n\n# --- Recode Analysis Variables ---\ndat.full &lt;- dat.full %&gt;%\n  mutate(\n    Age = cut(RIDAGEYR,\n              breaks = c(20, 40, 60, Inf),\n              right = FALSE,\n              labels = c(\"20-39\", \"40-59\", \"&gt;=60\")),\n\n    race = case_when(\n      RIDRETH3 == \"Non-Hispanic White\" ~ \"Non-Hispanic white\",\n      RIDRETH3 == \"Non-Hispanic Black\" ~ \"Non-Hispanic black\",\n      RIDRETH3 == \"Non-Hispanic Asian\" ~ \"Non-Hispanic Asian\",\n      RIDRETH3 %in% c(\"Mexican American\", \"Other Hispanic\") ~ \"Hispanic\",\n      TRUE ~ \"Other\" # Consolidating \"Other Race\" for simplicity\n    ),\n    race = factor(race, levels = c(\"Non-Hispanic white\", \"Non-Hispanic black\", \"Non-Hispanic Asian\", \"Hispanic\", \"Other\")),\n\n    obese = factor(ifelse(BMXBMI &gt;= 30, \"Yes\", \"No\"), levels = c(\"No\", \"Yes\")),\n\n    smoking = case_when(\n      SMQ020 == \"No\" ~ \"Never smoker\",\n      SMQ020 == \"Yes\" & SMQ040 == \"Not at all\" ~ \"Former smoker\",\n      SMQ020 == \"Yes\" ~ \"Current smoker\",\n      TRUE ~ NA_character_\n    ),\n    smoking = factor(smoking, levels = c(\"Never smoker\", \"Former smoker\", \"Current smoker\")),\n\n    education = case_when(\n        DMDEDUC2 %in% c(\"Less than 9th grade\", \"9-11th grade (Includes 12th grad)\") ~ \"&lt;High school\",\n        DMDEDUC2 == \"High school graduate/GED or equi\" ~ \"High school\",\n        DMDEDUC2 %in% c(\"Some college or AA degree\", \"College graduate or above\") ~ \"&gt;High school\",\n        TRUE ~ NA_character_\n    ),\n    education = factor(education, levels = c(\"High school\", \"&lt;High school\", \"&gt;High school\"))\n  )\n\n# --- Create Survey Design Object ---\noptions(survey.lonely.psu = \"adjust\")\nsvy.design.full &lt;- svydesign(id = ~SDMVPSU,\n                             strata = ~SDMVSTRA,\n                             weights = ~WTINT2YR,\n                             nest = TRUE,\n                             data = dat.full)\nanalytic_design &lt;- subset(svy.design.full, SEQN %in% dat.analytic$SEQN)",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Performance"
    ]
  },
  {
    "objectID": "surveydata9c.html#fitting-a-logistic-regression-model",
    "href": "surveydata9c.html#fitting-a-logistic-regression-model",
    "title": "NHANES: Performance",
    "section": "2. Fitting a Logistic Regression Model 📊",
    "text": "2. Fitting a Logistic Regression Model 📊\nNext, we fit a survey-weighted logistic regression model using svyglm(). Our goal is to predict obesity status based on Age, race, smoking status, and education level.\n\n# Fit the survey-weighted logistic regression model\nfit_obesity &lt;- svyglm(\n  I(obese == \"Yes\") ~ Age + race + smoking + education,\n  design = analytic_design,\n  family = quasibinomial()\n)",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Performance"
    ]
  },
  {
    "objectID": "surveydata9c.html#model-performance-roc-curve-and-auc",
    "href": "surveydata9c.html#model-performance-roc-curve-and-auc",
    "title": "NHANES: Performance",
    "section": "3. Model Performance: ROC Curve and AUC 📈",
    "text": "3. Model Performance: ROC Curve and AUC 📈\nWe calculate the Area Under the ROC Curve (AUC) to measure predictive performance. The svyAUC() function from svyTable1 computes a design-corrected AUC and confidence interval, which requires a replicate-weights survey design.\nCreate Replicate Design & Refit Model\n\n# svyAUC() requires a replicate-weights design object\nrep_design &lt;- as.svrepdesign(analytic_design)\n# Restrict the replicate design to complete cases\nvars_for_model &lt;- c(\"obese\", \"Age\", \"race\", \"smoking\", \"education\")\nrep_design_complete &lt;- subset(rep_design, \n                              complete.cases(rep_design$variables[, vars_for_model]))\n\n# Refit model on the complete-case replicate design\nfit_obesity_rep &lt;- svyglm(\n  I(obese == \"Yes\") ~ Age + race + smoking + education,\n  design = rep_design_complete,\n  family = quasibinomial()\n)\n\nCalculate AUC\n\n# Calculate the design-correct AUC\nauc_results_list &lt;- svyAUC(fit_obesity_rep, rep_design_complete, plot = TRUE)\n\n\n\n\n\n\n\n# Display the AUC summary\nknitr::kable(auc_results_list$summary)\n\n\n\nAUC\nSE\nCI_Lower\nCI_Upper\n\n\n0.5967987\n0.0108604\n0.5755123\n0.618085\n\n\n\n\nInterpretation: An AUC of 0.5967987 indicates poor discrimination. While better than simpler models, it suggests that the predictors do not have strong predictive power for obesity in this dataset.\nVisualize the ROC Curve\n\nggplot(auc_results_list$roc_data, aes(x = FPR, y = TPR)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_abline(linetype = \"dashed\") +\n  labs(\n    title = \"Survey-Weighted ROC Curve\",\n    subtitle = paste0(\"AUC = \", round(auc_results_list$summary$AUC, 3)),\n    x = \"1 - Specificity (FPR)\",\n    y = \"Sensitivity (TPR)\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Performance"
    ]
  },
  {
    "objectID": "surveydata9c.html#goodness-of-fit-archer-and-lemeshow-test",
    "href": "surveydata9c.html#goodness-of-fit-archer-and-lemeshow-test",
    "title": "NHANES: Performance",
    "section": "4. Goodness-of-Fit: Archer and Lemeshow Test ✅",
    "text": "4. Goodness-of-Fit: Archer and Lemeshow Test ✅\nWe assess model fit using the Archer-Lemeshow test. A non-significant p-value (p &gt; 0.05) suggests a good model fit.\n\n# Run the goodness-of-fit test\ngof_results &lt;- svygof(fit_obesity, analytic_design)\n\n# Display the results\nknitr::kable(gof_results)\n\n\n\nF_statistic\ndf1\ndf2\np_value\n\n\n2.168827\n9\n6\n0.1791469\n\n\n\n\nInterpretation: The p-value (0.1791469) is greater than 0.05, indicating no evidence of a poor fit. The model’s predictions align well with observed outcomes, suggesting good calibration.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Performance"
    ]
  },
  {
    "objectID": "surveydata9c.html#summary",
    "href": "surveydata9c.html#summary",
    "title": "NHANES: Performance",
    "section": "Summary",
    "text": "Summary\n\nThe AUC shows poor discriminatory power, meaning the model does not strongly distinguish between obese and non-obese individuals.\nThe Archer-Lemeshow test indicates good model fit (p &gt; 0.05).\n\nConclusion: While well-calibrated, the model’s low predictive power implies other factors (e.g., cholesterol, blood pressure, diet) may better explain obesity risk.",
    "crumbs": [
      "Survey data analysis",
      "NHANES: Performance"
    ]
  },
  {
    "objectID": "surveydataF.html",
    "href": "surveydataF.html",
    "title": "R functions (D)",
    "section": "",
    "text": "The list of new R functions introduced in this Survey data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nAIC\nbase/stats\nTo extract the AIC value of a model\n\n\nas.character\nbase\nTo create a character vector\n\n\nas.numeric\nbase\nTo create a numeric vector\n\n\neval\nbase\nTo evaluate an expression\n\n\nfitted\nbase/stats\nTo extract fitted values of a model\n\n\nls\nbase\nTo see the list of objects\n\n\npsrsq\nsurvey\nTo compute the Nagelkerke and Cox-Snell pseudo R-squared statistics for survey data\n\n\nregTermTest\nsurvey\nTo test for an additional variable in a regression model\n\n\nresiduals\nbase/stats\nTo extract residuals of a model\n\n\nstepAIC\nMASS\nTo choose a model by stepwise AIC\n\n\nstep\nbase/stats\nTo choose a model by stepwise AIC but it can keep the pre-specified variables in the model\n\n\nsumm\njtools\nTo show/publish regression tables\n\n\nsvyboxplot\nsurvey\nTo produce a box plot for survey data\n\n\nsvyby\nsurvey\nTo see the summary statistics for a survey design\n\n\nsvychisq\nsurvey\nTo test the bivariate assocaition between two categorical variables for survey data\n\n\nsvyCreateTableOne\ntableone\nTo create a frequency table with a survey design\n\n\nsvydesign\nsurvey\nTo create a design for the survey data analysis\n\n\nsvyglm\nsurvey\nTo run design-adjusted generalized linear models\n\n\nupdate\nbase/stats\nTo update and re-fit a regression model",
    "crumbs": [
      "Survey data analysis",
      "R functions (D)"
    ]
  },
  {
    "objectID": "surveydataQ.html",
    "href": "surveydataQ.html",
    "title": "Quiz (D)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Survey data analysis",
      "Quiz (D)"
    ]
  },
  {
    "objectID": "surveydataQ.html#live-quiz",
    "href": "surveydataQ.html#live-quiz",
    "title": "Quiz (D)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Survey data analysis",
      "Quiz (D)"
    ]
  },
  {
    "objectID": "surveydataQ.html#download-quiz",
    "href": "surveydataQ.html#download-quiz",
    "title": "Quiz (D)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Survey data analysis",
      "Quiz (D)"
    ]
  },
  {
    "objectID": "surveydataS.html",
    "href": "surveydataS.html",
    "title": "App (D)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can choose variables for bivariate analysis, and decide whether to apply survey features or not.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveD\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, survey and publish packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Survey data analysis",
      "App (D)"
    ]
  },
  {
    "objectID": "surveydataE.html",
    "href": "surveydataE.html",
    "title": "Exercise 1 (D)",
    "section": "",
    "text": "Problem Statement\nWe will revisit the article by Flegal et al. (2016). Our primary aim this time is to execute the survey data analysis more rigorously, specifically by incorporating essential survey features into our analysis.\nWe will reproduce some results from the article. The authors used NHANES 2013-14 dataset to create their main analytic dataset. The dataset contains 10,175 subjects with 12 relevant variables:",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 (D)"
    ]
  },
  {
    "objectID": "surveydataE.html#problem-statement",
    "href": "surveydataE.html#problem-statement",
    "title": "Exercise 1 (D)",
    "section": "",
    "text": "This is the same article that we discussed in our data access chapter!\n\n\nSEQN: Respondent sequence number\nRIDAGEYR: Age in years at screening\nRIAGENDR: Gender\nDMDEDUC2: Education level\nRIDRETH3: Race/ethnicity\nRIDEXPRG: Pregnancy status at exam\nWTINT2YR: Full sample 2 year weights\nSDMVPSU: Masked variance pseudo-PSU\nSDMVSTRA: Masked variance pseudo-stratum\nBMXBMI: Body mass index in kg/m**2\nSMQ020: Whether smoked at least 100 cigarettes in life\nSMQ040: Current status of smoking (Do you now smoke cigarettes?)",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 (D)"
    ]
  },
  {
    "objectID": "surveydataE.html#question-1-creating-data-and-table",
    "href": "surveydataE.html#question-1-creating-data-and-table",
    "title": "Exercise 1 (D)",
    "section": "Question 1: Creating data and table",
    "text": "Question 1: Creating data and table\n1(a) Importing dataset\n\n# you have to download the data in the same folder\nload(\"Data/surveydata/Flegal2016.RData\")\nls()\n#&gt; [1] \"dat.full\"\nnames(dat.full)\n#&gt;  [1] \"SEQN\"     \"RIDAGEYR\" \"RIAGENDR\" \"DMDEDUC2\" \"RIDRETH3\" \"RIDEXPRG\"\n#&gt;  [7] \"WTINT2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BMXBMI\"   \"SMQ020\"   \"SMQ040\"\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria described in the second paragraph of the Methods section.\n\nHint: The authors restricted their study to\n\nadults aged 20 years and more,\nnon-missing body mass index, and\nnon-pregnant.\n\n\n\nYour analytic sample size should be 5,455, as described in the first sentence in the Results section.\n\n# 20+\ndat.analytic &lt;- subset(dat.full, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\n\ndim(dat.analytic)\n#&gt; [1] 5455   12\n\n1(c) Reproduce Table 1\nReproduce Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Please be advised to order the categories as shown in the table. tableone package could be helpful.\nHint 2: the authors did not show the results for the Other race category. But in your table, you could include all race categories.\n\n\nlibrary(tableone)\n\ndat &lt;- dat.analytic\n\n# Age\ndat$age &lt;- cut(dat$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat$gender &lt;- dat$RIAGENDR\n\n# Race/Hispanic origin group\ndat$race &lt;- dat$RIDRETH3\ndat$race &lt;- car::recode(dat$race, \" 'Non-Hispanic White'='White'; 'Non-Hispanic Black'=\n                        'Black'; 'Non-Hispanic Asian'='Asian'; c('Mexican American',\n                        'Other Hispanic')='Hispanic'; 'Other Race - Including Multi-Rac'=\n                        'Other'; else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                      \"Hispanic\", \"Other\"))\n\n# Table 1: Overall \ntab11 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", data = dat, test = F, \n                        addOverall = T)\n\n# Table 1: Male\ntab12 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(dat, gender == \"Male\"))\n\n# Table 1: Female\ntab13 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(dat, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1a &lt;- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1a, format = \"f\") # Showing only frequencies \n#&gt; $Overall\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           5455    2343  1115  623   1214     160  \n#&gt;   age                                                 \n#&gt;      [20,40)  1810     734   362  216    412      86  \n#&gt;      [40,60)  1896     759   383  251    449      54  \n#&gt;      [60,Inf) 1749     850   370  156    353      20  \n#&gt; \n#&gt; $Male\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           2638    1130  556   300   573      79   \n#&gt;   age                                                 \n#&gt;      [20,40)   909     386  182   106   189      46   \n#&gt;      [40,60)   897     360  179   120   215      23   \n#&gt;      [60,Inf)  832     384  195    74   169      10   \n#&gt; \n#&gt; $Female\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           2817    1213  559   323   641      81   \n#&gt;   age                                                 \n#&gt;      [20,40)   901     348  180   110   223      40   \n#&gt;      [40,60)   999     399  204   131   234      31   \n#&gt;      [60,Inf)  917     466  175    82   184      10",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 (D)"
    ]
  },
  {
    "objectID": "surveydataE.html#question-2",
    "href": "surveydataE.html#question-2",
    "title": "Exercise 1 (D)",
    "section": "Question 2",
    "text": "Question 2\n2(a) Reproduce Table 1 with survey features [15% grade]\nNot in this article but in many other articles, you would see n comes from the analytic sample and % comes from the survey design that accounts for survey features such as strata, clusters and survey weights. In Question 1, you see how n comes from the analytic sample. Your task for Question 2(a) is to create % part of the Table 1 with survey features, i.e., % should come from the survey design that accounts for strata, clusters and survey weights.\n\nHint 1: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 2: Generate age, gender, and race variable in your full data (codes shown in Question 1 could be helpful).\nHint 3: Subset the design.\nHint 4: Reproduce Table 1 with the design. svyCreateTableOne could be a helpful function.\n\n\n## Create all variables in the full data\n# Age\ndat.full$age &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ndat.full$race &lt;- car::recode(dat.full$race, \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black'='Black'; 'Non-Hispanic Asian'='Asian'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             'Other Race - Including Multi-Rac'='Other'; \n                             else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                  \"Hispanic\", \"Other\"))\n\n## Subset the design\n# your codes here\n\n\n## Table 1 \n# your codes here\n\n#print(tab1b, format = \"p\") # Showing only percentages  \n\n2(b) Reproduce Table 3 [50% grade]\nReproduce the first column of Table 3 of the article (i.e., among men, explore the relationship between obesity and four predictors shown in the table).\n\nHint 1: If necessary, re-level or re-order the levels. Use Publish package to report the estimates.\nHint 2: Subset the design, not the sample. If you have generated a variable in your analytic dataset (based on eligibility), that variable should also be present in the full dataset.\nHint 3: The authors used SAS to produce the results vs. We are using R. The estimates could be slightly different (in second decimal point) from the estimates presented in Table 3, but they should be approximately similar.\nHint 4: You need to generate two variables, smoking status and education. The unweighted frequencies should be matched with the frequencies in eTable 1 and eTable 2.\n\nYour odds ratios could be look like as follows:\n\n\n\n\n\n\n\n\n\n## Recode Obese, Smoking status, Education - work on full data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Reproduce Table 3 - column 1\n# your codes here\n\n2(c) Model selection [25% grade]\nFrom the literature, you know that age and race needs to be adjusted in the model, but you are not sure about smoking and education. Run an AIC based backward selection process to figure out whether you want to add smoking or education, or both in the final model in 2(b). What is your conclusion [Expected answer: one short sentence]?\n\nHint 1: You need to make sure your design (that is based on eligibility) is free from missing values. Even after applying eligibility criteria, you may have some missing values on multiple variables (see eTable 1 and eTable 2). This is especially important for model selection process.\nHint 2: Work with the analytic data, keep only the relevant variables, and then remove missing values. Finally, subset the design and then select your final model.\n\n\n## Recode Obese, Smoking status, Education - work on analytic data\n# your codes here\n\n\n## Remove missing values - work on analytic data\n# your codes here\n\n\n## Set up the survey design\n# your codes here\n\n\n## Model selection \n# your codes here\n\n2(d) Testing for interactions [10% grade]\nCheck whether the interaction between age and smoking should be added in the 2(b) model (yes or no answer required, along with the code and p-value):\n\n# your codes here",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 (D)"
    ]
  },
  {
    "objectID": "surveydataEsolution.html",
    "href": "surveydataEsolution.html",
    "title": "Exercise 1 Solution (D)",
    "section": "",
    "text": "Question 1: Creating data and table\nWe will use the article. We will use the following article by Flegal et al. (2016).\nWe will reproduce some results from the article. The authors used NHANES 2013-14 dataset to create their main analytic dataset. The dataset contains 10,175 subjects with 12 relevant variables:",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 Solution (D)"
    ]
  },
  {
    "objectID": "surveydataEsolution.html#question-1-creating-data-and-table",
    "href": "surveydataEsolution.html#question-1-creating-data-and-table",
    "title": "Exercise 1 Solution (D)",
    "section": "",
    "text": "1(a) Importing dataset\n\n# load the dataset\nload(\"Data/surveydata/Flegal2016.RData\")\nls()\n#&gt; [1] \"dat.full\"\nnames(dat.full)\n#&gt;  [1] \"SEQN\"     \"RIDAGEYR\" \"RIAGENDR\" \"DMDEDUC2\" \"RIDRETH3\" \"RIDEXPRG\"\n#&gt;  [7] \"WTINT2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"BMXBMI\"   \"SMQ020\"   \"SMQ040\"\n\n1(b) Subsetting according to eligibility\nSubset the dataset according to the eligibility criteria described in the second paragraph of the Methods section.\n\nHint: The authors restricted their study to\n\nadults aged 20 years and more,\nnon-missing body mass index, and\nnon-pregnant.\n\n\n\nYour analytic sample size should be 5,455, as described in the first sentence in the Results section.\n\n# 20+\ndat.analytic &lt;- subset(dat.full, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\n\ndim(dat.analytic)\n#&gt; [1] 5455   12\n\n1(c) Reproduce Table 1\nReproduce Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Please be advised to order the categories as shown in the table. tableone package could be helpful.\nHint 2: the authors did not show the results for the Other race category. But in your table, you could include all race categories.\n\n\nlibrary(tableone)\n\ndat &lt;- dat.analytic\n\n# Age\ndat$age &lt;- cut(dat$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat$gender &lt;- dat$RIAGENDR\n\n# Race/Hispanic origin group\ndat$race &lt;- dat$RIDRETH3\ndat$race &lt;- car::recode(dat$race, \" 'Non-Hispanic White'='White'; 'Non-Hispanic Black'=\n                        'Black'; 'Non-Hispanic Asian'='Asian'; c('Mexican American',\n                        'Other Hispanic')='Hispanic'; 'Other Race - Including Multi-Rac'=\n                        'Other'; else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                      \"Hispanic\", \"Other\"))\n\n# Table 1: Overall \ntab11 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", data = dat, test = F, \n                        addOverall = T)\n\n# Table 1: Male\ntab12 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(dat, gender == \"Male\"))\n\n# Table 1: Female\ntab13 &lt;- CreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(dat, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1a &lt;- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1a, format = \"f\") # Showing only frequencies \n#&gt; $Overall\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           5455    2343  1115  623   1214     160  \n#&gt;   age                                                 \n#&gt;      [20,40)  1810     734   362  216    412      86  \n#&gt;      [40,60)  1896     759   383  251    449      54  \n#&gt;      [60,Inf) 1749     850   370  156    353      20  \n#&gt; \n#&gt; $Male\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           2638    1130  556   300   573      79   \n#&gt;   age                                                 \n#&gt;      [20,40)   909     386  182   106   189      46   \n#&gt;      [40,60)   897     360  179   120   215      23   \n#&gt;      [60,Inf)  832     384  195    74   169      10   \n#&gt; \n#&gt; $Female\n#&gt;              Stratified by race\n#&gt;               Overall White Black Asian Hispanic Other\n#&gt;   n           2817    1213  559   323   641      81   \n#&gt;   age                                                 \n#&gt;      [20,40)   901     348  180   110   223      40   \n#&gt;      [40,60)   999     399  204   131   234      31   \n#&gt;      [60,Inf)  917     466  175    82   184      10",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 Solution (D)"
    ]
  },
  {
    "objectID": "surveydataEsolution.html#question-2",
    "href": "surveydataEsolution.html#question-2",
    "title": "Exercise 1 Solution (D)",
    "section": "Question 2",
    "text": "Question 2\n2(a) Reproduce Table 1 with survey features\nNot in this article but in many other articles, you would see n comes from the analytic sample and % comes from the survey design that accounts for survey features such as strata, clusters and survey weights. In Question 1, you see how n comes from the analytic sample. Your task for Question 2(a) is to create % part of the Table 1 with survey features, i.e., % should come from the survey design that accounts for strata, clusters and survey weights. You do not need to show the frequiencis but show only the percentages (for categorical variables).\nHints:\n\nSubset the design, not the sample. For this step, you need to work with your full data. If you have generated a variable in your analytic dataset, that variable should also be present in the full dataset.\nGenerate age, gender, and race variable in your full data. Codes shown in Question 1 could be helpful.\nMake the design on the full data and then subset the design.\nReproduce Table 1 with the design from the previous step. The svyCreateTableOne function could be a helpful function.\n\n\n## Create all variables in the full data\n# Age\ndat.full$age &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ndat.full$race &lt;- car::recode(dat.full$race, \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black'='Black'; 'Non-Hispanic Asian'='Asian'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             'Other Race - Including Multi-Rac'='Other'; \n                             else=NA\", levels = c(\"White\", \"Black\", \"Asian\",\n                                                  \"Hispanic\", \"Other\"))\n\n# Survey features\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Subset the design\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Table 1: Overall \ntab11 &lt;- svyCreateTableOne(vars = \"age\", strata = \"race\", data = svy.design, \n                           test = F, addOverall = T)\n\n# Table 1: Male\ntab12 &lt;- svyCreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T, \n                        data = subset(svy.design, gender == \"Male\"))\n\n# Table 1: Female\ntab13 &lt;- svyCreateTableOne(vars = \"age\", strata = \"race\", test = F, addOverall = T,\n                        data = subset(svy.design, gender == \"Female\"))\n\n# Reproducing Table 1\ntab1b &lt;- list(Overall = tab11, Male = tab12, Female = tab13)\nprint(tab1b, format = \"p\") # Showing only percentages    \n#&gt; $Overall\n#&gt;              Stratified by race\n#&gt;               Overall     White       Black      Asian      Hispanic  \n#&gt;   n           217464332.1 143721046.2 24697511.9 11380679.8 31863752.6\n#&gt;   age (%)                                                             \n#&gt;      [20,40)         35.5        30.8       38.9       40.2       49.6\n#&gt;      [40,60)         37.5        37.5       39.3       38.5       35.6\n#&gt;      [60,Inf)        27.0        31.7       21.8       21.2       14.8\n#&gt;              Stratified by race\n#&gt;               Other    \n#&gt;   n           5801341.5\n#&gt;   age (%)              \n#&gt;      [20,40)       49.2\n#&gt;      [40,60)       38.4\n#&gt;      [60,Inf)      12.4\n#&gt; \n#&gt; $Male\n#&gt;              Stratified by race\n#&gt;               Overall     White      Black      Asian     Hispanic   Other    \n#&gt;   n           105744090.8 70126957.2 11285409.5 5233819.5 15947523.0 3150381.6\n#&gt;   age (%)                                                                     \n#&gt;      [20,40)         37.2       32.3       41.3      41.7       51.6      52.9\n#&gt;      [40,60)         37.6       37.9       39.0      38.5       35.1      36.1\n#&gt;      [60,Inf)        25.1       29.8       19.7      19.8       13.2      11.0\n#&gt; \n#&gt; $Female\n#&gt;              Stratified by race\n#&gt;               Overall     White      Black      Asian     Hispanic   Other    \n#&gt;   n           111720241.3 73594089.0 13412102.4 6146860.3 15916229.6 2650960.0\n#&gt;   age (%)                                                                     \n#&gt;      [20,40)         33.8       29.4       36.9      39.0       47.6      44.7\n#&gt;      [40,60)         37.4       37.1       39.5      38.6       36.0      41.2\n#&gt;      [60,Inf)        28.8       33.5       23.6      22.5       16.4      14.1\n\n2(b) Reproduce Table 3\nReproduce the first column of Table 3 of the article (i.e., among men, explore the relationship between obesity and four predictors shown in the table).\n\nIf necessary, re-level or re-order the levels.\nYou need to generate obesity as BMI \\(\\ge 30 \\text{ kg/m}^2\\)\nYou need to generate smoking status and education. The unweighted frequencies should be matched with the frequencies in eTable 1 and eTable 2. Make sure these variables are in your full dataset as well.\nSubset the design, not the sample.\nFit the model. Do not need to report the model summary.\nThe authors used SAS to produce the results vs. We are using R. The estimates could be slightly different (in second decimal point) from the estimates presented in Table 3, but they should be approximately similar.\nYou can use Publish or jtools package to report the odds ratios. Your odds ratios could be look like as follows:\n\n\n\n\n\n\n\n\n\n\n# Obese\ndat.full$obese &lt;- ifelse(dat.full$BMXBMI &gt;= 30, \"Yes\", \"No\")\n\n# Smoking status\ndat.full$smoking &lt;- dat.full$SMQ020\ndat.full$smoking &lt;- car::recode(dat.full$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA\",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat.full$smoking[dat.full$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\n\n# Education\ndat.full$education &lt;- dat.full$DMDEDUC2\ndat.full$education &lt;- car::recode(dat.full$education, \" c('Some college or AA degree', \n                             'College graduate or above') = '&gt;High school';\n                             'High school graduate/GED or equi' = 'High school';\n                             c('Less than 9th grade',\n                             '9-11th grade (Includes 12th grad') = '&lt;High school';\n                             else = NA\", levels = c(\"High school\", \"&lt;High school\",\n                                                    \"&gt;High school\"))\n\n# Survey features\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Subset the design\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Table 3 - column 1\nfit.male &lt;- svyglm(I(obese==\"Yes\") ~ age + race + smoking + education, \n                   family = binomial, design = subset(svy.design, gender == \"Male\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.male)\n#&gt;   Variable          Units OddsRatio       CI.95    p-value \n#&gt;        age        [20,40)       Ref                        \n#&gt;                   [40,60)      1.28 [0.94;1.74]   0.173590 \n#&gt;                  [60,Inf)      1.20 [0.77;1.85]   0.458803 \n#&gt;       race          White       Ref                        \n#&gt;                     Black      1.24 [0.93;1.65]   0.202249 \n#&gt;                     Asian      0.27 [0.20;0.37]   0.000345 \n#&gt;                  Hispanic      1.22 [0.88;1.69]   0.293132 \n#&gt;                     Other      1.23 [0.67;2.23]   0.532168 \n#&gt;    smoking   Never smoker       Ref                        \n#&gt;             Former smoker      1.25 [0.96;1.64]   0.157176 \n#&gt;            Current smoker      0.71 [0.55;0.92]   0.047198 \n#&gt;  education    High school       Ref                        \n#&gt;              &lt;High school      0.93 [0.67;1.29]   0.685577 \n#&gt;              &gt;High school      0.97 [0.72;1.29]   0.821357\n\n2(c) Model selection\nFrom the literature, you know that age and race needs to be adjusted in the model, but you are not sure about smoking and education. Run an AIC based backward selection process to figure out whether you want to add smoking or education, or both in the final model in 2(b). What is your conclusion, i.e., which variables are selected/dropped [Expected answer: one short sentence]?\nHints:\n\nYour design must be free from missing values. Even after applying eligibility criteria, you may have some missing values on multiple variables (see eTable 1 and eTable 2). This is especially important for model selection process.\nYou can create a complete case analytic dataset (i.e., dataset without missing values in obesity, four predictors, and survey features). Then create the design on the full data and subset the design for the complete case samples.\nstep function could be helpful.\n\n\n# Obese\ndat$obese &lt;- ifelse(dat$BMXBMI &gt;= 30, \"Yes\", \"No\")\ndat$obese &lt;- factor(dat$obese, levels = c(\"No\", \"Yes\"))\n\n# Smoking status\ndat$smoking &lt;- dat$SMQ020\ndat$smoking &lt;- car::recode(dat$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA\",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat$smoking[dat$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\n\n# Education\ndat$education &lt;- dat$DMDEDUC2\ndat$education &lt;- car::recode(dat$education, \" c('Some college or AA degree', \n                             'College graduate or above') = '&gt;High school';\n                             'High school graduate/GED or equi' = 'High school';\n                             c('Less than 9th grade',\n                             '9-11th grade (Includes 12th grad') = '&lt;High school';\n                             else = NA\", levels = c(\"High school\", \"&lt;High school\",\n                                                    \"&gt;High school\"))\n\n# Survey design\ndat$survey.weight &lt;- dat$WTINT2YR\ndat$psu &lt;- dat$SDMVPSU\ndat$strata &lt;- dat$SDMVSTRA\n\n# Select only relevant variables\ndat2 &lt;- dat[,c(\"SEQN\", \"age\", \"race\", \"smoking\", \"education\", \n                         \"survey.weight\", \"psu\", \"strata\")]\n\n# Remove missing values\ndat2 &lt;- dat2[complete.cases(dat2),] # 5 missing values dropped\n\n# Subset the design\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$SEQN %in% dat2$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Initial model\nfit1 &lt;- svyglm(I(obese==\"Yes\") ~ age + race + smoking + education, \n                   family = binomial, design = subset(svy.design, gender == \"Male\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nscope &lt;- list(upper = ~ age + race + smoking + education, \n              lower = ~ age + race)\n\n# Final model: Backward elimination using the AIC criteria\nfit2 &lt;- step(fit1, scope = scope, trace = FALSE, k = 2, direction = \"backward\")\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# Summary of the final model\npublish(fit2)\n#&gt;  Variable          Units OddsRatio       CI.95   p-value \n#&gt;       age        [20,40)       Ref                       \n#&gt;                  [40,60)      1.28 [0.94;1.73]   0.15579 \n#&gt;                 [60,Inf)      1.19 [0.76;1.86]   0.46397 \n#&gt;      race          White       Ref                       \n#&gt;                    Black      1.23 [0.92;1.65]   0.19682 \n#&gt;                    Asian      0.27 [0.20;0.37]   &lt; 1e-04 \n#&gt;                 Hispanic      1.21 [0.90;1.61]   0.24507 \n#&gt;                    Other      1.23 [0.68;2.23]   0.52048 \n#&gt;   smoking   Never smoker       Ref                       \n#&gt;            Former smoker      1.25 [0.96;1.64]   0.14296 \n#&gt;           Current smoker      0.71 [0.54;0.92]   0.03895\n\nThe education variable is dropped from the final model.\n2(d) Testing for interactions\nCheck whether the interaction between age and smoking should be added in the 2(b) model (yes or no answer required, along with the code and p-value):\n\n# Model with interaction between age and smoking\nfit2 &lt;- update(fit.male, .~. + age:smoking)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nanova(fit.male, fit2)$p\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; [1] 0.404668\n\nNo. We won’t keep the age and smoking interaction.",
    "crumbs": [
      "Survey data analysis",
      "Exercise 1 Solution (D)"
    ]
  },
  {
    "objectID": "missingdata.html",
    "href": "missingdata.html",
    "title": "Missing data analysis",
    "section": "",
    "text": "Background\nThe chapter provides a comprehensive guide on missing data analysis, emphasizing various imputation techniques to address data gaps. It begins by introducing the concept of imputation and the different types of missing data: MCAR, MAR, and MNAR. The tutorial then delves into multiple imputation methods for complex survey data, highlighting the importance of visualizing missing data patterns, creating multiple imputed datasets, and pooling results for a consolidated analysis. The challenges of imputing dependent and exposure variables are addressed, with a focus on the benefits of using auxiliary variables. The guide also explores the estimation of model performance in datasets with missing values, using metrics like the AUC and the Archer-Lemeshow test. Special attention is given to handling subpopulations with missing observations, testing the MCAR assumption empirically, and understanding effect modification within multiple imputation.",
    "crumbs": [
      "Missing data analysis"
    ]
  },
  {
    "objectID": "missingdata.html#background",
    "href": "missingdata.html#background",
    "title": "Missing data analysis",
    "section": "",
    "text": "In the vast landscape of survey data analysis, one challenge consistently emerges as both a hurdle and an opportunity: missing data. As we delve into this chapter, we’ll confront the often-encountered issue of incomplete or absent data points in survey datasets. Missing data isn’t just a challenge; it’s an invitation to refine our analytical techniques. This chapter will equip you with the tools and methodologies to handle missing data adeptly, ensuring that our survey data analysis remains robust and reliable.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Missing data analysis"
    ]
  },
  {
    "objectID": "missingdata.html#overview-of-tutorials",
    "href": "missingdata.html#overview-of-tutorials",
    "title": "Missing data analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nMissing data and imputation\nImputation is a technique used to replace missing data with substituted values. In health research, missing data is a common issue, and imputation helps in ensuring datasets are complete, leading to more accurate analyses. There are three types of missing data: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR). The type of missingness determines how the missing data should be handled. Various single imputation methods, such as mean imputation, regression imputation, and predictive mean matching, are used based on the nature of the missing data. Multiple imputation is a process where the incomplete dataset is imputed multiple times, and the results are pooled together for more accurate analyses. Variable selection is crucial when analyzing missing data, and methods like majority, stack, and Wald are used to determine the best model. It’s also essential to assess the impact of missing values on model fitting (convergence and diagnostics) to ensure the reliability of the results.\n\n\nMultiple imputation in complex survey data\nIn the tutorial involve understanding how to assess the missing data patterns and visualize to understand the extent of missingness. Multiple imputations are then performed to address the missing data, creating multiple versions of the dataset with varying imputations. After imputation, new variables are created or modified for analysis, and the integrity of the imputed data is checked both visually. The tutorial also emphasizes the importance of combining multiple imputed datasets for analysis. Logistic regression is applied to the imputed datasets, and the results are pooled to get a single set of estimates. The tutorial concludes with a variable selection process to identify the most relevant variables for the model.\n\n\nMultiple imputation then deletion (MID)\nThis tutorial emphasizes the challenges of imputing dependent and exposure variables. The tutorial underscores the potential benefits of using auxiliary variables in the imputation process. While traditional Multiple Imputation (MI) and MID can yield similar results, MID is particularly advantageous when there’s a significant percentage of missing values in the outcome variable. The tutorial walks through the process of data loading, identifying missing values, performing standard imputations, and adding missing indicators. Subsequent steps involve structuring the data for survey design, fitting statistical models to each imputed dataset, and pooling the results for a consolidated analysis. The final stages focus on calculating and presenting odds ratios to interpret the relationships between variables.\n\n\nModel performance from multiple imputed datasets\nIn the context of survey data analysis, the provided tutorial outlines the process of estimating model performance, particularly when dealing with weighted data that has missing values. The focus is about estimating treatment effects, both individually and in a pooled manner. Model performance is gauged using the Area Under the Curve (AUC) and the Archer-Lemeshow (AL) test. This is done for models with and without interactions. The results provide insights into the model’s accuracy and fit, with the AUC offering a measure of the model’s discriminative ability and the AL test indicating the model’s goodness of fit to the data. The appendix provides a closer look at the user-defined functions used throughout the analysis.\n\n\nDealing with subpopulations with missing observations\nThe primary objective is to showcase how to handle missing data analysis with multiple imputation in the backdrop of complex surveys, particularly when we are interested in subpopulations. The process involves working with the analytic data, imputing missing values from this dataset, accounting for ineligible subjects from the complete data, and reincorporating these ineligible subjects into the imputed datasets. This ensures that the survey’s features can be utilized and the design subsetted accordingly. After importing and inspecting the dataset, the analysis subsets the data based on eligibility criteria, imputes missing values, and prepares the survey design. The subsequent steps involve design-adjusted logistic regression and pooling of estimates using Rubin’s rule.\n\n\nTesting MCAR assumption empirically in the data\nThe tutorial discusses the process of testing for Missing Completely At Random (MCAR) in datasets. Initially, essential packages are loaded to facilitate the analysis. A DAG is defined to represent the causal relationships between dataset variables, and this DAG is used to simulate a dataset. The DAG is then visualized, and the simulated dataset undergoes random data omission to mimic MCAR scenarios. Various visualizations, such as margin plots, are employed to understand the distribution of missing values in relation to other variables. Little’s MCAR test, a statistical method, is applied to determine if the data is indeed MCAR. The test’s limitations are also discussed. Additionally, tests for multivariate normality and homoscedasticity are conducted. In a subsequent section, data is intentionally set to missing based on a specific rule, and similar analyses and visualizations are performed to understand the nature of this missingness.\n\n\nEffect modification within multiple imputation\nThe tutorial delves into the intricacies of effect modification within the realm of survey data analysis. A dataset is comprising several imputed datasets is used. The primary objective is to investigate how two specific variables interact in predicting a particular outcome. To this end, logistic regression models are constructed for each level of a categorical variable. Emphasis is placed on the significance of Odds Ratios (ORs) in interpreting these interactions. Subsequently, simple slopes analyses are performed for each imputed dataset, shedding light on the relationship between the predictor and the outcome at distinct levels of a moderating variable. The outcomes from each imputed dataset are then pooled to offer a comprehensive understanding of the effect modification.\n\n\nMissing data imputation in survival analysis\nThis tutorial demonstrates a survey-weighted survival analysis using NHANES data with missing predictors, replacing complete-case analysis with multiple imputation (MI) via the mice package to maintain statistical power and reduce bias. The workflow includes: data preparation; MI setup incorporating the Nelson-Aalen cumulative hazard (instead of raw survival time) to capture survival information for imputing the key predictor; configuring the predictorMatrix; then analyze-pool procedure fitting design-based Cox models on each imputed dataset with survey design, and pooling results using Rubin’s Rules for final hazard ratios. It generates fallacy-safe tables focusing on the main exposure.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Missing data analysis"
    ]
  },
  {
    "objectID": "missingdata0.html",
    "href": "missingdata0.html",
    "title": "Concepts (M)",
    "section": "",
    "text": "Missing Data Analysis\nThis section is about understanding, categorizing, and addressing missing data in clinical and epidemiological research. It highlights the prevalence of missing data in these fields, the common use of complete case analysis without considering the implications, and the types of missingness: Missing Completely at Random (MCAR), Missing at Random (MAR), and Not Missing at Random (NMAR), each requiring different approaches and considerations. The consequences of not properly addressing missing data are detailed as bias, incorrect standard errors/precision, and a substantial loss of power.\nThis section also delves into strategies for addressing missing data, focusing on ad-hoc approaches and imputation methods. Ad-hoc approaches, such as ignoring missing data or using a missing category indicator, are generally dismissed as statistically invalid. In contrast, imputation, particularly MI, is presented as a more robust and statistically sound method. Multiple imputation involves creating multiple complete datasets by predicting missing values and pooling the results to address the uncertainty associated with missing data. The section further discusses the types of imputation, the necessity of including a sufficient number of predictive variables, and the use of subject-area knowledge in building imputation models, providing a nuanced understanding of the challenges and solutions associated with missing data in research.\nReporting Guideline section delves into the complexities of handling missing data in statistical analysis, primarily through MI methods, especially Multiple Imputation by Chained Equations (MICE). It lays out the assumptions necessary for these methods (MCAR, MAR, MNAR). The guide also details how MICE works, using sequential regression imputation to create multiple imputed datasets, thereby allowing for more accurate and robust statistical inferences. Additionally, it provides comprehensive instructions on reporting MICE analysis, including detailing the missingness rates, the reasons for missing data, the assumptions made, and the specifics of the imputation and pooling methods used, ensuring transparency and reproducibility in research.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#reading-list",
    "href": "missingdata0.html#reading-list",
    "title": "Concepts (M)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Sterne and al. 2009)\nOptional reading: (Van Buuren 2018a)\nFurther optional readings: (Lumley 2011; Granger, Sergeant, and Lunt 2019; Hughes et al. 2019)",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#video-lessons",
    "href": "missingdata0.html#video-lessons",
    "title": "Concepts (M)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMissing Data Analysis\n\n\n\nThe Unseen Threat: Why Missing Data Matters in Research\nMissing data is an inevitable feature of nearly all clinical and epidemiological research. From survey non-response to equipment malfunction or participant dropout, gaps in a dataset are the rule, not the exception. For many years, the profound impact of these gaps on the validity of research findings was often overlooked, partly because the statistical methods to properly address them were not readily accessible to most researchers. However, the landscape has changed. Powerful, principled methods for handling missing data, such as multiple imputation, are now available in standard statistical software, raising the standard of evidence and the expectation of rigor for all quantitative research. Understanding and correctly applying these methods is no longer a niche specialty but a core competency for any modern researcher.\nThe Critical Consequences of Inaction\nIgnoring missing data or handling it improperly is not a neutral act; it actively degrades the quality of scientific inquiry. The consequences are severe and can undermine the very conclusions of a study. There are three primary ways in which missing data can corrupt research findings:\n\n\nBias: When the missing values are systematically different from the observed ones, any analysis that ignores this fact will produce biased results. The estimates, such as regression coefficients or odds ratios, will be consistently wrong, misrepresenting the true relationships that exist in the population.\n\nLoss of Power: The most common (and often default) method for handling missing data is to simply discard any observation that has a missing value. This approach, known as complete case analysis, reduces the sample size. A smaller sample size diminishes the statistical power of a study, meaning it reduces the ability to detect real effects or relationships, even when they truly exist.\n\nIncorrect Precision: Improperly handling missing data can lead to incorrect standard errors. For instance, some naive methods make the data appear more perfect and less variable than it truly is. This results in standard errors that are too small and confidence intervals that are too narrow, giving a false sense of certainty in the findings.\nCritique of Flawed “Ad-Hoc” Approaches\nGiven the challenges, researchers have often resorted to simple, “ad-hoc” solutions. While appealing in their simplicity, these methods are statistically invalid under most realistic conditions and should be avoided.\n\n\nComplete Case Analysis (Listwise Deletion): This method, the default in many software packages, involves analyzing only the subset of observations with no missing data on any variable. While simple, it is only statistically valid under a very strict and rare assumption about the missing data mechanism Its widespread use without proper justification is one of the most common and serious errors in the literature. As a general rule of thumb, some methodologists suggest that complete case analysis could be considered for the primary analysis if the percentage of missing observations across all variables combined is below approximately 5%, but this requires a very strong justification and should not be based solely on a statistical test. Furthermore, if only the outcome variable has missing values, complete case analysis can be more statistically efficient than multiple imputation.\n\nSingle Imputation (e.g., Mean/Median): This approach involves “filling in” each missing value with a single number, such as the mean or median of the observed values for that variable. While this creates a complete dataset, it artificially reduces the natural variability of the data. All the imputed values are identical, which shrinks the standard deviation and leads to underestimated standard errors and overly optimistic (i.e., too small) p-values.\n\nIndicator Method: Another flawed technique is to create a new “missing” category for a variable and include this indicator in a regression model (Greenland and Finkle 1995; Vach and Blettner 1991). This is not a valid statistical approach and can introduce significant bias into the model’s estimates. This method treats the lack of information as if it were a meaningful, substantive category. For example, if income data is missing for lower-income individuals, creating a “Missing” category can mask the true relationship between income and health, potentially causing the model to underestimate the effect of income. The bias can be especially noticeable if the variable with the missing category is an important confounder.\n\nThe persistence of these suboptimal methods points to a critical issue beyond mere statistical technique. The failure to explicitly report the extent of missing data or to justify the method used to handle it is a matter of scientific integrity. Principled missing data analysis is not just about getting a more accurate p-value; it is about a commitment to transparency and producing the most robust and honest results possible from the available evidence.\nThe “Why” Behind the “What”: Assumptions of Missingness\nBefore any action can be taken to address missing data, the researcher must make a reasoned judgment about the mechanism that caused the data to be missing. This is not a statistical procedure but a theoretical assessment based on subject-matter knowledge. The choice of assumption is the single most important step, as it dictates the entire analytical strategy that follows. There are three core mechanisms of missingness.\nA Detailed Breakdown of the Three Core Mechanisms\n\n\nMissing Completely at Random (MCAR): This is the simplest but most restrictive assumption. Data are said to be MCAR if the probability of a value being missing is completely unrelated to any other variable in the dataset, whether observed or unobserved. The missingness is a pure, random process. A classic, albeit rare, example would be a researcher accidentally dropping a test tube, causing a random data point to be lost. Under MCAR, the complete cases are a random subsample of the original target sample.\n\nMissing at Random (MAR): This is a more relaxed and often more plausible assumption. Data are MAR if the probability of a value being missing can be fully explained by other observed variables in the dataset. The “at random” part of the name can be misleading; it does not mean the missingness is truly random. It means that conditional on the data we have observed, the missingness is random. For example, in a health survey, men might be less likely than women to answer questions about depression. Here, the probability of missingness on the depression variable depends on the ‘gender’ variable, which is observed. As long as we account for gender in our analysis, we can correct for the potential bias.\n\nMissing Not at Random (NMAR): This is the most challenging scenario. Data are NMAR if the probability of a value being missing is related to the value of that variable itself, even after accounting for all other observed variables. In this case, the reason for the missingness is the unobserved value. For example, individuals with very high incomes may be less likely to report their income, or patients who are feeling very ill may be more likely to miss a follow-up appointment. Under NMAR, the missingness is non-ignorable, and standard methods are generally biased.\nTable: Summary of Missing Data Mechanisms\nA clear way to distinguish these abstract concepts is with a summary table. It serves as a quick reference and reinforces the key distinctions that guide the choice of analytical method.\n\n\n\n\n\n\n\n\nMechanism\nDefinition\nImplication for Analysis\nPlausibility in Practice\n\n\n\nMCAR\nMissingness is a purely random process, unrelated to any data.\nComplete Case Analysis is unbiased but may be inefficient (loss of power).\nRare. Often an unrealistic assumption.\n\n\nMAR\nMissingness is explainable by other observed variables.\nComplete Case Analysis is biased. Principled methods like Multiple Imputation are required and valid.\nOften considered a plausible working assumption, especially with a rich dataset containing many predictors of missingness.\n\n\nNMAR\nMissingness depends on the unobserved missing value itself.\nBoth Complete Case Analysis and standard Multiple Imputation are biased. Requires specialized sensitivity analyses.\nPlausible in many scenarios, especially those involving social stigma, extreme values, or health outcomes.\n\n\n\nThe crucial takeaway is that the most powerful and widely used methods for handling missing data, such as Multiple Imputation, operate under the MAR assumption. This leads to a fundamental challenge for the researcher. It is not possible to distinguish between MAR and MNAR using observed data. This creates an apparent paradox: to proceed with the best available methods, one must make an assumption that cannot be statistically proven or disproven with the data at hand.\nThe resolution to this paradox lies in shifting the burden of proof from a statistical test to a well-reasoned, subject-matter argument. A researcher cannot simply run a test to “choose” MAR. Instead, they must build a compelling case for why MAR is a plausible assumption in their specific research context. This involves a deep understanding of the data collection process and the substantive area of study. The strength of the final analysis rests not on a p-value from a test, but on the plausibility of this foundational, untestable assumption.\nCan We Test the Assumptions? The Role and Limits of MCAR Tests\nGiven the importance of the missingness assumption, it is natural to ask if there are formal statistical tests to guide the decision. While there is no test to distinguish between MAR and NMAR, there are tests for the strictest assumption, MCAR. These tests, however, should be seen as limited diagnostic tools, not as definitive oracles.\nConceptual Goal of MCAR Tests\nTests for MCAR, such as Little’s Chi-Squared Test, are designed to evaluate the null hypothesis that the data are, in fact, Missing Completely at Random. Conceptually, they work by partitioning the data based on the pattern of missingness (e.g., one group missing variable X, another group missing variable Y, a third group with complete data). The test then compares the characteristics of the observed data—typically the means of the variables—across these different groups. If the data are truly MCAR, one would expect the variable means to be similar across all patterns of missingness. A statistically significant test result suggests that the means differ, which provides evidence against the MCAR assumption.\nThe Critical Limitations\nWhile useful, it is essential to understand the significant limitations of MCAR tests to avoid misinterpreting their results.\n\n\nA One-Way Street: Hypothesis tests are designed to reject, not accept, a null hypothesis. Therefore, a significant p-value from an MCAR test provides evidence to reject the MCAR assumption. However, a non-significant result does not prove that the data are MCAR. It simply means there was insufficient evidence in the data to reject the null hypothesis. This could be due to the data truly being MCAR, or it could be due to low statistical power.\n\nThe MAR/NMAR Blind Spot: This is the most critical limitation. An MCAR test provides no information whatsoever to help distinguish between MAR and NMAR. If the test rejects MCAR, the researcher knows the data are either MAR or NMAR, but the test offers no guidance on which is more likely. This is often the more crucial decision for the subsequent analysis.\n\nPower Issues: MCAR tests can have low statistical power, especially in smaller datasets or when the departure from MCAR is subtle. This means the test might fail to detect a true deviation from MCAR, leading to a non-significant result even when the data are actually MAR or NMAR.\n\nThese tests should be viewed as one piece of exploratory evidence in a broader investigation of the missing data mechanism, not as a standalone decision-making algorithm. Their primary utility is to serve as a statistical “red flag.” If an MCAR test is significant, it provides strong evidence that a naive approach like complete case analysis is inappropriate and will likely lead to biased results. If the test is not significant, the researcher is not absolved of responsibility. They must still rely on their subject-matter expertise and knowledge of the data collection process to make a reasoned judgment about the plausibility of MAR versus NMAR before proceeding with more advanced methods.\nThe Solution: A Journey Through Imputation Methods\nOnce the missing data problem has been diagnosed and an assumption about its mechanism has been made, the next step is to implement a solution. The most principled solutions involve imputation, the process of filling in missing data with substituted values to create a complete dataset. This section explores the evolution of imputation techniques, from flawed single imputation methods to the more robust multiple imputation framework.\nSingle Imputation: A First Step\nSingle imputation methods replace each missing value with one plausible value. While this produces a conveniently complete dataset, it is a fundamentally flawed approach because it fails to account for the uncertainty inherent in the imputation process.\n\n\nMean Imputation: The simplest method, where each missing value is replaced by the mean of the observed values for that variable. This artificially reduces the variance of the variable and distorts its relationships with other variables.\n\nRegression Imputation: An improvement that uses the relationships between variables. A regression model is built using the complete cases to predict the missing variable from other variables. The missing values are then filled in with their predicted values. However, this method is still flawed because all the imputed values fall perfectly on the regression line, understating the true variability of the data.\n\nStochastic Regression Imputation: This method addresses the flaw of regression imputation by adding a random error term to each predicted value. This restores the natural variance but can sometimes produce implausible values (e.g., negative height) if the error term is large.\n\nHot-Deck Imputation: In this method, a missing value is filled with an observed response from a “donor” individual who is similar on key matching variables. The donor is picked at random from a pool of similar individuals, ensuring the imputed value is a realistic, observed value from the dataset.\n\nPredictive Mean Matching (PMM): A sophisticated and generally well-regarded single imputation method. Like regression imputation, it starts by generating a predicted value for each missing entry. However, instead of using this prediction directly, it identifies a small set of “donor” observations from the complete cases whose predicted values are closest to the prediction for the missing entry. It then randomly selects one of these donors and uses their actual, observed value as the imputed value. This ensures that all imputed values are plausible and realistic, as they are drawn from the set of observed data.\nWhen Single Imputation May Be Considered\nWhile generally discouraged for final inferential analysis, there are specific scenarios where single imputation may be considered a pragmatic choice :\n\n\nClinical Trials: It is often preferred for imputing missing baseline covariates in randomized clinical trials.\n\nMissing Outcome with Auxiliary Variables: If only the outcome variable is missing and strong auxiliary variables (proxies for the outcome) are available, single imputation may be more effective than complete case analysis.\n\nPrediction Problems: In machine learning contexts focused on prediction, single imputation methods can be used, though pooling results from multiple imputations is not straightforward (Hossain et al. 2025).\nThe Unifying Flaw of Single Imputation\nDespite their increasing sophistication, all single imputation methods share a critical, unifying flaw: the subsequent statistical analysis treats the imputed values as if they were real, observed data. This failure to acknowledge the uncertainty of the imputation process—the fact that we do not know the true missing value and have only made an educated guess—leads to standard errors that are too small, confidence intervals that are too narrow, and p-values that are artificially significant. The analysis becomes overly precise and overly optimistic.\nMultiple Imputation (MI): The Gold Standard\nMI was developed specifically to solve this uncertainty problem. Instead of creating one “complete” dataset, MI creates multiple (e.g., \\(m=20\\) or \\(m=40\\)) complete datasets. Each dataset is generated using a similar process to stochastic imputation, but because of the random component, the imputed values are slightly different in each of the m datasets. This collection of datasets explicitly represents our uncertainty about what the true missing values might have been.\nBy analyzing all m datasets and then formally combining the results, MI provides a single final estimate that correctly incorporates both the normal sampling uncertainty (from having a finite sample) and the additional uncertainty that arises from the missing data. This makes it the gold standard approach for handling missing data under the MAR assumption.\nThe Multiple Imputation Workflow in Detail\nThe MI process can be demystified by breaking it down into three conceptual steps: Impute, Analyze, and Pool. This workflow provides a flexible and powerful framework for obtaining valid statistical inferences in the presence of missing data.\nStep 1: The Imputation Phase - Creating Plausible Realities\nThe goal of this phase is to generate m complete datasets where the imputed values are plausible draws from their predicted distribution, conditional on all the observed data.\n\n\nMethod (MICE): The most common and flexible algorithm for this phase is Multiple Imputation by Chained Equations (MICE), also known as Fully Conditional Specification (FCS). MICE is an iterative process that handles missing data on multiple variables at once. It tackles the problem one variable at a time, cycling through the variables with missing data. For each variable, it fits a regression model to predict it from all other variables in the dataset and then imputes the missing values based on that model’s predictions, including a random component. This cycle is repeated several times until the process converges, resulting in one complete dataset. The entire process is then repeated m times to generate the m imputed datasets.\n\nBuilding the Imputation Model: The success of MI hinges on the quality of the imputation model. This model should be inclusive and, in general, more complex than the final scientific model. The goal of the imputation model is not to test a hypothesis but to accurately preserve the complex web of relationships (correlations, means, variances) among all variables in the dataset. A good imputation model should contain:\n\nAll variables from the final analysis model, including the outcome variable.\n\nAuxiliary variables: These are variables that are correlated with the variables that have missingness, or are correlated with the missingness itself, even if they are not of scientific interest in the final analysis. Including them helps make the MAR assumption more plausible and can improve the precision of the final estimates.\nHigher-order terms (e.g., squared terms) or interactions if they are thought to be important for capturing the relationships in the data.\n\n\n\nPractical Considerations for the Imputation Model:\n\n\nNumber of Imputations (m): A common rule of thumb suggests that the number of imputations, m, should be at least as large as the percentage of subjects with any missing data (Austin et al. 2021). Modern recommendations often suggest between 20 and 100 imputations.\n\nNumber of Iterations: MICE is an iterative algorithm. In each cycle, it updates the imputed values based on the progressively improved predictions from the other variables. The algorithm is run for a set number of iterations to allow the imputed values to stabilize, a state known as convergence.\n\nHandling Non-Normal Data: For continuous variables that are not normally distributed (e.g., skewed), one approach is to transform the variable before imputation and transform it back afterward. However, this can distort relationships and complicate interpretation. A more robust and often preferred strategy within MICE is to use PMM, which is well-suited for non-normal data because it imputes values directly from the observed data, thereby preserving the original distribution.\n\n\n\nA common point of confusion is why the outcome variable should be included as a predictor in the imputation model (White, Royston, and Wood 2011). This seems circular or like “cheating.” However, this stems from a misunderstanding of the imputation model’s goal. The goal is not merely to predict a missing covariate \\(X\\), but to impute \\(X\\) in a way that preserves its true relationship with the outcome Y. The outcome \\(Y\\) is often the single best predictor of \\(X\\). Excluding it from the imputation model would cause the imputed values of \\(X\\) to have a weaker relationship with \\(Y\\) than the observed values of \\(X\\) do, biasing any estimated association between \\(X\\) and \\(Y\\) towards zero. The imputation model’s purpose is structural preservation, which enables the subsequent analysis model to accurately test a specific hypothesis.\nStep 2: The Analysis Phase - Analyzing Each Reality\nOnce the m complete datasets have been generated, the researcher performs their primary scientific analysis independently on each of the datasets. For example, if the research question involves fitting a logistic regression model, that exact same model is fitted to dataset 1, dataset 2, and so on, up to dataset m. This step is straightforward and results in m different sets of parameter estimates (e.g., m regression coefficients) and m different standard errors.\nStep 3: The Pooling Phase - Synthesizing the Results with Rubin’s Rules\nThis is the final and crucial step where the results from the m separate analyses are combined into a single, valid inference using a set of formulas known as Rubin’s Rules.\n\n\nThe Pooled Estimate: The final point estimate for any parameter (e.g., a regression coefficient) is simply the average of the m estimates obtained in the analysis phase.\n\nThe Pooled Variance: This is the key to MI’s success. The total variance of the pooled estimate correctly accounts for all sources of uncertainty and is composed of two parts:\n\n\nWithin-Imputation Variance (\\(\\bar{U}\\)): This is the average of the variances from each of the m analyses. It represents the normal sampling uncertainty we would have if our data had been complete from the start.\n\nBetween-Imputation Variance (\\(B\\)): This is the variance of the parameter estimates across the m datasets. It directly captures the extra uncertainty that is due to the missing data. If the missing data were not very influential, the estimates from all m datasets would be very similar, and \\(B\\) would be small. If the missing data were very influential, the estimates would vary more, and \\(B\\) would be large.\n\n\n\nThe formula for the total variance (\\(T\\)) is \\(T = \\bar{U} + B(1 + 1/m)\\). This elegant formula shows how MI correctly inflates the standard error to account for the uncertainty from missing data (\\(B\\)), solving the primary problem of single imputation and yielding valid confidence intervals and p-values. The “fraction of missing information” (FMI) is a useful metric derived from this process, which quantifies the proportion of the total variance that is attributable to the missing data.\nStep 4: Convergence and Diagnostics\nAfter running the imputation, it is essential to perform diagnostic checks. A key diagnostic is the convergence plot, which traces the mean and standard deviation of the imputed values for each variable across the iterations for each imputed dataset. For healthy convergence, these trace lines should appear as stationary, horizontal bands of random noise, without any clear upward or downward trends. This indicates that the algorithm has stabilized and the imputed values are reliable.\nHandling Missing Outcomes with MID\nA common point of hesitation for researchers new to imputation is what to do when the dependent variable (outcome) itself is missing. There is often a fear that imputing the outcome might artificially create the very results the study aims to find. While this concern is understandable, simply deleting subjects with missing outcomes (complete case analysis) is often biased under the MAR assumption. A strategy known as ‘Multiple Imputation, then Deletion’ (MID) offers a principled solution.\nThe Dilemma of Imputing the Outcome\nIf a predictor variable \\(X\\) is missing for a subject, the value of their outcome \\(Y\\) can be very informative for imputing \\(X\\). Ignoring subjects with a missing outcome during the imputation phase means throwing away valuable information that could have improved the imputation of other variables. However, some argue that using the imputed outcomes in the final analysis model may add unnecessary noise, especially if the imputation model for the outcome is not perfectly specified.\nThe ‘Multiple Imputation, then Deletion’ (MID) Strategy\nThe MID approach cleverly navigates this dilemma with a three-step conceptual process. It is particularly popular when there is a high percentage of missing values in the outcome (e.g., 20%-50%).\n\n\nStep A (Impute): Perform a standard multiple imputation on the entire dataset. Crucially, the outcome variable (\\(Y\\)) is included in the imputation model and is itself imputed. This ensures that all available information, including from subjects with missing outcomes, is used to create the best possible imputations for the predictor variables (\\(X\\)s).\n\nStep B (Delete): After the imputation phase is complete and the m datasets have been generated, delete the observations for which the outcome variable was originally missing. This means the imputed values of \\(Y\\) are discarded and will not be used in the final analysis model.\n\nStep C (Analyze & Pool): Proceed with the standard analysis and pooling steps using only the observations that had an observed outcome from the beginning. The analysis is performed on the m datasets, each of which now contains only subjects with observed outcomes but has fully imputed predictors.\nRationale, Extensions, and Sensitivity Analysis\nThe core idea behind MID is to separate the task of imputing predictors from the task of estimating the relationship of interest. It uses the information from the full dataset (including subjects with missing \\(Y\\)) to get the best possible imputations for the predictors, and then uses only the reliable, observed data to get the best possible estimate of the relationship between those predictors and the outcome. This strategy operates under the assumption that the imputed outcomes themselves do not add useful information to the regression analysis of interest and may only add statistical noise. The same MID logic can be applied if a key exposure variable is missing. When in doubt, MID can also be used as a sensitivity analysis: a researcher can compare the results from a full MI analysis with the results from an MID analysis to gauge the impact of the imputed outcomes on the final conclusions.\nEffect Modification Analysis with MI\nThe flexible Impute -&gt; Analyze -&gt; Pool framework of MI is not limited to simple main effects models. It can be readily extended to investigate more complex scientific questions.\nEffect modification occurs when the effect of an exposure on an outcome differs across levels of a third variable, the effect modifier. For example, a new drug’s effect on blood pressure might be stronger in women than in men. Here, gender is an effect modifier. Statistically, this is often tested by including an interaction term in a regression model (e.g., \\(Y \\sim \\text{Drug} + \\text{Gender} + \\text{Drug} \\times \\text{Gender}\\)).\nTo test for effect modification in the presence of missing data, the MI workflow is adapted as follows:\n\n\nStep 1 (Impute): Perform multiple imputation as usual. It is critical that the exposure, the outcome, and the potential effect modifier are all included in the imputation model. To best preserve the potential interaction, it is also highly recommended to include the interaction term itself in the imputation model.\n\nStep 2 (Analyze): In the analysis phase, fit the regression model that includes the interaction term (e.g., \\(Y \\sim X + Z + X \\times Z\\)) to each of the m imputed datasets.\n\nStep 3 (Pool): Pool the results from the m models using Rubin’s Rules. This will yield a single pooled estimate, standard error, and p-value for the main effects of \\(X\\) and \\(Z\\), and, most importantly, for the interaction term \\(X \\times Z\\). A statistically significant pooled interaction term provides evidence for effect modification.\n\nWhile pooling the interaction term is statistically valid, interpreting the coefficient for an interaction term can be non-intuitive. A more practical and often more interpretable approach involves performing a stratified analysis in Step 2. Instead of fitting one interaction model, one can fit separate, simpler models for each level of the effect modifier. This process yields stratum-specific effect estimates (e.g., the final pooled Odds Ratio for treatment in males and the final pooled Odds Ratio for treatment in females). These can then be directly compared to assess effect modification in a way that is often easier to communicate and understand than an interaction coefficient.\nVariable Selection with MI\nA common challenge is how to perform variable selection (e.g., stepwise regression) when using MI. Because the analysis is run on m different datasets, the variable selection procedure might choose a different set of “best” predictors for each one, making it difficult to pool the results into a single final model. Several strategies have been proposed to handle this :\n\n\nMajority Rule: Perform variable selection on each of the m imputed datasets. The final model includes only those variables that are selected in a majority (more than half) of the analyses.\n\nStacked Regression: Stack all m imputed datasets into one large dataset. Then, perform a single variable selection procedure on this large, stacked dataset.\n\nWald Test Approach: This method involves fitting nested models and using a pooled Wald test (or a similar test statistic) to compare them. This is generally considered a highly principled approach for variable selection with multiply imputed data.\n\nThe Challenge of NMAR and Sensitivity Analysis\nThe most difficult missing data mechanism to handle is Missing Not at Random (NMAR), where the probability of missingness depends on the unobserved value itself.\nWhy NMAR Produces Bias\nStandard methods like complete case analysis and MAR-based multiple imputation assume that the missingness can be explained by observed data. This assumption is violated under NMAR. For example, if patients who are sicker are more likely to drop out of a study, their missing health data is directly related to their unobserved, worsening health status. Because the reason for missingness cannot be directly observed or modeled from the available data, standard methods will produce biased estimates.\nSensitivity Analysis for NMAR\nSince the NMAR assumption cannot be formally tested against MAR, the recommended approach is to conduct a sensitivity analysis. This involves intentionally imputing the missing values under different plausible NMAR scenarios to see how sensitive the study’s conclusions are to these changes. For example, one might impute missing health data under a “best-case” scenario (assuming dropouts were healthier than observed) and a “worst-case” scenario (assuming they were sicker). If the study’s main conclusions remain unchanged across these different scenarios, it provides greater confidence in the robustness of the findings. One common technique for this is delta-adjustment, where the imputed values are systematically shifted to reflect a hypothesized difference between the missing and observed groups.\n\n\n\nImputation vs. Pooling: How Non-Normality Affects Inference\nWhen you use MI to handle missing data, you combine the results from several imputed datasets using a set of formulas called Rubin’s rules (Rubin 1988). A key assumption behind these rules is that the estimates you’re combining, e.g., ORs or HRs, follow a roughly bell-shaped (normal) distribution. If this assumption is violated, the validity of your final confidence intervals and p-values can be compromised. The problem is worse in small samples or with high rates of missingness because the Central Limit Theorem is less effective.\nThis issue, known as, non-normality, can arise from two distinct sources: problems with the (1) imputation process itself, or the (2) inherent nature of the statistic (HR, OR) you are estimating.\n(1) Imputation-Induced Skewness: Garbage In, Garbage Out: This happens when your imputation model is a poor fit for the variable with missing data. The model then generates imputed values that are implausible, which distorts the variable’s distribution and, in turn, skews the results of your analysis model.\nFor example, consider the scenario when you are missing data on patient income, a variable that is almost always right-skewed. If you use a standard imputation method that assumes normality (e.g., simple linear regression), it might generate negative or unrealistically high income values. When you then run a regression model using these flawed imputed datasets, the resulting odds ratios can become skewed or spread out unnaturally. This increases the between-imputation variance (the measure of uncertainty from the missing data), often leading to overly wide confidence intervals and a loss of statistical power. The fix here is to improve the imputation model. Use methods that respect the natural distribution of your data, such as PMM (pmm), or apply a transformation (e.g., a log transform) to the variable before imputing it.\n(2) Parameter-Inherent Skewness: It’s Not the Imputation, It’s the Ratio: Sometimes, the imputation model is perfect, but the statistic you are trying to estimate is naturally skewed. Ratios, such as HRs or ORs, and bounded measures such as correlation coefficients, are prime examples of parameters with inherently non-normal sampling distributions. An HR, for instance, cannot be less than zero but can be very large, leading to a right-skewed distribution.\nWhen you directly pool HRs or ORs from your imputed datasets, their inherent skew can make the standard error calculation from Rubin’s rules less accurate. The t-distribution used to construct the confidence interval doesn’t quite fit, which can lead to confidence intervals that are too narrow (inflating your Type I error rate) or too wide.\nThe standard and recommended approach is to transform the estimate before pooling (Van Buuren 2018b):\n\nFor each imputed dataset, calculate the log(HR) or log(OR) and its standard error.\n\nUse Rubin’s rules to pool the log-transformed estimates and their standard errors.\n\nBack-transform the final pooled estimate and its confidence interval limits (by exponentiating them) to get your result back on the original HR or OR scale.\n\nPractical Steps and Diagnostics:\nA simple diagnostic is to plot a histogram or density plot of your estimates (e.g., the 20+ odds ratios from your imputed datasets).\n\nIf the plot shows a few extreme outliers and is widely spread, it suggests an imputation problem. You should revisit your imputation model.\n\nIf the plot is consistently skewed in one direction across all imputations, it likely points to parameter-inherent skewness. You should use the transform–pool–backtransform approach.\n\nBy correctly diagnosing the source of non-normality, you can ensure your final results are both valid and reliable, preserving the statistical power that multiple imputation is designed to provide. Increasing the number of imputations (m) does not solve either of these fundamental non-normality problems. While increasing m does not fix non-normality, it can stabilize the Monte Carlo error of pooled estimates: so increasing m helps precision but not distributional validity. Better methods, not more imputations, are the key.\n\n\n\n\n\n\n\n\n\n\nReporting guidelines when missing data is present\n\n\n\nBest Practices for Transparent Reporting\nThis guide has journeyed from the fundamental problems caused by missing data—bias, power loss, and incorrect precision—to the principled, modern solution of Multiple Imputation. The core takeaways are that handling missing data requires careful thought, that the choice of an underlying assumption like MAR is a reasoned argument based on subject-matter knowledge rather than a statistical fact, and that the ultimate goal of imputation is not just to fill in blanks, but to do so in a way that preserves the original data structure and correctly represents our uncertainty about the missing values.\nTo ensure that research is both reproducible and credible, transparent reporting is paramount. Based on common problems identified in the scientific literature, any analysis using MI should be accompanied by a clear and detailed description of the process.\nA Blueprint for Reporting Multiple Imputation\nA robust report or publication should include the following key elements:\n\n\nExtent of Missing Data: Report the percentage of missing observations for each variable included in the analysis.\n\nAssumed Missing Data Mechanism: Explicitly state the assumed mechanism (e.g., MAR) and provide a brief, clear justification for why this assumption is plausible in the context of the study’s design and data collection procedures.\n\nImputation Software: State the specific software package and version used to perform the multiple imputation (e.g., mice package in R, version 3.13.0).\n\nImputation Model Specification: Describe the imputation model in detail. This includes listing all variables used as predictors in the imputation model, specifying any auxiliary variables that were included to improve the imputation, and noting the type of model used for each variable being imputed (e.g., predictive mean matching, logistic regression).\n\nNumber of Imputations: Report the number of imputed datasets (m) that were created.\n\nPooling Method: State that the results were combined across the m datasets using Rubin’s Rules.\n\nDiagnostics: Briefly mention any diagnostic checks that were performed to assess the convergence of the imputation algorithm and the plausibility of the imputed values.\n\nBy following this blueprint, researchers can provide the necessary information for readers and reviewers to critically evaluate the analysis, thereby strengthening the credibility of the findings and contributing to a more rigorous and transparent scientific culture.\n\n\n\nChecklist: RAISE-MI (Reproducible Analysis and Imputation Standards for Epidemiology – Multiple Imputation)\nRAISE-MI is a structured reporting framework developed to promote transparency, reproducibility, and methodological rigor in epidemiologic analyses that use MI: particularly when working with complex survey data such as NHANES. The guideline organizes reporting expectations across five domains, guiding researchers from conceptual justification to interpretation. It is designed for both manuscript preparation and instructional use, ensuring that analytic workflows are well-documented, assumptions are explicitly justified, and reproducibility is verifiable through software, code, and diagnostic reporting.\nExplanation of the Five Sections\n\n\nJustify Mechanism: The first domain establishes the foundation by requiring authors to state and defend the assumed missing-data mechanism (e.g., MCAR, MAR, MNAR) and explain its plausibility within the study context and survey design.\n\nCharacterize Missingness (2a–2c): This group focuses on describing the scope and structure of missingness in the dataset. It includes quantifying missing data across variables (2a), documenting any excluded records and their impact (2b), and performing basic diagnostic tests for randomness of missingness, such as Little’s MCAR test (2c).\n\nImputation Model Specification (3a–3c): These items require full transparency about the imputation model and computational setup. This includes listing variables and predictors used (3a), describing imputation parameters such as the number of imputations and method (3b), and reporting software versions and random seeds to ensure reproducibility (3c).\n\nAnalysis and Evaluation (4a–4b): This section addresses how imputed data are analyzed and validated. Authors should explain how survey design features (weights, strata, clusters) are incorporated and how results are pooled using Rubin’s Rules (4a), and provide evidence of imputation quality through diagnostic checks like trace or density plots (4b).\n\nRobustness and Interpretation (5a–5b): The final domain emphasizes critical reflection. It calls for conducting and reporting sensitivity analyses comparing MI to complete-case or alternative models (5a), and discussing limitations, including residual bias and the plausibility of assumptions like MAR (5b).\n\n\n\nRAISE-MI Checklist\n\nItem\nDescription\n\n\n\n1. Justify Mechanism\nState the assumed missing-data mechanism (typically Missing at Random, MAR). Provide a subject-matter rationale for why MAR is plausible in the NHANES context (e.g., missingness related to health status, age, or survey design features).\n\n\n2a. Report Missingness Extent\nFor each variable, report the number and percentage of missing values. Highlight variables with high missingness (e.g., &gt;20–30%) and discuss whether they were imputed, handled separately, or excluded. Include an overall summary (e.g., proportion of participants with ≥1 missing variable).\n\n\n2b. Describe Data Exclusions\nDocument any records excluded from the analytic dataset (e.g., missing weights or design variables). Report the resulting sample size and, if appropriate, use a flow diagram to illustrate inclusion/exclusion and missingness.\n\n\n2c. Conduct MCAR Diagnostics\nReport results from an exploratory test for Missing Completely at Random (e.g., Little’s MCAR test), acknowledging the test’s limitations (low power and restrictive assumptions).\n\n\n3a. Detail Imputation Model\nList all variables included in the imputation model—outcome(s), exposures, confounders, auxiliary variables, and survey design variables (weights, strata, clusters). Describe handling of structural or skip-pattern missingness.\n\n\n3b. Specify Imputation Parameters\nReport the number of imputations (m) and justify it (e.g., one per percent of incomplete cases). Indicate the number of iterations used for convergence and the imputation method for each variable type (e.g., predictive mean matching for continuous, logistic for binary).\n\n\n3c. Report Software & Seed\nState the software, version, and key packages used (e.g., R 4.4.0, mice 3.16.0). Document the random seed(s) or initialization method to ensure reproducibility.\n\n\n4a. Describe Analysis & Pooling\nExplain how survey design features (weights, strata, clusters) were incorporated into the analysis. Describe how estimates were pooled across imputations (e.g., Rubin’s Rules) and report the fraction of missing information (λ) for key parameters when available.\n\n\n4b. Provide Diagnostics\nPresent convergence diagnostics (e.g., trace plots) and distributional checks (e.g., density plots, mean comparisons) to demonstrate imputation stability and plausibility of imputed values.\n\n\n5a. Include Sensitivity Analyses\nSummarize results from sensitivity analyses assessing robustness to different assumptions (e.g., compare MI with complete-case results, re-run with new seeds, or apply delta-adjustments for MNAR scenarios).\n\n\n5b. Discuss Limitations\nIn the discussion, reflect on the potential impact of missing data on study conclusions. Revisit the plausibility of MAR, acknowledge unmeasured factors, and discuss possible bias if data are MNAR.\n\n\n\n\nAssessment of Missing Data Analysis Reporting though RAISE-MI Checklist\nWe take an example article to assess adherence to the above checklist (Hossain et al. 2022). In this article, however, complete-case analysis was treated as the primary analysis (implicitly assuming MCAR). MI was performed as a sensitivity analysis under a MAR assumption to verify whether conclusions were robust to missing data treatment. The article is accompanied by additional information through an Appendix.\n\n\nRAISE-MI Checklist assessing Hossain et al. (2022) (with Appendix)\n\n\n\n\n\n\nItem\nDescription\nManuscript Compliance: Met vs. Not Met\n\n\n\n1. Justify Mechanism\nState the assumed missing-data mechanism (typically Missing at Random, MAR). Provide a subject-matter rationale for why MAR is plausible in the NHANES context.\n\nMet. The appendix explicitly states the missing data assumption used for the sensitivity analysis.What they wrote: “Briefly, we imputed five datasets with five iterations under the missing at random (MAR) assumption.”\n\n\n2a. Report Missingness Extent\nFor each variable, report the number and percentage of missing values. Highlight variables with high missingness and discuss whether they were imputed, handled separately, or excluded.\n\nPartially Met. The appendix provides percentages for key covariates but not a full list.What they wrote: “In our primary analysis using complete cases, we excluded 8,122 respondents due to missing data in covariates, particularly for family income (7.4%), hypertension (4.5%), and BMI (6.5%).”What they should have written: A supplementary table listing missingness for each covariate.\n\n\n2b. Describe Data Exclusions\nDocument any records excluded from the analytic dataset (e.g., missing weights or design variables). Report the resulting sample size and, if appropriate, use a flow diagram.\n\nMet. The main manuscript uses a flow diagram to document exclusions.What they wrote: “We excluded participants with other arthritis types (e.g., osteoarthritis, psoriatic arthritis)...”\n\n\n2c. Conduct MCAR Diagnostics\nReport results from an exploratory test for Missing Completely at Random (e.g., Little’s test), acknowledging the test’s limitations.\n\nNot Met. No formal MCAR diagnostic (e.g., Little’s test) reported.What they should have written: “We conducted Little’s MCAR test (p &lt; .001), suggesting MCAR was violated and justifying MI under MAR.”\n\n\n3a. Detail Imputation Model\nList all variables included in the imputation model—outcome(s), exposures, confounders, auxiliary variables, and survey design variables (weights, strata, clusters).\n\nPartially Met. Appendix lists predictor types but omits survey weights and clusters.What they wrote: “Predictors used included all confounders and risk factors, as well as geographical strata.”What they should have written: “Survey design variables (SDMVPSU, SDMVSTRA, WTMEC2YR) were also included.”\n\n\n3b. Specify Imputation Parameters\nReport the number of imputations (m) and justify it. Indicate the number of iterations used for convergence and the imputation method for each variable type.\n\nPartially Met. Appendix states 5 imputations and 5 iterations, but not the method for variable types.What they wrote: “We imputed five datasets with five iterations under MAR.”\n\n\n3c. Report Software & Seed\nState the software, version, and key packages used. Document the random seed(s) or initialization method to ensure reproducibility.\n\nPartially Met. Software and packages listed but versions and random seed omitted.What they wrote: “We used R 4.0.5... and the mice package.”What they should have written: “mice (v3.13.0); random seed = 123.”\n\n\n4a. Describe Analysis & Pooling\nExplain how survey design features were incorporated into the analysis. Describe how estimates were pooled across imputations (e.g., Rubin’s Rules).\n\nMet. The manuscript and appendix describe survey-aware models and pooling via Rubin’s Rules.What they wrote: “We implemented design-based models in all imputed datasets and pooled estimates using Rubin’s rule.”\n\n\n4b. Provide Diagnostics\nPresent convergence diagnostics (e.g., trace plots) and distributional checks to demonstrate imputation stability and plausibility of imputed values.\n\nNot Met. No diagnostics reported for imputation procedure.What they should have written: “Convergence assessed via trace plots; imputed vs. observed distributions compared via density plots.”\n\n\n5a. Include Sensitivity Analyses\nSummarize results from sensitivity analyses assessing robustness to different assumptions (e.g., compare MI with complete-case results).\n\nMet. MI framed and reported as sensitivity analysis.What they wrote: “The RA–CVD association did not change substantially in sensitivity analyses using multiple imputations (eTable 4).”\n\n\n5b. Discuss Limitations\nIn the discussion, reflect on the potential impact of missing data on study conclusions. Revisit the plausibility of MAR and discuss possible bias if data are MNAR.\n\nMet. Limitations section discusses robustness to missing data assumptions.What they wrote: “Sensitivity analyses using multiple imputations yielded similar estimates, suggesting robustness to MCAR and MAR assumptions.”",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#the-unseen-threat-why-missing-data-matters-in-research",
    "href": "missingdata0.html#the-unseen-threat-why-missing-data-matters-in-research",
    "title": "Concepts (M)",
    "section": "The Unseen Threat: Why Missing Data Matters in Research",
    "text": "The Unseen Threat: Why Missing Data Matters in Research\nMissing data is an inevitable feature of nearly all clinical and epidemiological research. From survey non-response to equipment malfunction or participant dropout, gaps in a dataset are the rule, not the exception. For many years, the profound impact of these gaps on the validity of research findings was often overlooked, partly because the statistical methods to properly address them were not readily accessible to most researchers. However, the landscape has changed. Powerful, principled methods for handling missing data, such as multiple imputation, are now available in standard statistical software, raising the standard of evidence and the expectation of rigor for all quantitative research. Understanding and correctly applying these methods is no longer a niche specialty but a core competency for any modern researcher.\nThe Critical Consequences of Inaction\nIgnoring missing data or handling it improperly is not a neutral act; it actively degrades the quality of scientific inquiry. The consequences are severe and can undermine the very conclusions of a study. There are three primary ways in which missing data can corrupt research findings:\n\n\nBias: When the missing values are systematically different from the observed ones, any analysis that ignores this fact will produce biased results. The estimates, such as regression coefficients or odds ratios, will be consistently wrong, misrepresenting the true relationships that exist in the population.\n\nLoss of Power: The most common (and often default) method for handling missing data is to simply discard any observation that has a missing value. This approach, known as complete case analysis, reduces the sample size. A smaller sample size diminishes the statistical power of a study, meaning it reduces the ability to detect real effects or relationships, even when they truly exist.\n\nIncorrect Precision: Improperly handling missing data can lead to incorrect standard errors. For instance, some naive methods make the data appear more perfect and less variable than it truly is. This results in standard errors that are too small and confidence intervals that are too narrow, giving a false sense of certainty in the findings.\nCritique of Flawed “Ad-Hoc” Approaches\nGiven the challenges, researchers have often resorted to simple, “ad-hoc” solutions. While appealing in their simplicity, these methods are statistically invalid under most realistic conditions and should be avoided.\n\n\nComplete Case Analysis (Listwise Deletion): This method, the default in many software packages, involves analyzing only the subset of observations with no missing data on any variable. While simple, it is only statistically valid under a very strict and rare assumption about the missing data mechanism Its widespread use without proper justification is one of the most common and serious errors in the literature. As a general rule of thumb, some methodologists suggest that complete case analysis could be considered for the primary analysis if the percentage of missing observations across all variables combined is below approximately 5%, but this requires a very strong justification and should not be based solely on a statistical test. Furthermore, if only the outcome variable has missing values, complete case analysis can be more statistically efficient than multiple imputation.\n\nSingle Imputation (e.g., Mean/Median): This approach involves “filling in” each missing value with a single number, such as the mean or median of the observed values for that variable. While this creates a complete dataset, it artificially reduces the natural variability of the data. All the imputed values are identical, which shrinks the standard deviation and leads to underestimated standard errors and overly optimistic (i.e., too small) p-values.\n\nIndicator Method: Another flawed technique is to create a new “missing” category for a variable and include this indicator in a regression model (Greenland and Finkle 1995; Vach and Blettner 1991). This is not a valid statistical approach and can introduce significant bias into the model’s estimates. This method treats the lack of information as if it were a meaningful, substantive category. For example, if income data is missing for lower-income individuals, creating a “Missing” category can mask the true relationship between income and health, potentially causing the model to underestimate the effect of income. The bias can be especially noticeable if the variable with the missing category is an important confounder.\n\nThe persistence of these suboptimal methods points to a critical issue beyond mere statistical technique. The failure to explicitly report the extent of missing data or to justify the method used to handle it is a matter of scientific integrity. Principled missing data analysis is not just about getting a more accurate p-value; it is about a commitment to transparency and producing the most robust and honest results possible from the available evidence.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#the-why-behind-the-what-assumptions-of-missingness",
    "href": "missingdata0.html#the-why-behind-the-what-assumptions-of-missingness",
    "title": "Concepts (M)",
    "section": "The “Why” Behind the “What”: Assumptions of Missingness",
    "text": "The “Why” Behind the “What”: Assumptions of Missingness\nBefore any action can be taken to address missing data, the researcher must make a reasoned judgment about the mechanism that caused the data to be missing. This is not a statistical procedure but a theoretical assessment based on subject-matter knowledge. The choice of assumption is the single most important step, as it dictates the entire analytical strategy that follows. There are three core mechanisms of missingness.\nA Detailed Breakdown of the Three Core Mechanisms\n\n\nMissing Completely at Random (MCAR): This is the simplest but most restrictive assumption. Data are said to be MCAR if the probability of a value being missing is completely unrelated to any other variable in the dataset, whether observed or unobserved. The missingness is a pure, random process. A classic, albeit rare, example would be a researcher accidentally dropping a test tube, causing a random data point to be lost. Under MCAR, the complete cases are a random subsample of the original target sample.\n\nMissing at Random (MAR): This is a more relaxed and often more plausible assumption. Data are MAR if the probability of a value being missing can be fully explained by other observed variables in the dataset. The “at random” part of the name can be misleading; it does not mean the missingness is truly random. It means that conditional on the data we have observed, the missingness is random. For example, in a health survey, men might be less likely than women to answer questions about depression. Here, the probability of missingness on the depression variable depends on the ‘gender’ variable, which is observed. As long as we account for gender in our analysis, we can correct for the potential bias.\n\nMissing Not at Random (NMAR): This is the most challenging scenario. Data are NMAR if the probability of a value being missing is related to the value of that variable itself, even after accounting for all other observed variables. In this case, the reason for the missingness is the unobserved value. For example, individuals with very high incomes may be less likely to report their income, or patients who are feeling very ill may be more likely to miss a follow-up appointment. Under NMAR, the missingness is non-ignorable, and standard methods are generally biased.\nTable: Summary of Missing Data Mechanisms\nA clear way to distinguish these abstract concepts is with a summary table. It serves as a quick reference and reinforces the key distinctions that guide the choice of analytical method.\n\n\n\n\n\n\n\n\nMechanism\nDefinition\nImplication for Analysis\nPlausibility in Practice\n\n\n\nMCAR\nMissingness is a purely random process, unrelated to any data.\nComplete Case Analysis is unbiased but may be inefficient (loss of power).\nRare. Often an unrealistic assumption.\n\n\nMAR\nMissingness is explainable by other observed variables.\nComplete Case Analysis is biased. Principled methods like Multiple Imputation are required and valid.\nOften considered a plausible working assumption, especially with a rich dataset containing many predictors of missingness.\n\n\nNMAR\nMissingness depends on the unobserved missing value itself.\nBoth Complete Case Analysis and standard Multiple Imputation are biased. Requires specialized sensitivity analyses.\nPlausible in many scenarios, especially those involving social stigma, extreme values, or health outcomes.\n\n\n\nThe crucial takeaway is that the most powerful and widely used methods for handling missing data, such as Multiple Imputation, operate under the MAR assumption. This leads to a fundamental challenge for the researcher. It is not possible to distinguish between MAR and MNAR using observed data. This creates an apparent paradox: to proceed with the best available methods, one must make an assumption that cannot be statistically proven or disproven with the data at hand.\nThe resolution to this paradox lies in shifting the burden of proof from a statistical test to a well-reasoned, subject-matter argument. A researcher cannot simply run a test to “choose” MAR. Instead, they must build a compelling case for why MAR is a plausible assumption in their specific research context. This involves a deep understanding of the data collection process and the substantive area of study. The strength of the final analysis rests not on a p-value from a test, but on the plausibility of this foundational, untestable assumption.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#can-we-test-the-assumptions-the-role-and-limits-of-mcar-tests",
    "href": "missingdata0.html#can-we-test-the-assumptions-the-role-and-limits-of-mcar-tests",
    "title": "Concepts (M)",
    "section": "Can We Test the Assumptions? The Role and Limits of MCAR Tests",
    "text": "Can We Test the Assumptions? The Role and Limits of MCAR Tests\nGiven the importance of the missingness assumption, it is natural to ask if there are formal statistical tests to guide the decision. While there is no test to distinguish between MAR and NMAR, there are tests for the strictest assumption, MCAR. These tests, however, should be seen as limited diagnostic tools, not as definitive oracles.\nConceptual Goal of MCAR Tests\nTests for MCAR, such as Little’s Chi-Squared Test, are designed to evaluate the null hypothesis that the data are, in fact, Missing Completely at Random. Conceptually, they work by partitioning the data based on the pattern of missingness (e.g., one group missing variable X, another group missing variable Y, a third group with complete data). The test then compares the characteristics of the observed data—typically the means of the variables—across these different groups. If the data are truly MCAR, one would expect the variable means to be similar across all patterns of missingness. A statistically significant test result suggests that the means differ, which provides evidence against the MCAR assumption.\nThe Critical Limitations\nWhile useful, it is essential to understand the significant limitations of MCAR tests to avoid misinterpreting their results.\n\n\nA One-Way Street: Hypothesis tests are designed to reject, not accept, a null hypothesis. Therefore, a significant p-value from an MCAR test provides evidence to reject the MCAR assumption. However, a non-significant result does not prove that the data are MCAR. It simply means there was insufficient evidence in the data to reject the null hypothesis. This could be due to the data truly being MCAR, or it could be due to low statistical power.\n\nThe MAR/NMAR Blind Spot: This is the most critical limitation. An MCAR test provides no information whatsoever to help distinguish between MAR and NMAR. If the test rejects MCAR, the researcher knows the data are either MAR or NMAR, but the test offers no guidance on which is more likely. This is often the more crucial decision for the subsequent analysis.\n\nPower Issues: MCAR tests can have low statistical power, especially in smaller datasets or when the departure from MCAR is subtle. This means the test might fail to detect a true deviation from MCAR, leading to a non-significant result even when the data are actually MAR or NMAR.\n\nThese tests should be viewed as one piece of exploratory evidence in a broader investigation of the missing data mechanism, not as a standalone decision-making algorithm. Their primary utility is to serve as a statistical “red flag.” If an MCAR test is significant, it provides strong evidence that a naive approach like complete case analysis is inappropriate and will likely lead to biased results. If the test is not significant, the researcher is not absolved of responsibility. They must still rely on their subject-matter expertise and knowledge of the data collection process to make a reasoned judgment about the plausibility of MAR versus NMAR before proceeding with more advanced methods.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#the-solution-a-journey-through-imputation-methods",
    "href": "missingdata0.html#the-solution-a-journey-through-imputation-methods",
    "title": "Concepts (M)",
    "section": "The Solution: A Journey Through Imputation Methods",
    "text": "The Solution: A Journey Through Imputation Methods\nOnce the missing data problem has been diagnosed and an assumption about its mechanism has been made, the next step is to implement a solution. The most principled solutions involve imputation, the process of filling in missing data with substituted values to create a complete dataset. This section explores the evolution of imputation techniques, from flawed single imputation methods to the more robust multiple imputation framework.\nSingle Imputation: A First Step\nSingle imputation methods replace each missing value with one plausible value. While this produces a conveniently complete dataset, it is a fundamentally flawed approach because it fails to account for the uncertainty inherent in the imputation process.\n\n\nMean Imputation: The simplest method, where each missing value is replaced by the mean of the observed values for that variable. This artificially reduces the variance of the variable and distorts its relationships with other variables.\n\nRegression Imputation: An improvement that uses the relationships between variables. A regression model is built using the complete cases to predict the missing variable from other variables. The missing values are then filled in with their predicted values. However, this method is still flawed because all the imputed values fall perfectly on the regression line, understating the true variability of the data.\n\nStochastic Regression Imputation: This method addresses the flaw of regression imputation by adding a random error term to each predicted value. This restores the natural variance but can sometimes produce implausible values (e.g., negative height) if the error term is large.\n\nHot-Deck Imputation: In this method, a missing value is filled with an observed response from a “donor” individual who is similar on key matching variables. The donor is picked at random from a pool of similar individuals, ensuring the imputed value is a realistic, observed value from the dataset.\n\nPredictive Mean Matching (PMM): A sophisticated and generally well-regarded single imputation method. Like regression imputation, it starts by generating a predicted value for each missing entry. However, instead of using this prediction directly, it identifies a small set of “donor” observations from the complete cases whose predicted values are closest to the prediction for the missing entry. It then randomly selects one of these donors and uses their actual, observed value as the imputed value. This ensures that all imputed values are plausible and realistic, as they are drawn from the set of observed data.\nWhen Single Imputation May Be Considered\nWhile generally discouraged for final inferential analysis, there are specific scenarios where single imputation may be considered a pragmatic choice :\n\n\nClinical Trials: It is often preferred for imputing missing baseline covariates in randomized clinical trials.\n\nMissing Outcome with Auxiliary Variables: If only the outcome variable is missing and strong auxiliary variables (proxies for the outcome) are available, single imputation may be more effective than complete case analysis.\n\nPrediction Problems: In machine learning contexts focused on prediction, single imputation methods can be used, though pooling results from multiple imputations is not straightforward (Hossain et al. 2025).\nThe Unifying Flaw of Single Imputation\nDespite their increasing sophistication, all single imputation methods share a critical, unifying flaw: the subsequent statistical analysis treats the imputed values as if they were real, observed data. This failure to acknowledge the uncertainty of the imputation process—the fact that we do not know the true missing value and have only made an educated guess—leads to standard errors that are too small, confidence intervals that are too narrow, and p-values that are artificially significant. The analysis becomes overly precise and overly optimistic.\nMultiple Imputation (MI): The Gold Standard\nMI was developed specifically to solve this uncertainty problem. Instead of creating one “complete” dataset, MI creates multiple (e.g., \\(m=20\\) or \\(m=40\\)) complete datasets. Each dataset is generated using a similar process to stochastic imputation, but because of the random component, the imputed values are slightly different in each of the m datasets. This collection of datasets explicitly represents our uncertainty about what the true missing values might have been.\nBy analyzing all m datasets and then formally combining the results, MI provides a single final estimate that correctly incorporates both the normal sampling uncertainty (from having a finite sample) and the additional uncertainty that arises from the missing data. This makes it the gold standard approach for handling missing data under the MAR assumption.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#the-multiple-imputation-workflow-in-detail",
    "href": "missingdata0.html#the-multiple-imputation-workflow-in-detail",
    "title": "Concepts (M)",
    "section": "The Multiple Imputation Workflow in Detail",
    "text": "The Multiple Imputation Workflow in Detail\nThe MI process can be demystified by breaking it down into three conceptual steps: Impute, Analyze, and Pool. This workflow provides a flexible and powerful framework for obtaining valid statistical inferences in the presence of missing data.\nStep 1: The Imputation Phase - Creating Plausible Realities\nThe goal of this phase is to generate m complete datasets where the imputed values are plausible draws from their predicted distribution, conditional on all the observed data.\n\n\nMethod (MICE): The most common and flexible algorithm for this phase is Multiple Imputation by Chained Equations (MICE), also known as Fully Conditional Specification (FCS). MICE is an iterative process that handles missing data on multiple variables at once. It tackles the problem one variable at a time, cycling through the variables with missing data. For each variable, it fits a regression model to predict it from all other variables in the dataset and then imputes the missing values based on that model’s predictions, including a random component. This cycle is repeated several times until the process converges, resulting in one complete dataset. The entire process is then repeated m times to generate the m imputed datasets.\n\nBuilding the Imputation Model: The success of MI hinges on the quality of the imputation model. This model should be inclusive and, in general, more complex than the final scientific model. The goal of the imputation model is not to test a hypothesis but to accurately preserve the complex web of relationships (correlations, means, variances) among all variables in the dataset. A good imputation model should contain:\n\nAll variables from the final analysis model, including the outcome variable.\n\nAuxiliary variables: These are variables that are correlated with the variables that have missingness, or are correlated with the missingness itself, even if they are not of scientific interest in the final analysis. Including them helps make the MAR assumption more plausible and can improve the precision of the final estimates.\nHigher-order terms (e.g., squared terms) or interactions if they are thought to be important for capturing the relationships in the data.\n\n\n\nPractical Considerations for the Imputation Model:\n\n\nNumber of Imputations (m): A common rule of thumb suggests that the number of imputations, m, should be at least as large as the percentage of subjects with any missing data (Austin et al. 2021). Modern recommendations often suggest between 20 and 100 imputations.\n\nNumber of Iterations: MICE is an iterative algorithm. In each cycle, it updates the imputed values based on the progressively improved predictions from the other variables. The algorithm is run for a set number of iterations to allow the imputed values to stabilize, a state known as convergence.\n\nHandling Non-Normal Data: For continuous variables that are not normally distributed (e.g., skewed), one approach is to transform the variable before imputation and transform it back afterward. However, this can distort relationships and complicate interpretation. A more robust and often preferred strategy within MICE is to use PMM, which is well-suited for non-normal data because it imputes values directly from the observed data, thereby preserving the original distribution.\n\n\n\nA common point of confusion is why the outcome variable should be included as a predictor in the imputation model (White, Royston, and Wood 2011). This seems circular or like “cheating.” However, this stems from a misunderstanding of the imputation model’s goal. The goal is not merely to predict a missing covariate \\(X\\), but to impute \\(X\\) in a way that preserves its true relationship with the outcome Y. The outcome \\(Y\\) is often the single best predictor of \\(X\\). Excluding it from the imputation model would cause the imputed values of \\(X\\) to have a weaker relationship with \\(Y\\) than the observed values of \\(X\\) do, biasing any estimated association between \\(X\\) and \\(Y\\) towards zero. The imputation model’s purpose is structural preservation, which enables the subsequent analysis model to accurately test a specific hypothesis.\nStep 2: The Analysis Phase - Analyzing Each Reality\nOnce the m complete datasets have been generated, the researcher performs their primary scientific analysis independently on each of the datasets. For example, if the research question involves fitting a logistic regression model, that exact same model is fitted to dataset 1, dataset 2, and so on, up to dataset m. This step is straightforward and results in m different sets of parameter estimates (e.g., m regression coefficients) and m different standard errors.\nStep 3: The Pooling Phase - Synthesizing the Results with Rubin’s Rules\nThis is the final and crucial step where the results from the m separate analyses are combined into a single, valid inference using a set of formulas known as Rubin’s Rules.\n\n\nThe Pooled Estimate: The final point estimate for any parameter (e.g., a regression coefficient) is simply the average of the m estimates obtained in the analysis phase.\n\nThe Pooled Variance: This is the key to MI’s success. The total variance of the pooled estimate correctly accounts for all sources of uncertainty and is composed of two parts:\n\n\nWithin-Imputation Variance (\\(\\bar{U}\\)): This is the average of the variances from each of the m analyses. It represents the normal sampling uncertainty we would have if our data had been complete from the start.\n\nBetween-Imputation Variance (\\(B\\)): This is the variance of the parameter estimates across the m datasets. It directly captures the extra uncertainty that is due to the missing data. If the missing data were not very influential, the estimates from all m datasets would be very similar, and \\(B\\) would be small. If the missing data were very influential, the estimates would vary more, and \\(B\\) would be large.\n\n\n\nThe formula for the total variance (\\(T\\)) is \\(T = \\bar{U} + B(1 + 1/m)\\). This elegant formula shows how MI correctly inflates the standard error to account for the uncertainty from missing data (\\(B\\)), solving the primary problem of single imputation and yielding valid confidence intervals and p-values. The “fraction of missing information” (FMI) is a useful metric derived from this process, which quantifies the proportion of the total variance that is attributable to the missing data.\nStep 4: Convergence and Diagnostics\nAfter running the imputation, it is essential to perform diagnostic checks. A key diagnostic is the convergence plot, which traces the mean and standard deviation of the imputed values for each variable across the iterations for each imputed dataset. For healthy convergence, these trace lines should appear as stationary, horizontal bands of random noise, without any clear upward or downward trends. This indicates that the algorithm has stabilized and the imputed values are reliable.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#handling-missing-outcomes-with-mid",
    "href": "missingdata0.html#handling-missing-outcomes-with-mid",
    "title": "Concepts (M)",
    "section": "Handling Missing Outcomes with MID",
    "text": "Handling Missing Outcomes with MID\nA common point of hesitation for researchers new to imputation is what to do when the dependent variable (outcome) itself is missing. There is often a fear that imputing the outcome might artificially create the very results the study aims to find. While this concern is understandable, simply deleting subjects with missing outcomes (complete case analysis) is often biased under the MAR assumption. A strategy known as ‘Multiple Imputation, then Deletion’ (MID) offers a principled solution.\nThe Dilemma of Imputing the Outcome\nIf a predictor variable \\(X\\) is missing for a subject, the value of their outcome \\(Y\\) can be very informative for imputing \\(X\\). Ignoring subjects with a missing outcome during the imputation phase means throwing away valuable information that could have improved the imputation of other variables. However, some argue that using the imputed outcomes in the final analysis model may add unnecessary noise, especially if the imputation model for the outcome is not perfectly specified.\nThe ‘Multiple Imputation, then Deletion’ (MID) Strategy\nThe MID approach cleverly navigates this dilemma with a three-step conceptual process. It is particularly popular when there is a high percentage of missing values in the outcome (e.g., 20%-50%).\n\n\nStep A (Impute): Perform a standard multiple imputation on the entire dataset. Crucially, the outcome variable (\\(Y\\)) is included in the imputation model and is itself imputed. This ensures that all available information, including from subjects with missing outcomes, is used to create the best possible imputations for the predictor variables (\\(X\\)s).\n\nStep B (Delete): After the imputation phase is complete and the m datasets have been generated, delete the observations for which the outcome variable was originally missing. This means the imputed values of \\(Y\\) are discarded and will not be used in the final analysis model.\n\nStep C (Analyze & Pool): Proceed with the standard analysis and pooling steps using only the observations that had an observed outcome from the beginning. The analysis is performed on the m datasets, each of which now contains only subjects with observed outcomes but has fully imputed predictors.\nRationale, Extensions, and Sensitivity Analysis\nThe core idea behind MID is to separate the task of imputing predictors from the task of estimating the relationship of interest. It uses the information from the full dataset (including subjects with missing \\(Y\\)) to get the best possible imputations for the predictors, and then uses only the reliable, observed data to get the best possible estimate of the relationship between those predictors and the outcome. This strategy operates under the assumption that the imputed outcomes themselves do not add useful information to the regression analysis of interest and may only add statistical noise. The same MID logic can be applied if a key exposure variable is missing. When in doubt, MID can also be used as a sensitivity analysis: a researcher can compare the results from a full MI analysis with the results from an MID analysis to gauge the impact of the imputed outcomes on the final conclusions.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#effect-modification-analysis-with-mi",
    "href": "missingdata0.html#effect-modification-analysis-with-mi",
    "title": "Concepts (M)",
    "section": "Effect Modification Analysis with MI",
    "text": "Effect Modification Analysis with MI\nThe flexible Impute -&gt; Analyze -&gt; Pool framework of MI is not limited to simple main effects models. It can be readily extended to investigate more complex scientific questions.\nEffect modification occurs when the effect of an exposure on an outcome differs across levels of a third variable, the effect modifier. For example, a new drug’s effect on blood pressure might be stronger in women than in men. Here, gender is an effect modifier. Statistically, this is often tested by including an interaction term in a regression model (e.g., \\(Y \\sim \\text{Drug} + \\text{Gender} + \\text{Drug} \\times \\text{Gender}\\)).\nTo test for effect modification in the presence of missing data, the MI workflow is adapted as follows:\n\n\nStep 1 (Impute): Perform multiple imputation as usual. It is critical that the exposure, the outcome, and the potential effect modifier are all included in the imputation model. To best preserve the potential interaction, it is also highly recommended to include the interaction term itself in the imputation model.\n\nStep 2 (Analyze): In the analysis phase, fit the regression model that includes the interaction term (e.g., \\(Y \\sim X + Z + X \\times Z\\)) to each of the m imputed datasets.\n\nStep 3 (Pool): Pool the results from the m models using Rubin’s Rules. This will yield a single pooled estimate, standard error, and p-value for the main effects of \\(X\\) and \\(Z\\), and, most importantly, for the interaction term \\(X \\times Z\\). A statistically significant pooled interaction term provides evidence for effect modification.\n\nWhile pooling the interaction term is statistically valid, interpreting the coefficient for an interaction term can be non-intuitive. A more practical and often more interpretable approach involves performing a stratified analysis in Step 2. Instead of fitting one interaction model, one can fit separate, simpler models for each level of the effect modifier. This process yields stratum-specific effect estimates (e.g., the final pooled Odds Ratio for treatment in males and the final pooled Odds Ratio for treatment in females). These can then be directly compared to assess effect modification in a way that is often easier to communicate and understand than an interaction coefficient.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#variable-selection-with-mi",
    "href": "missingdata0.html#variable-selection-with-mi",
    "title": "Concepts (M)",
    "section": "Variable Selection with MI",
    "text": "Variable Selection with MI\nA common challenge is how to perform variable selection (e.g., stepwise regression) when using MI. Because the analysis is run on m different datasets, the variable selection procedure might choose a different set of “best” predictors for each one, making it difficult to pool the results into a single final model. Several strategies have been proposed to handle this :\n\n\nMajority Rule: Perform variable selection on each of the m imputed datasets. The final model includes only those variables that are selected in a majority (more than half) of the analyses.\n\nStacked Regression: Stack all m imputed datasets into one large dataset. Then, perform a single variable selection procedure on this large, stacked dataset.\n\nWald Test Approach: This method involves fitting nested models and using a pooled Wald test (or a similar test statistic) to compare them. This is generally considered a highly principled approach for variable selection with multiply imputed data.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#the-challenge-of-nmar-and-sensitivity-analysis",
    "href": "missingdata0.html#the-challenge-of-nmar-and-sensitivity-analysis",
    "title": "Concepts (M)",
    "section": "The Challenge of NMAR and Sensitivity Analysis",
    "text": "The Challenge of NMAR and Sensitivity Analysis\nThe most difficult missing data mechanism to handle is Missing Not at Random (NMAR), where the probability of missingness depends on the unobserved value itself.\nWhy NMAR Produces Bias\nStandard methods like complete case analysis and MAR-based multiple imputation assume that the missingness can be explained by observed data. This assumption is violated under NMAR. For example, if patients who are sicker are more likely to drop out of a study, their missing health data is directly related to their unobserved, worsening health status. Because the reason for missingness cannot be directly observed or modeled from the available data, standard methods will produce biased estimates.\nSensitivity Analysis for NMAR\nSince the NMAR assumption cannot be formally tested against MAR, the recommended approach is to conduct a sensitivity analysis. This involves intentionally imputing the missing values under different plausible NMAR scenarios to see how sensitive the study’s conclusions are to these changes. For example, one might impute missing health data under a “best-case” scenario (assuming dropouts were healthier than observed) and a “worst-case” scenario (assuming they were sicker). If the study’s main conclusions remain unchanged across these different scenarios, it provides greater confidence in the robustness of the findings. One common technique for this is delta-adjustment, where the imputed values are systematically shifted to reflect a hypothesized difference between the missing and observed groups.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#video-lesson-slides",
    "href": "missingdata0.html#video-lesson-slides",
    "title": "Concepts (M)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nMissing data\n\n\n\n\nReporting guideline",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#links",
    "href": "missingdata0.html#links",
    "title": "Concepts (M)",
    "section": "Links",
    "text": "Links\nVideo Lessons\n\nGoogle Slides\nPDF Slides\nPDF Slides for FAQ\n\nReporting guideline\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata0.html#references",
    "href": "missingdata0.html#references",
    "title": "Concepts (M)",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C, Ian R White, Douglas S Lee, and Stef van Buuren. 2021. “Missing Data in Clinical Research: A Tutorial on Multiple Imputation.” Canadian Journal of Cardiology 37 (9): 1322–31.\n\n\nGranger, Elizabeth, Jamie C. Sergeant, and Mark Lunt. 2019. “Avoiding Pitfalls When Combining Multiple Imputation and Propensity Scores.” Statistics in Medicine 38 (26): 5120–32.\n\n\nGreenland, Sander, and William D Finkle. 1995. “A Critical Look at Methods for Handling Missing Covariates in Epidemiologic Regression Analyses.” American Journal of Epidemiology 142 (12): 1255–64.\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.\n\n\nHossain, Md Belal, Mohsen Sadatsafavi, James C Johnston, Hubert Wong, Victoria J Cook, and Mohammad Ehsanul Karim. 2025. “LASSO-Based Survival Prediction Modelling with Multiply Imputed Data: A Case Study in Tuberculosis Mortality Prediction.” The American Statistician, no. just-accepted: 1–20.\n\n\nHughes, Rachael A., Jon Heron, Jonathan A. Sterne, and Kate Tilling. 2019. “Accounting for Missing Data in Statistical Analyses: Multiple Imputation Is Not Always the Answer.” International Journal of Epidemiology 1: 11.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using r. Vol. 565. John Wiley & Sons.\n\n\nRubin, Donald B. 1988. “An Overview of Multiple Imputation.” In Proceedings of the Survey Research Methods Section of the American Statistical Association, 79:84.\n\n\nSterne, Jonathan A., and et al. 2009. “Multiple Imputation for Missing Data in Epidemiological and Clinical Research: Potential and Pitfalls.” BMJ 338: b2393.\n\n\nVach, Werner, and Mana Blettner. 1991. “Biased Estimation of the Odds Ratio in Case-Control Studies Due to the Use of Ad Hoc Methods of Correcting for Missing Values for Confounding Variables.” American Journal of Epidemiology 134 (8): 895–907.\n\n\nVan Buuren, Stef. 2018b. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\n\n———. 2018a. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\n\nWhite, Ian R, Patrick Royston, and Angela M Wood. 2011. “Multiple Imputation Using Chained Equations: Issues and Guidance for Practice.” Statistics in Medicine 30 (4): 377–99.",
    "crumbs": [
      "Missing data analysis",
      "Concepts (M)"
    ]
  },
  {
    "objectID": "missingdata1.html",
    "href": "missingdata1.html",
    "title": "Imputation",
    "section": "",
    "text": "What is imputation?\nImputation is the process of replacing missing data with substituted values. In health research, it’s common to have missing data. This tutorial teaches you how to handle and replace these missing values using the mice package in R.\nWhy is imputation important?\nMissing data can lead to biased or incorrect results. Imputation helps in making the dataset complete, which can lead to more accurate analyses.\nKey reference\nIn this discussion, our primary guide and source of information is the work titled “Flexible Imputation of Missing Data” by Stef van Buuren, denoted here as (Van Buuren 2018). This book is an invaluable resource for anyone looking to delve deeper into the intricacies of handling missing data, especially in the context of statistical analyses. Below we also cited the relevant section numbers.\nFirst, you need to load the necessary libraries:\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(mitools)\n\nType of missing data\n\nRef: (Van Buuren 2018), Section 1.2\n\nIn this section, we are going to introduce three types of missing data that we will encounter in data analysis.\n\n\nMissing Completely at Random (MCAR):\n\nThe reason data is missing is completely random and not related to any measured or unmeasured variables. It’s often an unrealistic assumption.\n\n\nMissing at Random (MAR):\n\nThe missing data is related to variables that are observed.\n\n\nMissing Not at Random (MNAR):\n\nThe missing data is related to variables that are not observed.\nWhy does missingness type matter?\nThe type of missingness affects how you handle the missing data:\n\nIf data is MCAR, you can still analyze the complete cases without introducing bias.\nIf data is MAR, you can use imputation to replace the missing values.\nIf data is MNAR, it’s challenging to address, and estimates will likely be biased. We could do some sensitivity analyses to check the impact.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nData imputation\nGetting to know the data\nBefore imputing, you should understand the data. The tutorial uses the analytic.with.miss dataset from NHANES. Various plots and functions are used to inspect the missing data pattern and relationships between variables.\n\n\n\n\n\n\nImportant\n\n\n\n\nTake a look here for those who are interested in how the analytic data was created.\nFor the purposes of this lab, we are just going to treat the data as SRS, and not going to deal with intricacies of survey data analysis.\n\n\n\n\nrequire(VIM)\nload(\"Data/missingdata/NHANES17.RData\")\nNHANES17s &lt;- analytic.with.miss[1:30,c(\"age\", \"bmi\", \"cholesterol\",\"diastolicBP\")]\nNHANES17s\n\n\n  \n\n\nNHANES17s[complete.cases(NHANES17s),]\n\n\n  \n\n\nmd.pattern(NHANES17s) \n\n\n\n\n\n\n#&gt;    bmi cholesterol age diastolicBP   \n#&gt; 15   1           1   1           1  0\n#&gt; 4    1           1   1           0  1\n#&gt; 4    1           1   0           1  1\n#&gt; 1    1           0   1           1  1\n#&gt; 4    1           0   0           0  3\n#&gt; 2    0           0   0           0  4\n#&gt;      2           7  10          10 29\n# Inspect the missing data pattern (each row = pattern)\n# possible missingness (0,1) pattern and counts\n# last col = missing counts for each variables\n# last row = how many variable values missing in the row\n# First col: Frequency of the pattern \n# e,g, 2 cases missing for bmi\n\nrequire(DataExplorer)\nplot_missing(NHANES17s)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n# check the missingness\n\nrequire(VIM)\nmarginplot(NHANES17s[, c(\"diastolicBP\", \"bmi\")])\n\n\n\n\n\n\nmarginplot(NHANES17s[, c(\"diastolicBP\", \"cholesterol\")])\n\n\n\n\n\n\nmarginplot(NHANES17s[, c(\"cholesterol\", \"bmi\")])\n\n\n\n\n\n\n# distribution of observed data given the other variable is observed\n# for MCAR, blue and red box plots should be similar\n\nSingle imputation\n\nRef: (Van Buuren 2018), Section 1.3\n\nImpute NA only once. Below are some examples (Van Buuren and Groothuis-Oudshoorn 2011):\nMean imputation\n\nRef: (Van Buuren 2018), Section 1.3.3, and (Buuren and Groothuis-Oudshoorn 2010)\n\n\nMean imputation is a straightforward method where missing values in a dataset are replaced with the mean of the observed values. While it’s simple and intuitive, this approach can reduce the overall variability of the data, leading to an underestimation of variance. This can be problematic in statistical analyses where understanding data spread is crucial.\n\n# Replace missing values by mean \nimputation1 &lt;- mice(NHANES17s, \n                   method = \"mean\", # Replace by mean of the other values\n                   m = 1, # Number of multiple imputations. \n                   maxit = 1) # Number of iteration; mostly useful for convergence\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  age  bmi  cholesterol  diastolicBP\nimputation1$imp\n#&gt; $age\n#&gt;       1\n#&gt; 1  57.4\n#&gt; 2  57.4\n#&gt; 4  57.4\n#&gt; 5  57.4\n#&gt; 8  57.4\n#&gt; 10 57.4\n#&gt; 17 57.4\n#&gt; 18 57.4\n#&gt; 22 57.4\n#&gt; 23 57.4\n#&gt; \n#&gt; $bmi\n#&gt;           1\n#&gt; 8  25.12857\n#&gt; 18 25.12857\n#&gt; \n#&gt; $cholesterol\n#&gt;           1\n#&gt; 1  178.8261\n#&gt; 2  178.8261\n#&gt; 8  178.8261\n#&gt; 18 178.8261\n#&gt; 22 178.8261\n#&gt; 23 178.8261\n#&gt; 30 178.8261\n#&gt; \n#&gt; $diastolicBP\n#&gt;       1\n#&gt; 1  67.6\n#&gt; 2  67.6\n#&gt; 3  67.6\n#&gt; 6  67.6\n#&gt; 8  67.6\n#&gt; 12 67.6\n#&gt; 18 67.6\n#&gt; 22 67.6\n#&gt; 23 67.6\n#&gt; 25 67.6\ncomplete(imputation1, action = 1) # this is a function from mice\n\n\n  \n\n\n# there is another function in tidyr with the same name!\n# use mice::complete() to avoid conflict\n## the imputed dataset\n\nRegression Imputation\n\nRef: (Van Buuren 2018), Section 1.3.4\n\nRegression imputation offers a more nuanced approach, especially when dealing with interrelated variables. By building a regression model using observed data, missing values are predicted based on the relationships between variables. This method can provide more accurate estimates for missing values by leveraging the inherent correlations within the data, making it a preferred choice in many scenarios over mean imputation.\n\\(Y \\sim X\\)\n\\(age \\sim bmi + cholesterol + diastolicBP\\)\n\nimputation2 &lt;- mice(NHANES17s, \n            method = \"norm.predict\", # regression imputation\n            seed = 1,\n            m = 1, \n            print = FALSE)\n\n# look at all imputed values\nimputation2$imp\n#&gt; $age\n#&gt;           1\n#&gt; 1  55.32215\n#&gt; 2  54.99604\n#&gt; 4  55.65437\n#&gt; 5  53.68539\n#&gt; 8  56.70424\n#&gt; 10 55.79329\n#&gt; 17 54.38372\n#&gt; 18 56.70422\n#&gt; 22 54.81486\n#&gt; 23 55.06851\n#&gt; \n#&gt; $bmi\n#&gt;           1\n#&gt; 8  25.12857\n#&gt; 18 25.12857\n#&gt; \n#&gt; $cholesterol\n#&gt;           1\n#&gt; 1  183.3772\n#&gt; 2  184.2347\n#&gt; 8  179.7431\n#&gt; 18 179.7431\n#&gt; 22 184.7111\n#&gt; 23 184.0442\n#&gt; 30 183.4399\n#&gt; \n#&gt; $diastolicBP\n#&gt;           1\n#&gt; 1  66.48453\n#&gt; 2  66.24254\n#&gt; 3  68.82463\n#&gt; 6  67.47980\n#&gt; 8  67.51011\n#&gt; 12 68.91318\n#&gt; 18 67.51011\n#&gt; 22 66.10810\n#&gt; 23 66.29631\n#&gt; 25 67.93401\n\n# examine the correlation between age and bmi before and after imputation\nfit1 &lt;- lm(age ~ bmi, NHANES17s) \n\nsummary(fit1) ## original data\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = age ~ bmi, data = NHANES17s)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -37.383  -5.194   3.168   9.444  15.965 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#&gt; bmi           0.1462     0.6063   0.241  0.81219   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 15.51 on 18 degrees of freedom\n#&gt;   (10 observations deleted due to missingness)\n#&gt; Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#&gt; F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nsqrt(summary(fit1)$r.squared)\n#&gt; [1] 0.05674047\n\nfit2 &lt;- lm(age ~ bmi, mice::complete(imputation2)) \nsummary(fit2) ## imputed complete data\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = age ~ bmi, data = mice::complete(imputation2))\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -37.152  -1.407   0.000   8.026  15.989 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  52.1516     9.2485   5.639 4.86e-06 ***\n#&gt; bmi           0.1812     0.3568   0.508    0.616    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 12.45 on 28 degrees of freedom\n#&gt; Multiple R-squared:  0.009127,   Adjusted R-squared:  -0.02626 \n#&gt; F-statistic: 0.2579 on 1 and 28 DF,  p-value: 0.6155\nsqrt(summary(fit2)$r.squared)\n#&gt; [1] 0.09553283\n## Relationship become stronger before imputation. \n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#&gt; [1] 0.05674047\nwith(data = mice::complete(imputation2), cor(age, bmi))\n#&gt; [1] 0.09553283\n\nStochastic regression imputation\n\nRef: (Van Buuren 2018), Section 1.3.5\n\nRegression imputation, while powerful, has an inherent limitation. When it employs the fitted model to predict missing values, it does so without incorporating the error terms. This means that the imputed values are precisely on the regression line, leading to an overly perfect fit. As a result, the natural variability present in real-world data is not captured, causing the imputed dataset to exhibit biased correlations and reduced variance. Essentially, the data becomes too “clean,” and this lack of variability can mislead subsequent analyses, making them overly optimistic or even erroneous.\nRecognizing this limitation, stochastic regression imputation was introduced as an enhancement. Instead of merely using the fitted model, it adds a randomly drawn error term during the imputation process. This error term reintroduces the natural variability that the original regression imputation method missed. By doing so, the imputed values are scattered around the regression line, more accurately reflecting the true correlations and distributions in the dataset. This method, therefore, offers a more realistic representation of the data, ensuring that subsequent analyses are grounded in a dataset that mirrors genuine variability and relationships.\n\\(Y \\sim X + e\\)\n\\(age \\sim bmi + cholesterol + diastolicBP + error\\)\n\nimputation3 &lt;- mice(NHANES17s, method = \"norm.nob\", # stochastic regression imputation\n                    m = 1, maxit = 1, seed = 504, print = FALSE)\n\n# look at all imputed values\nimputation3$imp\n#&gt; $age\n#&gt;           1\n#&gt; 1  79.59513\n#&gt; 2  53.94601\n#&gt; 4  73.76486\n#&gt; 5  43.69817\n#&gt; 8  49.70112\n#&gt; 10 43.81002\n#&gt; 17 29.72590\n#&gt; 18 61.15172\n#&gt; 22 58.75506\n#&gt; 23 78.39545\n#&gt; \n#&gt; $bmi\n#&gt;           1\n#&gt; 8  27.53270\n#&gt; 18 31.52568\n#&gt; \n#&gt; $cholesterol\n#&gt;           1\n#&gt; 1  252.2928\n#&gt; 2  209.7680\n#&gt; 8  169.2450\n#&gt; 18 107.7585\n#&gt; 22 181.8617\n#&gt; 23 239.9008\n#&gt; 30 131.8489\n#&gt; \n#&gt; $diastolicBP\n#&gt;           1\n#&gt; 1  75.02181\n#&gt; 2  44.45935\n#&gt; 3  86.69637\n#&gt; 6  60.54256\n#&gt; 8  63.80884\n#&gt; 12 60.03311\n#&gt; 18 73.94575\n#&gt; 22 36.70323\n#&gt; 23 73.95647\n#&gt; 25 65.84012\n#mice::complete(imputation3)\n\n# examine the correlation between age and bmi before and after imputation\nfit1 &lt;- lm(age ~ bmi, NHANES17s) \nsummary(fit1) \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = age ~ bmi, data = NHANES17s)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -37.383  -5.194   3.168   9.444  15.965 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)  53.3482    17.1585   3.109  0.00606 **\n#&gt; bmi           0.1462     0.6063   0.241  0.81219   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 15.51 on 18 degrees of freedom\n#&gt;   (10 observations deleted due to missingness)\n#&gt; Multiple R-squared:  0.003219,   Adjusted R-squared:  -0.05216 \n#&gt; F-statistic: 0.05814 on 1 and 18 DF,  p-value: 0.8122\nfit3 &lt;- lm(age ~ bmi, mice::complete(imputation3)) \nsummary(fit3)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = age ~ bmi, data = mice::complete(imputation3))\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -37.089  -6.691   3.183  10.104  21.288 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  60.4173    11.4671   5.269 1.33e-05 ***\n#&gt; bmi          -0.1206     0.4371  -0.276    0.785    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 15.53 on 28 degrees of freedom\n#&gt; Multiple R-squared:  0.002712,   Adjusted R-squared:  -0.03291 \n#&gt; F-statistic: 0.07614 on 1 and 28 DF,  p-value: 0.7846\n## Fitted coefficients of bmi are much closer before and after imputation\n# with(data=NHANES17s, cor(age, bmi, use = \"complete.obs\"))\n\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#&gt; [1] 0.05674047\nwith(data = mice::complete(imputation3), cor(age, bmi))\n#&gt; [1] -0.05207502\n# see the direction change?\n\nPredictive mean matching\nPredictive Mean Matching (PMM) is an advanced imputation technique that aims to provide more realistic imputations for missing data. Let’s break it down:\nIn this context, we’re trying to fill in missing values for the variable ‘age’. To do this, we use other variables like ‘bmi’, ‘cholesterol’, and ‘diastolicBP’ to predict ‘age’. First, a regression model is run using the available data to estimate the relationship between ‘age’ and the predictor variables. From this model, we get a coefficient, which is then adjusted slightly to introduce some randomness. Using this adjusted coefficient, we predict the missing ‘age’ values for all subjects. For example, if ‘subject 19’ has a missing age value, we might predict it to be 45.5 years. Instead of using this predicted value directly, we look for other subjects who have actual age values and whose predicted ages are close to 45.5 years. From these subjects, one is randomly chosen, and their real age is used as the imputed value for ‘subject 19’. In this way, PMM ensures that the imputed values are based on real, observed data from the dataset.\n\n\n\n\n\n\nTip\n\n\n\n\nAssume \\(Y\\) = age, a variable with some missing values. \\(X\\) (say, bmi, cholesterol, diastolicBP) are predictors of \\(Y\\).\nEstimate beta coef \\(\\beta\\) from complete case running \\(Y \\sim X + e\\)\n\ngenerate new \\(\\beta* \\sim Normal(b,se_b)\\).\nusing \\(\\beta*\\), predict new \\(\\hat{Y}\\) predicted age for all subjects (those with missing and observed age):\n\nIf subject 19 (say) has missing values in age variable, find out his predicted age \\(\\hat{Y}\\) (say, 45.5).\nFind others subjects, subjects 2, 15, 24 (say) who has their age measured and their predicted age \\(\\hat{Y}\\) (say, predicted ages 43.9,45.7,46.1 with real ages 43,45,46 respectively) are close to subject 19 (predicted age 45.5).\nRandomly select subject 2 with real/observed age 43, and impute 43 for subject 19’s missing age.\n\n\n\n\n\nThe strength of PMM lies in its approach. Instead of imputing a potentially artificial value based on a prediction, it imputes a real, observed value from the dataset. This ensures that the imputed data retains the original data’s characteristics and doesn’t introduce any unrealistic values. It offers a safeguard against extrapolation, ensuring that the imputed values are always within the plausible range of the dataset.\n\nimputation3b &lt;- mice(NHANES17s, method = \"pmm\", \n                    m = 1, maxit = 1,\n                    seed = 504, print = FALSE)\nwith(data=NHANES17s, cor(age, bmi, use = \"pairwise.complete.obs\"))\n#&gt; [1] 0.05674047\nwith(data = mice::complete(imputation3b), cor(age, bmi))\n#&gt; [1] -0.08029207\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nMultiple imputation and workflow\n\nRef: (Van Buuren 2018), Sections 1.4 and 5.1\nRef: (Buuren and Groothuis-Oudshoorn 2010)\n\n\nWe have learned different methods of imputation. In this section, we will introduce how to incorporate the data imputation into data analysis. In multiple imputation data analysis, three steps will be taken:\n\nStep 0: Set imputation model: Before starting the imputation process, it’s crucial to determine the appropriate imputation model based on the nature of the missing data and the relationships between variables. This model will guide how the missing values are estimated. For instance, if the data is missing at random, a linear regression model might be used for continuous data, while logistic regression might be used for binary data. The choice of model can significantly impact the quality of the imputed data, so it’s essential to understand the underlying mechanisms causing the missingness and select a model accordingly.\nStep 1: The incomplete dataset will be imputed \\(m\\) times: In this step, the incomplete dataset is imputed multiple times, resulting in \\(m\\) different “complete” datasets. The reason for creating multiple datasets is to capture the uncertainty around the missing values. Each of these datasets will have slightly different imputed values, reflecting the variability and uncertainty in the imputation process. The number of imputations, \\(m\\), is typically chosen based on the percentage of missing data and the desired level of accuracy. Common choices for \\(m\\) range from 5 to 50, but more imputations provide more accurate results, especially when the percentage of missing data is high.\nStep 2: Each \\(m\\) complete datasets will be analyzed separately by standard analysis (e.g., regression model): Once the \\(m\\) complete datasets are generated, each one is analyzed separately using standard statistical methods. For example, if the research question involves understanding the relationship between two variables, a regression model might be applied to each dataset. This step produces \\(m\\) sets of analysis results, one for each imputed dataset.\nStep 3: The analysis results will be pooled / aggregated together by Rubin’s rules (1987): The final step involves combining the results from the \\(m\\) separate analyses into a single set of results. This is done using Rubin’s rules (1987) (Little and Rubin 1987), which provide a way to aggregate the estimates and adjust for the variability between the imputed datasets. The pooled results give a more accurate and robust estimate than analyzing a single imputed dataset. Rubin’s rules ensure that the combined results reflect both the within-imputation variability (the variability in results from analyzing each dataset separately) and the between-imputation variability (the differences in results across the imputed datasets).\n\nStep 0\nSet imputation model:\n\nini &lt;- mice(data = NHANES17s, maxit = 0, print = FALSE)\npred &lt;- ini$pred\npred\n#&gt;             age bmi cholesterol diastolicBP\n#&gt; age           0   1           1           1\n#&gt; bmi           1   0           1           1\n#&gt; cholesterol   1   1           0           1\n#&gt; diastolicBP   1   1           1           0\n# A value of 1 indicates that column variables (say, bmi, cholesterol, diastolicBP) \n# are used as a predictor to impute the a row variable (say, age).\npred[,\"diastolicBP\"] &lt;- 0 \n# if you believe 'diastolicBP' should not be a predictor in any imputation model\npred\n#&gt;             age bmi cholesterol diastolicBP\n#&gt; age           0   1           1           0\n#&gt; bmi           1   0           1           0\n#&gt; cholesterol   1   1           0           0\n#&gt; diastolicBP   1   1           1           0\n# for cholesterol: bmi and age used to predict cholesterol (diastolicBP is not a predictor)\n# for diastolicBP: bmi, age and cholesterol used to predict diastolicBP \n# (diastolicBP itself is not a predictor) \n\nSet imputation method:\nSee Table 1 of (Van Buuren and Groothuis-Oudshoorn 2011).\n\nmeth &lt;- ini$meth\nmeth\n#&gt;         age         bmi cholesterol diastolicBP \n#&gt;       \"pmm\"       \"pmm\"       \"pmm\"       \"pmm\"\n# pmm is generally a good method, \n# but let's see how to work with other methods\n# just as an example.\n# Specifying imputation method:\nmeth[\"bmi\"] &lt;- \"mean\" \n# for BMI: no predictor used in mean method \n# (only average of observed bmi)\nmeth[\"cholesterol\"] &lt;- \"norm.predict\" \nmeth[\"diastolicBP\"] &lt;- \"norm.nob\"\nmeth\n#&gt;            age            bmi    cholesterol    diastolicBP \n#&gt;          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\"\n\nSet imputation model based on correlation alone:\n\npredictor.selection &lt;- quickpred(NHANES17s, \n                                 mincor=0.1, # absolute correlation \n                                 minpuc=0.1) # proportion of usable cases\npredictor.selection\n#&gt;             age bmi cholesterol diastolicBP\n#&gt; age           0   1           1           1\n#&gt; bmi           0   0           0           0\n#&gt; cholesterol   1   1           0           1\n#&gt; diastolicBP   1   1           1           0\n\nStep 1\n\n# Step 1 Impute the incomplete data m=10 times\nimputation4 &lt;- mice(data=NHANES17s, \n                    seed=504,\n                    method = meth,\n                    predictorMatrix = predictor.selection,\n                    m=10, # imputation will be done 10 times (i.e., 10 imputed datasets)\n                    maxit=3)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  age  bmi  cholesterol  diastolicBP\n#&gt;   1   2  age  bmi  cholesterol  diastolicBP\n#&gt;   1   3  age  bmi  cholesterol  diastolicBP\n#&gt;   1   4  age  bmi  cholesterol  diastolicBP\n#&gt;   1   5  age  bmi  cholesterol  diastolicBP\n#&gt;   1   6  age  bmi  cholesterol  diastolicBP\n#&gt;   1   7  age  bmi  cholesterol  diastolicBP\n#&gt;   1   8  age  bmi  cholesterol  diastolicBP\n#&gt;   1   9  age  bmi  cholesterol  diastolicBP\n#&gt;   1   10  age  bmi  cholesterol  diastolicBP\n#&gt;   2   1  age  bmi  cholesterol  diastolicBP\n#&gt;   2   2  age  bmi  cholesterol  diastolicBP\n#&gt;   2   3  age  bmi  cholesterol  diastolicBP\n#&gt;   2   4  age  bmi  cholesterol  diastolicBP\n#&gt;   2   5  age  bmi  cholesterol  diastolicBP\n#&gt;   2   6  age  bmi  cholesterol  diastolicBP\n#&gt;   2   7  age  bmi  cholesterol  diastolicBP\n#&gt;   2   8  age  bmi  cholesterol  diastolicBP\n#&gt;   2   9  age  bmi  cholesterol  diastolicBP\n#&gt;   2   10  age  bmi  cholesterol  diastolicBP\n#&gt;   3   1  age  bmi  cholesterol  diastolicBP\n#&gt;   3   2  age  bmi  cholesterol  diastolicBP\n#&gt;   3   3  age  bmi  cholesterol  diastolicBP\n#&gt;   3   4  age  bmi  cholesterol  diastolicBP\n#&gt;   3   5  age  bmi  cholesterol  diastolicBP\n#&gt;   3   6  age  bmi  cholesterol  diastolicBP\n#&gt;   3   7  age  bmi  cholesterol  diastolicBP\n#&gt;   3   8  age  bmi  cholesterol  diastolicBP\n#&gt;   3   9  age  bmi  cholesterol  diastolicBP\n#&gt;   3   10  age  bmi  cholesterol  diastolicBP\nimputation4$pred\n#&gt;             age bmi cholesterol diastolicBP\n#&gt; age           0   1           1           1\n#&gt; bmi           0   0           0           0\n#&gt; cholesterol   1   1           0           1\n#&gt; diastolicBP   1   1           1           0\n## look at the variables used for imputation\nmice::complete(imputation4, action = 1) # 1 imputed data  \n\n\n  \n\n\nall &lt;- mice::complete(imputation4, action=\"long\") # combine all 5 imputed datasets\ndim(all)\n#&gt; [1] 300   6\nhead(all)\n\n\n  \n\n\n## you can change the way of displaying the data\ndata_hori &lt;- mice::complete(imputation4, action=\"broad\") # display five imputations horizontally\n#&gt; New names:\n#&gt; • `age` -&gt; `age...1`\n#&gt; • `bmi` -&gt; `bmi...2`\n#&gt; • `cholesterol` -&gt; `cholesterol...3`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...4`\n#&gt; • `age` -&gt; `age...5`\n#&gt; • `bmi` -&gt; `bmi...6`\n#&gt; • `cholesterol` -&gt; `cholesterol...7`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...8`\n#&gt; • `age` -&gt; `age...9`\n#&gt; • `bmi` -&gt; `bmi...10`\n#&gt; • `cholesterol` -&gt; `cholesterol...11`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...12`\n#&gt; • `age` -&gt; `age...13`\n#&gt; • `bmi` -&gt; `bmi...14`\n#&gt; • `cholesterol` -&gt; `cholesterol...15`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...16`\n#&gt; • `age` -&gt; `age...17`\n#&gt; • `bmi` -&gt; `bmi...18`\n#&gt; • `cholesterol` -&gt; `cholesterol...19`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...20`\n#&gt; • `age` -&gt; `age...21`\n#&gt; • `bmi` -&gt; `bmi...22`\n#&gt; • `cholesterol` -&gt; `cholesterol...23`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...24`\n#&gt; • `age` -&gt; `age...25`\n#&gt; • `bmi` -&gt; `bmi...26`\n#&gt; • `cholesterol` -&gt; `cholesterol...27`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...28`\n#&gt; • `age` -&gt; `age...29`\n#&gt; • `bmi` -&gt; `bmi...30`\n#&gt; • `cholesterol` -&gt; `cholesterol...31`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...32`\n#&gt; • `age` -&gt; `age...33`\n#&gt; • `bmi` -&gt; `bmi...34`\n#&gt; • `cholesterol` -&gt; `cholesterol...35`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...36`\n#&gt; • `age` -&gt; `age...37`\n#&gt; • `bmi` -&gt; `bmi...38`\n#&gt; • `cholesterol` -&gt; `cholesterol...39`\n#&gt; • `diastolicBP` -&gt; `diastolicBP...40`\n\ndim(data_hori)\n#&gt; [1] 30 40\nhead(data_hori)\n\n\n  \n\n\n\n## Compare means of each imputed dataset\ncolMeans(data_hori)\n#&gt;          age.1          bmi.1  cholesterol.1  diastolicBP.1          age.2 \n#&gt;       61.06667       25.12857      179.47152       68.06998       58.20000 \n#&gt;          bmi.2  cholesterol.2  diastolicBP.2          age.3          bmi.3 \n#&gt;       25.12857      179.71210       66.61289       57.40000       25.12857 \n#&gt;  cholesterol.3  diastolicBP.3          age.4          bmi.4  cholesterol.4 \n#&gt;      180.28681       68.77854       54.86667       25.12857      179.58873 \n#&gt;  diastolicBP.4          age.5          bmi.5  cholesterol.5  diastolicBP.5 \n#&gt;       66.13461       52.80000       25.12857      179.35478       66.74254 \n#&gt;          age.6          bmi.6  cholesterol.6  diastolicBP.6          age.7 \n#&gt;       60.00000       25.12857      179.22499       66.76592       58.43333 \n#&gt;          bmi.7  cholesterol.7  diastolicBP.7          age.8          bmi.8 \n#&gt;       25.12857      178.99900       66.82760       56.20000       25.12857 \n#&gt;  cholesterol.8  diastolicBP.8          age.9          bmi.9  cholesterol.9 \n#&gt;      179.63656       67.31160       59.06667       25.12857      179.58295 \n#&gt;  diastolicBP.9         age.10         bmi.10 cholesterol.10 diastolicBP.10 \n#&gt;       66.37078       55.63333       25.12857      179.69042       67.60832\n\nStep 2\n\nimputation4\n#&gt; Class: mids\n#&gt; Number of multiple imputations:  10 \n#&gt; Imputation methods:\n#&gt;            age            bmi    cholesterol    diastolicBP \n#&gt;          \"pmm\"         \"mean\" \"norm.predict\"     \"norm.nob\" \n#&gt; PredictorMatrix:\n#&gt;             age bmi cholesterol diastolicBP\n#&gt; age           0   1           1           1\n#&gt; bmi           0   0           0           0\n#&gt; cholesterol   1   1           0           1\n#&gt; diastolicBP   1   1           1           0\n\n\nimputation4[[1]]\n\n\n  \n\n\n\n\nmice::complete(imputation4, action = 1)\n\n\n  \n\n\n\n\nmice::complete(imputation4, action = 10)\n\n\n  \n\n\n\n\n# Step 2 Analyze the imputed data\nfit4 &lt;- with(data = imputation4, exp = lm(cholesterol ~ age + bmi + diastolicBP))\n## fit model with each of 10 datasets separately\nfit4\n#&gt; call :\n#&gt; with.mids(data = imputation4, expr = lm(cholesterol ~ age + bmi + \n#&gt;     diastolicBP))\n#&gt; \n#&gt; call1 :\n#&gt; mice(data = NHANES17s, m = 10, method = meth, predictorMatrix = predictor.selection, \n#&gt;     maxit = 3, seed = 504)\n#&gt; \n#&gt; nmis :\n#&gt; [1] 10  2  7 10\n#&gt; \n#&gt; analyses :\n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;   210.68650     -0.19085     -0.52112     -0.09498  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;   185.56395      0.06366     -0.49154      0.04196  \n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;   188.01460     -0.07922     -0.52325      0.14493  \n#&gt; \n#&gt; \n#&gt; [[4]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;    210.2814       0.1096      -0.4602      -0.3802  \n#&gt; \n#&gt; \n#&gt; [[5]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;    167.8965       0.7171      -0.7613      -0.1090  \n#&gt; \n#&gt; \n#&gt; [[6]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;    187.4324       0.1727      -0.3452      -0.1482  \n#&gt; \n#&gt; \n#&gt; [[7]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;   203.45344     -0.22713     -0.37039     -0.02806  \n#&gt; \n#&gt; \n#&gt; [[8]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;  213.721570    -0.003491    -0.517870    -0.310132  \n#&gt; \n#&gt; \n#&gt; [[9]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;   205.74248     -0.16224     -0.46983     -0.07188  \n#&gt; \n#&gt; \n#&gt; [[10]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = cholesterol ~ age + bmi + diastolicBP)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          age          bmi  diastolicBP  \n#&gt;    181.2751       0.0698      -0.5425       0.1208\n\nStep 3\nUnderstanding the pooled results\nWe will show the result of entire pool later. First we want to show the pooled results for the age variable only an an example.\n\nrequire(dplyr)\nres10 &lt;- summary(fit4) %&gt;% as_tibble %&gt;% print(n=40)\n#&gt; # A tibble: 40 × 7\n#&gt;    term         estimate std.error statistic   p.value  nobs df.residual\n#&gt;    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n#&gt;  1 (Intercept) 211.         53.6     3.93    0.000561     30          26\n#&gt;  2 age          -0.191       0.452  -0.422   0.676        30          26\n#&gt;  3 bmi          -0.521       0.943  -0.553   0.585        30          26\n#&gt;  4 diastolicBP  -0.0950      0.563  -0.169   0.867        30          26\n#&gt;  5 (Intercept) 186.         47.9     3.88    0.000644     30          26\n#&gt;  6 age           0.0637      0.459   0.139   0.891        30          26\n#&gt;  7 bmi          -0.492       0.939  -0.524   0.605        30          26\n#&gt;  8 diastolicBP   0.0420      0.533   0.0787  0.938        30          26\n#&gt;  9 (Intercept) 188.         48.9     3.85    0.000694     30          26\n#&gt; 10 age          -0.0792      0.470  -0.169   0.867        30          26\n#&gt; 11 bmi          -0.523       0.926  -0.565   0.577        30          26\n#&gt; 12 diastolicBP   0.145       0.547   0.265   0.793        30          26\n#&gt; 13 (Intercept) 210.         38.6     5.44    0.0000105    30          26\n#&gt; 14 age           0.110       0.372   0.295   0.771        30          26\n#&gt; 15 bmi          -0.460       0.950  -0.485   0.632        30          26\n#&gt; 16 diastolicBP  -0.380       0.534  -0.712   0.483        30          26\n#&gt; 17 (Intercept) 168.         39.6     4.24    0.000253     30          26\n#&gt; 18 age           0.717       0.299   2.39    0.0241       30          26\n#&gt; 19 bmi          -0.761       0.898  -0.848   0.404        30          26\n#&gt; 20 diastolicBP  -0.109       0.500  -0.218   0.829        30          26\n#&gt; 21 (Intercept) 187.         57.1     3.28    0.00294      30          26\n#&gt; 22 age           0.173       0.437   0.395   0.696        30          26\n#&gt; 23 bmi          -0.345       0.943  -0.366   0.717        30          26\n#&gt; 24 diastolicBP  -0.148       0.569  -0.260   0.797        30          26\n#&gt; 25 (Intercept) 203.         48.5     4.20    0.000278     30          26\n#&gt; 26 age          -0.227       0.390  -0.583   0.565        30          26\n#&gt; 27 bmi          -0.370       0.921  -0.402   0.691        30          26\n#&gt; 28 diastolicBP  -0.0281      0.536  -0.0523  0.959        30          26\n#&gt; 29 (Intercept) 214.         51.5     4.15    0.000313     30          26\n#&gt; 30 age          -0.00349     0.450  -0.00776 0.994        30          26\n#&gt; 31 bmi          -0.518       0.927  -0.559   0.581        30          26\n#&gt; 32 diastolicBP  -0.310       0.525  -0.590   0.560        30          26\n#&gt; 33 (Intercept) 206.         47.8     4.30    0.000213     30          26\n#&gt; 34 age          -0.162       0.392  -0.414   0.682        30          26\n#&gt; 35 bmi          -0.470       0.921  -0.510   0.614        30          26\n#&gt; 36 diastolicBP  -0.0719      0.523  -0.137   0.892        30          26\n#&gt; 37 (Intercept) 181.         44.4     4.08    0.000379     30          26\n#&gt; 38 age           0.0698      0.353   0.198   0.845        30          26\n#&gt; 39 bmi          -0.543       0.970  -0.559   0.581        30          26\n#&gt; 40 diastolicBP   0.121       0.558   0.216   0.830        30          26\nm10 &lt;- res10[res10$term == \"age\",]\nm10\n\n\n  \n\n\n\nLet us describe the components of a pool for the age variable only:\n\nm.number &lt;- 10\n# estimate = pooled estimate \n# = sum of (m “beta-hat” estimates) / m (mean of m estimated statistics)\nestimate &lt;- mean(m10$estimate)\nestimate\n#&gt; [1] 0.04699243\n# ubar = sum of (m variance[beta] estimates) / m \n# = within-imputation variance (mean of estimated variances)\nubar.var &lt;- mean(m10$std.error^2)\nubar.var\n#&gt; [1] 0.1686837\n# b =  variance of (m “beta-hat” estimates) \n# = between-imputation variance \n# (degree to which estimated statistic / \n# “beta-hat” varies across m imputed datasets). \n# This b is not available for single imputation when m = 1.\nb.var &lt;- var(m10$estimate)\nb.var\n#&gt; [1] 0.07372796\n# t = ubar + b + b/m = total variance according to Rubin’s rules \n# (within-imputation & between imputation variation)\nt.var &lt;- ubar.var + b.var + b.var/m.number\nt.var\n#&gt; [1] 0.2497845\n# riv = relative increase in variance\nriv = (b.var + b.var/m.number)/ubar.var\nriv\n#&gt; [1] 0.4807859\n# lambda = proportion of variance to due nonresponse\nlambda = (b.var + b.var/m.number)/t.var\nlambda\n#&gt; [1] 0.3246829\n# df (approximate for large sample without correction)\ndf.large.sample &lt;- (m.number - 1)/lambda^2\ndf.large.sample\n#&gt; [1] 85.37359\n# df (hypothetical complete data)\ndfcom &lt;- m10$nobs[1] - 4 # n = 30, # parameters = 4\ndfcom\n#&gt; [1] 26\n# df (Barnard-Rubin correction)\ndf.obs &lt;- (dfcom + 1)/(dfcom + 3) * dfcom * (1 - lambda)\ndf.c &lt;- df.large.sample * df.obs/(df.large.sample + df.obs)\ndf.c\n#&gt; [1] 13.72019\n# fmi = fraction of missing information per parameter\nfmi = (riv + 2/(df.large.sample +3)) / (1 + riv)\nfmi # based on large sample approximation\n#&gt; [1] 0.3399662\nfmi = (riv + 2/(df.c +3)) / (1 + riv)\nfmi # Barnard-Rubin correction\n#&gt; [1] 0.4054616\n\nPooled estimate\nCompare above results with the pooled table from mice below. Note that df is based on Barnard-Rubin correction and fmi value is calculated based on that corrected df.\n\n# Step 3 pool the analysis results\nest1 &lt;- mice::pool(fit4)\n## pool all estimated together using Rubin's rule \nest1\n\nClass: mipo    m = 10 (transposed version to accommodate space)\n\n\nTerm\n(Intercept)\nage\nbmi\ndiastolicBP\n\n\n\nm\n10\n10\n10\n10\n\n\nEstimate\n195.40679314\n0.04699243\n-0.50032666\n-0.08347279\n\n\n\\(\\bar{u}\\)\n2313.6362339\n0.1686837\n0.8722547\n0.2909291\n\n\nb\n237.04075365\n0.07372796\n0.01274940\n0.02857675\n\n\nt\n2574.3810629\n0.2497845\n0.8862790\n0.3223635\n\n\ndf_com\n26\n26\n26\n26\n\n\ndf\n21.22870\n13.72019\n23.80807\n21.35356\n\n\nRIV\n0.11269915\n0.48078595\n0.01607826\n0.10804843\n\n\n\\(\\lambda\\)\n0.10128447\n0.32468295\n0.01582384\n0.09751237\n\n\nFMI\n0.17547051\n0.40546159\n0.08924771\n0.17162783\n\n\n\nHere:\n\ndfcom = df for complete data\ndf = df with Barnard-Rubin correction\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSpecial case: Variable selection\nVariable selection in analyzing missing data\n\nRef: (Van Buuren 2018), Section 5.4\n\nThe common workflow for analyzing missing data are (as mentioned above):\n\nImputing the data \\(m\\) times\nAnalyzing the \\(m\\) dataset\nPool all analysis together\n\nWe could apply variable selection in step 2, especially when we have no idea what is the best model to analyze the data. Howevere, it may become challenging when we pull all data together. With different dataset, the final model may or may not be the same.\nWe present the three method of variable selection on each imputed dataset presented by Buuren:\n\nMajority: perform the model selection separately with m dataset and choose the variables that appears at least m/2 times\nStack: combine m datasets into a single dataset, and perform variable selection on this dataset\nWald (Rubin’s rule): model selection was performed at model fitting step and combine the estimates using Rubin’s rules. This is considered as gold standard.\n\nMajority using NHANES17s\n\ndata &lt;- NHANES17s\nimp &lt;- mice(data, seed = 504, m = 100, print = FALSE)\n## Multiple imputation with 100 imputations, resulting in 100 imputed datasets\nscope0 &lt;- list(upper = ~ age + bmi + cholesterol, lower = ~1)\nexpr &lt;- expression(f1 &lt;- lm(diastolicBP ~ age),\n                   f2 &lt;- step(f1, scope = scope0, trace = FALSE))\nfit5 &lt;- with(imp, expr)\n\n## apply stepwise on each of the imputed dataset separately\nformulas &lt;- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms &lt;- lapply(formulas, terms)\nvotes &lt;- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#&gt; votes\n#&gt;         age         bmi cholesterol \n#&gt;           6          12           1\n\n\n## Set up the stepwise variable selection, from null model to full model\nscope &lt;- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\n## Set up the stepwise variable selection, from important only model to full model\nexpr &lt;- expression(f1 &lt;- lm(diastolicBP ~ age),\n                   f2 &lt;- step(f1, scope = scope, trace = FALSE))\nfit5 &lt;- with(imp, expr)\n## apply stepwise on each of the imputed dataset separately\nformulas &lt;- lapply(fit5$analyses, formula)\n## fit5$analyses returns the selection result for each imputed dataset\nterms &lt;- lapply(formulas, terms)\nvotes &lt;- unlist(lapply(terms, labels))\n## look at the terms on each models\ntable(votes)\n#&gt; votes\n#&gt;         age         bmi cholesterol \n#&gt;         100          11           1\n\nStack using NHANES17s\n\nStack.data &lt;- mice::complete(imp, action=\"long\")\nhead(Stack.data)\n\n\n  \n\n\ntail(Stack.data)\n\n\n  \n\n\nfitx &lt;- lm(diastolicBP ~ age + bmi + cholesterol, data = Stack.data)\nfity &lt;- step(fitx, scope = scope0, trace = FALSE)\nrequire(Publish)\n#&gt; Loading required package: Publish\n#&gt; Loading required package: prodlim\npublish(fity)\n#&gt;     Variable Units Coefficient         CI.95 p-value \n#&gt;  (Intercept)             63.70 [62.12;65.28] &lt; 1e-04 \n#&gt;          bmi              0.14   [0.08;0.20] &lt; 1e-04\n\nWald using NHANES17s\n\n# m = 100\nfit7 &lt;- with(data=imp, expr=lm(diastolicBP ~ 1))\nnames(fit7)\n#&gt; [1] \"call\"     \"call1\"    \"nmis\"     \"analyses\"\nfit7$analyses[[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diastolicBP ~ 1)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  \n#&gt;       68.47\nfit7$analyses[[100]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diastolicBP ~ 1)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  \n#&gt;       65.47\nfit8 &lt;- with(data=imp, expr=lm(diastolicBP ~ bmi))\nfit8$analyses[[45]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diastolicBP ~ bmi)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          bmi  \n#&gt;    63.93092      0.09209\nfit8$analyses[[99]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diastolicBP ~ bmi)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          bmi  \n#&gt;    68.34740      0.01797\n# The D1-statistics is the multivariate Wald test.\nstat &lt;- D1(fit8, fit7)\n## use Wald test to see if we should add bmi into the model\nstat\n#&gt;    test statistic df1      df2 dfcom   p.value       riv\n#&gt;  1 ~~ 2 0.1215668   1 22.70437    28 0.7305539 0.5550013\n# which indicates that adding bmi into our model might not be useful\n\n\nfit9 &lt;- with(data=imp, expr=lm(diastolicBP ~ age + bmi))\nstat &lt;- D1(fit9, fit8)\n## use Wald test to see if we should add age into the model\nstat\n#&gt;    test   statistic df1      df2 dfcom   p.value       riv\n#&gt;  1 ~~ 2 0.006608523   1 22.46746    27 0.9359289 0.4545242\n# which indicates that adding age into our model might not be useful\n\n\nfit10 &lt;- with(data=imp, expr=lm(diastolicBP ~ age + bmi + cholesterol))\nstat &lt;- D1(fit10, fit9)\n## use Wald test to see if we should add cholesterol into the model\nstat\n#&gt;    test    statistic df1      df2 dfcom   p.value       riv\n#&gt;  1 ~~ 2 0.0003547819   1 22.14158    26 0.9851409 0.3615345\n# which indicates that adding cholesterol into our model might not be useful\n\nTry method=\"likelihood\" as well.\n\nstat &lt;- pool.compare(fit10, fit7, method = \"likelihood\", data=imp)\n## test to see if we should add all 3 variables into the model\nstat$pvalue\n#&gt; [1] 0.9432629\n# which indicates that adding none of the variables into our model might be useful\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nAssess the impact of missing values in model fitting\nWhen working with datasets, missing values are a common challenge. These gaps in data can introduce biases and uncertainties, especially when we try to fit models to the data. To address this, researchers often use imputation methods to fill in the missing values based on the observed information. However, imputation itself can introduce uncertainties. Therefore, it’s essential to assess the impact of these missing values on model fitting. Buuren, as referenced in (Van Buuren 2018) Section 5.4.3, provides methods to do this. Out of the four methods presented by Buuren, the following two are the most commonly used:\n\nMultiple imputation with more number of imputations (i.e., 200). Perform variable selection on each imputed dataset. The differences are attributed to the missing values\nBootstrapping the data from a single imputed dataset and do variable selection for each bootstrapping sample. We could evaluate sampling variation using this method\n\nBootstrap using NHANES17s\n\nimpx &lt;- mice(NHANES17s, seed = 504, m=1, print=FALSE)\ncompletedata &lt;- mice::complete(impx)\n  \nset.seed(504) \nvotes &lt;-c()\nformula0 &lt;- as.formula(\"diastolicBP ~ age + bmi + cholesterol\")\nscope &lt;- list(upper = ~ age + bmi + cholesterol, lower = ~ age)\n\nfor (i in 1:200){\n     ind &lt;- sample(1:nrow(completedata),replace = TRUE)\n     newdata &lt;- completedata[ind,]\n     full.model &lt;- glm(formula0, data = newdata)\n     f2 &lt;- MASS::stepAIC(full.model, \n                   scope = scope, trace = FALSE)\n     formulas &lt;- as.formula(f2)\n     temp &lt;- unlist(labels(terms(formulas)))\n     votes &lt;- c(votes,temp)\n }\n table(votes)\n#&gt; votes\n#&gt;         age         bmi cholesterol \n#&gt;         200          59          17\n ## among 200 bootstrap samples how many times that each \n ## variable appears in the final model. Models have different\n ## variables are attributed to sampling variation\n\nConvergence and diagnostics\nConvergence\nWhen an algorithm converges, it means that the sequence of estimates generated by the algorithm stabilizes and reaches a distribution that does not depend on the initial values. This is crucial for imputation and other statistical analyses because it indicates that the estimates are representative of the true underlying statistical properties and are not biased by initial assumptions or specific starting conditions.\n\nRef: (Van Buuren 2018), Section 6.5.2\nMCMC Algorithm in MICE: The MICE package implements a MCMC algorithm for imputation. The coefficients should be converged and irrelevant to the order which variable is imputed first.\nUnderstanding pattern: For convergence to be achieved, these chains should mix well with each other, meaning their paths should overlap and crisscross freely. If they show distinct, separate trends or paths, it indicates a lack of convergence, suggesting that the imputation may not be reliable.\nVisualizing Convergence: We could plot the imputation object to see the streams.\n\n\n## Recall the imputation we have done before\nimputation5 &lt;- mice(NHANES17s, seed = 504, \n                   m=10,\n                   maxit = 5,\n                   print=FALSE) \nplot(imputation5)\n\n\n\n\n\n\n\n\n\n\n\n\n## We hope to see no pattern in the trace lines\n## Sometimes to comfirm this we may want to run with more iterations\nimputation5_2 &lt;- mice(NHANES17s, seed = 504, \n                    m=10,\n                    maxit = 50,\n                    print=FALSE) \nplot(imputation5_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics\nModel diagnostics plays a pivotal role in ensuring the robustness and accuracy of model fitting. Particularly in the realm of missing value imputations, where observed data serves as the foundation for estimating absent values, it becomes imperative to rigorously assess the imputation process. A straightforward diagnostic technique involves comparing the distributions of the observed data with the imputed values, especially when segmented or conditioned based on the variables that originally had missing entries. This comparison helps in discerning any discrepancies or biases introduced during the imputation, ensuring that the filled values align well with the inherent patterns of the observed data.\n\nRef: (Van Buuren 2018), Section 6.6\n\n\n## We could compare the imputed and observed data using Density plots\ndensityplot(imputation5, layout = c(2, 2))\n\n\n\n\n\n\nimputation5_3 &lt;- mice(NHANES17s, seed = 504, \n                    m=50,\n                    maxit = 50,\n                    print=FALSE)\ndensityplot(imputation5_3)\n\n\n\n\n\n\n## a subjective judgment on whether you think if there is significant discrepancy\nbwplot(imputation5, age + bmi + cholesterol +diastolicBP ~ .imp, layout = c(2, 2))\n\n\n\n\n\n\nbwplot(imputation5_3)\n\n\n\n\n\n\n## Plot a box plot to compare the imputed and observed values\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nLittle, RJA, and DB Rubin. 1987. “Multiple Imputation for Nonresponse in Surveys.” John Wiley & Sons, Inc.. Doi 10: 9780470316696.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45: 1–67.",
    "crumbs": [
      "Missing data analysis",
      "Imputation"
    ]
  },
  {
    "objectID": "missingdata2.html",
    "href": "missingdata2.html",
    "title": "Imputation in NHANES",
    "section": "",
    "text": "This tutorial provides a comprehensive guide on handling and analyzing complex survey data with missing values. In analyzing complex survey data, a distinct approach is required compared to handling regular datasets. Specifically, the intricacies of survey design necessitate the consideration of primary sampling units/cluster, sampling weights, and stratification factors. These elements ensure that the analysis accurately reflects the survey’s design and the underlying population structure. Recognizing and incorporating these factors is crucial for obtaining valid and representative insights from the data. As we delve into this tutorial, we’ll explore how to effectively integrate these components into our missing data analysis process.\nComplex Survey data\nIn the initial chunk, we load all the necessary libraries that will be used throughout the tutorial. These libraries provide functions and tools for data exploration, visualization, imputation, and analysis.\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\nNext, we load a dataset that contains survey data with some missing values. We then select specific columns or variables from this dataset that we are interested in analyzing. To understand the extent and pattern of missingness in our data, we visualize it and display the missing data pattern.\n\nload(\"Data/missingdata/NHANES17.RData\")\n\nVars &lt;- c(\"ID\", \"weight\", \"psu\", \"strata\", \n          \"gender\", \"born\", \"race\", \n          \"bmi\", \"cholesterol\", \"diabetes\")\nanalyticx &lt;- analytic.with.miss[,Vars]\nplot_missing(analyticx)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\nmd.pattern(analyticx)\n\n\n\n\n\n\n#&gt;      ID weight psu strata gender race born diabetes  bmi cholesterol     \n#&gt; 6636  1      1   1      1      1    1    1        1    1           1    0\n#&gt; 1364  1      1   1      1      1    1    1        1    1           0    1\n#&gt; 97    1      1   1      1      1    1    1        1    0           1    1\n#&gt; 795   1      1   1      1      1    1    1        1    0           0    2\n#&gt; 4     1      1   1      1      1    1    1        0    1           1    1\n#&gt; 357   1      1   1      1      1    1    1        0    0           0    3\n#&gt; 1     1      1   1      1      1    1    0        1    1           1    1\n#&gt;       0      0   0      0      0    0    1      361 1249        2516 4127\n\nImputing\nIn the following chunk, we address the missing data by performing multiple imputations. This means that instead of filling in each missing value with a single estimate, we create multiple versions (datasets) where each missing value is filled in differently based on a specified algorithm. This helps in capturing the uncertainty around the missing values. The chunk sets up the parameters for multiple imputations, ensuring reproducibility and efficiency, and then performs the imputations on the dataset with missing values.\n\n# imputation &lt;- mice(analyticx, m=5, maxit=5, seed = 504007)\nset.seed(504)\nimputation &lt;- parlmice(analyticx, m=5, maxit=5, cluster.seed=504007)\n#&gt; Warning in parlmice(analyticx, m = 5, maxit = 5, cluster.seed = 504007): 'parlmice' is deprecated.\n#&gt; Use 'futuremice' instead.\n#&gt; See help(\"Deprecated\")\n\n\n\nData Input: The primary input for the imputation function is the dataset with missing values. This dataset is what we aim to impute.\n\nNumber of Imputations: The option m=5 indicates that we want to create 5 different imputed datasets. Each of these datasets will have the missing values filled in slightly differently, based on the underlying imputation algorithm and the randomness introduced.\n\nMaximum Iterations: The imputation process is iterative, meaning it refines its estimates over several rounds. The option maxit=5 specifies that the algorithm should run for a maximum of 5 iterations. This helps in achieving more accurate imputations, especially when the missing data mechanism is complex.\n\nSetting Seed: In computational processes that involve randomness, it’s often useful to set a “seed” value. This ensures that the random processes are reproducible. If you run the imputation multiple times with the same seed, you’ll get the same results each time. Two seed values are set in the chunk: one using the general set.seed() function and another specifically for the imputation function as cluster.seed.\n\nParallel Processing: The function parlmice used for imputation indicates that the imputations are done in parallel. This means that instead of imputing one dataset after the other, the function tries to impute multiple datasets simultaneously (if the computational resources allow). This can speed up the process, especially when dealing with large datasets or when creating many imputed datasets.\nCreate new variable\nAfter imputation, we might want to create new variables or modify existing ones to better suit our analysis. Here, we transform one of the variables into a binary category based on a threshold. The choice of thresholds should be based on the data (i.e. median, quintiles, etc.) or on the literature (clinically relevant). This can help in simplifying the analysis or making the results more interpretable.\n\nimpdata &lt;- complete(imputation, action=\"long\") #stacked data\nimpdata$cholesterol.bin &lt;- ifelse(impdata$cholesterol &lt; 200, \"healthy\", \"unhealthy\")\nimpdata$cholesterol.bin &lt;- as.factor(impdata$cholesterol.bin)\ndim(impdata)\n#&gt; [1] 46270    13\nhead(impdata)\n\n\n  \n\n\n\nChecking the data\nAfter imputation, it’s crucial to ensure that the imputed data maintains the integrity and structure of the original dataset. The following chunks are designed to help you visually and programmatically inspect the imputed data.\nVisual Inspection of Missing Data:\nIn this chunk, we visually inspect the imputed datasets to see if there are any remaining missing values. We specifically look at the first two imputed datasets. Visualization tools like these can quickly show if the imputation process was successful in filling all missing values.\n\nplot_missing(subset(impdata, subset=.imp==1))\n\n\n\n\n\n\nplot_missing(subset(impdata, subset=.imp==2))\n\n\n\n\n\n\n\nComparing Original and Imputed Data (First Imputed Dataset):\n\nIn this chunk, we focus on the first imputed dataset. We extract this dataset and display the initial entries to get a sense of the data.\nWe then remove any remaining missing values (if any) and display the initial entries of this complete dataset.\nNext, we compare the IDs (or unique identifiers) of the entries in the complete dataset with the original dataset to see which entries had missing values.\nWe then create a new variable that indicates whether an entry had missing values or not and tabulate this information.\n\n\nanalytic.miss1 &lt;- subset(impdata, subset=.imp==1)\nhead(analytic.miss1$ID) # full data\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nanalytic1 &lt;- as.data.frame(na.omit(analytic.miss1))\nhead(analytic1$ID) # complete case\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss1$ID[analytic.miss1$ID %in% analytic1$ID])\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss1$miss &lt;- 1\nanalytic.miss1$miss[analytic.miss1$ID %in% analytic1$ID] &lt;- 0\ntable(analytic.miss1$miss)\n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#&gt; [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#&gt; [1] 102840 102862 102919 102927 102928 102942\n\nComparing Original and Imputed Data (Second Imputed Dataset):\nThe this chunk is similar to the above but focuses on the second imputed dataset. We perform the same steps: extracting the dataset, removing missing values, comparing IDs, and creating a variable to indicate missingness.\n\nanalytic.miss2 &lt;- subset(impdata, subset=.imp==2)\nhead(analytic.miss2$ID) # full data\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nanalytic2 &lt;- as.data.frame(na.omit(analytic.miss2))\nhead(analytic2$ID) # complete case\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nhead(analytic.miss2$ID[analytic.miss2$ID %in% analytic2$ID])\n#&gt; [1] 93703 93704 93705 93706 93707 93708\n\nanalytic.miss2$miss &lt;- 1\nanalytic.miss2$miss[analytic.miss2$ID %in% analytic2$ID] &lt;- 0\ntable(analytic.miss2$miss)\n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362\n\nhead(analytic.miss1$ID[analytic.miss1$miss==1])\n#&gt; [1] 93710 93748 93786 93854 93865 93934\ntail(analytic.miss1$ID[analytic.miss1$miss==1])\n#&gt; [1] 102840 102862 102919 102927 102928 102942\n\nAggregating Missingness Information Across All Imputed Datasets:\n\nIn the fourth chunk, we aim to consolidate the missingness information across all imputed datasets. We initialize a variable in the main dataset to indicate missingness.\nWe then loop through each of the imputed datasets and update the main dataset’s missingness variable based on the missingness information from each imputed dataset. This gives us a consolidated view of which entries had missing values across all imputed datasets.\n\n\nimpdata$miss&lt;-1\nm &lt;- 5\nfor (i in 1:m){\n  impdata$miss[impdata$.imp == i] &lt;- analytic.miss2$miss\n  print(table(impdata$miss[impdata$.imp == i]))\n}\n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362 \n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362 \n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362 \n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362 \n#&gt; \n#&gt;    0    1 \n#&gt; 8892  362\n\nCombining data\nSince we have multiple versions of the imputed dataset, we need a way to combine them for analysis. In the next chunks, we use a method to merge these datasets into a single list, making it easier to apply subsequent analyses on all datasets simultaneously.\n\nlibrary(mitools) \nallImputations &lt;- imputationList(list(\n  subset(impdata, subset=.imp==1),\n  subset(impdata, subset=.imp==2),\n  subset(impdata, subset=.imp==3),\n  subset(impdata, subset=.imp==4), \n  subset(impdata, subset=.imp==5)))\nstr(allImputations)\n#&gt; List of 2\n#&gt;  $ imputations:List of 5\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#&gt;  - attr(*, \"class\")= chr \"imputationList\"\n\nCombining data efficiently\n\nm &lt;- 5\nset.seed(123)\nallImputations &lt;-  imputationList(lapply(1:m, \n                                         function(n)  \n                                           subset(impdata, subset=.imp==n)))\n                                           #mice::complete(imputation, action = n)))\nsummary(allImputations)\n#&gt;             Length Class  Mode\n#&gt; imputations 5      -none- list\n#&gt; call        2      -none- call\nstr(allImputations)\n#&gt; List of 2\n#&gt;  $ imputations:List of 5\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 18.3 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 160 186 157 148 189 209 176 162 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 44.8 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 107 153 157 148 189 209 176 195 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 13.6 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 153 110 157 148 189 209 176 141 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 19.2 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 160 159 157 148 189 209 176 196 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 4 4 4 4 4 4 4 4 4 4 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 1 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID             : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ weight         : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ psu            : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata         : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ gender         : chr [1:9254] \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;   .. ..$ born           : chr [1:9254] \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;   .. ..$ race           : chr [1:9254] \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;   .. ..$ bmi            : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 26 21.3 19.7 ...\n#&gt;   .. ..$ cholesterol    : int [1:9254] 132 198 157 148 189 209 176 255 238 182 ...\n#&gt;   .. ..$ diabetes       : chr [1:9254] \"No\" \"No\" \"No\" \"No\" ...\n#&gt;   .. ..$ .imp           : int [1:9254] 5 5 5 5 5 5 5 5 5 5 ...\n#&gt;   .. ..$ .id            : int [1:9254] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ cholesterol.bin: Factor w/ 2 levels \"healthy\",\"unhealthy\": 1 1 1 1 1 2 1 2 2 1 ...\n#&gt;   .. ..$ miss           : num [1:9254] 0 0 0 0 0 0 0 1 0 0 ...\n#&gt;  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#&gt;  - attr(*, \"class\")= chr \"imputationList\"\n\nLogistic regression\nWith our imputed datasets ready, we proceed to fit a statistical model. Here, we use logistic regression as an example. We fit the model to each imputed dataset separately and then extract relevant statistics like odds ratios and confidence intervals.\n\nrequire(jtools)\nrequire(survey)\ndata.list &lt;- vector(\"list\", m)\nmodel.formula &lt;- as.formula(\"I(cholesterol.bin=='healthy')~diabetes+gender+born+race+bmi\")\n\n\nsummary(allImputations$imputations[[1]]$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12347   21060   34671   37562  419763\nsum(allImputations$imputations[[1]]$weight==0)\n#&gt; [1] 550\nw.design0 &lt;- svydesign(ids=~psu, weights=~weight, strata=~strata,\n                           data = allImputations, nest = TRUE)\nw.design &lt;- subset(w.design0, miss == 0)\nfits &lt;- with(w.design, svyglm(model.formula, family=quasibinomial))\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n# Estimate from first data\nexp(coef(fits[[1]]))[2]\n#&gt; diabetesYes \n#&gt;    1.409246\nexp(confint(fits[[1]]))[2,]\n#&gt;    2.5 %   97.5 % \n#&gt; 1.129997 1.757503\n# Estimate from second data\nexp(coef(fits[[2]]))[2]\n#&gt; diabetesYes \n#&gt;    1.437951\nexp(confint(fits[[2]]))[2,]\n#&gt;    2.5 %   97.5 % \n#&gt; 1.171160 1.765518\n\nPooled / averaged estimates\nAfter analyzing each imputed dataset separately, we need to combine the results to get a single set of estimates. This is done using a method that pools the results, taking into account the variability between the different imputed datasets.\n\npooled.estimates &lt;- MIcombine(fits)\nsum.pooled &lt;- summary(pooled.estimates)\n#&gt; Multiple imputation results:\n#&gt;       with(w.design, svyglm(model.formula, family = quasibinomial))\n#&gt;       MIcombine.default(fits)\n#&gt;                   results          se        (lower       upper) missInfo\n#&gt; (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#&gt; diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#&gt; genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#&gt; bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#&gt; bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#&gt; raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#&gt; raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#&gt; raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#&gt; bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nexp(sum.pooled[,1])\n#&gt; [1] 8.458294e+00 1.422037e+00 1.193362e+00 5.507777e-01 4.405626e-06\n#&gt; [6] 1.149279e+00 8.407373e-01 6.989885e-01 9.606814e-01\nOR &lt;- round(exp(pooled.estimates$coefficients),2) \nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)),2)\nsig &lt;- (CI[,1] &lt; 1 & CI[,2] &gt; 1)\nsig &lt;- ifelse(sig==FALSE, \"*\", \"\")\nOR &lt;- cbind(OR,CI,sig)\nOR\n\n\n  \n\n\n\nStep-by-step example\nThis segment offers a hands-on approach to understanding the imputation process. Here’s a breakdown:\n\n\nFitting Models to Individual Imputed Datasets:\n\nA list is initialized to store the results of models fitted to each imputed dataset.\nFor every dataset, the specific imputed data is extracted.\nA survey design is established, considering factors like primary sampling units, stratification, and weights. This ensures the analysis aligns with the survey’s design.\nThis design is then refined to only consider complete data entries.\nA logistic regression model is then applied to this refined data.\nThe results of this modeling are stored and displayed for review.\n\n\n\nPooling Results from All Models:\n\nAfter individual analysis, the next step is to combine or ‘pool’ these results.\nA special function is used to merge the results from all the models. This function accounts for variations between datasets and offers a combined estimate.\nA summary of this combined data is then displayed, offering insights like coefficients, standard errors, and more. Another version of this summary, focusing on log-effects, is also presented for deeper insights.\n\n\n\n\nfits2 &lt;- vector(\"list\", m)\nfor (i in 1:m) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  w.design0.i &lt;- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i &lt;- subset(w.design0.i, miss == 0)\n  fit &lt;- svyglm(model.formula, design=w.design.i, \n                family = quasibinomial(\"logit\"))\n  print(summ(fit))\n  fits2[[i]] &lt;- fit\n}\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; MODEL INFO:\n#&gt; Observations: 8892\n#&gt; Dependent Variable: I(cholesterol.bin == \"healthy\")\n#&gt; Type: Analysis of complex survey design \n#&gt;  Family: quasibinomial \n#&gt;  Link function: logit \n#&gt; \n#&gt; MODEL FIT:\n#&gt; Pseudo-R² (Cragg-Uhler) = 0.05\n#&gt; Pseudo-R² (McFadden) = 0.03\n#&gt; AIC =  NA \n#&gt; \n#&gt; --------------------------------------------------\n#&gt;                        Est.   S.E.   t val.      p\n#&gt; ------------------ -------- ------ -------- ------\n#&gt; (Intercept)            2.26   0.20    11.32   0.00\n#&gt; diabetesYes            0.34   0.09     3.67   0.01\n#&gt; genderMale             0.17   0.09     1.88   0.10\n#&gt; bornOthers            -0.62   0.10    -6.36   0.00\n#&gt; bornRefused          -12.35   0.70   -17.63   0.00\n#&gt; raceHispanic           0.18   0.14     1.27   0.25\n#&gt; raceOther             -0.20   0.11    -1.77   0.12\n#&gt; raceWhite             -0.38   0.13    -2.83   0.03\n#&gt; bmi                   -0.04   0.01    -7.92   0.00\n#&gt; --------------------------------------------------\n#&gt; \n#&gt; Estimated dispersion parameter = 0.99\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; MODEL INFO:\n#&gt; Observations: 8892\n#&gt; Dependent Variable: I(cholesterol.bin == \"healthy\")\n#&gt; Type: Analysis of complex survey design \n#&gt;  Family: quasibinomial \n#&gt;  Link function: logit \n#&gt; \n#&gt; MODEL FIT:\n#&gt; Pseudo-R² (Cragg-Uhler) = 0.05\n#&gt; Pseudo-R² (McFadden) = 0.03\n#&gt; AIC =  NA \n#&gt; \n#&gt; --------------------------------------------------\n#&gt;                        Est.   S.E.   t val.      p\n#&gt; ------------------ -------- ------ -------- ------\n#&gt; (Intercept)            2.08   0.23     9.17   0.00\n#&gt; diabetesYes            0.36   0.09     4.19   0.00\n#&gt; genderMale             0.20   0.08     2.49   0.04\n#&gt; bornOthers            -0.63   0.08    -7.41   0.00\n#&gt; bornRefused          -12.33   0.71   -17.25   0.00\n#&gt; raceHispanic           0.13   0.14     0.95   0.37\n#&gt; raceOther             -0.16   0.10    -1.57   0.16\n#&gt; raceWhite             -0.37   0.14    -2.71   0.03\n#&gt; bmi                   -0.04   0.01    -6.05   0.00\n#&gt; --------------------------------------------------\n#&gt; \n#&gt; Estimated dispersion parameter = 1\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; MODEL INFO:\n#&gt; Observations: 8892\n#&gt; Dependent Variable: I(cholesterol.bin == \"healthy\")\n#&gt; Type: Analysis of complex survey design \n#&gt;  Family: quasibinomial \n#&gt;  Link function: logit \n#&gt; \n#&gt; MODEL FIT:\n#&gt; Pseudo-R² (Cragg-Uhler) = 0.04\n#&gt; Pseudo-R² (McFadden) = 0.02\n#&gt; AIC =  NA \n#&gt; \n#&gt; --------------------------------------------------\n#&gt;                        Est.   S.E.   t val.      p\n#&gt; ------------------ -------- ------ -------- ------\n#&gt; (Intercept)            2.03   0.22     9.22   0.00\n#&gt; diabetesYes            0.35   0.08     4.30   0.00\n#&gt; genderMale             0.17   0.09     1.88   0.10\n#&gt; bornOthers            -0.56   0.08    -7.21   0.00\n#&gt; bornRefused          -12.30   0.70   -17.49   0.00\n#&gt; raceHispanic           0.10   0.13     0.76   0.47\n#&gt; raceOther             -0.19   0.10    -1.81   0.11\n#&gt; raceWhite             -0.34   0.13    -2.59   0.04\n#&gt; bmi                   -0.04   0.01    -6.58   0.00\n#&gt; --------------------------------------------------\n#&gt; \n#&gt; Estimated dispersion parameter = 0.99\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; MODEL INFO:\n#&gt; Observations: 8892\n#&gt; Dependent Variable: I(cholesterol.bin == \"healthy\")\n#&gt; Type: Analysis of complex survey design \n#&gt;  Family: quasibinomial \n#&gt;  Link function: logit \n#&gt; \n#&gt; MODEL FIT:\n#&gt; Pseudo-R² (Cragg-Uhler) = 0.05\n#&gt; Pseudo-R² (McFadden) = 0.03\n#&gt; AIC =  NA \n#&gt; \n#&gt; --------------------------------------------------\n#&gt;                        Est.   S.E.   t val.      p\n#&gt; ------------------ -------- ------ -------- ------\n#&gt; (Intercept)            2.18   0.21    10.57   0.00\n#&gt; diabetesYes            0.36   0.09     3.75   0.01\n#&gt; genderMale             0.16   0.09     1.78   0.12\n#&gt; bornOthers            -0.57   0.10    -5.98   0.00\n#&gt; bornRefused          -12.35   0.71   -17.32   0.00\n#&gt; raceHispanic           0.12   0.14     0.83   0.43\n#&gt; raceOther             -0.16   0.10    -1.61   0.15\n#&gt; raceWhite             -0.34   0.13    -2.55   0.04\n#&gt; bmi                   -0.04   0.01    -7.11   0.00\n#&gt; --------------------------------------------------\n#&gt; \n#&gt; Estimated dispersion parameter = 0.99\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; MODEL INFO:\n#&gt; Observations: 8892\n#&gt; Dependent Variable: I(cholesterol.bin == \"healthy\")\n#&gt; Type: Analysis of complex survey design \n#&gt;  Family: quasibinomial \n#&gt;  Link function: logit \n#&gt; \n#&gt; MODEL FIT:\n#&gt; Pseudo-R² (Cragg-Uhler) = 0.05\n#&gt; Pseudo-R² (McFadden) = 0.03\n#&gt; AIC =  NA \n#&gt; \n#&gt; --------------------------------------------------\n#&gt;                        Est.   S.E.   t val.      p\n#&gt; ------------------ -------- ------ -------- ------\n#&gt; (Intercept)            2.12   0.22     9.67   0.00\n#&gt; diabetesYes            0.35   0.09     4.02   0.01\n#&gt; genderMale             0.19   0.08     2.30   0.06\n#&gt; bornOthers            -0.60   0.10    -6.06   0.00\n#&gt; bornRefused          -12.33   0.71   -17.40   0.00\n#&gt; raceHispanic           0.17   0.14     1.20   0.27\n#&gt; raceOther             -0.17   0.10    -1.64   0.14\n#&gt; raceWhite             -0.36   0.13    -2.78   0.03\n#&gt; bmi                   -0.04   0.01    -6.65   0.00\n#&gt; --------------------------------------------------\n#&gt; \n#&gt; Estimated dispersion parameter = 0.99\n\n\npooled.estimates &lt;- MIcombine(fits2)\nsummary(pooled.estimates)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fits2)\n#&gt;                   results          se        (lower       upper) missInfo\n#&gt; (Intercept)    2.13514754 0.236310332   1.667691110   2.60260397     19 %\n#&gt; diabetesYes    0.35209050 0.089197557   0.177261457   0.52691955      1 %\n#&gt; genderMale     0.17677460 0.088502097   0.003168621   0.35038058      5 %\n#&gt; bornOthers    -0.59642404 0.096782869  -0.786797572  -0.40605051     11 %\n#&gt; bornRefused  -12.33262819 0.708505945 -13.721274703 -10.94398169      0 %\n#&gt; raceHispanic   0.13913500 0.142910787  -0.141238429   0.41950842      6 %\n#&gt; raceOther     -0.17347602 0.105412185  -0.380201753   0.03324971      5 %\n#&gt; raceWhite     -0.35812104 0.134733737  -0.622240268  -0.09400181      2 %\n#&gt; bmi           -0.04011243 0.006523474  -0.053037557  -0.02718730     20 %\nsummary(pooled.estimates,logeffect=TRUE, digits = 2)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fits2)\n#&gt;              results      se  (lower  upper) missInfo\n#&gt; (Intercept)  8.5e+00 2.0e+00 5.3e+00 1.3e+01     19 %\n#&gt; diabetesYes  1.4e+00 1.3e-01 1.2e+00 1.7e+00      1 %\n#&gt; genderMale   1.2e+00 1.1e-01 1.0e+00 1.4e+00      5 %\n#&gt; bornOthers   5.5e-01 5.3e-02 4.6e-01 6.7e-01     11 %\n#&gt; bornRefused  4.4e-06 3.1e-06 1.1e-06 1.8e-05      0 %\n#&gt; raceHispanic 1.1e+00 1.6e-01 8.7e-01 1.5e+00      6 %\n#&gt; raceOther    8.4e-01 8.9e-02 6.8e-01 1.0e+00      5 %\n#&gt; raceWhite    7.0e-01 9.4e-02 5.4e-01 9.1e-01      2 %\n#&gt; bmi          9.6e-01 6.3e-03 9.5e-01 9.7e-01     20 %\n\nVariable selection\nSometimes, not all variables in the dataset are relevant for our analysis. In the final chunks, we apply a method to select the most relevant variables for our model. This can help in simplifying the model and improving its interpretability.\n\nrequire(jtools)\nrequire(survey)\ndata.list &lt;- vector(\"list\", m)\nmodel.formula &lt;- as.formula(\"cholesterol~diabetes+gender+born+race+bmi\")\nscope &lt;- list(upper = ~ diabetes+gender+born+race+bmi,\n              lower = ~ diabetes)\nfor (i in 1:m) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  w.design0.i &lt;- svydesign(id=~psu, strata=~strata, weights=~weight,\n                        data=analytic.i, nest = TRUE)\n  w.design.i &lt;- subset(w.design0.i, miss == 0)\n  fit &lt;- svyglm(model.formula, design=w.design.i)\n  fitstep &lt;- step(fit, scope = scope, trace = FALSE,\n                              direction = \"backward\")\n  data.list[[i]] &lt;- fitstep\n}\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(g): observations with zero weight not used for\n#&gt; calculating dispersion\n#&gt; Warning in summary.glm(glm.object): observations with zero weight not used for\n#&gt; calculating dispersion\n\nCheck out the variables selected\n\nx &lt;- all.vars(formula(fit))\nfor (i in 1:m) x &lt;- c(x, all.vars(formula(data.list[[i]])))\ntable(x)-1\n#&gt; x\n#&gt;         bmi        born cholesterol    diabetes      gender        race \n#&gt;           5           5           5           5           5           5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Missing data analysis",
      "Imputation in NHANES"
    ]
  },
  {
    "objectID": "missingdata3.html",
    "href": "missingdata3.html",
    "title": "Missing in outcome",
    "section": "",
    "text": "This section provides a theoretical background on the concept of Multiple Imputation and then Deletion (MID). It highlights the challenges of imputing dependent and exposure variables and introduces the idea of using auxiliary variables to aid imputation. The section also contrasts the results of traditional MI with MID, especially when the number of imputed datasets is high.\n\nOften researchers are reluctant to impute values in the dependent variable (and exposure variable). Particularly, for dependent variable, imputation might not help too much.\nHowever, if you have a good auxiliary variable (e.g., strongly correlated predictor, that are not used in the main analysis), often multiple imputation method can help. Use of auxiliary variables is one of the greatest strengths of MI methods.\nMI algorithm generally do not have any special treatment for dependent variable in its original form, and hence ignoring dependent variable completely may not be a good idea in many scenarios.\nMultiple imputation followed by deletion of imputed outcomes is known as MID. This is very popular, especially when you have high percentage missing values in the outcome variable (e.g., 20%-50%). For low missing % in outcome, the advantage can be minimal.\nWe are extending this idea to deletion of imputed exposures as well (researchers are often reluctant to impute primary exposure of interest).\nOriginal MI and MID may result in similar results when m (number of imputed datasets) is higher.\n\nData\nIn the initial chunk, we load several packages that provide functions and tools necessary for the subsequent analysis. These packages facilitate multiple imputation, data visualization, and statistical modeling among other tasks.\n\n# Load required packages\nlibrary(mice)\nlibrary(DataExplorer)\nlibrary(VIM)\nlibrary(jtools)\nlibrary(survey)\nlibrary(mitools)\n\nWe load the necessary data:\n\nload(\"Data/missingdata/NHANES17.RData\")\n\nThe data is briefly inspected to understand its structure. An identifier column is added to uniquely identify each row or observation in the dataset.\n\nrequire(mice)\nnhanes2\n\n\n  \n\n\nnhanes2$id &lt;- 1:nrow(nhanes2)\nnhanes2\n\n\n  \n\n\n\nOutcome and exposure has missing\nThis chunk focuses on identifying which rows have missing values in both the outcome and exposure variables. The outcome and exposure variables are crucial for the analysis, so understanding where they are missing is essential.\n\n# assume outcome = bmi and exposure = chl \nnhanes2.excludingYA &lt;- subset(nhanes2, !is.na(bmi) & !is.na(chl) )\nnhanes2.excludingYA # data without missing A and Y\n\n\n  \n\n\n# identify ids of subjects with missing A & Y \nnhanes2.excludingYA$id\n#&gt;  [1]  2  5  7  8  9 13 14 17 18 19 22 23 25\n\nImpute as usual\nUsing the entire dataset, missing values are imputed. This is done by first initializing an imputation model and then performing the imputation to create multiple datasets where missing values are filled in. The result is a list of datasets with imputed values. That means, we impute Y and A for now, as well as other covariates with missing values.\n\n# use full data to impute \nini &lt;- mice(nhanes2, pri = FALSE)\nini$method\n#&gt;      age      bmi      hyp      chl       id \n#&gt;       \"\"    \"pmm\" \"logreg\"    \"pmm\"       \"\"\npred &lt;- ini$predictorMatrix\npred\n#&gt;     age bmi hyp chl id\n#&gt; age   0   1   1   1  1\n#&gt; bmi   1   0   1   1  1\n#&gt; hyp   1   1   0   1  1\n#&gt; chl   1   1   1   0  1\n#&gt; id    1   1   1   1  0\npred[,\"id\"] &lt;- 0 # as this is not a predictor\nm &lt;- 5\nimp &lt;- mice(data=nhanes2, m=m, maxit=3, seed=504007)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  bmi  hyp  chl\n#&gt;   1   2  bmi  hyp  chl\n#&gt;   1   3  bmi  hyp  chl\n#&gt;   1   4  bmi  hyp  chl\n#&gt;   1   5  bmi  hyp  chl\n#&gt;   2   1  bmi  hyp  chl\n#&gt;   2   2  bmi  hyp  chl\n#&gt;   2   3  bmi  hyp  chl\n#&gt;   2   4  bmi  hyp  chl\n#&gt;   2   5  bmi  hyp  chl\n#&gt;   3   1  bmi  hyp  chl\n#&gt;   3   2  bmi  hyp  chl\n#&gt;   3   3  bmi  hyp  chl\n#&gt;   3   4  bmi  hyp  chl\n#&gt;   3   5  bmi  hyp  chl\n# list format in m data\nimpdata &lt;- mice::complete(imp, action = \"all\")\nimpdata # all IDs are present\n#&gt; $`1`\n#&gt;      age  bmi hyp chl id\n#&gt; 1  20-39 20.4  no 187  1\n#&gt; 2  40-59 22.7  no 187  2\n#&gt; 3  20-39 27.4  no 187  3\n#&gt; 4  60-99 25.5 yes 204  4\n#&gt; 5  20-39 20.4  no 113  5\n#&gt; 6  60-99 22.5 yes 184  6\n#&gt; 7  20-39 22.5  no 118  7\n#&gt; 8  20-39 30.1  no 187  8\n#&gt; 9  40-59 22.0  no 238  9\n#&gt; 10 40-59 20.4  no 199 10\n#&gt; 11 20-39 27.5  no 199 11\n#&gt; 12 40-59 26.3  no 284 12\n#&gt; 13 60-99 21.7  no 206 13\n#&gt; 14 40-59 28.7 yes 204 14\n#&gt; 15 20-39 29.6  no 187 15\n#&gt; 16 20-39 35.3 yes 204 16\n#&gt; 17 60-99 27.2 yes 284 17\n#&gt; 18 40-59 26.3 yes 199 18\n#&gt; 19 20-39 35.3  no 218 19\n#&gt; 20 60-99 25.5 yes 284 20\n#&gt; 21 20-39 35.3 yes 184 21\n#&gt; 22 20-39 33.2  no 229 22\n#&gt; 23 20-39 27.5  no 131 23\n#&gt; 24 60-99 24.9  no 218 24\n#&gt; 25 40-59 27.4  no 186 25\n#&gt; \n#&gt; $`2`\n#&gt;      age  bmi hyp chl id\n#&gt; 1  20-39 28.7 yes 238  1\n#&gt; 2  40-59 22.7  no 187  2\n#&gt; 3  20-39 30.1  no 187  3\n#&gt; 4  60-99 22.5 yes 218  4\n#&gt; 5  20-39 20.4  no 113  5\n#&gt; 6  60-99 20.4 yes 184  6\n#&gt; 7  20-39 22.5  no 118  7\n#&gt; 8  20-39 30.1  no 187  8\n#&gt; 9  40-59 22.0  no 238  9\n#&gt; 10 40-59 22.5 yes 238 10\n#&gt; 11 20-39 26.3 yes 118 11\n#&gt; 12 40-59 20.4  no 199 12\n#&gt; 13 60-99 21.7  no 206 13\n#&gt; 14 40-59 28.7 yes 204 14\n#&gt; 15 20-39 29.6  no 187 15\n#&gt; 16 20-39 24.9  no 238 16\n#&gt; 17 60-99 27.2 yes 284 17\n#&gt; 18 40-59 26.3 yes 199 18\n#&gt; 19 20-39 35.3  no 218 19\n#&gt; 20 60-99 25.5 yes 218 20\n#&gt; 21 20-39 27.5  no 187 21\n#&gt; 22 20-39 33.2  no 229 22\n#&gt; 23 20-39 27.5  no 131 23\n#&gt; 24 60-99 24.9  no 218 24\n#&gt; 25 40-59 27.4  no 186 25\n#&gt; \n#&gt; $`3`\n#&gt;      age  bmi hyp chl id\n#&gt; 1  20-39 25.5  no 229  1\n#&gt; 2  40-59 22.7  no 187  2\n#&gt; 3  20-39 22.0  no 187  3\n#&gt; 4  60-99 20.4  no 199  4\n#&gt; 5  20-39 20.4  no 113  5\n#&gt; 6  60-99 22.7  no 184  6\n#&gt; 7  20-39 22.5  no 118  7\n#&gt; 8  20-39 30.1  no 187  8\n#&gt; 9  40-59 22.0  no 238  9\n#&gt; 10 40-59 27.4  no 184 10\n#&gt; 11 20-39 30.1  no 238 11\n#&gt; 12 40-59 22.0  no 186 12\n#&gt; 13 60-99 21.7  no 206 13\n#&gt; 14 40-59 28.7 yes 204 14\n#&gt; 15 20-39 29.6  no 229 15\n#&gt; 16 20-39 22.0  no 118 16\n#&gt; 17 60-99 27.2 yes 284 17\n#&gt; 18 40-59 26.3 yes 199 18\n#&gt; 19 20-39 35.3  no 218 19\n#&gt; 20 60-99 25.5 yes 218 20\n#&gt; 21 20-39 29.6  no 131 21\n#&gt; 22 20-39 33.2  no 229 22\n#&gt; 23 20-39 27.5  no 131 23\n#&gt; 24 60-99 24.9  no 218 24\n#&gt; 25 40-59 27.4  no 186 25\n#&gt; \n#&gt; $`4`\n#&gt;      age  bmi hyp chl id\n#&gt; 1  20-39 24.9  no 187  1\n#&gt; 2  40-59 22.7  no 187  2\n#&gt; 3  20-39 27.4  no 187  3\n#&gt; 4  60-99 24.9 yes 206  4\n#&gt; 5  20-39 20.4  no 113  5\n#&gt; 6  60-99 22.5  no 184  6\n#&gt; 7  20-39 22.5  no 118  7\n#&gt; 8  20-39 30.1  no 187  8\n#&gt; 9  40-59 22.0  no 238  9\n#&gt; 10 40-59 26.3 yes 187 10\n#&gt; 11 20-39 29.6  no 229 11\n#&gt; 12 40-59 27.4  no 204 12\n#&gt; 13 60-99 21.7  no 206 13\n#&gt; 14 40-59 28.7 yes 204 14\n#&gt; 15 20-39 29.6  no 186 15\n#&gt; 16 20-39 35.3  no 184 16\n#&gt; 17 60-99 27.2 yes 284 17\n#&gt; 18 40-59 26.3 yes 199 18\n#&gt; 19 20-39 35.3  no 218 19\n#&gt; 20 60-99 25.5 yes 199 20\n#&gt; 21 20-39 27.5  no 187 21\n#&gt; 22 20-39 33.2  no 229 22\n#&gt; 23 20-39 27.5  no 131 23\n#&gt; 24 60-99 24.9  no 284 24\n#&gt; 25 40-59 27.4  no 186 25\n#&gt; \n#&gt; $`5`\n#&gt;      age  bmi hyp chl id\n#&gt; 1  20-39 27.2  no 238  1\n#&gt; 2  40-59 22.7  no 187  2\n#&gt; 3  20-39 24.9  no 187  3\n#&gt; 4  60-99 20.4  no 229  4\n#&gt; 5  20-39 20.4  no 113  5\n#&gt; 6  60-99 21.7  no 184  6\n#&gt; 7  20-39 22.5  no 118  7\n#&gt; 8  20-39 30.1  no 187  8\n#&gt; 9  40-59 22.0  no 238  9\n#&gt; 10 40-59 20.4 yes 187 10\n#&gt; 11 20-39 25.5  no 118 11\n#&gt; 12 40-59 21.7  no 187 12\n#&gt; 13 60-99 21.7  no 206 13\n#&gt; 14 40-59 28.7 yes 204 14\n#&gt; 15 20-39 29.6  no 199 15\n#&gt; 16 20-39 27.5  no 187 16\n#&gt; 17 60-99 27.2 yes 284 17\n#&gt; 18 40-59 26.3 yes 199 18\n#&gt; 19 20-39 35.3  no 218 19\n#&gt; 20 60-99 25.5 yes 206 20\n#&gt; 21 20-39 33.2  no 206 21\n#&gt; 22 20-39 33.2  no 229 22\n#&gt; 23 20-39 27.5  no 131 23\n#&gt; 24 60-99 24.9  no 204 24\n#&gt; 25 40-59 27.4  no 186 25\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"mild\" \"list\"\n\nInclude a missing indicator (Y & A)\nFor each imputed dataset, a new column is added to indicate whether the outcome and exposure variables were originally missing. This “missing indicator” column will be used later to subset the data.\n\n# Define formula (making binary Y)\nformula &lt;- as.formula(\"I(bmi&gt;25) ~ chl + hyp\")\ndata.list &lt;- vector(\"list\", m)\n# subset the data without Y and A's that had missing values\n# and record those subset data\nfor (i in 1:m) {\n  analytic.i &lt;- impdata[[i]]\n  analytic.i$miss &lt;- 1\n  analytic.i$miss[analytic.i$id %in% nhanes2.excludingYA$id] &lt;- 0\n  data.list[[i]] &lt;- analytic.i\n}\ndata.list  # only relevant IDs are present\n#&gt; [[1]]\n#&gt;      age  bmi hyp chl id miss\n#&gt; 1  20-39 20.4  no 187  1    1\n#&gt; 2  40-59 22.7  no 187  2    0\n#&gt; 3  20-39 27.4  no 187  3    1\n#&gt; 4  60-99 25.5 yes 204  4    1\n#&gt; 5  20-39 20.4  no 113  5    0\n#&gt; 6  60-99 22.5 yes 184  6    1\n#&gt; 7  20-39 22.5  no 118  7    0\n#&gt; 8  20-39 30.1  no 187  8    0\n#&gt; 9  40-59 22.0  no 238  9    0\n#&gt; 10 40-59 20.4  no 199 10    1\n#&gt; 11 20-39 27.5  no 199 11    1\n#&gt; 12 40-59 26.3  no 284 12    1\n#&gt; 13 60-99 21.7  no 206 13    0\n#&gt; 14 40-59 28.7 yes 204 14    0\n#&gt; 15 20-39 29.6  no 187 15    1\n#&gt; 16 20-39 35.3 yes 204 16    1\n#&gt; 17 60-99 27.2 yes 284 17    0\n#&gt; 18 40-59 26.3 yes 199 18    0\n#&gt; 19 20-39 35.3  no 218 19    0\n#&gt; 20 60-99 25.5 yes 284 20    1\n#&gt; 21 20-39 35.3 yes 184 21    1\n#&gt; 22 20-39 33.2  no 229 22    0\n#&gt; 23 20-39 27.5  no 131 23    0\n#&gt; 24 60-99 24.9  no 218 24    1\n#&gt; 25 40-59 27.4  no 186 25    0\n#&gt; \n#&gt; [[2]]\n#&gt;      age  bmi hyp chl id miss\n#&gt; 1  20-39 28.7 yes 238  1    1\n#&gt; 2  40-59 22.7  no 187  2    0\n#&gt; 3  20-39 30.1  no 187  3    1\n#&gt; 4  60-99 22.5 yes 218  4    1\n#&gt; 5  20-39 20.4  no 113  5    0\n#&gt; 6  60-99 20.4 yes 184  6    1\n#&gt; 7  20-39 22.5  no 118  7    0\n#&gt; 8  20-39 30.1  no 187  8    0\n#&gt; 9  40-59 22.0  no 238  9    0\n#&gt; 10 40-59 22.5 yes 238 10    1\n#&gt; 11 20-39 26.3 yes 118 11    1\n#&gt; 12 40-59 20.4  no 199 12    1\n#&gt; 13 60-99 21.7  no 206 13    0\n#&gt; 14 40-59 28.7 yes 204 14    0\n#&gt; 15 20-39 29.6  no 187 15    1\n#&gt; 16 20-39 24.9  no 238 16    1\n#&gt; 17 60-99 27.2 yes 284 17    0\n#&gt; 18 40-59 26.3 yes 199 18    0\n#&gt; 19 20-39 35.3  no 218 19    0\n#&gt; 20 60-99 25.5 yes 218 20    1\n#&gt; 21 20-39 27.5  no 187 21    1\n#&gt; 22 20-39 33.2  no 229 22    0\n#&gt; 23 20-39 27.5  no 131 23    0\n#&gt; 24 60-99 24.9  no 218 24    1\n#&gt; 25 40-59 27.4  no 186 25    0\n#&gt; \n#&gt; [[3]]\n#&gt;      age  bmi hyp chl id miss\n#&gt; 1  20-39 25.5  no 229  1    1\n#&gt; 2  40-59 22.7  no 187  2    0\n#&gt; 3  20-39 22.0  no 187  3    1\n#&gt; 4  60-99 20.4  no 199  4    1\n#&gt; 5  20-39 20.4  no 113  5    0\n#&gt; 6  60-99 22.7  no 184  6    1\n#&gt; 7  20-39 22.5  no 118  7    0\n#&gt; 8  20-39 30.1  no 187  8    0\n#&gt; 9  40-59 22.0  no 238  9    0\n#&gt; 10 40-59 27.4  no 184 10    1\n#&gt; 11 20-39 30.1  no 238 11    1\n#&gt; 12 40-59 22.0  no 186 12    1\n#&gt; 13 60-99 21.7  no 206 13    0\n#&gt; 14 40-59 28.7 yes 204 14    0\n#&gt; 15 20-39 29.6  no 229 15    1\n#&gt; 16 20-39 22.0  no 118 16    1\n#&gt; 17 60-99 27.2 yes 284 17    0\n#&gt; 18 40-59 26.3 yes 199 18    0\n#&gt; 19 20-39 35.3  no 218 19    0\n#&gt; 20 60-99 25.5 yes 218 20    1\n#&gt; 21 20-39 29.6  no 131 21    1\n#&gt; 22 20-39 33.2  no 229 22    0\n#&gt; 23 20-39 27.5  no 131 23    0\n#&gt; 24 60-99 24.9  no 218 24    1\n#&gt; 25 40-59 27.4  no 186 25    0\n#&gt; \n#&gt; [[4]]\n#&gt;      age  bmi hyp chl id miss\n#&gt; 1  20-39 24.9  no 187  1    1\n#&gt; 2  40-59 22.7  no 187  2    0\n#&gt; 3  20-39 27.4  no 187  3    1\n#&gt; 4  60-99 24.9 yes 206  4    1\n#&gt; 5  20-39 20.4  no 113  5    0\n#&gt; 6  60-99 22.5  no 184  6    1\n#&gt; 7  20-39 22.5  no 118  7    0\n#&gt; 8  20-39 30.1  no 187  8    0\n#&gt; 9  40-59 22.0  no 238  9    0\n#&gt; 10 40-59 26.3 yes 187 10    1\n#&gt; 11 20-39 29.6  no 229 11    1\n#&gt; 12 40-59 27.4  no 204 12    1\n#&gt; 13 60-99 21.7  no 206 13    0\n#&gt; 14 40-59 28.7 yes 204 14    0\n#&gt; 15 20-39 29.6  no 186 15    1\n#&gt; 16 20-39 35.3  no 184 16    1\n#&gt; 17 60-99 27.2 yes 284 17    0\n#&gt; 18 40-59 26.3 yes 199 18    0\n#&gt; 19 20-39 35.3  no 218 19    0\n#&gt; 20 60-99 25.5 yes 199 20    1\n#&gt; 21 20-39 27.5  no 187 21    1\n#&gt; 22 20-39 33.2  no 229 22    0\n#&gt; 23 20-39 27.5  no 131 23    0\n#&gt; 24 60-99 24.9  no 284 24    1\n#&gt; 25 40-59 27.4  no 186 25    0\n#&gt; \n#&gt; [[5]]\n#&gt;      age  bmi hyp chl id miss\n#&gt; 1  20-39 27.2  no 238  1    1\n#&gt; 2  40-59 22.7  no 187  2    0\n#&gt; 3  20-39 24.9  no 187  3    1\n#&gt; 4  60-99 20.4  no 229  4    1\n#&gt; 5  20-39 20.4  no 113  5    0\n#&gt; 6  60-99 21.7  no 184  6    1\n#&gt; 7  20-39 22.5  no 118  7    0\n#&gt; 8  20-39 30.1  no 187  8    0\n#&gt; 9  40-59 22.0  no 238  9    0\n#&gt; 10 40-59 20.4 yes 187 10    1\n#&gt; 11 20-39 25.5  no 118 11    1\n#&gt; 12 40-59 21.7  no 187 12    1\n#&gt; 13 60-99 21.7  no 206 13    0\n#&gt; 14 40-59 28.7 yes 204 14    0\n#&gt; 15 20-39 29.6  no 199 15    1\n#&gt; 16 20-39 27.5  no 187 16    1\n#&gt; 17 60-99 27.2 yes 284 17    0\n#&gt; 18 40-59 26.3 yes 199 18    0\n#&gt; 19 20-39 35.3  no 218 19    0\n#&gt; 20 60-99 25.5 yes 206 20    1\n#&gt; 21 20-39 33.2  no 206 21    1\n#&gt; 22 20-39 33.2  no 229 22    0\n#&gt; 23 20-39 27.5  no 131 23    0\n#&gt; 24 60-99 24.9  no 204 24    1\n#&gt; 25 40-59 27.4  no 186 25    0\n# record the fits from each data\n\nDesign, subset and fit\nFor each imputed dataset, a statistical model is fitted. Before fitting, the data is structured to account for survey design features. Only rows without originally missing outcome and exposure values are used for model fitting. The results of the model fitting for each dataset are stored for later analysis.\n\nrequire(survey)\nfit.list &lt;- vector(\"list\", 5)\nfor (i in 1:m) {\n  analytic.i &lt;- data.list[[i]]\n  # assigning survey features = 1\n  w.design0 &lt;- svydesign(id=~1, weights=~1,\n                        data=analytic.i)\n  w.design &lt;- subset(w.design0, miss == 0)\n  fit &lt;- svyglm(formula, design=w.design, family=binomial)\n  fit.list[[i]] &lt;-  fit\n}\n\nPooled results\nAfter fitting models to each imputed dataset, the results are combined or “pooled”. This pooled result provides a more robust estimate by considering the variability across the imputed datasets.\n\nrequire(mitools)\npooled.estimates &lt;- MIcombine(fit.list)\npooled.estimates\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fit.list)\n#&gt;                  results         se\n#&gt; (Intercept) -1.769918773 2.73723080\n#&gt; chl          0.009747869 0.01499608\n#&gt; hypyes      18.128613035 1.05775401\n\n# or you can do it this way\nbetas&lt;-MIextract(fit.list,fun=coef)\nvars&lt;-MIextract(fit.list, fun=vcov)\nsummary(MIcombine(betas,vars))\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(betas, vars)\n#&gt;                  results         se      (lower      upper) missInfo\n#&gt; (Intercept) -1.769918773 2.73723080 -7.13479255  3.59495501      0 %\n#&gt; chl          0.009747869 0.01499608 -0.01964391  0.03913964      0 %\n#&gt; hypyes      18.128613035 1.05775401 16.05545326 20.20177281      0 %\n\n# report beta coef\nsum.pooled &lt;- summary(pooled.estimates, digits = 2)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fit.list)\n#&gt;             results    se (lower upper) missInfo\n#&gt; (Intercept) -1.7699 2.737  -7.13  3.595      0 %\n#&gt; chl          0.0097 0.015  -0.02  0.039      0 %\n#&gt; hypyes      18.1286 1.058  16.06 20.202      0 %\nsum.pooled\n\n\n  \n\n\n\nReport OR\nThe pooled results are further processed to calculate and report odds ratios, which provide insights into the relationships between variables in the context of logistic regression.\n\nsum.pooled.OR &lt;- summary(pooled.estimates, logeffect=TRUE, digits = 2)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fit.list)\n#&gt;             results      se  (lower  upper) missInfo\n#&gt; (Intercept) 1.7e-01 4.7e-01 8.0e-04 3.6e+01      0 %\n#&gt; chl         1.0e+00 1.5e-02 9.8e-01 1.0e+00      0 %\n#&gt; hypyes      7.5e+07 7.9e+07 9.4e+06 5.9e+08      0 %\nsum.pooled.OR\n\n\n  \n\n\n\nUsing publish package may be possible, but requires complicated process. Look for publish.MIresult (but this can be complicated).",
    "crumbs": [
      "Missing data analysis",
      "Missing in outcome"
    ]
  },
  {
    "objectID": "missingdata4.html",
    "href": "missingdata4.html",
    "title": "Performance with NA",
    "section": "",
    "text": "This is a tutorial of how to estimate model performance while analyzing survey data with missing values (for predictive goals).\n\n# Load required packages\nlibrary(survey)\nlibrary(ROCR)\nlibrary(WeightedROC)\n\nUseful functions\nThe functions below could be helpful in\n\n\nsvyROCw3: calculating area under the ROC curve (AUC) value with survey data with logistic regression\n\n\nAL.gof3: testing Archer-Lemeshow goodness of fit for survey data with logistic regression\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# ROC curve for Survey Data with Logistic Regression\nsvyROCw3 &lt;- function(fit=fit7,outcome=analytic2$CVD==\"event\", weight = NULL,plot=FALSE){\n  if (is.null(weight)){ # require(ROCR)\n    prob &lt;- predict(fit, type = \"response\")\n  pred &lt;- prediction(as.vector(prob), outcome)\n  perf &lt;- performance(pred, \"tpr\", \"fpr\")\n  auc &lt;- performance(pred, measure = \"auc\")\n  auc &lt;- auc@y.values[[1]]\n  if (plot == TRUE){\n    roc.data &lt;- data.frame(fpr = unlist(perf@x.values), tpr = unlist(perf@y.values), \n      model = \"Logistic\")\n    with(data = roc.data,plot(fpr, tpr, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  mtext(\"Unweighted ROC\")\n  abline(0,1, lty=2)\n  }\n  } else { # library(WeightedROC)\n    outcome &lt;- as.numeric(outcome)\n  pred &lt;- predict(fit, type = \"response\")\n  tp.fp &lt;- WeightedROC(pred, outcome, weight)\n  auc &lt;- WeightedAUC(tp.fp)\n  if (plot == TRUE){\n    with(data = tp.fp,plot(FPR, TPR, type=\"l\", xlim=c(0,1), ylim=c(0,1), lwd=1,\n     xlab=\"1 - specificity\", ylab=\"Sensitivity\",\n     main = paste(\"AUC = \", round(auc,3))))\n  abline(0,1, lty=2)\n  mtext(\"Weighted ROC\")\n  }\n  }\n  return(auc)\n}\n\n# Archer-Lemeshow Goodness of Fit Test for Survey Data with Logistic Regression\nAL.gof3 &lt;- function(fit=fit7, data = analytic2, weight = \"weight\", psu = \"psu\", \n                    strata= \"strata\"){\n  r &lt;- residuals(fit, type=\"response\") \n  f&lt;-fitted(fit) \n  breaks.g &lt;- c(-Inf, quantile(f,  (1:9)/10), Inf)\n  breaks.g &lt;- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n  g&lt;- cut(f, breaks.g)\n  data2g &lt;- cbind(data,r,g)\n  if (is.null(psu)){\n    newdesign &lt;- svydesign(id=~1,\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  if (!is.null(psu)) {\n    newdesign &lt;- svydesign(id=as.formula(paste0(\"~\",psu)),\n                         strata=as.formula(paste0(\"~\",strata)),\n                         weights=as.formula(paste0(\"~\",weight)), \n                        data=data2g, nest = TRUE)\n  }\n  decilemodel&lt;- svyglm(r~g, design=newdesign) \n  res &lt;- regTermTest(decilemodel, ~g)\n  return(as.numeric(res$p)) \n}\n\nLoad imputed 5 sets of data\nSaved at the end of Lab 6 part 2.\n\n# Saved from last lab\nload(\"Data/missingdata/missOA123CVDnorth.RData\")\nstr(allImputations)\n#&gt; List of 2\n#&gt;  $ imputations:List of 5\n#&gt;   ..$ :'data.frame': 135448 obs. of  19 variables:\n#&gt;   .. ..$ .imp   : int [1:135448] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#&gt;   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#&gt;   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#&gt;   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#&gt;   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#&gt;   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#&gt;   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#&gt;   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 2 2 2 2 2 ...\n#&gt;   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#&gt;   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ edu    : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#&gt;   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#&gt;   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#&gt;   ..$ :'data.frame': 135448 obs. of  19 variables:\n#&gt;   .. ..$ .imp   : int [1:135448] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#&gt;   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#&gt;   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#&gt;   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#&gt;   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#&gt;   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#&gt;   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#&gt;   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 2 2 ...\n#&gt;   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#&gt;   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ edu    : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#&gt;   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#&gt;   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#&gt;   ..$ :'data.frame': 135448 obs. of  19 variables:\n#&gt;   .. ..$ .imp   : int [1:135448] 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#&gt;   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#&gt;   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#&gt;   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#&gt;   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#&gt;   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#&gt;   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#&gt;   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 2 2 2 2 ...\n#&gt;   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#&gt;   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ edu    : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#&gt;   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#&gt;   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#&gt;   ..$ :'data.frame': 135448 obs. of  19 variables:\n#&gt;   .. ..$ .imp   : int [1:135448] 4 4 4 4 4 4 4 4 4 4 ...\n#&gt;   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#&gt;   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#&gt;   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#&gt;   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#&gt;   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#&gt;   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#&gt;   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#&gt;   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 1 2 1 2 2 ...\n#&gt;   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#&gt;   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ edu    : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#&gt;   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#&gt;   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#&gt;   ..$ :'data.frame': 135448 obs. of  19 variables:\n#&gt;   .. ..$ .imp   : int [1:135448] 5 5 5 5 5 5 5 5 5 5 ...\n#&gt;   .. ..$ .id    : int [1:135448] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   .. ..$ CVD    : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ age    : Factor w/ 4 levels \"20-39 years\",..: 1 2 1 2 3 1 3 2 1 2 ...\n#&gt;   .. ..$ sex    : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 2 2 2 1 1 1 1 ...\n#&gt;   .. ..$ income : Factor w/ 4 levels \"$29,999 or less\",..: 4 1 3 4 2 2 1 4 4 3 ...\n#&gt;   .. ..$ race   : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   .. ..$ bmicat : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 2 1 1 2 1 2 2 3 2 ...\n#&gt;   .. ..$ phyact : Factor w/ 3 levels \"Active\",\"Inactive\",..: 2 1 2 2 3 3 2 1 2 2 ...\n#&gt;   .. ..$ smoke  : Factor w/ 3 levels \"Current smoker\",..: 2 2 2 2 2 3 2 2 3 3 ...\n#&gt;   .. ..$ fruit  : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 2 2 1 2 3 3 2 ...\n#&gt;   .. ..$ painmed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 2 2 2 ...\n#&gt;   .. ..$ ht     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n#&gt;   .. ..$ copd   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ diab   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ edu    : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 4 4 4 4 2 4 1 4 4 4 ...\n#&gt;   .. ..$ weight : num [1:135448] 56.1 56.1 39.2 44 59.9 ...\n#&gt;   .. ..$ OA     : Factor w/ 2 levels \"Control\",\"OA\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   .. ..$ ID     : int [1:135448] 1 3 6 7 12 13 14 16 20 21 ...\n#&gt;  $ call       : language imputationList(list(subset(impdata, subset = .imp == 1), subset(impdata,      subset = .imp == 2), subset(impdata| __truncated__ ...\n#&gt;  - attr(*, \"class\")= chr \"imputationList\"\n\nEstimating treatment effect\nIndividual beta estimates\n\nlibrary(survey)\nw.design &lt;- svydesign(ids=~1, weights=~weight,\n                           data = allImputations)\nmodel.formula &lt;- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nestimates &lt;- with(w.design, svyglm(model.formula, family=quasibinomial))\nestimates\n#&gt; [[1]]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; \n#&gt; Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#&gt; \n#&gt; Coefficients:\n#&gt;            (Intercept)                    OAOA          age40-49 years  \n#&gt;                -5.6809                  1.1063                  0.7911  \n#&gt;         age50-59 years          age60-64 years                 sexMale  \n#&gt;                 1.6233                  2.0076                  0.6278  \n#&gt;  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#&gt;                -0.4347                 -0.6031                 -0.6805  \n#&gt;              raceWhite              painmedYes                   htYes  \n#&gt;                 0.2136                  0.8277                  1.0344  \n#&gt;                copdYes                 diabYes         OAOA:painmedYes  \n#&gt;                 1.9143                  0.8166                 -0.8183  \n#&gt; age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#&gt;                -1.1404                 -0.7286                 -0.9360  \n#&gt;        sexMale:copdYes  \n#&gt;                 0.6167  \n#&gt; \n#&gt; Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#&gt; Null Deviance:       36680 \n#&gt; Residual Deviance: 31100     AIC: NA\n#&gt; \n#&gt; [[2]]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; \n#&gt; Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#&gt; \n#&gt; Coefficients:\n#&gt;            (Intercept)                    OAOA          age40-49 years  \n#&gt;                -5.4837                  0.8766                  0.7781  \n#&gt;         age50-59 years          age60-64 years                 sexMale  \n#&gt;                 1.6090                  1.9901                  0.6083  \n#&gt;  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#&gt;                -0.4415                 -0.5969                 -0.6840  \n#&gt;              raceWhite              painmedYes                   htYes  \n#&gt;                 0.2374                  0.5877                  1.0413  \n#&gt;                copdYes                 diabYes         OAOA:painmedYes  \n#&gt;                 1.8837                  0.8054                 -0.5388  \n#&gt; age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#&gt;                -1.1303                 -0.6944                 -0.8545  \n#&gt;        sexMale:copdYes  \n#&gt;                 0.5857  \n#&gt; \n#&gt; Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#&gt; Null Deviance:       36680 \n#&gt; Residual Deviance: 31260     AIC: NA\n#&gt; \n#&gt; [[3]]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; \n#&gt; Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#&gt; \n#&gt; Coefficients:\n#&gt;            (Intercept)                    OAOA          age40-49 years  \n#&gt;                -5.6365                  1.0389                  0.7815  \n#&gt;         age50-59 years          age60-64 years                 sexMale  \n#&gt;                 1.6140                  1.9986                  0.6180  \n#&gt;  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#&gt;                -0.4334                 -0.5945                 -0.6843  \n#&gt;              raceWhite              painmedYes                   htYes  \n#&gt;                 0.2042                  0.7963                  1.0386  \n#&gt;                copdYes                 diabYes         OAOA:painmedYes  \n#&gt;                 1.9468                  0.8020                 -0.7330  \n#&gt; age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#&gt;                -1.1185                 -0.7775                 -0.9414  \n#&gt;        sexMale:copdYes  \n#&gt;                 0.5548  \n#&gt; \n#&gt; Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#&gt; Null Deviance:       36680 \n#&gt; Residual Deviance: 31130     AIC: NA\n#&gt; \n#&gt; [[4]]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; \n#&gt; Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#&gt; \n#&gt; Coefficients:\n#&gt;            (Intercept)                    OAOA          age40-49 years  \n#&gt;                -5.6811                  1.2937                  0.7838  \n#&gt;         age50-59 years          age60-64 years                 sexMale  \n#&gt;                 1.6248                  2.0097                  0.6285  \n#&gt;  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#&gt;                -0.4431                 -0.6012                 -0.6877  \n#&gt;              raceWhite              painmedYes                   htYes  \n#&gt;                 0.2325                  0.8179                  1.0401  \n#&gt;                copdYes                 diabYes         OAOA:painmedYes  \n#&gt;                 1.9368                  0.8022                 -1.0624  \n#&gt; age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#&gt;                -1.0381                 -0.7407                 -0.9356  \n#&gt;        sexMale:copdYes  \n#&gt;                 0.5422  \n#&gt; \n#&gt; Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#&gt; Null Deviance:       36680 \n#&gt; Residual Deviance: 31100     AIC: NA\n#&gt; \n#&gt; [[5]]\n#&gt; Independent Sampling design (with replacement)\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; \n#&gt; Call:  svyglm(formula = model.formula, design = .design, family = quasibinomial)\n#&gt; \n#&gt; Coefficients:\n#&gt;            (Intercept)                    OAOA          age40-49 years  \n#&gt;                -5.4487                  0.7569                  0.7752  \n#&gt;         age50-59 years          age60-64 years                 sexMale  \n#&gt;                 1.6021                  1.9847                  0.6107  \n#&gt;  income$30,000-$49,999   income$50,000-$79,999   income$80,000 or more  \n#&gt;                -0.4393                 -0.5977                 -0.6788  \n#&gt;              raceWhite              painmedYes                   htYes  \n#&gt;                 0.2351                  0.5457                  1.0421  \n#&gt;                copdYes                 diabYes         OAOA:painmedYes  \n#&gt;                 1.9235                  0.8057                 -0.3884  \n#&gt; age40-49 years:copdYes  age50-59 years:copdYes  age60-64 years:copdYes  \n#&gt;                -1.1275                 -0.7370                 -0.9152  \n#&gt;        sexMale:copdYes  \n#&gt;                 0.5871  \n#&gt; \n#&gt; Degrees of Freedom: 135447 Total (i.e. Null);  135429 Residual\n#&gt; Null Deviance:       36680 \n#&gt; Residual Deviance: 31290     AIC: NA\n#&gt; \n#&gt; attr(,\"call\")\n#&gt; with(w.design, svyglm(model.formula, family = quasibinomial))\n\nPooled / averaged estimates for beta and OR\n\nlibrary(\"mitools\")\npooled.estimates &lt;- MIcombine(estimates)\npooled.estimates\n#&gt; Multiple imputation results:\n#&gt;       with(w.design, svyglm(model.formula, family = quasibinomial))\n#&gt;       MIcombine.default(estimates)\n#&gt;                           results         se\n#&gt; (Intercept)            -5.5861842 0.19239878\n#&gt; OAOA                    1.0144758 0.27097789\n#&gt; age40-49 years          0.7819429 0.10165218\n#&gt; age50-59 years          1.6146502 0.09905167\n#&gt; age60-64 years          1.9981358 0.10380099\n#&gt; sexMale                 0.6186653 0.05484262\n#&gt; income$30,000-$49,999  -0.4383959 0.06787453\n#&gt; income$50,000-$79,999  -0.5987066 0.06593903\n#&gt; income$80,000 or more  -0.6830429 0.06907307\n#&gt; raceWhite               0.2245784 0.10567063\n#&gt; painmedYes              0.7150386 0.16508180\n#&gt; htYes                   1.0393147 0.05522682\n#&gt; copdYes                 1.9210219 0.65240500\n#&gt; diabYes                 0.8063590 0.08169615\n#&gt; OAOA:painmedYes        -0.7081707 0.32575000\n#&gt; age40-49 years:copdYes -1.1109564 0.69441469\n#&gt; age50-59 years:copdYes -0.7356335 0.67352566\n#&gt; age60-64 years:copdYes -0.9165474 0.65626790\n#&gt; sexMale:copdYes         0.5772949 0.30888502\n\nsum.pooled &lt;- summary(pooled.estimates,logeffect=TRUE, digits = 2)\n#&gt; Multiple imputation results:\n#&gt;       with(w.design, svyglm(model.formula, family = quasibinomial))\n#&gt;       MIcombine.default(estimates)\n#&gt;                        results      se (lower  upper) missInfo\n#&gt; (Intercept)             0.0037 0.00072 0.0025  0.0056     45 %\n#&gt; OAOA                    2.7579 0.74733 1.4778  5.1469     76 %\n#&gt; age40-49 years          2.1857 0.22218 1.7909  2.6676      0 %\n#&gt; age50-59 years          5.0261 0.49785 4.1392  6.1031      1 %\n#&gt; age60-64 years          7.3753 0.76556 6.0175  9.0394      1 %\n#&gt; sexMale                 1.8564 0.10181 1.6672  2.0672      4 %\n#&gt; income$30,000-$49,999   0.6451 0.04378 0.5647  0.7369      0 %\n#&gt; income$50,000-$79,999   0.5495 0.03623 0.4829  0.6253      0 %\n#&gt; income$80,000 or more   0.5051 0.03489 0.4411  0.5783      0 %\n#&gt; raceWhite               1.2518 0.13228 1.0176  1.5399      2 %\n#&gt; painmedYes              2.0443 0.33747 1.3628  3.0664     86 %\n#&gt; htYes                   2.8273 0.15614 2.5372  3.1505      0 %\n#&gt; copdYes                 6.8279 4.45458 1.9009 24.5255      0 %\n#&gt; diabYes                 2.2397 0.18298 1.9083  2.6287      1 %\n#&gt; OAOA:painmedYes         0.4925 0.16045 0.2275  1.0663     81 %\n#&gt; age40-49 years:copdYes  0.3292 0.22863 0.0844  1.2841      0 %\n#&gt; age50-59 years:copdYes  0.4792 0.32275 0.1280  1.7940      0 %\n#&gt; age60-64 years:copdYes  0.3999 0.26244 0.1105  1.4473      0 %\n#&gt; sexMale:copdYes         1.7812 0.55019 0.9723  3.2632      1 %\nsum.pooled\n\n\n  \n\n\n\nEstimating model performance (AUC and AL)\nIndividual AUC estimates (with interactions)\n\nlibrary(ROCR)\nlibrary(WeightedROC)\nmodel.formula &lt;- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab + OA:painmed +\n                            age:copd + sex:copd\")\nAL.scalar &lt;- AUC.scalar &lt;- vector(\"list\", 5)\nfor (i in 1:5) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  w.design &lt;- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit &lt;- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc &lt;- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL &lt;- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] &lt;- AL\n  AUC.scalar[[i]] &lt;- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#&gt; AUC calculated for data 1 \n#&gt; AUC calculated for data 2 \n#&gt; AUC calculated for data 3 \n#&gt; AUC calculated for data 4 \n#&gt; AUC calculated for data 5\nstr(AUC.scalar)\n#&gt; List of 5\n#&gt;  $ : num 0.8\n#&gt;  $ : num 0.795\n#&gt;  $ : num 0.798\n#&gt;  $ : num 0.8\n#&gt;  $ : num 0.794\nAL.scalar\n#&gt; [[1]]\n#&gt; [1] 0.01243738\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0.5471927\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0.13081\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 0.644286\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 0.6049137\n\nAveraged estimates for AUC (with interactions)\n\n# summary of AUC\nmean(unlist(AUC.scalar))\n#&gt; [1] 0.7973746\nsd(unlist(AUC.scalar))\n#&gt; [1] 0.002688964\nround(range(unlist(AUC.scalar)),3)\n#&gt; [1] 0.794 0.800\n# p-values (from AL) by majority\nsum(AL.scalar&gt;0.05)\n#&gt; [1] 4\n\nModel performance without interactions\nIndividual AUC estimates / AL p-values\n\nlibrary(ROCR)\nlibrary(WeightedROC)\nAL.scalar &lt;- AUC.scalar &lt;- vector(\"list\", 5)\nmodel.formula &lt;- as.formula(\"I(CVD=='event') ~ OA + age + sex +\n                            income + race + painmed + ht +\n                            copd + diab\")\nfor (i in 1:5) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  w.design &lt;- svydesign(id=~1, weights=~weight,\n                        data=analytic.i)\n  model.fit &lt;- svyglm(model.formula, design=w.design, family=quasibinomial)\n  auc &lt;- svyROCw3(fit=model.fit,outcome=w.design$variables$CVD=='event', \n                  weight = w.design$variables$weight, plot = FALSE)\n  \n  AL &lt;- AL.gof3(fit=model.fit, data = analytic.i, \n                   weight = \"weight\", \n                   psu = NULL, \n                   strata= NULL)\n  AL.scalar[[i]] &lt;- AL\n  AUC.scalar[[i]] &lt;- auc \n  cat(\"AUC calculated for data\", i, \"\\n\")\n}\n#&gt; AUC calculated for data 1 \n#&gt; AUC calculated for data 2 \n#&gt; AUC calculated for data 3 \n#&gt; AUC calculated for data 4 \n#&gt; AUC calculated for data 5\nstr(AUC.scalar)\n#&gt; List of 5\n#&gt;  $ : num 0.798\n#&gt;  $ : num 0.794\n#&gt;  $ : num 0.797\n#&gt;  $ : num 0.798\n#&gt;  $ : num 0.794\nAL.scalar\n#&gt; [[1]]\n#&gt; [1] 0.01839624\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0.2836256\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0.02439659\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 0.8333214\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 0.2050434\n\nAveraged estimates for AUC / majority of AL p-values\n\n# summary of AUC\nmean(unlist(AUC.scalar))\n#&gt; [1] 0.7964061\nsd(unlist(AUC.scalar))\n#&gt; [1] 0.002002636\nround(range(unlist(AUC.scalar)),3)\n#&gt; [1] 0.794 0.798\n# p-values (from AL) by majority\nsum(AL.scalar&gt;0.05)\n#&gt; [1] 3\n\nAppendix\nUser-written svyROCw3 and AL.gof3 functions\n\nsvyROCw3\n#&gt; function (fit = fit7, outcome = analytic2$CVD == \"event\", weight = NULL, \n#&gt;     plot = FALSE) \n#&gt; {\n#&gt;     if (is.null(weight)) {\n#&gt;         prob &lt;- predict(fit, type = \"response\")\n#&gt;         pred &lt;- prediction(as.vector(prob), outcome)\n#&gt;         perf &lt;- performance(pred, \"tpr\", \"fpr\")\n#&gt;         auc &lt;- performance(pred, measure = \"auc\")\n#&gt;         auc &lt;- auc@y.values[[1]]\n#&gt;         if (plot == TRUE) {\n#&gt;             roc.data &lt;- data.frame(fpr = unlist(perf@x.values), \n#&gt;                 tpr = unlist(perf@y.values), model = \"Logistic\")\n#&gt;             with(data = roc.data, plot(fpr, tpr, type = \"l\", \n#&gt;                 xlim = c(0, 1), ylim = c(0, 1), lwd = 1, xlab = \"1 - specificity\", \n#&gt;                 ylab = \"Sensitivity\", main = paste(\"AUC = \", \n#&gt;                   round(auc, 3))))\n#&gt;             mtext(\"Unweighted ROC\")\n#&gt;             abline(0, 1, lty = 2)\n#&gt;         }\n#&gt;     }\n#&gt;     else {\n#&gt;         outcome &lt;- as.numeric(outcome)\n#&gt;         pred &lt;- predict(fit, type = \"response\")\n#&gt;         tp.fp &lt;- WeightedROC(pred, outcome, weight)\n#&gt;         auc &lt;- WeightedAUC(tp.fp)\n#&gt;         if (plot == TRUE) {\n#&gt;             with(data = tp.fp, plot(FPR, TPR, type = \"l\", xlim = c(0, \n#&gt;                 1), ylim = c(0, 1), lwd = 1, xlab = \"1 - specificity\", \n#&gt;                 ylab = \"Sensitivity\", main = paste(\"AUC = \", \n#&gt;                   round(auc, 3))))\n#&gt;             abline(0, 1, lty = 2)\n#&gt;             mtext(\"Weighted ROC\")\n#&gt;         }\n#&gt;     }\n#&gt;     return(auc)\n#&gt; }\n#&gt; &lt;bytecode: 0x000001a88cf2d008&gt;\nAL.gof3\n#&gt; function (fit = fit7, data = analytic2, weight = \"weight\", psu = \"psu\", \n#&gt;     strata = \"strata\") \n#&gt; {\n#&gt;     r &lt;- residuals(fit, type = \"response\")\n#&gt;     f &lt;- fitted(fit)\n#&gt;     breaks.g &lt;- c(-Inf, quantile(f, (1:9)/10), Inf)\n#&gt;     breaks.g &lt;- breaks.g + seq_along(breaks.g) * .Machine$double.eps\n#&gt;     g &lt;- cut(f, breaks.g)\n#&gt;     data2g &lt;- cbind(data, r, g)\n#&gt;     if (is.null(psu)) {\n#&gt;         newdesign &lt;- svydesign(id = ~1, weights = as.formula(paste0(\"~\", \n#&gt;             weight)), data = data2g, nest = TRUE)\n#&gt;     }\n#&gt;     if (!is.null(psu)) {\n#&gt;         newdesign &lt;- svydesign(id = as.formula(paste0(\"~\", psu)), \n#&gt;             strata = as.formula(paste0(\"~\", strata)), weights = as.formula(paste0(\"~\", \n#&gt;                 weight)), data = data2g, nest = TRUE)\n#&gt;     }\n#&gt;     decilemodel &lt;- svyglm(r ~ g, design = newdesign)\n#&gt;     res &lt;- regTermTest(decilemodel, ~g)\n#&gt;     return(as.numeric(res$p))\n#&gt; }\n#&gt; &lt;bytecode: 0x000001a88cf73f98&gt;",
    "crumbs": [
      "Missing data analysis",
      "Performance with NA"
    ]
  },
  {
    "objectID": "missingdata5.html",
    "href": "missingdata5.html",
    "title": "Subpopulations",
    "section": "",
    "text": "This tutorial demonstrates how to manage missing data in complex surveys using multiple imputation, focusing on specific subpopulations defined by the study’s eligibility criteria.\nPurpose\nLet us we are interested in exploring the relationship between rheumatoid arthritis and cardiovascular disease (CVD) among US adults aged 20 years or more. For that, we will use NHANES 2017–2018 dataset, which follows a complex survey design.\n\n\nIn this tutorial, we used a similar approach to the one in a published article by Hossain et al. (2022), but we used less data (restricted to only 2017–2018) to speed up the analysis. Ref link.\nThe purpose of this example is to demonstrate how to do the missing data analysis with multiple imputation in the context of complex surveys.\nThe main idea is:\n\nworking with the analytic data\nimputing missing values based on that analytic dataset\nkeep count of the ineligible subjects from the full data who are not included in the analytic data\nadding those ineligible subjects back in the imputed datasets, so that we can utilize the survey features and subset the design.\n\n\n# Load required packages\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tableone)\nlibrary(survey)\nlibrary(Publish)\nlibrary(DataExplorer)\nlibrary(mice)\nlibrary(mitools)\n\nLet us import the dataset:\n\nload(\"Data/missingdata/MIexample.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\n\ndim(dat.full)\n#&gt; [1] 9254   15\nhead(dat.full)\n\n\n  \n\n\n\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nAnalytic dataset\nSubsetting according to eligibility\nLet us create an analytic dataset for\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\n# Drop &lt; 20 years\ndat.with.miss &lt;- subset(dat.full, age &gt;= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3857  337 1375\n\n# Drop missing in outcome and exposure \n# i.e., dataset with missing values only in covariates\ndat.analytic &lt;- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#&gt; [1] 4191\n\nAs we can see, we have 4,191 participants aged 20 years or more without missing values in outcome or exposure. Let us count the ineligible subjects from the full data and create an indicator variable.\n\ndat.full$ineligible &lt;- 1\ndat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] &lt;- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#&gt; \n#&gt;    0    1 &lt;NA&gt; \n#&gt; 4191 5063    0\n\nWe have 4,191 eligible and 5,063 ineligible subjects based on the eligibility criteria.\nGeneral strategy of solution:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nlater we will include 5,063 ineligible subjects in the data so that we can utilize survey features.\nTable 1\nLet us see the summary statistics, i.e., create Table 1 stratified by outcome (cvd). Before that, we will categorize age and recode rheumatoid:\n\n# Categorical age\ndat.analytic$age.cat &lt;- \n  with(dat.analytic, ifelse(age &gt;= 20 & age &lt; 50, \"20-49\", \n                            ifelse(age &gt;= 50 & age &lt; 65, \n                                   \"50-64\", \"65+\")))\ndat.analytic$age.cat &lt;- factor(dat.analytic$age.cat, \n                               levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.analytic$age.cat, useNA = \"always\")\n#&gt; \n#&gt; 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  2280  1097   814     0\n\n# Recode rheumatoid to arthritis\ndat.analytic$arthritis &lt;- \ncar::recode(dat.analytic$rheumatoid, \" 'No' = 'No arthritis';\n            'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.analytic$arthritis, useNA = \"always\")\n#&gt; \n#&gt;         No arthritis Rheumatoid arthritis                 &lt;NA&gt; \n#&gt;                 3854                  337                    0\n\n# Keep only relevant variables\nvars &lt;-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\",\n           \"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\",\n           \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 &lt;- dat.analytic[, vars]\n\n\n# Create Table 1\nvars &lt;- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"cvd\", \n                       data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#&gt;                  Stratified by cvd\n#&gt;                   level                     Overall      No          \n#&gt;   n                                          4191         3823       \n#&gt;   arthritis       No arthritis               3854         3580       \n#&gt;                   Rheumatoid arthritis        337          243       \n#&gt;   age.cat         20-49                      2280         2240       \n#&gt;                   50-64                      1097          979       \n#&gt;                   65+                         814          604       \n#&gt;   sex             Male                       2126         1884       \n#&gt;                   Female                     2065         1939       \n#&gt;   education       Less than high school       828          728       \n#&gt;                   High school                2292         2094       \n#&gt;                   College graduate or above  1063          993       \n#&gt;   race            White                      1275         1113       \n#&gt;                   Black                       998          898       \n#&gt;                   Hispanic                   1015          958       \n#&gt;                   Others                      903          854       \n#&gt;   income          less than $20,000           659          557       \n#&gt;                   $20,000 to $74,999         1967         1796       \n#&gt;                   $75,000 and Over           1143         1079       \n#&gt;   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#&gt;   smoking         Never smoker               2570         2427       \n#&gt;                   Previous smoker             882          726       \n#&gt;                   Current smoker              739          670       \n#&gt;   htn             No                         1424         1380       \n#&gt;                   Yes                        2415         2107       \n#&gt;   diabetes        No                         3622         3396       \n#&gt;                   Yes                         566          424       \n#&gt;                  Stratified by cvd\n#&gt;                   Yes         \n#&gt;   n                 368       \n#&gt;   arthritis         274       \n#&gt;                      94       \n#&gt;   age.cat            40       \n#&gt;                     118       \n#&gt;                     210       \n#&gt;   sex               242       \n#&gt;                     126       \n#&gt;   education         100       \n#&gt;                     198       \n#&gt;                      70       \n#&gt;   race              162       \n#&gt;                     100       \n#&gt;                      57       \n#&gt;                      49       \n#&gt;   income            102       \n#&gt;                     171       \n#&gt;                      64       \n#&gt;   bmi (mean (SD)) 30.09 (7.29)\n#&gt;   smoking           143       \n#&gt;                     156       \n#&gt;                      69       \n#&gt;   htn                44       \n#&gt;                     308       \n#&gt;   diabetes          226       \n#&gt;                     142\n\nCheck missingness using a plot\nNow we will see the percentage of missing values in the variables.\n\nDataExplorer::plot_missing(dat.analytic2)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nWe have about 10% missing values in income, followed by hypertension (8.4%), bmi (6.8%), education (0.2%), and diabetes (0.1%).\nDealing with missing values in covariates\n\nNow we will perform multiple imputation to deal with missing values only in covariates. We will use the dat.analytic2 dataset that contains missing values in the covariates but no missing values in the outcome or exposure.\nFor this exercise, we will consider 5 imputed datasets, 3 iterations, and the predictive mean matching method for bmi and income.\n\nWe have already set up the data such that the variables are of appropriate types, e.g., numeric bmi, factor age, sex, and so on.\nWe will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nNow we will set up the initial model and set up the methods and predictor matrix before imputing 5 datasets.\n\n\n\nStep 0: Set up the imputation model\n\n\nIn this tutorial, we used stata as an auxiliary variable. There are two ways we can deal with this:\n\nUse strata to predict other variables, but don’t impute strata: meth[\"strata\"] &lt;- \"\" (this skips strata’s imputation). In our case, since strata has no missing observations, this step is unnecessary, as MICE will automatically not impute fully observed variables. However, this strategy is useful for blocking the imputation of auxiliary variables with missing data.\n\n\nAlso, do not change the predictorMatrix for auxiliary variables, allowing them (e.g., strata in our case) to be used as predictors for other variables.\nSetting pred[\"strata\", ] &lt;- 0 would remove all predictors for strata, which means strata cannot be imputed because there are no variables to predict its missing values. However, strata can still be used as a predictor for other variables. If strata has no missing values (as in our case), this step is unnecessary since MICE does not attempt to impute fully observed variables.\n\n\n\nImpute strata and use it as a predictor (if strata had missing values): If strata had missing data, you wouldn’t need to change anything in predictorMatrix or method. MICE would impute strata and use it as a predictor by default. In our case, since strata has no missing values, imputation is not necessary. Note that:\n\n\nYou should not set pred[\"strata\", ] &lt;- 1 because this would incorrectly make strata a predictor for itself, which is invalid. MICE automatically handles this by setting diagonal entries to 0, so you don’t need to manually ensure this unless you’ve changed it. If you do so (set the row values to 1), you need to then set pred[\"strata\", \"strata\"] &lt;- 0 to ensure strata does not predict itself.\nDo not use pred[, \"strata\"] &lt;- 0 because this would prevent strata from being used as a predictor for other variables, which is contrary to the idea of using it as an auxiliary variable.\n\nWhether to impute or not impute auxiliary variables depends on their role in the analysis and the potential impact of their missing data. If they are strong predictors or are necessary for future analyses, imputing them may be justified. However, if their missingness is not consequential, and imputing them introduces more noise or complexity, it may be better to leave them unimputed.\n\n# Step 0: Set imputation model\nini &lt;- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred &lt;- ini$pred\n\nkable(pred)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyid\nsurvey.weight\npsu\nstrata\ncvd\narthritis\nage.cat\nsex\neducation\nrace\nincome\nbmi\nsmoking\nhtn\ndiabetes\n\n\n\nstudyid\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nsurvey.weight\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\npsu\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nstrata\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\ncvd\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\narthritis\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nage.cat\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nsex\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n\n\neducation\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\nrace\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n\n\nincome\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n\n\nbmi\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n\n\nsmoking\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n\n\nhtn\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\ndiabetes\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",]\n#&gt;       studyid survey.weight           psu        strata           cvd \n#&gt;             1             1             1             0             1 \n#&gt;     arthritis       age.cat           sex     education          race \n#&gt;             1             1             1             1             1 \n#&gt;        income           bmi       smoking           htn      diabetes \n#&gt;             1             1             1             1             1\n# Do not change the pred matrix for strata. \n# This allows strata to be used as a predictor for other variables.\n\n# Do not use id, survey weight or PSU variable as auxiliary variables\npred[, c(\"studyid\", \"psu\", \"survey.weight\")] &lt;- 0\n# Remove them as predictors for other variables\n\nkable(pred)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyid\nsurvey.weight\npsu\nstrata\ncvd\narthritis\nage.cat\nsex\neducation\nrace\nincome\nbmi\nsmoking\nhtn\ndiabetes\n\n\n\nstudyid\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nsurvey.weight\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\npsu\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nstrata\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\ncvd\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\narthritis\n0\n0\n0\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nage.cat\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nsex\n0\n0\n0\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n\n\neducation\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\nrace\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n\n\nincome\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n\n\nbmi\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n\n\nsmoking\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n\n\nhtn\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\ndiabetes\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\nThe rows represent the variables that can be imputed. The entries in each row show which variables will be used as predictors to impute the variable corresponding to that row.\nThe columns represent the variables that can be used as predictors to impute other variables. An entry of 1 in a cell means that the corresponding variable (from the column) will be used to predict the variable represented by the row.\n\npred[, c(\"studyid\", \"psu\", \"survey.weight\")] &lt;- 0 action sets all values in the columns for studyid, psu, and survey.weight to 0. Effectively, it tells MICE not to use them as predictors when imputing other variables. In other words, these variables will not be used to predict missing values in other variables.\n\n\n# Set imputation method\nmeth &lt;- ini$meth\nmeth[\"bmi\"] &lt;- \"pmm\"\nmeth[\"income\"] &lt;- \"pmm\"\nmeth\n#&gt;       studyid survey.weight           psu        strata           cvd \n#&gt;            \"\"            \"\"            \"\"            \"\"            \"\" \n#&gt;     arthritis       age.cat           sex     education          race \n#&gt;            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#&gt;        income           bmi       smoking           htn      diabetes \n#&gt;         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\nmeth[\"strata\"] # method used for strata to be imputed\n#&gt; strata \n#&gt;     \"\"\n# meth[\"strata\"] &lt;- \"\" # you can skip auxiliary variable's imputation\n# Modify the method to skip imputation of \"studyid\", \"psu\", and \"survey.weight\"\nmeth[\"studyid\"] &lt;- \"\"\nmeth[\"psu\"] &lt;- \"\"\nmeth[\"survey.weight\"] &lt;- \"\"\nmeth\n#&gt;       studyid survey.weight           psu        strata           cvd \n#&gt;            \"\"            \"\"            \"\"            \"\"            \"\" \n#&gt;     arthritis       age.cat           sex     education          race \n#&gt;            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#&gt;        income           bmi       smoking           htn      diabetes \n#&gt;         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\n\nThere is no missing for studyid, survey.weight, psu, strata, cvd, arthritis, age, sex, race, smoking. Hence, no method is assigned for these variables.\nFor education, polyreg (Polytomous logistic regression) will be used.\nSimilarly, we will use pmm (Predictive mean matching) for bmi, income and used logreg (Logistic regression) for htn, diabetes. See the Imputation chapter to see how the PMM method works.\nStep 1: Imputing missing values using mice\n1.1 Imputing dataset for eligible subjects\n\n# Step 1: impute the incomplete data\nimputation &lt;- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nimpdata &lt;- mice::complete(imputation, action=\"long\")\n\ntable(impdata$age.cat)\n#&gt; \n#&gt; 20-49 50-64   65+ \n#&gt; 11400  5485  4070\n\nNote that age has no missing values, and everyone is above 20.\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id &lt;- NULL\n\n# Create an indicator of eligible subjects \nimpdata$ineligible &lt;- 0\n\n# Number of subjects\nnrow(impdata)\n#&gt; [1] 20955\n\nLet’s see whether there is any missing value after imputation:\n\nDataExplorer::plot_missing(impdata)\n\n\n\n\n\n\n\n\nThere is no missing value after imputation.\nAs we can see, there is an additional variable (.imp) in the imputed dataset. This .imp goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\n1.2 Preparing dataset for ineligible subjects\nThe next task is adding the ineligible subjects in the imputed datasets, so that we can set up the survey design on the full dataset and then subset the design.\n\n# Number of ineligible subjects\n#dat.full$ineligible &lt;- 1\n#dat.full$ineligible[dat.full$studyid %in% dat.analytic$studyid] &lt;- 0\ntable(dat.full$ineligible, useNA = \"always\")\n#&gt; \n#&gt;    0    1 &lt;NA&gt; \n#&gt; 4191 5063    0\n\nNow we will subset the data for ineligible subjects and create m = 5 copies.\n\n# Subset for ineligible\ndat.ineligible &lt;- subset(dat.full, ineligible == 1)\n\n# Create m = 5 datasets with .imp 1 to m = 5\ndat31 &lt;- dat.ineligible; dat31$.imp &lt;- 1\ndat32 &lt;- dat.ineligible; dat32$.imp &lt;- 2\ndat33 &lt;- dat.ineligible; dat33$.imp &lt;- 3\ndat34 &lt;- dat.ineligible; dat34$.imp &lt;- 4\ndat35 &lt;- dat.ineligible; dat35$.imp &lt;- 5\n\nThe next step is combining ineligible datasets. Before merging the stacked dataset for ineligible subjects to the imputed stacked dataset for eligible subjects, we must ensure the variable names are the same.\n\n# Stacked data for ineligible subjects\ndat.ineligible.stacked &lt;- rbind(dat31, dat32, dat33, dat34, dat35)\n\nWe should have missing value in this ineligible part of the data:\n\nDataExplorer::plot_missing(dat.ineligible.stacked)\n\n\n\n\n\n\n\n1.3 Combining eligible (imputed) and ineligible (unimputed) subjects\n\nnames(impdata)\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"arthritis\"     \"age.cat\"       \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \".imp\"         \n#&gt; [17] \"ineligible\"\n\n\nnames(dat.ineligible.stacked)\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \"ineligible\"   \n#&gt; [17] \".imp\"\n\nAs we can see, the variable names are different in the two datasets. Particularly, arthritis and age.cat variables are not available in the dat.ineligible.stacked dataset. Now we will recode these variables in the same format as done for impdata:\n\n# Categorical age\nsummary(dat.ineligible.stacked$age)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.00    5.00   12.00   23.25   41.00   80.00\ndat.ineligible.stacked$age.cat &lt;- \n  with(dat.ineligible.stacked, \n       ifelse(age &gt;= 20 & age &lt; 50, \"20-49\", \n              ifelse(age &gt;= 50 & age &lt; 65, \"50-64\", \n                     ifelse(age &gt;= 65, \"65+\", NA))))\ndat.ineligible.stacked$age.cat &lt;- \n  factor(dat.ineligible.stacked$age.cat, \n         levels = c(\"20-49\", \"50-64\", \"65+\"))\n\nNote that, we are assigning anyone with less than 20 age as missing.\n\ntable(dat.ineligible.stacked$age.cat, useNA = \"always\")\n#&gt; \n#&gt; 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  1100  2360  3430 18425\n# Recode arthritis\ndat.ineligible.stacked$arthritis &lt;- \n  car::recode(dat.ineligible.stacked$rheumatoid, \n  \" 'No' = 'No arthritis'; 'Yes' = 'Rheumatoid arthritis' \", \n  as.factor = T)\n\nNote: In the above step, we could also create two variables with missing values rather than recoding. The reason is that we will subset the design; no matter whether we recode or create missing values for ineligible, the only information we need from ineligible subjects is their survey features when creating the design.\nThe next step is to combine these two datasets (impdata and dat.ineligible.stacked).\n\n# Variable names in the imputed dataset\nvars &lt;- names(impdata) \n\n# Set up the dataset for ineligible - same variables as impdata\ndat.ineligible.stacked &lt;- dat.ineligible.stacked[, vars]\n\nNow we will merge ineligible and eligible subjects to make the full dataset of 5 \\(\\times\\) 9,254 = 46,270 subjects.\n\nimpdata2 &lt;- rbind(impdata, dat.ineligible.stacked)\nimpdata2 &lt;- impdata2[order(impdata2$.imp, impdata2$studyid),]\ndim(impdata2)\n#&gt; [1] 46270    17\n\n1.4 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on full dataset [with eligible (imputed) and ineligible (unimputed) subjects] of 5 \\(\\times\\) 9,254 = 46,270 subjects and subset the design for 5 \\(\\times\\) 4,716 = 23,580 subjects.\n\nm &lt;- 5\nallImputations &lt;- imputationList(lapply(1:m, function(n) \n  subset(impdata2, subset=.imp==n)))\n\n# Step 2: Survey data analysis\nw.design0 &lt;- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) # Design on full data\nw.design &lt;- subset(w.design0, ineligible == 0) # Subset the design\n\nWe can see the length of the subsetted design:\n\ndim(w.design)\n#&gt; [1] 4191   17    5\n\nThe subsetted design contains 4,191 subjects with 17 variables and 5 imputed datasets. Now we will run the design-adjusted logistic regression on and pool the estimate using Rubin’s rule:\nStep 2: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit &lt;- with(w.design, \n            svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + \n                     sex + education + race + income + bmi + \n                     smoking + htn + diabetes, \n                   family = quasibinomial))\nres &lt;- exp(as.data.frame(cbind(coef(fit[[1]]),\n      coef(fit[[2]]),\n      coef(fit[[3]]),\n      coef(fit[[4]]),\n      coef(fit[[5]]))))\nnames(res) &lt;- paste(\"OR from m =\", 1:5)\nres.rounded &lt;- round(t(res),2)\nkable(res.rounded)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\narthritisRheumatoid arthritis\nage.cat50-64\nage.cat65+\nsexFemale\neducationHigh school\neducationCollege graduate or above\nraceBlack\nraceHispanic\nraceOthers\nincome$20,000 to $74,999\nincome$75,000 and Over\nbmi\nsmokingPrevious smoker\nsmokingCurrent smoker\nhtnYes\ndiabetesYes\n\n\n\nOR from m = 1\n0.02\n2.12\n4.72\n13.04\n0.46\n0.73\n0.73\n0.94\n0.53\n0.86\n0.47\n0.32\n1.03\n1.57\n1.44\n1.34\n3.05\n\n\nOR from m = 2\n0.02\n2.10\n4.75\n13.24\n0.46\n0.72\n0.73\n0.99\n0.54\n0.87\n0.53\n0.36\n1.03\n1.57\n1.48\n1.28\n3.06\n\n\nOR from m = 3\n0.02\n2.12\n4.56\n11.91\n0.47\n0.71\n0.71\n0.97\n0.53\n0.84\n0.49\n0.35\n1.02\n1.60\n1.43\n1.49\n3.14\n\n\nOR from m = 4\n0.02\n2.07\n4.64\n12.72\n0.46\n0.71\n0.73\n0.95\n0.53\n0.88\n0.52\n0.34\n1.03\n1.58\n1.49\n1.45\n2.96\n\n\nOR from m = 5\n0.02\n2.11\n4.58\n12.29\n0.46\n0.74\n0.77\n0.92\n0.53\n0.85\n0.43\n0.29\n1.02\n1.59\n1.43\n1.51\n3.11\n\n\n\n\n\nStep 3: Pooling estimates\n\n# Step 3: Pooled estimates\npooled.estimates &lt;- MIcombine(fit)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nConclusion\nAmong US adults aged 20 years or more, the odds of CVD was approximately twice among those with rheumatoid arthritis than no arthritis.\nReferences\n\n\n\n\nHossain, Md Belal, Jacek A Kopec, Mohammad Atiquzzaman, and Mohammad Ehsanul Karim. 2022. “The Association Between Rheumatoid Arthritis and Cardiovascular Disease Among Adults in the United States During 1999–2018, and Age-Related Effect Modification in Relative and Absolute Scales.” Annals of Epidemiology 71: 23–30.",
    "crumbs": [
      "Missing data analysis",
      "Subpopulations"
    ]
  },
  {
    "objectID": "missingdata6.html",
    "href": "missingdata6.html",
    "title": "MCAR tests",
    "section": "",
    "text": "MCAR tests are essential tools in the data analysis process. They help researchers understand the nature of missingness in their datasets, guide appropriate imputation strategies, and ensure the validity and reliability of statistical analyses.\nMCAR data\nIn the initial chunk, several packages are loaded. These packages provide functions and tools necessary for the subsequent analysis, including multiple imputation, statistical modeling, and data visualization.\n\n# Load required packages\nrequire(mice)\nrequire(mitools)\nrequire(survey)\nrequire(remotes)\nrequire(simcausal)\n\nData generating process\nA Directed Acyclic Graph (DAG) is defined, which represents a causal model of how different variables in the dataset relate to each other. This DAG is used to simulate data based on the relationships defined. We generate L as a function of P and B.\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"B\", distr = \"rnorm\", mean = 0, sd = 1) +\n  node(\"P\", distr = \"rnorm\", mean = 0, sd = .7) +\n  node(\"L\", distr = \"rnorm\", mean = 2 + 2 * P + 3 * B, sd = 3) + \n  node(\"A\", distr = \"rnorm\", mean = 0.5 + L + 2 * P, sd = 1) + \n  node(\"Y\", distr = \"rnorm\", mean = 1.1 * L + 1.3 * A + B + 2 * P, sd = .5)\nDset &lt;- set.DAG(D)\n\nGenerate DAG\nThe previously defined DAG is visualized, providing a graphical representation of the relationships between variables.\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n\n\n\n\n\n\n\nGenerate Data\nUsing the DAG, a dataset is simulated. This dataset will be used for subsequent analysis.\n\nObs.Data &lt;- sim(DAG = Dset, n = 10000, rndseed = 123)\nhead(Obs.Data)\n\n\n  \n\n\nObs.Data.original &lt;- Obs.Data\n\nRandomly set some data to missing\nSome values in the dataset are randomly set to missing (i.e., randomly assign some L values to missing). This simulates a scenario where data might be missing completely at random (MCAR).\n\nset.seed(123)\nObs.Data$L[sample(1:length(Obs.Data$L), size = 1000)] &lt;- NA\nsummary(Obs.Data$L)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt; -17.116  -1.096   1.896   1.968   4.996  20.129    1000\n\nThe missing data patterns in the dataset are visualized. This helps in understanding which variables have missing values and how they might be related.\n\nrequire(VIM)\nres &lt;- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\n\n\n\nVisualize via margin plots\nMargin plots are used to compare the distributions of variables when a particular variable is missing versus when it is observed. This provides insights into how missingness might be related to the values of other variables.\n\nThe red boxplot depicts the distribution of a variable in the data where L has a missing value.\nThe blue boxplot depicts the distribution of the values of a variable in the data where L has an observed value.\n\nSame median and spread (range) may mean no difference in the distribution.\n\n\nmarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\n\n\n\nLittle’s MCAR test\nA statistical test is conducted to determine if data is missing entirely at random (MCAR). The outcome of this test offers a deeper understanding of the reasons behind the missing data.\nLittle’s 1988 chi-squared test evaluates if data is MCAR by checking for significant differences in the means of various missing-value patterns (Little 1988). Based on the test’s statistic and p-value, we can infer if the data is MCAR. The null hypothesis for this test is that the data is MCAR.\n\nThe essence of this test is to compare means across groups with different missing data patterns. It uses a likelihood ratio test and assumes the data follows a multivariate normal distribution.\nIf this test is rejected (indicated by a low p-value or a high statistic), it suggests the data might not be MCAR.\n\nHowever, this test has several limitations (rdrr 2023):\n\nThe test doesn’t pinpoint which specific variables don’t adhere to MCAR, meaning it doesn’t highlight potential variables associated with missingness.\nIt assumes multivariate normality. If this assumption isn’t met, especially with non-normal or categorical variables, the test might not be reliable unless there’s a large sample size.\nThe test assumes that all missing data patterns have the same covariance matrix. This means it can’t detect deviations from MCAR that are based on covariance, especially if the data is Missing at Random (MAR) or Missing Not at Random (MNAR).\nResearch has shown that Little’s MCAR test might have low power, especially when few variables don’t follow MCAR, the association between the data and its missingness is weak, or if the data is MNAR.\nThe test can only reject the MCAR assumption but can’t confirm it. A non-significant result doesn’t necessarily confirm that the data is MCAR.\nEven if the test result is significant, it doesn’t rule out the possibility of the data being MNAR.\n\n\nrequire(naniar)\nmcar_test(Obs.Data)\n\n\n  \n\n\n\nrequire(misty)\nna.test(Obs.Data)\n#&gt;  Little's MCAR Test\n#&gt; \n#&gt;       n nIncomp nPattern chi2 df     p \n#&gt;   10000    1000        2 5.22  5 0.390\n\nMCAR and normality test\nHawkins, in 1981, introduced a method to assess both multivariate normality and the consistency in variances, known as homoscedasticity. This method not only checks for consistent variances but also for mean equality (Hawkins 1981).\nIn 2010, Jamshidian and Jalal suggested a technique to compare covariances among groups with the same missing data patterns. They utilized the Hawkins test for data assumed to be normal and a non-parametric approach for other data types (Jamshidian and Jalal 2010).\nThe following package (Jamshidian and Jalal 2010) tests multivariate normality and homoscedasticity in the context of missing data.\n\n#library(devtools)\n#install_github(\"cran/MissMech\")\nlibrary(MissMech)\ntest.result &lt;- TestMCARNormality(data = Obs.Data)\ntest.result\n#&gt; Call:\n#&gt; TestMCARNormality(data = Obs.Data)\n#&gt; \n#&gt; Number of Patterns:  2 \n#&gt; \n#&gt; Total number of cases used in the analysis:  10000 \n#&gt; \n#&gt;  Pattern(s) used:\n#&gt;           ID   B   P    L   A   Y   Number of cases\n#&gt; group.1    1   1   1   NA   1   1              1000\n#&gt; group.2    1   1   1    1   1   1              9000\n#&gt; \n#&gt; \n#&gt;     Test of normality and Homoscedasticity:\n#&gt;   -------------------------------------------\n#&gt; \n#&gt; Hawkins Test:\n#&gt; \n#&gt;     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#&gt; \n#&gt;     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#&gt;     Provided that normality can be assumed, the hypothesis of MCAR is \n#&gt;     rejected at 0.05 significance level. \n#&gt; \n#&gt; Non-Parametric Test:\n#&gt; \n#&gt;     P-value for the non-parametric test of homoscedasticity:  0.4019219 \n#&gt; \n#&gt;     Reject Normality at 0.05 significance level.\n#&gt;     There is not sufficient evidence to reject MCAR at 0.05 significance level.\nsummary(test.result)\n#&gt; \n#&gt; Number of imputation:  1 \n#&gt; \n#&gt; Number of Patterns:  2 \n#&gt; \n#&gt; Total number of cases used in the analysis:  10000 \n#&gt; \n#&gt;  Pattern(s) used:\n#&gt;           ID   B   P    L   A   Y   Number of cases\n#&gt; group.1    1   1   1   NA   1   1              1000\n#&gt; group.2    1   1   1    1   1   1              9000\n#&gt; \n#&gt; \n#&gt;     Test of normality and Homoscedasticity:\n#&gt;   -------------------------------------------\n#&gt; \n#&gt; Hawkins Test:\n#&gt; \n#&gt;     P-value for the Hawkins test of normality and homoscedasticity:  1.749746e-15 \n#&gt; \n#&gt; Non-Parametric Test:\n#&gt; \n#&gt;     P-value for the non-parametric test of homoscedasticity:  0.4019219\n\n\npng(\"E:/GitHub/EpiMethods/Images/missingdata/boxplot.png\", width = 600, height = 600)\nboxplot(test.result)\ndev.off()\n\n\nboxplot(test.result)\n\n\n\n\n\n\n\n\n\nNon-MCAR data\nSet some data to missing based on a rule\nIn this section, the original dataset is restored. Then, for a specific column, any value greater than a certain threshold is set to ‘missing’. A summary of this column is then provided to understand the distribution of missing values.\n\nObs.Data &lt;- Obs.Data.original\nObs.Data$L[Obs.Data$L &gt; 7.79] &lt;- NA\nsummary(Obs.Data$L)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt; -17.116  -1.482   1.330   1.050   3.954   7.787    1035\n\nThe dataset’s missing data patterns are visualized. This visualization helps in understanding which data points are missing and their distribution across the dataset.\n\nres &lt;- aggr(Obs.Data, plot = FALSE)\nplot(res, numbers = TRUE, prop = FALSE)\n\n\n\n\n\n\n\nVisualize via margin plots\nMargin plots are used to visualize the relationship between two variables, especially when one of them has missing values. Here, the relationship of a specific column with missing values is visualized against other columns in the dataset. This helps in understanding how the missingness in one variable might relate to other variables.\n\nmarginplot(Obs.Data[,c(\"L\", \"P\")])\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"B\")])\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"A\")])\n\n\n\n\n\n\nmarginplot(Obs.Data[,c(\"L\", \"Y\")])\n\n\n\n\n\n\n\nLittle’s MCAR test\nLittle’s MCAR test is applied to the dataset to check if the data is missing completely at random. This test provides a statistic and a p-value to determine the nature of missingness in the data.\n\nmcar_test(Obs.Data)\n\n\n  \n\n\nna.test(Obs.Data)\n#&gt;  Little's MCAR Test\n#&gt; \n#&gt;       n nIncomp nPattern    chi2 df     p \n#&gt;   10000    1035        2 4347.36  5 0.000\n\nMCAR and normality test\nA test is conducted to check if the data follows a multivariate normal distribution and if the variances across different groups are consistent (homoscedasticity). The results of this test, along with a summary and a boxplot visualization, are provided to understand the distribution and characteristics of the data.\n\ntest.result &lt;- TestMCARNormality(data = Obs.Data)\ntest.result\n#&gt; Call:\n#&gt; TestMCARNormality(data = Obs.Data)\n#&gt; \n#&gt; Number of Patterns:  2 \n#&gt; \n#&gt; Total number of cases used in the analysis:  10000 \n#&gt; \n#&gt;  Pattern(s) used:\n#&gt;           ID   B   P    L   A   Y   Number of cases\n#&gt; group.1    1   1   1    1   1   1              8965\n#&gt; group.2    1   1   1   NA   1   1              1035\n#&gt; \n#&gt; \n#&gt;     Test of normality and Homoscedasticity:\n#&gt;   -------------------------------------------\n#&gt; \n#&gt; Hawkins Test:\n#&gt; \n#&gt;     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#&gt; \n#&gt;     Either the test of multivariate normality or homoscedasticity (or both) is rejected.\n#&gt;     Provided that normality can be assumed, the hypothesis of MCAR is \n#&gt;     rejected at 0.05 significance level. \n#&gt; \n#&gt; Non-Parametric Test:\n#&gt; \n#&gt;     P-value for the non-parametric test of homoscedasticity:  0 \n#&gt; \n#&gt;     Hypothesis of MCAR is rejected at  0.05 significance level.\n#&gt;     The multivariate normality test is inconclusive.\nsummary(test.result)\n#&gt; \n#&gt; Number of imputation:  1 \n#&gt; \n#&gt; Number of Patterns:  2 \n#&gt; \n#&gt; Total number of cases used in the analysis:  10000 \n#&gt; \n#&gt;  Pattern(s) used:\n#&gt;           ID   B   P    L   A   Y   Number of cases\n#&gt; group.1    1   1   1    1   1   1              8965\n#&gt; group.2    1   1   1   NA   1   1              1035\n#&gt; \n#&gt; \n#&gt;     Test of normality and Homoscedasticity:\n#&gt;   -------------------------------------------\n#&gt; \n#&gt; Hawkins Test:\n#&gt; \n#&gt;     P-value for the Hawkins test of normality and homoscedasticity:  2.401873e-30 \n#&gt; \n#&gt; Non-Parametric Test:\n#&gt; \n#&gt;     P-value for the non-parametric test of homoscedasticity:  0\n\n\npng(\"E:/GitHub/EpiMethods/Images/missingdata/boxplot1.png\", width = 600, height = 600)\nboxplot(test.result)\ndev.off()\n\n\nboxplot(test.result)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHawkins, Douglas M. 1981. “A New Test for Multivariate Normality and Homoscedasticity.” Technometrics 23 (1): 105–10.\n\n\nJamshidian, Mortaza, and Siavash Jalal. 2010. “Tests of Homoscedasticity, Normality, and Missing Completely at Random for Incomplete Multivariate Data.” Psychometrika 75 (4): 649–74.\n\n\nLittle, Roderick JA. 1988. “A Test of Missing Completely at Random for Multivariate Data with Missing Values.” Journal of the American Statistical Association 83 (404): 1198–1202.\n\n\nrdrr. 2023. “Na.test: Little’s Missing Completely at Random (MCAR) Test.” https://rdrr.io/cran/misty/man/na.test.html.",
    "crumbs": [
      "Missing data analysis",
      "MCAR tests"
    ]
  },
  {
    "objectID": "missingdata7.html",
    "href": "missingdata7.html",
    "title": "Effect modification",
    "section": "",
    "text": "In this tutorial, we delve into the concept of effect modification.\n\n\n\n\n\n\nImportant\n\n\n\nWe discussed about effect modification in an earlier discussion.\n\n\nWe start by loading the necessary packages that will aid in the analysis. We also define a function tidy.pool_mi to streamline the pooling process for multiple imputation results.\n\n# Load required packages\nlibrary(survey)\nrequire(interactions)\nrequire(mitools)\nrequire(mice)\nrequire(miceadds)\nlibrary(modelsummary)\n\ntidy.pool_mi &lt;- function(x, ...) {\n  msg &lt;- capture.output(out &lt;- summary(x, ...))\n  out$term &lt;- row.names(out)\n  colnames(out) &lt;- c(\"estimate\", \"std.error\", \"statistic\", \"p.value\",\n                     \"conf.low\", \"conf.high\", \"miss\", \"term\")\n  return(out)\n}\n\nData\nWe load a dataset named smi. This dataset is a list of multiple imputed datasets, which is evident from the structure and the way we access its elements.\n\nrequire(mitools)\ndata(smi)\nlength(smi)\n#&gt; [1] 2\nlength(smi[[1]])\n#&gt; [1] 5\nhead(smi[[1]][[1]])\n\n\n  \n\n\n\nModel with interaction and ORs\nWe’re interested in understanding how the variable wave interacts with sex in predicting drinkreg. We fit two logistic regression models, one for each level of the sex variable (0 males, 1 females), to understand this interaction. wave is the exposure variable here.\nFor effect modifier sex = 0 (males)\n\nmodels &lt;- with(smi, glm(drinkreg~ wave + sex + wave*sex, family = binomial()))\nsummary(pool(models, rule = \"rubin1987\"), conf.int = TRUE, exponentiate = TRUE)[2,]\n\n\n  \n\n\n\nFor effect modifier sex = 1 (females) (just changing reference)\n\nmodels2&lt;-with(smi, glm(drinkreg~ wave + I(sex==0) + wave*I(sex==0),family=binomial()))\nsummary(pool(models2, rule = \"rubin1987\"),conf.int = TRUE, exponentiate = TRUE)[2,]\n\n\n  \n\n\n\n\nNotice the ORs for wave in the above 2 analyses. These are basically our target.\nFor proper survey data analysis, you will have to work with design and make sure you subset your subpopulation (those eligible) appropriately.\nSimple slopes analyses\nWe perform a simple slopes analysis for each imputed dataset. This analysis helps in understanding the relationship between the predictor and the outcome at specific levels of the moderator.\n\na1 &lt;- sim_slopes(models[[1]], pred = wave, modx = sex)\na2 &lt;- sim_slopes(models[[2]], pred = wave, modx = sex)\na3 &lt;- sim_slopes(models[[3]], pred = wave, modx = sex)\na4 &lt;- sim_slopes(models[[4]], pred = wave, modx = sex)\na5 &lt;- sim_slopes(models[[5]], pred = wave, modx = sex)\n\nAfter obtaining the results from each imputed dataset, we pool them to get a consolidated result. This is done separately for each level of the sex variable.\nPooled results for sex = 0\n\n# For sex = 0\nef.lev &lt;- 1\nest &lt;- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse &lt;- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr &lt;- se^2\nOR &lt;- exp(est)\nOR.se &lt;- OR * se\nOR.v &lt;- OR.se^2\n\nmod_pooled &lt;- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n\n\n  \n\n\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(as.list(OR), as.list(OR.v))\n#&gt;    results         se   (lower  upper) missInfo\n#&gt; 1 1.272164 0.08386552 1.107118 1.43721     12 %\n\nPooled results for sex = 1\n\n# For sex = 1\nef.lev &lt;- 2\nest &lt;- c(a1$slopes$Est.[ef.lev],\n         a2$slopes$Est.[ef.lev],\n         a3$slopes$Est.[ef.lev],\n         a4$slopes$Est.[ef.lev],\n         a5$slopes$Est.[ef.lev])\nse &lt;- c(a1$slopes$S.E.[ef.lev],\n        a2$slopes$S.E.[ef.lev],\n        a3$slopes$S.E.[ef.lev],\n        a4$slopes$S.E.[ef.lev],\n        a5$slopes$S.E.[ef.lev])\nvr &lt;- se^2\nOR &lt;- exp(est)\nOR.se &lt;- OR * se\nOR.v &lt;- OR.se^2\n\nmod_pooled &lt;- miceadds::pool_mi(qhat=OR, u=OR.v)\ntidy.pool_mi(mod_pooled)\n\n\n  \n\n\nsummary(MIcombine(as.list(OR), as.list(OR.v)))\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(as.list(OR), as.list(OR.v))\n#&gt;    results         se   (lower   upper) missInfo\n#&gt; 1 1.225598 0.07204679 1.083838 1.367357     12 %",
    "crumbs": [
      "Missing data analysis",
      "Effect modification"
    ]
  },
  {
    "objectID": "missingdata12.html",
    "href": "missingdata12.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Setup\nThis tutorial demonstrates how to perform a survey-weighted survival analysis using NHANES data when there are missing values in the predictors. Instead of relying on a complete-case analysis, which can lose statistical power and introduce bias, we will use multiple imputation with the mice package in R.\nThe workflow follows the structure of a standard epidemiological analysis:\nFirst, we load the necessary R packages. We then load the dat.full.with.mortality.RDS file (from here), which contains the merged NHANES and mortality data from 1999-2018.\n# Load all necessary packages for the analysis\nlibrary(dplyr)\nlibrary(car)\nlibrary(survival)\nlibrary(mice)         # For imputation\nlibrary(survey)       # For survey analysis (svydesign, svyglm, etc.)\nlibrary(mitools)      # FOR imputationList() and other MI tools\nlibrary(Publish)\nlibrary(DataExplorer)\nlibrary(knitr)\nlibrary(kableExtra)\n# devtools::install_github(\"ehsanx/svyTable1\", build_vignettes = TRUE, dependencies = TRUE)\nlibrary(svyTable1) # for svypooled\n# Set survey option for compatibility\noptions(survey.want.obsolete = TRUE)\n\n# Load the full, merged dataset with mortality information\ndat.full.with.mortality &lt;- readRDS(\"Data/missingdata/dat.full.with.mortality.RDS\")",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#data-preparation-and-cleaning",
    "href": "missingdata12.html#data-preparation-and-cleaning",
    "title": "Survival Analysis",
    "section": "Data Preparation and Cleaning",
    "text": "Data Preparation and Cleaning\nBefore imputation, we must create the final analytic variables and clean the dataset. This includes creating the exposure, survival time, and status variables, and then dropping all unnecessary raw or intermediate columns.\n\n# --- Create Analytic Variables ---\n\n# 1. Exposure Variable ('exposure.cat')\ndat.full.with.mortality$exposure.cat &lt;- car::recode(\n  dat.full.with.mortality$smoking.age,\n  \"0 = 'Never smoked'; 1:9 = 'Started before 10';\n   10:14 = 'Started at 10-14'; 15:17 = 'Started at 15-17';\n   18:20 = 'Started at 18-20'; 21:80 = 'Started after 20';\n   else = NA\",\n  as.factor = TRUE\n)\ndat.full.with.mortality$exposure.cat &lt;- factor(\n  dat.full.with.mortality$exposure.cat,\n  levels = c(\"Never smoked\", \"Started before 10\", \"Started at 10-14\",\n             \"Started at 15-17\", \"Started at 18-20\", \"Started after 20\")\n)\n\n# 2. Survival Time ('stime.since.birth') and Status ('status_all')\ndat.full.with.mortality$stime.since.birth &lt;-\n  ((dat.full.with.mortality$age * 12) + dat.full.with.mortality$mort_permth_int) / 12\n# 'status_all' is our event indicator, derived from 'mort_stat'. It's essential for the Surv() object.\ndat.full.with.mortality$status_all &lt;- dat.full.with.mortality$mort_stat\n\n# 3. Categorical Year ('year.cat')\ndat.full.with.mortality$year.cat &lt;- dat.full.with.mortality$year\nlevels(dat.full.with.mortality$year.cat) &lt;- c(\n  \"1999-2000\", \"2001-2002\", \"2003-2004\", \"2005-2006\", \"2007-2008\",\n  \"2009-2010\", \"2011-2012\", \"2013-2014\", \"2015-2016\", \"2017-2018\"\n)\n\n# --- Define the Analytic Cohort & Drop Unnecessary Variables ---\n\n# 4. Apply age restriction (20-79 years)\ndat.analytic &lt;- subset(dat.full.with.mortality, age &gt;= 20 & age &lt; 80)\n\n# 5. Drop all raw, intermediate, or unused columns\nvars_to_drop &lt;- c(\n  \"age\", \"born\", \"smoking.age\", \"smoked.while.child\", \"smoking\", \"year\",\n  \"mort_eligstat\", \"mort_stat\", \"mort_ucod_leading\", \"mort_diabetes\",\n  \"mort_hyperten\", \"mort_permth_int\", \"mort_permth_exm\"\n)\ndat.analytic[vars_to_drop] &lt;- NULL\n\n# Verify the cleaned data\ncat(\"Remaining columns for analysis:\\n\")\n#&gt; Remaining columns for analysis:\nnames(dat.analytic)\n#&gt;  [1] \"id\"                \"sex\"               \"race\"             \n#&gt;  [4] \"psu\"               \"strata\"            \"survey.weight.new\"\n#&gt;  [7] \"exposure.cat\"      \"stime.since.birth\" \"status_all\"       \n#&gt; [10] \"year.cat\"\nplot_missing(dat.analytic)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\nprofile_missing(dat.analytic)",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#multiple-imputation-with-mice",
    "href": "missingdata12.html#multiple-imputation-with-mice",
    "title": "Survival Analysis",
    "section": "Multiple Imputation with mice 🪄",
    "text": "Multiple Imputation with mice 🪄\nWe will impute the missing values in dat.analytic before running the final models.\nPreparing for Imputation\nWe must include the survival outcome information in the imputation model. The best way to do this is by creating and including the Nelson-Aalen cumulative hazard estimate (White and Royston 2009).\n\n# Create the Nelson-Aalen cumulative hazard estimate\n# It's a more informative summary of survival than time alone\ndat.analytic$nelson_aalen &lt;- nelsonaalen(\n  dat.analytic,\n  time = stime.since.birth,\n  status = status_all\n)\nsummary(dat.analytic$nelson_aalen)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n#&gt; 0.000000 0.006434 0.026145 0.122797 0.112218 2.763365      134\nsummary(dat.analytic$stime.since.birth)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;   21.08   43.67   57.08   57.22   70.58   99.42     134\ntable(dat.analytic$status_all)\n#&gt; \n#&gt;     0     1 \n#&gt; 44475  6215\nhist(dat.analytic$nelson_aalen)\n\n\n\n\n\n\n\nConfiguring the Predictor Matrix\nThe predictorMatrix tells mice what to do. Here’s the logic for our setup:\n\n\nWhat variable are we imputing?\n\nexposure.cat\n\n\n\nSince exposure.cat is the only predictor with missing data, it’s the only variable we will actively impute in this tutorial.\n\n\nWhat about the missing outcome data?\n\nData shows that stime.since.birth and status_all also have missing values.\nIt is standard practice not to impute the outcome variables in a survival analysis.\n\n\n\nWhat variables will help the imputation (i.e., act as predictors)?\n\n\nOutcome Information: status_all and nelson_aalen. These are crucial for making the imputation model compatible with the survival analysis.\n\nConfounders: sex, race, year.cat.\n\nAuxiliary Variables: psu, strata, and survey.weight.new. Including the survey design variables makes the imputation “survey-aware” and more accurate.\n\n\n\nWhat variables will we ignore as predictors?\n\n\nid (it’s just an identifier).\n\nstime.since.birth (its information is better and more simply captured by nelson_aalen).\n\n\n\n\n# Initialize the predictor matrix\npred_matrix &lt;- make.predictorMatrix(dat.analytic)\n\n# --- DO NOT use these variables AS PREDICTORS ---\n# We exclude the raw survival time and identifier variables from being predictors.\npred_matrix[, c(\"id\", \"stime.since.birth\")] &lt;- 0\n\n# --- DO NOT IMPUTE these variables ---\n# These variables are complete, identifiers, or part of the outcome.\npred_matrix[c(\"id\", \"sex\", \"race\", \"psu\", \"strata\", \"survey.weight.new\",\n              \"stime.since.birth\", \"status_all\", \"year.cat\",\n              \"nelson_aalen\"), ] &lt;- 0\n\n# Run the imputation. m and maxit are low for demonstration\nimputed_data &lt;- mice(\n  dat.analytic,\n  m = 2,              # Number of imputed datasets\n  maxit = 2,         # Number of iterations per imputation\n  predictorMatrix = pred_matrix,\n  method = 'pmm',     # Predictive Mean Matching is a good default\n  seed = 123          # For reproducibility\n)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   1   2  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   2   1  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt;   2   2  exposure.cat*  stime.since.birth  status_all  nelson_aalen\n#&gt; Warning: Number of logged events: 8\n\n\n\nkable(pred_matrix)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nsex\nrace\npsu\nstrata\nsurvey.weight.new\nexposure.cat\nstime.since.birth\nstatus_all\nyear.cat\nnelson_aalen\n\n\n\nid\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsex\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nrace\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\npsu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nstrata\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsurvey.weight.new\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nexposure.cat\n0\n1\n1\n1\n1\n1\n0\n0\n1\n1\n1\n\n\nstime.since.birth\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nstatus_all\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nyear.cat\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nnelson_aalen\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#survival-analysis-on-imputed-data",
    "href": "missingdata12.html#survival-analysis-on-imputed-data",
    "title": "Survival Analysis",
    "section": "Survival Analysis on Imputed Data 📊",
    "text": "Survival Analysis on Imputed Data 📊\nWith our m=2 complete datasets, we follow the “analyze then pool” procedure:\n\n\nAnalyze: Run the svycoxph model on each of the 2 datasets.\n\nPool: Combine the 2 sets of results into a single, final estimate using pool().\n\n\n# --- 5. Survival Analysis on Imputed Data  ---\n\n# --- Step 5.1: Re-integrate Ineligible Subjects for Correct Survey Variance ---\n\n# First, extract the 'm' imputed datasets into a single long-format data frame.\n# Add a flag to identify this group as our analytic/eligible sample.\nimputed_analytic_data &lt;- complete(imputed_data, \"long\", include = FALSE)\nimputed_analytic_data$eligible &lt;- 1\n\n# Next, identify the subjects from the original full dataset who were NOT in our analytic sample.\n# The analytic sample was defined as age &gt;= 20 & age &lt; 80.\ndat_ineligible &lt;- subset(dat.full.with.mortality, !(age &gt;= 20 & age &lt; 80))\n\n# Replicate this ineligible dataset 'm' times, once for each imputation.\nineligible_list &lt;- lapply(1:imputed_data$m, function(i) {\n  df &lt;- dat_ineligible\n  df$.imp &lt;- i # Add the imputation number\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# Now, align the columns. Add columns that exist in the imputed data (like 'nelson_aalen')\n# to the ineligible data, filling them with NA.\ncols_to_add &lt;- setdiff(names(imputed_analytic_data), names(ineligible_stacked))\nineligible_stacked[, cols_to_add] &lt;- NA\n\n# Set the eligibility flag for this group to 0.\nineligible_stacked$eligible &lt;- 0\n\n# CRITICAL: Ensure the column order is identical before row-binding.\nineligible_final &lt;- ineligible_stacked[, names(imputed_analytic_data)]\n\n# Finally, combine the imputed analytic data with the prepared ineligible data.\nimputed_full_data &lt;- rbind(imputed_analytic_data, ineligible_final)\n\n\n# --- Step 5.2: Create Survey Design and Run Pooled Analysis ---\n\n# Create the complex survey design object using an `imputationList`.\n# This tells the survey package how to handle the 'm' imputed datasets.\n# The design is specified on the *full* data to capture the total population structure.\ndesign_full &lt;- svydesign(ids = ~psu,\n                         strata = ~strata,\n                         weights = ~survey.weight.new,\n                         nest = TRUE,\n                         data = imputationList(split(imputed_full_data, imputed_full_data$.imp)))\n\n# Subset the design object to include only the eligible participants for the analysis.\n# This ensures variance is calculated correctly based on the full sample design.\ndesign_analytic &lt;- subset(design_full, eligible == 1)\n\n# Fit the Cox model across all 'm' imputed datasets using the `with()` function.\n# This is more efficient than a for-loop.\nfit_pooled &lt;- with(design_analytic,\n                   svycoxph(Surv(stime.since.birth, status_all) ~ exposure.cat + sex + race + year.cat))\n\n# Pool the results from the list of model fits using Rubin's Rules.\npooled_results &lt;- pool(fit_pooled)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (301) clusters.\n#&gt; svydesign(ids = ids, probs = probs, strata = strata, variables = variables, \n#&gt;     fpc = fpc, nest = nest, check.strata = check.strata, weights = weights, \n#&gt;     data = d, pps = pps, calibrate.formula = calibrate.formula, \n#&gt;     ...)\n\n# Display the final, pooled results.\nprint(\"--- Final Adjusted Cox Model Results (from Pooled Imputed Data) ---\")\n#&gt; [1] \"--- Final Adjusted Cox Model Results (from Pooled Imputed Data) ---\"\nsummary(pooled_results, conf.int = TRUE, exponentiate = TRUE)\n\n\n  \n\n\n\n\n# Option A: Fallacy-safe table showing only the main exposure\nsvypooled(\n  pooled_model = pooled_results,\n  main_exposure = \"exposure.cat\",\n  adj_var_names = c(\"sex\", \"race\", \"year.cat\"),\n  measure = \"HR\",\n  title = \"Adjusted Hazard Ratios for All-Cause Mortality\"\n)\n\n\nAdjusted Hazard Ratios for All-Cause Mortality\n\nCharacteristic\nHR (95% CI)\np-value\n\n\n\nexposure.cat\n\n\nStarted before 10\n2.71 (2.05, 3.59)\n&lt;0.001\n\n\nStarted at 10-14\n2.37 (2.10, 2.68)\n&lt;0.001\n\n\nStarted at 15-17\n1.95 (1.77, 2.16)\n&lt;0.001\n\n\nStarted at 18-20\n1.48 (1.34, 1.65)\n&lt;0.001\n\n\nStarted after 20\n1.54 (1.39, 1.70)\n&lt;0.001\n\n\n\n\n   Adjusted for: sex, race, year.cat\n\n\n\n\n\n\n# Option B: Full table for an appendix\nsvypooled(\n  pooled_model = pooled_results,\n  main_exposure = \"exposure.cat\",\n  adj_var_names = c(\"sex\", \"race\", \"year.cat\"),\n  measure = \"HR\",\n  title = \"Full Adjusted Model Results (for Appendix)\",\n  fallacy_safe = FALSE\n)\n\n\nFull Adjusted Model Results (for Appendix)\n\nCharacteristic\nHR (95% CI)\np-value\n\n\n\nexposure.cat\n\n\nStarted before 10\n2.71 (2.05, 3.59)\n&lt;0.001\n\n\nStarted at 10-14\n2.37 (2.10, 2.68)\n&lt;0.001\n\n\nStarted at 15-17\n1.95 (1.77, 2.16)\n&lt;0.001\n\n\nStarted at 18-20\n1.48 (1.34, 1.65)\n&lt;0.001\n\n\nStarted after 20\n1.54 (1.39, 1.70)\n&lt;0.001\n\n\nsex\n\n\nFemale\n0.74 (0.69, 0.79)\n&lt;0.001\n\n\nrace\n\n\nBlack\n1.60 (1.47, 1.74)\n&lt;0.001\n\n\nHispanic\n1.04 (0.93, 1.16)\n0.493\n\n\nOthers\n1.13 (0.96, 1.34)\n0.148\n\n\nyear.cat\n\n\n2001-2002\n0.97 (0.86, 1.08)\n0.538\n\n\n2003-2004\n0.84 (0.75, 0.95)\n0.004\n\n\n2005-2006\n0.76 (0.68, 0.85)\n&lt;0.001\n\n\n2007-2008\n0.78 (0.67, 0.90)\n0.001\n\n\n2009-2010\n0.67 (0.59, 0.78)\n&lt;0.001\n\n\n2011-2012\n0.63 (0.52, 0.77)\n&lt;0.001\n\n\n2013-2014\n0.58 (0.49, 0.70)\n&lt;0.001\n\n\n2015-2016\n0.35 (0.27, 0.46)\n&lt;0.001\n\n\n2017-2018\n0.20 (0.13, 0.29)\n&lt;0.001",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#conclusion",
    "href": "missingdata12.html#conclusion",
    "title": "Survival Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial demonstrated how to replace a complete-case analysis with a multiple imputation workflow for a survey-weighted survival analysis. By correctly preparing the data, configuring mice with survival-specific information, and pooling the final results, we can generate valid estimates that properly account for missing data.",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdata12.html#references",
    "href": "missingdata12.html#references",
    "title": "Survival Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nKarim, Mohammad Ehsanul, Md Belal Hossain, and Chuyi Zheng. 2025. “Examining the Role of Race/Ethnicity and Sex in Modifying the Association Between Early Smoking Initiation and Mortality: A 20-Year NHANES Analysis.” AJPM Focus 4 (2): 100282.\n\n\nWhite, Ian R, and Patrick Royston. 2009. “Imputing Missing Covariate Values for the Cox Model.” Statistics in Medicine 28 (15): 1982–98.",
    "crumbs": [
      "Missing data analysis",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "missingdataF.html",
    "href": "missingdataF.html",
    "title": "R functions (M)",
    "section": "",
    "text": "The list of new R functions introduced in this Missing data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\naggr\nVIM\nTo calculate/plot the missing values in the variables\n\n\nboxplot\nbase/graphics\nTo produce a box plot\n\n\nbwplot\nmice\nTo produce box plot to compare the imputed and observed values\n\n\ncolMeans\nbase\nTo compute the column-wise mean, i.e., mean for each variable/column\n\n\ncomplete\nmice\nTo extract the imputed dataset\n\n\ncomplete.cases\nbase/stats\nTo select the complete cases, i.e., observations without missing values\n\n\nD1\nmice\nTo conduct the multivariate Wald test with D1-statistic\n\n\ndensityplot\nmice\nTo produce desnsity plots\n\n\nexpression\nbase\nTo set/create an expression\n\n\nimputationList\nmice\nTo combine multiple imputed datasets\n\n\nmarginplot\nVIM\nTo draw a scatterplot with additional information when there are missing values\n\n\nmcar_test\nnaniar\nTo conduct Little's MCAR test\n\n\nmd.pattern\nmice\nTo see the pattern of the missing data\n\n\nmice\nmice\nTo impute missing data where the argument m represents the number of multiple imputation\n\n\nMIcombine\nmitools\nTo combine/pool the results using Rubin's rule\n\n\nMIextract\nmitools\nTo extract parameters from a list of outputs\n\n\nna.test\nmisty\nTo conduct Little's MCAR test\n\n\nparlmice\nmice\nTo run `mice` function in parallel, i.e., parallel computing of mice\n\n\nplot_missing\nDataExplorer\nTo plot the profile of missing values, e.g., the percentage of missing per variable\n\n\npool\nmice\nTo pool the results using Rubin's rule\n\n\npool.compare\nmice\nTo compare two nested models\n\n\npool_mi\nmiceadds\nTo combine/pool the results using Rubin's rule\n\n\nquickpred\nmice\nTo set imputation model based on the correlation\n\n\nsim_slopes\ninteractions\nTo perform simple slope analyses\n\n\nTestMCARNormality\nMissMech\nTo test multivariate normality and homoscedasticity in the context of missing data\n\n\nunlist\nbase\nTo convert a list to a vector",
    "crumbs": [
      "Missing data analysis",
      "R functions (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html",
    "href": "missingdataQ.html",
    "title": "Quiz (M)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html#live-quiz",
    "href": "missingdataQ.html#live-quiz",
    "title": "Quiz (M)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataQ.html#download-quiz",
    "href": "missingdataQ.html#download-quiz",
    "title": "Quiz (M)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Missing data analysis",
      "Quiz (M)"
    ]
  },
  {
    "objectID": "missingdataS.html",
    "href": "missingdataS.html",
    "title": "App (M)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from various imputations as well as pooled results from multiple imputation.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveM\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, and ggplot2 packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Missing data analysis",
      "App (M)"
    ]
  },
  {
    "objectID": "missingdataE.html",
    "href": "missingdataE.html",
    "title": "Exercise 1 (M)",
    "section": "",
    "text": "Problem Statement\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n4 Outcome variables\n4 predictors (i.e., exposure variables)\nConfounders and other variables",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#problem-statement",
    "href": "missingdataE.html#problem-statement",
    "title": "Exercise 1 (M)",
    "section": "",
    "text": "id: Respondent sequence number\nsurvey.weight: Full sample 4 year interview weight\npsu: Masked pseudo PSU\nstrata: Masked pseudo strata (strata is nested within PSU)\n\n\n\nweight.loss.behavior: doing lifestyle behavior changes - controlling or losing weight\nexercise.behavior: doing lifestyle behavior changes - increasing exercise\nsalt.behavior: doing lifestyle behavior changes - reducing salt in diet\nfat.behavior: doing lifestyle behavior changes - reducing fat in diet\n\n\n\nweight.loss.advice: told by a doctor or health professional - to control/lose weight\nexercise.advice: told by a doctor or health professional - to exercise\nsalt.advice: told by a doctor or health professional - to reduce salt in diet\nfat.advice: told by a doctor or health professional - to reduce fat/calories\n\n\n\ngender: Gender\nage: Age in years at screening\nincome: The ratio of family income to federal poverty level\nrace: Race/Ethnicity\nbmi: Body Mass Index in kg/m\\(^2\\)\n\ncomorbidity: Comorbidity index\nDIQ010: Self-report to have been informed by a provider to have diabetes\nBPQ020: Self-report to have been informed by a provider to have hypertension",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-1-analytic-dataset",
    "href": "missingdataE.html#question-1-analytic-dataset",
    "title": "Exercise 1 (M)",
    "section": "Question 1: Analytic dataset",
    "text": "Question 1: Analytic dataset\n1(a) Importing dataset\n\n# download the data in the same folder\nload(\"Data/missingdata/Williams2021.RData\")\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\n# Drop &lt; 18 years\ndat &lt;- dat.full\ndat &lt;- dat[dat$age &gt;= 18,] \n\n# Eligibility\ndat &lt;- dat[dat$DIQ010==\"Yes\" | dat$BPQ020==\"Yes\",] \n\n# Dataset with missing values in outcomes, predictors, and confounders\ndat.with.miss &lt;- dat\nnrow(dat.with.miss) # N = 4,746\n#&gt; [1] 4746\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the “Self-reported behavior change and receipt of advice” paragraph.\n\n\ndat &lt;- dat.with.miss\n\n# Drop missing or don't know outcomes \ndat &lt;- dat[complete.cases(dat$weight.loss.behavior),]\ndat &lt;- dat[complete.cases(dat$exercise.behavior),]\ndat &lt;- dat[complete.cases(dat$salt.behavior),]\ndat &lt;- dat[complete.cases(dat$fat.behavior),]\n\n# Drop missing or don't know predictors\ndat &lt;- dat[complete.cases(dat$weight.loss.advice),]\ndat &lt;- dat[complete.cases(dat$exercise.advice),]\ndat &lt;- dat[complete.cases(dat$salt.advice),]\ndat &lt;- dat[complete.cases(dat$fat.advice),] \n\n# Dataset without missing in outcomes and predictors but missing in confounders \ndat.with.miss2 &lt;- dat\nnrow(dat.with.miss2) # N = 4,716\n#&gt; [1] 4716\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nHint 1: The authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nHint 2: You may need to generate the Condition variable.\nHint 3: age and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\ndat &lt;- dat.with.miss2\n\n# Create the condition variable\ndat$condition &lt;- NA\ndat$condition[dat$BPQ020 == \"Yes\"] &lt;- \"Hypertension Only\"\ndat$condition[dat$DIQ010 == \"Yes\"] &lt;- \"Diabetes Only\"\ndat$condition[dat$BPQ020 == \"Yes\" & dat$DIQ010 == \"Yes\"] &lt;- \"Both\"\ndat$condition &lt;- factor(dat$condition, levels=c(\"Hypertension Only\", \"Diabetes Only\",\n                                                \"Both\"))\ntable(dat$condition, useNA = \"always\")\n#&gt; \n#&gt; Hypertension Only     Diabetes Only              Both              &lt;NA&gt; \n#&gt;              3004               533              1179                 0\n\n\n# First column of Table 1\nvars &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 &lt;- CreateTableOne(vars = vars, data = dat, includeNA = F)\nprint(tab1, format = \"f\")\n#&gt;                          \n#&gt;                           Overall      \n#&gt;   n                        4716        \n#&gt;   gender = Male            2332        \n#&gt;   age (mean (SD))         59.94 (14.96)\n#&gt;   income                               \n#&gt;      &lt;100%                  881        \n#&gt;      100-199%              1193        \n#&gt;      200-299%               672        \n#&gt;      300-399%               424        \n#&gt;      400+%                  930        \n#&gt;   race                                 \n#&gt;      Hispanic              1161        \n#&gt;      Non-Hispanic white    1630        \n#&gt;      Non-Hispanic black    1239        \n#&gt;      Others                 686        \n#&gt;   bmi                                  \n#&gt;      Reference              753        \n#&gt;      Overweight            1372        \n#&gt;      Obese                 2287        \n#&gt;   condition                            \n#&gt;      Hypertension Only     3004        \n#&gt;      Diabetes Only          533        \n#&gt;      Both                  1179        \n#&gt;   comorbidity (mean (SD))  1.29 (1.45)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "href": "missingdataE.html#question-2-dealing-with-missing-values-in-confoudners-100-grade",
    "title": "Exercise 1 (M)",
    "section": "Question 2: Dealing with missing values in confoudners [100% grade]",
    "text": "Question 2: Dealing with missing values in confoudners [100% grade]\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nHint 1: There are four outcome variables and four predictor variables used in the study.\nHint 2: The authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\n# Create the condition variable in the analytic dataset\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\"] &lt;- \"Hypertension Only\"\ndat.with.miss2$condition[dat.with.miss2$DIQ010 == \"Yes\"] &lt;- \"Diabetes Only\"\ndat.with.miss2$condition[dat.with.miss2$BPQ020 == \"Yes\" & \n                          dat.with.miss2$DIQ010 == \"Yes\"] &lt;- \"Both\"\ndat.with.miss2$condition &lt;- factor(dat.with.miss2$condition, \n                                  levels=c(\"Hypertension Only\", \"Diabetes Only\", \"Both\"))\n\n# Variables of interest\nvars &lt;- c(\n  # Outcome\n  \"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\",\n  \n  #Predictors\n  \"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\",\n   \n  # Confounders       \n  \"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\n\n# Plot missing values using DataExplorer\nplot_missing(dat.with.miss2[,vars])\n\n\n\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nPerform multiple imputations to deal with missing values only in confounders. Use the dataset created in Dataset with missing values only in confounders (dat.with.miss2). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to lose weights, i.e., create only the first column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types. lapply function could be helpful.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 5: Set your seed to 123.\nHint 6: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 7: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 8: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Setup the data such that the variables are of appropriate types\nfactor.names &lt;- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \n                  \"fat.behavior\", \"weight.loss.advice\", \"exercise.advice\", \n                  \"salt.advice\", \"fat.advice\", \"gender\", \"income\", \"race\", \"bmi\", \n                  \"condition\")\n# your codes \n\n\n## Change the reference categories\n# your codes\n\n\n## Imputation model set up\n# your codes\n\n\n## Regression analysis\n# your codes\n\n\n## Pooled estimates\n# your codes",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "href": "missingdataE.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners-optional",
    "title": "Exercise 1 (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners [optional]\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nHint 1: Setup the data such that the variables are of appropriate types.\nHint 2: Relevel the confounders as shown in Table 3.\nHint 3: Use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nHint 4: Include all 4 outcomes and 4 predictors in your imputation model.\nHint 5: Consider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nHint 6: Set your seed to 123.\nHint 7: Remove any subject ID variable from the imputation model, if created in an intermediate step.\nHint 8: The point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nHint 9: Remember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n## Create a missing indicator so that MID can be applied\n# your codes here\n\n## MID\n# your codes here",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html",
    "href": "missingdataEsolution.html",
    "title": "Exercise 1 Solution (M)",
    "section": "",
    "text": "Question 1: Analytic dataset\nWe will use the article by Williams AR, Wilson-Genderson M, Thomson MD. (2021)\nWe will reproduce some results from the article. The authors used NHANES 2015-16 and 2017-18 datasets to create their analytic dataset. The combined dataset contains 19,225 subjects with 20 relevant variables for this exercise:\nSurvey information\n4 Outcome variables\n4 predictors (i.e., exposure variables)\nConfounders and other variables",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-1-analytic-dataset",
    "href": "missingdataEsolution.html#question-1-analytic-dataset",
    "title": "Exercise 1 Solution (M)",
    "section": "",
    "text": "1(a) Importing dataset\n\n# 1(a) Importing dataset\nload(\"Data/missingdata/Williams2021.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\n\ndim(dat.full)\n#&gt; [1] 19225    20\n\n1(b) Subsetting according to eligibility\nCreate a dataset with missing values in outcomes, predictors, and confounders. As shown in Figure 1, the sample size should be 4,746.\n\n# 1(b) Subsetting according to eligibility (N = 4,746)\ndat.analytic &lt;- dat.full %&gt;%\n  filter(age &gt;= 18, DIQ010 == \"Yes\" | BPQ020 == \"Yes\")\n\n# Dataset with missing values in outcomes, predictors, and confounders\nnrow(dat.analytic) # N = 4,746\n#&gt; [1] 4746\n\n1(c) Dataset with missing values only in confounders\nCreate a dataset with missing values in only in confounders. There should not be any missing values in the outcomes or predictors. As shown in Figure 1, the sample size should be 4,716.\n\nHint: there are four outcome variables and four predictors in this paper. Read the “Self-reported behavior change and receipt of advice” paragraph.\n\n\n# 1(c) Dataset with no missingness in outcomes/predictors (N = 4,716)\noutcome_vars &lt;- c(\"weight.loss.behavior\", \"exercise.behavior\", \"salt.behavior\", \"fat.behavior\")\npredictor_vars &lt;- c(\"weight.loss.advice\", \"exercise.advice\", \"salt.advice\", \"fat.advice\")\n\ndat.with.miss &lt;- dat.analytic %&gt;%\n  filter(complete.cases(.[, c(outcome_vars, predictor_vars)]))\n\n# Dataset without missing in outcomes and predictors but missing in confounders \nnrow(dat.with.miss) # N = 4,716\n#&gt; [1] 4716\n\n1(d) Reproduce Table 1\nCreate the first column of Table 1 of the article.\n\nThe authors reported unweighted frequencies, and thus, survey features should not be utilized to answer this question. Use tableone package.\nYou may need to generate the Condition variable.\nage and comorbidity are numerical variables. tableone package gives mean (SD) for numerical variables by default. For this exercise, instead of reporting the frequency, you could report the mean (SD) for age and comorbidity.\n\n\n# 1(d) Reproduce Table 1\ndat.with.miss &lt;- dat.with.miss %&gt;%\n  mutate(condition = case_when(\n    BPQ020 == \"Yes\" & DIQ010 == \"Yes\" ~ \"Both\",\n    DIQ010 == \"Yes\"                   ~ \"Diabetes Only\",\n    BPQ020 == \"Yes\"                   ~ \"Hypertension Only\"\n  )) %&gt;%\n  mutate(condition = factor(condition, levels = c(\"Hypertension Only\", \"Diabetes Only\", \"Both\")))\n\n\nvars_for_table1 &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\ntab1 &lt;- CreateTableOne(vars = vars_for_table1, data = dat.with.miss, includeNA = FALSE)\nprint(tab1, format = \"f\")\n#&gt;                          \n#&gt;                           Overall      \n#&gt;   n                        4716        \n#&gt;   gender = Male            2332        \n#&gt;   age (mean (SD))         59.94 (14.96)\n#&gt;   income                               \n#&gt;      &lt;100%                  881        \n#&gt;      100-199%              1193        \n#&gt;      200-299%               672        \n#&gt;      300-399%               424        \n#&gt;      400+%                  930        \n#&gt;   race                                 \n#&gt;      Hispanic              1161        \n#&gt;      Non-Hispanic white    1630        \n#&gt;      Non-Hispanic black    1239        \n#&gt;      Others                 686        \n#&gt;   bmi                                  \n#&gt;      Reference              753        \n#&gt;      Overweight            1372        \n#&gt;      Obese                 2287        \n#&gt;   condition                            \n#&gt;      Hypertension Only     3004        \n#&gt;      Diabetes Only          533        \n#&gt;      Both                  1179        \n#&gt;   comorbidity (mean (SD))  1.29 (1.45)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-2-dealing-with-missing-values-in-confoudners",
    "href": "missingdataEsolution.html#question-2-dealing-with-missing-values-in-confoudners",
    "title": "Exercise 1 Solution (M)",
    "section": "Question 2: Dealing with missing values in confoudners",
    "text": "Question 2: Dealing with missing values in confoudners\n2(a) Check missingness using a plot\nIn the dataset created in 1(c), use a plot to check missingness. In the plot, include only the outcome variables, predictors, and confounders.\n\nThere are four outcome variables and four predictor variables used in the study.\nThe authors considered the following confounders: gender, age, income, race, bmi, condition, and comorbidity.\n\n\n# 2(a) Check missingness using a plot\nconfounder_vars &lt;- c(\"gender\", \"age\", \"income\", \"race\", \"bmi\", \"condition\", \"comorbidity\")\nplot_missing(dat.with.miss[, c(outcome_vars, predictor_vars, confounder_vars)])\n\n\n\n\n\n\n\n2(b) Reproduce Table 3: Multiple imputation\nLet’s we are interested in exploring the relationship between weight loss advice (exposure) and weight loss behavior (outcome). Perform multiple imputations to deal with missing values only in confounders. Use the dataset dat.with.miss.\nConsider:\n\n5 imputed datasets\n10 iterations\nFit the design-adjusted logistic regression in all of the 5 imputed datasets\nObtain the pooled adjusted odds ratio with the 95% confidence intervals, i.e., create only the first column of Table 3.\n\nYou must:\n\nSetup the data such that the variables are of appropriate types (e.g., factors, numeric). lapply function could be helpful.\nRelevel the confounders as shown in Table 3.\nUse the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nThere are four exposure and four outcome variables in the dataset. Include all these variables in the imputation model.\nConsider predictive mean matching (pmm) method for bmi and comorbidity variable in the imputation model.\nSet your seed to 123.\nRemove any subject ID variable from the imputation model, if created in an intermediate step.\n\nHints:\n\nThe point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nRemember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n# Helper Function to Reduce Duplication\n# This function automates the post-analysis process for imputed data.\n# It takes a 'mira' object (the result of running a model on multiple imputations)\n# and returns a clean, formatted table of odds ratios and confidence intervals.\npool_and_format_results &lt;- function(fit_mira) {\n  # 'MIcombine' applies Rubin's Rules to pool the estimates from each imputed dataset.\n  pooled &lt;- MIcombine(fit_mira)\n  \n  # Exponentiate the log-odds coefficients and confidence intervals to get Odds Ratios (ORs).\n  or_estimates &lt;- exp(coef(pooled))\n  ci_estimates &lt;- exp(confint(pooled))\n  \n  # Create and return a final data frame, rounding the results for clarity.\n  results_df &lt;- data.frame(\n    OR = round(or_estimates, 2),\n    `2.5 %` = round(ci_estimates[, 1], 2),\n    `97.5 %` = round(ci_estimates[, 2], 2)\n  )\n  return(results_df)\n}\n\n#----------------------------------------------------------------\n# Question 2(b) Reproduce Table 3 (First Column)\n#----------------------------------------------------------------\n\n# Convert character variables to factors for regression modeling.\nfactor_vars &lt;- c(outcome_vars, predictor_vars, \"gender\", \"income\", \"race\", \"bmi\", \"condition\")\ndat.with.miss[, factor_vars] &lt;- lapply(dat.with.miss[, factor_vars], factor)\n\n# Set the reference levels for categorical variables to match the study's analysis.\n# This ensures the odds ratios are interpreted correctly (e.g., comparing 'Female' to 'Male').\ndat.with.miss &lt;- dat.with.miss %&gt;%\n  mutate(\n    gender = relevel(gender, ref = \"Male\"),\n    income = relevel(income, ref = \"400+%\"),\n    race = relevel(race, ref = \"Non-Hispanic white\"),\n    condition = relevel(condition, ref = \"Both\")\n  )\n\n# Remove the original variables used to create 'condition' to prevent\n# perfect multicollinearity during the imputation step.\ndat_for_imputation &lt;- dat.with.miss %&gt;%\n  select(-DIQ010, -BPQ020)\n\n# Run the multiple imputation using the 'mice' package.\n# m = 5: Creates 5 imputed datasets.\n# maxit = 10: Runs 10 iterations for the Gibbs sampler to converge.\n# meth = c(bmi = \"pmm\"): Specifies predictive mean matching for imputing BMI.\nmeth &lt;- make.method(dat_for_imputation)\nmeth[\"bmi\"] &lt;- \"pmm\"\nmeth[\"comorbidity\"] &lt;- \"pmm\"\nimputation_q2 &lt;- mice(dat_for_imputation, m = 5, maxit = 10, seed = 123, print = FALSE,\n                      meth = meth)\n\n# Extract the 5 imputed datasets into a single \"long\" format data frame.\nimpdata_q2 &lt;- complete(imputation_q2, \"long\", include = FALSE)\n\n\n# Re-integrate Ineligible Subjects for Correct Survey Variance\n# This entire block is necessary to correctly estimate variance with survey data.\n# The 'svydesign' object needs the full sample structure, even those outside the analysis.\n\n# Add a flag to identify the analytic (eligible) group in the imputed data.\nimpdata_q2$eligible &lt;- 1\n\n# 1. Isolate all subjects from the original dataset who were not in our analytic sample.\ndat_ineligible &lt;- filter(dat.full, !(id %in% dat.with.miss$id))\n\n# 2. Replicate the ineligible dataset 5 times, once for each imputation (.imp = 1 to 5).\nineligible_list &lt;- lapply(1:5, function(i) {\n  df &lt;- dat_ineligible\n  df$.imp &lt;- i\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# 3. Find which columns exist in the imputed data but not in the ineligible data.\ncols_to_add &lt;- setdiff(names(impdata_q2), names(ineligible_stacked))\n\n# 4. Add these missing columns (e.g., 'condition', imputed values) to the ineligible data, filling with NA.\nineligible_stacked[, cols_to_add] &lt;- NA\n\n# 5. Set the eligibility flag for this group to 0.\nineligible_stacked$eligible &lt;- 0\n\n# 6. CRITICAL: Force the ineligible data to have the exact same column names and order\n#    as the imputed data. This prevents errors when row-binding.\nineligible_final &lt;- ineligible_stacked[, names(impdata_q2)]\n\n# 7. Combine the imputed analytic data with the prepared ineligible data.\nimpdata2_full &lt;- rbind(impdata_q2, ineligible_final)\n\n\n# --- Survey Analysis --- #\n\n# Create the complex survey design object.\n# 'imputationList' tells the survey package how to handle the 5 imputed datasets.\n# The design is specified on the *full* data to capture the total population structure.\ndesign_full &lt;- svydesign(ids = ~psu, weights = ~survey.weight, strata = ~strata,\n                         data = imputationList(split(impdata2_full, impdata2_full$.imp)), nest = TRUE)\n\n# Subset the design object to include only the eligible participants for analysis.\ndesign_analytic &lt;- subset(design_full, eligible == 1)\n\n# Fit the logistic regression model across each of the 5 imputed datasets.\n# 'with()' applies the 'svyglm' function to each dataset in the 'design_analytic' object.\nfit_q2 &lt;- with(design_analytic,\n               svyglm(I(weight.loss.behavior == \"Yes\") ~ weight.loss.advice + gender + age +\n                        income + race + bmi + condition + comorbidity, family = binomial(\"logit\")))\n\n# Use our helper function to pool the 5 models and display the final formatted results.\npool_and_format_results(fit_q2)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "missingdataEsolution.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners",
    "href": "missingdataEsolution.html#question-3-dealing-with-missing-values-in-outcome-predictor-and-confoudners",
    "title": "Exercise 1 Solution (M)",
    "section": "Question 3: Dealing with missing values in outcome, predictor, and confoudners",
    "text": "Question 3: Dealing with missing values in outcome, predictor, and confoudners\nPerform multiple imputations to deal with missing values only in outcome, predictor, confounders. Use the Multiple Imputation then deletion (MID) approach. Use the dataset created in Subsetting according to eligibility (dat.with.miss). Consider 5 imputed datasets, 5 iterations, and fit the design-adjusted logistic regression in all of the 5 imputed datasets. Obtain the pooled adjusted odds ratio with the 95% confidence intervals. In this case, consider only one outcome and one predictor that are related to reduce fat/calories, i.e., create only the fourth column of Table 3.\n\nSetup the data such that the variables are of appropriate types.\nRelevel the confounders as shown in Table 3.\nUse the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nInclude all 4 outcomes and 4 predictors in your imputation model.\nConsider predictive mean matching method for bmi and comorbidity variable in the imputation model.\nSet your seed to 123.\nRemove any subject ID variable from the imputation model, if created in an intermediate step.\nThe point and interval estimates could be slightly different than shown in Table 3. But they should very close.\nRemember to keep count of the ineligible subjects from the full data, and consider adding them back in the imputed datasets (so that all the weight, strata and cluster information are available in the design).\n\n\n#----------------------------------------------------------------\n# Question 3: Dealing with Missing Values in All Variables (MID)\n#----------------------------------------------------------------\n\n# For this question, we start with 'dat.analytic' (N=4,746), which includes\n# subjects who have missing values in the outcome and predictor variables.\n\n# STEP 1: Prepare the data for imputation.\n# This involves creating the 'condition' variable and setting correct factor levels.\ndat_q3_prepped &lt;- dat.analytic %&gt;%\n  mutate(\n    # Create the 'condition' variable based on diabetes and hypertension status.\n    condition = case_when(\n      BPQ020 == \"Yes\" & DIQ010 == \"Yes\" ~ \"Both\",\n      DIQ010 == \"Yes\"                   ~ \"Diabetes Only\",\n      BPQ020 == \"Yes\"                   ~ \"Hypertension Only\"\n    ),\n    # Set the reference levels for categorical variables to ensure correct interpretation of model results.\n    condition = factor(condition, levels = c(\"Hypertension Only\", \"Diabetes Only\", \"Both\")),\n    gender = relevel(as.factor(gender), ref = \"Male\"),\n    income = relevel(as.factor(income), ref = \"400+%\"),\n    race = relevel(as.factor(race), ref = \"Non-Hispanic white\"),\n    condition = relevel(condition, ref = \"Both\")\n  )\n\n# STEP 2: Convert all relevant columns to the factor data type *after* creating 'condition'.\n# This avoids the \"undefined columns selected\" error.\nfactor_vars &lt;- c(outcome_vars, predictor_vars, \"gender\", \"income\", \"race\", \"bmi\", \"condition\")\ndat_q3_prepped[, factor_vars] &lt;- lapply(dat_q3_prepped[, factor_vars], factor)\n\n# STEP 3: Remove the original source variables to prevent perfect multicollinearity during imputation.\ndat_for_imputation_q3 &lt;- dat_q3_prepped %&gt;%\n  select(-DIQ010, -BPQ020)\n\n# STEP 4: Perform multiple imputation on the prepared dataset.\n# This will impute missing values in outcomes, predictors, and confounders.\nimputation_q3 &lt;- mice(dat_for_imputation_q3, m = 5, maxit = 5, seed = 123, print = FALSE,\n                     meth = meth) # Use predictive mean matching for BMI.\n\n# Extract the 5 complete datasets into a single \"long\" data frame.\nimpdata_q3 &lt;- complete(imputation_q3, \"long\", include = FALSE)\n\n\n# Re-integrate Ineligible Subjects for Correct Survey Variance \n# The survey design requires the full sample structure to correctly estimate variance.\n\n# Add an 'eligible' flag to the imputed analytic data.\nimpdata_q3$eligible &lt;- 1\n\n# Isolate subjects from the original full dataset who were not part of this analysis.\ndat_ineligible_q3 &lt;- filter(dat.full, !(id %in% dat.analytic$id))\n\n# Replicate the ineligible dataset 5 times, one for each imputation set.\nineligible_list &lt;- lapply(1:5, function(i) {\n  df &lt;- dat_ineligible_q3\n  df$.imp &lt;- i      # Add imputation number\n  return(df)\n})\nineligible_stacked &lt;- do.call(rbind, ineligible_list)\n\n# Align the structure of the ineligible data to perfectly match the imputed data.\ncols_to_add &lt;- setdiff(names(impdata_q3), names(ineligible_stacked))\nineligible_stacked[, cols_to_add] &lt;- NA # Add missing columns (e.g., 'condition', '.id') as NA.\nineligible_stacked$eligible &lt;- 0         # Set eligibility flag for this group.\n\n# CRITICAL: Reorder columns to prevent 'rbind' errors.\nineligible_final &lt;- ineligible_stacked[, names(impdata_q3)]\n\n# Combine the imputed analytic data and the prepared ineligible data.\nimpdata3_full &lt;- rbind(impdata_q3, ineligible_final)\n\n\n# --- Survey Analysis using Multiple Imputation then Deletion (MID) --- #\n\n# Create the complex survey design object on the full combined data.\ndesign_full_q3 &lt;- svydesign(ids = ~psu, weights = ~survey.weight, strata = ~strata,\n                         data = imputationList(split(impdata3_full, impdata3_full$.imp)), nest = TRUE)\n\n# Apply the \"Deletion\" step of MID: subset the design to the analytic group\n# AND only those with complete data for the specific variables in this model.\ndesign_analytic_q3 &lt;- subset(design_full_q3, eligible == 1 & complete.cases(fat.behavior, fat.advice))\n\n# Fit the logistic regression model across each of the 5 imputed datasets.\nfit_q3 &lt;- with(design_analytic_q3,\n               svyglm(I(fat.behavior == \"Yes\") ~ fat.advice + gender + age + income +\n                        race + bmi + condition + comorbidity, family = binomial(\"logit\")))\n\n# Use our helper function to pool the results and display the final formatted table.\npool_and_format_results(fit_q3)",
    "crumbs": [
      "Missing data analysis",
      "Exercise 1 Solution (M)"
    ]
  },
  {
    "objectID": "propensityscore.html",
    "href": "propensityscore.html",
    "title": "Propensity score",
    "section": "",
    "text": "Background\nThis chapter provides a comprehensive set of tutorials that guide readers through various methodologies of Propensity Score Matching (PSM) and Multiple Imputation (MI) using R, with practical applications using datasets like the Canadian Community Health Survey (CCHS) and the National Health and Nutrition Examination Survey (NHANES). The tutorials explore different scenarios and methodologies in handling and analyzing data, particularly focusing on estimating treatment effects and managing missing data. They delve into specific examples, such as exploring the relationship between Osteoarthritis (OA) and Cardiovascular Disease (CVD), and between Body Mass Index (BMI) and diabetes, while emphasizing the importance of accurate data handling, variable management, and robust analysis through PSM and MI. The tutorials are meticulously structured, providing step-by-step guides, code snippets, and thorough explanations, ensuring that readers can comprehend and replicate the processes in their research, thereby enhancing the reliability and robustness of their analyses, especially in the presence of missing data.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#background",
    "href": "propensityscore.html#background",
    "title": "Propensity score",
    "section": "",
    "text": "Stepping into this chapter, we are diving deeper into the world of survey data analysis, exploring how to combine propensity score matching (PSM) and strategies for handling missing data. PSM helps us balance our data, making sure our study groups are comparable, while managing missing data ensures our results are as accurate as possible. In the upcoming tutorials, we will weave through the steps of using PSM while also dealing with the gaps in our data, ensuring our analyses are solid and dependable. So, this chapter is not just a next step, but a leap into a more advanced exploration, blending matching methods with careful data handling strategies.\n\n\n\n\n\n\nNote\n\n\n\nShould you find yourself seeking a refresher on PSM, we invite you to revisit our dedicated/external tutorial, which elucidates PSM within a non-survey data analysis context. This resource not only provides a foundational understanding but also serves as a comprehensive guide through the nuanced steps of PSM. Additionally, our external discussion page offers a succinct summary of the tutorial and thoughtfully extends the conversation into more intricate directions, exploring the complexities and advanced applications of PSM (propensity score weighting, categorical and continuous exposure). Both resources are crafted to enhance your understanding and application of PSM, ensuring a robust and informed approach to your data analysis journey\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#overview-of-tutorials",
    "href": "propensityscore.html#overview-of-tutorials",
    "title": "Propensity score",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nCovariate matching using CCHS: example of OA-CVD\nThe tutorial illustrate a comprehensive data analysis workflow using R, focusing on matching methods to estimate treatment effects with the CCHS data. Initially, we conduct data pre-processing steps to handle categorical variables and missing data. Subsequent sections delve into setting up design objects for survey-weighted analyses and conducting preliminary analyses to explore variable distributions and treatment effects. The core of the analysis involves implementing matching techniques, starting with a single variable and progressively including more variables to refine the matching. Various matching scenarios are explored, each followed by logistic regression models to estimate treatment effects.\n\n\nPropensity score matching using CCHS: revisiting example of OA-CVD\nThe tutorial provides a thorough walkthrough of implementing Propensity Score Matching (PSM) in R, specifically in the context of an OA - CVD health study from the CCHS. PSM is utilized to mitigate bias from confounding variables in observational studies by pairing treated and control units with analogous propensity scores. The guide underscores that PSM is iterative, often requiring refinement of the matching strategy to achieve satisfactory covariate balance in the matched sample. Various strategies for estimating treatment effects in the matched sample are explored, each with distinct assumptions and implications. The tutorial also delves into different matching strategies, such as nearest-neighbor matching with and without calipers, matching with different ratios, and matching with replacement, all while emphasizing the importance of assessing and re-assessing covariate balance at each step using both graphical and numerical methods.\n\n\nPropensity score matching using NHANES: example of OA - CVD\nThe provided text outlines methodologies for conducting PSM using the NHANES dataset, with a particular emphasis on handling survey design and weights in the analysis. Three distinct approaches, attributed to Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), are delineated, each with a structured four-step process: 1) specifying the propensity score model, 2) matching treated and untreated subjects based on estimated propensity scores, 3) comparing baseline characteristics between matched groups, and 4) estimating treatment effects using the matched sample. The procedures utilize various R packages and functions to manipulate data, visualize missing data patterns, format variables, and perform analyses, ensuring that survey weights and design are appropriately considered to avoid bias in population-level effect estimates. The text underscores the importance of incorporating survey design into at least propensity score outcome analysis (e.g., during step 4: treatment effect estimation), as neglecting survey weights can significantly impact the estimates of population-level effects.\n\n\nPropensity score matching using NHANES: example of BMI - diabetes\nThe tutorial provides a comprehensive guide on implementing PSM in R, utilizing the NHANES dataset, with a specific focus on diabetes as an outcome and body mass index (BMI) as an exposure variable. The methodology encompasses ensuring accurate and reproducible results in PSM. The tutorial, again, meticulously follows three distinct approaches for PSM, as recommended by Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018), each providing a unique perspective on handling and analyzing variables within the propensity score model. Notably, the tutorial introduces a nuanced approach to variable handling, model specifications, and matching steps, ensuring a thorough understanding of implementing PSM with varied methodologies. Furthermore, the tutorial introduces a “double adjustment” step in each approach, providing a robust estimate of the treatment effect while adjusting for covariates, thereby offering readers a holistic view on conducting PSM with a different set of variables and methodologies in the analysis steps.\n\n\nPropensity score matching using NHANES when some variables have missing observations\nThis tutorial offers a clear and straightforward guide on how to use Propensity Score Matching (PSM) and Multiple Imputation (MI) in R, using the NHANES dataset for practical illustration. The main goal is to explore the relationship between “diabetes” (outcome) and being “born in the US” (exposure), while effectively managing missing data through MI. The first part of the tutorial, focusing on logistic regression, explains how to perform multiple imputations, fit a logistic regression model to all imputed datasets, and then obtain pooled Odds Ratios (OR) and 95% confidence intervals. Following this, the PSM analysis section carefully applies the PSM method, following Zanutto E. L. (2006), to all imputed datasets, and presents the pooled OR estimates and 95% confidence intervals. The tutorial emphasizes the crucial role of managing missing data through multiple imputation and provides a detailed, step-by-step guide, including code and thorough explanations, to ensure a deep understanding and ability to replicate the PSM with MI process in epidemiological research. This resource is invaluable for researchers and data analysts looking to strengthen their analyses when dealing with missing data.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting",
    "href": "propensityscore.html#propensity-score-weighting",
    "title": "Propensity score",
    "section": "Propensity score weighting",
    "text": "Propensity score weighting\nIn this section, propensity score weighting with different methods is employed to estimate treatment effects in a modified NHANES dataset. Three different propensity score weighting approaches are applied: Zanutto’s (2006), DuGoff et al.’s (2014), and Austin et al.’s (2018). Each approach involves multiple steps, including the estimation of propensity scores, the calculation of weights (both unstabilized and stabilized), balance checking to ensure covariate balancing, and fitting outcome models using the weighted data. The double adjustment technique is also considered in each approach.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-matching-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score matching with multiple imputation in subpopulations",
    "text": "Propensity score matching with multiple imputation in subpopulations\nIn this section, the goal is to use propensity score matching (PSM) with multiple imputation (MI) to analyze a modified dataset from NHANES 2017-2018. The analysis focuses on specific subpopulations defined by eligibility criteria. This analysis provides insights into how to handle complex survey data with missing values and perform PSM with MI for subpopulations defined by eligibility criteria.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "href": "propensityscore.html#propensity-score-weighting-with-multiple-imputation-in-subpopulations",
    "title": "Propensity score",
    "section": "Propensity score weighting with multiple imputation in subpopulations",
    "text": "Propensity score weighting with multiple imputation in subpopulations\nIn this chapter, we employ propensity score (PS) weighting with MI to analyze a modified dataset from NHANES 2017-2018, focusing on specific subpopulations defined by eligibility criteria. Here we demonstrate how to perform PS weighting with MI for subpopulations defined by eligibility criteria.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "href": "propensityscore.html#propensity-score-weighting-for-multiple-treatment-categories",
    "title": "Propensity score",
    "section": "Propensity score weighting for multiple treatment categories",
    "text": "Propensity score weighting for multiple treatment categories\nIn this chapter, we use propensity score weighting for multiple treatment categories using CCHS data.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Propensity score"
    ]
  },
  {
    "objectID": "propensityscore0.html",
    "href": "propensityscore0.html",
    "title": "Concepts (S)",
    "section": "",
    "text": "Propensity Score Analysis\nThis section provides a comprehensive exploration into various facets of propensity score (PS) methods and their application in observational studies and surveys. Beginning with an in-depth look into key concepts and calculations related to ATE and ATT, the content navigates through the practical application and diagnostic checks of covariate balance using the SMD. It further elucidates the methodology and application of PS, particularly focusing on matching and weighting to mitigate bias and create comparable groups for causal inference. The intricacies of employing PS methods within surveys are explored, highlighting different approaches and the incorporation of design variables in PS and outcome models. Fundamental assumptions for causal inference, namely Conditional Exchangeability, Positivity, and Causal Consistency, are dissected to form a foundational understanding for conducting robust causal analyses. Additionally, the content optionally delves into the nuances of implementing IPW in surveys. Lastly, additional optional content features an insightful workshop, offering more explanations of PS method implementations in research contexts.",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#reading-list",
    "href": "propensityscore0.html#reading-list",
    "title": "Concepts (S)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Peter C. Austin 2011)\nOptional reading:\n\nPropensity score introduction (Karim 2021) External link\nExtensions of Propensity score approaches External link: prepared for Guest Lecture in SPPH 500/007 (Analytical Methods in Epidemiological Research)\nPropensity score for complex surveys External link: Uses the same lectures here, with some added text descriptions. This also includes a a structured framework for reporting analyses using PS methods in research manuscripts.\nReporting guideline (Stuart 2018; Simoneau et al. 2022)\nAssumptions (Hernán and Robins 2020)\n\nTheoretical references for propensity score analyses in complex surveys:\n(Peter C. Austin, Jembere, and Chiu 2018; DuGoff, Schuler, and Stuart 2014; Zanutto 2006; Leite, Stapleton, and Bettini 2018; Lenis, Ackerman, and Stuart 2018; Lenis et al. 2017; Ridgeway et al. 2015)",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#video-lessons",
    "href": "propensityscore0.html#video-lessons",
    "title": "Concepts (S)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nTarget parameters\n\n\n\nAverage Treatment Effect (ATE) vs. Average Treatment effect on the Treated (ATT)\n\n\n\n\n\n\n\n\n\n\n\n\nBalance\n\n\n\nBalance and standardized mean difference (SMD) in observational studies\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score matching in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity score weighting in complex survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConference Workshop (Optional)\n\n\n\nPost Conference Workshop for 2021 Conference - Canadian Society for Epidemiology and Biostatistics (CSEB)",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#video-lesson-slides",
    "href": "propensityscore0.html#video-lesson-slides",
    "title": "Concepts (S)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nTarget parameters\n\n\nBalance\n\n\nPropensity score matching\n\n\nPropensity score matching in complex survey\n\n\nPropensity score weighting in complex survey\n\n\nCausal Assumptions\n\n\nFAQ",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#links",
    "href": "propensityscore0.html#links",
    "title": "Concepts (S)",
    "section": "Links",
    "text": "Links\nTarget parameters\n\nGoogle Slides\nPDF Slides\n\nBalance\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching\n\nGoogle Slides\nPDF Slides\n\nPropensity score matching in complex survey\n\nGoogle Slides\nPDF Slides\n\nPropensity score weighting in complex survey\n\nGoogle Slides\nPDF Slides\n\nCausal Assumptions\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore0.html#references",
    "href": "propensityscore0.html#references",
    "title": "Concepts (S)",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C. 2011. “A Tutorial and Case Study in Propensity Score Analysis: An Application to Estimating the Effect of in-Hospital Smoking Cessation Counseling on Mortality.” Multivariate Behavioral Research 46 (1): 119–51.\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nHernán, Miguel A., and James M. Robins. 2020. “Chapter 3.” In Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\n\n\nKarim, ME. 2021. “Understanding Propensity Score Matching.” 2021. https://ehsanx.github.io/psw/.\n\n\nLeite, Walter L., Laura M. Stapleton, and Eduardo F. Bettini. 2018. “Propensity Score Analysis of Complex Survey Data with Structural Equation Modeling: A Tutorial with Mplus.” Structural Equation Modeling: A Multidisciplinary Journal, 1–22.\n\n\nLenis, Diego, Benjamin Ackerman, and Elizabeth A. Stuart. 2018. “Measuring Model Misspecification: Application to Propensity Score Methods with Complex Survey Data.” Computational Statistics & Data Analysis.\n\n\nLenis, Diego, Thuan Quoc Nguyen, Dong, and Elizabeth A. Stuart. 2017. “It’s All about Balance: Propensity Score Matching in the Context of Complex Survey Data.” Biostatistics.\n\n\nRidgeway, Greg, Stephanie A. Kovalchik, Beth Ann Griffin, and Mohammed U. Kabeto. 2015. “Propensity Score Analysis with Survey Weighted Data.” Journal of Causal Inference 3 (2): 237–49.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette, Johanna Muñoz, Robert W Platt, John Petkau, et al. 2022. “Recommendations for the Use of Propensity Score Methods in Multiple Sclerosis Research.” Multiple Sclerosis Journal 28 (9): 1467–80.\n\n\nStuart, Elizabeth A. 2018. “Chapter 28. Propensity Scores and Matching Methods.” In The Reviewer’s Guide to Quantitative Methods in the Social Sciences, Second Edition, edited by Gregory R. Hancock, Ralph O. Mueller, and Laura M. Stapleton. Routledge.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91.",
    "crumbs": [
      "Propensity score",
      "Concepts (S)"
    ]
  },
  {
    "objectID": "propensityscore1.html",
    "href": "propensityscore1.html",
    "title": "Exact Matching (CCHS)",
    "section": "",
    "text": "In the following code chunk, we load the necessary R libraries for our analysis. MatchIt is used for matching methods to find comparable control units, tableone for creating Table 1 to describe baseline characteristics, Publish for generating readable output of regression analysis, and survey for analyzing complex survey samples.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(Publish)\nrequire(survey)\n\nLoad data\nIn the following code chunk, we load the CCHS dataset which is related to the Canadian Community Health Survey (CCHS). We then use ls() to list all objects in the workspace and str to display the structure of the data frame, providing a quick overview of the data and checking for any character variables.\n\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"\nstr(analytic.miss) # is there any character variable?\n#&gt; 'data.frame':    397173 obs. of  22 variables:\n#&gt;  $ CVD      : chr  \"event\" \"no event\" \"no event\" \"no event\" ...\n#&gt;  $ age      : chr  \"65 years and over\" \"65 years and over\" \"30-39 years\" \"65 years and over\" ...\n#&gt;  $ sex      : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n#&gt;  $ married  : chr  \"single\" \"single\" \"not single\" \"single\" ...\n#&gt;  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#&gt;  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" \"Post-2nd grad.\" ...\n#&gt;  $ income   : chr  \"$29,999 or less\" \"$29,999 or less\" \"$80,000 or more\" \"$29,999 or less\" ...\n#&gt;  $ bmi      : Factor w/ 3 levels \"Underweight\",..: NA NA 2 NA 2 NA 3 NA 2 3 ...\n#&gt;  $ phyact   : chr  \"Inactive\" \"Inactive\" \"Inactive\" \"Inactive\" ...\n#&gt;  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n#&gt;  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"stressed\" \"Not too stressed\" ...\n#&gt;  $ smoke    : Factor w/ 3 levels \"Never smoker\",..: 3 1 3 3 2 2 3 1 2 2 ...\n#&gt;  $ drink    : Factor w/ 3 levels \"Never drank\",..: 2 1 2 2 2 2 3 1 2 2 ...\n#&gt;  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 2 3 3 3 2 2 2 2 ...\n#&gt;  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ province : Factor w/ 2 levels \"South\",\"North\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ weight   : num  142.8 71.4 168.3 71.4 196.1 ...\n#&gt;  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ OA       : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n#&gt;  $ immigrate: Factor w/ 3 levels \"not immigrant\",..: 1 1 3 1 1 1 1 1 1 1 ...\n\nData pre-pocessing\nIn the following code chunk, we define a vector containing the names of variables of interest that needs to be converted to factor variables. We then convert these variables to factors, ensuring they are treated as categorical in subsequent analyses. We also recode the Osteoarthritis (OA) variable into a numeric binary format and display the frequency table of OA before and after the transformation.\n\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"doctor\", \"drink\", \"bp\", \"province\",\n               \"immigrate\", \"fruit\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic.miss[var.names] &lt;- lapply(analytic.miss[var.names] , factor)\ntable(analytic.miss$OA)\n#&gt; \n#&gt; Control      OA \n#&gt;  314542   40943\nanalytic.miss$OA &lt;- as.numeric(analytic.miss$OA==\"OA\") \ntable(analytic.miss$OA)\n#&gt; \n#&gt;      0      1 \n#&gt; 314542  40943\n\nIdentify subjects with missing\nIn the following code chunk, we create a new variable miss and initially assign all its values to 1 in the full dataset (that contains some missing observations). We then adjust this assignment by setting miss to 0 for observations that are also present in another complete case dataset. That means any row with miss equal to 0 means that row has no missing observations. Finally, we display the frequency table of the miss variable to check the number of missing and non-missing observations.\n\nanalytic.miss$miss &lt;- 1\nhead(analytic.miss$ID) # full data\n#&gt; [1] 1 2 3 4 5 6\nhead(analytic2$ID) # complete case\n#&gt; [1]  3  5  7 10 11 13\nhead(analytic.miss$ID[analytic.miss$ID %in% analytic2$ID])\n#&gt; [1]  3  5  7 10 11 13\n# if associated with complete case, assign miss &lt;- 0\nanalytic.miss$miss[analytic.miss$ID %in% analytic2$ID] &lt;- 0\ntable(analytic.miss$miss)\n#&gt; \n#&gt;      0      1 \n#&gt; 185613 211560\n\nSetting Design\nUnconditional design\nIn the following code chunk, we explore the summary of the weight variable and establish an unconditional survey design object w.design0 using the svydesign function, which will be used for subsequent survey-weighted analyses. We then explore the summary, standard deviation, and sum of the weights within our design object.\n\nsummary(analytic.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   65.28  126.63  200.09  243.21 7154.95\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nsummary(weights(w.design0))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   65.28  126.63  200.09  243.21 7154.95\nsd(weights(w.design0))\n#&gt; [1] 241.0279\nsum(weights(w.design0))\n#&gt; [1] 79468929\n\nConditioning the design\nIn the following code chunk, we create a new survey design object w.design by subsetting w.design0 to only include observations without missing data (miss == 0). We then explore the summary, standard deviation, and sum of the weights within this new design object.\n\nw.design &lt;- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.17   71.56  137.95  214.61  261.91 7154.95\nsd(weights(w.design))\n#&gt; [1] 254.9346\nsum(weights(w.design))\n#&gt; [1] 39835061\n\nSubset data (more!)\nWe subset the data for fast results (less computation). We will only work with cycle 1.1, and the people from Northern provinces in Canada.\n\nw.design1 &lt;- subset(w.design, cycle == 11 & province == \"North\")\nsum(weights(w.design1))\n#&gt; [1] 42786.28\n\nPreliminary analysis\nTable 1\nIn the following code chunk, we define a new variable vector var.names and create a categorical table using svyCreateCatTable to explore the distribution of age and sex across strata of OA within our subsetted design object w.design1. We then print the table with standardized mean differences (SMD) to assess the balance of these variables across strata.\n\nvar.names &lt;- c(\"age\", \"sex\")\ntab0 &lt;- svyCreateCatTable(var = var.names, strata= \"OA\", data=w.design1,test=FALSE)\nprint(tab0, smd = TRUE)\n#&gt;                       Stratified by OA\n#&gt;                        0               1              SMD   \n#&gt;   n                    40691.2         2095.1               \n#&gt;   age (%)                                              1.084\n#&gt;      20-29 years       10889.4 (26.8)   120.9 ( 5.8)        \n#&gt;      30-39 years       12251.7 (30.1)   237.8 (11.3)        \n#&gt;      40-49 years       11094.0 (27.3)   572.7 (27.3)        \n#&gt;      50-59 years        5346.6 (13.1)  1092.4 (52.1)        \n#&gt;      60-64 years        1109.4 ( 2.7)    71.4 ( 3.4)        \n#&gt;      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   sex = Male (%)       20824.6 (51.2)  1050.8 (50.2)   0.020\n\nTreatment effect\nIn the following code chunk, we fit a logistic regression model using svyglm to estimate the effect of OA and other covariates on the binary outcome CVD (cardiovascular disease). We then use publish to display the results in a readable format.\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design1,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#&gt;   Variable             Units OddsRatio         CI.95    p-value \n#&gt;         OA                        0.89   [0.17;4.59]   0.887411 \n#&gt;        age       20-29 years       Ref                          \n#&gt;                  30-39 years      2.62  [0.29;23.43]   0.389521 \n#&gt;                  40-49 years      4.89  [0.59;40.73]   0.142280 \n#&gt;                  50-59 years     17.95 [2.59;124.68]   0.003550 \n#&gt;                  60-64 years     23.95 [3.41;168.27]   0.001439 \n#&gt;        sex            Female       Ref                          \n#&gt;                         Male      1.32   [0.64;2.71]   0.456222 \n#&gt;     stress  Not too stressed       Ref                          \n#&gt;                     stressed      0.54   [0.21;1.39]   0.198815 \n#&gt;    married        not single       Ref                          \n#&gt;                       single      0.75   [0.31;1.80]   0.513807 \n#&gt;     income   $29,999 or less       Ref                          \n#&gt;              $30,000-$49,999      0.72   [0.24;2.16]   0.556703 \n#&gt;              $50,000-$79,999      0.95   [0.27;3.40]   0.939104 \n#&gt;              $80,000 or more      0.47   [0.10;2.15]   0.332557 \n#&gt;       race         Non-white       Ref                          \n#&gt;                        White      0.33   [0.11;0.94]   0.038131 \n#&gt;        bmi       Underweight       Ref                          \n#&gt;               healthy weight      0.29   [0.03;3.20]   0.310237 \n#&gt;                   Overweight      0.44   [0.04;4.77]   0.503130 \n#&gt;     phyact            Active       Ref                          \n#&gt;                     Inactive      0.84   [0.30;2.40]   0.751345 \n#&gt;                     Moderate      1.02   [0.32;3.27]   0.979528 \n#&gt;      smoke      Never smoker       Ref                          \n#&gt;               Current smoker      0.98   [0.26;3.76]   0.981454 \n#&gt;                Former smoker      0.71   [0.18;2.71]   0.612518 \n#&gt;  immigrate     not immigrant       Ref                          \n#&gt;                   &gt; 10 years      0.14   [0.03;0.78]   0.025010 \n#&gt;                       recent      0.00   [0.00;0.00]    &lt; 1e-04 \n#&gt;      fruit 0-3 daily serving       Ref                          \n#&gt;            4-6 daily serving      1.15   [0.52;2.56]   0.725722 \n#&gt;             6+ daily serving      0.68   [0.17;2.71]   0.583752 \n#&gt;       diab                No       Ref                          \n#&gt;                          Yes      3.08  [0.93;10.23]   0.066677 \n#&gt;        edu          &lt; 2ndary       Ref                          \n#&gt;                    2nd grad.      4.12  [0.87;19.43]   0.074178 \n#&gt;              Other 2nd grad.      3.04  [0.63;14.67]   0.167135 \n#&gt;               Post-2nd grad.      3.00  [0.82;10.98]   0.096939\n\nMatching: Estimating treatment effect\nGoing back to the data (not working on design here while matching)\nIn the following code chunk, we create a new dataset by omitting NA values from analytic.miss and converting it to a data frame. We then create a subset analytic11n which includes only observations from cycle 1.1 and the Northern provinces. We display the dimensions of this subset, as well as frequency tables of OA and a cross-tabulation of OA and age to understand the distribution of our target variable and a key covariate.\n\n# Create the dataset without design features\nanalytic2 &lt;- as.data.frame(na.omit(analytic.miss))\nanalytic11n &lt;- subset(analytic2, cycle == 11 & province == \"North\")\ndim(analytic11n)\n#&gt; [1] 1424   23\ntable(analytic11n$OA)\n#&gt; \n#&gt;    0    1 \n#&gt; 1357   67\ntable(analytic11n$OA,analytic11n$age)\n#&gt;    \n#&gt;     20-29 years 30-39 years 40-49 years 50-59 years 60-64 years\n#&gt;   0         345         432         358         177          45\n#&gt;   1           4          11          18          31           3\n#&gt;    \n#&gt;     65 years and over teen\n#&gt;   0                 0    0\n#&gt;   1                 0    0\n\nMatching by 1 matching variable\nIn the following code chunk, we perform exact matching using a single variable, age. We define the matching formula and apply the matchit function to create matched sets of treated and control units. The resulting matching.obj object is displayed to summarize the matching results.\n\nmatch.formula &lt;- as.formula(\"OA ~ age\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1424 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age\n\nMatching by 2 matching variables\nIn the following code chunk, we extend the matching to include two variables, age and sex. We create a new variable var.comb that concatenates these two variables and display its frequency table and the number of unique combinations. We then perform exact matching using both variables and display the resulting object.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex')])\ntable(var.comb)\n#&gt; var.comb\n#&gt; 20-29 yearsFemale   20-29 yearsMale 30-39 yearsFemale   30-39 yearsMale \n#&gt;               184               165               220               223 \n#&gt; 40-49 yearsFemale   40-49 yearsMale 50-59 yearsFemale   50-59 yearsMale \n#&gt;               187               189               101               107 \n#&gt; 60-64 yearsFemale   60-64 yearsMale \n#&gt;                24                24\nlength(table(var.comb))\n#&gt; [1] 10\nmatch.formula &lt;- as.formula(\"OA ~ age + sex\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1424 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex\n\nMatching by 3 matching variables\nIn the following code chunk, we further extend the matching to include three variables: age, sex, and stress. We explore the unique combinations of these variables and their distribution across levels of OA. We then perform exact matching using these three variables and display the resulting object.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex', 'stress')])\ntable(var.comb)\n#&gt; var.comb\n#&gt; 20-29 yearsFemaleNot too stressed         20-29 yearsFemalestressed \n#&gt;                               157                                27 \n#&gt;   20-29 yearsMaleNot too stressed           20-29 yearsMalestressed \n#&gt;                               147                                18 \n#&gt; 30-39 yearsFemaleNot too stressed         30-39 yearsFemalestressed \n#&gt;                               170                                50 \n#&gt;   30-39 yearsMaleNot too stressed           30-39 yearsMalestressed \n#&gt;                               183                                40 \n#&gt; 40-49 yearsFemaleNot too stressed         40-49 yearsFemalestressed \n#&gt;                               142                                45 \n#&gt;   40-49 yearsMaleNot too stressed           40-49 yearsMalestressed \n#&gt;                               141                                48 \n#&gt; 50-59 yearsFemaleNot too stressed         50-59 yearsFemalestressed \n#&gt;                                72                                29 \n#&gt;   50-59 yearsMaleNot too stressed           50-59 yearsMalestressed \n#&gt;                                78                                29 \n#&gt; 60-64 yearsFemaleNot too stressed         60-64 yearsFemalestressed \n#&gt;                                18                                 6 \n#&gt;   60-64 yearsMaleNot too stressed           60-64 yearsMalestressed \n#&gt;                                20                                 4\nlength(table(var.comb))\n#&gt; [1] 20\ntable(var.comb,analytic11n$OA)\n#&gt;                                    \n#&gt; var.comb                              0   1\n#&gt;   20-29 yearsFemaleNot too stressed 156   1\n#&gt;   20-29 yearsFemalestressed          27   0\n#&gt;   20-29 yearsMaleNot too stressed   144   3\n#&gt;   20-29 yearsMalestressed            18   0\n#&gt;   30-39 yearsFemaleNot too stressed 168   2\n#&gt;   30-39 yearsFemalestressed          49   1\n#&gt;   30-39 yearsMaleNot too stressed   178   5\n#&gt;   30-39 yearsMalestressed            37   3\n#&gt;   40-49 yearsFemaleNot too stressed 130  12\n#&gt;   40-49 yearsFemalestressed          42   3\n#&gt;   40-49 yearsMaleNot too stressed   138   3\n#&gt;   40-49 yearsMalestressed            48   0\n#&gt;   50-59 yearsFemaleNot too stressed  65   7\n#&gt;   50-59 yearsFemalestressed          22   7\n#&gt;   50-59 yearsMaleNot too stressed    67  11\n#&gt;   50-59 yearsMalestressed            23   6\n#&gt;   60-64 yearsFemaleNot too stressed  17   1\n#&gt;   60-64 yearsFemalestressed           5   1\n#&gt;   60-64 yearsMaleNot too stressed    19   1\n#&gt;   60-64 yearsMalestressed             4   0\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 1327 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress\n\nMatching by 4 matching variables\nThe process of matching by 4 variables involves creating combinations of the 4 variables, exploring their distributions, and performing exact matching.\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income')])\n#table(var.comb)\nlength(table(var.comb))\n#&gt; [1] 76\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 900 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income\n\nMatching by 5 matching variables\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race')])\nlength(table(var.comb))\n#&gt; [1] 146\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income + race\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 616 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income, race\n\nMatching by 6 matching variables\n\nvar.comb &lt;- do.call('paste0', \n                    analytic11n[, c('age', 'sex',\n                                    'stress','income','race','edu')])\nlength(table(var.comb))\n#&gt; [1] 354\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + income + race + edu\")\nmatching.obj &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 399 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, income, race, edu\nOACVD.match.11n &lt;- match.data(matching.obj)\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"income\", \"race\", \"edu\")\ntab1 &lt;- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n,test=FALSE)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     337         62               \n#&gt;   age (%)                                       0.565\n#&gt;      20-29 years         63 (18.7)   4 ( 6.5)        \n#&gt;      30-39 years         61 (18.1)  11 (17.7)        \n#&gt;      40-49 years        127 (37.7)  17 (27.4)        \n#&gt;      50-59 years         82 (24.3)  28 (45.2)        \n#&gt;      60-64 years          4 ( 1.2)   2 ( 3.2)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        211 (62.6)  29 (46.8)   0.322\n#&gt;   stress = stressed (%)  42 (12.5)  17 (27.4)   0.381\n#&gt;   income (%)                                    0.115\n#&gt;      $29,999 or less     69 (20.5)  11 (17.7)        \n#&gt;      $30,000-$49,999     57 (16.9)  13 (21.0)        \n#&gt;      $50,000-$79,999     69 (20.5)  12 (19.4)        \n#&gt;      $80,000 or more    142 (42.1)  26 (41.9)        \n#&gt;   race = White (%)      242 (71.8)  43 (69.4)   0.054\n#&gt;   edu (%)                                       0.146\n#&gt;      &lt; 2ndary            73 (21.7)  11 (17.7)        \n#&gt;      2nd grad.            5 ( 1.5)   2 ( 3.2)        \n#&gt;      Other 2nd grad.      0 ( 0.0)   0 ( 0.0)        \n#&gt;      Post-2nd grad.     259 (76.9)  49 (79.0)\n\nTreatment effect\nConvert data to design\nIn the following code chunk, we create a new variable matched in the analytic.miss dataset to indicate whether an observation was included in the matched dataset OACVD.match.11n. We then create a new survey design object w.design.m that includes only the matched observations for subsequent analyses.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match.11n$ID) # matched data\n#&gt; [1] 399\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n$ID])\n#&gt; [1] 399\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match.11n$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396774    399\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\nOutcome analysis\nThe subsequent code chunks involve fitting logistic regression models to estimate the treatment effect, both in a crude and adjusted manner, respectively. The models are fitted using the matched survey design object and the results are displayed in a readable format.\nCrude\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA,\n                   design = w.design.m,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.outcome)\n#&gt;  Variable Units OddsRatio        CI.95  p-value \n#&gt;        OA            3.14 [0.80;12.40]   0.1025\n\nAdjusted\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + \n                        age + sex + stress + income + race + edu,\n                   design = w.design.m,\n                   family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\npublish(fit.outcome)\n#&gt;  Variable            Units    OddsRatio                        CI.95   p-value \n#&gt;        OA                          2.04                 [0.34;12.16]   0.43593 \n#&gt;       age      20-29 years          Ref                                        \n#&gt;                30-39 years         0.54                  [0.08;3.51]   0.51962 \n#&gt;                40-49 years  30148597.85    [7758796.44;117149349.12]   &lt; 1e-04 \n#&gt;                50-59 years  63290825.96   [12589873.89;318170673.23]   &lt; 1e-04 \n#&gt;                60-64 years         0.31                  [0.01;9.33]   0.49735 \n#&gt;       sex           Female          Ref                                        \n#&gt;                       Male         1.58                  [0.29;8.62]   0.59729 \n#&gt;    stress Not too stressed          Ref                                        \n#&gt;                   stressed         0.15                  [0.01;1.80]   0.13666 \n#&gt;    income  $29,999 or less          Ref                                        \n#&gt;            $30,000-$49,999         0.20                  [0.01;3.84]   0.28640 \n#&gt;            $50,000-$79,999         0.20                  [0.02;1.95]   0.16543 \n#&gt;            $80,000 or more         0.08                  [0.01;0.68]   0.02122 \n#&gt;      race        Non-white          Ref                                        \n#&gt;                      White         1.02                  [0.11;9.45]   0.98723 \n#&gt;       edu         &lt; 2ndary          Ref                                        \n#&gt;                  2nd grad. 845233466.89 [44642865.50;16002996347.89]   &lt; 1e-04 \n#&gt;             Post-2nd grad.  69867459.42    [9660579.88;505296985.12]   &lt; 1e-04\n\nQuestions for the students\n\nLook at all the ORs. Some of them are VERY high. Why?\nLook at the CI in the above table. Some of them are Inf. Why?\nShould we match matching variables in the regression?\nMatching by a lot of variables\nThe code chunks involve performing matching using a large number of variables and estimating the treatment effect using the matched data. The process involves creating matched datasets, converting them to survey design objects, and fitting logistic regression models.\nMatching part in data\n\nmatch.formula &lt;- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu\")\nmatching.obj2 &lt;- matchit(match.formula,\n                        data = analytic11n,\n                        method = \"exact\")\nmatching.obj2\n#&gt; A `matchit` object\n#&gt;  - method: Exact matching\n#&gt;  - number of obs.: 1424 (original), 22 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, immigrate, fruit, diab, edu\nOACVD.match.11n2 &lt;- match.data(matching.obj2)\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \"income\", \"race\", \n               \"bmi\", \"phyact\", \"smoke\", \"immigrate\", \"fruit\", \"diab\", \"edu\")\ntab2 &lt;- CreateCatTable(var = var.names, strata= \"OA\", data=OACVD.match.11n2,test=FALSE)\nprint(tab2, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     11         11               \n#&gt;   age (%)                                     &lt;0.001\n#&gt;      20-29 years         3 (27.3)   3 (27.3)        \n#&gt;      30-39 years         1 ( 9.1)   1 ( 9.1)        \n#&gt;      40-49 years         4 (36.4)   4 (36.4)        \n#&gt;      50-59 years         3 (27.3)   3 (27.3)        \n#&gt;      60-64 years         0 ( 0.0)   0 ( 0.0)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)         6 (54.5)   6 (54.5)  &lt;0.001\n#&gt;   stress = stressed (%)  1 ( 9.1)   1 ( 9.1)  &lt;0.001\n#&gt;   married = single (%)   3 (27.3)   3 (27.3)  &lt;0.001\n#&gt;   income (%)                                  &lt;0.001\n#&gt;      $29,999 or less     1 ( 9.1)   1 ( 9.1)        \n#&gt;      $30,000-$49,999     2 (18.2)   2 (18.2)        \n#&gt;      $50,000-$79,999     2 (18.2)   2 (18.2)        \n#&gt;      $80,000 or more     6 (54.5)   6 (54.5)        \n#&gt;   race = White (%)      10 (90.9)  10 (90.9)  &lt;0.001\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      4 (36.4)   4 (36.4)        \n#&gt;      Overweight          7 (63.6)   7 (63.6)        \n#&gt;   phyact (%)                                  &lt;0.001\n#&gt;      Active              3 (27.3)   3 (27.3)        \n#&gt;      Inactive            5 (45.5)   5 (45.5)        \n#&gt;      Moderate            3 (27.3)   3 (27.3)        \n#&gt;   smoke (%)                                   &lt;0.001\n#&gt;      Never smoker        3 (27.3)   3 (27.3)        \n#&gt;      Current smoker      2 (18.2)   2 (18.2)        \n#&gt;      Former smoker       6 (54.5)   6 (54.5)        \n#&gt;   immigrate (%)                               &lt;0.001\n#&gt;      not immigrant      10 (90.9)  10 (90.9)        \n#&gt;      &gt; 10 years          1 ( 9.1)   1 ( 9.1)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                   &lt;0.001\n#&gt;      0-3 daily serving   3 (27.3)   3 (27.3)        \n#&gt;      4-6 daily serving   6 (54.5)   6 (54.5)        \n#&gt;      6+ daily serving    2 (18.2)   2 (18.2)        \n#&gt;   diab = Yes (%)         0 ( 0.0)   0 ( 0.0)  &lt;0.001\n#&gt;   edu (%)                                     &lt;0.001\n#&gt;      &lt; 2ndary            1 ( 9.1)   1 ( 9.1)        \n#&gt;      2nd grad.           0 ( 0.0)   0 ( 0.0)        \n#&gt;      Other 2nd grad.     0 ( 0.0)   0 ( 0.0)        \n#&gt;      Post-2nd grad.     10 (90.9)  10 (90.9)\n\nTreatment effect estimation in design\nCreate design\n\nanalytic.miss$matched2 &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match.11n2$ID) # matched data\n#&gt; [1] 22\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match.11n2$ID])\n#&gt; [1] 22\nanalytic.miss$matched2[analytic.miss$ID %in% OACVD.match.11n2$ID] &lt;- 1\ntable(analytic.miss$matched2)\n#&gt; \n#&gt;      0      1 \n#&gt; 397151     22\nw.design0 &lt;- svydesign(id=~1, weights=~weight, \n                      data=analytic.miss)\nw.design.m2 &lt;- subset(w.design0, matched2 == 1)\n\noutcome analysis\n\nfit.outcome &lt;- svyglm(I(CVD==\"event\") ~ OA + age + sex + stress + married +\n                         income + race + bmi + phyact + smoke +\n                         immigrate + fruit + diab + edu,\n                   design = w.design.m2,\n                   family = binomial(logit))\npublish(fit.outcome)\n# Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : \n#   contrasts can be applied only to factors with 2 or more levels\n\nQuestions for the students\n\nWhy the above model not fitting?\nSave data for later use\n\nsave(analytic11n, analytic2, analytic.miss, file=\"Data/propensityscore/cchs123c.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Propensity score",
      "Exact Matching (CCHS)"
    ]
  },
  {
    "objectID": "propensityscore2.html",
    "href": "propensityscore2.html",
    "title": "PSM in OA-CVD (CCHS)",
    "section": "",
    "text": "This tutorial is a comprehensive guide on implementing Propensity Score Matching (PSM) using R, particularly focusing on a OA - CVD health study from the Canadian Community Health Survey (CCHS). This PSM method is used to reduce bias due to confounding variables in observational studies by matching treated and control units with similar propensity scores. The tutorial illustrates that PSM is an iterative process, where researchers may need to refine their matching strategy to achieve satisfactory balance in the matched sample. Different strategies for estimating the treatment effect in the matched sample are explored, each with its own assumptions and implications.\nLoad packages\nAt first, various R packages are loaded to utilize their functions for data manipulation, statistical analysis, and visualization.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\n\nLoad data\nThe dataset is loaded into the R environment. Variables are renamed to avoid conflicts in subsequent analyses.\n\nload(file=\"Data/propensityscore/cchs123c.RData\")\nhead(analytic11n)\n\n\n  \n\n\n\n# later we will create another variable called weights\n# hence to avoid any conflict/ambiguity,\n# renaming weight variable to survey.weight\nanalytic.miss$survey.weight &lt;- analytic.miss$weight\nanalytic11n$survey.weight &lt;- analytic11n$weight\nanalytic.miss$weight &lt;- analytic11n$weight &lt;- NULL\n\nAnalysis\nWe are going to apply propensity score analysis (Matching) in our OA - CVD problem from CCHS. For computation considerations, we will only work with cycle 1.1, and the people from Northern provinces in Canada (analytic11n data).\nStep 1\nSpecify PS\nA logistic regression model formula is specified to calculate the propensity scores (PS), which is the probability of receiving the treatment given the observed covariates.\n\nps.formula &lt;- as.formula(\"OA ~ age + sex + stress + married + \n                         income + race + bmi + phyact + smoke +\n                        doctor + drink + bp + \n                         immigrate + fruit + diab + edu\")\nvar.names &lt;- c(\"age\", \"sex\", \"stress\", \"married\", \n               \"income\", \"race\", \"bmi\", \"phyact\", \"smoke\", \n               \"doctor\", \"drink\", \"bp\", \n               \"immigrate\", \"fruit\", \"diab\", \"edu\")\n\nFit model\nThe software fits the PS model using a logistic regression by default. This package actually performs step 1 and 2 with one command matchit.\nLook at the website for arguments of matchit (RDocumentation 2023)]. It looks like this\n\nmatchit(formula, data, model=\"logit\", discard=0, reestimate=FALSE, nearest=TRUE,\n                 replace=FALSE, m.order=2, ratio=1, caliper=0, calclosest=FALSE,\n                 subclass=0, sub.by=\"treat\", mahvars=NULL, exact=FALSE, counter=TRUE, full=FALSE, full.options=list(),...)\n\n\n\n\n\n\n\nTip\n\n\n\nNearest-Neighbor Matching:\nNearest-neighbor matching is a widely used technique in PSM to pair treated and control units based on the proximity of their propensity scores. It is straightforward and computationally efficient, making it a popular choice in many applications of PSM. Nearest-neighbor matching is often termed a “greedy” algorithm because it matches units in order, without considering the global distribution of propensity scores. Once a match is made, it is not revisited, even if a later unit would have been a better match. The method seeks to minimize bias by creating closely matched pairs but can increase variance if the pool of potential matches is reduced too much (e.g., using a very narrow caliper). It is essential to ensure that there is a common support region where the distributions of propensity scores for treated and control units overlap, ensuring comparability.\n\n\nStep 2\nMatch subjects by PS\nWe are going to match using a Nearest neighbor algorithm. This is a greedy matching algorithm. Note that we are not even defining any caliper.\n\n\n\n\n\n\nTip\n\n\n\nCaliper:\nIn the context of PSM, a caliper is a predefined maximum allowable difference between the propensity scores of matched units. Essentially, it sets a threshold for how dissimilar matched units can be in terms of their propensity scores. When a caliper is used, a treated unit is only matched with a control unit if the absolute difference in their propensity scores is less than or equal to the specified caliper width. The caliper is used to avoid bad matches and thereby minimize bias in the estimated treatment effect. The size of the caliper is crucial. Too wide a caliper may allow poor matches, while too narrow a caliper may result in many units going unmatched. Implementing a caliper involves a trade-off between bias and efficiency. Using a caliper reduces bias by avoiding poor matches but may increase variance by reducing the number of matched pairs available for analysis. Therefore, the use of a caliper in PSM is a strategic decision to enhance the quality of matches and thereby improve the validity of causal inferences drawn from observational data. It is a practical tool to ensure that matched units are sufficiently similar in terms of their propensity scores, reducing the likelihood of bias due to poor matches.\n\n\n\n# set seed\nset.seed(123)\n# match\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score\n#&gt;              - estimated with logistic regression\n#&gt;  - number of obs.: 1424 (original), 134 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\n# create the \"matched\"\" data\nOACVD.match &lt;- match.data(matching.obj)\n# see the dimension\ndim(analytic11n)\n#&gt; [1] 1424   23\ndim(OACVD.match)\n#&gt; [1] 134  26\n\nLet’s try to understand how this is working.\nExtract matched IDs\n\nm.mat&lt;-matching.obj$match.matrix\nhead(m.mat)\n#&gt;       [,1]    \n#&gt; 17864 \"96719\" \n#&gt; 17921 \"17846\" \n#&gt; 18191 \"97168\" \n#&gt; 18256 \"111999\"\n#&gt; 18264 \"17989\" \n#&gt; 18383 \"108197\"\n\nExtract the matched treated IDs\n\ntreated.id&lt;-as.numeric(row.names(m.mat))\ntreated.id # basically row names\n#&gt;  [1]  17864  17921  18191  18256  18264  18383  18389  18475  39105  96344\n#&gt; [11]  96364  96407  96424  96460  96484  96571  96582  96625  96632  96641\n#&gt; [21]  96657  96686  96693  96696  96705  96734  96795  96840  96913  97027\n#&gt; [31]  97065  97125 108178 108183 108185 108192 111809 111813 111856 111859\n#&gt; [41] 111895 111896 111920 111942 112014 112026 112046 112083 112086 112114\n#&gt; [51] 112122 112151 112167 112189 112197 112215 112232 112245 112275 112284\n#&gt; [61] 112289 112290 112300 112325 112375 126477 126522\n\nExtract the matched untreated IDs\n\nuntreated.id &lt;- as.numeric(m.mat)\nuntreated.id # basically row names\n#&gt;  [1]  96719  17846  97168 111999  17989 108197 112384  17909 126561 111880\n#&gt; [11] 112184  18117  96865  18120  97023 112379  97017 126562  96356 126470\n#&gt; [21] 126385  96374  18203  18262  96972 111924  96354  96983  18235  96882\n#&gt; [31] 112054  18321 112349  18426  38996 126516 111814 112087  96569 111932\n#&gt; [41] 126539  18315  96665  18225 112052 112324 112165  18329  96609 126376\n#&gt; [51]  96474 126570 126547 126343  96680  96558 111931  96718  96533 111823\n#&gt; [61] 112177  17953  17904 111908 111962  96644  96576\n\nExtract the matched treated data\n\ntx &lt;- analytic11n[rownames(analytic11n) %in% treated.id,]\nhead(tx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n  \n\n\n\nExtract the matched untreated data\n\nutx &lt;- analytic11n[rownames(analytic11n) %in% untreated.id,]\nhead(utx[c(\"OA\", \"CVD\", \"sex\", \"age\", \"race\", \"edu\")])\n\n\n  \n\n\n\nExtract the matched data altogether\nSimply using match.data is enough (as done earlier).\n\nOACVD.match &lt;- match.data(matching.obj)\n\nAssign match ID\n\nOACVD.match$match.ID &lt;- NA\nOACVD.match$match.ID[rownames(OACVD.match) %in% treated.id] &lt;- 1:length(treated.id)\nOACVD.match$match.ID[rownames(OACVD.match) %in% untreated.id] &lt;- 1:length(untreated.id)\ntable(OACVD.match$match.ID)\n#&gt; \n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n#&gt; 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n#&gt;  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n\nTake a look at individual matches for the first match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 1,])\n\n\n  \n\n\n\nTake a look at individual matches for the second match\n\nna.omit(OACVD.match[OACVD.match$match.ID == 2,])\n\n\n  \n\n\n\nStep 3\nBoth graphical and numerical methods are used to assess the quality of the matches and the balance of covariates in the matched sample.\nExamining PS graphically\nVisually inspect the PS and assess the balance of covariates in the matched sample. Various plots are generated to visualize the distribution of PS across treatment groups and to check the balance of covariates before and after matching.\nmatchit package\n\n# plot(matching.obj) # covariate balance\nplot(matching.obj, type = \"jitter\") # propensity score locations\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(matching.obj, type = \"hist\") #check matched treated vs matched control\n\n\n\n\n\n\nsummrize.output &lt;- summary(matching.obj, standardize = TRUE)\nplot(summrize.output)\n\n\n\n\n\n\n\nOveralp check\n\n# plot propensity scores by exposure group\nplot(density(OACVD.match$distance[OACVD.match$OA==1]), \n     col = \"red\", main = \"\")\nlines(density(OACVD.match$distance[OACVD.match$OA==0]), \n      col = \"blue\", lty = 2)\nlegend(\"topright\", c(\"Non-arthritis\",\"OA\"), \n       col = c(\"red\", \"blue\"), lty=1:2)\n\n\n\n\n\n\n\ncobalt package\nOverlap check in a more convenient way\n\n# different badwidth\nbal.plot(matching.obj, var.name = \"distance\")\n#&gt; Ignoring unknown labels:\n#&gt; • colour : \"Treatment\"\n\n\n\n\n\n\n\nLook at the data\n\n# what is distance variable here?\nhead(OACVD.match)\n\n\n  \n\n\n\nNumerical values of PS\n\nsummary(OACVD.match$distance)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.044834 0.099094 0.138969 0.200576 0.611166\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.047344 0.099215 0.135042 0.199279 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.047346 0.098973 0.142897 0.198741 0.611166\n\nQuestion for the students\n\nAre you happy with the matching after reviewing the plots?\nCovariate balance in matched sample\nCovariate balance is assessed numerically using standardized mean differences (SMD).\n\n\n\n\n\n\nTip\n\n\n\nStandardized mean differences: SMD is a versatile and widely used statistical measure that facilitates the comparison of groups in research by providing a scale-free metric of difference and balance. In the context of propensity score matching, achieving low SMD values for covariates after matching is crucial to ensuring the validity of causal inferences drawn from the matched sample.\nBenifits:\n\nSMD is not influenced by the scale of the measured variable, making it suitable for comparing the balance of different variables measured on different scales.\nUnlike hypothesis testing, SMD is not affected by sample size, making it a reliable measure for assessing balance in matched samples.\n\n\n\n\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     67         67               \n#&gt;   age (%)                                      0.190\n#&gt;      20-29 years         4 ( 6.0)   4 ( 6.0)        \n#&gt;      30-39 years        16 (23.9)  11 (16.4)        \n#&gt;      40-49 years        16 (23.9)  18 (26.9)        \n#&gt;      50-59 years        28 (41.8)  31 (46.3)        \n#&gt;      60-64 years         3 ( 4.5)   3 ( 4.5)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        28 (41.8)  32 (47.8)   0.120\n#&gt;   stress = stressed (%) 20 (29.9)  21 (31.3)   0.032\n#&gt;   married = single (%)  22 (32.8)  23 (34.3)   0.032\n#&gt;   income (%)                                   0.183\n#&gt;      $29,999 or less    11 (16.4)  13 (19.4)        \n#&gt;      $30,000-$49,999    19 (28.4)  15 (22.4)        \n#&gt;      $50,000-$79,999    14 (20.9)  12 (17.9)        \n#&gt;      $80,000 or more    23 (34.3)  27 (40.3)        \n#&gt;   race = White (%)      43 (64.2)  44 (65.7)   0.031\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight     18 (26.9)  18 (26.9)        \n#&gt;      Overweight         49 (73.1)  49 (73.1)        \n#&gt;   phyact (%)                                   0.041\n#&gt;      Active             16 (23.9)  16 (23.9)        \n#&gt;      Inactive           40 (59.7)  39 (58.2)        \n#&gt;      Moderate           11 (16.4)  12 (17.9)        \n#&gt;   smoke (%)                                    0.258\n#&gt;      Never smoker       14 (20.9)   9 (13.4)        \n#&gt;      Current smoker     20 (29.9)  27 (40.3)        \n#&gt;      Former smoker      33 (49.3)  31 (46.3)        \n#&gt;   doctor = Yes (%)      51 (76.1)  47 (70.1)   0.135\n#&gt;   drink (%)                                    0.116\n#&gt;      Never drank         2 ( 3.0)   2 ( 3.0)        \n#&gt;      Current drinker    54 (80.6)  51 (76.1)        \n#&gt;      Former driker      11 (16.4)  14 (20.9)        \n#&gt;   bp = Yes (%)           7 (10.4)   5 ( 7.5)   0.105\n#&gt;   immigrate (%)                                0.180\n#&gt;      not immigrant      64 (95.5)  61 (91.0)        \n#&gt;      &gt; 10 years          3 ( 4.5)   6 ( 9.0)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                    0.146\n#&gt;      0-3 daily serving  19 (28.4)  19 (28.4)        \n#&gt;      4-6 daily serving  28 (41.8)  32 (47.8)        \n#&gt;      6+ daily serving   20 (29.9)  16 (23.9)        \n#&gt;   diab = Yes (%)         1 ( 1.5)   4 ( 6.0)   0.238\n#&gt;   edu (%)                                      0.105\n#&gt;      &lt; 2ndary           14 (20.9)  13 (19.4)        \n#&gt;      2nd grad.           1 ( 1.5)   2 ( 3.0)        \n#&gt;      Other 2nd grad.     1 ( 1.5)   1 ( 1.5)        \n#&gt;      Post-2nd grad.     51 (76.1)  51 (76.1)\n\nQuestion for the students\n\nAll SMD &lt; 0.20?\nOther balance measures\nIndividual categories\nIf you want to check balance at each category (not very useful in general situations). We are generally interested if the variables are balanced or not (not categories).\n\nbaltab &lt;- bal.tab(matching.obj)\nbaltab\n#&gt; Balance Measures\n#&gt;                             Type Diff.Adj\n#&gt; distance                Distance   0.0597\n#&gt; age_20-29 years           Binary   0.0000\n#&gt; age_30-39 years           Binary  -0.0746\n#&gt; age_40-49 years           Binary   0.0299\n#&gt; age_50-59 years           Binary   0.0448\n#&gt; age_60-64 years           Binary   0.0000\n#&gt; sex_Male                  Binary   0.0597\n#&gt; stress_stressed           Binary   0.0149\n#&gt; married_single            Binary   0.0149\n#&gt; income_$29,999 or less    Binary   0.0299\n#&gt; income_$30,000-$49,999    Binary  -0.0597\n#&gt; income_$50,000-$79,999    Binary  -0.0299\n#&gt; income_$80,000 or more    Binary   0.0597\n#&gt; race_White                Binary   0.0149\n#&gt; bmi_Underweight           Binary   0.0000\n#&gt; bmi_healthy weight        Binary   0.0000\n#&gt; bmi_Overweight            Binary   0.0000\n#&gt; phyact_Active             Binary   0.0000\n#&gt; phyact_Inactive           Binary  -0.0149\n#&gt; phyact_Moderate           Binary   0.0149\n#&gt; smoke_Never smoker        Binary  -0.0746\n#&gt; smoke_Current smoker      Binary   0.1045\n#&gt; smoke_Former smoker       Binary  -0.0299\n#&gt; doctor_Yes                Binary  -0.0597\n#&gt; drink_Never drank         Binary   0.0000\n#&gt; drink_Current drinker     Binary  -0.0448\n#&gt; drink_Former driker       Binary   0.0448\n#&gt; bp_Yes                    Binary  -0.0299\n#&gt; immigrate_not immigrant   Binary  -0.0448\n#&gt; immigrate_&gt; 10 years      Binary   0.0448\n#&gt; immigrate_recent          Binary   0.0000\n#&gt; fruit_0-3 daily serving   Binary   0.0000\n#&gt; fruit_4-6 daily serving   Binary   0.0597\n#&gt; fruit_6+ daily serving    Binary  -0.0597\n#&gt; diab_Yes                  Binary   0.0448\n#&gt; edu_&lt; 2ndary              Binary  -0.0149\n#&gt; edu_2nd grad.             Binary   0.0149\n#&gt; edu_Other 2nd grad.       Binary   0.0000\n#&gt; edu_Post-2nd grad.        Binary   0.0000\n#&gt; \n#&gt; Sample sizes\n#&gt;           Control Treated\n#&gt; All          1357      67\n#&gt; Matched        67      67\n#&gt; Unmatched    1290       0\n\nIndividual plots\nYou could plot each variables individually\n\nbal.plot(matching.obj, var.name = \"income\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"age\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"race\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"diab\")\n\n\n\n\n\n\nbal.plot(matching.obj, var.name = \"immigrate\")\n\n\n\n\n\n\n\nLove plot\n\n# Individual categories again\nlove.plot(baltab, threshold = .2)\n#&gt; Warning: Unadjusted values are missing. This can occur when `un = FALSE` and\n#&gt; `quick = TRUE` in the original call to `bal.tab()`.\n#&gt; Warning: Standardized mean differences and raw mean differences are present in\n#&gt; the same plot. Use the `stars` argument to distinguish between them and\n#&gt; appropriately label the x-axis. See `?love.plot` for details.\n\n\n\n\n\n\n\nRepeat of Step 1-3 again\nCovariate balance is reassessed in each step to ensure the quality of the match.\nAdd caliper\nThe matching process is repeated, this time introducing a caliper to ensure that matches are only made within a specified range of PS.\n\nlogitPS &lt;-  -log(1/OACVD.match$distance - 1) \n# logit of the propensity score\n.2*sd(logitPS) # suggested in the literature\n#&gt; [1] 0.2334615\n\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 1,\n                        caliper = .2*sd(logitPS))\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.015)\n#&gt;  - number of obs.: 1424 (original), 128 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n\n\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041740 0.094963 0.124665 0.184103 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0          1          SMD   \n#&gt;   n                     64         64               \n#&gt;   age (%)                                      0.196\n#&gt;      20-29 years         4 ( 6.2)   4 ( 6.2)        \n#&gt;      30-39 years        16 (25.0)  11 (17.2)        \n#&gt;      40-49 years        16 (25.0)  18 (28.1)        \n#&gt;      50-59 years        25 (39.1)  28 (43.8)        \n#&gt;      60-64 years         3 ( 4.7)   3 ( 4.7)        \n#&gt;      65 years and over   0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        27 (42.2)  30 (46.9)   0.094\n#&gt;   stress = stressed (%) 18 (28.1)  18 (28.1)  &lt;0.001\n#&gt;   married = single (%)  21 (32.8)  23 (35.9)   0.066\n#&gt;   income (%)                                   0.204\n#&gt;      $29,999 or less    11 (17.2)  13 (20.3)        \n#&gt;      $30,000-$49,999    19 (29.7)  14 (21.9)        \n#&gt;      $50,000-$79,999    13 (20.3)  12 (18.8)        \n#&gt;      $80,000 or more    21 (32.8)  25 (39.1)        \n#&gt;   race = White (%)      40 (62.5)  42 (65.6)   0.065\n#&gt;   bmi (%)                                     &lt;0.001\n#&gt;      Underweight         0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight     18 (28.1)  18 (28.1)        \n#&gt;      Overweight         46 (71.9)  46 (71.9)        \n#&gt;   phyact (%)                                   0.096\n#&gt;      Active             14 (21.9)  16 (25.0)        \n#&gt;      Inactive           39 (60.9)  36 (56.2)        \n#&gt;      Moderate           11 (17.2)  12 (18.8)        \n#&gt;   smoke (%)                                    0.267\n#&gt;      Never smoker       14 (21.9)   9 (14.1)        \n#&gt;      Current smoker     19 (29.7)  26 (40.6)        \n#&gt;      Former smoker      31 (48.4)  29 (45.3)        \n#&gt;   doctor = Yes (%)      48 (75.0)  44 (68.8)   0.139\n#&gt;   drink (%)                                    0.123\n#&gt;      Never drank         2 ( 3.1)   2 ( 3.1)        \n#&gt;      Current drinker    52 (81.2)  49 (76.6)        \n#&gt;      Former driker      10 (15.6)  13 (20.3)        \n#&gt;   bp = Yes (%)           7 (10.9)   5 ( 7.8)   0.107\n#&gt;   immigrate (%)                                0.260\n#&gt;      not immigrant      62 (96.9)  58 (90.6)        \n#&gt;      &gt; 10 years          2 ( 3.1)   6 ( 9.4)        \n#&gt;      recent              0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                    0.116\n#&gt;      0-3 daily serving  19 (29.7)  19 (29.7)        \n#&gt;      4-6 daily serving  27 (42.2)  30 (46.9)        \n#&gt;      6+ daily serving   18 (28.1)  15 (23.4)        \n#&gt;   diab = Yes (%)         0 ( 0.0)   3 ( 4.7)   0.314\n#&gt;   edu (%)                                      0.108\n#&gt;      &lt; 2ndary           14 (21.9)  13 (20.3)        \n#&gt;      2nd grad.           1 ( 1.6)   2 ( 3.1)        \n#&gt;      Other 2nd grad.     1 ( 1.6)   1 ( 1.6)        \n#&gt;      Post-2nd grad.     48 (75.0)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable for pair matching?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       1       1       1       1       1       1\n\nStep 4\nEstimate treatment effect for matched data\nDifferent models (e.g., unconditional logistic regression, survey design) are fitted to estimate the treatment effect in the matched sample.\nUnconditional logistic\n\n# Wrong model for population!!\noutcome.model &lt;- glm(CVD ~ OA, data = OACVD.match, family = binomial())\npublish(outcome.model)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.48 [0.09;2.74]   0.4119\n\nSurvey design\nConvert data to design\nThe matched data is converted to a survey design object to account for the matched pairs in the analysis.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 128\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 128\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 397045    128\nw.design0 &lt;- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\nBalance in matched population?\n\ntab1 &lt;- svyCreateTableOne(strata = \"OA\", data = w.design.m, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0              1              SMD   \n#&gt;   n                     1783.6         2002.1               \n#&gt;   age (%)                                              0.307\n#&gt;      20-29 years         131.0 ( 7.3)   120.9 ( 6.0)        \n#&gt;      30-39 years         388.0 (21.8)   237.8 (11.9)        \n#&gt;      40-49 years         518.3 (29.1)   572.7 (28.6)        \n#&gt;      50-59 years         680.1 (38.1)   999.3 (49.9)        \n#&gt;      60-64 years          66.1 ( 3.7)    71.4 ( 3.6)        \n#&gt;      65 years and over     0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      teen                  0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   sex = Male (%)         852.4 (47.8)   985.1 (49.2)   0.028\n#&gt;   stress = stressed (%)  544.6 (30.5)   531.8 (26.6)   0.088\n#&gt;   married = single (%)   419.8 (23.5)   427.5 (21.4)   0.052\n#&gt;   income (%)                                           0.222\n#&gt;      $29,999 or less     266.6 (14.9)   352.4 (17.6)        \n#&gt;      $30,000-$49,999     462.8 (25.9)   348.8 (17.4)        \n#&gt;      $50,000-$79,999     298.6 (16.7)   315.7 (15.8)        \n#&gt;      $80,000 or more     755.5 (42.4)   985.2 (49.2)        \n#&gt;   race = White (%)      1129.2 (63.3)  1364.9 (68.2)   0.103\n#&gt;   bmi (%)                                              0.045\n#&gt;      Underweight           0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;      healthy weight      483.3 (27.1)   583.3 (29.1)        \n#&gt;      Overweight         1300.2 (72.9)  1418.8 (70.9)        \n#&gt;   phyact (%)                                           0.054\n#&gt;      Active              448.0 (25.1)   493.9 (24.7)        \n#&gt;      Inactive           1075.2 (60.3)  1176.6 (58.8)        \n#&gt;      Moderate            260.3 (14.6)   331.5 (16.6)        \n#&gt;   smoke (%)                                            0.288\n#&gt;      Never smoker        400.2 (22.4)   265.8 (13.3)        \n#&gt;      Current smoker      548.8 (30.8)   836.6 (41.8)        \n#&gt;      Former smoker       834.6 (46.8)   899.6 (44.9)        \n#&gt;   doctor = Yes (%)      1376.0 (77.1)  1430.1 (71.4)   0.131\n#&gt;   drink (%)                                            0.194\n#&gt;      Never drank          44.0 ( 2.5)   112.6 ( 5.6)        \n#&gt;      Current drinker    1464.1 (82.1)  1510.6 (75.5)        \n#&gt;      Former driker       275.4 (15.4)   378.9 (18.9)        \n#&gt;   bp = Yes (%)           166.6 ( 9.3)   153.5 ( 7.7)   0.060\n#&gt;   immigrate (%)                                        0.235\n#&gt;      not immigrant      1694.8 (95.0)  1774.1 (88.6)        \n#&gt;      &gt; 10 years           88.8 ( 5.0)   228.0 (11.4)        \n#&gt;      recent                0.0 ( 0.0)     0.0 ( 0.0)        \n#&gt;   fruit (%)                                            0.293\n#&gt;      0-3 daily serving   426.1 (23.9)   480.8 (24.0)        \n#&gt;      4-6 daily serving   748.1 (41.9)  1082.3 (54.1)        \n#&gt;      6+ daily serving    609.4 (34.2)   439.0 (21.9)        \n#&gt;   diab = Yes (%)           0.0 ( 0.0)    83.6 ( 4.2)   0.295\n#&gt;   edu (%)                                              0.172\n#&gt;      &lt; 2ndary            324.9 (18.2)   342.8 (17.1)        \n#&gt;      2nd grad.            18.8 ( 1.1)    47.6 ( 2.4)        \n#&gt;      Other 2nd grad.      15.2 ( 0.9)    52.2 ( 2.6)        \n#&gt;      Post-2nd grad.     1424.7 (79.9)  1559.4 (77.9)\n\nOutcome analysis\n\nfit.design &lt;- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.50 [0.08;3.09]   0.4535\n\nMatched data with increase ratio\nThe matching process is repeated with a different ratio (e.g., 1:5) to explore how changing the ratio affects the covariate balance and treatment effect estimation.\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 5:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.013)\n#&gt;  - number of obs.: 1424 (original), 349 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004643 0.039181 0.084421 0.101576 0.146403 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.041694 0.095089 0.125262 0.183739 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     285         64               \n#&gt;   age (%)                                       0.217\n#&gt;      20-29 years         24 ( 8.4)   4 ( 6.2)        \n#&gt;      30-39 years         59 (20.7)  11 (17.2)        \n#&gt;      40-49 years         94 (33.0)  18 (28.1)        \n#&gt;      50-59 years         98 (34.4)  28 (43.8)        \n#&gt;      60-64 years         10 ( 3.5)   3 ( 4.7)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        132 (46.3)  30 (46.9)   0.011\n#&gt;   stress = stressed (%)  81 (28.4)  18 (28.1)   0.007\n#&gt;   married = single (%)   91 (31.9)  23 (35.9)   0.085\n#&gt;   income (%)                                    0.065\n#&gt;      $29,999 or less     64 (22.5)  13 (20.3)        \n#&gt;      $30,000-$49,999     65 (22.8)  14 (21.9)        \n#&gt;      $50,000-$79,999     51 (17.9)  12 (18.8)        \n#&gt;      $80,000 or more    105 (36.8)  25 (39.1)        \n#&gt;   race = White (%)      169 (59.3)  42 (65.6)   0.131\n#&gt;   bmi (%)                                       0.097\n#&gt;      Underweight          0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      68 (23.9)  18 (28.1)        \n#&gt;      Overweight         217 (76.1)  46 (71.9)        \n#&gt;   phyact (%)                                    0.136\n#&gt;      Active              57 (20.0)  16 (25.0)        \n#&gt;      Inactive           178 (62.5)  36 (56.2)        \n#&gt;      Moderate            50 (17.5)  12 (18.8)        \n#&gt;   smoke (%)                                     0.097\n#&gt;      Never smoker        46 (16.1)   9 (14.1)        \n#&gt;      Current smoker     123 (43.2)  26 (40.6)        \n#&gt;      Former smoker      116 (40.7)  29 (45.3)        \n#&gt;   doctor = Yes (%)      183 (64.2)  44 (68.8)   0.096\n#&gt;   drink (%)                                     0.051\n#&gt;      Never drank         10 ( 3.5)   2 ( 3.1)        \n#&gt;      Current drinker    212 (74.4)  49 (76.6)        \n#&gt;      Former driker       63 (22.1)  13 (20.3)        \n#&gt;   bp = Yes (%)           22 ( 7.7)   5 ( 7.8)   0.003\n#&gt;   immigrate (%)                                 0.100\n#&gt;      not immigrant      266 (93.3)  58 (90.6)        \n#&gt;      &gt; 10 years          19 ( 6.7)   6 ( 9.4)        \n#&gt;      recent               0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                     0.149\n#&gt;      0-3 daily serving  104 (36.5)  19 (29.7)        \n#&gt;      4-6 daily serving  124 (43.5)  30 (46.9)        \n#&gt;      6+ daily serving    57 (20.0)  15 (23.4)        \n#&gt;   diab = Yes (%)         12 ( 4.2)   3 ( 4.7)   0.023\n#&gt;   edu (%)                                       0.137\n#&gt;      &lt; 2ndary            67 (23.5)  13 (20.3)        \n#&gt;      2nd grad.            9 ( 3.2)   2 ( 3.1)        \n#&gt;      Other 2nd grad.      9 ( 3.2)   1 ( 1.6)        \n#&gt;      Post-2nd grad.     200 (70.2)  48 (75.0)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.8906  0.8906  0.8906  1.0000  0.8906  4.4531\n\nCombining matching weights\nDifferent approaches to incorporating weights (e.g., matching weights, survey weights) are explored.\nNot incorporating matching weights\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396824    349\nw.design0 &lt;- svydesign(id=~1, weights=~survey.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(CVD ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95 p-value \n#&gt;        OA            0.80 [0.23;2.80]   0.722\n\nIncorporating matching weights\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 349\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 349\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396824    349\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight &lt;- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] &lt;-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 &lt;- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            1.14 [0.32;4.07]   0.8419\n\nMatched with replacement\nMatching is performed with replacement, allowing control units to be used in more than one match.\n\n# Step 1 and 2\nmatching.obj &lt;- matchit(ps.formula,\n                        data = analytic11n,\n                        method = \"nearest\",\n                        ratio = 5,\n                        caliper = 0.2,\n                        replace = TRUE)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n# see how many matched\nmatching.obj\n#&gt; A `matchit` object\n#&gt;  - method: 5:1 nearest neighbor matching with replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.013)\n#&gt;  - number of obs.: 1424 (original), 308 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age, sex, stress, married, income, race, bmi, phyact, smoke, doctor, drink, bp, immigrate, fruit, diab, edu\nOACVD.match &lt;- match.data(matching.obj)\n# Step 3\nby(OACVD.match$distance, OACVD.match$OA, summary)\n#&gt; OACVD.match$OA: 0\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004643 0.034244 0.067224 0.100844 0.148958 0.418206 \n#&gt; ------------------------------------------------------------ \n#&gt; OACVD.match$OA: 1\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.004671 0.042322 0.097877 0.129743 0.189256 0.424895\ntab1 &lt;- CreateTableOne(strata = \"OA\", data = OACVD.match, \n                       test = FALSE, vars = var.names)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by OA\n#&gt;                         0           1          SMD   \n#&gt;   n                     243         65               \n#&gt;   age (%)                                       0.266\n#&gt;      20-29 years         22 ( 9.1)   4 ( 6.2)        \n#&gt;      30-39 years         57 (23.5)  11 (16.9)        \n#&gt;      40-49 years         74 (30.5)  18 (27.7)        \n#&gt;      50-59 years         82 (33.7)  29 (44.6)        \n#&gt;      60-64 years          8 ( 3.3)   3 ( 4.6)        \n#&gt;      65 years and over    0 ( 0.0)   0 ( 0.0)        \n#&gt;      teen                 0 ( 0.0)   0 ( 0.0)        \n#&gt;   sex = Male (%)        113 (46.5)  31 (47.7)   0.024\n#&gt;   stress = stressed (%)  67 (27.6)  19 (29.2)   0.037\n#&gt;   married = single (%)   75 (30.9)  23 (35.4)   0.096\n#&gt;   income (%)                                    0.079\n#&gt;      $29,999 or less     53 (21.8)  13 (20.0)        \n#&gt;      $30,000-$49,999     57 (23.5)  14 (21.5)        \n#&gt;      $50,000-$79,999     44 (18.1)  12 (18.5)        \n#&gt;      $80,000 or more     89 (36.6)  26 (40.0)        \n#&gt;   race = White (%)      146 (60.1)  42 (64.6)   0.094\n#&gt;   bmi (%)                                       0.088\n#&gt;      Underweight          0 ( 0.0)   0 ( 0.0)        \n#&gt;      healthy weight      58 (23.9)  18 (27.7)        \n#&gt;      Overweight         185 (76.1)  47 (72.3)        \n#&gt;   phyact (%)                                    0.160\n#&gt;      Active              45 (18.5)  16 (24.6)        \n#&gt;      Inactive           155 (63.8)  37 (56.9)        \n#&gt;      Moderate            43 (17.7)  12 (18.5)        \n#&gt;   smoke (%)                                     0.095\n#&gt;      Never smoker        40 (16.5)   9 (13.8)        \n#&gt;      Current smoker     101 (41.6)  26 (40.0)        \n#&gt;      Former smoker      102 (42.0)  30 (46.2)        \n#&gt;   doctor = Yes (%)      152 (62.6)  45 (69.2)   0.141\n#&gt;   drink (%)                                     0.059\n#&gt;      Never drank          9 ( 3.7)   2 ( 3.1)        \n#&gt;      Current drinker    181 (74.5)  50 (76.9)        \n#&gt;      Former driker       53 (21.8)  13 (20.0)        \n#&gt;   bp = Yes (%)           19 ( 7.8)   5 ( 7.7)   0.005\n#&gt;   immigrate (%)                                 0.115\n#&gt;      not immigrant      228 (93.8)  59 (90.8)        \n#&gt;      &gt; 10 years          15 ( 6.2)   6 ( 9.2)        \n#&gt;      recent               0 ( 0.0)   0 ( 0.0)        \n#&gt;   fruit (%)                                     0.147\n#&gt;      0-3 daily serving   87 (35.8)  19 (29.2)        \n#&gt;      4-6 daily serving  109 (44.9)  31 (47.7)        \n#&gt;      6+ daily serving    47 (19.3)  15 (23.1)        \n#&gt;   diab = Yes (%)         11 ( 4.5)   3 ( 4.6)   0.004\n#&gt;   edu (%)                                       0.175\n#&gt;      &lt; 2ndary            58 (23.9)  13 (20.0)        \n#&gt;      2nd grad.            8 ( 3.3)   2 ( 3.1)        \n#&gt;      Other 2nd grad.      9 ( 3.7)   1 ( 1.5)        \n#&gt;      Post-2nd grad.     168 (69.1)  49 (75.4)\n\nQuestion for the students\n\nDid all of the SMDs decrease?\nLook at the data\n\n# what is weights variable now for 1:5 ratio?\nhead(OACVD.match)\n\n\n  \n\n\nsummary(OACVD.match$weights)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.7477  0.7477  0.7477  1.0000  1.0000  7.4769\n\nSurvey design\nThe matched data is converted into a survey design object, and the treatment effect is estimated while accounting for the complex survey design.\n\nanalytic.miss$matched &lt;- 0\nlength(analytic.miss$ID) # full data\n#&gt; [1] 397173\nlength(OACVD.match$ID) # matched data\n#&gt; [1] 308\nlength(analytic.miss$ID[analytic.miss$ID %in% OACVD.match$ID])\n#&gt; [1] 308\nanalytic.miss$matched[analytic.miss$ID %in% OACVD.match$ID] &lt;- 1\ntable(analytic.miss$matched)\n#&gt; \n#&gt;      0      1 \n#&gt; 396865    308\n\n\n# multiply with matching (ratio) weights with survey weights\nanalytic.miss$combined.weight &lt;- 0\nanalytic.miss$combined.weight[analytic.miss$ID %in% OACVD.match$ID] &lt;-\n  OACVD.match$weights*OACVD.match$survey.weight\nw.design0 &lt;- svydesign(id=~1, weights=~combined.weight, \n                      data=analytic.miss)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nfit.design &lt;- svyglm(I(CVD==\"event\") ~ OA, design = w.design.m, \n       family = binomial(logit))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.design)\n#&gt;  Variable Units OddsRatio       CI.95  p-value \n#&gt;        OA            0.99 [0.26;3.72]   0.9909\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nRDocumentation. 2023. “Matchit: Matchit: Matching Software for Causal Inference.” https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit.",
    "crumbs": [
      "Propensity score",
      "PSM in OA-CVD (CCHS)"
    ]
  },
  {
    "objectID": "propensityscore3.html",
    "href": "propensityscore3.html",
    "title": "PSM in OA-CVD (US)",
    "section": "",
    "text": "Pre-processing\nLoad data\nLoad the dataset and inspect its structure and variables.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") \nls()\n#&gt; [1] \"analytic\"           \"analytic.with.miss\"\n\nVisualize missing data patterns.\n\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:data.table':\n#&gt; \n#&gt;     between, first, last\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nanalytic.with.miss &lt;- dplyr::select(analytic.with.miss, \n                  cholesterol, #outcome\n                  gender, age, born, race, education, \n                  married, income, bmi, diabetes, #predictors\n                  weight, psu, strata) #survey features\n\ndim(analytic.with.miss)\n#&gt; [1] 9254   13\nstr(analytic.with.miss)\n#&gt; 'data.frame':    9254 obs. of  13 variables:\n#&gt;  $ cholesterol: 'labelled' int  NA NA 157 148 189 209 176 NA 238 182 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Total Cholesterol (mg/dL)\"\n#&gt;  $ gender     : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n#&gt;  $ age        : 'labelled' int  NA NA 66 NA NA 66 75 NA 56 NA ...\n#&gt;   ..- attr(*, \"label\")= chr \"Age in years at screening\"\n#&gt;  $ born       : chr  \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" \"Born in 50 US states or Washingt\" ...\n#&gt;  $ race       : chr  \"Other\" \"White\" \"Black\" \"Other\" ...\n#&gt;  $ education  : chr  NA NA \"High.School\" NA ...\n#&gt;  $ married    : chr  NA NA \"Previously.married\" NA ...\n#&gt;  $ income     : chr  \"Over100k\" \"Over100k\" \"&lt;25k\" NA ...\n#&gt;  $ bmi        : 'labelled' num  17.5 15.7 31.7 21.5 18.1 23.7 38.9 NA 21.3 19.7 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Body Mass Index (kg/m**2)\"\n#&gt;  $ diabetes   : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ weight     : 'labelled' num  8540 42567 8338 8723 7065 ...\n#&gt;   ..- attr(*, \"label\")= chr \"Full sample 2 year MEC exam weight\"\n#&gt;  $ psu        : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;  $ strata     : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\nnames(analytic.with.miss)\n#&gt;  [1] \"cholesterol\" \"gender\"      \"age\"         \"born\"        \"race\"       \n#&gt;  [6] \"education\"   \"married\"     \"income\"      \"bmi\"         \"diabetes\"   \n#&gt; [11] \"weight\"      \"psu\"         \"strata\"\n\nlibrary(DataExplorer)\nplot_missing(analytic.with.miss)\n\n\n\n\n\n\n\nFormatting variables\nRename variables to avoid conflicts. Recode variables into binary or categorical as needed. Ensure variable types (factor, numeric) are appropriate.\n\n# to avaoid any confusion later\n# rename weight variable as weights \n# is reserved for matching weights\nanalytic.with.miss$survey.weight &lt;- analytic.with.miss$weight\nanalytic.with.miss$weight &lt;- NULL\n\n#Creating binary variable for cholesterol\nanalytic.with.miss$cholesterol.bin &lt;- ifelse(analytic.with.miss$cholesterol &lt;200, \n                                             1, #\"healthy\",\n                                             0) #\"unhealthy\")\n# exposure recoding\nanalytic.with.miss$diabetes &lt;- ifelse(analytic.with.miss$diabetes == \"Yes\", 1, 0)\n\n# ID\nanalytic.with.miss$ID &lt;- 1:nrow(analytic.with.miss)\n\n# covariates\nanalytic.with.miss$born &lt;- ifelse(analytic.with.miss$born == \"Other\", \n                                             0,\n                                             1)\n\nvars = c(\"gender\", \"race\", \"education\", \n         \"married\", \"income\", \"bmi\")\n\nnumeric.names &lt;- c(\"cholesterol\", \"bmi\")\nfactor.names &lt;- vars[!vars %in% numeric.names] \n\nanalytic.with.miss[factor.names] &lt;- apply(X = analytic.with.miss[factor.names],\n                               MARGIN = 2, FUN = as.factor)\n\nanalytic.with.miss[numeric.names] &lt;- apply(X = analytic.with.miss[numeric.names],\n                                MARGIN = 2, FUN =function (x) \n                                  as.numeric(as.character(x)))\nanalytic.with.miss$income &lt;- factor(analytic.with.miss$income, \n                                    ordered = TRUE, \n                                levels = c(\"&lt;25k\", \"Between.25kto54k\", \n                                           \"Between.55kto99k\", \n                                           \"Over100k\"))\n\n# features\ntable(analytic.with.miss$strata)\n#&gt; \n#&gt; 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#&gt; 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\ntable(analytic.with.miss$psu)\n#&gt; \n#&gt;    1    2 \n#&gt; 4464 4790\ntable(analytic.with.miss$strata,analytic.with.miss$psu)\n#&gt;      \n#&gt;         1   2\n#&gt;   134 215 295\n#&gt;   135 316 322\n#&gt;   136 320 375\n#&gt;   137 306 248\n#&gt;   138 308 297\n#&gt;   139 278 375\n#&gt;   140 315 297\n#&gt;   141 282 411\n#&gt;   142 349 386\n#&gt;   143 232 319\n#&gt;   144 351 338\n#&gt;   145 339 270\n#&gt;   146 277 327\n#&gt;   147 335 261\n#&gt;   148 241 269\n\n# impute\n# require(mice)\n# imputation1 &lt;- mice(analytic.with.miss, seed = 123,\n#                    m = 1, # Number of multiple imputations. \n#                    maxit = 10 # Number of iteration; mostly useful for convergence\n#                    )\n# analytic.with.miss &lt;- complete(imputation1)\n# plot_missing(analytic.with.miss)\n\nComplete case data\nCreate a dataset (analytic.data) without NA values for analysis. This is done for simplified analysis, but this approach has it’s own challenges. In a next tutorial, we will appropriately deal with missing observations in a propensity score modelling.\n\ndim(analytic.with.miss)\n#&gt; [1] 9254   15\nanalytic.data &lt;- as.data.frame(na.omit(analytic.with.miss))\ndim(analytic.data) # complete case\n#&gt; [1] 4167   15\n\nZanutto (2006)\n\nRef: Zanutto (2006)\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nSet seed\n\nset.seed(123)\n\n\n“it is not necessary to use survey-weighted estimation for the propensity score model”\n\nPropensity score analysis in 4 steps:\n\nStep 1: PS model specification\nStep 2: Matching based on the estimated propensity scores\nStep 3: Balance checking\nStep 4: Outcome modelling\nStep 1\nSpecify the propensity score model to estimate propensity scores\n\nps.formula &lt;- as.formula(diabetes ~ gender + born +\n                         race + education + married + income + bmi)\n\nStep 2\nMatch treated and untreated subjects based on the estimated propensity scores. Perform nearest-neighbor matching using the propensity scores. Visualize the distribution of propensity scores before and after matching.\n\nrequire(MatchIt)\nset.seed(123)\n# This function fits propensity score model (using logistic \n# regression as above) when specified distance = 'logit'\n# performs nearest-neighbor (NN) matching, \n# without replacement \n# with caliper = .2*SD of propensity score  \n# within which to draw control units \n# with 1:1 ratio (pair-matching)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\n# see matchit function options here\n# https://www.rdocumentation.org/packages/MatchIt/versions/1.0-1/topics/matchit\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02901 0.12255 0.17658 0.18982 0.23876 0.82164\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02901 0.11509 0.16687 0.17949 0.22816 0.75968 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.04793 0.16489 0.21300 0.23395 0.27768 0.82164\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.019)\n#&gt;  - number of obs.: 4167 (original), 1564 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data &lt;- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nrequire(tableone)\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1 &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars,\n                       data = analytic.data, test = FALSE)\nprint(tab1, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                      3376           791               \n#&gt;   gender = Male (%)      1578 (46.7)    434 (54.9)   0.163\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.060\n#&gt;      Black                728 (21.6)    183 (23.1)        \n#&gt;      Hispanic             727 (21.5)    170 (21.5)        \n#&gt;      Other                642 (19.0)    159 (20.1)        \n#&gt;      White               1279 (37.9)    279 (35.3)        \n#&gt;   education (%)                                      0.185\n#&gt;      College             1992 (59.0)    415 (52.5)        \n#&gt;      High.School         1174 (34.8)    290 (36.7)        \n#&gt;      School               210 ( 6.2)     86 (10.9)        \n#&gt;   married (%)                                        0.316\n#&gt;      Married             2027 (60.0)    488 (61.7)        \n#&gt;      Never.married        631 (18.7)     70 ( 8.8)        \n#&gt;      Previously.married   718 (21.3)    233 (29.5)        \n#&gt;   income (%)                                         0.092\n#&gt;      &lt;25k                 830 (24.6)    225 (28.4)        \n#&gt;      Between.25kto54k    1064 (31.5)    244 (30.8)        \n#&gt;      Between.55kto99k     778 (23.0)    173 (21.9)        \n#&gt;      Over100k             704 (20.9)    149 (18.8)        \n#&gt;   bmi (mean (SD))       29.29 (7.11)  32.31 (8.03)   0.399\n\n\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                       782           782               \n#&gt;   gender = Male (%)       422 (54.0)    430 (55.0)   0.021\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.068\n#&gt;      Black                180 (23.0)    179 (22.9)        \n#&gt;      Hispanic             151 (19.3)    170 (21.7)        \n#&gt;      Other                171 (21.9)    156 (19.9)        \n#&gt;      White                280 (35.8)    277 (35.4)        \n#&gt;   education (%)                                      0.077\n#&gt;      College              441 (56.4)    411 (52.6)        \n#&gt;      High.School          262 (33.5)    286 (36.6)        \n#&gt;      School                79 (10.1)     85 (10.9)        \n#&gt;   married (%)                                        0.044\n#&gt;      Married              502 (64.2)    486 (62.1)        \n#&gt;      Never.married         63 ( 8.1)     69 ( 8.8)        \n#&gt;      Previously.married   217 (27.7)    227 (29.0)        \n#&gt;   income (%)                                         0.065\n#&gt;      &lt;25k                 202 (25.8)    218 (27.9)        \n#&gt;      Between.25kto54k     236 (30.2)    244 (31.2)        \n#&gt;      Between.55kto99k     187 (23.9)    171 (21.9)        \n#&gt;      Over100k             157 (20.1)    149 (19.1)        \n#&gt;   bmi (mean (SD))       32.12 (8.29)  32.05 (7.58)   0.009\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample. Use the matched sample to estimate the treatment effect, considering survey design.\nIncorporating the survey design into both linear regression and propensity score analysis is crucial. Neglecting the survey weights can significantly impact the estimates, altering the representation of population-level effects.\n\nrequire(survey)\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data$ID) # matched data\n#&gt; [1] 1564\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#&gt; [1] 1564\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7690 1564\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial(logit), design = w.design.m)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1564\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.02\n\n\nPseudo-R² (McFadden)\n0.01\n\n\nAIC\n1925.24\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.33\n0.97\n1.80\n1.79\n0.09\n\n\ndiabetes\n1.68\n1.17\n2.41\n2.84\n0.01\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDuGoff et al. (2014)\n\nRef: DuGoff, Schuler, and Stuart (2014)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Similar to Zanutto but includes additional covariates in the model.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula &lt;- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi+\n                           psu+strata+survey.weight)\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = 'logit', \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00363 0.11341 0.17509 0.18982 0.24766 0.80853\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.00363 0.10394 0.16243 0.17690 0.23461 0.73143 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01182 0.17245 0.22748 0.24500 0.29948 0.80853\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: Propensity score [caliper]\n#&gt; \n#&gt;              - estimated with logistic regression\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4167 (original), 1570 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi, psu, strata, survey.weight\n# extract matched data\nmatched.data &lt;- match.data(match.obj)\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nrequire(tableone)\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\", \n                  \"psu\", \"strata\", \"survey.weight\")\nmatched.data$survey.weight &lt;- as.numeric(as.character(matched.data$survey.weight))\nmatched.data$strata &lt;- as.numeric(as.character(matched.data$strata))\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", vars = baselinevars, \n                        data = matched.data, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                            Stratified by diabetes\n#&gt;                             0                   1                   SMD   \n#&gt;   n                              785                 785                  \n#&gt;   gender = Male (%)              433 (55.2)          431 (54.9)      0.005\n#&gt;   born (mean (SD))              1.00 (0.00)         1.00 (0.00)     &lt;0.001\n#&gt;   race (%)                                                           0.048\n#&gt;      Black                       193 (24.6)          180 (22.9)           \n#&gt;      Hispanic                    163 (20.8)          170 (21.7)           \n#&gt;      Other                       163 (20.8)          158 (20.1)           \n#&gt;      White                       266 (33.9)          277 (35.3)           \n#&gt;   education (%)                                                      0.032\n#&gt;      College                     403 (51.3)          412 (52.5)           \n#&gt;      High.School                 300 (38.2)          288 (36.7)           \n#&gt;      School                       82 (10.4)           85 (10.8)           \n#&gt;   married (%)                                                        0.030\n#&gt;      Married                     473 (60.3)          484 (61.7)           \n#&gt;      Never.married                71 ( 9.0)           70 ( 8.9)           \n#&gt;      Previously.married          241 (30.7)          231 (29.4)           \n#&gt;   income (%)                                                         0.035\n#&gt;      &lt;25k                        232 (29.6)          222 (28.3)           \n#&gt;      Between.25kto54k            236 (30.1)          242 (30.8)           \n#&gt;      Between.55kto99k            176 (22.4)          173 (22.0)           \n#&gt;      Over100k                    141 (18.0)          148 (18.9)           \n#&gt;   bmi (mean (SD))              31.92 (8.33)        32.09 (7.52)      0.020\n#&gt;   psu = 2 (%)                    382 (48.7)          394 (50.2)      0.031\n#&gt;   strata (mean (SD))          140.84 (4.24)       140.97 (4.23)      0.031\n#&gt;   survey.weight (mean (SD)) 35647.01 (37699.98) 35596.81 (45212.82)  0.001\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample\n\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data$ID) # matched data\n#&gt; [1] 1570\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data$ID])\n#&gt; [1] 1570\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7684 1570\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial, design = w.design.m)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1570\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.01\n\n\nPseudo-R² (McFadden)\n0.00\n\n\nAIC\n1918.09\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.69\n1.33\n2.15\n4.24\n0.00\n\n\ndiabetes\n1.32\n0.99\n1.77\n1.86\n0.08\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nAustin et al. (2018)\n\nRef: Austin, Jembere, and Chiu (2018)\n\n\nPropensity score analysis in 4 steps (PATT)\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nStep 1\nSpecify the propensity score model to estimate propensity scores. Use survey logistic regression to account for survey design in propensity score estimation.\n\n# response = exposure variable\n# independent variables = baseline covariates\nps.formula &lt;- as.formula(diabetes ~ gender + born + race + education + \n                            married + income + bmi)\nrequire(survey)\nanalytic.design &lt;- svydesign(id=~psu,weights=~survey.weight, \n                             strata=~strata,\n                             data=analytic.data, nest=TRUE)\nps.fit &lt;- svyglm(ps.formula, design=analytic.design, family=quasibinomial)\nanalytic.data$PS &lt;- fitted(ps.fit)\nsummary(analytic.data$PS)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\n\nStep 2\nMatch treated and untreated subjects on the estimated propensity scores. Two methods are explored: using the Matching package and the MatchIt package.\n\nrequire(Matching)\n#&gt; Loading required package: Matching\n#&gt; Loading required package: MASS\n#&gt; \n#&gt; Attaching package: 'MASS'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     select\n#&gt; ## \n#&gt; ##  Matching (Version 4.10-15, Build Date: 2024-10-14)\n#&gt; ##  See https://www.jsekhon.com for additional documentation.\n#&gt; ##  Please cite software as:\n#&gt; ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n#&gt; ##   Software with Automated Balance Optimization: The Matching package for R.''\n#&gt; ##   Journal of Statistical Software, 42(7): 1-52. \n#&gt; ##\nmatch.obj2 &lt;- Match(Y=analytic.data$cholesterol, \n                    Tr=analytic.data$diabetes, \n                    X=analytic.data$PS, \n                    M=1, \n                    estimand = \"ATT\",\n                    replace=FALSE, \n                    caliper = 0.2)\nsummary(match.obj2)\n#&gt; \n#&gt; Estimate...  -15.287 \n#&gt; SE.........  2.118 \n#&gt; T-stat.....  -7.2175 \n#&gt; p.val......  5.2958e-13 \n#&gt; \n#&gt; Original number of observations..............  4167 \n#&gt; Original number of treated obs...............  791 \n#&gt; Matched number of observations...............  781 \n#&gt; Matched number of observations  (unweighted).  781 \n#&gt; \n#&gt; Caliper (SDs)........................................   0.2 \n#&gt; Number of obs dropped by 'exact' or 'caliper'  10\nmatched.data2 &lt;- analytic.data[c(match.obj2$index.treated, \n                                 match.obj2$index.control),]\ndim(matched.data2)\n#&gt; [1] 1562   16\n\n\nrequire(MatchIt)\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, data = analytic.data,\n                     distance = analytic.data$PS, \n                     method = \"nearest\", \n                     replace=FALSE,\n                     caliper = .2, \n                     ratio = 1)\nanalytic.data$PS &lt;- match.obj$distance\nsummary(match.obj$distance)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08788 0.13399 0.15120 0.19285 0.86375\nplot(match.obj, type = \"jitter\")\n\n\n\n\n\n\n#&gt; To identify the units, use first mouse button; to stop, use second.\nplot(match.obj, type = \"hist\")\n\n\n\n\n\n\ntapply(analytic.data$PS, analytic.data$diabetes, summary)\n#&gt; $`0`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.01352 0.08182 0.12644 0.14119 0.18267 0.75047 \n#&gt; \n#&gt; $`1`\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.02352 0.12362 0.16998 0.19389 0.23171 0.86375\n# check how many matched\nmatch.obj\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.019)\n#&gt;  - number of obs.: 4167 (original), 1568 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: gender, born, race, education, married, income, bmi\n# extract matched data\nmatched.data2 &lt;- match.data(match.obj)\ndim(matched.data2)\n#&gt; [1] 1568   19\n\nStep 3\nCompare the similarity of baseline characteristics between treated and untreated subjects in a the propensity score-matched sample. In this case, we will compare SMD &lt; 0.2 or not.\n\nbaselinevars &lt;- c(\"gender\", \"born\", \"race\", \"education\", \n                  \"married\", \"income\", \"bmi\")\ntab1m &lt;- CreateTableOne(strata = \"diabetes\", \n                           vars = baselinevars,\n                           data = matched.data2, test = FALSE)\nprint(tab1m, smd = TRUE)\n#&gt;                        Stratified by diabetes\n#&gt;                         0             1             SMD   \n#&gt;   n                       784           784               \n#&gt;   gender = Male (%)       405 (51.7)    431 (55.0)   0.067\n#&gt;   born (mean (SD))       1.00 (0.00)   1.00 (0.00)  &lt;0.001\n#&gt;   race (%)                                           0.134\n#&gt;      Black                163 (20.8)    182 (23.2)        \n#&gt;      Hispanic             139 (17.7)    170 (21.7)        \n#&gt;      Other                171 (21.8)    157 (20.0)        \n#&gt;      White                311 (39.7)    275 (35.1)        \n#&gt;   education (%)                                      0.040\n#&gt;      College              428 (54.6)    413 (52.7)        \n#&gt;      High.School          274 (34.9)    288 (36.7)        \n#&gt;      School                82 (10.5)     83 (10.6)        \n#&gt;   married (%)                                        0.070\n#&gt;      Married              509 (64.9)    485 (61.9)        \n#&gt;      Never.married         59 ( 7.5)     70 ( 8.9)        \n#&gt;      Previously.married   216 (27.6)    229 (29.2)        \n#&gt;   income (%)                                         0.063\n#&gt;      &lt;25k                 220 (28.1)    220 (28.1)        \n#&gt;      Between.25kto54k     226 (28.8)    242 (30.9)        \n#&gt;      Between.55kto99k     192 (24.5)    173 (22.1)        \n#&gt;      Over100k             146 (18.6)    149 (19.0)        \n#&gt;   bmi (mean (SD))       31.93 (8.16)  32.11 (7.61)   0.023\n\nStep 4\nEstimate the effect of treatment on outcomes using propensity score-matched sample.\n\n# setup the design with survey features\nanalytic.with.miss$matched &lt;- 0\nlength(analytic.with.miss$ID) # full data\n#&gt; [1] 9254\nlength(matched.data2$ID) # matched data\n#&gt; [1] 1568\nlength(analytic.with.miss$ID[analytic.with.miss$ID %in% matched.data2$ID])\n#&gt; [1] 1568\nanalytic.with.miss$matched[analytic.with.miss$ID %in% matched.data2$ID] &lt;- 1\ntable(analytic.with.miss$matched)\n#&gt; \n#&gt;    0    1 \n#&gt; 7686 1568\nw.design0 &lt;- svydesign(strata=~strata, id=~psu, weights=~survey.weight, \n                      data=analytic.with.miss, nest=TRUE)\nw.design.m2 &lt;- subset(w.design0, matched == 1)\n\n\nout.formula &lt;- as.formula(cholesterol.bin ~ diabetes)\nsfit &lt;- svyglm(out.formula,family=binomial, design = w.design.m2)\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nrequire(jtools)\nsumm(sfit, exp = TRUE, confint = TRUE)\n\n\n\n\nObservations\n1568\n\n\nDependent variable\ncholesterol.bin\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.01\n\n\nPseudo-R² (McFadden)\n0.01\n\n\nAIC\n1919.53\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.46\n1.15\n1.86\n3.11\n0.01\n\n\ndiabetes\n1.52\n1.21\n1.90\n3.65\n0.00\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91.",
    "crumbs": [
      "Propensity score",
      "PSM in OA-CVD (US)"
    ]
  },
  {
    "objectID": "propensityscore4.html",
    "href": "propensityscore4.html",
    "title": "PSM in BMI-diabetes",
    "section": "",
    "text": "Propensity analysis problem\nSee explained in the previous chapter on PSM in OA-CVD (US), there are four steps of the propensity score matching:\nOnly Step 1 is different for Zanutto (2006), DuGoff et al. (2014), and Austin et al. (2018) approaches.",
    "crumbs": [
      "Propensity score",
      "PSM in BMI-diabetes"
    ]
  },
  {
    "objectID": "propensityscore4.html#references",
    "href": "propensityscore4.html#references",
    "title": "PSM in BMI-diabetes",
    "section": "References",
    "text": "References\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91.",
    "crumbs": [
      "Propensity score",
      "PSM in BMI-diabetes"
    ]
  },
  {
    "objectID": "propensityscore5.html",
    "href": "propensityscore5.html",
    "title": "PSM with MI",
    "section": "",
    "text": "The tutorial provides a detailed walkthrough of implementing Propensity Score Matching (PSM) combined with Multiple Imputation (MI) in a statistical analysis, focusing on handling missing data and mitigating bias in observational studies.\nThe initial chunk is dedicated to loading various R packages that will be utilized throughout the tutorial. These libraries provide functions and tools that facilitate data manipulation, statistical modeling, visualization, and more.\n\n# Load required packages\nlibrary(MatchIt)\nrequire(tableone)\nrequire(survey)\nrequire(cobalt)\nrequire(Publish)\nrequire(optmatch)\nrequire(data.table)\nrequire(jtools)\nrequire(ggstance)\nrequire(DataExplorer)\nrequire(mitools)\nlibrary(kableExtra)\nlibrary(mice)\n\nProblem Statement\nLogistic regression\n\nPerform multiple imputation to deal with missing values; with 3 imputed datasets, 5 iterations,\nfit survey featured logistic regression in all of the 3 imputed datasets, and\nobtain the pooled OR (adjusted) and the corresponding 95% confidence intervals.\n\nHints\n\nUse the covariates (listed below) in the imputation model.\n\nImputation model covariates can be different than the original analysis covariates. You are encouraged to use variables in the imputation model that can be predictive of the variables with missing observations. In this example, we use the strata variable as an auxiliary variable in the imputation model, but not the survey weight or PSU variable.\nAlso the imputation model specification can be modified. For example, we use pmm method for bmi in the imputation model.\nRemove any subject ID variable from the imputation model, if created in an intermediate step. Indeed ID variables should not be in the imputation model, if they are not predictive of the variables with missing observations.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPredictive Mean Matching:\nThe “Predictive Mean Matching” (PMM) method in Multiple Imputation (MI) is a widely used technique to handle missing data, particularly well-suited for continuous variables. PMM operates by first creating a predictive model for the variable with missing data, using observed values from other variables in the dataset. For each missing value, PMM identifies a set of observed values with predicted scores that are close to the predicted score for the missing value, derived from the predictive model. Then, instead of imputing a predicted score directly, PMM randomly selects one of the observed values from this set and assigns it as the imputed value. This method retains the original distribution of the imputed variable since it only uses observed values for imputation, and it also tends to preserve relationships between variables. PMM is particularly advantageous when the normality assumption of the imputed variable is questionable, providing a robust and practical approach to managing missing data in various research contexts.\n\n\nPropensity score matching (Zanutto, 2006)\n\nUse the propensity score matching as per Zanutto E. L. (2006)’s recommendation in all of the imputed datasets.\nReport the pooled OR estimates (adjusted) and corresponding 95% confidence intervals (adjusted OR).\nData and variables\nAnalytic data\nThe analytic dataset is saved as NHANES17.RData.\nVariables\nWe are primarily interested in outcome diabetes and exposure whether born in the US (born).\nVariables under consideration:\n\nsurvey features\n\nPSU\nstrata\nsurvey weight\n\n\nCovariates\n\nrace\nage\nmarriage\neducation\ngender\nBMI\nsystolic blood pressure\n\n\nPre-processing\nThe data is loaded and variables of interest are identified.\n\nload(file=\"Data/propensityscore/NHANES17.RData\") # read data\nls()\n#&gt; [1] \"analytic\"           \"analytic.with.miss\"\ndim(analytic.with.miss)\n#&gt; [1] 9254   34\nvars &lt;- c(\"ID\", # ID\n          \"psu\", \"strata\", \"weight\", # Survey features \n          \"race\", \"age\", \"married\",\"education\",\"gender\",\"bmi\",\"systolicBP\", # Covariates\n          \"born\", # Exposure\n          \"diabetes\") # Outcome\n\nSubset the dataset\nThe dataset is then subsetted to retain only the relevant variables, ensuring that subsequent analyses are focused and computationally efficient.\n\ndat.with.miss &lt;- analytic.with.miss[,vars]\ndim(analytic.with.miss)\n#&gt; [1] 9254   34\n\nInspect weights\nThe weights of the observations are inspected and adjusted to avoid issues in subsequent analyses.\n\nsummary(dat.with.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0   12347   21060   34671   37562  419763\n# weight = 0 would create problem in the analysis\n# ad-hoc solution to 0 weight problem\ndat.with.miss$weight[dat.with.miss$weight == 0] &lt;- 0.00000001\n\nRecode the exposure variable\nThe exposure variable is recoded for clarity and ease of interpretation in results.\n\ndat.with.miss$born &lt;- car::recode(dat.with.miss$born, \nrecodes = \" 'Born in 50 US states or Washingt' = \n'Born in US'; 'Others' = 'Others'; else = NA \" )\ndat.with.miss$born &lt;- factor(dat.with.miss$born, levels = c(\"Born in US\", \"Others\"))\n\nvariable types\nVariable types are set, ensuring that each variable is treated appropriately in the analyses.\n\nfactor.names &lt;- c(\"race\", \"married\", \"education\", \"gender\", \"diabetes\")\ndat.with.miss[,factor.names] &lt;- lapply(dat.with.miss[,factor.names], factor)\n\nInspect extent of missing data problem\nA visualization is generated to explore the extent and pattern of missing data in the dataset, which informs the strategy for handling them.\n\nrequire(DataExplorer)\nplot_missing(dat.with.miss)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nNote that, multiple imputation then delete (MID) approach can be applied if the outcome had some missing values. Due to the small number of missingness, MICE may not impute the outcomes BTW.\n\n\n\n\n\n\nTip\n\n\n\nMultiple imputation then delete (MID):\nMID is a specific approach used in the context of multiple imputation (MI) when dealing with missing outcome data. All missing values, including those in the outcome variable, are imputed to create several complete datasets. In subsequent analyses, the imputed values for the outcome variable are deleted, so that only observed outcome values are analyzed. Each dataset (with observed outcome values and imputed predictor values) is analyzed separately, and results are pooled to provide a single estimate.\n\n\nLogistic regression\nInitialization\nThe MI process is initialized, setting up the framework for subsequent imputations.\n\nimputation &lt;- mice(data = dat.with.miss, maxit = 0, print = FALSE)\n\nSetting imputation model covariates\nThe predictor matrix is adjusted to specify which variables will be used to predict missing values in the imputation model. Setting strata as auxiliary variable:\n\npred &lt;- imputation$pred\npred\n#&gt;            ID psu strata weight race age married education gender bmi\n#&gt; ID          0   1      1      1    1   1       1         1      1   1\n#&gt; psu         1   0      1      1    1   1       1         1      1   1\n#&gt; strata      1   1      0      1    1   1       1         1      1   1\n#&gt; weight      1   1      1      0    1   1       1         1      1   1\n#&gt; race        1   1      1      1    0   1       1         1      1   1\n#&gt; age         1   1      1      1    1   0       1         1      1   1\n#&gt; married     1   1      1      1    1   1       0         1      1   1\n#&gt; education   1   1      1      1    1   1       1         0      1   1\n#&gt; gender      1   1      1      1    1   1       1         1      0   1\n#&gt; bmi         1   1      1      1    1   1       1         1      1   0\n#&gt; systolicBP  1   1      1      1    1   1       1         1      1   1\n#&gt; born        1   1      1      1    1   1       1         1      1   1\n#&gt; diabetes    1   1      1      1    1   1       1         1      1   1\n#&gt;            systolicBP born diabetes\n#&gt; ID                  1    1        1\n#&gt; psu                 1    1        1\n#&gt; strata              1    1        1\n#&gt; weight              1    1        1\n#&gt; race                1    1        1\n#&gt; age                 1    1        1\n#&gt; married             1    1        1\n#&gt; education           1    1        1\n#&gt; gender              1    1        1\n#&gt; bmi                 1    1        1\n#&gt; systolicBP          0    1        1\n#&gt; born                1    0        1\n#&gt; diabetes            1    1        0\npred[,\"ID\"] &lt;- pred[\"ID\",] &lt;- 0\npred[,\"psu\"] &lt;- pred[\"psu\",] &lt;- 0\npred[,\"weight\"] &lt;- pred[\"weight\",] &lt;- 0\npred[\"strata\",] &lt;- 0\npred\n#&gt;            ID psu strata weight race age married education gender bmi\n#&gt; ID          0   0      0      0    0   0       0         0      0   0\n#&gt; psu         0   0      0      0    0   0       0         0      0   0\n#&gt; strata      0   0      0      0    0   0       0         0      0   0\n#&gt; weight      0   0      0      0    0   0       0         0      0   0\n#&gt; race        0   0      1      0    0   1       1         1      1   1\n#&gt; age         0   0      1      0    1   0       1         1      1   1\n#&gt; married     0   0      1      0    1   1       0         1      1   1\n#&gt; education   0   0      1      0    1   1       1         0      1   1\n#&gt; gender      0   0      1      0    1   1       1         1      0   1\n#&gt; bmi         0   0      1      0    1   1       1         1      1   0\n#&gt; systolicBP  0   0      1      0    1   1       1         1      1   1\n#&gt; born        0   0      1      0    1   1       1         1      1   1\n#&gt; diabetes    0   0      1      0    1   1       1         1      1   1\n#&gt;            systolicBP born diabetes\n#&gt; ID                  0    0        0\n#&gt; psu                 0    0        0\n#&gt; strata              0    0        0\n#&gt; weight              0    0        0\n#&gt; race                1    1        1\n#&gt; age                 1    1        1\n#&gt; married             1    1        1\n#&gt; education           1    1        1\n#&gt; gender              1    1        1\n#&gt; bmi                 1    1        1\n#&gt; systolicBP          0    1        1\n#&gt; born                1    0        1\n#&gt; diabetes            1    1        0\n\nSetting imputation model specification\nThe method for imputing a particular variable is specified (e.g., using Predictive Mean Matching). Here, we add pmm for bmi:\n\nmeth &lt;- imputation$meth\nmeth[\"bmi\"] &lt;- \"pmm\"\n\nImpute incomplete data\nMultiple datasets are imputed, each providing a different “guess” at the missing values, based on observed data. We are imputing m = 3 times.\n\nimputation &lt;- mice(data = dat.with.miss, \n                   seed = 123, \n                   predictorMatrix = pred,\n                   method = meth, \n                   m = 3, \n                   maxit = 5, \n                   print = FALSE)\nimpdata &lt;- mice::complete(imputation, action=\"long\")\nimpdata$.id &lt;- NULL\nm &lt;- 3\nset.seed(123)\nallImputations &lt;-  imputationList(lapply(1:m, \n                                         function(n)\n                                           subset(impdata, \n                                                  subset=.imp==n)))\nstr(allImputations)\n#&gt; List of 2\n#&gt;  $ imputations:List of 3\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 57 46 66 50 23 66 75 49 56 36 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 2 1 3 3 1 1 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 2 2 1 1 3 1 2 1 3 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 31.2 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 108 96 200 112 128 124 120 122 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 24 49 66 32 34 66 75 80 56 28 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 2 1 3 2 1 1 3 3 1 2 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 2 3 1 1 1 1 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 24.9 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 102 104 136 112 128 120 120 120 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;   ..$ :'data.frame': 9254 obs. of  14 variables:\n#&gt;   .. ..$ ID        : int [1:9254] 93703 93704 93705 93706 93707 93708 93709 93710 93711 93712 ...\n#&gt;   .. ..$ psu       : Factor w/ 2 levels \"1\",\"2\": 2 1 2 2 1 2 1 1 2 2 ...\n#&gt;   .. ..$ strata    : Factor w/ 15 levels \"134\",\"135\",\"136\",..: 12 10 12 1 5 5 3 1 1 14 ...\n#&gt;   .. ..$ weight    : num [1:9254] 8540 42567 8338 8723 7065 ...\n#&gt;   .. ..$ race      : Factor w/ 4 levels \"Black\",\"Hispanic\",..: 3 4 1 3 3 3 1 4 3 2 ...\n#&gt;   .. ..$ age       : int [1:9254] 47 71 66 71 45 66 75 37 56 47 ...\n#&gt;   .. ..$ married   : Factor w/ 3 levels \"Married\",\"Never.married\",..: 1 1 3 1 1 1 3 1 1 1 ...\n#&gt;   .. ..$ education : Factor w/ 3 levels \"College\",\"High.School\",..: 1 1 2 1 1 3 1 1 1 2 ...\n#&gt;   .. ..$ gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 2 2 1 1 1 2 2 ...\n#&gt;   .. ..$ bmi       : num [1:9254] 17.5 15.7 31.7 21.5 18.1 23.7 38.9 15.9 21.3 19.7 ...\n#&gt;   .. ..$ systolicBP: int [1:9254] 100 114 162 112 128 166 120 116 108 112 ...\n#&gt;   .. ..$ born      : Factor w/ 2 levels \"Born in US\",\"Others\": 1 1 1 1 1 2 1 1 2 2 ...\n#&gt;   .. ..$ diabetes  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#&gt;   .. ..$ .imp      : int [1:9254] 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ call       : language imputationList(lapply(1:m, function(n) subset(impdata, subset = .imp ==      n)))\n#&gt;  - attr(*, \"class\")= chr \"imputationList\"\n\nDesign\nA survey design object is created, ensuring that subsequent analyses appropriately account for the survey design.\n\nw.design &lt;- svydesign(ids = ~psu, weights = ~weight, strata = ~strata,\n                      data = allImputations, nest = TRUE)\n\nSurvey data analysis\nA logistic regression model is fitted to each imputed dataset.\n\nmodel.formula &lt;- as.formula(I(diabetes == 'Yes') ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\nfit.from.logistic &lt;- with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPooled estimates\nResults from models across all imputed datasets are pooled to provide a single estimate, accounting for the uncertainty due to missing data.\n\npooled.estimates &lt;- MIcombine(fit.from.logistic)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#&gt; Multiple imputation results:\n#&gt;       with(w.design, svyglm(model.formula, family = binomial(\"logit\")))\n#&gt;       MIcombine.default(fit.from.logistic)\n#&gt;                           results      se  (lower  upper) missInfo\n#&gt; (Intercept)               0.00013 7.5e-05 3.9e-05 0.00041     22 %\n#&gt; bornOthers                1.44729 2.7e-01 1.0e+00 2.07384      0 %\n#&gt; raceHispanic              0.81619 1.1e-01 6.3e-01 1.05882      0 %\n#&gt; raceOther                 1.43817 2.6e-01 1.0e+00 2.04954      3 %\n#&gt; raceWhite                 0.86411 1.3e-01 6.5e-01 1.14994      3 %\n#&gt; age                       1.06157 3.6e-03 1.1e+00 1.06874      6 %\n#&gt; marriedNever.married      0.83242 1.6e-01 5.7e-01 1.20809     10 %\n#&gt; marriedPreviously.married 0.88401 1.1e-01 6.8e-01 1.14163     11 %\n#&gt; educationHigh.School      1.16331 1.9e-01 8.4e-01 1.60803      0 %\n#&gt; educationSchool           1.41943 2.4e-01 1.0e+00 1.98397      7 %\n#&gt; genderMale                1.53458 1.8e-01 1.2e+00 1.94217      3 %\n#&gt; bmi                       1.10597 1.2e-02 1.1e+00 1.12956      1 %\n#&gt; systolicBP                1.00325 3.2e-03 1.0e+00 1.01001     39 %\nOR &lt;- round(exp(pooled.estimates$coefficients), 2) \nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR[2,]\n\n\n  \n\n\n\nPropensity score matching analysis\nInitialization\nThe MI process is re-initialized to facilitate PSM in the context of MI.\n\nimputation &lt;- mice(data = dat.with.miss, maxit = 0, print = FALSE)\nimpdata &lt;- mice::complete(imputation, action=\"long\")\nm &lt;- 3\nallImputations &lt;- imputationList(lapply(1:m, \n                                        function(n) \n                                          subset(impdata, \n                                                 subset=.imp==n)))\n\nZanutto E. L. (2006) under multiple imputation\n\n\n\n\n\n\nTip\n\n\n\nAn iterative process is performed within each imputed dataset, which involves:\n\nEstimating propensity scores.\nMatching treated and untreated subjects based on these scores.\nExtracting matched data and checking the balance of covariates across matched groups.\nFitting outcome models to the survey weighted matched data and estimating treatment effects.\n\n\n\nNotice that we are performing multi-step process within MI\n\nmatch.statm &lt;- SMDm &lt;- tab1m &lt;- vector(\"list\", m) \nfit.from.PS &lt;- vector(\"list\", m)\n\nfor (i in 1:m) {\n  analytic.i &lt;- allImputations$imputations[[i]]\n  # Rename the weight variable into survey.weight\n  names(analytic.i)[names(analytic.i) == \"weight\"] &lt;- \"survey.weight\"\n  \n  # Specify the PS model to estimate propensity scores\n  ps.formula &lt;- as.formula(I(born==\"Others\") ~ \n                             race + age + married + education + \n                             gender + bmi + systolicBP)\n\n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = analytic.i, family = binomial(\"logit\"))\n  analytic.i$PS &lt;- fitted(ps.fit)\n  \n  # Match exposed and unexposed subjects \n  set.seed(123)\n  match.obj &lt;- matchit(ps.formula, data = analytic.i, \n                       distance = analytic.i$PS, \n                       method = \"nearest\", \n                       replace = FALSE,\n                       caliper = 0.2, \n                       ratio = 1)\n  match.statm[[i]] &lt;- match.obj\n  analytic.i$PS &lt;- match.obj$distance\n  \n  # Extract matched data\n  matched.data &lt;- match.data(match.obj) \n  \n  # Balance checking\n  cov &lt;- c(\"race\", \"age\", \"married\", \"education\", \"gender\", \"bmi\", \"systolicBP\")\n  \n  tab1m[[i]] &lt;- CreateTableOne(strata = \"born\", \n                               vars = cov, data = matched.data, \n                               test = FALSE, smd = TRUE)\n  SMDm[[i]] &lt;- ExtractSmd(tab1m[[i]])\n  \n  # Setup the design with survey features\n  analytic.i$matched &lt;- 0\n  analytic.i$matched[analytic.i$ID %in% matched.data$ID] &lt;- 1\n  \n  # Survey setup for full data\n  w.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                         data = analytic.i, nest = TRUE)\n  \n  # Subset matched data\n  w.design.m &lt;- subset(w.design0, matched == 1)\n  \n  # Outcome model (double adjustment)\n  out.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ \n                              born + race + age + married + \n                              education + gender + bmi + systolicBP)\n  fit.from.PS[[i]] &lt;- svyglm(out.formula, design = w.design.m, \n                     family = quasibinomial(\"logit\"))\n}\n\nCheck matched data\nThe matched data is inspected to ensure that matching was successful and appropriate.\n\nmatch.statm\n#&gt; [[1]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3590 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n#&gt; \n#&gt; [[2]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3598 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n#&gt; \n#&gt; [[3]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.044)\n#&gt;  - number of obs.: 9254 (original), 3594 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: race, age, married, education, gender, bmi, systolicBP\n\nCheck balance in matched data\nThe balance of covariates across matched groups is assessed to ensure that matching has successfully reduced bias.\n\nSMDm\n#&gt; [[1]]\n#&gt;                 1 vs 2\n#&gt; race       0.028883793\n#&gt; age        0.033614763\n#&gt; married    0.007318561\n#&gt; education  0.117536503\n#&gt; gender     0.040145831\n#&gt; bmi        0.043350560\n#&gt; systolicBP 0.054549772\n#&gt; \n#&gt; [[2]]\n#&gt;                 1 vs 2\n#&gt; race       0.019901420\n#&gt; age        0.016267050\n#&gt; married    0.017196043\n#&gt; education  0.128811588\n#&gt; gender     0.003338016\n#&gt; bmi        0.057014434\n#&gt; systolicBP 0.071553721\n#&gt; \n#&gt; [[3]]\n#&gt;                1 vs 2\n#&gt; race       0.04490482\n#&gt; age        0.01959377\n#&gt; married    0.03687394\n#&gt; education  0.13301810\n#&gt; gender     0.01225625\n#&gt; bmi        0.03697878\n#&gt; systolicBP 0.10025529\n\nPooled estimate\nFinally, the treatment effect estimates from the matched analyses across all imputed datasets are pooled to provide a single, overall estimate, ensuring that the final result appropriately accounts for the uncertainty due to both the matching process and the imputation of missing data.\n\npooled.estimates &lt;- MIcombine(fit.from.PS)\nsummary(pooled.estimates, digits = 2, logeffect=TRUE)\n#&gt; Multiple imputation results:\n#&gt;       MIcombine.default(fit.from.PS)\n#&gt;                           results      se  (lower  upper) missInfo\n#&gt; (Intercept)               8.9e-05 4.9e-05 0.00003 0.00026      8 %\n#&gt; bornOthers                2.0e+00 3.1e-01 1.47719 2.73325     16 %\n#&gt; raceHispanic              7.0e-01 1.7e-01 0.42593 1.15504     27 %\n#&gt; raceOther                 1.4e+00 4.1e-01 0.77209 2.53278     26 %\n#&gt; raceWhite                 4.9e-01 2.7e-01 0.15853 1.52308     28 %\n#&gt; age                       1.1e+00 4.6e-03 1.04472 1.06298      8 %\n#&gt; marriedNever.married      5.9e-01 2.0e-01 0.28824 1.18926     38 %\n#&gt; marriedPreviously.married 1.0e+00 2.5e-01 0.62417 1.64438     11 %\n#&gt; educationHigh.School      1.4e+00 3.0e-01 0.89403 2.10852      2 %\n#&gt; educationSchool           1.3e+00 3.4e-01 0.81042 2.21438      7 %\n#&gt; genderMale                1.3e+00 2.3e-01 0.86695 1.83880     31 %\n#&gt; bmi                       1.1e+00 1.1e-02 1.08062 1.12285      5 %\n#&gt; systolicBP                1.0e+00 3.0e-03 1.00314 1.01494      3 %\nOR &lt;- round(exp(pooled.estimates$coefficients), 2) \nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR[2,]",
    "crumbs": [
      "Propensity score",
      "PSM with MI"
    ]
  },
  {
    "objectID": "propensityscore6.html",
    "href": "propensityscore6.html",
    "title": "PS Weighting (US)",
    "section": "",
    "text": "Propensity analysis problem\nIn this chapter, we will use propensity score weighting (using SMD cut-point 0.2, may adjust for imbalanced and/or all covariates in the outcome model, if any) analysis as per the following recommendations - (Zanutto 2006) - (DuGoff, Schuler, and Stuart 2014) - (Austin, Jembere, and Chiu 2018)\nDataset\n\nThe following modified NHANES dataset\n\nNHANES15lab5.RData\n\n\nuse the data set “analytic.with.miss” within this file.\n\nfor obtaining the final treatment effect estimates, you can omit missing values, but only after creating the design (e.g., subset the design, not the data itself directly).\n\n\nThe same dataset was used for propensity score weighting\n\nVariables\n\nOutcome: diabetes\n\n‘No’ as the reference category\n\n\nExposure: bmi\n\nconvert to binary with &gt;25 vs. &lt;= 25,\nwith &gt; 25 as the reference category\n\n\nConfounder list:\n\ngender\nage\n\nassume continuous\n\n\nrace\nincome\neducation\nmarried\ncholesterol\ndiastolicBP\nsystolicBP\n\n\nMediator:\n\nphysical.work\n\n‘No’ as the reference category\n\n\n\n\nSurvey features\n\npsu\nstrata\nweight\n\n\nPre-processing\nLoad data\n\nload(file=\"Data/propensityscore/NHANES15lab5.RData\")\n\nVariable summary\n\n# Full data\ndat.full &lt;- analytic.with.miss\n\n# Exposure\ndat.full$bmi &lt;- with(dat.full, ifelse(bmi&gt;25, \"Overweight\", \n                                      ifelse(bmi&lt;=25, \"Not overweight\", NA)))\ndat.full$bmi &lt;- as.factor(dat.full$bmi)\ndat.full$bmi &lt;- relevel(dat.full$bmi, ref = \"Overweight\")\n\n# Drop unnecessary variables \ndat.full$born &lt;- NULL\ndat.full$physical.work &lt;- NULL\n\n# Rename the weight variable into interview.weight\nnames(dat.full)[names(dat.full) == \"weight\"] &lt;- \"interview.weight\"\n\nComplete case data\nWe will use the complete case data to perform the analysis.\n\n# Complete case data \nanalytic.data &lt;- dat.full[complete.cases(dat.full),]\ndim(analytic.data)\n#&gt; [1] 6316   15\n\nReproducibility\n\nset.seed(504)\n\nApproach by Zanutto (2006)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(bmi==\"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\nStep 2\nFor the second step, we will calculate the both unstabilized and stabilized weights. However, stabilized inverse probability weight is often recommended to prevent from extreme weights (Hernán and Robins 2006).\n\n# Propensity scores\nps.fit &lt;- glm(ps.formula, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps &lt;- predict(ps.fit, type = \"response\", newdata = analytic.data)\n\n# Unstabilized weight\nanalytic.data$usweight &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                     1/ps, 1/(1-ps)))\n\n# Unstabilized weight summary\nround(summary(analytic.data$usweight), 2)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.01    1.33    1.63    2.10    2.14   62.31\n\n# Stabilized weight\nanalytic.data$sweight &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps)))\n\n# Stabilized weight summary\nround(summary(analytic.data$sweight), 2)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.44    0.70    0.84    1.04    1.08   25.40\n\nWe can see that the mean of stabilized weights is 1, while it is approximately 2.1 for unstabilized weights. For both unstabilized and stabilized weights, it seems there are extreme weights, particularly for the unstabilized weights. Extreme weights could be dealt with weight truncation, typically truncated at the 1st and 99th percentiles (Cole and Hernán 2008).\nLet us truncate the weights at the 1st and 99th percentiles\n\n# Truncating unstabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(usweight_t = pmin(pmax(usweight, quantile(usweight, 0.01)), \n                           quantile(usweight, 0.99)))\nsummary(analytic.data$usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.055   1.335   1.629   2.020   2.140  10.912\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight_t = pmin(pmax(sweight, quantile(sweight, 0.01)), \n                          quantile(sweight, 0.99)))\nsummary(analytic.data$sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4861  0.7035  0.8400  1.0044  1.0833  4.4901\n\nStep 3\nNow we will check the distribution of the covariates by the exposure status on the pseudo population in terms of pre-specified SMD.\n\n# Covariates\nvars &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated unstabilized weight\ndesign.unstab &lt;- svydesign(ids = ~ID, weights = ~usweight_t, data = analytic.data)\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight_t, data = analytic.data)\n\n# Balance checking with truncated unstabilized weight\ntab.unstab &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.unstab, test = F)\nprint(tab.unstab, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        6199.3          6559.7               \n#&gt;   gender = Male (%)        2999.3 (48.4)   3126.6 (47.7)   0.014\n#&gt;   age (mean (SD))           48.38 (17.38)   49.83 (18.11)  0.082\n#&gt;   race (%)                                                 0.047\n#&gt;      Black                 1304.7 (21.0)   1407.8 (21.5)        \n#&gt;      Hispanic              1901.9 (30.7)   1873.7 (28.6)        \n#&gt;      Other                  956.4 (15.4)   1029.3 (15.7)        \n#&gt;      White                 2036.2 (32.8)   2249.0 (34.3)        \n#&gt;   income (%)                                               0.033\n#&gt;      &lt;25k                  1664.3 (26.8)   1829.5 (27.9)        \n#&gt;      Between.25kto54k      1986.5 (32.0)   2133.5 (32.5)        \n#&gt;      Between.55kto99k      1449.1 (23.4)   1466.6 (22.4)        \n#&gt;      Over100k              1099.4 (17.7)   1130.0 (17.2)        \n#&gt;   education (%)                                            0.019\n#&gt;      College               3469.5 (56.0)   3610.4 (55.0)        \n#&gt;      High.School           2047.7 (33.0)   2217.9 (33.8)        \n#&gt;      School                 682.1 (11.0)    731.4 (11.1)        \n#&gt;   married (%)                                              0.040\n#&gt;      Married               3728.2 (60.1)   3853.6 (58.7)        \n#&gt;      Never.married         1101.9 (17.8)   1147.0 (17.5)        \n#&gt;      Previously.married    1369.2 (22.1)   1559.1 (23.8)        \n#&gt;   cholesterol (mean (SD))  181.58 (40.93)  183.13 (43.59)  0.037\n#&gt;   diastolicBP (mean (SD))   66.31 (14.64)   66.87 (14.78)  0.038\n#&gt;   systolicBP (mean (SD))   121.04 (16.61)  123.60 (23.62)  0.125\n\n# Balance checking with truncated stabilized weight\ntab.stab &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3665.9          2677.9               \n#&gt;   gender = Male (%)        1771.7 (48.3)   1276.5 (47.7)   0.013\n#&gt;   age (mean (SD))           48.40 (17.38)   49.84 (18.12)  0.081\n#&gt;   race (%)                                                 0.048\n#&gt;      Black                  772.6 (21.1)    575.0 (21.5)        \n#&gt;      Hispanic              1126.3 (30.7)    764.6 (28.6)        \n#&gt;      Other                  561.1 (15.3)    420.5 (15.7)        \n#&gt;      White                 1206.0 (32.9)    917.9 (34.3)        \n#&gt;   income (%)                                               0.033\n#&gt;      &lt;25k                   982.8 (26.8)    747.3 (27.9)        \n#&gt;      Between.25kto54k      1176.4 (32.1)    870.8 (32.5)        \n#&gt;      Between.55kto99k       857.6 (23.4)    598.5 (22.3)        \n#&gt;      Over100k               649.2 (17.7)    461.3 (17.2)        \n#&gt;   education (%)                                            0.019\n#&gt;      College               2051.4 (56.0)   1473.3 (55.0)        \n#&gt;      High.School           1210.5 (33.0)    905.9 (33.8)        \n#&gt;      School                 404.0 (11.0)    298.7 (11.2)        \n#&gt;   married (%)                                              0.040\n#&gt;      Married               2205.9 (60.2)   1572.9 (58.7)        \n#&gt;      Never.married          650.0 (17.7)    468.0 (17.5)        \n#&gt;      Previously.married     810.1 (22.1)    637.1 (23.8)        \n#&gt;   cholesterol (mean (SD))  181.62 (40.91)  183.15 (43.61)  0.036\n#&gt;   diastolicBP (mean (SD))   66.37 (14.53)   66.87 (14.81)  0.034\n#&gt;   systolicBP (mean (SD))   121.06 (16.58)  123.63 (23.65)  0.126\n\nAs we can see, all SMDs are less than our specified cut-point of 0.2, indicating that there is good covariate balancing. next, we will fit the outcome model on the pseudo population (i.e., weighted data). Note that we must utilize the survey feature as the design for the population-level estimate. For this step, we will multiply propensity score weight and survey weight and create a new weight variable.\nStep 4 - with unstabilized weight\n\nrequire(survey)\nrequire(jtools)\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * unstabilized weight \nanalytic.data$new.usweight_t &lt;- with(analytic.data, interview.weight * usweight_t)\nsummary(analytic.data$new.usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    6437   26567   42862   77768   88106 1831548\n\n#  New weight variable in the full dataset\ndat.full$new.usweight_t &lt;- 0\ndat.full$new.usweight_t[dat.full$ID %in% analytic.data$ID] &lt;- \n  analytic.data$new.usweight_t\nsummary(dat.full$new.usweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   24068   49261   55161 1831548\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.usweight_t, \n                      data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit &lt;- svyglm(out.formula, design = w.design.s, family = binomial(\"logit\"))\nsumm(fit, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.062\n\n\nPseudo-R² (McFadden)\n0.047\n\n\nAIC\n3371.907\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.164\n0.141\n0.190\n-24.213\n0.000\n\n\nbmiNot overweight\n0.280\n0.217\n0.361\n-9.814\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nStep 4 - with stabilized weight\nSimilarly, we can fit the outcome model with stabilized weights.\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight_t &lt;- with(analytic.data, interview.weight * sweight_t)\nsummary(analytic.data$new.sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2985   13311   22494   39174   43896  753678\n\n#  New weight variable in the full dataset\ndat.full$new.sweight_t &lt;- 0\ndat.full$new.sweight_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight_t\nsummary(dat.full$new.sweight_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12115   24814   28232  753678\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.055\n\n\nPseudo-R² (McFadden)\n0.041\n\n\nAIC\n3712.451\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.164\n0.141\n0.190\n-24.235\n0.000\n\n\nbmiNot overweight\n0.280\n0.217\n0.361\n-9.815\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\nlibrary(survey)\n# Outcome model with covariates adjustment\nfit.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.194\n\n\nPseudo-R² (McFadden)\n0.149\n\n\nAIC\n3331.238\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.045\n0.015\n0.133\n-5.608\nNA\n\n\nbmiNot overweight\n0.234\n0.175\n0.312\n-9.826\nNA\n\n\ngenderMale\n1.402\n1.190\n1.652\n4.038\nNA\n\n\nage\n1.050\n1.041\n1.058\n11.793\nNA\n\n\nraceHispanic\n0.790\n0.600\n1.040\n-1.677\nNA\n\n\nraceOther\n0.849\n0.428\n1.683\n-0.470\nNA\n\n\nraceWhite\n0.541\n0.371\n0.789\n-3.192\nNA\n\n\nincomeBetween.25kto54k\n0.723\n0.478\n1.093\n-1.540\nNA\n\n\nincomeBetween.55kto99k\n0.647\n0.411\n1.018\n-1.883\nNA\n\n\nincomeOver100k\n0.476\n0.309\n0.733\n-3.371\nNA\n\n\neducationHigh.School\n0.937\n0.725\n1.211\n-0.498\nNA\n\n\neducationSchool\n0.930\n0.568\n1.524\n-0.286\nNA\n\n\nmarriedNever.married\n0.895\n0.628\n1.275\n-0.615\nNA\n\n\nmarriedPreviously.married\n0.791\n0.529\n1.184\n-1.137\nNA\n\n\ncholesterol\n0.993\n0.990\n0.997\n-3.760\nNA\n\n\ndiastolicBP\n1.007\n0.999\n1.014\n1.744\nNA\n\n\nsystolicBP\n1.002\n0.993\n1.012\n0.529\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.095813   0.552073  -5.608 4.99e-05 ***\n#&gt; bmiNot overweight         -1.454514   0.148024  -9.826 6.29e-08 ***\n#&gt; genderMale                 0.338112   0.083733   4.038  0.00107 ** \n#&gt; age                        0.048468   0.004110  11.793 5.48e-09 ***\n#&gt; raceHispanic              -0.235285   0.140287  -1.677  0.11422    \n#&gt; raceOther                 -0.164132   0.349277  -0.470  0.64517    \n#&gt; raceWhite                 -0.613643   0.192233  -3.192  0.00606 ** \n#&gt; incomeBetween.25kto54k    -0.324911   0.211048  -1.540  0.14451    \n#&gt; incomeBetween.55kto99k    -0.435472   0.231288  -1.883  0.07927 .  \n#&gt; incomeOver100k            -0.742995   0.220399  -3.371  0.00420 ** \n#&gt; educationHigh.School      -0.065157   0.130721  -0.498  0.62540    \n#&gt; educationSchool           -0.072034   0.251806  -0.286  0.77874    \n#&gt; marriedNever.married      -0.110932   0.180496  -0.615  0.54803    \n#&gt; marriedPreviously.married -0.233875   0.205646  -1.137  0.27327    \n#&gt; cholesterol               -0.006873   0.001828  -3.760  0.00189 ** \n#&gt; diastolicBP                0.006728   0.003859   1.744  0.10169    \n#&gt; systolicBP                 0.002438   0.004605   0.529  0.60430    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9452878)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\nTip\n\n\n\nDouble adjustment:\nAs explained in the previous chapter, double adjustment should be applied thoughtfully, with careful consideration of model specification, covariate selection, and underlying assumptions to ensure valid and reliable results. Always consider the specific context of the study and consult statistical guidelines or experts when applying advanced methods like double adjustment in propensity score analysis.\n\n\nApproach by DuGoff et al. (2014)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula2 &lt;- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP + \n                           psu + strata + interview.weight)\n\nStep 2\n\n# Propensity scores\nps.fit2 &lt;- glm(ps.formula2, data = analytic.data, family = binomial(\"logit\"))\nanalytic.data$ps2 &lt;- fitted(ps.fit2)\n\n# Stabilized weight\nanalytic.data$sweight.dug &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps2, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps2)))\nsummary(analytic.data$sweight.dug)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4349  0.6942  0.8296  1.0307  1.0859 23.0226\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight.dug_t = pmin(pmax(sweight.dug, quantile(sweight.dug, 0.01)), \n                          quantile(sweight.dug, 0.99)))\nsummary(analytic.data$sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4802  0.6942  0.8296  1.0019  1.0859  4.4041\n\nStep 3\n\n# Balance checking\ncov2 &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n         \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight.dug_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab2 &lt;- svyCreateTableOne(vars = cov2, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab2, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3652.9          2675.0               \n#&gt;   gender = Male (%)        1766.5 (48.4)   1276.8 (47.7)   0.013\n#&gt;   age (mean (SD))           48.47 (17.42)   49.80 (17.99)  0.075\n#&gt;   race (%)                                                 0.056\n#&gt;      Black                  769.1 (21.1)    568.7 (21.3)        \n#&gt;      Hispanic              1121.5 (30.7)    755.4 (28.2)        \n#&gt;      Other                  553.0 (15.1)    417.0 (15.6)        \n#&gt;      White                 1209.4 (33.1)    933.9 (34.9)        \n#&gt;   income (%)                                               0.023\n#&gt;      &lt;25k                   982.1 (26.9)    734.0 (27.4)        \n#&gt;      Between.25kto54k      1172.1 (32.1)    871.1 (32.6)        \n#&gt;      Between.55kto99k       846.1 (23.2)    597.0 (22.3)        \n#&gt;      Over100k               652.6 (17.9)    472.9 (17.7)        \n#&gt;   education (%)                                            0.012\n#&gt;      College               2049.1 (56.1)   1484.2 (55.5)        \n#&gt;      High.School           1202.5 (32.9)    893.7 (33.4)        \n#&gt;      School                 401.3 (11.0)    297.1 (11.1)        \n#&gt;   married (%)                                              0.035\n#&gt;      Married               2201.3 (60.3)   1581.2 (59.1)        \n#&gt;      Never.married          647.4 (17.7)    465.6 (17.4)        \n#&gt;      Previously.married     804.2 (22.0)    628.2 (23.5)        \n#&gt;   cholesterol (mean (SD))  181.74 (40.94)  182.97 (43.25)  0.029\n#&gt;   diastolicBP (mean (SD))   66.46 (14.42)   66.88 (14.79)  0.029\n#&gt;   systolicBP (mean (SD))   121.14 (16.59)  123.39 (23.38)  0.111\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.dug_t &lt;- with(analytic.data, interview.weight * sweight.dug_t)\nsummary(analytic.data$new.sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2996   13343   22260   39436   43914  855144\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.dug_t &lt;- 0\ndat.full$new.sweight.dug_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight.dug_t\nsummary(dat.full$new.sweight.dug_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12058   24980   28225  855144\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.dug_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.dug &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.dug, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.060\n\n\nPseudo-R² (McFadden)\n0.045\n\n\nAIC\n3518.691\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.161\n0.140\n0.185\n-26.027\n0.000\n\n\nbmiNot overweight\n0.272\n0.203\n0.364\n-8.727\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit2.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.196\n\n\nPseudo-R² (McFadden)\n0.152\n\n\nAIC\n3154.781\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.043\n0.014\n0.131\n-5.537\nNA\n\n\nbmiNot overweight\n0.235\n0.169\n0.325\n-8.730\nNA\n\n\ngenderMale\n1.393\n1.150\n1.688\n3.381\nNA\n\n\nage\n1.049\n1.042\n1.056\n13.564\nNA\n\n\nraceHispanic\n0.813\n0.624\n1.061\n-1.524\nNA\n\n\nraceOther\n0.841\n0.454\n1.558\n-0.549\nNA\n\n\nraceWhite\n0.535\n0.369\n0.774\n-3.316\nNA\n\n\nincomeBetween.25kto54k\n0.720\n0.492\n1.053\n-1.693\nNA\n\n\nincomeBetween.55kto99k\n0.659\n0.440\n0.985\n-2.035\nNA\n\n\nincomeOver100k\n0.491\n0.338\n0.713\n-3.735\nNA\n\n\neducationHigh.School\n0.930\n0.753\n1.150\n-0.667\nNA\n\n\neducationSchool\n0.994\n0.625\n1.580\n-0.026\nNA\n\n\nmarriedNever.married\n0.982\n0.671\n1.439\n-0.092\nNA\n\n\nmarriedPreviously.married\n0.783\n0.534\n1.150\n-1.245\nNA\n\n\ncholesterol\n0.993\n0.989\n0.996\n-3.913\nNA\n\n\ndiastolicBP\n1.006\n0.998\n1.013\n1.426\nNA\n\n\nsystolicBP\n1.004\n0.995\n1.012\n0.853\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit2.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.143943   0.567807  -5.537 5.70e-05 ***\n#&gt; bmiNot overweight         -1.450047   0.166099  -8.730 2.89e-07 ***\n#&gt; genderMale                 0.331561   0.098056   3.381  0.00411 ** \n#&gt; age                        0.047790   0.003523  13.564 7.97e-10 ***\n#&gt; raceHispanic              -0.206657   0.135568  -1.524  0.14822    \n#&gt; raceOther                 -0.172590   0.314341  -0.549  0.59105    \n#&gt; raceWhite                 -0.625821   0.188746  -3.316  0.00471 ** \n#&gt; incomeBetween.25kto54k    -0.328863   0.194223  -1.693  0.11107    \n#&gt; incomeBetween.55kto99k    -0.417687   0.205223  -2.035  0.05989 .  \n#&gt; incomeOver100k            -0.711500   0.190509  -3.735  0.00199 ** \n#&gt; educationHigh.School      -0.072053   0.108040  -0.667  0.51496    \n#&gt; educationSchool           -0.006136   0.236588  -0.026  0.97965    \n#&gt; marriedNever.married      -0.017832   0.194660  -0.092  0.92822    \n#&gt; marriedPreviously.married -0.244058   0.196041  -1.245  0.23226    \n#&gt; cholesterol               -0.007067   0.001806  -3.913  0.00138 ** \n#&gt; diastolicBP                0.005516   0.003869   1.426  0.17440    \n#&gt; systolicBP                 0.003739   0.004381   0.853  0.40682    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9373219)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\nApproach by Austin et al. (2018)\nStep 1\n\n# Specify the PS model to estimate propensity scores\nps.formula3 &lt;- as.formula(I(bmi == \"Not overweight\") ~ gender + age + race + income + education + \n                           married + cholesterol + diastolicBP + systolicBP)\n\n# Survey design\nrequire(survey)\nanalytic.design &lt;- svydesign(id = ~psu, weights = ~interview.weight, strata = ~strata,\n                             data = analytic.data, nest = TRUE)\n\nStep 2\n\n# Propensity scores\nps.fit3 &lt;- svyglm(ps.formula3, design = analytic.design, family = binomial(\"logit\"))\nanalytic.data$ps3 &lt;- fitted(ps.fit3)\n\n# Stabilized weight\nanalytic.data$sweight.aus &lt;- with(analytic.data, ifelse(I(bmi==\"Not overweight\"), \n                                                    mean(I(bmi==\"Not overweight\"))/ps3, \n                                                    (1-mean(I(bmi==\"Not overweight\")))/(1-ps3)))\nsummary(analytic.data$sweight.aus)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.4436  0.7138  0.8410  1.0392  1.0645 26.2776\n\n# Truncating stabilized weight\nanalytic.data &lt;- analytic.data %&gt;% \n  mutate(sweight.aus_t = pmin(pmax(sweight.aus, quantile(sweight.aus, 0.01)), \n                          quantile(sweight.aus, 0.99)))\nsummary(analytic.data$sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.5198  0.7138  0.8410  1.0071  1.0645  4.8830\n\nStep 3\n\n# Balance checking\nvars &lt;- c(\"gender\", \"age\", \"race\", \"income\", \"education\", \"married\", \"cholesterol\", \n          \"diastolicBP\", \"systolicBP\")\n\n# Design with truncated stabilized weight\ndesign.stab &lt;- svydesign(ids = ~ID, weights = ~sweight.aus_t, data = analytic.data)\n\n# Balance checking with truncated stabilized weight\ntab.stab.aus &lt;- svyCreateTableOne(vars = vars, strata = \"bmi\", data = design.stab, test = F)\nprint(tab.stab.aus, smd = T)\n#&gt;                          Stratified by bmi\n#&gt;                           Overweight      Not overweight  SMD   \n#&gt;   n                        3434.3          2926.8               \n#&gt;   gender = Male (%)        1609.8 (46.9)   1458.6 (49.8)   0.059\n#&gt;   age (mean (SD))           48.51 (17.32)   49.95 (18.14)  0.081\n#&gt;   race (%)                                                 0.099\n#&gt;      Black                  737.1 (21.5)    623.8 (21.3)        \n#&gt;      Hispanic              1085.0 (31.6)    826.0 (28.2)        \n#&gt;      Other                  471.2 (13.7)    489.7 (16.7)        \n#&gt;      White                 1141.0 (33.2)    987.3 (33.7)        \n#&gt;   income (%)                                               0.041\n#&gt;      &lt;25k                   933.7 (27.2)    813.0 (27.8)        \n#&gt;      Between.25kto54k      1108.2 (32.3)    946.8 (32.3)        \n#&gt;      Between.55kto99k       776.7 (22.6)    685.4 (23.4)        \n#&gt;      Over100k               615.7 (17.9)    481.6 (16.5)        \n#&gt;   education (%)                                            0.039\n#&gt;      College               1908.2 (55.6)   1601.1 (54.7)        \n#&gt;      High.School           1129.9 (32.9)   1011.4 (34.6)        \n#&gt;      School                 396.2 (11.5)    314.3 (10.7)        \n#&gt;   married (%)                                              0.030\n#&gt;      Married               2063.9 (60.1)   1737.6 (59.4)        \n#&gt;      Never.married          605.8 (17.6)    501.4 (17.1)        \n#&gt;      Previously.married     764.6 (22.3)    687.8 (23.5)        \n#&gt;   cholesterol (mean (SD))  182.67 (41.11)  182.70 (43.38)  0.001\n#&gt;   diastolicBP (mean (SD))   66.76 (14.28)   66.85 (14.85)  0.006\n#&gt;   systolicBP (mean (SD))   121.39 (16.74)  123.98 (23.72)  0.126\n\nAll SMDs are less than our specified cut-point of 0.2.\nStep 4\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% analytic.data$ID] &lt;- 1\n\n# New weight = interview weight * stabilized weight\nanalytic.data$new.sweight.aus_t &lt;- with(analytic.data, interview.weight * sweight.aus_t)\nsummary(analytic.data$new.sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    3255   13604   22527   38953   43663  819622\n\n#  New weight variable in the full dataset\ndat.full$new.sweight.aus_t &lt;- 0\ndat.full$new.sweight.aus_t[dat.full$ID %in% analytic.data$ID] &lt;- analytic.data$new.sweight.aus_t\nsummary(dat.full$new.sweight.aus_t)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0   12356   24675   28169  819622\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~psu, strata = ~strata, weights = ~new.sweight.aus_t, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design for analytic sample\nw.design.s2 &lt;- subset(w.design0, ind == 1)\n\n# Outcome model\nout.formula &lt;- as.formula(I(diabetes == \"Yes\") ~ bmi)\nfit.stab.aus &lt;- svyglm(out.formula, design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit.stab.aus, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.055\n\n\nPseudo-R² (McFadden)\n0.041\n\n\nAIC\n3622.951\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.162\n0.140\n0.187\n-24.912\n0.000\n\n\nbmiNot overweight\n0.291\n0.225\n0.377\n-9.397\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit3.DA &lt;- svyglm(I(diabetes == \"Yes\") ~ bmi + gender + age + race + income + \n                 education + married + cholesterol + diastolicBP + systolicBP, \n               design = w.design.s2, family = binomial(\"logit\"))\nsumm(fit3.DA, exp = TRUE, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n6316\n\n\nDependent variable\nI(diabetes == \"Yes\")\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.193\n\n\nPseudo-R² (McFadden)\n0.149\n\n\nAIC\n3247.730\n\n\n\n \n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n0.045\n0.015\n0.129\n-5.735\nNA\n\n\nbmiNot overweight\n0.233\n0.175\n0.311\n-9.886\nNA\n\n\ngenderMale\n1.416\n1.186\n1.690\n3.850\nNA\n\n\nage\n1.049\n1.041\n1.057\n12.066\nNA\n\n\nraceHispanic\n0.773\n0.585\n1.022\n-1.808\nNA\n\n\nraceOther\n0.867\n0.451\n1.666\n-0.428\nNA\n\n\nraceWhite\n0.520\n0.353\n0.765\n-3.316\nNA\n\n\nincomeBetween.25kto54k\n0.700\n0.461\n1.062\n-1.677\nNA\n\n\nincomeBetween.55kto99k\n0.650\n0.407\n1.039\n-1.801\nNA\n\n\nincomeOver100k\n0.467\n0.303\n0.721\n-3.437\nNA\n\n\neducationHigh.School\n0.948\n0.744\n1.209\n-0.428\nNA\n\n\neducationSchool\n0.976\n0.598\n1.592\n-0.099\nNA\n\n\nmarriedNever.married\n0.896\n0.629\n1.276\n-0.609\nNA\n\n\nmarriedPreviously.married\n0.784\n0.525\n1.169\n-1.195\nNA\n\n\ncholesterol\n0.993\n0.990\n0.997\n-3.653\nNA\n\n\ndiastolicBP\n1.005\n0.998\n1.013\n1.412\nNA\n\n\nsystolicBP\n1.004\n0.995\n1.012\n0.826\nNA\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Log odds ratio with p-values\nsummary(fit3.DA, df.resid = degf(w.design.s2))\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = I(diabetes == \"Yes\") ~ bmi + gender + age + \n#&gt;     race + income + education + married + cholesterol + diastolicBP + \n#&gt;     systolicBP, design = w.design.s2, family = binomial(\"logit\"))\n#&gt; \n#&gt; Survey design:\n#&gt; subset(w.design0, ind == 1)\n#&gt; \n#&gt; Coefficients:\n#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               -3.109181   0.542121  -5.735 3.94e-05 ***\n#&gt; bmiNot overweight         -1.456869   0.147369  -9.886 5.81e-08 ***\n#&gt; genderMale                 0.347655   0.090292   3.850  0.00157 ** \n#&gt; age                        0.047457   0.003933  12.066 4.01e-09 ***\n#&gt; raceHispanic              -0.257364   0.142343  -1.808  0.09069 .  \n#&gt; raceOther                 -0.142755   0.333175  -0.428  0.67440    \n#&gt; raceWhite                 -0.653902   0.197190  -3.316  0.00470 ** \n#&gt; incomeBetween.25kto54k    -0.357122   0.212996  -1.677  0.11432    \n#&gt; incomeBetween.55kto99k    -0.430289   0.238964  -1.801  0.09190 .  \n#&gt; incomeOver100k            -0.761311   0.221498  -3.437  0.00367 ** \n#&gt; educationHigh.School      -0.052990   0.123816  -0.428  0.67475    \n#&gt; educationSchool           -0.024734   0.249884  -0.099  0.92246    \n#&gt; marriedNever.married      -0.109989   0.180532  -0.609  0.55148    \n#&gt; marriedPreviously.married -0.243725   0.203990  -1.195  0.25072    \n#&gt; cholesterol               -0.006605   0.001808  -3.653  0.00235 ** \n#&gt; diastolicBP                0.005253   0.003721   1.412  0.17842    \n#&gt; systolicBP                 0.003618   0.004380   0.826  0.42172    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Zero or negative residual df; p-values not defined\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 0.9416613)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\nReferences\n\n\n\n\nAustin, Peter C, Nathaniel Jembere, and Maria Chiu. 2018. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57.\n\n\nCole, Stephen R, and Miguel A Hernán. 2008. “Constructing Inverse Probability Weights for Marginal Structural Models.” American Journal of Epidemiology 168 (6): 656–64.\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.\n\n\nHernán, Miguel A, and James M Robins. 2006. “Estimating Causal Effects from Epidemiological Data.” Journal of Epidemiology and Community Health 60 (7): 578.\n\n\nZanutto, Elaine L. 2006. “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data.” Journal of Data Science 4 (1): 67–91.",
    "crumbs": [
      "Propensity score",
      "PS Weighting (US)"
    ]
  },
  {
    "objectID": "propensityscore7.html",
    "href": "propensityscore7.html",
    "title": "PSM with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score matching (PSM) with multiple imputation, focusing on specific subpopulations defined by the study’s eligibility criteria. We will use PSM as per DuGoff, Schuler, and Stuart (2014) recommendation, with SMD cut-point 0.2 and adjust for imbalanced and/or all covariates in the outcome model, if any.\nThe modified dataset from NHANES 2017- 2018, which was also used in missing data subpopulations chapter, will be used. This example aims to demonstrate how to do the missing data analysis using multiple imputation with PSM in the context of complex surveys.\nPre-processing\nLoad data\nLet us import the dataset:\n\nload(\"Data/missingdata/MIexample.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\nVariables\nThe dataset (dat.full) contains 9,254 subjects with 15 variables:\nSurvey information\n\n\nstudyid: Respondent sequence number\n\nsurvey.weight: Full sample 2 year interview weight\n\npsu: Masked pseudo PSU\n\nstrata: Masked pseudo strata\n\nOutcome variable\n\n\ncvd: Whether having cardiovascular disease\n\nExposure variable\n\n\nrheumatoid: Whether having rheumatoid arthritis\n\nCovariates\n\n\nage: age in years at screening\nsex\neducation\n\nrace: Race/Ethnicity\n\nincome: Family income in $\n\nbmi: Body Mass Index in kg/m\\(^2\\)\n\n\nsmoking: Smoking status\n\nhtn: Having hypertension\n\ndiabetes: Having diabetes\nData pre-processng\n\n# Categorical age\ndat.full$age.cat &lt;- with(dat.full, ifelse(age &gt;= 20 & age &lt; 50, \"20-49\", \n                                  ifelse(age &gt;= 50 & age &lt; 65, \"50-64\", \"65+\")))\ndat.full$age.cat &lt;- factor(dat.full$age.cat, levels = c(\"20-49\", \"50-64\", \"65+\"))\ntable(dat.full$age.cat, useNA = \"always\")\n#&gt; \n#&gt; 20-49 50-64   65+  &lt;NA&gt; \n#&gt;  2500  1569  5185     0\n\n# Recode rheumatoid to arthritis\ndat.full$arthritis &lt;- car::recode(dat.full$rheumatoid, \" 'No' = 'No arthritis';\n                                      'Yes' = 'Rheumatoid arthritis' \", as.factor = T)\ntable(dat.full$arthritis, useNA = \"always\")\n#&gt; \n#&gt;         No arthritis Rheumatoid arthritis                 &lt;NA&gt; \n#&gt;                 3857                  337                 5060\n\nSubsetting according to eligibility\nWe will create the analytic dataset with\n\nadults aged 20 years or more\nwithout missing values in outcome (cvd) or exposure (rheumatoid arthritis).\n\n\n# Drop &lt; 20 years\ndat.with.miss &lt;- subset(dat.full, age &gt;= 20)\n\n# Frequency for outcome and exposure \ntable(dat.with.miss$cvd, useNA = \"always\") # 6 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 4872  691    6\ntable(dat.with.miss$rheumatoid, useNA = \"always\") # 1375 missing\n#&gt; \n#&gt;   No  Yes &lt;NA&gt; \n#&gt; 3857  337 1375\n\n# Drop missing in outcome and exposure - dataset with missing values only in covariates\ndat.analytic &lt;- dat.with.miss[complete.cases(dat.with.miss$cvd),]\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$rheumatoid),]\nnrow(dat.analytic)\n#&gt; [1] 4191\n\nWe have 4,191 participants in our analytic dataset. The general strategy of solution to implement PSM with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects, and\nApply PSM on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin’s rule\nvariable summary\nLet us see the summary statistics as we did in the missing data analysis:\n\n# Keep only relevant variables\nvars &lt;-  c(\"studyid\", \"survey.weight\", \"psu\", \"strata\", \"cvd\", \"arthritis\", \"age.cat\", \n           \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \"htn\", \"diabetes\")\ndat.analytic2 &lt;- dat.analytic[, vars]\n\n# Create Table 1\nvars &lt;- c(\"arthritis\", \"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\",\n          \"htn\", \"diabetes\")\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"cvd\", data = dat.analytic2, includeNA = F,\n                       addOverall = T, test = F)\nprint(tab1, format = \"f\", showAllLevels = T)\n#&gt;                  Stratified by cvd\n#&gt;                   level                     Overall      No          \n#&gt;   n                                          4191         3823       \n#&gt;   arthritis       No arthritis               3854         3580       \n#&gt;                   Rheumatoid arthritis        337          243       \n#&gt;   age.cat         20-49                      2280         2240       \n#&gt;                   50-64                      1097          979       \n#&gt;                   65+                         814          604       \n#&gt;   sex             Male                       2126         1884       \n#&gt;                   Female                     2065         1939       \n#&gt;   education       Less than high school       828          728       \n#&gt;                   High school                2292         2094       \n#&gt;                   College graduate or above  1063          993       \n#&gt;   race            White                      1275         1113       \n#&gt;                   Black                       998          898       \n#&gt;                   Hispanic                   1015          958       \n#&gt;                   Others                      903          854       \n#&gt;   income          less than $20,000           659          557       \n#&gt;                   $20,000 to $74,999         1967         1796       \n#&gt;                   $75,000 and Over           1143         1079       \n#&gt;   bmi (mean (SD))                           29.28 (7.19) 29.20 (7.18)\n#&gt;   smoking         Never smoker               2570         2427       \n#&gt;                   Previous smoker             882          726       \n#&gt;                   Current smoker              739          670       \n#&gt;   htn             No                         1424         1380       \n#&gt;                   Yes                        2415         2107       \n#&gt;   diabetes        No                         3622         3396       \n#&gt;                   Yes                         566          424       \n#&gt;                  Stratified by cvd\n#&gt;                   Yes         \n#&gt;   n                 368       \n#&gt;   arthritis         274       \n#&gt;                      94       \n#&gt;   age.cat            40       \n#&gt;                     118       \n#&gt;                     210       \n#&gt;   sex               242       \n#&gt;                     126       \n#&gt;   education         100       \n#&gt;                     198       \n#&gt;                      70       \n#&gt;   race              162       \n#&gt;                     100       \n#&gt;                      57       \n#&gt;                      49       \n#&gt;   income            102       \n#&gt;                     171       \n#&gt;                      64       \n#&gt;   bmi (mean (SD)) 30.09 (7.29)\n#&gt;   smoking           143       \n#&gt;                     156       \n#&gt;                      69       \n#&gt;   htn                44       \n#&gt;                     308       \n#&gt;   diabetes          226       \n#&gt;                     142\n\n\n# missingness\nDataExplorer::plot_missing(dat.analytic2)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nDealing with missing values in covariates\nSimilar to the previous exercise, we will create 5 imputed datasets with 3 iterations, and the predictive mean matching method for bmi and income. We will use the strata variable as an auxiliary variable in the imputation model but not the survey weight or PSU variable.\nStep 0: Set up the imputation model\n\n# Step 0: Set imputation model\nini &lt;- mice(data = dat.analytic2, maxit = 0, print = FALSE)\npred &lt;- ini$pred\n\n# Use the strata variable as an auxiliary variable in the imputation model\npred[\"strata\",] &lt;- 0\n\n# Do not use survey weight or PSU variable as auxiliary variables\npred[,\"studyid\"] &lt;- pred[\"studyid\",] &lt;- 0\npred[,\"psu\"] &lt;- pred[\"psu\",] &lt;- 0\npred[,\"survey.weight\"] &lt;- pred[\"survey.weight\",] &lt;- 0\n\n# Set imputation method\nmeth &lt;- ini$meth\nmeth[\"bmi\"] &lt;- \"pmm\"\nmeth[\"income\"] &lt;- \"pmm\"\nmeth\n#&gt;       studyid survey.weight           psu        strata           cvd \n#&gt;            \"\"            \"\"            \"\"            \"\"            \"\" \n#&gt;     arthritis       age.cat           sex     education          race \n#&gt;            \"\"            \"\"            \"\"     \"polyreg\"            \"\" \n#&gt;        income           bmi       smoking           htn      diabetes \n#&gt;         \"pmm\"         \"pmm\"            \"\"      \"logreg\"      \"logreg\"\n\nStep 1: Imputing missing values using mice for eligible subjects\n\n# Step 1: impute the incomplete data\nimputation &lt;- mice(data = dat.analytic2,\n                   seed = 123,\n                   predictorMatrix = pred,\n                   method = meth,\n                   m = 5,\n                   maxit = 3,\n                   print = FALSE)\n\nLet us save the datasets.\n\nsave(dat.full, dat.analytic, dat.analytic2, imputation, \n     file = \"Data/propensityscore/analytic_imputed.RData\")\n\nNow we will combine m = 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\nimpdata &lt;- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#&gt; [1] 20955    17\n\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id &lt;- NULL\n\n# Number of subjects\nnrow(impdata)\n#&gt; [1] 20955\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n\n\n\n\n\n\n\nThere is no missing value after imputation. There is an additional variable (.imp) in the imputed dataset, which goes from 1 to m = 5, indicating the first to the fifth imputed datasets.\nStep 2: PSM steps 1-3 by DuGoff et al. (2014)\nOur next step is to use steps 1-3 of the PSM analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Match an exposed subject without replacement within the caliper of 0.2 times the standard deviation of the logit of PS.\nStep 2.3: Balance checking using SMD. Consider SMD &lt;0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and matching for each imputed dataset\n\n# Null vector or list to store values\ncaliper &lt;- NULL\ndat.matched &lt;- match.obj &lt;- list(NULL)\n\nm &lt;- 5 # 5 imputed dataset\n\n# PSM on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed &lt;- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps &lt;- fitted(ps.fit)\n  \n  # Caliper fixing to 0.2*sd(logit of PS)\n  caliper[ii] &lt;- 0.2*sd(log(dat.imputed$ps/(1-dat.imputed$ps)))\n  \n  # 1:1 PS matching  \n  set.seed(504)\n  match.obj[[ii]] &lt;- matchit(ps.formula, data = dat.imputed,\n                        distance = dat.imputed$ps, \n                        method = \"nearest\", \n                        replace = FALSE,\n                        caliper = caliper[ii], \n                        ratio = 1)\n  dat.imputed$ps &lt;- match.obj[[ii]]$distance\n  \n  # Extract matched data\n  dat.matched[[ii]] &lt;- match.data(match.obj[[ii]]) \n}\nmatch.obj\n#&gt; [[1]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 668 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[2]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.02)\n#&gt;  - number of obs.: 4191 (original), 666 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[3]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 672 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[4]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 664 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n#&gt; \n#&gt; [[5]]\n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.021)\n#&gt;  - number of obs.: 4191 (original), 662 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: age.cat, sex, education, race, income, bmi, smoking, htn, diabetes, psu, strata, survey.weight\n\n\n# Dimension of each of the matched dataset\nlapply(dat.matched, dim)\n#&gt; [[1]]\n#&gt; [1] 668  20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 666  20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 672  20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 664  20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 662  20\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m &lt;- list(NULL)\nfor (ii in 1:m) {\n  # Matched data\n  dat &lt;- dat.matched[[ii]]\n  \n  # Covariates\n  vars &lt;- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Balance checking \n  tab1m[[ii]] &lt;- CreateTableOne(strata = \"arthritis\", vars = vars, data = dat, test = F)\n}\nprint(tab1m, smd = TRUE)\n#&gt; [[1]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              334           334                      \n#&gt;   age.cat (%)                                                      0.018\n#&gt;      20-49                        51 (15.3)     52 (15.6)               \n#&gt;      50-64                       131 (39.2)    133 (39.8)               \n#&gt;      65+                         152 (45.5)    149 (44.6)               \n#&gt;   sex = Female (%)               172 (51.5)    178 (53.3)          0.036\n#&gt;   education (%)                                                    0.034\n#&gt;      Less than high school        80 (24.0)     84 (25.1)               \n#&gt;      High school                 204 (61.1)    203 (60.8)               \n#&gt;      College graduate or above    50 (15.0)     47 (14.1)               \n#&gt;   race (%)                                                         0.049\n#&gt;      White                       105 (31.4)    103 (30.8)               \n#&gt;      Black                       107 (32.0)    112 (33.5)               \n#&gt;      Hispanic                     67 (20.1)     69 (20.7)               \n#&gt;      Others                       55 (16.5)     50 (15.0)               \n#&gt;   income (%)                                                       0.073\n#&gt;      less than $20,000            95 (28.4)    100 (29.9)               \n#&gt;      $20,000 to $74,999          162 (48.5)    167 (50.0)               \n#&gt;      $75,000 and Over             77 (23.1)     67 (20.1)               \n#&gt;   bmi (mean (SD))              30.62 (8.16)  30.54 (7.34)          0.011\n#&gt;   smoking (%)                                                      0.052\n#&gt;      Never smoker                158 (47.3)    161 (48.2)               \n#&gt;      Previous smoker             102 (30.5)    106 (31.7)               \n#&gt;      Current smoker               74 (22.2)     67 (20.1)               \n#&gt;   htn = Yes (%)                  283 (84.7)    279 (83.5)          0.033\n#&gt;   diabetes = Yes (%)             106 (31.7)    100 (29.9)          0.039\n#&gt; \n#&gt; [[2]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              333           333                      \n#&gt;   age.cat (%)                                                      0.055\n#&gt;      20-49                        50 (15.0)     52 (15.6)               \n#&gt;      50-64                       142 (42.6)    133 (39.9)               \n#&gt;      65+                         141 (42.3)    148 (44.4)               \n#&gt;   sex = Female (%)               172 (51.7)    176 (52.9)          0.024\n#&gt;   education (%)                                                    0.068\n#&gt;      Less than high school        92 (27.6)     84 (25.2)               \n#&gt;      High school                 191 (57.4)    202 (60.7)               \n#&gt;      College graduate or above    50 (15.0)     47 (14.1)               \n#&gt;   race (%)                                                         0.146\n#&gt;      White                        91 (27.3)    104 (31.2)               \n#&gt;      Black                       100 (30.0)    110 (33.0)               \n#&gt;      Hispanic                     79 (23.7)     69 (20.7)               \n#&gt;      Others                       63 (18.9)     50 (15.0)               \n#&gt;   income (%)                                                       0.131\n#&gt;      less than $20,000           115 (34.5)     96 (28.8)               \n#&gt;      $20,000 to $74,999          166 (49.8)    175 (52.6)               \n#&gt;      $75,000 and Over             52 (15.6)     62 (18.6)               \n#&gt;   bmi (mean (SD))              30.50 (7.88)  30.51 (7.46)          0.001\n#&gt;   smoking (%)                                                      0.015\n#&gt;      Never smoker                160 (48.0)    161 (48.3)               \n#&gt;      Previous smoker             104 (31.2)    105 (31.5)               \n#&gt;      Current smoker               69 (20.7)     67 (20.1)               \n#&gt;   htn = Yes (%)                  288 (86.5)    278 (83.5)          0.084\n#&gt;   diabetes = Yes (%)              97 (29.1)     99 (29.7)          0.013\n#&gt; \n#&gt; [[3]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              336           336                      \n#&gt;   age.cat (%)                                                      0.077\n#&gt;      20-49                        46 (13.7)     52 (15.5)               \n#&gt;      50-64                       145 (43.2)    133 (39.6)               \n#&gt;      65+                         145 (43.2)    151 (44.9)               \n#&gt;   sex = Female (%)               176 (52.4)    179 (53.3)          0.018\n#&gt;   education (%)                                                    0.076\n#&gt;      Less than high school        80 (23.8)     86 (25.6)               \n#&gt;      High school                 215 (64.0)    203 (60.4)               \n#&gt;      College graduate or above    41 (12.2)     47 (14.0)               \n#&gt;   race (%)                                                         0.084\n#&gt;      White                       114 (33.9)    105 (31.2)               \n#&gt;      Black                       114 (33.9)    112 (33.3)               \n#&gt;      Hispanic                     59 (17.6)     69 (20.5)               \n#&gt;      Others                       49 (14.6)     50 (14.9)               \n#&gt;   income (%)                                                       0.118\n#&gt;      less than $20,000            87 (25.9)    104 (31.0)               \n#&gt;      $20,000 to $74,999          183 (54.5)    166 (49.4)               \n#&gt;      $75,000 and Over             66 (19.6)     66 (19.6)               \n#&gt;   bmi (mean (SD))              30.56 (7.71)  30.54 (7.53)          0.003\n#&gt;   smoking (%)                                                      0.127\n#&gt;      Never smoker                180 (53.6)    161 (47.9)               \n#&gt;      Previous smoker              89 (26.5)    107 (31.8)               \n#&gt;      Current smoker               67 (19.9)     68 (20.2)               \n#&gt;   htn = Yes (%)                  280 (83.3)    281 (83.6)          0.008\n#&gt;   diabetes = Yes (%)             104 (31.0)    102 (30.4)          0.013\n#&gt; \n#&gt; [[4]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              332           332                      \n#&gt;   age.cat (%)                                                      0.062\n#&gt;      20-49                        48 (14.5)     52 (15.7)               \n#&gt;      50-64                       127 (38.3)    133 (40.1)               \n#&gt;      65+                         157 (47.3)    147 (44.3)               \n#&gt;   sex = Female (%)               174 (52.4)    175 (52.7)          0.006\n#&gt;   education (%)                                                    0.059\n#&gt;      Less than high school        79 (23.8)     85 (25.6)               \n#&gt;      High school                 200 (60.2)    200 (60.2)               \n#&gt;      College graduate or above    53 (16.0)     47 (14.2)               \n#&gt;   race (%)                                                         0.034\n#&gt;      White                       100 (30.1)    105 (31.6)               \n#&gt;      Black                       111 (33.4)    108 (32.5)               \n#&gt;      Hispanic                     71 (21.4)     69 (20.8)               \n#&gt;      Others                       50 (15.1)     50 (15.1)               \n#&gt;   income (%)                                                       0.067\n#&gt;      less than $20,000            93 (28.0)     95 (28.6)               \n#&gt;      $20,000 to $74,999          165 (49.7)    172 (51.8)               \n#&gt;      $75,000 and Over             74 (22.3)     65 (19.6)               \n#&gt;   bmi (mean (SD))              30.06 (7.85)  30.46 (7.49)          0.052\n#&gt;   smoking (%)                                                      0.053\n#&gt;      Never smoker                166 (50.0)    161 (48.5)               \n#&gt;      Previous smoker              97 (29.2)    105 (31.6)               \n#&gt;      Current smoker               69 (20.8)     66 (19.9)               \n#&gt;   htn = Yes (%)                  281 (84.6)    278 (83.7)          0.025\n#&gt;   diabetes = Yes (%)              93 (28.0)     98 (29.5)          0.033\n#&gt; \n#&gt; [[5]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis  Rheumatoid arthritis SMD   \n#&gt;   n                              331           331                      \n#&gt;   age.cat (%)                                                      0.026\n#&gt;      20-49                        50 (15.1)     52 (15.7)               \n#&gt;      50-64                       137 (41.4)    133 (40.2)               \n#&gt;      65+                         144 (43.5)    146 (44.1)               \n#&gt;   sex = Female (%)               172 (52.0)    175 (52.9)          0.018\n#&gt;   education (%)                                                    0.025\n#&gt;      Less than high school        86 (26.0)     84 (25.4)               \n#&gt;      High school                 196 (59.2)    200 (60.4)               \n#&gt;      College graduate or above    49 (14.8)     47 (14.2)               \n#&gt;   race (%)                                                         0.026\n#&gt;      White                        99 (29.9)    103 (31.1)               \n#&gt;      Black                       111 (33.5)    109 (32.9)               \n#&gt;      Hispanic                     70 (21.1)     69 (20.8)               \n#&gt;      Others                       51 (15.4)     50 (15.1)               \n#&gt;   income (%)                                                       0.074\n#&gt;      less than $20,000            84 (25.4)     94 (28.4)               \n#&gt;      $20,000 to $74,999          181 (54.7)    170 (51.4)               \n#&gt;      $75,000 and Over             66 (19.9)     67 (20.2)               \n#&gt;   bmi (mean (SD))              30.17 (7.58)  30.49 (7.86)          0.042\n#&gt;   smoking (%)                                                      0.027\n#&gt;      Never smoker                165 (49.8)    161 (48.6)               \n#&gt;      Previous smoker             104 (31.4)    105 (31.7)               \n#&gt;      Current smoker               62 (18.7)     65 (19.6)               \n#&gt;   htn = Yes (%)                  276 (83.4)    277 (83.7)          0.008\n#&gt;   diabetes = Yes (%)             102 (30.8)     97 (29.3)          0.033\n\nFor each of the datasets, all SMDs are less than our specified cut-point of 0.2.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Remember, we must utilize survey features to correctly estimate the standard error.\n3.1 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction and unmatched) with the matched datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible &lt;- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat &lt;- dat.matched[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible &lt;- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] &lt;- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] &lt;- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp &lt;- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#&gt; [[1]]\n#&gt; [1] 8586   19\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 8588   19\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 8582   19\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 8590   19\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 8592   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.matched[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"arthritis\"     \"age.cat\"       \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \".imp\"         \n#&gt; [17] \"ps\"            \"distance\"      \"weights\"       \"subclass\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#&gt; [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\nFour variables (ps, distance, weights, and subclass) are unavailable in our full, analytic, or ineligible datasets but in the matched datasets. We need to create these 4 variables in the ineligible datasets.\n\ndat.ineligible2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  dat &lt;- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible &lt;- NULL\n  \n  # Create ps, distance, weights, and subclass\n  dat$ps &lt;- NA\n  dat$distance &lt;- NA\n  dat$weights &lt;- NA\n  dat$subclass &lt;- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars &lt;- names(dat.matched[[1]])\n  dat &lt;- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] &lt;- dat\n}\n\nWe created ps, distance, weights, and subclass with missing values for the ineligible participants. Note that it doesn’t matter whether there are missing covariate values for ineligible. Since we will create the design on the full dataset and subset the design for only eligible (i.e., matched participants), missing covariate values for ineligible will not impact our analysis.\n3.2 Combining eligible (matched) and ineligible (unimputed + unmatched) subjects\nNow, we will merge matched eligible and unimputed and unmatched ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 &lt;- data.frame(dat.matched[[ii]])\n  d1$eligible &lt;- 1\n  \n  # Ineligible\n  d2 &lt;- data.frame(dat.ineligible2[[ii]])\n  d2$eligible &lt;- 0\n  \n  # Full data\n  d3 &lt;- rbind(d1, d2)\n  dat.full2[[ii]] &lt;- d3\n}\nlapply(dat.full2, dim)\n#&gt; [[1]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 9254   21\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 9254   21\n\n# Stacked dataset\ndat.stacked &lt;- rbindlist(dat.full2)\ndim(dat.stacked)\n#&gt; [1] 46270    21\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset.\n\nallImputations &lt;- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 &lt;- svydesign(ids = ~psu, \n                       weights = ~survey.weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design &lt;- subset(w.design0, eligible == 1) \n#&gt; Warning in subset.svyimputationList(w.design0, eligible == 1): subset differed\n#&gt; between imputations\n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#&gt; [[1]]\n#&gt; [1] 668  21\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 666  21\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 672  21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 664  21\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 662  21\n\nNow we will run the design-adjusted logistic regression on and pool the estimate using Rubin’s rule:\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis, family = quasibinomial))\nres &lt;- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) &lt;- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#&gt;               (Intercept) arthritisRheumatoid arthritis\n#&gt; OR from m = 1        0.18                          1.53\n#&gt; OR from m = 2        0.24                          1.09\n#&gt; OR from m = 3        0.21                          1.28\n#&gt; OR from m = 4        0.17                          1.61\n#&gt; OR from m = 5        0.12                          2.32\n\nStep 3.5: Pooling estimates\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit2)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nReferences\n\n\n\n\nDuGoff, Eva H, Megan Schuler, and Elizabeth A Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” Health Services Research 49 (1): 284–303.",
    "crumbs": [
      "Propensity score",
      "PSM with MI in subset"
    ]
  },
  {
    "objectID": "propensityscore8.html",
    "href": "propensityscore8.html",
    "title": "PSW with MI in subset",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score (PS) weighting with multiple imputation (MI), focusing on specific subpopulations defined by the study’s eligibility criteria. Similar to the previous chapter on PSM with MI for subpopulation, the modified dataset from NHANES 2017- 2018, will be used.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/analytic_imputed.RData\")\nls()\n#&gt; [1] \"dat.analytic\"  \"dat.analytic2\" \"dat.full\"      \"imputation\"\n\n\n\ndat.full: Full dataset of 9,254 subjects\n\ndat.analytic and dat.analytic2: Analytic dataset of 4,191 participants with only missing in the covariates. There are no missing values for the exposure or outcomes.\n\nimputation: m = 5 imputed datasets from dat.analytic2 using MI.\n\nThe general strategy of solution to implement PS weighting with MI is as follows:\n\nWe will build the imputation model on 4,191 eligible subjects.\nApply PS weghting on each of the imputed datasets, where we will utilize survey features for population-level estimate\nPool the estimates using Rubin’s rule\nDealing with missing values in covariates\nStep 1: Imputing missing values using mice for eligible subjects\nWe already completed this step in the previous chapter, where we imputed m = 5 datasets using MI.\nNow we will combine 5 datasets and create a stacked dataset. This dataset should contain 5*4,191 = 20,955 rows.\n\n# Stacked imputed dataset\nimpdata &lt;- mice::complete(imputation, action=\"long\")\ndim(impdata)\n#&gt; [1] 20955    17\n\n#Remove .id variable from the model as it was created in an intermediate step\nimpdata$.id &lt;- NULL\n\n# Missing after imputation\nDataExplorer::plot_missing(impdata)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nStep 2: PS weighting steps 1-3 by DuGoff et al. (2014)\nOur next step is to use steps 1-3 of the PS weighting analysis:\n\nStep 2.1: Fit the PS model by considering survey features as covariates.\nStep 2.2: Calculate PS weights\nStep 2.3: Balance checking using SMD. Consider SMD &lt;0.2 as a good covariate balancing.\n\nStep 2.1: PS model specification\n\n# Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(arthritis == \"Rheumatoid arthritis\") ~ age.cat + sex + \n                           education + race + income + bmi + smoking + htn + \n                           diabetes + psu + strata + survey.weight)\n\nStep 2.2: Estimating PS and calculating weights\n\ndat.ps &lt;- list(NULL)\n\nm &lt;- 5 # 5 imputed dataset\n\n# PS weighting on each of the imputed datasets\nfor (ii in 1:m) {\n  # Imputed dataset\n  dat.imputed &lt;- subset(impdata, .imp == ii)\n  \n  # Propensity scores\n  ps.fit &lt;- glm(ps.formula, data = dat.imputed, family = binomial(\"logit\"))\n  dat.imputed$ps &lt;- fitted(ps.fit)\n  \n  # Stabilized weight\n  dat.imputed$sweight &lt;- with(dat.imputed, \n                              ifelse(I(arthritis == \"Rheumatoid arthritis\"), \n                                     mean(I(arthritis == \"Rheumatoid arthritis\"))/ps, \n                                     (1-mean(I(arthritis == \"Rheumatoid arthritis\")))/(1-ps)))\n\n  # Dataset\n  dat.ps[[ii]] &lt;- dat.imputed\n}\n\n# Weight summary\npurrr::map_df(dat.ps, function(df){summary(df$sweight)})\n\n\n  \n\n\n\n\n# Dimension of each of the imputed dataset\nlapply(dat.ps, dim)\n#&gt; [[1]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4191   18\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 4191   18\n\nStep 2.3: Balance checking for each imputed dataset\nNow we will check balance in terms of SMD on each dataset.\n\ntab1m &lt;- list(NULL)\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat &lt;- dat.ps[[ii]]\n  \n  # Covariates\n  vars &lt;- c(\"age.cat\", \"sex\", \"education\", \"race\", \"income\", \"bmi\", \"smoking\", \n            \"htn\", \"diabetes\")\n  \n  # Design with truncated stabilized weight\n  wdesign &lt;- svydesign(ids = ~studyid, weights = ~sweight, data = dat)\n  \n  # Balance checking \n  tab1m[[ii]] &lt;- svyCreateTableOne(vars = vars, strata = \"arthritis\", data = wdesign,\n                                   test = F)\n}\nprint(tab1m, smd = TRUE)\n#&gt; [[1]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.0          316.3                     \n#&gt;   age.cat (%)                                                        0.077\n#&gt;      20-49                      2096.4 (54.4)   159.8 (50.5)              \n#&gt;      50-64                      1010.5 (26.2)    89.2 (28.2)              \n#&gt;      65+                         749.1 (19.4)    67.3 (21.3)              \n#&gt;   sex = Female (%)              1900.0 (49.3)   144.1 (45.6)         0.075\n#&gt;   education (%)                                                      0.053\n#&gt;      Less than high school       764.6 (19.8)    60.9 (19.3)              \n#&gt;      High school                2109.7 (54.7)   180.9 (57.2)              \n#&gt;      College graduate or above   981.8 (25.5)    74.4 (23.5)              \n#&gt;   race (%)                                                           0.074\n#&gt;      White                      1173.6 (30.4)   107.0 (33.8)              \n#&gt;      Black                       917.9 (23.8)    70.1 (22.2)              \n#&gt;      Hispanic                    933.5 (24.2)    73.4 (23.2)              \n#&gt;      Others                      831.0 (21.6)    65.8 (20.8)              \n#&gt;   income (%)                                                         0.045\n#&gt;      less than $20,000           685.9 (17.8)    59.3 (18.7)              \n#&gt;      $20,000 to $74,999         2015.6 (52.3)   158.2 (50.0)              \n#&gt;      $75,000 and Over           1154.5 (29.9)    98.8 (31.3)              \n#&gt;   bmi (mean (SD))                29.34 (7.29)   29.93 (6.90)         0.082\n#&gt;   smoking (%)                                                        0.211\n#&gt;      Never smoker               2362.5 (61.3)   162.7 (51.4)              \n#&gt;      Previous smoker             814.5 (21.1)    91.9 (29.1)              \n#&gt;      Current smoker              679.1 (17.6)    61.6 (19.5)              \n#&gt;   htn = Yes (%)                 2403.5 (62.3)   211.4 (66.8)         0.094\n#&gt;   diabetes = Yes (%)             523.3 (13.6)    50.9 (16.1)         0.071\n#&gt; \n#&gt; [[2]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3855.7          317.6                     \n#&gt;   age.cat (%)                                                        0.079\n#&gt;      20-49                      2096.4 (54.4)   160.2 (50.4)              \n#&gt;      50-64                      1010.3 (26.2)    89.8 (28.3)              \n#&gt;      65+                         749.0 (19.4)    67.6 (21.3)              \n#&gt;   sex = Female (%)              1899.4 (49.3)   144.0 (45.3)         0.079\n#&gt;   education (%)                                                      0.050\n#&gt;      Less than high school       765.1 (19.8)    61.3 (19.3)              \n#&gt;      High school                2113.3 (54.8)   181.6 (57.2)              \n#&gt;      College graduate or above   977.2 (25.3)    74.7 (23.5)              \n#&gt;   race (%)                                                           0.076\n#&gt;      White                      1173.6 (30.4)   107.8 (33.9)              \n#&gt;      Black                       917.9 (23.8)    71.0 (22.4)              \n#&gt;      Hispanic                    933.2 (24.2)    72.0 (22.7)              \n#&gt;      Others                      831.0 (21.6)    66.8 (21.0)              \n#&gt;   income (%)                                                         0.039\n#&gt;      less than $20,000           688.0 (17.8)    57.8 (18.2)              \n#&gt;      $20,000 to $74,999         2015.9 (52.3)   160.1 (50.4)              \n#&gt;      $75,000 and Over           1151.8 (29.9)    99.7 (31.4)              \n#&gt;   bmi (mean (SD))                29.33 (7.22)   29.90 (6.95)         0.081\n#&gt;   smoking (%)                                                        0.213\n#&gt;      Never smoker               2362.4 (61.3)   162.6 (51.2)              \n#&gt;      Previous smoker             813.9 (21.1)    91.7 (28.9)              \n#&gt;      Current smoker              679.3 (17.6)    63.3 (19.9)              \n#&gt;   htn = Yes (%)                 2394.0 (62.1)   213.6 (67.3)         0.108\n#&gt;   diabetes = Yes (%)             522.8 (13.6)    50.3 (15.8)         0.065\n#&gt; \n#&gt; [[3]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.6          313.7                     \n#&gt;   age.cat (%)                                                        0.088\n#&gt;      20-49                      2096.4 (54.4)   156.9 (50.0)              \n#&gt;      50-64                      1010.1 (26.2)    88.9 (28.3)              \n#&gt;      65+                         750.1 (19.4)    67.9 (21.6)              \n#&gt;   sex = Female (%)              1900.1 (49.3)   142.2 (45.3)         0.079\n#&gt;   education (%)                                                      0.069\n#&gt;      Less than high school       765.0 (19.8)    57.8 (18.4)              \n#&gt;      High school                2112.6 (54.8)   182.5 (58.2)              \n#&gt;      College graduate or above   979.0 (25.4)    73.4 (23.4)              \n#&gt;   race (%)                                                           0.082\n#&gt;      White                      1174.1 (30.4)   106.8 (34.1)              \n#&gt;      Black                       918.2 (23.8)    69.9 (22.3)              \n#&gt;      Hispanic                    933.4 (24.2)    69.8 (22.2)              \n#&gt;      Others                      830.9 (21.5)    67.2 (21.4)              \n#&gt;   income (%)                                                         0.095\n#&gt;      less than $20,000           698.8 (18.1)    63.2 (20.1)              \n#&gt;      $20,000 to $74,999         2003.2 (51.9)   148.1 (47.2)              \n#&gt;      $75,000 and Over           1154.6 (29.9)   102.4 (32.7)              \n#&gt;   bmi (mean (SD))                29.29 (7.21)   29.80 (6.97)         0.072\n#&gt;   smoking (%)                                                        0.216\n#&gt;      Never smoker               2362.7 (61.3)   160.3 (51.1)              \n#&gt;      Previous smoker             814.6 (21.1)    91.0 (29.0)              \n#&gt;      Current smoker              679.2 (17.6)    62.5 (19.9)              \n#&gt;   htn = Yes (%)                 2398.6 (62.2)   206.3 (65.8)         0.074\n#&gt;   diabetes = Yes (%)             523.7 (13.6)    50.9 (16.2)         0.075\n#&gt; \n#&gt; [[4]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3856.2          312.9                     \n#&gt;   age.cat (%)                                                        0.098\n#&gt;      20-49                      2096.3 (54.4)   154.7 (49.5)              \n#&gt;      50-64                      1010.8 (26.2)    91.2 (29.1)              \n#&gt;      65+                         749.1 (19.4)    67.0 (21.4)              \n#&gt;   sex = Female (%)              1900.1 (49.3)   144.4 (46.2)         0.063\n#&gt;   education (%)                                                      0.064\n#&gt;      Less than high school       763.7 (19.8)    58.4 (18.7)              \n#&gt;      High school                2115.2 (54.9)   181.5 (58.0)              \n#&gt;      College graduate or above   977.2 (25.3)    72.9 (23.3)              \n#&gt;   race (%)                                                           0.078\n#&gt;      White                      1173.7 (30.4)   106.5 (34.0)              \n#&gt;      Black                       918.0 (23.8)    70.1 (22.4)              \n#&gt;      Hispanic                    933.5 (24.2)    71.2 (22.8)              \n#&gt;      Others                      831.0 (21.5)    65.0 (20.8)              \n#&gt;   income (%)                                                         0.096\n#&gt;      less than $20,000           694.1 (18.0)    63.9 (20.4)              \n#&gt;      $20,000 to $74,999         2009.3 (52.1)   148.3 (47.4)              \n#&gt;      $75,000 and Over           1152.9 (29.9)   100.7 (32.2)              \n#&gt;   bmi (mean (SD))                29.28 (7.21)   29.86 (7.13)         0.081\n#&gt;   smoking (%)                                                        0.209\n#&gt;      Never smoker               2362.7 (61.3)   161.3 (51.6)              \n#&gt;      Previous smoker             813.8 (21.1)    90.7 (29.0)              \n#&gt;      Current smoker              679.7 (17.6)    60.9 (19.5)              \n#&gt;   htn = Yes (%)                 2394.5 (62.1)   206.6 (66.0)         0.082\n#&gt;   diabetes = Yes (%)             523.0 (13.6)    50.2 (16.0)         0.069\n#&gt; \n#&gt; [[5]]\n#&gt;                               Stratified by arthritis\n#&gt;                                No arthritis    Rheumatoid arthritis SMD   \n#&gt;   n                             3855.7          316.8                     \n#&gt;   age.cat (%)                                                        0.080\n#&gt;      20-49                      2096.4 (54.4)   159.6 (50.4)              \n#&gt;      50-64                      1010.3 (26.2)    89.8 (28.4)              \n#&gt;      65+                         749.1 (19.4)    67.4 (21.3)              \n#&gt;   sex = Female (%)              1899.6 (49.3)   143.9 (45.4)         0.077\n#&gt;   education (%)                                                      0.060\n#&gt;      Less than high school       767.4 (19.9)    62.1 (19.6)              \n#&gt;      High school                2110.3 (54.7)   181.8 (57.4)              \n#&gt;      College graduate or above   978.0 (25.4)    72.9 (23.0)              \n#&gt;   race (%)                                                           0.074\n#&gt;      White                      1173.5 (30.4)   107.1 (33.8)              \n#&gt;      Black                       917.9 (23.8)    70.1 (22.1)              \n#&gt;      Hispanic                    933.4 (24.2)    73.1 (23.1)              \n#&gt;      Others                      830.9 (21.5)    66.5 (21.0)              \n#&gt;   income (%)                                                         0.047\n#&gt;      less than $20,000           685.0 (17.8)    60.6 (19.1)              \n#&gt;      $20,000 to $74,999         2022.6 (52.5)   159.2 (50.3)              \n#&gt;      $75,000 and Over           1148.1 (29.8)    97.0 (30.6)              \n#&gt;   bmi (mean (SD))                29.31 (7.20)   29.77 (7.04)         0.064\n#&gt;   smoking (%)                                                        0.206\n#&gt;      Never smoker               2362.7 (61.3)   163.1 (51.5)              \n#&gt;      Previous smoker             813.3 (21.1)    90.0 (28.4)              \n#&gt;      Current smoker              679.7 (17.6)    63.6 (20.1)              \n#&gt;   htn = Yes (%)                 2409.4 (62.5)   210.2 (66.3)         0.080\n#&gt;   diabetes = Yes (%)             522.5 (13.6)    50.3 (15.9)         0.066\n\nFor each of the datasets, all SMDs except for smoking are less than our specified cut-point of 0.2. We will adjust our outcome model for smoking.\nStep 3: Outcome modelling\nOur next step is to fit the outcome model on each of the imputed dataset. Note that, we must utilize survey features to correctly estimate the standard error. For this step, we will multiply PS weight and survey weight and create a new weight variable.\n3.1 Calculating new weights\n\ndat.ps2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # PS weighted imputed data\n  dat &lt;- dat.ps[[ii]]\n  \n  # New weight = survey weight * PS weight \n  dat$new_weight &lt;- with(dat, survey.weight * sweight)\n  \n  dat.ps2[[ii]] &lt;- dat\n}\n\n3.2 Preparing dataset for ineligible subjects\nNow we will add the ineligible subjects(ineligible by study restriction) with the PS weighted datasets, so that we can set up the survey design on the full dataset and then subset the design.\nLet us subset the data for ineligible subjects:\n\n# Subset for ineligible\ndat.ineligible &lt;- list(NULL)\n\nfor(ii in 1:m){\n  # Matched dataset\n  dat &lt;- dat.ps[[ii]]\n  \n  # Create an indicator variable in the full dataset\n  dat.full$ineligible &lt;- 1\n  dat.full$ineligible[dat.full$studyid %in% dat$studyid] &lt;- 0\n  \n  # Subset for ineligible\n  dat.ineligible[[ii]] &lt;- subset(dat.full, ineligible == 1)\n  \n  # Create the .imp variable on each dataset with .imp 1 to m = 5\n  dat.ineligible[[ii]]$.imp &lt;- ii\n}\n\n# Dimension of each dataset\nlapply(dat.ineligible, dim)\n#&gt; [[1]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 5063   19\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 5063   19\n\nThe next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.\n\n# Variables in the matched datasets\nnames(dat.ps2[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"arthritis\"     \"age.cat\"       \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \".imp\"         \n#&gt; [17] \"ps\"            \"sweight\"       \"new_weight\"\n\n# Variables in the ineligible datasets\nnames(dat.ineligible[[3]])\n#&gt;  [1] \"studyid\"       \"survey.weight\" \"psu\"           \"strata\"       \n#&gt;  [5] \"cvd\"           \"rheumatoid\"    \"age\"           \"sex\"          \n#&gt;  [9] \"education\"     \"race\"          \"income\"        \"bmi\"          \n#&gt; [13] \"smoking\"       \"htn\"           \"diabetes\"      \"age.cat\"      \n#&gt; [17] \"arthritis\"     \"ineligible\"    \".imp\"\n\n\ndat.ineligible2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  dat &lt;- dat.ineligible[[ii]]\n  \n  # Drop the ineligible variable from the dataset\n  dat$ineligible &lt;- NULL\n  \n  # Create ps and sweight\n  dat$ps &lt;- NA\n  dat$sweight &lt;- NA\n  dat$new_weight &lt;- NA\n  \n  # Keep only those variables available in the matched dataset\n  vars &lt;- names(dat.ps2[[1]])\n  dat &lt;- dat[,vars]\n  \n  # Ineligible datasets in list format\n  dat.ineligible2[[ii]] &lt;- dat\n}\n\n3.2 Combining eligible (imputed and PS weighted) and ineligible (unimputed) subjects\nNow, we will merge imputed eligible and unimputed ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.\n\ndat.full2 &lt;- list(NULL)\n\nfor (ii in 1:m) {\n  # Eligible\n  d1 &lt;- data.frame(dat.ps2[[ii]])\n  d1$eligible &lt;- 1\n  \n  # Ineligible\n  d2 &lt;- data.frame(dat.ineligible2[[ii]])\n  d2$eligible &lt;- 0\n  \n  # Full data\n  d3 &lt;- rbind(d1, d2)\n  \n  #  New weight variable in the full dataset\n  d3$new_weight &lt;- 0\n  d3$new_weight[d3$studyid %in% d1$studyid] &lt;- d1$new_weight\n  \n  # Full data in list format\n  dat.full2[[ii]] &lt;- d3\n}\nlapply(dat.full2, dim)\n#&gt; [[1]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 9254   20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 9254   20\n\n# Stacked dataset\ndat.stacked &lt;- rbindlist(dat.full2)\ndim(dat.stacked)\n#&gt; [1] 46270    20\n\n3.3 Prepating Survey design and subpopulation of eligible\nThe next step is to create the design on the combined dataset. Make sure to use the new weight variable that combines survey weights and PS weights.\n\nallImputations &lt;- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))\n\n# Design on full data\nw.design0 &lt;- svydesign(ids = ~psu, \n                       weights = ~new_weight, \n                       strata = ~strata,\n                      data = allImputations, \n                      nest = TRUE) \n\n# Subset the design\nw.design &lt;- subset(w.design0, eligible == 1) \n\nWe can see the length of the subsetted design:\n\nlapply(w.design$designs, dim)\n#&gt; [[1]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4191   20\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 4191   20\n\nNow we will run the design-adjusted logistic regression, adjusting for smoking since smoking was not balanced in terms of SMD.\nStep 3.4: Design adjusted regression analysis\n\n# Design-adjusted logistic regression\nfit &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + smoking, family = quasibinomial))\nres &lt;- exp(as.data.frame(cbind(coef(fit[[1]]),\n                               coef(fit[[2]]),\n                               coef(fit[[3]]),\n                               coef(fit[[4]]),\n                               coef(fit[[5]]))))\nnames(res) &lt;- paste(\"OR from m =\", 1:m)\nround(t(res),2)\n#&gt;               (Intercept) arthritisRheumatoid arthritis smokingPrevious smoker\n#&gt; OR from m = 1        0.04                          1.21                   2.93\n#&gt; OR from m = 2        0.04                          1.24                   2.93\n#&gt; OR from m = 3        0.04                          1.23                   2.93\n#&gt; OR from m = 4        0.04                          1.22                   2.92\n#&gt; OR from m = 5        0.04                          1.22                   2.92\n#&gt;               smokingCurrent smoker\n#&gt; OR from m = 1                  1.55\n#&gt; OR from m = 2                  1.56\n#&gt; OR from m = 3                  1.54\n#&gt; OR from m = 4                  1.55\n#&gt; OR from m = 5                  1.55\n\nStep 3.5: Pooling estimates\nNow, we will pool the estimate using Rubin’s rule:\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nDouble adjustment\n\n# Outcome model with covariates adjustment\nfit2 &lt;- with(w.design, svyglm(I(cvd == \"Yes\") ~ arthritis + age.cat + sex + education + \n                               race + income + bmi + smoking + htn + diabetes, \n                             family = quasibinomial))\n\n# Pooled estimate\npooled.estimates &lt;- MIcombine(fit2)\nOR &lt;- round(exp(pooled.estimates$coefficients), 2)\nOR &lt;- as.data.frame(OR)\nCI &lt;- round(exp(confint(pooled.estimates)), 2)\nOR &lt;- cbind(OR, CI)\nOR\n\n\n  \n\n\n\nReferences",
    "crumbs": [
      "Propensity score",
      "PSW with MI in subset"
    ]
  },
  {
    "objectID": "propensityscore9.html",
    "href": "propensityscore9.html",
    "title": "PSW with multiple tx",
    "section": "",
    "text": "Problem\nIn this chapter, we will use propensity score weighting (PSW) for multiple treatment categories. We will use CCHS data that was used in the previous chapter on exact matching with CCHS.\nLoad data\nLet us import the dataset:\n\nrm(list = ls())\nload(\"Data/propensityscore/cchs123b.RData\")\nls()\n#&gt; [1] \"analytic.miss\" \"analytic2\"\n\n\n\nanalytic.miss: Full dataset of 397,173 participants from CCHS cycles 1.1, 2.1, and 3.1 with missing values in some covariates\n\nanalytic2: Analytic dataset of 185,613 participants without missing in the covariates.\nPre-processing\nLet us create the full and analytic datasets for only CCHS 3.1.\n\n# Full dataset with missing\ndat.full &lt;- subset(analytic.miss, cycle == \"31\")\ndim(dat.full)\n#&gt; [1] 132221     22\n\n# Analytic dataset without missing\ndat.analytic &lt;- subset(analytic2, cycle == \"31\")\ndim(dat.analytic)\n#&gt; [1] 39634    22\n\nWe will use the analytic dataset (dat.analytic) to run our PSW analysis with the following variables: - Outcome: CVD - Exposure: phyact (3-level physical activity) - Covariates: age, sex, married (marital status), race, edu (education), income, bmi (body mass index), doctor (whether visited to a doctor), stress, smoke, drink (drink alcohol or not), fruit (fruit consumption), bp (blood pressure), diab (diabetes), OA (osteoarthritis), immigrate (immigrant or not)\n- Sampling weight: weight\n\n# Is there any character variable?\nstr(dat.analytic) \n#&gt; 'data.frame':    39634 obs. of  22 variables:\n#&gt;  $ CVD      : chr  \"no event\" \"no event\" \"no event\" \"no event\" ...\n#&gt;  $ age      : chr  \"20-29 years\" \"65 years and over\" \"20-29 years\" \"20-29 years\" ...\n#&gt;  $ sex      : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n#&gt;  $ married  : chr  \"not single\" \"single\" \"single\" \"single\" ...\n#&gt;  $ race     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#&gt;  $ edu      : chr  \"2nd grad.\" \"Post-2nd grad.\" \"2nd grad.\" \"Other 2nd grad.\" ...\n#&gt;  $ income   : chr  \"$50,000-$79,999\" \"$29,999 or less\" \"$29,999 or less\" \"$50,000-$79,999\" ...\n#&gt;  $ bmi      : Factor w/ 3 levels \"Underweight\",..: 3 2 2 2 2 3 2 3 3 2 ...\n#&gt;  $ phyact   : chr  \"Inactive\" \"Moderate\" \"Active\" \"Moderate\" ...\n#&gt;  $ doctor   : chr  \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n#&gt;  $ stress   : chr  \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" \"Not too stressed\" ...\n#&gt;  $ smoke    : chr  \"Former smoker\" \"Never smoker\" \"Never smoker\" \"Never smoker\" ...\n#&gt;  $ drink    : chr  \"Current drinker\" \"Former driker\" \"Never drank\" \"Current drinker\" ...\n#&gt;  $ fruit    : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 1 1 3 2 2 2 2 3 ...\n#&gt;  $ bp       : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ diab     : chr  \"No\" \"No\" \"No\" \"No\" ...\n#&gt;  $ province : chr  \"South\" \"South\" \"South\" \"South\" ...\n#&gt;  $ weight   : num  93.5 111.4 120.4 328.2 810.6 ...\n#&gt;  $ cycle    : Factor w/ 3 levels \"11\",\"21\",\"31\": 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ ID       : int  264953 264954 264961 264962 264963 264964 264969 264971 264975 264976 ...\n#&gt;  $ OA       : chr  \"Control\" \"OA\" \"Control\" \"Control\" ...\n#&gt;  $ immigrate: chr  \"not immigrant\" \"not immigrant\" \"not immigrant\" \"not immigrant\" ...\n\n# Make all variables (except for ID and weight) as factor\nvar.names &lt;- c(\"CVD\", \"phyact\", \"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \n               \"stress\", \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"province\", \"OA\", \"immigrate\")\ndat.full[var.names] &lt;- lapply(dat.full[var.names] , factor)\ndat.analytic[var.names] &lt;- lapply(dat.analytic[var.names], factor)\n\n# Outcome - CVD\ntable(dat.analytic$CVD, useNA = \"always\")\n#&gt; \n#&gt;    event no event     &lt;NA&gt; \n#&gt;     1931    37703        0\ndat.full$CVD &lt;- car::recode(dat.full$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.full$CVD &lt;- factor(dat.full$CVD, levels = c(\"No\", \"Yes\"))\n\ndat.analytic$CVD &lt;- car::recode(dat.analytic$CVD, \"'no event' = 'No'; 'event' = 'Yes'; else = NA \")\ndat.analytic$CVD &lt;- factor(dat.analytic$CVD, levels = c(\"No\", \"Yes\"))\ntable(dat.analytic$CVD, useNA = \"always\")\n#&gt; \n#&gt;    No   Yes  &lt;NA&gt; \n#&gt; 37703  1931     0\n\n# Exposure - physical activity\ntable(dat.analytic$phyact, useNA = \"always\")\n#&gt; \n#&gt;   Active Inactive Moderate     &lt;NA&gt; \n#&gt;    11508    17569    10557        0\ndat.full$phyact &lt;- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\ndat.analytic$phyact &lt;- factor(dat.analytic$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"))\n\n# Table 1\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\ntab1 &lt;- CreateTableOne(vars = vars, strata = \"phyact\", data = dat.analytic, test = F)\nprint(tab1, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive      Moderate      Active       \n#&gt;   n                               17569         10557         11508        \n#&gt;   age (%)       20-29 years        2537 (14.4)   1709 (16.2)   2316 (20.1) \n#&gt;                 30-39 years        3526 (20.1)   2265 (21.5)   2276 (19.8) \n#&gt;                 40-49 years        3270 (18.6)   1939 (18.4)   2037 (17.7) \n#&gt;                 50-59 years        2901 (16.5)   1659 (15.7)   1480 (12.9) \n#&gt;                 60-64 years        1130 ( 6.4)    665 ( 6.3)    570 ( 5.0) \n#&gt;                 65 years and over  3310 (18.8)   1568 (14.9)   1183 (10.3) \n#&gt;                 teen                895 ( 5.1)    752 ( 7.1)   1646 (14.3) \n#&gt;   sex (%)       Female             9403 (53.5)   5709 (54.1)   5526 (48.0) \n#&gt;                 Male               8166 (46.5)   4848 (45.9)   5982 (52.0) \n#&gt;   married (%)   not single         9600 (54.6)   5920 (56.1)   5637 (49.0) \n#&gt;                 single             7969 (45.4)   4637 (43.9)   5871 (51.0) \n#&gt;   race (%)      Non-white          1757 (10.0)    886 ( 8.4)   1066 ( 9.3) \n#&gt;                 White             15812 (90.0)   9671 (91.6)  10442 (90.7) \n#&gt;   edu (%)       &lt; 2ndary           3690 (21.0)   1635 (15.5)   2039 (17.7) \n#&gt;                 2nd grad.          3246 (18.5)   1749 (16.6)   1845 (16.0) \n#&gt;                 Other 2nd grad.    1566 ( 8.9)    969 ( 9.2)   1150 (10.0) \n#&gt;                 Post-2nd grad.     9067 (51.6)   6204 (58.8)   6474 (56.3) \n#&gt;   income (%)    $29,999 or less    4480 (25.5)   1929 (18.3)   1906 (16.6) \n#&gt;                 $30,000-$49,999    4018 (22.9)   2059 (19.5)   2097 (18.2) \n#&gt;                 $50,000-$79,999    4512 (25.7)   2974 (28.2)   3095 (26.9) \n#&gt;                 $80,000 or more    4559 (25.9)   3595 (34.1)   4410 (38.3) \n#&gt;   bmi (%)       Underweight         532 ( 3.0)    243 ( 2.3)    340 ( 3.0) \n#&gt;                 healthy weight     7349 (41.8)   5023 (47.6)   6233 (54.2) \n#&gt;                 Overweight         9688 (55.1)   5291 (50.1)   4935 (42.9) \n#&gt;   doctor (%)    No                 2322 (13.2)   1272 (12.0)   1520 (13.2) \n#&gt;                 Yes               15247 (86.8)   9285 (88.0)   9988 (86.8) \n#&gt;   stress (%)    Not too stressed  13544 (77.1)   8371 (79.3)   9314 (80.9) \n#&gt;                 stressed           4025 (22.9)   2186 (20.7)   2194 (19.1) \n#&gt;   smoke (%)     Current smoker     5032 (28.6)   2386 (22.6)   2488 (21.6) \n#&gt;                 Former smoker      6900 (39.3)   4562 (43.2)   4672 (40.6) \n#&gt;                 Never smoker       5637 (32.1)   3609 (34.2)   4348 (37.8) \n#&gt;   drink (%)     Current drinker   13913 (79.2)   9016 (85.4)   9863 (85.7) \n#&gt;                 Former driker      2582 (14.7)   1102 (10.4)   1063 ( 9.2) \n#&gt;                 Never drank        1074 ( 6.1)    439 ( 4.2)    582 ( 5.1) \n#&gt;   fruit (%)     0-3 daily serving  5610 (31.9)   2270 (21.5)   1902 (16.5) \n#&gt;                 4-6 daily serving  8827 (50.2)   5445 (51.6)   5481 (47.6) \n#&gt;                 6+ daily serving   3132 (17.8)   2842 (26.9)   4125 (35.8) \n#&gt;   bp (%)        No                14188 (80.8)   8976 (85.0)  10349 (89.9) \n#&gt;                 Yes                3381 (19.2)   1581 (15.0)   1159 (10.1) \n#&gt;   diab (%)      No                16393 (93.3)  10114 (95.8)  11165 (97.0) \n#&gt;                 Yes                1176 ( 6.7)    443 ( 4.2)    343 ( 3.0) \n#&gt;   OA (%)        Control           14864 (84.6)   9310 (88.2)  10565 (91.8) \n#&gt;                 OA                 2705 (15.4)   1247 (11.8)    943 ( 8.2) \n#&gt;   immigrate (%) not immigrant     16557 (94.2)  10150 (96.1)  11098 (96.4) \n#&gt;                 recent             1012 ( 5.8)    407 ( 3.9)    410 ( 3.6) \n#&gt;                Stratified by phyact\n#&gt;                 SMD   \n#&gt;   n                   \n#&gt;   age (%)        0.285\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   sex (%)        0.081\n#&gt;                       \n#&gt;   married (%)    0.095\n#&gt;                       \n#&gt;   race (%)       0.037\n#&gt;                       \n#&gt;   edu (%)        0.119\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   income (%)     0.213\n#&gt;                       \n#&gt;                       \n#&gt;                       \n#&gt;   bmi (%)        0.173\n#&gt;                       \n#&gt;                       \n#&gt;   doctor (%)     0.023\n#&gt;                       \n#&gt;   stress (%)     0.063\n#&gt;                       \n#&gt;   smoke (%)      0.129\n#&gt;                       \n#&gt;                       \n#&gt;   drink (%)      0.133\n#&gt;                       \n#&gt;                       \n#&gt;   fruit (%)      0.323\n#&gt;                       \n#&gt;                       \n#&gt;   bp (%)         0.175\n#&gt;                       \n#&gt;   diab (%)       0.116\n#&gt;                       \n#&gt;   OA (%)         0.150\n#&gt;                       \n#&gt;   immigrate (%)  0.070\n#&gt; \n\nPSW for multiple tx\nNominal categories (option 1)\nFor this part (option 1), we consider physical activity as a nominal variable.\nEstimating Propensity score\nLet us fit the PS model by considering physical activity as a nominal variable and estimate the propensity scores:\n\n# Formula\nps.formula &lt;- formula(phyact ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\n#&gt; Loading required package: stats4\n#&gt; Loading required package: splines\n#&gt; \n#&gt; Attaching package: 'VGAM'\n#&gt; The following object is masked from 'package:survey':\n#&gt; \n#&gt;     calibrate\nps.fit &lt;- vglm(ps.formula, weights = weight, data = dat.analytic, \n               family = multinomial(parallel = FALSE))\n\n# Propensity scores\nps &lt;- data.frame(fitted(ps.fit))\nhead(ps)\n\n\n  \n\n\n\n# Summary\napply(ps, 2, summary)\n#&gt;           Inactive   Moderate    Active\n#&gt; Min.    0.06879258 0.07957314 0.0285961\n#&gt; 1st Qu. 0.34233334 0.23371928 0.1867680\n#&gt; Median  0.44756388 0.27020295 0.2634537\n#&gt; Mean    0.44778510 0.26768926 0.2845256\n#&gt; 3rd Qu. 0.55452938 0.30446807 0.3616572\n#&gt; Max.    0.89058237 0.39864744 0.7524817\n\nCreating weights\nLet us create PS weights. For subject \\(i\\), PS weight is calculated as\n\\[w_i = \\frac{1}{P(A_i = a|L)}, \\] where \\(A\\) is the exposure with levels \\(a\\) (Inactive, Moderate, and Active in our case), and \\(L\\) is the list of covariates.\n\n# IPW\ndat.analytic$ipw &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps$Moderate, \n                                  1/ps$Inactive))\nwith(dat.analytic, by(ipw, phyact, summary))\n#&gt; phyact: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.123   1.671   1.994   2.233   2.505  14.536 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   2.508   3.206   3.576   3.743   4.087  11.695 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.329   2.310   3.086   3.534   4.225  25.049\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw, data = dat.analytic)\n\n# Table 1\ntabw &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               39231.2         39519.8        \n#&gt;   age (%)       20-29 years        6477.4 (16.5)   6856.1 (17.3) \n#&gt;                 30-39 years        7706.8 (19.6)   8174.8 (20.7) \n#&gt;                 40-49 years        7098.2 (18.1)   7163.0 (18.1) \n#&gt;                 50-59 years        5922.4 (15.1)   5901.2 (14.9) \n#&gt;                 60-64 years        2414.8 ( 6.2)   2265.4 ( 5.7) \n#&gt;                 65 years and over  6187.1 (15.8)   5843.8 (14.8) \n#&gt;                 teen               3424.6 ( 8.7)   3315.4 ( 8.4) \n#&gt;   sex (%)       Female            20188.3 (51.5)  20693.4 (52.4) \n#&gt;                 Male              19042.9 (48.5)  18826.3 (47.6) \n#&gt;   married (%)   not single        20665.9 (52.7)  20907.0 (52.9) \n#&gt;                 single            18565.3 (47.3)  18612.8 (47.1) \n#&gt;   race (%)      Non-white          3471.9 ( 8.8)   4095.5 (10.4) \n#&gt;                 White             35759.3 (91.2)  35424.2 (89.6) \n#&gt;   edu (%)       &lt; 2ndary           7267.3 (18.5)   7288.2 (18.4) \n#&gt;                 2nd grad.          6902.6 (17.6)   6901.3 (17.5) \n#&gt;                 Other 2nd grad.    3783.0 ( 9.6)   3671.6 ( 9.3) \n#&gt;                 Post-2nd grad.    21278.3 (54.2)  21658.7 (54.8) \n#&gt;   income (%)    $29,999 or less    8432.3 (21.5)   8425.7 (21.3) \n#&gt;                 $30,000-$49,999    8115.6 (20.7)   8109.8 (20.5) \n#&gt;                 $50,000-$79,999   10169.1 (25.9)  10602.5 (26.8) \n#&gt;                 $80,000 or more   12514.2 (31.9)  12381.9 (31.3) \n#&gt;   bmi (%)       Underweight        1092.7 ( 2.8)   1110.6 ( 2.8) \n#&gt;                 healthy weight    18191.1 (46.4)  18613.7 (47.1) \n#&gt;                 Overweight        19947.4 (50.8)  19795.5 (50.1) \n#&gt;   doctor (%)    No                 5073.1 (12.9)   5127.6 (13.0) \n#&gt;                 Yes               34158.1 (87.1)  34392.2 (87.0) \n#&gt;   stress (%)    Not too stressed  30921.0 (78.8)  31245.6 (79.1) \n#&gt;                 stressed           8310.3 (21.2)   8274.2 (20.9) \n#&gt;   smoke (%)     Current smoker    10018.8 (25.5)  10121.1 (25.6) \n#&gt;                 Former smoker     15860.1 (40.4)  15737.2 (39.8) \n#&gt;                 Never smoker      13352.4 (34.0)  13661.5 (34.6) \n#&gt;   drink (%)     Current drinker   32257.4 (82.2)  32836.4 (83.1) \n#&gt;                 Former driker      4857.5 (12.4)   4621.5 (11.7) \n#&gt;                 Never drank        2116.4 ( 5.4)   2061.9 ( 5.2) \n#&gt;   fruit (%)     0-3 daily serving  9738.2 (24.8)   9786.2 (24.8) \n#&gt;                 4-6 daily serving 19523.6 (49.8)  19803.8 (50.1) \n#&gt;                 6+ daily serving   9969.4 (25.4)   9929.8 (25.1) \n#&gt;   bp (%)        No                33045.9 (84.2)  33662.1 (85.2) \n#&gt;                 Yes                6185.3 (15.8)   5857.7 (14.8) \n#&gt;   diab (%)      No                37227.0 (94.9)  37572.5 (95.1) \n#&gt;                 Yes                2004.2 ( 5.1)   1947.3 ( 4.9) \n#&gt;   OA (%)        Control           34297.7 (87.4)  34835.2 (88.1) \n#&gt;                 OA                 4933.5 (12.6)   4684.6 (11.9) \n#&gt;   immigrate (%) not immigrant     37503.8 (95.6)  37532.8 (95.0) \n#&gt;                 recent             1727.4 ( 4.4)   1987.0 ( 5.0) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             40667.9               \n#&gt;   age (%)        6467.5 (15.9)   0.046\n#&gt;                  8588.7 (21.1)        \n#&gt;                  7538.2 (18.5)        \n#&gt;                  6327.6 (15.6)        \n#&gt;                  2420.1 ( 6.0)        \n#&gt;                  5989.1 (14.7)        \n#&gt;                  3336.8 ( 8.2)        \n#&gt;   sex (%)       21026.2 (51.7)   0.012\n#&gt;                 19641.7 (48.3)        \n#&gt;   married (%)   22651.2 (55.7)   0.040\n#&gt;                 18016.8 (44.3)        \n#&gt;   race (%)       3834.0 ( 9.4)   0.034\n#&gt;                 36834.0 (90.6)        \n#&gt;   edu (%)        7395.4 (18.2)   0.023\n#&gt;                  6825.8 (16.8)        \n#&gt;                  3765.2 ( 9.3)        \n#&gt;                 22681.6 (55.8)        \n#&gt;   income (%)     8029.8 (19.7)   0.041\n#&gt;                  8544.1 (21.0)        \n#&gt;                 11407.3 (28.0)        \n#&gt;                 12686.7 (31.2)        \n#&gt;   bmi (%)        1241.6 ( 3.1)   0.017\n#&gt;                 19120.5 (47.0)        \n#&gt;                 20305.8 (49.9)        \n#&gt;   doctor (%)     5272.7 (13.0)   0.001\n#&gt;                 35395.3 (87.0)        \n#&gt;   stress (%)    32100.6 (78.9)   0.004\n#&gt;                  8567.4 (21.1)        \n#&gt;   smoke (%)      9760.6 (24.0)   0.032\n#&gt;                 17024.0 (41.9)        \n#&gt;                 13883.4 (34.1)        \n#&gt;   drink (%)     33927.5 (83.4)   0.022\n#&gt;                  4634.1 (11.4)        \n#&gt;                  2106.3 ( 5.2)        \n#&gt;   fruit (%)     10170.0 (25.0)   0.007\n#&gt;                 20200.6 (49.7)        \n#&gt;                 10297.3 (25.3)        \n#&gt;   bp (%)        34372.4 (84.5)   0.017\n#&gt;                  6295.6 (15.5)        \n#&gt;   diab (%)      38850.3 (95.5)   0.020\n#&gt;                  1817.7 ( 4.5)        \n#&gt;   OA (%)        35666.9 (87.7)   0.015\n#&gt;                  5001.1 (12.3)        \n#&gt;   immigrate (%) 38662.6 (95.1)   0.020\n#&gt;                  2005.4 ( 4.9)\n\nAll covariates are balanced in terms of SMD (SMD \\(\\le\\) 0.20).\nOutcome analysis\nNow, we will fit the outcome model. To get the correct estimate of the standard error, we will set up the design with full data and subset the design.\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight &lt;- with(dat.analytic, ipw * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight &lt;- 0\ndat.full$ATEweight[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n  \n\n\n\n# Outcome model\nfit &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95  p-value \n#&gt;    phyact Inactive       Ref                      \n#&gt;           Moderate      0.94 [0.64;1.38]   0.7693 \n#&gt;             Active      0.83 [0.53;1.29]   0.4133\n\nOrdinal categories (for comparison)\nFor comparison, let us consider physical activity as a ordinal variable (option 2).\nDefine ordinal variable\n\n# Exposure - ordinal physical activity\ndat.full$phyact.ord &lt;- factor(dat.full$phyact, levels = c(\"Inactive\", \"Moderate\", \"Active\"), \n                              ordered = T)\ndat.analytic$phyact.ord &lt;- factor(dat.analytic$phyact, \n                                  levels = c(\"Inactive\", \"Moderate\", \"Active\"), ordered = T)\nhead(dat.analytic$phyact.ord)\n#&gt; [1] Inactive Moderate Active   Moderate Active   Active  \n#&gt; Levels: Inactive &lt; Moderate &lt; Active\n\nEstimating Propensity score\n\n# Formula\nps.formula2 &lt;- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\nlibrary(VGAM)\nps.fit2 &lt;- vglm(ps.formula2, weights = weight, data = dat.analytic, family = propodds)\n\n# Propensity scores\nps2 &lt;- data.frame(fitted(ps.fit2))\nhead(ps2)\n\n\n  \n\n\n\n# Summary\napply(ps2, 2, summary)\n#&gt;           Inactive   Moderate     Active\n#&gt; Min.    0.07517679 0.07505615 0.03456101\n#&gt; 1st Qu. 0.34108922 0.25002048 0.18907940\n#&gt; Median  0.44711595 0.28026693 0.26446898\n#&gt; Mean    0.44780744 0.26682382 0.28536874\n#&gt; 3rd Qu. 0.55497781 0.29473347 0.35967960\n#&gt; Max.    0.89038285 0.29934482 0.78152250\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw2 &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps2$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps2$Moderate, \n                                  1/ps2$Inactive))\nwith(dat.analytic, by(ipw2, phyact.ord, summary))\n#&gt; phyact.ord: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.123   1.668   2.001   2.237   2.516  13.302 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact.ord: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   3.341   3.382   3.525   3.748   3.875  11.248 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact.ord: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.280   2.319   3.073   3.504   4.211  19.768\n\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw2, data = dat.analytic)\n\n# Table 1\ntabw2 &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw2, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               39305.5         39566.3        \n#&gt;   age (%)       20-29 years        6679.4 (17.0)   6125.6 (15.5) \n#&gt;                 30-39 years        7635.7 (19.4)   8357.8 (21.1) \n#&gt;                 40-49 years        7085.9 (18.0)   7124.1 (18.0) \n#&gt;                 50-59 years        5820.8 (14.8)   6307.9 (15.9) \n#&gt;                 60-64 years        2339.1 ( 6.0)   2494.0 ( 6.3) \n#&gt;                 65 years and over  6108.9 (15.5)   6264.9 (15.8) \n#&gt;                 teen               3635.8 ( 9.3)   2892.0 ( 7.3) \n#&gt;   sex (%)       Female            19987.1 (50.9)  21622.5 (54.6) \n#&gt;                 Male              19318.4 (49.1)  17943.8 (45.4) \n#&gt;   married (%)   not single        20342.7 (51.8)  22151.4 (56.0) \n#&gt;                 single            18962.9 (48.2)  17414.9 (44.0) \n#&gt;   race (%)      Non-white          3595.2 ( 9.1)   3507.2 ( 8.9) \n#&gt;                 White             35710.4 (90.9)  36059.1 (91.1) \n#&gt;   edu (%)       &lt; 2ndary           7467.4 (19.0)   6739.9 (17.0) \n#&gt;                 2nd grad.          7027.3 (17.9)   6644.2 (16.8) \n#&gt;                 Other 2nd grad.    3815.2 ( 9.7)   3628.7 ( 9.2) \n#&gt;                 Post-2nd grad.    20995.6 (53.4)  22553.5 (57.0) \n#&gt;   income (%)    $29,999 or less    8596.0 (21.9)   7724.2 (19.5) \n#&gt;                 $30,000-$49,999    8182.3 (20.8)   7888.9 (19.9) \n#&gt;                 $50,000-$79,999   10126.1 (25.8)  11034.9 (27.9) \n#&gt;                 $80,000 or more   12401.1 (31.6)  12918.3 (32.6) \n#&gt;   bmi (%)       Underweight        1134.1 ( 2.9)    960.4 ( 2.4) \n#&gt;                 healthy weight    18284.0 (46.5)  18426.4 (46.6) \n#&gt;                 Overweight        19887.4 (50.6)  20179.5 (51.0) \n#&gt;   doctor (%)    No                 5183.0 (13.2)   4741.2 (12.0) \n#&gt;                 Yes               34122.5 (86.8)  34825.0 (88.0) \n#&gt;   stress (%)    Not too stressed  31032.0 (79.0)  31233.5 (78.9) \n#&gt;                 stressed           8273.6 (21.0)   8332.8 (21.1) \n#&gt;   smoke (%)     Current smoker    10263.8 (26.1)   9253.9 (23.4) \n#&gt;                 Former smoker     15572.0 (39.6)  16822.9 (42.5) \n#&gt;                 Never smoker      13469.8 (34.3)  13489.4 (34.1) \n#&gt;   drink (%)     Current drinker   32206.8 (81.9)  33304.9 (84.2) \n#&gt;                 Former driker      4898.9 (12.5)   4438.8 (11.2) \n#&gt;                 Never drank        2199.8 ( 5.6)   1822.5 ( 4.6) \n#&gt;   fruit (%)     0-3 daily serving  9841.8 (25.0)   9509.3 (24.0) \n#&gt;                 4-6 daily serving 19586.3 (49.8)  19922.3 (50.4) \n#&gt;                 6+ daily serving   9877.5 (25.1)  10134.7 (25.6) \n#&gt;   bp (%)        No                33233.3 (84.6)  33115.0 (83.7) \n#&gt;                 Yes                6072.2 (15.4)   6451.2 (16.3) \n#&gt;   diab (%)      No                37292.2 (94.9)  37648.3 (95.2) \n#&gt;                 Yes                2013.4 ( 5.1)   1918.0 ( 4.8) \n#&gt;   OA (%)        Control           34458.3 (87.7)  34493.7 (87.2) \n#&gt;                 OA                 4847.3 (12.3)   5072.6 (12.8) \n#&gt;   immigrate (%) not immigrant     37534.6 (95.5)  37816.7 (95.6) \n#&gt;                 recent             1770.9 ( 4.5)   1749.5 ( 4.4) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             40328.5               \n#&gt;   age (%)        6789.1 (16.8)   0.080\n#&gt;                  8504.7 (21.1)        \n#&gt;                  7562.1 (18.8)        \n#&gt;                  6079.6 (15.1)        \n#&gt;                  2283.6 ( 5.7)        \n#&gt;                  5614.4 (13.9)        \n#&gt;                  3495.0 ( 8.7)        \n#&gt;   sex (%)       20379.8 (50.5)   0.055\n#&gt;                 19948.8 (49.5)        \n#&gt;   married (%)   21840.5 (54.2)   0.057\n#&gt;                 18488.0 (45.8)        \n#&gt;   race (%)       4125.6 (10.2)   0.031\n#&gt;                 36202.9 (89.8)        \n#&gt;   edu (%)        7564.8 (18.8)   0.052\n#&gt;                  6886.8 (17.1)        \n#&gt;                  3777.5 ( 9.4)        \n#&gt;                 22099.4 (54.8)        \n#&gt;   income (%)     8371.7 (20.8)   0.056\n#&gt;                  8581.4 (21.3)        \n#&gt;                 11013.7 (27.3)        \n#&gt;                 12361.7 (30.7)        \n#&gt;   bmi (%)        1294.7 ( 3.2)   0.037\n#&gt;                 19134.4 (47.4)        \n#&gt;                 19899.5 (49.3)        \n#&gt;   doctor (%)     5467.9 (13.6)   0.031\n#&gt;                 34860.6 (86.4)        \n#&gt;   stress (%)    31816.9 (78.9)   0.001\n#&gt;                  8511.6 (21.1)        \n#&gt;   smoke (%)     10153.3 (25.2)   0.048\n#&gt;                 16300.6 (40.4)        \n#&gt;                 13874.6 (34.4)        \n#&gt;   drink (%)     33415.6 (82.9)   0.043\n#&gt;                  4714.1 (11.7)        \n#&gt;                  2198.9 ( 5.5)        \n#&gt;   fruit (%)     10190.6 (25.3)   0.020\n#&gt;                 19953.3 (49.5)        \n#&gt;                 10184.6 (25.3)        \n#&gt;   bp (%)        34557.1 (85.7)   0.037\n#&gt;                  5771.5 (14.3)        \n#&gt;   diab (%)      38519.4 (95.5)   0.020\n#&gt;                  1809.1 ( 4.5)        \n#&gt;   OA (%)        35641.0 (88.4)   0.024\n#&gt;                  4687.5 (11.6)        \n#&gt;   immigrate (%) 38153.4 (94.6)   0.030\n#&gt;                  2175.2 ( 5.4)\n\nAgain, all covariates are balanced in terms of SMD.\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight2 &lt;- with(dat.analytic, ipw2 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight2 &lt;- 0\ndat.full$ATEweight2[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight2\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight2, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop2 &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop2\n\n\n  \n\n\n\n# Outcome model\nfit2 &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit2, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95  p-value \n#&gt;    phyact Inactive       Ref                      \n#&gt;           Moderate      1.01 [0.71;1.43]   0.9775 \n#&gt;             Active      0.81 [0.52;1.27]   0.3645\n\nMachine learning / GBM (option 3)\nIn this part, we will use Gradient Boosting as one of the machine learning techniques to estimate the propensity scores.\nEstimating Propensity score\n\n# Formula\nps.formula3 &lt;- formula(phyact.ord ~ age + sex + married + race + edu + income + bmi + \n                        doctor + stress + smoke + drink + fruit + bp + diab + \n                        OA + immigrate)\n\n# PS model\npacman::p_load(twang)\n#&gt; package 'deldir' successfully unpacked and MD5 sums checked\n#&gt; package 'interp' successfully unpacked and MD5 sums checked\n#&gt; package 'gbm' successfully unpacked and MD5 sums checked\n#&gt; package 'latticeExtra' successfully unpacked and MD5 sums checked\n#&gt; package 'twang' successfully unpacked and MD5 sums checked\n#&gt; \n#&gt; The downloaded binary packages are in\n#&gt;  C:\\Users\\wilds\\AppData\\Local\\Temp\\RtmpCkwiCl\\downloaded_packages\nset.seed(123)\nps.fit3 &lt;- mnps(ps.formula3, data = dat.analytic, estimand = \"ATE\", verbose = FALSE,\n                stop.method = c(\"es.max\"), n.trees = 200, sampw = dat.analytic$weight)\nsummary(ps.fit3)\n#&gt; Summary of pairwise comparisons:\n#&gt;   max.std.eff.sz min.p max.ks min.ks.pval stop.method\n#&gt; 1      0.4134866     0      1           0         unw\n#&gt; 2      0.1880231     0      1           0      es.max\n#&gt; \n#&gt; Sample sizes and effective sample sizes:\n#&gt;   treatment     n ESS:es.max\n#&gt; 1  Inactive 17569   7361.137\n#&gt; 2  Moderate 10557   4875.807\n#&gt; 3    Active 11508   5376.500\n\n\n# Propensity scores\nps3 &lt;- data.frame(Active = ps.fit3$psList$Active$ps,\n                 Moderate = ps.fit3$psList$Moderate$ps,\n                 Inactive = ps.fit3$psList$Inactive$ps)\nnames(ps3) &lt;- c(\"Active\",\"Moderate\",\"Inactive\")\nhead(ps3)\n\n\n  \n\n\n\n# Summary\napply(ps3, 2, summary)\n#&gt;            Active  Moderate  Inactive\n#&gt; Min.    0.1827539 0.1884745 0.2759573\n#&gt; 1st Qu. 0.2316300 0.2478217 0.3726945\n#&gt; Median  0.2664345 0.2666218 0.4485734\n#&gt; Mean    0.2894693 0.2679683 0.4420585\n#&gt; 3rd Qu. 0.3343953 0.2859762 0.5015505\n#&gt; Max.    0.5330670 0.3255325 0.6389846\n\nCreating weights\nLet us create PS weights:\n\n# IPW\ndat.analytic$ipw3 &lt;- ifelse(dat.analytic$phyact==\"Active\", 1/ps3$Active, \n                           ifelse(dat.analytic$phyact==\"Moderate\", 1/ps3$Moderate, \n                                  1/ps3$Inactive))\nwith(dat.analytic, by(ipw3, phyact, summary))\n#&gt; phyact: Inactive\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.565   1.877   2.086   2.206   2.484   3.624 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Moderate\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   3.072   3.415   3.652   3.709   3.947   5.306 \n#&gt; ------------------------------------------------------------ \n#&gt; phyact: Active\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.876   2.716   3.229   3.320   3.952   5.465\n\nWeights are not large compared to options 1 and 2.\nBalance checking\nNow, we will check the balance in terms of SMD:\n\nlibrary(survey)\nvars &lt;- c(\"age\", \"sex\", \"married\", \"race\", \"edu\", \"income\", \"bmi\", \"doctor\", \"stress\", \n          \"smoke\", \"drink\", \"fruit\", \"bp\", \"diab\", \"OA\", \"immigrate\")\n\n# Design\nw.design &lt;- svydesign(id = ~1, weights = ~ipw3, data = dat.analytic)\n\n# Table 1\ntabw &lt;- svyCreateTableOne(vars = vars, strata = \"phyact\", data = w.design, test = F)\nprint(tabw, smd = T, showAllLevels = T)\n#&gt;                Stratified by phyact\n#&gt;                 level             Inactive        Moderate       \n#&gt;   n                               38763.2         39159.6        \n#&gt;   age (%)       20-29 years        6086.5 (15.7)   6641.5 (17.0) \n#&gt;                 30-39 years        7624.6 (19.7)   8171.6 (20.9) \n#&gt;                 40-49 years        7057.4 (18.2)   7073.0 (18.1) \n#&gt;                 50-59 years        6267.5 (16.2)   5894.9 (15.1) \n#&gt;                 60-64 years        2399.6 ( 6.2)   2390.7 ( 6.1) \n#&gt;                 65 years and over  6883.8 (17.8)   5859.8 (15.0) \n#&gt;                 teen               2443.7 ( 6.3)   3128.1 ( 8.0) \n#&gt;   sex (%)       Female            20928.4 (54.0)  21097.6 (53.9) \n#&gt;                 Male              17834.8 (46.0)  18062.0 (46.1) \n#&gt;   married (%)   not single        21244.6 (54.8)  21279.5 (54.3) \n#&gt;                 single            17518.5 (45.2)  17880.1 (45.7) \n#&gt;   race (%)      Non-white          3683.4 ( 9.5)   3666.8 ( 9.4) \n#&gt;                 White             35079.7 (90.5)  35492.8 (90.6) \n#&gt;   edu (%)       &lt; 2ndary           7610.9 (19.6)   6692.3 (17.1) \n#&gt;                 2nd grad.          7088.9 (18.3)   6693.3 (17.1) \n#&gt;                 Other 2nd grad.    3558.1 ( 9.2)   3619.6 ( 9.2) \n#&gt;                 Post-2nd grad.    20505.3 (52.9)  22154.2 (56.6) \n#&gt;   income (%)    $29,999 or less    9138.3 (23.6)   7750.6 (19.8) \n#&gt;                 $30,000-$49,999    8400.8 (21.7)   7956.3 (20.3) \n#&gt;                 $50,000-$79,999    9946.0 (25.7)  10696.8 (27.3) \n#&gt;                 $80,000 or more   11278.0 (29.1)  12755.9 (32.6) \n#&gt;   bmi (%)       Underweight        1195.7 ( 3.1)    967.7 ( 2.5) \n#&gt;                 healthy weight    16766.4 (43.3)  18791.6 (48.0) \n#&gt;                 Overweight        20801.0 (53.7)  19400.2 (49.5) \n#&gt;   doctor (%)    No                 5070.5 (13.1)   4827.3 (12.3) \n#&gt;                 Yes               33692.7 (86.9)  34332.3 (87.7) \n#&gt;   stress (%)    Not too stressed  29885.7 (77.1)  31099.5 (79.4) \n#&gt;                 stressed           8877.5 (22.9)   8060.0 (20.6) \n#&gt;   smoke (%)     Current smoker    10737.3 (27.7)   9401.4 (24.0) \n#&gt;                 Former smoker     15204.3 (39.2)  16211.3 (41.4) \n#&gt;                 Never smoker      12821.5 (33.1)  13546.9 (34.6) \n#&gt;   drink (%)     Current drinker   31123.7 (80.3)  33043.2 (84.4) \n#&gt;                 Former driker      5322.0 (13.7)   4301.7 (11.0) \n#&gt;                 Never drank        2317.5 ( 6.0)   1814.6 ( 4.6) \n#&gt;   fruit (%)     0-3 daily serving 10529.0 (27.2)   8996.8 (23.0) \n#&gt;                 4-6 daily serving 19466.5 (50.2)  19851.5 (50.7) \n#&gt;                 6+ daily serving   8767.6 (22.6)  10311.3 (26.3) \n#&gt;   bp (%)        No                31712.5 (81.8)  33335.5 (85.1) \n#&gt;                 Yes                7050.7 (18.2)   5824.1 (14.9) \n#&gt;   diab (%)      No                36326.4 (93.7)  37499.3 (95.8) \n#&gt;                 Yes                2436.8 ( 6.3)   1660.2 ( 4.2) \n#&gt;   OA (%)        Control           33060.8 (85.3)  34579.5 (88.3) \n#&gt;                 OA                 5702.3 (14.7)   4580.0 (11.7) \n#&gt;   immigrate (%) not immigrant     36812.8 (95.0)  37425.9 (95.6) \n#&gt;                 recent             1950.3 ( 5.0)   1733.7 ( 4.4) \n#&gt;                Stratified by phyact\n#&gt;                 Active          SMD   \n#&gt;   n             38211.2               \n#&gt;   age (%)        6567.9 (17.2)   0.145\n#&gt;                  8208.6 (21.5)        \n#&gt;                  7313.3 (19.1)        \n#&gt;                  5555.3 (14.5)        \n#&gt;                  2186.9 ( 5.7)        \n#&gt;                  4539.2 (11.9)        \n#&gt;                  3840.0 (10.0)        \n#&gt;   sex (%)       18441.4 (48.3)   0.077\n#&gt;                 19769.8 (51.7)        \n#&gt;   married (%)   20012.7 (52.4)   0.033\n#&gt;                 18198.5 (47.6)        \n#&gt;   race (%)       3397.2 ( 8.9)   0.014\n#&gt;                 34814.0 (91.1)        \n#&gt;   edu (%)        6186.6 (16.2)   0.081\n#&gt;                  6159.6 (16.1)        \n#&gt;                  3630.7 ( 9.5)        \n#&gt;                 22234.2 (58.2)        \n#&gt;   income (%)     6758.9 (17.7)   0.120\n#&gt;                  7420.8 (19.4)        \n#&gt;                 10612.9 (27.8)        \n#&gt;                 13418.7 (35.1)        \n#&gt;   bmi (%)        1041.5 ( 2.7)   0.112\n#&gt;                 19648.4 (51.4)        \n#&gt;                 17521.2 (45.9)        \n#&gt;   doctor (%)     5001.8 (13.1)   0.015\n#&gt;                 33209.4 (86.9)        \n#&gt;   stress (%)    30845.7 (80.7)   0.059\n#&gt;                  7365.5 (19.3)        \n#&gt;   smoke (%)      8531.3 (22.3)   0.083\n#&gt;                 16200.7 (42.4)        \n#&gt;                 13479.2 (35.3)        \n#&gt;   drink (%)     32817.1 (85.9)   0.101\n#&gt;                  3703.9 ( 9.7)        \n#&gt;                  1690.1 ( 4.4)        \n#&gt;   fruit (%)      7924.2 (20.7)   0.120\n#&gt;                 19284.4 (50.5)        \n#&gt;                 11002.5 (28.8)        \n#&gt;   bp (%)        33698.1 (88.2)   0.120\n#&gt;                  4513.1 (11.8)        \n#&gt;   diab (%)      36930.8 (96.6)   0.092\n#&gt;                  1280.4 ( 3.4)        \n#&gt;   OA (%)        34640.5 (90.7)   0.110\n#&gt;                  3570.7 ( 9.3)        \n#&gt;   immigrate (%) 36764.3 (96.2)   0.040\n#&gt;                  1446.8 ( 3.8)\n\nAll covariates are balanced in terms of SMD.\n\nplot(ps.fit3, color = TRUE, plots = 2, figureRows = 1)\n\n\n\n\n\n\n#&gt; [[1]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\n\nplot(ps.fit3, plots = 3, color=TRUE, pairwiseMax = FALSE)\n\n\n\n\n\n\n#&gt; [[1]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\nOutcome analysis\nNow, we will fit the outcome model:\n\n# Create an indicator variable in the full dataset\ndat.full$ind &lt;- 0\ndat.full$ind[dat.full$ID %in% dat.analytic$ID] &lt;- 1\n\n# New weight = IPW * survey weight\ndat.analytic$ATEweight3 &lt;- with(dat.analytic, ipw3 * weight)\n\n# New weight variable in the full dataset\ndat.full$ATEweight3 &lt;- 0\ndat.full$ATEweight3[dat.full$ID %in% dat.analytic$ID] &lt;- dat.analytic$ATEweight3\n\n# Survey setup with full data \nw.design0 &lt;- svydesign(id = ~1, weights = ~ATEweight3, data = dat.full)\n\n# Subset the design for analytic sample\nw.design1 &lt;- subset(w.design0, ind == 1)\n\n# Weighted proportion\nw.prop &lt;- svyby(formula = ~CVD, by = ~phyact, design = w.design1, FUN = svymean)\nw.prop\n\n\n  \n\n\n\n# Outcome model\nfit3 &lt;- svyglm(CVD ~ phyact, design = w.design1, family = binomial)\npublish(fit3, confint.method = \"robust\", pvalue.method = \"robust\")\n#&gt;  Variable    Units OddsRatio       CI.95   p-value \n#&gt;    phyact Inactive       Ref                       \n#&gt;           Moderate      0.82 [0.56;1.21]   0.32241 \n#&gt;             Active      0.65 [0.44;0.96]   0.03203\n\nOther approches for multiple treatments\nNot covered here, but possible to do in a multiple treatments context:\n\nPropensity score matching\nPropensity score stratification\nMarginal mean weighting",
    "crumbs": [
      "Propensity score",
      "PSW with multiple tx"
    ]
  },
  {
    "objectID": "propensityscoreF.html",
    "href": "propensityscoreF.html",
    "title": "R functions (S)",
    "section": "",
    "text": "The list of new R functions introduced in this Propensity score analyis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nbal.plot\ncobalt\nTo produce a overalp/balance plot for propensity scoes\n\n\nbal.tab\ncobalt\nTo check the balance at each category of covariates\n\n\nCreateCatTable\ntableone\nTo create a frequency table with categorical variables only\n\n\ndo.call\nbase\nTo execute a function call\n\n\nlove.plot\ncobalt\nTo plot the standardized mean differences at each category of covariates\n\n\nmatch.data\nMatchIt\nTo extract the matched dataste from a matchit object\n\n\nmatchit\nMatchIt\nTo match an exposed/treated to m unexposed/controls. The argument `ratio` determines the value of m.\n\n\nrownames\nbase\nNames of the rows",
    "crumbs": [
      "Propensity score",
      "R functions (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html",
    "href": "propensityscoreQ.html",
    "title": "Quiz (S)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html#live-quiz",
    "href": "propensityscoreQ.html#live-quiz",
    "title": "Quiz (S)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreQ.html#download-quiz",
    "href": "propensityscoreQ.html#download-quiz",
    "title": "Quiz (S)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Propensity score",
      "Quiz (S)"
    ]
  },
  {
    "objectID": "propensityscoreS.html",
    "href": "propensityscoreS.html",
    "title": "App (S)",
    "section": "",
    "text": "Below is an example of an app that utilizes the NHANES dataset following the tutorial materials. Users can visualize the results from a propensity score analysis using complex survey data, and implications of choosing different list of covariates for the propensity score model fit in each step of the analysis.\n\n\n\n\n\n\nTip\n\n\n\nWeb App may need some time to load the necessary files, depending on your internet speed.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to run the app locally in your computer instead for faster load time: you can download the app directly in your R via shiny::runGitHub(\"EpiShinyLiveS\", \"ehsanx\", subdir = \"app\"). Make sure that shiny, ggplot2, survey, tableone, tibble, dplyr, tidyr, broom.mixed and jtools packages are installed in your computer/R.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the app.",
    "crumbs": [
      "Propensity score",
      "App (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html",
    "href": "propensityscoreE.html",
    "title": "Exercise 1 (S)",
    "section": "",
    "text": "Problem Statement\nWe will use the article by Moon et al. (2021):\nWe will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\nOutcome variable\nExposure\nConfounders and other variables\nTwo important warnings before we start:",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#problem-statement",
    "href": "propensityscoreE.html#problem-statement",
    "title": "Exercise 1 (S)",
    "section": "",
    "text": "SEQN: Respondent sequence number\nstrata: Masked pseudo strata (strata is nested within PSU)\npsu: Masked pseudo PSU\nsurvey.weight: Full sample 8 year interview weight divided by 4\nsurvey.cycle: NHANES cycle\n\n\n\ncvd: Cardiovascular disease\n\n\n\nnocturia: Binary nocturia\n\n\n\nage: Age in years at screening\ngender: Gender\nrace: Race/Ethnicity\nsmoking: 100+ cigarettes in life\nalcohol: Alcohol consumption (12+ drinks in 1 year)\nsleep: Sleep duration, h\nbmi: Body Mass Index in kg/m\\(^2\\)\n\nsystolic: Systolic blood pressure, mmHg\ndiastolic: Diastolic blood pressure, mmHg\ntcholesterol: Total cholesterol, mg/dl\ntriglycerides: Triglycerides, mg/dl\nhdl: HDL‐cholesterol, mg/dl\ndiabetes: Diabetes mellitus\nhypertension: Hypertension\n\n\n\nIn this paper, there is insufficient information to create the analytic dataset. This is mainly because of not sufficiently defining the covariates and not explicitly explaining the inclusion/exclusion criteria.\nThe authors did incorrect analyses. For example, they didn’t consider survey features. Since we will utilize survey features in our analysis, our results will likely be different than the results shown by the authors in Table 2.",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-1-0-grade",
    "href": "propensityscoreE.html#question-1-0-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 1: [0% grade]",
    "text": "Question 1: [0% grade]\n1(a) Importing dataset\n\nload(file = \"Data/propensityscore/Moon2021.RData\")\n\n1(b) Subsetting according to eligibility\n\n# Age 20+\ndat.analytic &lt;- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars &lt;- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic &lt;- dat.analytic[,vars]\n\n# Complete case\ndat.analytic &lt;- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#&gt; [1] 15404    21\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, hypertension, diabetes mellitus, and survey cycles.\n\nHint 1: the authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nHint 2: Adjust the model for age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, and survey cycle.\nHint 3: Use Publish package to report the odds ratio with the 95% CI and p-value.\n\n\n# Create an indicator variable in the full data\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\n\n# Design setup\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Design-adjusted logistic\nfit.logit &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#&gt;       Variable              Units OddsRatio         CI.95     p-value \n#&gt;       nocturia                 &lt;2       Ref                           \n#&gt;                                2+      1.44   [1.21;1.71]   0.0001496 \n#&gt;            age            [20,40)       Ref                           \n#&gt;                           [40,60)      4.21   [3.05;5.82]     &lt; 1e-04 \n#&gt;                           [60,80)     11.46  [7.89;16.64]     &lt; 1e-04 \n#&gt;                          [80,Inf)     25.28 [17.51;36.50]     &lt; 1e-04 \n#&gt;         gender               Male       Ref                           \n#&gt;                            Female      0.68   [0.58;0.79]     &lt; 1e-04 \n#&gt;           race          Hispanics       Ref                           \n#&gt;                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#&gt;                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#&gt;                       Other races      1.55   [1.05;2.30]   0.0319116 \n#&gt;            bmi                         1.02   [1.01;1.03]   0.0003273 \n#&gt;        smoking                 No       Ref                           \n#&gt;                               Yes      1.74   [1.46;2.07]     &lt; 1e-04 \n#&gt;        alcohol                 No       Ref                           \n#&gt;                               Yes      0.92   [0.59;1.45]   0.7273627 \n#&gt;          sleep                         0.96   [0.90;1.01]   0.1146287 \n#&gt;   tcholesterol                         0.99   [0.99;0.99]     &lt; 1e-04 \n#&gt;  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#&gt;            hdl                         0.99   [0.98;1.00]   0.0416900 \n#&gt;   hypertension                 No       Ref                           \n#&gt;                               Yes      2.73   [2.27;3.29]     &lt; 1e-04 \n#&gt;       diabetes                 No       Ref                           \n#&gt;                               Yes      1.83   [1.51;2.22]     &lt; 1e-04 \n#&gt;   survey.cycle            2005-06       Ref                           \n#&gt;                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#&gt;                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#&gt;                           2011-11      0.82   [0.68;0.99]   0.0398975",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "href": "propensityscoreE.html#question-2-propensity-score-matching-by-dugoff-et-al.-2014-50-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]",
    "text": "Question 2: Propensity score matching by DuGoff et al. (2014) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Question 1) using the propensity score 1:1 matching analysis as per DuGoff et al. (2014) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia &lt;2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare your results with the results reported by the authors. [Expected answer: 2-3 sentences]\n\n\n# your codes here",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "href": "propensityscoreE.html#question-3-propensity-score-matching-by-austin-et-al.-2018-50-grade",
    "title": "Exercise 1 (S)",
    "section": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]",
    "text": "Question 3: Propensity score matching by Austin et al. (2018) [50% grade]\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Questions 1 and 2) using the propensity score 1:4 matching analysis as per Austin et al. (2018) recommendations.\nPlease read the hints carefully:\n\n\nHint 1: You should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are: age, gender, race, bmi, smoking, alcohol, sleep, tcholesterol, triglycerides, hdl, hypertension, diabetes, systolic, diastolic, and survey cycle.\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia &lt;2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing. Remember, you need to multiply matching weights and survey weights to get survey-based estimates.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with a 95% CI. For step 4, you should utilize the survey feature as the design (NOT covariates).\n\n\nHint 2: Compare the results with Question 2. What’s the overall conclusion? [Expected answer: 2-3 sentences]\n\n\n# your codes here",
    "crumbs": [
      "Propensity score",
      "Exercise 1 (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html",
    "href": "propensityscoreEsolution.html",
    "title": "Exercise 1 solution (S)",
    "section": "",
    "text": "Problem 1:\nWe will use the article by Moon et al. (2021). We will reproduce some results from the article. The authors aggregated 4 NHANES cycles 2005-12 to create their analytic dataset. The full dataset contains 40,790 subjects with the following relevant variables for this exercise:\nSurvey information\nOutcome variable\nExposure\nConfounders and other variables\nTwo important warnings before we start:",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-1",
    "href": "propensityscoreEsolution.html#problem-1",
    "title": "Exercise 1 solution (S)",
    "section": "",
    "text": "1(a) Importing dataset\n\nload(file = \"Data/propensityscore/Moon2021.RData\")\nls()\n#&gt; [1] \"dat.full\"\n\n1(b) Subsetting according to eligibility\n\n# Age 20+\ndat.analytic &lt;- dat.full[complete.cases(dat.full$age),]\n\n# Complete outcome and exposure information\ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$cvd),] \ndat.analytic &lt;- dat.analytic[complete.cases(dat.analytic$nocturia),] \n\n# Keep important variables only\nvars &lt;- c(\n  # Survey features\n  \"SEQN\", \"strata\", \"psu\", \"survey.weight\", \n  \n  # Survey cycle\n  \"survey.cycle\", \n  \n  # Binary exposure\n  \"nocturia\",\n  \n  # Outcome\n  \"cvd\",\n  \n  # Covariates\n  \"age\", \"gender\", \"race\" , \"smoking\", \"alcohol\", \"sleep\", \"bmi\", \"diabetes\", \n  \"hypertension\", \"tcholesterol\", \"triglycerides\", \"hdl\", \"systolic\", \"diastolic\")\n\ndat.analytic &lt;- dat.analytic[,vars]\n\n# Complete case\ndat.analytic &lt;- na.omit(dat.analytic) #  N = 15,404 (numbers do not match with Fig 1)\ndim(dat.analytic)\n#&gt; [1] 15404    21\n\n1(c) Run the design-adjusted logistic regression\nCreate the first column of Table 2 of the article, i.e., explore the relationship between binary nocturia and CVD among adults aged 20 years and more. Adjust the model for the following covariates: age, gender, race, body mass index, smoking status, alcohol consumption, sleep duration, total cholesterol, triglycerides, HDL-cholesterol, hypertension, diabetes mellitus, and survey cycles.\nNote:\n\nThe authors did not utilize the survey features (e.g., strata, psu, survey weights). But you should utilize the survey features to answer this question.\nYou must create your design on the full data and then subset the design.\nReport the odds ratio with the 95% CI.\n\n\n# Create an indicator variable in the full data\ndat.full$indicator &lt;- 1\ndat.full$indicator[dat.full$SEQN %in% dat.analytic$SEQN] &lt;- 0\n\n# Design setup\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full, nest = TRUE)\n\n# Subset the design\nsvy.design &lt;- subset(svy.design0, indicator == 0)\n\n# Design-adjusted logistic\nfit.logit &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia + age + gender + race + bmi + \n                      smoking + alcohol + sleep + tcholesterol + triglycerides + \n                      hdl + hypertension + diabetes + survey.cycle, \n                    family = binomial, design = svy.design)\npublish(fit.logit)\n#&gt;       Variable              Units OddsRatio         CI.95     p-value \n#&gt;       nocturia                 &lt;2       Ref                           \n#&gt;                                2+      1.44   [1.21;1.71]   0.0001496 \n#&gt;            age            [20,40)       Ref                           \n#&gt;                           [40,60)      4.21   [3.05;5.82]     &lt; 1e-04 \n#&gt;                           [60,80)     11.46  [7.89;16.64]     &lt; 1e-04 \n#&gt;                          [80,Inf)     25.28 [17.51;36.50]     &lt; 1e-04 \n#&gt;         gender               Male       Ref                           \n#&gt;                            Female      0.68   [0.58;0.79]     &lt; 1e-04 \n#&gt;           race          Hispanics       Ref                           \n#&gt;                Non-Hispanic White      1.32   [1.10;1.57]   0.0036168 \n#&gt;                Non-Hispanic Black      1.15   [0.92;1.44]   0.2362499 \n#&gt;                       Other races      1.55   [1.05;2.30]   0.0319116 \n#&gt;            bmi                         1.02   [1.01;1.03]   0.0003273 \n#&gt;        smoking                 No       Ref                           \n#&gt;                               Yes      1.74   [1.46;2.07]     &lt; 1e-04 \n#&gt;        alcohol                 No       Ref                           \n#&gt;                               Yes      0.92   [0.59;1.45]   0.7273627 \n#&gt;          sleep                         0.96   [0.90;1.01]   0.1146287 \n#&gt;   tcholesterol                         0.99   [0.99;0.99]     &lt; 1e-04 \n#&gt;  triglycerides                         1.00   [1.00;1.00]   0.4801803 \n#&gt;            hdl                         0.99   [0.98;1.00]   0.0416900 \n#&gt;   hypertension                 No       Ref                           \n#&gt;                               Yes      2.73   [2.27;3.29]     &lt; 1e-04 \n#&gt;       diabetes                 No       Ref                           \n#&gt;                               Yes      1.83   [1.51;2.22]     &lt; 1e-04 \n#&gt;   survey.cycle            2005-06       Ref                           \n#&gt;                           2007-08      0.84   [0.65;1.07]   0.1644272 \n#&gt;                           2009-10      0.91   [0.73;1.12]   0.3793696 \n#&gt;                           2011-11      0.82   [0.68;0.99]   0.0398975",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-2-propensity-score-matching-by-dugoff-et-al.-2014",
    "href": "propensityscoreEsolution.html#problem-2-propensity-score-matching-by-dugoff-et-al.-2014",
    "title": "Exercise 1 solution (S)",
    "section": "Problem 2: Propensity score matching by DuGoff et al. (2014)",
    "text": "Problem 2: Propensity score matching by DuGoff et al. (2014)\n2(a): 1:1 matching\nCreate the second column of Table 2 (exploring the relationship between binary nocturia and CVD; the same exposure and outcome used in Problem 1) using the propensity score 1:1 matching analysis as per DuGoff et al. (2014) recommendations.\nYou should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as covariates. Other covariates for the PS model are the covariates used in 1(c).\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with a control subject (nocturia &lt;2 times) without replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with the 95% CI. You should utilize the survey feature as the design (NOT covariates).\n\nStep 1\n\n## Specify the PS model to estimate propensity scores\nps.formula &lt;- as.formula(I(nocturia==\"2+\") ~ age + gender + race + bmi + smoking + \n                           alcohol + sleep + tcholesterol + triglycerides + hdl + \n                           hypertension + diabetes + survey.cycle + \n                           psu + strata + survey.weight)\n\n## Fit the PS model\nps.fit &lt;- glm(ps.formula, data = dat.analytic, family = binomial(\"logit\"))\ndat.analytic$PS &lt;- predict(ps.fit, type = \"response\")\n#summary(dat.analytic$PS)\n\n## Caliper: 0.2*sd(logit of PS)\ncaliper &lt;- 0.2*sd(log(dat.analytic$PS/(1-dat.analytic$PS)))\n#caliper\n\nStep 2\n\n# Match exposed and unexposed subjects \nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, \n                     data = dat.analytic,\n                     distance = dat.analytic$PS, \n                     method = \"nearest\", \n                     replace = FALSE,\n                     caliper = caliper, \n                     ratio = 1)\ndat.analytic$PS &lt;- match.obj$distance\n#summary(match.obj$distance)\n\n## See how many matched\n#match.obj\n\n## Extract matched data\nmatched.data &lt;- match.data(match.obj) \n\nStep 3\n\n## Summary of PS by the exposure\n#tapply(matched.data$distance, matched.data$nocturia, summary)\n\n## Balance checking\nvars &lt;- c(\"age\", \"gender\", \"race\", \"bmi\", \"smoking\", \"alcohol\", \"sleep\", \"tcholesterol\",\n         \"triglycerides\", \"hdl\", \"hypertension\", \"diabetes\", \"survey.cycle\")\n\n\ntab1m &lt;- CreateTableOne(vars = vars, strata = \"nocturia\", data = matched.data, \n                           test = F, includeNA = F)\nprint(tab1m, smd = TRUE, format = \"p\") # All SMDs &lt;0.1\n#&gt;                            Stratified by nocturia\n#&gt;                             &lt;2              2+              SMD   \n#&gt;   n                           4502            4502                \n#&gt;   age (%)                                                    0.047\n#&gt;      [20,40)                  19.5            20.2                \n#&gt;      [40,60)                  32.9            31.7                \n#&gt;      [60,80)                  39.2            38.7                \n#&gt;      [80,Inf)                  8.4             9.5                \n#&gt;   gender = Female (%)         50.7            49.8           0.017\n#&gt;   race (%)                                                   0.020\n#&gt;      Hispanics                24.4            24.3                \n#&gt;      Non-Hispanic White       46.0            45.3                \n#&gt;      Non-Hispanic Black       25.4            26.0                \n#&gt;      Other races               4.2             4.4                \n#&gt;   bmi (mean (SD))            29.96 (7.07)    30.18 (7.06)    0.031\n#&gt;   smoking = Yes (%)           57.5            57.0           0.009\n#&gt;   alcohol = Yes (%)            4.0             3.5           0.022\n#&gt;   sleep (mean (SD))           6.77 (1.39)     6.73 (1.60)    0.024\n#&gt;   tcholesterol (mean (SD))  199.01 (42.48)  197.51 (44.94)   0.034\n#&gt;   triglycerides (mean (SD)) 163.55 (140.06) 164.00 (144.94)  0.003\n#&gt;   hdl (mean (SD))            53.44 (16.85)   53.20 (16.81)   0.014\n#&gt;   hypertension = Yes (%)      47.3            48.7           0.029\n#&gt;   diabetes = Yes (%)          15.3            17.4           0.058\n#&gt;   survey.cycle (%)                                           0.030\n#&gt;      2005-06                  22.8            22.0                \n#&gt;      2007-08                  27.5            27.3                \n#&gt;      2009-10                  28.2            28.0                \n#&gt;      2011-11                  21.6            22.8\n\nStep 4\n\n## Setup the design with survey features\ndat.full$matched &lt;- 0\ndat.full$matched[dat.full$SEQN %in% matched.data$SEQN] &lt;- 1\n\n## Survey setup for full data \nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n## Outcome model\nfit.psm &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia, design = w.design.m, family = binomial)\npublish(fit.psm)\n#&gt;  Variable Units OddsRatio       CI.95     p-value \n#&gt;  nocturia    &lt;2       Ref                         \n#&gt;              2+      1.33 [1.14;1.57]   0.0007554\n\n2(b): Interpretation\nCompare your results with the results reported by the authors. [Expected answer: 1-2 sentences]\nIn the US population aged 20 years or more, the odds of CVD was 33% higher among adults with \\(\\ge 2\\) times nocturia compared to PS matched controls with &lt;2 nocturia. Unlike the results shown by the authors in Table 2, this odds ratio of 1.33 is a population-level estimate (i.e., the estimate is PATT).",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "propensityscoreEsolution.html#problem-3-propensity-score-matching-by-austin-et-al.-2018",
    "href": "propensityscoreEsolution.html#problem-3-propensity-score-matching-by-austin-et-al.-2018",
    "title": "Exercise 1 solution (S)",
    "section": "Problem 3: Propensity score matching by Austin et al. (2018)",
    "text": "Problem 3: Propensity score matching by Austin et al. (2018)\n3(a): 1:4 matching\nRepeat Problem 2(a), i.e., create the second column of Table 2 (exploring the relationship between binary nocturia and CVD), but using the propensity score 1:4 matching analysis as per Austin et al. (2018) recommendations.\nYou should consider all four steps in the propensity score (PS) analysis:\n\nStep 1: Fit the PS model by considering survey features as design, i.e., fit the design-adjusted PS model. Other covariates for the PS model are the covariates used in 1(c).\nStep 2: Match an exposed subject (nocturia \\(\\ge2\\) times) with 4 control subjects (nocturia &lt;2 times) with replacement within the caliper of 0.2 times the standard deviation of the logit of PS. Set your seed to 123.\nStep 3: Balance checking using SMD. Consider SMD &lt;0.1 as a good covariate balancing. Remember, you need to consider matching weights in checking the covariate balance.\nStep 4: Fit the outcome model on the matched data. If needed, adjust for imbalanced covariates in the outcome model. Report the odds ratio with the 95% CI. You should utilize the survey feature as the design (NOT covariates).\n\nNote:\n\nFor step 4, you need to multiply matching weights and survey weights when creating your design. After creating the design with the new weight, subset the design for the matched sample. This step is required to get survey-based estimates.\n\nStep 1\n\n## Specify the PS model to estimate propensity scores\nps.formula3 &lt;- as.formula(I(nocturia==\"2+\") ~ age + gender + race + bmi + smoking + \n                           alcohol + sleep + tcholesterol + triglycerides + hdl + \n                           hypertension + diabetes + survey.cycle)\n\nps.design &lt;- svydesign(id = ~psu, weights = ~survey.weight, strata = ~strata,\n                       data = dat.analytic, nest = TRUE)\n\n## Fit the PS model\nps.fit3 &lt;- svyglm(ps.formula3, design = ps.design, family = binomial)\ndat.analytic$PS3 &lt;- predict(ps.fit3, type = \"response\")\n#summary(dat.analytic$PS3)\n\n## Caliper: 0.2*sd(logit of PS)\ncaliper3 &lt;- 0.2*sd(log(dat.analytic$PS3/(1-dat.analytic$PS3)))\n#caliper3\n\nStep 2\n\n# Match exposed and unexposed subjects \nset.seed(123)\nmatch.obj3 &lt;- matchit(ps.formula3, \n                      data = dat.analytic,\n                      distance = dat.analytic$PS3, \n                      method = \"nearest\", \n                      replace = TRUE,\n                      caliper = caliper3, \n                      ratio = 4)\ndat.analytic$PS3 &lt;- match.obj$distance\n#summary(match.obj$distance)\n\n## See how many matched\n#match.obj3\n\n## Extract matched data\nmatched.data3 &lt;- match.data(match.obj3) \n\nStep 3\n\n## Summary of PS by the exposure\n#tapply(matched.data3$distance, matched.data3$nocturia, summary)\n\n## Balance checking\nvars &lt;- c(\"age\", \"gender\", \"race\", \"bmi\", \"smoking\", \"alcohol\", \"sleep\", \"tcholesterol\",\n         \"triglycerides\", \"hdl\", \"hypertension\", \"diabetes\", \"survey.cycle\")\n\n## Setup the design with survey features\ndat.full$matched &lt;- 0\ndat.full$matched[dat.full$SEQN %in% matched.data3$SEQN] &lt;- 1\n\n## Merging PS weights in full data\ndat.full$psweight &lt;- 0\ndat.full$psweight[dat.full$SEQN %in% matched.data3$SEQN] &lt;- matched.data3$weights\n\n## Survey setup for full data with ps weights - for balance checking\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~psweight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\ntab13m &lt;- svyCreateTableOne(vars = vars, strata = \"nocturia\", data = w.design.m, \n                           test = F, includeNA = F)\nprint(tab13m, smd = TRUE, format = \"p\") # All SMDs &lt;0.1\n#&gt;                            Stratified by nocturia\n#&gt;                             &lt;2               2+               SMD   \n#&gt;   n                          7185.0           4544.0                \n#&gt;   age (%)                                                      0.053\n#&gt;      [20,40)                   18.4             20.0                \n#&gt;      [40,60)                   33.3             31.4                \n#&gt;      [60,80)                   39.3             38.9                \n#&gt;      [80,Inf)                   9.1              9.7                \n#&gt;   gender = Female (%)          51.8             49.9           0.038\n#&gt;   race (%)                                                     0.015\n#&gt;      Hispanics                 24.3             24.1                \n#&gt;      Non-Hispanic White        44.6             45.1                \n#&gt;      Non-Hispanic Black        26.9             26.5                \n#&gt;      Other races                4.2              4.4                \n#&gt;   bmi (mean (SD))             29.97 (7.09)     30.21 (7.08)    0.035\n#&gt;   smoking = Yes (%)            58.3             57.2           0.022\n#&gt;   alcohol = Yes (%)             4.0              3.6           0.023\n#&gt;   sleep (mean (SD))            6.73 (1.38)      6.73 (1.60)    0.004\n#&gt;   tcholesterol (mean (SD))   197.49 (42.18)   197.44 (45.02)   0.001\n#&gt;   triglycerides (mean (SD))  159.71 (129.75)  163.66 (144.57)  0.029\n#&gt;   hdl (mean (SD))             53.27 (16.71)    53.25 (16.82)   0.001\n#&gt;   hypertension = Yes (%)       47.6             49.2           0.031\n#&gt;   diabetes = Yes (%)           16.9             18.0           0.028\n#&gt;   survey.cycle (%)                                             0.016\n#&gt;      2005-06                   21.4             22.0                \n#&gt;      2007-08                   27.7             27.2                \n#&gt;      2009-10                   28.0             27.8                \n#&gt;      2011-11                   23.0             23.0\n\nStep 4\n\n## Survey setup for full data with new weight = survey weights * ps weights\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight*psweight, \n                      data = dat.full, nest = TRUE)\n\n## Subset matched data\nw.design.m &lt;- subset(w.design0, matched == 1)\n\n## Outcome model\nfit.psm &lt;- svyglm(I(cvd == \"Yes\") ~ nocturia, design = w.design.m, family = binomial)\npublish(fit.psm)\n#&gt;  Variable Units OddsRatio       CI.95     p-value \n#&gt;  nocturia    &lt;2       Ref                         \n#&gt;              2+      1.33 [1.14;1.54]   0.0005248\n\n3(b): Interpretation\nCompare the results with Problem 2. What’s the overall conclusion? [Expected answer: 2-3 sentences]\nIn this exercise, we observed that the association between nocturia and CVD is approximately the same using DuGoff et al.’s 1:1 matching and Austin et al.’s 1:4 matching. At the population-level, we observed 33% higher odds of CVD among adults with \\(\\ge 2\\) times nocturia than PS matched controls with &lt;2 nocturia.",
    "crumbs": [
      "Propensity score",
      "Exercise 1 solution (S)"
    ]
  },
  {
    "objectID": "machinelearning.html",
    "href": "machinelearning.html",
    "title": "Machine learning (ML)",
    "section": "",
    "text": "Background\nThe chapter encompasses a series of instructional content that sequentially explores various facets of predictive modeling and machine learning, connecting them with a previous chapter. Beginning with a tutorial that revisits the application of regression for predicting continuous outcomes, it underscores the importance of understanding prediction error and overfitting, and introduces foundational machine learning concepts. The subsequent tutorial emphasizes the pivotal role of data splitting in predictive modeling, illustrating how to partition data into training and test sets and evaluate model performance across different data scenarios. Moving forward, the concept of cross-validation is explored, detailing the k-fold cross-validation method and demonstrating its implementation both manually and using the caret package. Another tutorial navigates through predicting binary outcomes using logistic regression, evaluating model performance using various metrics, and employing k-fold cross-validation.\nThe series then delves into supervised learning, exploring regularization techniques, decision trees, and ensemble methods, while employing various model evaluation metrics and cross-validation techniques. Lastly, unsupervised learning is introduced with a focus on the k-means clustering algorithm, discussing its implementation, determining the optimal number of clusters, and addressing associated challenges. Throughout, the tutorials provide practical examples, code snippets, and visual aids, offering a comprehensive and applied exploration of predictive modeling and machine learning concepts.",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning.html#background",
    "href": "machinelearning.html#background",
    "title": "Machine learning (ML)",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning.html#overview-of-tutorials",
    "href": "machinelearning.html#overview-of-tutorials",
    "title": "Machine learning (ML)",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nBuilding upon the foundational concepts introduced in a previous chapter about prediction research, this chapter takes our understanding to the next level. We’ve already explored the fundamentals of propensity score methods, and now it is time to harness the power of machine learning and prediction techniques within a causal inference context, a journey that will ultimately lead us to the concept of double robust estimation methods, such as TMLE.\nBefore we embark on this exciting journey, we will bridge the gap by dedicating this chapter to a deeper exploration of machine learning. By discussing various types of machine learning algorithms and diving into the intricacies of predictive modeling, we aim to provide you with a robust foundation.\n\nRevisiting: Explore Relationships for Continuous Outcomes\nIn this tutorial, the focus is on utilizing regression to predict continuous outcomes, specifically employing multiple linear regression to construct an initial prediction model. The tutorial revisits fundamental concepts related to prediction and introduces foundational ideas pertinent to machine learning, all while using a distinct dataset compared to previous tutorials. The process involves loading a dataset, defining variables, and fitting a model using linear regression. Subsequent sections delve into the creation of a design matrix, obtaining predictions, and measuring prediction error through various metrics like R^2 and RMSE. The tutorial also addresses the critical concept of overfitting, discussing its causes, consequences, and potential solutions, such as internal and external validation methods.\n\n\nRevisiting: Data Spliting\nThis tutorial emphasizes the crucial concept of data splitting in the context of predictive modeling and machine learning, utilizing a different dataset than previous tutorials. The process begins with loading a dataset and then strategically splitting it into training and test subsets, ensuring a robust approach to model validation. A model is trained using the training data, and its performance is evaluated using various metrics, such as R^2 and RMSE, through a custom function that extracts these performance measures. This function facilitates the evaluation of the model’s predictive accuracy and fit by applying it to different datasets (training, test, and the entire dataset), thereby enabling a comprehensive understanding of the model’s performance across different data scenarios.\n\n\nRevisiting: Cross-vaildation\nThis tutorial delves into the concept of cross-validation, a pivotal technique in predictive modeling and machine learning, using a distinct dataset for illustrative purposes. The process of k-fold cross-validation is explored, wherein the data is partitioned into ‘k’ subsets, and the model is trained ‘k’ times, each time using a different subset as the test set and the remaining data as the training set. This method is employed to assess the model’s predictive performance and to mitigate the risk of results being dependent on the initial data split. The tutorial demonstrates both manual calculations for individual folds and the utilization of the caret package to automate the cross-validation process, thereby providing a comprehensive overview of the method.\n\n\nRevisiting: Explore Relationships for Binary Outcomes\nThe tutorial navigates through the concept of predicting binary outcomes using logistic regression, emphasizing the application of various model evaluation metrics and methodologies in a machine learning context. It begins by ensuring that the outcome variable is treated as a factor and then proceeds to model fitting, where logistic regression is applied to predict a binary outcome. The model’s predictive performance is evaluated using metrics like the Area Under the Curve (AUC) and the Brier Score, which respectively assess the model’s classification accuracy and the mean squared difference between predicted probabilities and the actual outcomes. Furthermore, the tutorial explores k-fold cross-validation using the caret package, providing a robust method to assess the model’s predictive performance while avoiding overfitting. It also touches upon variable selection using stepwise regression with the Akaike Information Criterion (AIC) as a selection criterion.\n\n\nSupervised Learning\nThis tutorial delves into the realm of supervised learning, exploring beyond statistical regression and introducing various machine learning methods tailored for both continuous and binary outcomes. The tutorial explores different regularization techniques, such as LASSO, Ridge, and Elastic Net, which are used to prevent overfitting by penalizing large coefficients in regression models. It also introduces decision trees (CART), which provide a flexible, hierarchical approach to modeling data, and can automatically incorporate non-linear effects and interactions. The tutorial further explores ensemble methods, which combine predictions from multiple models to improve predictive accuracy. Two types of ensemble methods are discussed: Type I, which trains the same model on different samples of the data (e.g., bagging and boosting), and Type II, which trains different models on the same data (e.g., Super Learner). Various model evaluation metrics and cross-validation techniques are utilized throughout to assess and enhance the predictive performance of the models.\n\n\nUnsupervised Learning\nThe tutorial introduces unsupervised learning, with a focus on clustering, a technique that categorizes data into distinct groups based on similarity without using predefined labels. The k-means clustering algorithm is highlighted, which partitions data into k groups by minimizing within-cluster variation, typically using the sum of squares of Euclidean distances. The algorithm iteratively assigns data points to clusters based on the mean of the data points in each cluster and recalculates the cluster means until the cluster assignments no longer change. Various examples illustrate how to apply k-means clustering to different datasets and variable combinations. The tutorial also discusses determining the optimal number of clusters, k, and addresses challenges such as the influence of outliers and the sensitivity to the initial assignment of cluster means.\n\n\nMachine Learning for Health Survey Data using NHIS data\nThis tutorial provides a step-by-step guide to fitting machine learning models with health survey data, specifically using the National Health Interview Survey (NHIS) 2016 dataset to predict high impact chronic pain (HICP) among adults aged 65 years or older. The tutorial covers the use of (1) LASSO and (2) random forest models with sampling weights for population-level predictions. It also discusses the split-sample approach for internal validation, though it acknowledges that cross-validation and bootstrapping may be better alternatives. The tutorial includes the exploration of the analytic dataset, weight normalization, split-sample creation, defining regression formulas, fitting LASSO models with optimal lambda, and evaluating model performance with metrics such as AUC, calibration slope, and Brier score. The same process is then repeated for fitting random forest models, and a variable importance plot is generated to identify influential predictors. Finally, a performance comparison table is provided.\n\n\nReplicate Results from a Published Article\nThe tutorial guides users on implementing machine learning techniques using health survey data, specifically for predicting high impact chronic pain (HICP). It seeks to replicate results from a 2023 article, which utilized the National Health Interview Survey (NHIS) 2016 dataset. For simplicity, complete case data was used in this tutorial. In the original research, the authors developed prediction models for HICP and evaluated their performance within specific sociodemographic groups, such as gender, age groups, and race/ethnicity. They adopted LASSO and random forest models, applying 5-fold cross-validation. They also factored in survey weights in both models to achieve population-level predictions.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences",
    "crumbs": [
      "Machine learning (ML)"
    ]
  },
  {
    "objectID": "machinelearning0.html",
    "href": "machinelearning0.html",
    "title": "Concepts (L)",
    "section": "",
    "text": "Machine learning\nMachine learning focuses on developing algorithms and models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. In epidemiology, machine learning has several important uses and applications. This section is a very basic introduction to machine learning for Epidemiology.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#reading-list",
    "href": "machinelearning0.html#reading-list",
    "title": "Concepts (L)",
    "section": "Reading list",
    "text": "Reading list\nKey reference\n\n(Bi et al. 2019)\n\nFollowing ate optional but useful references\n\n(Karim 2021)\n(Liu et al. 2019)\n(Kuhn et al. 2013)\n(James et al. 2013)\n(Vittinghoff et al. 2012)\n(Steyerberg 2019)",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#video-lessons",
    "href": "machinelearning0.html#video-lessons",
    "title": "Concepts (L)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning Reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning Terminologies\n\n\n\nWhat is included in this Video Lesson:\n\nReference 0:08\nTypes of Epidemiological models 0:32\nAnalyzing Epidemiological Study data 2:44\nMachine learning 5:42\nTerminologies 9:15\nClassification of Machine learning 12:55\nClassification of Supervised learning 17:30\nOther classifications 18:42\nPopular algorithms 20:06\nDecision tree 20:50\nShrinkage Methods 26:37\nEnsemble methods 37:04\nVariable Importance measure 40:20\nEpidemiologic applications 44:29\nFuture Reading 53:17\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#video-lesson-slides",
    "href": "machinelearning0.html#video-lesson-slides",
    "title": "Concepts (L)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#links",
    "href": "machinelearning0.html#links",
    "title": "Concepts (L)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning0.html#references",
    "href": "machinelearning0.html#references",
    "title": "Concepts (L)",
    "section": "References",
    "text": "References\n\n\n\n\nBi, Qifang, Katherine E Goodman, Joshua Kaminsky, and Justin Lessler. 2019. “What Is Machine Learning? A Primer for the Epidemiologist.” American Journal of Epidemiology 188 (12): 2222–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKarim, Ehsan. 2021. “Understanding Basics and Usage of Machine Learning in Medical Literature.” 2021. https://ehsanx.github.io/into2ML/.\n\n\nKuhn, Max, Kjell Johnson, Max Kuhn, and Kjell Johnson. 2013. “Over-Fitting and Model Tuning.” Applied Predictive Modeling, 61–92.\n\n\nLiu, Yun, Po-Hsuan Cameron Chen, Jonathan Krause, and Lily Peng. 2019. “How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature.” Jama 322 (18): 1806–16.\n\n\nSteyerberg, Ewout W. 2019. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. Vol. 2. Springer.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, Charles E McCulloch, Eric Vittinghoff, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. “Predictor Selection.” Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models, 395–429.",
    "crumbs": [
      "Machine learning (ML)",
      "Concepts (L)"
    ]
  },
  {
    "objectID": "machinelearning1.html",
    "href": "machinelearning1.html",
    "title": "Continuous outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction model.\nLoad dataset\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n  \n\n\n\nPrediction for length of stay\nNow, we show the regression fitting when outcome is continuous (length of stay).\nVariables\n\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#&gt;  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#&gt;  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#&gt;  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#&gt; [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#&gt; [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#&gt; [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#&gt; [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#&gt; [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#&gt; [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#&gt; [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#&gt; [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#&gt; [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#&gt; [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#&gt; [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#&gt; [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#&gt; [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#&gt; [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula1 &lt;- as.formula(paste(\"Length.of.Stay~ \", \n                               paste(baselinevars, \n                                     collapse = \"+\")))\nsaveRDS(out.formula1, file = \"Data/machinelearning/form1.RDS\")\nfit1 &lt;- lm(out.formula1, data = ObsData)\nrequire(Publish)\nadj.fit1 &lt;- publish(fit1, digits=1)$regressionTable\n\n\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nadj.fit1\n\n\n  \n\n\n\nDesign Matrix\n\nNotations\n\nn is number of observations\np is number of covariates\n\n\n\nExpands factors to a set of dummy variables.\n\ndim(ObsData)\n#&gt; [1] 5735   52\nlength(attr(terms(out.formula1), \"term.labels\"))\n#&gt; [1] 50\n\n\nhead(model.matrix(fit1))\n#&gt;   (Intercept) Disease.categoryCHF Disease.categoryOther Disease.categoryMOSF\n#&gt; 1           1                   0                     1                    0\n#&gt; 2           1                   0                     0                    1\n#&gt; 3           1                   0                     0                    1\n#&gt; 4           1                   0                     0                    0\n#&gt; 5           1                   0                     0                    1\n#&gt; 6           1                   0                     1                    0\n#&gt;   CancerLocalized (Yes) CancerMetastatic Cardiovascular1 Congestive.HF1\n#&gt; 1                     1                0               0              0\n#&gt; 2                     0                0               1              1\n#&gt; 3                     1                0               0              0\n#&gt; 4                     0                0               0              0\n#&gt; 5                     0                0               0              0\n#&gt; 6                     0                0               0              1\n#&gt;   Dementia1 Psychiatric1 Pulmonary1 Renal1 Hepatic1 GI.Bleed1 Tumor1\n#&gt; 1         0            0          1      0        0         0      1\n#&gt; 2         0            0          0      0        0         0      0\n#&gt; 3         0            0          0      0        0         0      1\n#&gt; 4         0            0          0      0        0         0      0\n#&gt; 5         0            0          0      0        0         0      0\n#&gt; 6         0            0          1      0        0         0      0\n#&gt;   Immunosupperssion1 Transfer.hx1 MI1 age[50,60) age[60,70) age[70,80)\n#&gt; 1                  0            0   0          0          0          1\n#&gt; 2                  1            1   0          0          0          1\n#&gt; 3                  1            0   0          0          0          0\n#&gt; 4                  1            0   0          0          0          1\n#&gt; 5                  0            0   0          0          1          0\n#&gt; 6                  0            0   0          0          0          0\n#&gt;   age[80, Inf) sexFemale       edu DASIndex APACHE.score Glasgow.Coma.Score\n#&gt; 1            0         0 12.000000 23.50000           46                  0\n#&gt; 2            0         1 12.000000 14.75195           50                  0\n#&gt; 3            0         1 14.069916 18.13672           82                  0\n#&gt; 4            0         1  9.000000 22.92969           48                  0\n#&gt; 5            0         0  9.945259 21.05078           72                 41\n#&gt; 6            1         1  8.000000 17.50000           38                  0\n#&gt;   blood.pressure         WBC Heart.rate Respiratory.rate Temperature\n#&gt; 1             41 22.09765620        124               10    38.69531\n#&gt; 2             63 28.89843750        137               38    38.89844\n#&gt; 3             57  0.04999542        130               40    36.39844\n#&gt; 4             55 23.29687500         58               26    35.79688\n#&gt; 5             65 29.69921880        125               27    34.79688\n#&gt; 6            115 18.00000000        134               36    39.19531\n#&gt;   PaO2vs.FIO2  Albumin Hematocrit Bilirubin Creatinine Sodium Potassium PaCo2\n#&gt; 1     68.0000 3.500000   58.00000 1.0097656  1.1999512    145  4.000000    40\n#&gt; 2    218.3125 2.599609   32.50000 0.6999512  0.5999756    137  3.299805    34\n#&gt; 3    275.5000 3.500000   21.09766 1.0097656  2.5996094    146  2.899902    16\n#&gt; 4    156.6562 3.500000   26.29688 0.3999634  1.6999512    117  5.799805    30\n#&gt; 5    478.0000 3.500000   24.00000 1.0097656  3.5996094    126  5.799805    17\n#&gt; 6    184.1875 3.099609   30.50000 1.0097656  1.3999023    138  5.399414    68\n#&gt;         PH   Weight DNR.statusYes Medical.insuranceMedicare\n#&gt; 1 7.359375 64.69995             0                         1\n#&gt; 2 7.329102 45.69998             0                         0\n#&gt; 3 7.359375  0.00000             0                         0\n#&gt; 4 7.459961 54.59998             0                         0\n#&gt; 5 7.229492 78.39996             1                         1\n#&gt; 6 7.299805 54.89999             0                         1\n#&gt;   Medical.insuranceMedicare & Medicaid Medical.insuranceNo insurance\n#&gt; 1                                    0                             0\n#&gt; 2                                    0                             0\n#&gt; 3                                    0                             0\n#&gt; 4                                    0                             0\n#&gt; 5                                    0                             0\n#&gt; 6                                    0                             0\n#&gt;   Medical.insurancePrivate Medical.insurancePrivate & Medicare\n#&gt; 1                        0                                   0\n#&gt; 2                        0                                   1\n#&gt; 3                        1                                   0\n#&gt; 4                        0                                   1\n#&gt; 5                        0                                   0\n#&gt; 6                        0                                   0\n#&gt;   Respiratory.DiagYes Cardiovascular.DiagYes Neurological.DiagYes\n#&gt; 1                   1                      1                    0\n#&gt; 2                   0                      0                    0\n#&gt; 3                   0                      1                    0\n#&gt; 4                   1                      0                    0\n#&gt; 5                   0                      1                    0\n#&gt; 6                   1                      0                    0\n#&gt;   Gastrointestinal.DiagYes Renal.DiagYes Metabolic.DiagYes Hematologic.DiagYes\n#&gt; 1                        0             0                 0                   0\n#&gt; 2                        0             0                 0                   0\n#&gt; 3                        0             0                 0                   0\n#&gt; 4                        0             0                 0                   0\n#&gt; 5                        0             0                 0                   0\n#&gt; 6                        0             0                 0                   0\n#&gt;   Sepsis.DiagYes Trauma.DiagYes Orthopedic.DiagYes raceblack raceother\n#&gt; 1              0              0                  0         0         0\n#&gt; 2              1              0                  0         0         0\n#&gt; 3              0              0                  0         0         0\n#&gt; 4              0              0                  0         0         0\n#&gt; 5              0              0                  0         0         0\n#&gt; 6              0              0                  0         0         0\n#&gt;   income$25-$50k income&gt; $50k incomeUnder $11k RHC.use\n#&gt; 1              0            0                1       0\n#&gt; 2              0            0                1       1\n#&gt; 3              1            0                0       1\n#&gt; 4              0            0                0       0\n#&gt; 5              0            0                1       1\n#&gt; 6              0            0                1       0\ndim(model.matrix(fit1))\n#&gt; [1] 5735   64\np &lt;- dim(model.matrix(fit1))[2] # intercept + slopes\np\n#&gt; [1] 64\n\nObtain prediction\n\nobs.y &lt;- ObsData$Length.of.Stay\nsummary(obs.y)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    2.00    7.00   14.00   21.56   25.00  394.00\n# Predict the above fit on ObsData data\npred.y1 &lt;- predict(fit1, ObsData)\nsummary(pred.y1)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  -32.76   16.62   21.96   21.56   26.73   42.67\nn &lt;- length(pred.y1)\nn\n#&gt; [1] 5735\nplot(obs.y,pred.y1)\nlines(lowess(obs.y,pred.y1), col = \"red\")\n\n\n\n\n\n\n\nMeasuring prediction error\nPrediction error measures how well the model can predict the outcome for new data that were not used in developing the prediction model.\n\nBias reduced for models with more variables\nUnimportant variables lead to noise / variability\nBias variance trade-off / need penalization\n\nR2\nThe provided information describes a statistical context involving a dataset of n values, \\(y_1, ..., y_n\\) (referred to as \\(y_i\\) or as a vector \\(y = [y_1,...,y_n]^T\\)), each paired with a fitted value \\(f_1,...,f_n\\) (denoted as \\(f_i\\) or sometimes \\(\\hat{y_i}\\), and as a vector \\(f\\)). The residuals, represented as \\(e_i\\), are defined as the differences between the observed and the fitted values: $ e_i = y_i − f_i$\nThe mean of the observed data is denoted by \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i \\]\nThe variability of the dataset can be quantified using two sums of squares formulas: 1. Residual Sum of Squares (SSres) or SSE: It quantifies the variance remaining in the data after fitting a model, calculated as: \\[ SS_{res} = \\sum_{i}(y_i - f_i)^2 = \\sum_{i}e_i^2 \\] 2. Total Sum of Squares (SStot) or SST: It represents the total variance in the observed data, calculated as: \\[ SS_{tot} = \\sum_{i}(y_i - \\bar{y})^2 \\]\nThe Coefficient of Determination (R²) or R.2, which provides a measure of how well the model’s predictions match the observed data, is defined as: \\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nIn the ideal scenario where the model fits the data perfectly, we have \\(SS_{res} = 0\\) and thus \\(R^2 = 1\\). Conversely, a baseline model, which always predicts the mean \\(\\bar{y}\\) of the observed data, would yield \\(R^2 = 0\\). Models performing worse than this baseline model would result in a negative R² value. This metric is widely utilized in regression analysis to evaluate model performance, where a higher R² indicates a better fit of the model to the data.\n\n# Find SSE\nSSE &lt;- sum( (obs.y - pred.y1)^2 )\nSSE\n#&gt; [1] 3536398\n# Find SST\nmean.obs.y &lt;- mean(obs.y)\nSST &lt;- sum( (obs.y - mean.obs.y)^2 )\nSST\n#&gt; [1] 3836690\n# Find R2\nR.2 &lt;- 1- SSE/SST\nR.2\n#&gt; [1] 0.07826832\nrequire(caret)\ncaret::R2(pred.y1, obs.y)\n#&gt; [1] 0.07826832\n\nref\nRMSE\n\n# Find RMSE\nRmse &lt;- sqrt(SSE/(n-p)) \nRmse\n#&gt; [1] 24.97185\ncaret::RMSE(pred.y1, obs.y)\n#&gt; [1] 24.83212\n\nSee (Wikipedia 2023b)\nAdj R2\nThe Adjusted R² statistic modifies the \\(R^2\\) value to counteract the automatic increase of \\(R^2\\) when extra explanatory variables are added to a model, even if they do not improve the model fit. This adjustment is crucial for ensuring that the metric offers a reliable indication of the explanatory power of the model, especially in multiple regression where several predictors are involved.\nThe commonly used formula is defined as:\n\\[\n\\bar{R}^{2} = 1 - \\frac{SS_{\\text{res}} / df_{\\text{res}}}{SS_{\\text{tot}} / df_{\\text{tot}}}\n\\]\nWhere:\n\n\n\\(SS_{\\text{res}}\\) and \\(SS_{\\text{tot}}\\) represent the residual and total sums of squares respectively.\n\n\\(df_{\\text{res}}\\) and \\(df_{\\text{tot}}\\) refer to the degrees of freedom of the residual and total sums of squares. Usually, \\(df_{\\text{res}} = n - p\\) and \\(df_{\\text{tot}} = n - 1\\), where:\n\n\n\\(n\\) signifies the sample size.\n\n\\(p\\) denotes the number of variables in the model.\n\n\n\nThis metric plays a vital role in model selection and safeguards against overfitting by penalizing the inclusion of non-informative variables\nThe alternate formula is:\n\\[\n\\bar{R}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n\\]\nThis formula modifies the \\(R^2\\) value, accounting for the number of predictors and offering a more parsimonious model fit measure.\n\n# Find adj R2\nadjR2 &lt;- 1-(1-R.2)*((n-1)/(n-p-1))\nadjR2\n#&gt; [1] 0.06786429\n\nSee (Wikipedia 2023a)\nOverfitting and Optimism\n\nModel usually performs very well in the empirical data where the model was fitted in the same data (optimistic)\nModel performs poorly in the new data (generalization is not as good)\n\nCauses\n\nModel determined by data at hand without expert opinion\nToo many model parameters (\\(age\\), \\(age^2\\), \\(age^3\\)) / predictors\nToo small dataset (training) / data too noisy\nConsequences\n\nOverestimation of effects of predictors\nReduction in model performance in new observations\nProposed solutions\nWe generally use procedures such as\n\nInternal validation\n\nsample splitting\ncross-validation\nbootstrap\n\n\nExternal validation\n\nTemporal\nGeographical\nDifferent data source to calculate same variable\nDifferent disease\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Coefficient of Determination.” https://en.wikipedia.org/wiki/Coefficient_of_determination.\n\n\n———. 2023b. “One-Way Analysis of Variance.” https://en.wikipedia.org/wiki/One-way_analysis_of_variance.",
    "crumbs": [
      "Machine learning (ML)",
      "Continuous outcome"
    ]
  },
  {
    "objectID": "machinelearning2.html",
    "href": "machinelearning2.html",
    "title": "Data spliting",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nLoad dataset\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nhead(ObsData)\n\n\n  \n\n\n\nSee (KDnuggets 2023; Kuhn 2023)\n\n# Using a seed to randomize in a reproducible way \nset.seed(123)\nrequire(caret)\nsplit&lt;-createDataPartition(y = ObsData$Length.of.Stay, \n                           p = 0.7, list = FALSE)\nstr(split)\n#&gt;  int [1:4017, 1] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  - attr(*, \"dimnames\")=List of 2\n#&gt;   ..$ : NULL\n#&gt;   ..$ : chr \"Resample1\"\ndim(split)\n#&gt; [1] 4017    1\ndim(ObsData)*.7 # approximate train data\n#&gt; [1] 4014.5   36.4\ndim(ObsData)*(1-.7) # approximate train data\n#&gt; [1] 1720.5   15.6\n\nSplit the data\n\n# create train data\ntrain.data&lt;-ObsData[split,]\ndim(train.data)\n#&gt; [1] 4017   52\n# create test data\ntest.data&lt;-ObsData[-split,]\ndim(test.data)\n#&gt; [1] 1718   52\n\nTrain the model\n\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nfit.train1&lt;-lm(out.formula1, data = train.data)\n# summary(fit.train1)\n\nFunction that gives performance measures\n\nperform &lt;- function(new.data,\n                    model.fit,model.formula=NULL, \n                    y.name = \"Y\",\n                    digits=3){\n  # data dimension\n  p &lt;- dim(model.matrix(model.fit))[2]\n  # predicted value\n  pred.y &lt;- predict(model.fit, new.data)\n  # sample size\n  n &lt;- length(pred.y)\n  # outcome\n  new.data.y &lt;- as.numeric(new.data[,y.name])\n  # R2\n  R2 &lt;- caret:::R2(pred.y, new.data.y)\n  # adj R2 using alternate formula\n  df.residual &lt;- n-p\n  adjR2 &lt;- 1-(1-R2)*((n-1)/df.residual)\n  # RMSE\n  RMSE &lt;-  caret:::RMSE(pred.y, new.data.y)\n  # combine all of the results\n  res &lt;- round(cbind(n,p,R2,adjR2,RMSE),digits)\n  # returning object\n  return(res)\n}\n\nExtract performance measures\n\nperform(new.data=train.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 4017 64 0.081 0.067 24.647\nperform(new.data=test.data,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 1718 64 0.056  0.02 25.488\nperform(new.data=ObsData,\n        y.name = \"Length.of.Stay\",\n        model.fit=fit.train1)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 5735 64 0.073 0.063 24.902\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKDnuggets. 2023. “Dataset Splitting Best Practices in Python.” https://www.kdnuggets.com/2020/05/dataset-splitting-best-practices-python.html.\n\n\nKuhn, Max. 2023. “Data Splitting.” https://topepo.github.io/caret/data-splitting.html.",
    "crumbs": [
      "Machine learning (ML)",
      "Data spliting"
    ]
  },
  {
    "objectID": "machinelearning3.html",
    "href": "machinelearning3.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nNow, we will describe the ideas of cross-validation.\nLoad previously saved data\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\n\nk-fold cross-vaildation\nSee (Wikipedia 2023)\n\n\n\n\n\n\n\n\n\nk = 5\ndim(ObsData)\n#&gt; [1] 5735   52\nset.seed(567)\n# create folds (based on outcome)\nfolds &lt;- createFolds(ObsData$Length.of.Stay, k = k, \n                     list = TRUE, returnTrain = TRUE)\nmode(folds)\n#&gt; [1] \"list\"\ndim(ObsData)*4/5 # approximate training data size\n#&gt; [1] 4588.0   41.6\ndim(ObsData)/5  # approximate test data size\n#&gt; [1] 1147.0   10.4\nlength(folds[[1]])\n#&gt; [1] 4588\nlength(folds[[5]])\n#&gt; [1] 4587\nstr(folds[[1]])\n#&gt;  int [1:4588] 1 2 4 6 7 8 9 10 11 13 ...\nstr(folds[[5]])\n#&gt;  int [1:4587] 1 3 5 6 7 8 10 11 12 13 ...\n\nCalculation for Fold 1\n\nfold.index &lt;- 1\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1] 1 2 4 6 7 8\nfold1.train &lt;- ObsData[fold1.train.ids,]\nfold1.test &lt;- ObsData[-fold1.train.ids,]\nout.formula1\n#&gt; Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + \n#&gt;     Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + \n#&gt;     Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + \n#&gt;     MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + \n#&gt;     blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + \n#&gt;     PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + \n#&gt;     Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nmodel.fit &lt;- lm(out.formula1, data = fold1.train)\npredictions &lt;- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#&gt;         n  p    R2  adjR2  RMSE\n#&gt; [1,] 1147 64 0.051 -0.004 24.86\n\nCalculation for Fold 2\n\nfold.index &lt;- 2\nfold1.train.ids &lt;- folds[[fold.index]]\nhead(fold1.train.ids)\n#&gt; [1] 2 3 4 5 6 7\nfold1.train &lt;- ObsData[fold1.train.ids,]\nfold1.test &lt;- ObsData[-fold1.train.ids,]\nmodel.fit &lt;- lm(out.formula1, data = fold1.train)\npredictions &lt;- predict(model.fit, \n                       newdata = fold1.test)\nperform(new.data=fold1.test,\n        y.name = \"Length.of.Stay\",\n        model.fit=model.fit)\n#&gt;         n  p    R2 adjR2   RMSE\n#&gt; [1,] 1147 64 0.066 0.011 24.714\n\nUsing caret package to automate\nSee (Kuhn 2023)\n\n# Using Caret package\nset.seed(504)\n# make a 5-fold CV\nctrl&lt;-trainControl(method = \"cv\",number = 5)\n# fit the model with formula = out.formula1\n# use training method lm\nfit.cv&lt;-train(out.formula1, trControl = ctrl,\n               data = ObsData, method = \"lm\")\nfit.cv\n#&gt; Linear Regression \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4588, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.05478  0.05980578  15.19515\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n# extract results from each test data \nsummary.res &lt;- fit.cv$resample\nsummary.res\n\n\n  \n\n\nmean(fit.cv$resample$Rsquared)\n#&gt; [1] 0.05980578\nsd(fit.cv$resample$Rsquared)\n#&gt; [1] 0.01204451\nmean(fit.cv$resample$RMSE)\n#&gt; [1] 25.05478\nsd(fit.cv$resample$RMSE)\n#&gt; [1] 2.240366\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nKuhn, Max. 2023. “Model Training and Tuning.” https://topepo.github.io/caret/model-training-and-tuning.html.\n\n\nWikipedia. 2023. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics).",
    "crumbs": [
      "Machine learning (ML)",
      "Cross-validation"
    ]
  },
  {
    "objectID": "machinelearning4.html",
    "href": "machinelearning4.html",
    "title": "Binary outcome",
    "section": "",
    "text": "Important\n\n\n\nThis tutorial is very similar to one of the previous tutorials, but uses a different data (we used RHC data here). We are revisiting concepts related to prediction before introducing ideas related to machine learning.\n\n\nIn this chapter, we will talk about Regression that deals with prediction of binary outcomes. We will use logistic regression to build the first prediction model.\nRead previously saved data\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nOutcome levels (factor)\n\nLabel\n\nPossible values of outcome\n\n\n\n\nlevels(ObsData$Death)=c(\"No\",\"Yes\") # this is useful for caret\n# ref: https://tinyurl.com/caretbin\nclass(ObsData$Death)\n#&gt; [1] \"factor\"\ntable(ObsData$Death)\n#&gt; \n#&gt;   No  Yes \n#&gt; 2013 3722\n\nMeasuring prediction error\n\n\nBrier score\n\nBrier score 0 means perfect prediction, and\nclose to zero means better prediction,\n1 being the worst prediction.\nLess accurate forecasts get higher score in Brier score.\n\n\n\nAUC\n\nThe area under a ROC curve is called as a c statistics.\nc being 0.5 means random prediction and\n1 indicates perfect prediction\n\n\nPrediction for death\nIn this section, we show the regression fitting when outcome is binary (death).\nVariables\n\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(Length.of.Stay,Death)))\nbaselinevars\n#&gt;  [1] \"Disease.category\"      \"Cancer\"                \"Cardiovascular\"       \n#&gt;  [4] \"Congestive.HF\"         \"Dementia\"              \"Psychiatric\"          \n#&gt;  [7] \"Pulmonary\"             \"Renal\"                 \"Hepatic\"              \n#&gt; [10] \"GI.Bleed\"              \"Tumor\"                 \"Immunosupperssion\"    \n#&gt; [13] \"Transfer.hx\"           \"MI\"                    \"age\"                  \n#&gt; [16] \"sex\"                   \"edu\"                   \"DASIndex\"             \n#&gt; [19] \"APACHE.score\"          \"Glasgow.Coma.Score\"    \"blood.pressure\"       \n#&gt; [22] \"WBC\"                   \"Heart.rate\"            \"Respiratory.rate\"     \n#&gt; [25] \"Temperature\"           \"PaO2vs.FIO2\"           \"Albumin\"              \n#&gt; [28] \"Hematocrit\"            \"Bilirubin\"             \"Creatinine\"           \n#&gt; [31] \"Sodium\"                \"Potassium\"             \"PaCo2\"                \n#&gt; [34] \"PH\"                    \"Weight\"                \"DNR.status\"           \n#&gt; [37] \"Medical.insurance\"     \"Respiratory.Diag\"      \"Cardiovascular.Diag\"  \n#&gt; [40] \"Neurological.Diag\"     \"Gastrointestinal.Diag\" \"Renal.Diag\"           \n#&gt; [43] \"Metabolic.Diag\"        \"Hematologic.Diag\"      \"Sepsis.Diag\"          \n#&gt; [46] \"Trauma.Diag\"           \"Orthopedic.Diag\"       \"race\"                 \n#&gt; [49] \"income\"                \"RHC.use\"\n\nModel\n\n# adjust covariates\nout.formula2 &lt;- as.formula(paste(\"Death~ \", paste(baselinevars, collapse = \"+\")))\nsaveRDS(out.formula2, file = \"Data/machinelearning/form2.RDS\")\nfit2 &lt;- glm(out.formula2, data = ObsData, \n            family = binomial(link = \"logit\"))\nrequire(Publish)\nadj.fit2 &lt;- publish(fit2, digits=1)$regressionTable\n\n\nout.formula2\n#&gt; Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#&gt;     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#&gt;     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#&gt;     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#&gt;     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#&gt;     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#&gt;     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nadj.fit2\n\n\n  \n\n\n\nMeasuring prediction error\nAUC\n\nrequire(pROC)\n#&gt; Loading required package: pROC\n#&gt; Type 'citation(\"pROC\")' for a citation.\n#&gt; \n#&gt; Attaching package: 'pROC'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cov, smooth, var\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- predict(fit2, type = \"response\")\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.7682\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.7682\n\nBrier Score\n\nrequire(DescTools)\n#&gt; Loading required package: DescTools\nBrierScore(fit2)\n#&gt; [1] 0.1812502\n\nCross-validation using caret\nBasic setup\n\n# Using Caret package\nset.seed(504)\n\n# make a 5-fold CV\nrequire(caret)\n#&gt; Loading required package: caret\n#&gt; Loading required package: ggplot2\n#&gt; Loading required package: lattice\n#&gt; \n#&gt; Attaching package: 'caret'\n#&gt; The following objects are masked from 'package:DescTools':\n#&gt; \n#&gt;     MAE, RMSE\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n\n# fit the model with formula = out.formula2\n# use training method glm (have to specify family)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\")\nfit.cv.bin\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7545115  0.4659618  0.8535653\n\nExtract results from each test data\n\nsummary.res &lt;- fit.cv.bin$resample\nsummary.res\n\n\n  \n\n\nmean(fit.cv.bin$resample$ROC)\n#&gt; [1] 0.7545115\nsd(fit.cv.bin$resample$ROC)\n#&gt; [1] 0.01651437\n\nMore options\n\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glm\",\n              family = binomial(),\n              metric=\"ROC\",\n              preProc = c(\"center\", \"scale\"))\nfit.cv.bin\n#&gt; Generalized Linear Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; Pre-processing: centered (63), scaled (63) \n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7548047  0.4629717  0.8530367\n\nVariable selection\nWe can also use stepwise regression that uses AIC as a criterion.\n\nset.seed(504)\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin.aic&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmStepAIC\",\n               direction =\"backward\",\n              family = binomial(),\n              metric=\"ROC\")\n\n\nfit.cv.bin.aic\n#&gt; Generalized Linear Model with Stepwise Feature Selection \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens      Spec     \n#&gt;   0.7540424  0.464468  0.8562535\nsummary(fit.cv.bin.aic)\n#&gt; \n#&gt; Call:\n#&gt; NULL\n#&gt; \n#&gt; Coefficients:\n#&gt;                                          Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)                             1.0783624  0.7822168   1.379 0.168019\n#&gt; Disease.categoryOther                   0.4495099  0.0919860   4.887 1.03e-06\n#&gt; `CancerLocalized (Yes)`                 1.8942512  0.5501880   3.443 0.000575\n#&gt; CancerMetastatic                        3.2703316  0.5858715   5.582 2.38e-08\n#&gt; Cardiovascular1                         0.2386749  0.0939617   2.540 0.011081\n#&gt; Congestive.HF1                          0.4539010  0.0971624   4.672 2.99e-06\n#&gt; Dementia1                               0.2380213  0.1162903   2.047 0.040679\n#&gt; Hepatic1                                0.3593093  0.1541762   2.331 0.019779\n#&gt; Tumor1                                 -1.2455123  0.5542624  -2.247 0.024630\n#&gt; Immunosupperssion1                      0.2174294  0.0730803   2.975 0.002928\n#&gt; Transfer.hx1                           -0.1849029  0.0945679  -1.955 0.050555\n#&gt; `age[50,60)`                            0.3621248  0.0984288   3.679 0.000234\n#&gt; `age[60,70)`                            0.6941924  0.0968434   7.168 7.60e-13\n#&gt; `age[70,80)`                            0.6804939  0.1126637   6.040 1.54e-09\n#&gt; `age[80, Inf)`                          0.9833851  0.1410563   6.972 3.13e-12\n#&gt; sexFemale                              -0.2805950  0.0653527  -4.294 1.76e-05\n#&gt; DASIndex                               -0.0429272  0.0062191  -6.902 5.11e-12\n#&gt; APACHE.score                            0.0174907  0.0020017   8.738  &lt; 2e-16\n#&gt; Glasgow.Coma.Score                      0.0093657  0.0012563   7.455 9.00e-14\n#&gt; WBC                                     0.0044518  0.0030090   1.479 0.139009\n#&gt; Temperature                            -0.0524703  0.0192757  -2.722 0.006487\n#&gt; PaO2vs.FIO2                             0.0004741  0.0003054   1.552 0.120548\n#&gt; Hematocrit                             -0.0154796  0.0041593  -3.722 0.000198\n#&gt; Bilirubin                               0.0313087  0.0094004   3.331 0.000867\n#&gt; Weight                                 -0.0031548  0.0011213  -2.813 0.004902\n#&gt; DNR.statusYes                           0.9347360  0.1326924   7.044 1.86e-12\n#&gt; Medical.insuranceMedicare               0.4764895  0.1257582   3.789 0.000151\n#&gt; `Medical.insuranceMedicare & Medicaid`  0.3364916  0.1584757   2.123 0.033729\n#&gt; `Medical.insuranceNo insurance`         0.3711345  0.1568820   2.366 0.017996\n#&gt; Medical.insurancePrivate                0.2632637  0.1139805   2.310 0.020903\n#&gt; `Medical.insurancePrivate & Medicare`   0.2819715  0.1313101   2.147 0.031764\n#&gt; Respiratory.DiagYes                     0.1393974  0.0769026   1.813 0.069886\n#&gt; Cardiovascular.DiagYes                  0.1804967  0.0836679   2.157 0.030982\n#&gt; Neurological.DiagYes                    0.4320266  0.1189357   3.632 0.000281\n#&gt; Gastrointestinal.DiagYes                0.2819563  0.1092206   2.582 0.009836\n#&gt; Hematologic.DiagYes                     0.9734424  0.1651363   5.895 3.75e-09\n#&gt; Sepsis.DiagYes                          0.1539651  0.0943235   1.632 0.102614\n#&gt; `incomeUnder $11k`                      0.2151437  0.0689392   3.121 0.001804\n#&gt; RHC.use                                 0.3552053  0.0713632   4.977 6.44e-07\n#&gt;                                           \n#&gt; (Intercept)                               \n#&gt; Disease.categoryOther                  ***\n#&gt; `CancerLocalized (Yes)`                ***\n#&gt; CancerMetastatic                       ***\n#&gt; Cardiovascular1                        *  \n#&gt; Congestive.HF1                         ***\n#&gt; Dementia1                              *  \n#&gt; Hepatic1                               *  \n#&gt; Tumor1                                 *  \n#&gt; Immunosupperssion1                     ** \n#&gt; Transfer.hx1                           .  \n#&gt; `age[50,60)`                           ***\n#&gt; `age[60,70)`                           ***\n#&gt; `age[70,80)`                           ***\n#&gt; `age[80, Inf)`                         ***\n#&gt; sexFemale                              ***\n#&gt; DASIndex                               ***\n#&gt; APACHE.score                           ***\n#&gt; Glasgow.Coma.Score                     ***\n#&gt; WBC                                       \n#&gt; Temperature                            ** \n#&gt; PaO2vs.FIO2                               \n#&gt; Hematocrit                             ***\n#&gt; Bilirubin                              ***\n#&gt; Weight                                 ** \n#&gt; DNR.statusYes                          ***\n#&gt; Medical.insuranceMedicare              ***\n#&gt; `Medical.insuranceMedicare & Medicaid` *  \n#&gt; `Medical.insuranceNo insurance`        *  \n#&gt; Medical.insurancePrivate               *  \n#&gt; `Medical.insurancePrivate & Medicare`  *  \n#&gt; Respiratory.DiagYes                    .  \n#&gt; Cardiovascular.DiagYes                 *  \n#&gt; Neurological.DiagYes                   ***\n#&gt; Gastrointestinal.DiagYes               ** \n#&gt; Hematologic.DiagYes                    ***\n#&gt; Sepsis.DiagYes                            \n#&gt; `incomeUnder $11k`                     ** \n#&gt; RHC.use                                ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 7433.3  on 5734  degrees of freedom\n#&gt; Residual deviance: 6198.0  on 5696  degrees of freedom\n#&gt; AIC: 6276\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Machine learning (ML)",
      "Binary outcome"
    ]
  },
  {
    "objectID": "machinelearning5.html",
    "href": "machinelearning5.html",
    "title": "Supervised learning",
    "section": "",
    "text": "In this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods.\nIn the first code chunk, we load necessary R libraries that will be utilized throughout the chapter for various machine learning methods and data visualization.\nRead previously saved data\nThe second chunk is dedicated to reading previously saved data and formulas from specified file paths, ensuring that the dataset and predefined formulas are available for subsequent analyses.\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\nlevels(ObsData$Death)=c(\"No\",\"Yes\")\nout.formula1 &lt;- readRDS(file = \"Data/machinelearning/form1.RDS\")\nout.formula2 &lt;- readRDS(file = \"Data/machinelearning/form2.RDS\")\n\nRidge, LASSO, and Elastic net\nThe traditional regression models (e.g., linear regression, logistic regression) show poor model performance when\n\npredictors are highly correlated or\nthere are many predictors\n\nIn both cases, the variance of the estimated regression coefficients could be highly variable. Hence, the model often results in poor predictions. The general solution to this problem is to reduce the variance at the cost of introducing some bias in the coefficients. This approach is called regularization or shrinking. Since we are interested in overall prediction rather than individual regression coefficients in a prediction context, this shrinkage approach is almost always beneficial for the model’s predictive performance. Ridge, LASSO, and Elastic Net are shrinkage machine learning techniques.\n\nRidge: Penalizes the sum of squared regression coefficients (the so-called \\(L_2\\) penalty). This approach does not remove irrelevant predictors, but minimizes the impact of the irrelevant predictors. There is a hyperparameter called \\(\\lambda\\) (lambda) that determines the amount of shrinkage of the coefficients. The larger \\(\\lambda\\) indicates more penalization of the coefficients.\nLASSO: The Least Absolute Shrinkage and Selection Operator (LASSO) is quite similar conceptually to the ridge regression. However, lasso penalizes the sum of the absolute values of regression coefficients (so-called \\(L_1\\) penalty). As a result, a high \\(\\lambda\\) value forces many coefficients to be exactly zero in lasso regression, suggesting a reduced model with fewer predictors, which is never the case in ridge regression.\nElastic Net: The elastic net combines the penalties of ridge regression and lasso to get the best of both methods. Two hyperparameters in the elastic net are \\(\\alpha\\) (alpha) and \\(\\lambda\\).\n\nWe can use the glmnet function in R to fit these there models.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nContinuous outcome\nCross-validation LASSO\n\n\n\n\n\n\n\n\nIn this code chunk, we implement a machine learning model training process with a focus on utilizing cross-validation and tuning parameters to optimize the model. Cross-validation is a technique used to assess how well the model will generalize to an independent dataset by partitioning the original dataset into a training set to train the model, and a test set to evaluate it. Here, we specify that we are using a particular type of cross-validation, denoted as “cv”, and that we will be creating 5 folds (or partitions) of the data, as indicated by number = 5.\nThe model being trained is specified to use a method known as “glmnet”, which is capable of performing lasso, ridge, and elastic net regularization regressions. Tuning parameters are crucial in controlling the behavior of our learning algorithm. In this instance, we specify lambda and alpha as our tuning parameters, which control the amount of regularization applied to the model and the mixing percentage between lasso and ridge regression, respectively. The tuneGrid argument is used to specify the exact values of alpha and lambda that the model should consider during training. The verbose = FALSE argument ensures that additional model training details are not printed during the training process. Finally, the trained model is stored in an object for further examination and use.\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nfit.cv.con &lt;- train(out.formula1, \n                    trControl = ctrl,\n                    data = ObsData, method = \"glmnet\",\n                    lambda= 0,\n                    tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                    verbose = FALSE)\nfit.cv.con\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.11407  0.05861443  15.21187\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 1\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Ridge\nSubsequent code chunks explore Ridge regression and Elastic Net, employing similar methodologies but adjusting tuning parameters accordingly.\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nfit.cv.con &lt;-train(out.formula1, \n                   trControl = ctrl,\n                   data = ObsData, method = \"glmnet\",\n                   lambda= 0,\n                   tuneGrid = expand.grid(alpha = 0, lambda = 0),\n                   verbose = FALSE)\nfit.cv.con\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4588, 4588, 4587, 4589 \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared    MAE     \n#&gt;   25.05711  0.06233138  15.15647\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 0\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nBinary outcome\nCross-validation LASSO\nWe then shift to binary outcomes, exploring LASSO and Ridge regression with similar implementations but adjusting for the binary nature of the outcome variable.\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, \n                  trControl = ctrl,\n                  data = ObsData, \n                  method = \"glmnet\",\n                  lambda= 0,\n                  tuneGrid = expand.grid(alpha = 1, lambda = 0),\n                  verbose = FALSE,\n                  metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7554574  0.4669704  0.8533019\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 1\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\n\nNot okay to select variables from a shrinkage model, and then use them in a regular regression\nCross-validation Ridge\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               lambda= 0,\n               tuneGrid = expand.grid(alpha = 0,  \n                                      lambda = 0),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4587, 4587, 4588, 4589 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens      Spec     \n#&gt;   0.7557931  0.468932  0.8519607\n#&gt; \n#&gt; Tuning parameter 'alpha' was held constant at a value of 0\n#&gt; Tuning\n#&gt;  parameter 'lambda' was held constant at a value of 0\n\nCross-validation Elastic net\n\nAlpha = mixing parameter\nLambda = regularization or tuning parameter\nWe can use expand.grid for model tuning\n\n\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"glmnet\",\n               tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  \n                                      lambda = seq(0.05,0.3,by = 0.05)),\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; glmnet \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4589, 4588, 4587, 4588, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   alpha  lambda  ROC        Sens          Spec     \n#&gt;   0.10   0.05    0.7501508  0.3661247114  0.8981789\n#&gt;   0.10   0.10    0.7468010  0.2747132822  0.9374017\n#&gt;   0.10   0.15    0.7417450  0.1758589188  0.9674937\n#&gt;   0.10   0.20    0.7363601  0.0874263916  0.9862983\n#&gt;   0.10   0.25    0.7300909  0.0258249694  0.9965079\n#&gt;   0.10   0.30    0.7233685  0.0009925558  0.9997312\n#&gt;   0.15   0.05    0.7492697  0.3437650457  0.9070455\n#&gt;   0.15   0.10    0.7422014  0.2205696085  0.9511056\n#&gt;   0.15   0.15    0.7335331  0.1023332469  0.9830739\n#&gt;   0.15   0.20    0.7226744  0.0188733751  0.9967764\n#&gt;   0.15   0.25    0.7152975  0.0000000000  1.0000000\n#&gt;   0.15   0.30    0.7098185  0.0000000000  1.0000000\n#&gt;   0.20   0.05    0.7476579  0.3229053245  0.9148355\n#&gt;   0.20   0.10    0.7367101  0.1679061269  0.9669564\n#&gt;   0.20   0.15    0.7223293  0.0382454971  0.9914029\n#&gt;   0.20   0.20    0.7125355  0.0004962779  0.9994627\n#&gt;   0.20   0.25    0.7062552  0.0000000000  1.0000000\n#&gt;   0.20   0.30    0.6962746  0.0000000000  1.0000000\n#&gt; \n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final values used for the model were alpha = 0.1 and lambda = 0.05.\nplot(fit.cv.bin)\n\n\n\n\n\n\n\nDecision tree\nDecision trees are then introduced and implemented, with visualizations and evaluation metrics provided to assess their performance.\n\nDecision tree\n\nReferred to as Classification and regression trees or CART\nCovers\n\nClassification (categorical outcome)\nRegression (continuous outcome)\n\n\nFlexible to incorporate non-linear effects automatically\n\nNo need to specify higher order terms / interactions\n\n\nUnstable, prone to overfitting, suffers from high variance\n\n\n\nSimple CART\n\nrequire(rpart)\nsummary(ObsData$DASIndex) # Duke Activity Status Index\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   11.00   16.06   19.75   20.50   23.43   33.00\ncart.fit &lt;- rpart(Death~DASIndex, data = ObsData)\npar(mfrow = c(1,1), xpd = NA)\nplot(cart.fit)\ntext(cart.fit, use.n = TRUE)\n\n\n\n\n\n\nprint(cart.fit)\n#&gt; n= 5735 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 5735 2013 Yes (0.3510026 0.6489974)  \n#&gt;   2) DASIndex&gt;=24.92383 1143  514 No (0.5503062 0.4496938)  \n#&gt;     4) DASIndex&gt;=29.14648 561  199 No (0.6452763 0.3547237) *\n#&gt;     5) DASIndex&lt; 29.14648 582  267 Yes (0.4587629 0.5412371) *\n#&gt;   3) DASIndex&lt; 24.92383 4592 1384 Yes (0.3013937 0.6986063) *\nrequire(rattle)\nrequire(rpart.plot)\nrequire(RColorBrewer)\nfancyRpartPlot(cart.fit, caption = NULL)\n\n\n\n\n\n\n\nAUC\n\nrequire(pROC)\n#&gt; Loading required package: pROC\n#&gt; Type 'citation(\"pROC\")' for a citation.\n#&gt; \n#&gt; Attaching package: 'pROC'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     cov, smooth, var\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.5912\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.5912\n\nComplex CART\nMore variables\n\nout.formula2\n#&gt; Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + \n#&gt;     Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + \n#&gt;     Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + \n#&gt;     edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + \n#&gt;     WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + \n#&gt;     Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + \n#&gt;     Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + \n#&gt;     Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + \n#&gt;     Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + \n#&gt;     Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + \n#&gt;     RHC.use\nrequire(rpart)\ncart.fit &lt;- rpart(out.formula2, data = ObsData)\n\nCART Variable importance\n\ncart.fit$variable.importance\n#&gt;            DASIndex              Cancer               Tumor                 age \n#&gt;         123.2102455          33.4559400          32.5418433          24.0804860 \n#&gt;   Medical.insurance                 WBC                 edu Cardiovascular.Diag \n#&gt;          14.5199953           5.6673997           3.7441554           3.6449371 \n#&gt;          Heart.rate      Cardiovascular         Trauma.Diag               PaCo2 \n#&gt;           3.4059248           3.1669125           0.5953098           0.2420672 \n#&gt;           Potassium              Sodium             Albumin \n#&gt;           0.2420672           0.2420672           0.1984366\n\nAUC\n\nrequire(pROC)\nobs.y2&lt;-ObsData$Death\npred.y2 &lt;- as.numeric(predict(cart.fit, type = \"prob\")[, 2])\nrocobj &lt;- roc(obs.y2, pred.y2)\n#&gt; Setting levels: control = No, case = Yes\n#&gt; Setting direction: controls &lt; cases\nrocobj\n#&gt; \n#&gt; Call:\n#&gt; roc.default(response = obs.y2, predictor = pred.y2)\n#&gt; \n#&gt; Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes).\n#&gt; Area under the curve: 0.5981\nplot(rocobj)\n\n\n\n\n\n\nauc(rocobj)\n#&gt; Area under the curve: 0.5981\n\nCross-validation CART\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5, \n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"rpart\",\n              metric=\"ROC\")\nfit.cv.bin\n#&gt; CART \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   cp           ROC        Sens       Spec     \n#&gt;   0.007203179  0.6304911  0.2816488  0.9086574\n#&gt;   0.039741679  0.5725283  0.2488649  0.8981807\n#&gt;   0.057128664  0.5380544  0.1287804  0.9473284\n#&gt; \n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final value used for the model was cp = 0.007203179.\n# extract results from each test data \nsummary.res &lt;- fit.cv.bin$resample\nsummary.res\n\n\n  \n\n\n\nEnsemble methods (Type I)\nWe explore ensemble methods, specifically bagging and boosting, through implementation and evaluation in the context of binary outcomes.\nTraining same model to different samples (of the same data)\nCross-validation bagging\n\nBagging or bootstrap aggregation\n\nindependent bootstrap samples (sampling with replacement, B times),\napplies CART on each i (no prunning)\nAverage the resulting predictions\nReduces variance as a result of using bootstrap\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"bag\",\n               bagControl = bagControl(fit = ldaBag$fit, \n                                       predict = ldaBag$pred, \n                                       aggregate = ldaBag$aggregate),\n               metric=\"ROC\")\n#&gt; Warning: executing %dopar% sequentially: no parallel backend registered\nfit.cv.bin\n#&gt; Bagged Model \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results:\n#&gt; \n#&gt;   ROC        Sens       Spec     \n#&gt;   0.7506666  0.4485809  0.8602811\n#&gt; \n#&gt; Tuning parameter 'vars' was held constant at a value of 63\n\n\nBagging improves prediction accuracy\n\nover prediction using a single tree\n\n\nLooses interpretability\n\nas this is an average of many diagrams now\n\n\nBut we can get a summary of the importance of each variable\n\nBagging Variable importance\n\ncaret::varImp(fit.cv.bin, scale = FALSE)\n#&gt; ROC curve variable importance\n#&gt; \n#&gt;   only 20 most important variables shown (out of 50)\n#&gt; \n#&gt;                    Importance\n#&gt; age                    0.6159\n#&gt; APACHE.score           0.6140\n#&gt; DASIndex               0.5962\n#&gt; Cancer                 0.5878\n#&gt; Creatinine             0.5835\n#&gt; Tumor                  0.5807\n#&gt; blood.pressure         0.5697\n#&gt; Glasgow.Coma.Score     0.5656\n#&gt; Disease.category       0.5641\n#&gt; Temperature            0.5584\n#&gt; DNR.status             0.5572\n#&gt; Hematocrit             0.5525\n#&gt; Weight                 0.5424\n#&gt; Bilirubin              0.5397\n#&gt; income                 0.5319\n#&gt; Immunosupperssion      0.5278\n#&gt; RHC.use                0.5263\n#&gt; Dementia               0.5252\n#&gt; Congestive.HF          0.5250\n#&gt; Hematologic.Diag       0.5250\n\nCross-validation boosting\n\nBoosting\n\nsequentially updated/weighted bootstrap based on previous learning\n\n\n\n\nset.seed(504)\nrequire(caret)\nctrl&lt;-trainControl(method = \"cv\", number = 5,\n                   classProbs = TRUE,\n                   summaryFunction = twoClassSummary)\n# fit the model with formula = out.formula2\nfit.cv.bin&lt;-train(out.formula2, trControl = ctrl,\n               data = ObsData, method = \"gbm\",\n               verbose = FALSE,\n               metric=\"ROC\")\nfit.cv.bin\n#&gt; Stochastic Gradient Boosting \n#&gt; \n#&gt; 5735 samples\n#&gt;   50 predictor\n#&gt;    2 classes: 'No', 'Yes' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (5 fold) \n#&gt; Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   interaction.depth  n.trees  ROC        Sens       Spec     \n#&gt;   1                   50      0.7218938  0.2145970  0.9505647\n#&gt;   1                  100      0.7410292  0.2980581  0.9234228\n#&gt;   1                  150      0.7483014  0.3487142  0.9030028\n#&gt;   2                   50      0.7414513  0.2960631  0.9263816\n#&gt;   2                  100      0.7534264  0.3869684  0.8917212\n#&gt;   2                  150      0.7575826  0.4187512  0.8777477\n#&gt;   3                   50      0.7496078  0.3626125  0.9070358\n#&gt;   3                  100      0.7579645  0.4078244  0.8764076\n#&gt;   3                  150      0.7637074  0.4445909  0.8702298\n#&gt; \n#&gt; Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#&gt; \n#&gt; Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#&gt; ROC was used to select the optimal model using the largest value.\n#&gt; The final values used for the model were n.trees = 150, interaction.depth =\n#&gt;  3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nplot(fit.cv.bin)\n\n\n\n\n\n\n\nEnsemble methods (Type II)\nWe introduce the concept of Super Learner, providing external resources for further exploration.\nTraining different models on the same data\nSuper Learner\n\nLarge number of candidate learners (CL) with different strengths\n\nParametric (logistic)\nNon-parametric (CART)\n\n\nCross-validation: CL applied on training data, prediction made on test data\nFinal prediction uses a weighted version of all predictions\n\nWeights = coef of Observed outcome ~ prediction from each CL\n\n\nSteps\nRefer to this tutorial for steps and examples! Refer to the next chapter for more details.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following is a brief exercise of super learners in the propensity score context, but we will explore more about this topic in the next chapter.",
    "crumbs": [
      "Machine learning (ML)",
      "Supervised learning"
    ]
  },
  {
    "objectID": "machinelearning6.html",
    "href": "machinelearning6.html",
    "title": "Unsupervised learning",
    "section": "",
    "text": "In this chapter, we will talk about unsupervised learning.\nIn the initial code chunk, we load a specific library that will be utilized for publishing-related functionality throughout the chapter.\nClustering\nClustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity.\nGroup characteristics include (to the extent that is possible)\n\nlow inter-class similarity: observation from different clusters would be dissimilar\nhigh intra-class similarity: observation from the same cluster would be similar\n\nWithin-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances (Wikipedia 2023a)\n\n\n\n\n\n\n\n\nK-means\nK-means is a very popular clustering algorithm, that partitions the data into \\(k\\) groups.\nAlgorithm:\n\nDetermine a number \\(k\\) (e.g., could be 3)\nrandomly select \\(k\\) subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster.\nBy Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong.\ncompute new mean value for each cluster.\nbased on this new mean, try to determine again in which cluster the data points belong.\nprocess continues until the data points do not change cluster membership.\nRead previously saved data\nWe read a previously saved dataset from a specified file path.\n\nObsData &lt;- readRDS(file = \"Data/machinelearning/rhcAnalytic.RDS\")\n\nIn the next few code chunks, we implement k-means clustering on various subsets of the data, visualizing the results and displaying the cluster centers. The first example uses two variables, the second example uses three, and in the third example, a larger subset of variables is selected but not immediately utilized in the clustering. In the subsequent code chunk, we apply k-means clustering to the larger subset of variables, displaying various results and aggregating data by cluster to display mean and standard deviation values for each variable within each cluster.\nExample 1\n\ndatax0 &lt;- ObsData[c(\"Heart.rate\", \"edu\")]\nkres0 &lt;- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#&gt;   Heart.rate      edu\n#&gt; 1   54.55138 11.44494\n#&gt; 2  134.96277 11.75466\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\n\n\n\nExample 2\n\ndatax0 &lt;- ObsData[c(\"blood.pressure\", \"Heart.rate\", \"Respiratory.rate\")]\nkres0 &lt;- kmeans(datax0, centers = 2, nstart = 10)\nkres0$centers\n#&gt;   blood.pressure Heart.rate Respiratory.rate\n#&gt; 1       73.71684   54.95789         22.76723\n#&gt; 2       80.10812  135.08956         29.85267\nplot(datax0, col = kres0$cluster, main = kres0$tot.withinss)\n\n\n\n\n\n\n\nExample with many variables\n\ndatax &lt;- ObsData[c(\"edu\", \"blood.pressure\", \"Heart.rate\", \n                   \"Respiratory.rate\" , \"Temperature\",\n                   \"PH\", \"Weight\", \"Length.of.Stay\")]\n\n\nkres &lt;- kmeans(datax, centers = 3)\n#kres\nhead(kres$cluster)\n#&gt; [1] 2 2 2 1 2 3\nkres$size\n#&gt; [1] 1252 2795 1688\nkres$centers\n#&gt;        edu blood.pressure Heart.rate Respiratory.rate Temperature       PH\n#&gt; 1 11.46447       65.46446   53.17332         22.66717    37.01512 7.378432\n#&gt; 2 11.85665       54.28086  136.34597         29.75277    37.85056 7.385267\n#&gt; 3 11.54214      128.33886  126.12026         29.36611    37.68129 7.401027\n#&gt;     Weight Length.of.Stay\n#&gt; 1 67.48365       18.59425\n#&gt; 2 68.67307       23.41789\n#&gt; 3 66.68351       20.68128\naggregate(datax, by = list(cluster = kres$cluster), mean)\n\n\n  \n\n\naggregate(datax, by = list(cluster = kres$cluster), sd)\n\n\n  \n\n\n\nOptimal number of clusters\nNext, we explore determining the optimal number of clusters, visualizing the total within-cluster sum of squares for different values of k and indicating a chosen value of k with a vertical line on the plot.\n\nrequire(factoextra)\n#&gt; Loading required package: factoextra\n#&gt; Loading required package: ggplot2\n#&gt; Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_nbclust(datax, kmeans, method = \"wss\")+\n  geom_vline(xintercept=3,linetype=3)\n\n\n\n\n\n\n\nHere the vertical line is chosen based on elbow method (Wikipedia 2023b).\nDiscussion\n\nWe need to supply a number, \\(k\\): but we can test different \\(k\\)s to identify optimal value\nClustering can be influenced by outliners, so median based clustering is possible\nmere ordering can influence clustering, hence we should choose different initial means (e.g., nstart should be greater than 1).\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReferences\n\n\n\n\nWikipedia. 2023a. “Cross-Validation (Statistics).” https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n\n\n———. 2023b. “Elbow Method (Clustering).” https://en.wikipedia.org/wiki/Elbow_method_(clustering).",
    "crumbs": [
      "Machine learning (ML)",
      "Unsupervised learning"
    ]
  },
  {
    "objectID": "machinelearning6a.html",
    "href": "machinelearning6a.html",
    "title": "NHIS Example",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning (ML) techniques with health survey data. We will use the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP) among adults aged 65 years or older. We will use LASSO and random forest models with sampling weights to obtain population-level predictions. In this tutorial, the split-sample approach as an internal validation technique will be used. You can review the earlier tutorial on data splitting technique. Note that this split-sample approach is flagged as a problematic approach in the literature (Steyerberg et al. 2001). The better approach could be cross-validation and bootstrapping Steyerberg and Steyerberg (2019). In the next tutorial, we will apply the ML techniques for survey data with cross-validation.\n\n\nSteyerberg EW, Harrell Jr FE, Borsboom GJ, Eijkemans MJ, Vergouwe Y, Habbema JD. Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of Clinical Epidemiology. 2001; 54(8):774-81. DOI: 10.1016/S0895-4356(01)00341-9\nSteyerberg EW, Steyerberg EW. Overfitting and optimism in prediction models. Clinical prediction models: A practical approach to development, validation, and updating. 2019:95-112. DOI: 10.1007/978-3-030-16399-0_5\n\n\n\n\n\n\nNote\n\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/nhis2016.RData\")\nls()\n#&gt; [1] \"dat.analytic\"\n\n\ndim(dat.analytic)\n#&gt; [1] 7828   14\n\nhead(dat.analytic)\n\n\n  \n\n\n\nThe dataset contains 7,828 complete case participants (i.e., no missing) with 14 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (high impact chronic pain, the binary outcome variable)\n\nsex: Sex\n\nmarital: Marital status\n\nrace: Race/ethnicity\n\npoverty.status: Poverty status\n\ndiabetes: Diabetes\n\nhigh.cholesterol: High cholesterol\n\nstroke: Stroke\n\narthritis: Arthritis and rheumatism\n\ncurrent.smoker: Current smoker\n\nLet’s see the descriptive statistics of the predictors stratified by the outcome variable (HICP).\nDescriptive statistics\n\n# Predictors\npredictors &lt;- c(\"sex\", \"marital\", \"race\", \"poverty.status\", \n                \"diabetes\", \"high.cholesterol\", \"stroke\",\n                \"arthritis\", \"current.smoker\")\n\n# Table 1 - Unweighted \n#tab1 &lt;- CreateTableOne(vars = predictors, strata = \"HICP\", \n#                       data = dat.analytic, test = F)\n#print(tab1, showAllLevels = T)\n\ntbl_summary(data = dat.analytic, include = predictors, \n            by = HICP, missing = \"no\") %&gt;% \n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nHICP\n\n\n\n\n0\nN = 6,8471\n\n\n1\nN = 9811\n\n\n\n\n\nsex\n\n\n\n\n    Female\n3,841 (56%)\n623 (64%)\n\n\n    Male\n3,006 (44%)\n358 (36%)\n\n\nmarital\n\n\n\n\n    Never married\n444 (6.5%)\n60 (6.1%)\n\n\n    Married/with partner\n3,183 (46%)\n379 (39%)\n\n\n    Divorced/separated\n1,252 (18%)\n198 (20%)\n\n\n    Widowed\n1,968 (29%)\n344 (35%)\n\n\nrace\n\n\n\n\n    White\n5,455 (80%)\n756 (77%)\n\n\n    Black\n606 (8.9%)\n99 (10%)\n\n\n    Hispanic\n431 (6.3%)\n79 (8.1%)\n\n\n    Others\n355 (5.2%)\n47 (4.8%)\n\n\npoverty.status\n\n\n\n\n    &lt;100% FPL\n543 (7.9%)\n176 (18%)\n\n\n    100-200% FPL\n1,520 (22%)\n307 (31%)\n\n\n    200-400% FPL\n2,309 (34%)\n309 (31%)\n\n\n    400%+ FPL\n2,475 (36%)\n189 (19%)\n\n\ndiabetes\n1,275 (19%)\n315 (32%)\n\n\nhigh.cholesterol\n3,602 (53%)\n633 (65%)\n\n\nstroke\n527 (7.7%)\n146 (15%)\n\n\narthritis\n3,226 (47%)\n769 (78%)\n\n\ncurrent.smoker\n619 (9.0%)\n123 (13%)\n\n\n\n\n1 n (%)\n\n\n\n\n\nWeight normalization\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Note that we are not interested in the statistical significance of the \\(\\beta\\) coefficients. Hence, not utilizing PSU and strata should not be an issue in this prediction problem. However, we still need to use sampling weights to get population-level predictions. Large weights are usually problematic, particularly with model evaluation. One way to solve the problem is weight normalization (Bruin 2023).\n\n# Normalize weight\ndat.analytic$wgt &lt;- dat.analytic$weight * \n  nrow(dat.analytic)/sum(dat.analytic$weight)\n\n# Weight summary\nsummary(dat.analytic$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     243    1521    2604    2914    3791   14662\nsummary(dat.analytic$wgt)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; 0.08339 0.52198 0.89365 1.00000 1.30109 5.03175\n\n# The weighted and unweighted n are equal\nnrow(dat.analytic)\n#&gt; [1] 7828\nsum(dat.analytic$wgt)\n#&gt; [1] 7828\n\nSplit-sample\nLet us create our training and test data using the split-sample approach. We created 70% training and 30% test data for our example.\n\nset.seed(604001)\ndat.analytic$datasplit &lt;- rbinom(nrow(dat.analytic), \n                                 size = 1, prob = 0.7) \ntable(dat.analytic$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2343 5485\n\n# Training data\ndat.train &lt;- dat.analytic[dat.analytic$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 5485   16\n\n# Test data\ndat.test &lt;- dat.analytic[dat.analytic$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2343   16\n\nRegression formula\nLet’s us define the regression formula:\n\nFormula &lt;- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#&gt; HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#&gt;     stroke + arthritis + current.smoker\n\nLASSO for Surveys\nNow, we will fit the LASSO model for our survey data. Here are the steps:\n\nWe will fit 5-fold cross-validation on the training data to find the value of lambda that gives minimum prediction error. We will incorporate sampling weights in the model to account for survey data.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nData in matrix\nTo perform LASSO with the glmnet package, we need to set the predictors in the data.matrix format and outcome variable as a vector.\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$HICP) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$HICP) \n\nLet us see the few rows of the data:\n\nhead(X.train)\n#&gt;    sexMale maritalMarried/with partner maritalDivorced/separated maritalWidowed\n#&gt; 12       1                           1                         0              0\n#&gt; 13       1                           0                         1              0\n#&gt; 16       0                           0                         1              0\n#&gt; 42       0                           0                         0              1\n#&gt; 63       0                           0                         0              1\n#&gt; 65       0                           0                         0              1\n#&gt;    raceBlack raceHispanic raceOthers poverty.status100-200% FPL\n#&gt; 12         0            0          0                          0\n#&gt; 13         0            0          0                          0\n#&gt; 16         1            0          0                          0\n#&gt; 42         0            0          0                          0\n#&gt; 63         0            0          0                          0\n#&gt; 65         0            1          0                          1\n#&gt;    poverty.status200-400% FPL poverty.status400%+ FPL diabetesYes\n#&gt; 12                          1                       0           0\n#&gt; 13                          0                       1           0\n#&gt; 16                          1                       0           0\n#&gt; 42                          0                       1           0\n#&gt; 63                          1                       0           0\n#&gt; 65                          0                       0           1\n#&gt;    high.cholesterolYes strokeYes arthritisYes current.smokerYes\n#&gt; 12                   0         0            1                 0\n#&gt; 13                   0         0            1                 0\n#&gt; 16                   1         0            1                 0\n#&gt; 42                   1         0            0                 0\n#&gt; 63                   0         0            0                 0\n#&gt; 65                   1         0            1                 0\n\nAs we can see, factor predictors are coded into dummy variables. It is important to note that the continuous predictors should be standardized. glmnet does this by default. Next, we will use the glmnet function to fit the LASSO model.\n\n\n\n\n\n\nNote\n\n\n\nIn glmnet function, alpha = 1 for the LASSO, alpha = 0 for the ridge, and setting alpha to some value between 0 and 1 is the elastic net model.\n\n\nFind best lambda\nNow, we will use k-fold cross-validation with the cv.glmnet function to find the best lambda value. In this example, we choose k = 5. Note that we must incorporate sampling weight to account for survey data.\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, \n                          nfolds = 5, alpha = 1, \n                          family = \"binomial\", \n                          weights = dat.train$wgt)\nfit.cv.lasso\n#&gt; \n#&gt; Call:  cv.glmnet(x = X.train, y = y.train, weights = dat.train$wgt,      nfolds = 5, alpha = 1, family = \"binomial\") \n#&gt; \n#&gt; Measure: Binomial Deviance \n#&gt; \n#&gt;       Lambda Index Measure      SE Nonzero\n#&gt; min 0.001355    43  0.6856 0.01905      12\n#&gt; 1se 0.020128    14  0.7036 0.01446       5\n\nWe can also plot all the lambda values against the deviance (i.e., prediction error).\n\nplot(fit.cv.lasso)\n\n\n\n\n\n\n\n\n# Best lambda\nfit.cv.lasso$lambda.min\n#&gt; [1] 0.001355482\n\nThe lambda value that has the lowest deviance is 0.0013555. Our next step is to fit the LASSO model with the best lambda. Again, we must incorporate sampling weight to account for survey data.\nLASSO with best lambda\n\n# Fit the model on the training set with optimum lambda\nfit.lasso &lt;- glmnet(x = X.train, y = y.train, \n                    alpha = 1, family = \"binomial\",\n                    lambda = fit.cv.lasso$lambda.min, \n                    weights = dat.train$wgt)\nfit.lasso\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df %Dev   Lambda\n#&gt; 1 12 9.68 0.001355\n\nLet’s check the coefficients from the model:\n\n# Intercept \nfit.lasso$a0\n#&gt;        s0 \n#&gt; -2.491484\n\n# Beta coefficients\nfit.lasso$beta\n#&gt; 15 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                       s0\n#&gt; sexMale                     -0.009832818\n#&gt; maritalMarried/with partner  .          \n#&gt; maritalDivorced/separated    .          \n#&gt; maritalWidowed               0.041111308\n#&gt; raceBlack                   -0.064853067\n#&gt; raceHispanic                -0.037985160\n#&gt; raceOthers                   .          \n#&gt; poverty.status100-200% FPL  -0.268494032\n#&gt; poverty.status200-400% FPL  -0.602097419\n#&gt; poverty.status400%+ FPL     -1.052635980\n#&gt; diabetesYes                  0.369667918\n#&gt; high.cholesterolYes          0.299475863\n#&gt; strokeYes                    0.449050679\n#&gt; arthritisYes                 1.241288036\n#&gt; current.smokerYes            0.253439388\n\nAs we can see, the coefficient is not shown for some predictors. This is because the LASSO model shrunk the coefficient to zero. In other words, these predictors were dropped entirely from the model because they were not contributing enough to predict the outcome. next, we will use the final model to make predictions on new observations or our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.lasso &lt;- predict(fit.lasso, \n                               newx = X.test, \n                               type = \"response\")\nhead(dat.test$pred.lasso)\n#&gt;            s0\n#&gt; 3  0.05711170\n#&gt; 11 0.03716633\n#&gt; 28 0.14049527\n#&gt; 30 0.05764351\n#&gt; 31 0.11408034\n#&gt; 59 0.32540267\n\nModel performance\nNow, we will calculate the model performance measures such as AUC, calibration slope, and Brier score Christodoulou et al. (2019). We will incorporate sampling weights to get population-level estimates.\n\n\nSteyerberg EW, Vickers AJ, Cook NR, Gerds T, Gonen M, Obuchowski N, Pencina MJ, Kattan MW. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.). 2010;21(1):128. DOI: 10.1097/EDE.0b013e3181c30fb2\nSteyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European heart journal. 2014;35(29):1925-31. DOI: 10.1093/eurheartj/ehu207\nChristodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology. 2019;110:12-22. DOI: 10.1016/j.jclinepi.2019.02.004\n\n\n\n\n\n\nNote\n\n\n\n\nArea under the curve (AUC) is a measure of discrimination or accuracy of a model. A higher AUC is better. An AUC value of 1 is considered a perfect prediction, while an AUC value of 0.50 is no better than a coin toss. In practice, AUC values of 0.70 to 0.80 are considered good, and those \\(&gt;0.80\\) are considered very good.\nCalibration is defined as the agreement between observed and predicted probability of the outcome. In this exercise, we will estimate the calibration slope as a measure of calibration. A calibration slope of 1 reflects a well-calibrated model, a calibration slope less than 1 indicates overfitting and greater than 1 indicates underfitting of the model.\nThe Brier score is a measure of overall performance. The Brier score can range from 0 to 1 and is similar to the mean squared error. A lower Brier score value (closer to 0) indicates a better model.\n\n\n\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, \n                                     dat.test$HICP, \n                                     weight = dat.test$wgt))\nauc.lasso\n#&gt; [1] 0.7662941\n\n\n# Logit of the predicted probability\ndat.test$pred.lasso.logit &lt;- Logit(dat.test$pred.lasso)\n\n# Weighted calibration slope\nmod.cal &lt;- glm(HICP ~ pred.lasso.logit, data = dat.test, \n               family = binomial, weights = wgt)\ncal.slope.lasso &lt;- summary(mod.cal)$coef[2,1]\ncal.slope.lasso\n#&gt; [1] 1.244645\n\n\n# Weighted Brier Score\nbrier.lasso &lt;- mean(brierscore(HICP ~ dat.test$pred.lasso, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier.lasso\n#&gt; [1] 0.09978551\n\nRandom Forest for Surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model:\n\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey data.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey data.\nCalculate predictive performance (e.g., AUC) on the test data.\n\nFormula\nWe will use the same formula defined above.\n\nFormula\n#&gt; HICP ~ sex + marital + race + poverty.status + diabetes + high.cholesterol + \n#&gt;     stroke + arthritis + current.smoker\n\nHyperparameter tuning\nFor tuning the hyperparameters, let’s use the grid search approach.\n\n# Grid with 1000 models - huge time consuming\n#grid.search &lt;- expand.grid(mtry = 1:10, node.size = 1:10, \n#                           num.trees = seq(50,500,50), \n#                           oob_error = 0)\n  \n# Grid with 36 models as an exercise\ngrid.search &lt;- expand.grid(mtry = 5:7, node.size = 1:3, \n                           num.trees = seq(200,500,100), \n                           oob_error = 0)\nhead(grid.search)\n\n\n  \n\n\n\nNow, we will fit the random forest model with the selected grids. We will incorporate sampling weight as the case weight in the ranger function.\n\n## Calculate prediction error for each grid \nfor(ii in 1:nrow(grid.search)) {\n  # Model on training set with grid\n  fit.rf.tune &lt;- ranger(formula = Formula, \n                        data = dat.train, \n                        num.trees = grid.search$num.trees[ii],\n                        mtry = grid.search$mtry[ii], \n                        min.node.size = grid.search$node.size[ii],\n                        importance = 'impurity', \n                        case.weights = dat.train$wgt)\n  \n  # Add Out-of-bag (OOB) error to each grid\n  grid.search$oob_error[ii] &lt;- sqrt(fit.rf.tune$prediction.error)\n}\nhead(grid.search)\n\n\n  \n\n\n\nLet’s check which combination of hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) gives minimum prediction error.\n\nposition &lt;- which.min(grid.search$oob_error)\ngrid.search[position,]\n\n\n  \n\n\n\nModel after tuning\nNow, we will fit the random forest model with the tuned hyperparameters.\n\n# Fit the model on the training set \nfit.rf &lt;- ranger(formula = Formula, \n                 data = dat.train, \n                 case.weights = dat.train$wgt, \n                 probability = T,\n                 num.trees = grid.search$num.trees[position], \n                 min.node.size = grid.search$node.size[position], \n                 mtry = grid.search$mtry[position], \n                 importance = 'impurity')\n\n# Fitted random forest model\nfit.rf\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  300 \n#&gt; Sample size:                      5485 \n#&gt; Number of independent variables:  9 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.1110941\n\nNow, we can use the model to make predictions on our test data.\n\n# Pr.(HICP = Yes) on the test set\ndat.test$pred.rf &lt;- predict(fit.rf, \n                            data = dat.test)$predictions[,2]\nhead(dat.test$pred.rf)\n#&gt; [1] 0.031093101 0.003712312 0.129992168 0.044802288 0.025764960 0.307125058\n\nModel performance\nThe same as the LASSO model, we can calculate the AUC, calibration slope, and Brier score.\n\n# AUC on the test set with sampling weights\nauc.rf &lt;- WeightedAUC(WeightedROC(dat.test$pred.rf, \n                                  dat.test$HICP, \n                                  weight = dat.test$wgt))\nauc.rf\n#&gt; [1] 0.6941022\n\n\n# Logit of the predicted probability\ndat.test$pred.rf[dat.test$pred.rf == 0] &lt;- 0.00001\ndat.test$pred.rf.logit &lt;- Logit(dat.test$pred.rf)\n\n# Weighted calibration slope\nmod.cal &lt;- glm(HICP ~ pred.rf.logit, \n               data = dat.test, \n               family = binomial, \n               weights = wgt)\ncal.slope.rf &lt;- summary(mod.cal)$coef[2,1]\ncal.slope.rf\n#&gt; [1] 0.4977901\n\n\n# Weighted Brier Score\nbrier.rf &lt;- mean(brierscore(HICP ~ dat.test$pred.rf, \n                            data = dat.test,\n                            wt = dat.test$wgt))\nbrier.rf\n#&gt; [1] 0.1095384\n\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\nggplot(\n  enframe(fit.rf$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") + \n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\nAs per the figure, marital status, poverty status, sex, and arthritis are the most influential predictors in predicting HICP, while stroke is the least important predictor.\nPerformance comparison\n\n\n\n\n\n\n\n\nModel\nAUC\nCalibration slope\nBrier score\n\n\n\nLASSO\n0.7662941\n1.2446448\n0.0997855\n\n\nRandom forest\n0.6941022\n0.4977901\n0.1095384\n\n\nReferences\n\n\n\n\nBruin, J. 2023. “Advanced Topics in Survey Data Analysis.” 2023. https://stats.oarc.ucla.edu/other/mult-pkg/seminars/advanced-topics-in-survey-data-analysis/.\n\n\nChristodoulou, Evangelia, Jie Ma, Gary S Collins, Ewout W Steyerberg, Jan Y Verbakel, and Ben Van Calster. 2019. “A Systematic Review Shows No Performance Benefit of Machine Learning over Logistic Regression for Clinical Prediction Models.” Journal of Clinical Epidemiology 110: 12–22.\n\n\nSteyerberg, Ewout W, Frank E Harrell Jr, Gerard JJM Borsboom, MJC Eijkemans, Yvonne Vergouwe, and J Dik F Habbema. 2001. “Internal Validation of Predictive Models: Efficiency of Some Procedures for Logistic Regression Analysis.” Journal of Clinical Epidemiology 54 (8): 774–81.\n\n\nSteyerberg, Ewout W, and Ewout W Steyerberg. 2019. “Overfitting and Optimism in Prediction Models.” Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating, 95–112.\n\n\nSteyerberg, Ewout W, and Yvonne Vergouwe. 2014. “Towards Better Clinical Prediction Models: Seven Steps for Development and an ABCD for Validation.” European Heart Journal 35 (29): 1925–31.\n\n\nSteyerberg, Ewout W, Andrew J Vickers, Nancy R Cook, Thomas Gerds, Mithat Gonen, Nancy Obuchowski, Michael J Pencina, and Michael W Kattan. 2010. “Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures.” Epidemiology (Cambridge, Mass.) 21 (1): 128.",
    "crumbs": [
      "Machine learning (ML)",
      "NHIS Example"
    ]
  },
  {
    "objectID": "machinelearning6b.html",
    "href": "machinelearning6b.html",
    "title": "Replicate Results",
    "section": "",
    "text": "The tutorial aims to guide the users through fitting machine learning techniques with health survey data. We will replicate some of the results of this article by Falasinnu et al. (2023).\n\n\nFalasinnu T, Hossain MB, Weber II KA, Helmick CG, Karim ME, Mackey S. The Problem of Pain in the United States: A Population-Based Characterization of Biopsychosocial Correlates of High Impact Chronic Pain Using the National Health Interview Survey. The Journal of Pain. 2023;24(6):1094-103. DOI: 10.1016/j.jpain.2023.03.008\nThe authors used the National Health Interview Survey (NHIS) 2016 dataset to develop prediction models for predicting high impact chronic pain (HICP). They also evaluated the predictive performances of the models within sociodemographic subgroups, such as sex (male, female), age (\\(&lt;65\\), \\(\\ge 65\\)), and race/ethnicity (White, Black, Hispanic). They used LASSO and random forest models with 5-fold cross-validation as an internal validation. To obtain population-level predictions, they account for survey weights in both models.\n\n\nFor those interested in the National Health Interview Survey (NHIS) dataset, can review the earlier tutorial about the dataset.\n\n\n\n\n\n\nNote\n\n\n\nTo handle missing data in the predictors, they used multiple imputation technique. However, for simplicity, this tutorial focuses on a complete case dataset. We will also only focus on predicting HICP for people aged 65 years or older (a dataset of ~8,800 participants compared to the dataset of 33,000 participants aged 18 years or older).\n\n\nLoad packages\nWe load several R packages required for fitting LASSO and random forest models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(gtsummary)\nlibrary(glmnet)\nlibrary(WeightedROC)\nlibrary(ranger)\nlibrary(scoring)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(mlr3misc)\n\nAnalytic dataset\nLoad\nWe load the dataset into the R environment and lists all available variables and objects.\n\nload(\"Data/machinelearning/Falasinnu2023.RData\")\nls()\n#&gt; [1] \"dat\"\n\n\ndim(dat)\n#&gt; [1] 8881   49\n\nThe dataset contains 8,881 participants aged 65 years or older with 49 variables:\n\n\nstudyid: Unique identifier\n\npsu: Pseudo-PSU\n\nstrata: Pseudo-stratum\n\nweight: Sampling weight\n\nHICP: HICP (binary outcome variable)\n\nage: Age\n\nsex: Sex\n\nhhsize: Number of people in household\n\nborn: Citizenship\n\nmarital: Marital status\n\nregion: Region\n\nrace: Race/ethnicity\n\neducation: Education\n\nemployment.status: Employment status\n\npoverty.status: Poverty status\n\nveteran: Veteran\n\ninsurance: Health insurance coverage\n\nsex.orientation: Sexual orientation\n\nworried.money: Worried about money\n\ngood.neighborhood: Good neighborhood\n\npsy.symptom: Psychological symptoms\n\nvisit.ED: Number of times in ER/ED\n\nsurgery: Number of surgeries in past 12 months\n\ndr.visit: Time since doctor visits\n\ncancer: Cancer\n\nasthma: Asthma\n\nhtn: Hypertension\n\nliver.disease: Liver disease\n\ndiabetes: Diabetes\n\nulcer: Ulcer\n\nstroke: Stroke\n\nemphysema: Emphysema\n\ncopd: COPD\n\nhigh.cholesterol: High cholesterol\n\ncoronary.heart.disease: Coronary heart disease\n\nangina: Angina pectoris\n\nheart.attack: Heart attack\n\nheart.disease: Heart condition/disease\n\narthritis: Arthritis and rheumatism\n\ncrohns.disease: Crohn’s disease\n\nplace.routine.care: Usual place for routine care\n\ntrouble.asleep: Trouble falling asleep\n\nobese: Obesity\n\ncurrent.smoker: Current smoker\n\nheavy.drinker: Heavy drinker\n\nhospitalization: Hospital stay days\n\nbetter.health.status: Better health status\n\nphysical.activity: Physical activity\n\nSee the NHIS 2016 dataset and the article for better understanding of the variables.\nComplete case data\n\n# Age\ntable(dat$age, useNA = \"always\")\n#&gt; \n#&gt;  &lt;65  65+ &lt;NA&gt; \n#&gt;    0 8881    0\n\nLet us consider a complete case dataset\n\ndat.complete &lt;- na.omit(dat)\ndim(dat.complete)\n#&gt; [1] 7280   49\n\nAs we can see, there are 7,280 participants with complete case information. Let’s see the descriptive statistics of the predictors stratified by HICP.\nDescriptive statistics\n\n# Predictors\npredictors &lt;- c(\"sex\", \"hhsize\", \"born\", \"marital\", \n                \"region\", \"race\", \"education\", \n                \"employment.status\", \"poverty.status\",\n                \"veteran\", \"insurance\", \n                \"sex.orientation\", \"worried.money\", \n                \"good.neighborhood\", \n                \"psy.symptom\", \"visit.ED\", \"surgery\", \n                \"dr.visit\", \"cancer\", \n                \"asthma\", \"htn\", \"liver.disease\", \n                \"diabetes\", \"ulcer\", \"stroke\",\n                \"emphysema\", \"copd\", \"high.cholesterol\",\n                \"coronary.heart.disease\", \n                \"angina\", \"heart.attack\", \n                \"heart.disease\", \"arthritis\", \n                \"crohns.disease\", \"place.routine.care\", \n                \"trouble.asleep\", \"obese\", \n                \"current.smoker\", \"heavy.drinker\",\n                \"hospitalization\", \n                \"better.health.status\", \n                \"physical.activity\")\n\n# Table 1 - Unweighted \ntbl_summary(data = dat.complete, \n            include = predictors, \n            by = HICP, missing = \"no\") %&gt;% \n  modify_spanning_header(c(\"stat_1\", \n                           \"stat_2\") ~ \"**HICP**\")\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nHICP\n\n\n\n\n0\nN = 6,3891\n\n\n1\nN = 8911\n\n\n\n\n\nsex\n\n\n\n\n    Female\n3,587 (56%)\n569 (64%)\n\n\n    Male\n2,802 (44%)\n322 (36%)\n\n\nhhsize\n2 (1, 2)\n2 (1, 2)\n\n\nborn\n\n\n\n\n    Born in US\n5,775 (90%)\n802 (90%)\n\n\n    Other place\n614 (9.6%)\n89 (10.0%)\n\n\nmarital\n\n\n\n\n    Never married\n411 (6.4%)\n57 (6.4%)\n\n\n    Married/with partner\n2,990 (47%)\n349 (39%)\n\n\n    Divorced/separated\n1,161 (18%)\n179 (20%)\n\n\n    Widowed\n1,827 (29%)\n306 (34%)\n\n\nregion\n\n\n\n\n    Northeast\n1,172 (18%)\n143 (16%)\n\n\n    Midwest\n1,451 (23%)\n189 (21%)\n\n\n    South\n2,203 (34%)\n331 (37%)\n\n\n    West\n1,563 (24%)\n228 (26%)\n\n\nrace\n\n\n\n\n    White\n5,090 (80%)\n694 (78%)\n\n\n    Black\n564 (8.8%)\n88 (9.9%)\n\n\n    Hispanic\n406 (6.4%)\n70 (7.9%)\n\n\n    Others\n329 (5.1%)\n39 (4.4%)\n\n\neducation\n\n\n\n\n    Less than high school\n954 (15%)\n220 (25%)\n\n\n    High school/GED\n1,863 (29%)\n269 (30%)\n\n\n    Some college\n1,716 (27%)\n248 (28%)\n\n\n    Bachelors degree or higher\n1,856 (29%)\n154 (17%)\n\n\nemployment.status\n\n\n\n\n    Employed hourly\n578 (9.0%)\n22 (2.5%)\n\n\n    Employed non-hourly\n608 (9.5%)\n27 (3.0%)\n\n\n    Worked previously\n4,923 (77%)\n777 (87%)\n\n\n    Never worked\n280 (4.4%)\n65 (7.3%)\n\n\npoverty.status\n\n\n\n\n    &lt;100% FPL\n496 (7.8%)\n154 (17%)\n\n\n    100-200% FPL\n1,401 (22%)\n276 (31%)\n\n\n    200-400% FPL\n2,157 (34%)\n283 (32%)\n\n\n    400%+ FPL\n2,335 (37%)\n178 (20%)\n\n\nveteran\n1,482 (23%)\n163 (18%)\n\n\ninsurance\n\n\n\n\n    Uninsured\n32 (0.5%)\n6 (0.7%)\n\n\n    Medicaid/Medicare\n2,995 (47%)\n478 (54%)\n\n\n    Privately Insured\n2,847 (45%)\n311 (35%)\n\n\n    Other\n515 (8.1%)\n96 (11%)\n\n\nsex.orientation\n\n\n\n\n    Heterosexual\n6,224 (97%)\n861 (97%)\n\n\n    Other\n165 (2.6%)\n30 (3.4%)\n\n\nworried.money\n2,351 (37%)\n491 (55%)\n\n\ngood.neighborhood\n5,988 (94%)\n781 (88%)\n\n\npsy.symptom\n694 (11%)\n353 (40%)\n\n\nvisit.ED\n\n\n\n\n    None\n5,050 (79%)\n531 (60%)\n\n\n    One\n949 (15%)\n187 (21%)\n\n\n    2-3\n313 (4.9%)\n123 (14%)\n\n\n    4+\n77 (1.2%)\n50 (5.6%)\n\n\nsurgery\n\n\n\n\n    None\n5,218 (82%)\n650 (73%)\n\n\n    One\n898 (14%)\n176 (20%)\n\n\n    Two\n206 (3.2%)\n44 (4.9%)\n\n\n    3+\n67 (1.0%)\n21 (2.4%)\n\n\ndr.visit\n\n\n\n\n    &lt;6 months\n5,390 (84%)\n837 (94%)\n\n\n    6-12 months\n591 (9.3%)\n41 (4.6%)\n\n\n    1-5 years\n281 (4.4%)\n10 (1.1%)\n\n\n    &gt;5 years/never\n127 (2.0%)\n3 (0.3%)\n\n\ncancer\n1,566 (25%)\n271 (30%)\n\n\nasthma\n645 (10%)\n176 (20%)\n\n\nhtn\n3,953 (62%)\n689 (77%)\n\n\nliver.disease\n122 (1.9%)\n50 (5.6%)\n\n\ndiabetes\n1,179 (18%)\n278 (31%)\n\n\nulcer\n545 (8.5%)\n161 (18%)\n\n\nstroke\n485 (7.6%)\n130 (15%)\n\n\nemphysema\n235 (3.7%)\n76 (8.5%)\n\n\ncopd\n496 (7.8%)\n145 (16%)\n\n\nhigh.cholesterol\n3,373 (53%)\n577 (65%)\n\n\ncoronary.heart.disease\n814 (13%)\n211 (24%)\n\n\nangina\n298 (4.7%)\n100 (11%)\n\n\nheart.attack\n552 (8.6%)\n154 (17%)\n\n\nheart.disease\n1,083 (17%)\n210 (24%)\n\n\narthritis\n3,017 (47%)\n700 (79%)\n\n\ncrohns.disease\n102 (1.6%)\n29 (3.3%)\n\n\nplace.routine.care\n\n\n\n\n    No place\n235 (3.7%)\n26 (2.9%)\n\n\n    Doctor's office\n4,634 (73%)\n621 (70%)\n\n\n    Hospital/Clinic\n1,403 (22%)\n221 (25%)\n\n\n    Other place\n117 (1.8%)\n23 (2.6%)\n\n\ntrouble.asleep\n1,970 (31%)\n424 (48%)\n\n\nobese\n1,757 (28%)\n397 (45%)\n\n\ncurrent.smoker\n565 (8.8%)\n111 (12%)\n\n\nheavy.drinker\n308 (4.8%)\n25 (2.8%)\n\n\nhospitalization\n\n\n\n\n    None\n5,009 (78%)\n503 (56%)\n\n\n    1-2 days\n592 (9.3%)\n57 (6.4%)\n\n\n    3-5 days\n360 (5.6%)\n63 (7.1%)\n\n\n    6+ days\n428 (6.7%)\n268 (30%)\n\n\nbetter.health.status\n890 (14%)\n119 (13%)\n\n\nphysical.activity\n\n\n\n\n    Less\n3,734 (58%)\n743 (83%)\n\n\n    Moderate\n1,794 (28%)\n108 (12%)\n\n\n    High\n861 (13%)\n40 (4.5%)\n\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n\n\n\nLASSO for surveys\nNow, we will fit the LASSO model for predicting binary HICP with the listed predictors. Similar to the previous chapter, we will normalize the weight.\nWeight normalization\n\n# Normalize weight\ndat.complete$wgt &lt;- dat.complete$weight * \n  nrow(dat.complete)/sum(dat.complete$weight)\n\n# Weight summary\nsummary(dat.complete$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     243    1503    2583    2886    3747   14662\nsummary(dat.complete$wgt)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.0842  0.5208  0.8950  1.0000  1.2983  5.0804\n\n# The weighted and unweighted n are equal\nnrow(dat.complete)\n#&gt; [1] 7280\nsum(dat.complete$wgt)\n#&gt; [1] 7280\n\nFolds\nLet’s create five random folds and specify the regression formula.\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, \n                 size = nrow(dat.complete), \n                 replace = T)\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1451 1457 1496 1468 1408\n\nFormula\n\nFormula &lt;- formula(paste(\"HICP ~ \", paste(predictors, \n                                          collapse=\" + \")))\nFormula\n#&gt; HICP ~ sex + hhsize + born + marital + region + race + education + \n#&gt;     employment.status + poverty.status + veteran + insurance + \n#&gt;     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#&gt;     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#&gt;     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#&gt;     coronary.heart.disease + angina + heart.attack + heart.disease + \n#&gt;     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#&gt;     obese + current.smoker + heavy.drinker + hospitalization + \n#&gt;     better.health.status + physical.activity\n\n5-fold CV LASSO\nNow, we will fit the LASSO model with 5-fold cross-validation (CV). Here are the steps:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit 5-fold cross-validation on the training set to find the value of lambda that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nFit LASSO on the training with the optimum lambda from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\n\nfit.lasso &lt;- list(NULL)\nauc.lasso &lt;- NULL\ncal.slope.lasso &lt;- NULL\nbrier.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$HICP)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$HICP)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, \n                            y = y.train, \n                            nfolds = 5, \n                            alpha = 1, \n                            family = \"binomial\", \n                            weights = dat.train$wgt)\n  \n  # Fit the model on the training set with optimum lambda\n  fit.lasso[[fold]] &lt;- glmnet(\n    x = X.train, \n    y = y.train, \n    alpha = 1, \n    family = \"binomial\",\n    lambda = fit.cv.lasso$lambda.min,\n    weights = dat.train$wgt)\n  \n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.lasso[[fold]], \n                                 newx = X.test, \n                                 type = \"response\")\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(\n    WeightedROC(dat.test$pred.lasso,\n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  mod.cal &lt;- glm(\n    HICP ~ Logit(dat.test$pred.lasso), \n    data = dat.test, \n    family = binomial, \n    weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.lasso[fold] &lt;- mean(\n    brierscore(HICP ~ dat.test$pred.lasso,\n               data = dat.test, \n               wt = dat.test$wgt))\n}\n\nModel performance\nLet’s check how prediction worked.\n\n# Fitted LASSO models\nfit.lasso[[1]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 46 23.91 0.002972\nfit.lasso[[2]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 47 25.32 0.002559\nfit.lasso[[3]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df %Dev   Lambda\n#&gt; 1 47 24.3 0.002724\nfit.lasso[[4]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 34 25.27 0.004726\nfit.lasso[[5]]\n#&gt; \n#&gt; Call:  glmnet(x = X.train, y = y.train, family = \"binomial\", weights = dat.train$wgt,      alpha = 1, lambda = fit.cv.lasso$lambda.min) \n#&gt; \n#&gt;   Df  %Dev   Lambda\n#&gt; 1 39 24.32 0.003525\n\n\n# Intercept from the LASSO models in different folds\nfit.lasso[[1]]$a0\n#&gt;        s0 \n#&gt; -3.733405\nfit.lasso[[2]]$a0\n#&gt;        s0 \n#&gt; -3.534898\nfit.lasso[[3]]$a0\n#&gt;        s0 \n#&gt; -3.486223\nfit.lasso[[4]]$a0\n#&gt;        s0 \n#&gt; -3.800206\nfit.lasso[[5]]$a0\n#&gt;       s0 \n#&gt; -3.68776\n\n\n# Beta coefficients from the LASSO models in different folds\nfit.lasso[[1]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                 s0\n#&gt; sexMale                               .           \n#&gt; hhsize                                0.0092060605\n#&gt; bornOther place                       .           \n#&gt; maritalMarried/with partner           .           \n#&gt; maritalDivorced/separated             .           \n#&gt; maritalWidowed                        0.0926365970\n#&gt; regionMidwest                         .           \n#&gt; regionSouth                           .           \n#&gt; regionWest                            0.1470219542\n#&gt; raceBlack                            -0.0436983700\n#&gt; raceHispanic                          .           \n#&gt; raceOthers                            .           \n#&gt; educationHigh school/GED              .           \n#&gt; educationSome college                 .           \n#&gt; educationBachelors degree or higher   .           \n#&gt; employment.statusEmployed non-hourly  .           \n#&gt; employment.statusWorked previously    0.5412332930\n#&gt; employment.statusNever worked         0.8300536966\n#&gt; poverty.status100-200% FPL            0.0636289868\n#&gt; poverty.status200-400% FPL           -0.0697612513\n#&gt; poverty.status400%+ FPL              -0.2887583235\n#&gt; veteranYes                           -0.0300714331\n#&gt; insuranceMedicaid/Medicare            .           \n#&gt; insurancePrivately Insured           -0.0924641963\n#&gt; insuranceOther                        0.1503323815\n#&gt; sex.orientationOther                  0.0765093892\n#&gt; worried.moneyYes                      0.2812674935\n#&gt; good.neighborhoodYes                 -0.2491796538\n#&gt; psy.symptomYes                        0.9726200151\n#&gt; visit.EDOne                           0.0674459188\n#&gt; visit.ED2-3                           0.1375781535\n#&gt; visit.ED4+                            0.4225671575\n#&gt; surgeryOne                            .           \n#&gt; surgeryTwo                            .           \n#&gt; surgery3+                            -0.0839622285\n#&gt; dr.visit6-12 months                  -0.2120063637\n#&gt; dr.visit1-5 years                    -0.4068474570\n#&gt; dr.visit&gt;5 years/never               -0.1453128227\n#&gt; cancerYes                             0.0993613856\n#&gt; asthmaYes                             0.2997325771\n#&gt; htnYes                                0.2082877598\n#&gt; liver.diseaseYes                      0.8058966864\n#&gt; diabetesYes                           0.0007357323\n#&gt; ulcerYes                              0.4180133887\n#&gt; strokeYes                             .           \n#&gt; emphysemaYes                         -0.0509549802\n#&gt; copdYes                               0.1002374105\n#&gt; high.cholesterolYes                   0.1021030868\n#&gt; coronary.heart.diseaseYes             0.0220558291\n#&gt; anginaYes                             0.0849595261\n#&gt; heart.attackYes                       0.2476656673\n#&gt; heart.diseaseYes                      .           \n#&gt; arthritisYes                          0.9746692568\n#&gt; crohns.diseaseYes                     .           \n#&gt; place.routine.careDoctor's office    -0.0583977619\n#&gt; place.routine.careHospital/Clinic     .           \n#&gt; place.routine.careOther place         .           \n#&gt; trouble.asleepYes                     0.2030296869\n#&gt; obeseYes                              0.3879895531\n#&gt; current.smokerYes                     0.3374178965\n#&gt; heavy.drinkerYes                     -0.1268971235\n#&gt; hospitalization1-2 days               0.1192556336\n#&gt; hospitalization3-5 days               .           \n#&gt; hospitalization6+ days                1.0866087297\n#&gt; better.health.statusYes              -0.1013785074\n#&gt; physical.activityModerate            -0.7523741651\n#&gt; physical.activityHigh                -0.6865233686\nfit.lasso[[2]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                s0\n#&gt; sexMale                               .          \n#&gt; hhsize                                0.018493189\n#&gt; bornOther place                       .          \n#&gt; maritalMarried/with partner           .          \n#&gt; maritalDivorced/separated            -0.001903136\n#&gt; maritalWidowed                        .          \n#&gt; regionMidwest                        -0.077656662\n#&gt; regionSouth                           0.066061919\n#&gt; regionWest                            0.198988965\n#&gt; raceBlack                            -0.313705357\n#&gt; raceHispanic                         -0.160881871\n#&gt; raceOthers                            .          \n#&gt; educationHigh school/GED             -0.073948767\n#&gt; educationSome college                 .          \n#&gt; educationBachelors degree or higher  -0.092601336\n#&gt; employment.statusEmployed non-hourly  .          \n#&gt; employment.statusWorked previously    0.662301617\n#&gt; employment.statusNever worked         0.990458783\n#&gt; poverty.status100-200% FPL            .          \n#&gt; poverty.status200-400% FPL           -0.253983648\n#&gt; poverty.status400%+ FPL              -0.485846823\n#&gt; veteranYes                           -0.095649252\n#&gt; insuranceMedicaid/Medicare            .          \n#&gt; insurancePrivately Insured           -0.005530756\n#&gt; insuranceOther                        0.249278523\n#&gt; sex.orientationOther                  .          \n#&gt; worried.moneyYes                      0.301627606\n#&gt; good.neighborhoodYes                 -0.554793692\n#&gt; psy.symptomYes                        0.975043141\n#&gt; visit.EDOne                           0.058872693\n#&gt; visit.ED2-3                           0.255785667\n#&gt; visit.ED4+                            0.422172434\n#&gt; surgeryOne                            0.002831717\n#&gt; surgeryTwo                            0.099101540\n#&gt; surgery3+                             .          \n#&gt; dr.visit6-12 months                  -0.105087667\n#&gt; dr.visit1-5 years                    -0.381233762\n#&gt; dr.visit&gt;5 years/never               -0.459656177\n#&gt; cancerYes                             0.281041121\n#&gt; asthmaYes                             0.427654297\n#&gt; htnYes                                0.219625784\n#&gt; liver.diseaseYes                      0.580991873\n#&gt; diabetesYes                           .          \n#&gt; ulcerYes                              0.348542693\n#&gt; strokeYes                             0.070369016\n#&gt; emphysemaYes                          .          \n#&gt; copdYes                               .          \n#&gt; high.cholesterolYes                   0.150862244\n#&gt; coronary.heart.diseaseYes             0.009536678\n#&gt; anginaYes                             .          \n#&gt; heart.attackYes                       0.405053759\n#&gt; heart.diseaseYes                      .          \n#&gt; arthritisYes                          0.954063592\n#&gt; crohns.diseaseYes                     .          \n#&gt; place.routine.careDoctor's office    -0.045764606\n#&gt; place.routine.careHospital/Clinic     .          \n#&gt; place.routine.careOther place         0.207253975\n#&gt; trouble.asleepYes                     0.212898902\n#&gt; obeseYes                              0.389340100\n#&gt; current.smokerYes                     0.332982403\n#&gt; heavy.drinkerYes                     -0.135194457\n#&gt; hospitalization1-2 days               .          \n#&gt; hospitalization3-5 days               .          \n#&gt; hospitalization6+ days                1.018420971\n#&gt; better.health.statusYes              -0.001173805\n#&gt; physical.activityModerate            -0.703703292\n#&gt; physical.activityHigh                -0.677811398\nfit.lasso[[3]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                               s0\n#&gt; sexMale                              -0.03555390\n#&gt; hhsize                                0.01276707\n#&gt; bornOther place                       .         \n#&gt; maritalMarried/with partner          -0.00226132\n#&gt; maritalDivorced/separated             .         \n#&gt; maritalWidowed                        .         \n#&gt; regionMidwest                         .         \n#&gt; regionSouth                           .         \n#&gt; regionWest                            0.25300309\n#&gt; raceBlack                            -0.21629021\n#&gt; raceHispanic                          .         \n#&gt; raceOthers                            0.07458935\n#&gt; educationHigh school/GED             -0.07527970\n#&gt; educationSome college                 .         \n#&gt; educationBachelors degree or higher  -0.08524865\n#&gt; employment.statusEmployed non-hourly  .         \n#&gt; employment.statusWorked previously    0.66693373\n#&gt; employment.statusNever worked         0.96274685\n#&gt; poverty.status100-200% FPL            .         \n#&gt; poverty.status200-400% FPL           -0.11543598\n#&gt; poverty.status400%+ FPL              -0.31422327\n#&gt; veteranYes                           -0.08849244\n#&gt; insuranceMedicaid/Medicare            .         \n#&gt; insurancePrivately Insured           -0.14856615\n#&gt; insuranceOther                        0.14368311\n#&gt; sex.orientationOther                  .         \n#&gt; worried.moneyYes                      0.28107756\n#&gt; good.neighborhoodYes                 -0.37214169\n#&gt; psy.symptomYes                        1.02129791\n#&gt; visit.EDOne                           0.11427725\n#&gt; visit.ED2-3                           0.23654896\n#&gt; visit.ED4+                            0.53011900\n#&gt; surgeryOne                            0.06030144\n#&gt; surgeryTwo                            .         \n#&gt; surgery3+                            -0.28355643\n#&gt; dr.visit6-12 months                  -0.04219568\n#&gt; dr.visit1-5 years                    -0.83131719\n#&gt; dr.visit&gt;5 years/never               -0.48832578\n#&gt; cancerYes                             0.18366379\n#&gt; asthmaYes                             0.13556093\n#&gt; htnYes                                0.12622357\n#&gt; liver.diseaseYes                      0.62123990\n#&gt; diabetesYes                           .         \n#&gt; ulcerYes                              0.39650846\n#&gt; strokeYes                             .         \n#&gt; emphysemaYes                          .         \n#&gt; copdYes                               0.16563326\n#&gt; high.cholesterolYes                   0.10541633\n#&gt; coronary.heart.diseaseYes             0.08229669\n#&gt; anginaYes                             .         \n#&gt; heart.attackYes                       0.37154172\n#&gt; heart.diseaseYes                      .         \n#&gt; arthritisYes                          0.91902311\n#&gt; crohns.diseaseYes                    -0.01478240\n#&gt; place.routine.careDoctor's office    -0.04968533\n#&gt; place.routine.careHospital/Clinic     .         \n#&gt; place.routine.careOther place         0.20094580\n#&gt; trouble.asleepYes                     0.07725267\n#&gt; obeseYes                              0.40542559\n#&gt; current.smokerYes                     0.27207489\n#&gt; heavy.drinkerYes                     -0.06932537\n#&gt; hospitalization1-2 days              -0.13525647\n#&gt; hospitalization3-5 days               .         \n#&gt; hospitalization6+ days                1.02580763\n#&gt; better.health.statusYes               .         \n#&gt; physical.activityModerate            -0.74686507\n#&gt; physical.activityHigh                -0.90554222\nfit.lasso[[4]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                                s0\n#&gt; sexMale                               .          \n#&gt; hhsize                                .          \n#&gt; bornOther place                       .          \n#&gt; maritalMarried/with partner           .          \n#&gt; maritalDivorced/separated             .          \n#&gt; maritalWidowed                        .          \n#&gt; regionMidwest                         .          \n#&gt; regionSouth                           .          \n#&gt; regionWest                            0.151732262\n#&gt; raceBlack                            -0.063747960\n#&gt; raceHispanic                          .          \n#&gt; raceOthers                            .          \n#&gt; educationHigh school/GED              .          \n#&gt; educationSome college                 .          \n#&gt; educationBachelors degree or higher   .          \n#&gt; employment.statusEmployed non-hourly  .          \n#&gt; employment.statusWorked previously    0.521879242\n#&gt; employment.statusNever worked         0.671265401\n#&gt; poverty.status100-200% FPL            .          \n#&gt; poverty.status200-400% FPL           -0.014851029\n#&gt; poverty.status400%+ FPL              -0.229122884\n#&gt; veteranYes                            .          \n#&gt; insuranceMedicaid/Medicare            .          \n#&gt; insurancePrivately Insured           -0.092128877\n#&gt; insuranceOther                        .          \n#&gt; sex.orientationOther                  0.150490732\n#&gt; worried.moneyYes                      0.282333603\n#&gt; good.neighborhoodYes                 -0.104236603\n#&gt; psy.symptomYes                        1.132555465\n#&gt; visit.EDOne                           0.009230339\n#&gt; visit.ED2-3                           0.288663967\n#&gt; visit.ED4+                            0.673934923\n#&gt; surgeryOne                            .          \n#&gt; surgeryTwo                            .          \n#&gt; surgery3+                             .          \n#&gt; dr.visit6-12 months                  -0.018831608\n#&gt; dr.visit1-5 years                    -0.496368171\n#&gt; dr.visit&gt;5 years/never               -0.415165313\n#&gt; cancerYes                             .          \n#&gt; asthmaYes                             0.240957776\n#&gt; htnYes                                0.125845727\n#&gt; liver.diseaseYes                      0.672282863\n#&gt; diabetesYes                           .          \n#&gt; ulcerYes                              0.352645788\n#&gt; strokeYes                             .          \n#&gt; emphysemaYes                          .          \n#&gt; copdYes                               0.054275559\n#&gt; high.cholesterolYes                   0.057735457\n#&gt; coronary.heart.diseaseYes             0.070550439\n#&gt; anginaYes                             .          \n#&gt; heart.attackYes                       0.234807175\n#&gt; heart.diseaseYes                      .          \n#&gt; arthritisYes                          1.043934915\n#&gt; crohns.diseaseYes                     0.086038829\n#&gt; place.routine.careDoctor's office     .          \n#&gt; place.routine.careHospital/Clinic     .          \n#&gt; place.routine.careOther place         .          \n#&gt; trouble.asleepYes                     0.177916442\n#&gt; obeseYes                              0.363088024\n#&gt; current.smokerYes                     0.339985811\n#&gt; heavy.drinkerYes                      .          \n#&gt; hospitalization1-2 days               .          \n#&gt; hospitalization3-5 days               0.073321609\n#&gt; hospitalization6+ days                1.066617646\n#&gt; better.health.statusYes               .          \n#&gt; physical.activityModerate            -0.699175264\n#&gt; physical.activityHigh                -0.642594150\nfit.lasso[[5]]$beta\n#&gt; 67 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                               s0\n#&gt; sexMale                               .         \n#&gt; hhsize                                0.01585559\n#&gt; bornOther place                       .         \n#&gt; maritalMarried/with partner           .         \n#&gt; maritalDivorced/separated            -0.04982466\n#&gt; maritalWidowed                        0.03837620\n#&gt; regionMidwest                        -0.25040909\n#&gt; regionSouth                           .         \n#&gt; regionWest                            0.08288425\n#&gt; raceBlack                            -0.07020324\n#&gt; raceHispanic                          .         \n#&gt; raceOthers                            .         \n#&gt; educationHigh school/GED              .         \n#&gt; educationSome college                 .         \n#&gt; educationBachelors degree or higher   .         \n#&gt; employment.statusEmployed non-hourly  .         \n#&gt; employment.statusWorked previously    0.64497666\n#&gt; employment.statusNever worked         0.83560645\n#&gt; poverty.status100-200% FPL            .         \n#&gt; poverty.status200-400% FPL           -0.03113577\n#&gt; poverty.status400%+ FPL              -0.22313059\n#&gt; veteranYes                           -0.20269803\n#&gt; insuranceMedicaid/Medicare            .         \n#&gt; insurancePrivately Insured           -0.14170986\n#&gt; insuranceOther                        0.19604683\n#&gt; sex.orientationOther                  .         \n#&gt; worried.moneyYes                      0.31572735\n#&gt; good.neighborhoodYes                 -0.31850186\n#&gt; psy.symptomYes                        1.15194136\n#&gt; visit.EDOne                           0.05961452\n#&gt; visit.ED2-3                           0.34349175\n#&gt; visit.ED4+                            0.25637410\n#&gt; surgeryOne                            .         \n#&gt; surgeryTwo                            .         \n#&gt; surgery3+                             .         \n#&gt; dr.visit6-12 months                   .         \n#&gt; dr.visit1-5 years                    -0.32790023\n#&gt; dr.visit&gt;5 years/never                .         \n#&gt; cancerYes                             0.21372688\n#&gt; asthmaYes                             0.21206722\n#&gt; htnYes                                0.13025900\n#&gt; liver.diseaseYes                      0.44892643\n#&gt; diabetesYes                           0.05119678\n#&gt; ulcerYes                              0.27325347\n#&gt; strokeYes                             .         \n#&gt; emphysemaYes                          .         \n#&gt; copdYes                               0.04954730\n#&gt; high.cholesterolYes                   0.14824572\n#&gt; coronary.heart.diseaseYes             .         \n#&gt; anginaYes                             .         \n#&gt; heart.attackYes                       0.27924438\n#&gt; heart.diseaseYes                      .         \n#&gt; arthritisYes                          1.00382986\n#&gt; crohns.diseaseYes                     .         \n#&gt; place.routine.careDoctor's office    -0.01943640\n#&gt; place.routine.careHospital/Clinic     .         \n#&gt; place.routine.careOther place         0.07273335\n#&gt; trouble.asleepYes                     0.14684831\n#&gt; obeseYes                              0.38029871\n#&gt; current.smokerYes                     0.14111139\n#&gt; heavy.drinkerYes                      .         \n#&gt; hospitalization1-2 days              -0.02310396\n#&gt; hospitalization3-5 days               .         \n#&gt; hospitalization6+ days                1.08731840\n#&gt; better.health.statusYes               .         \n#&gt; physical.activityModerate            -0.76639891\n#&gt; physical.activityHigh                -0.38546251\n\n\n# AUCs from different folds\nauc.lasso\n#&gt; [1] 0.8396619 0.8129805 0.8465035 0.7896878 0.8342721\n\n# Calibration slope from different folds\ncal.slope.lasso\n#&gt; [1] 1.1287166 0.9985467 1.1224240 0.8934633 1.0131154\n\n# Brier score from different folds\nbrier.lasso\n#&gt; [1] 0.08524781 0.08476661 0.08666820 0.09071329 0.08911735\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.lasso)\n#&gt; [1] 0.8246212\n\n# Average calibration slope\nmean(cal.slope.lasso)\n#&gt; [1] 1.031253\n\n# Average Brier score\nmean(brier.lasso)\n#&gt; [1] 0.08730265\n\nAlthough the authors used multiple imputation, our AUC from the LASSO model with complete case data analysis is not that different. Note: the authors reported the AUC values in Table 2.\nRandom forest for surveys\nNow, we will fit the random forest model for predicting binary HICP with the listed predictors. Here are the steps for fitting the model with 5-fold CV:\n\nFor fold 1, folds 2-5 is the training set and fold 1 is the test set\nFit random forest model on the training set to find the value of the hyperparameters (number of trees, number of predictors to split at in each node, and minimal node size to split at) that gives minimum prediction error. Incorporate sampling weights in the model to account for survey design.\nGrid-search with out-of-sample error approach is widely used in the literature. In this approach, we create a data frame from all combinations of the hyperparameters and check which combination gives the lowest out-of-sample error.\nFit the random forest model on the training with the selected hyperparameters from the previous step. Incorporate sampling weights in the model to account for survey design.\nCalculate predictive performance (e.g., AUC) on the test set\nRepeat the analysis for all folds.\n\nFolds\n\nk &lt;- 5\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1451 1457 1496 1468 1408\n\nFormula\n\nFormula\n#&gt; HICP ~ sex + hhsize + born + marital + region + race + education + \n#&gt;     employment.status + poverty.status + veteran + insurance + \n#&gt;     sex.orientation + worried.money + good.neighborhood + psy.symptom + \n#&gt;     visit.ED + surgery + dr.visit + cancer + asthma + htn + liver.disease + \n#&gt;     diabetes + ulcer + stroke + emphysema + copd + high.cholesterol + \n#&gt;     coronary.heart.disease + angina + heart.attack + heart.disease + \n#&gt;     arthritis + crohns.disease + place.routine.care + trouble.asleep + \n#&gt;     obese + current.smoker + heavy.drinker + hospitalization + \n#&gt;     better.health.status + physical.activity\n\n5-fold CV random forest\n\nfit.rf &lt;- list(NULL)\nauc.rf &lt;- NULL\ncal.slope.rf &lt;- brier.rf &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  \n  # Tuning the hyperparameters \n  ## Grid with 1000 models - huge time consuming\n  #grid.search &lt;- expand.grid(mtry = 1:10, node.size = 1:10, \n  #                          num.trees = seq(50,500,50), \n  #                           OOB_RMSE = 0)\n  \n  ## Grid with 36 models as an exercise\n  grid.search &lt;- expand.grid(\n    mtry = 5:7, \n    node.size = 1:3, \n    num.trees = seq(200,500,100),\n    OOB_RMSE = 0) \n  \n  ## Model with grids \n  for(ii in 1:nrow(grid.search)) {\n    # Model on training set with grid\n    fit.rf.tune &lt;- ranger(\n      formula = Formula,\n      data = dat.train, \n      num.trees = grid.search$num.trees[ii],\n      mtry = grid.search$mtry[ii], \n      min.node.size = grid.search$node.size[ii],\n      importance = 'impurity', \n      case.weights = dat.train$wgt)\n    \n    # Add Out-of-bag (OOB) error to grid\n    grid.search$OOB_RMSE[ii] &lt;- \n      sqrt(fit.rf.tune$prediction.error)\n  }\n  # Position of the tuned hyperparameters\n  position &lt;- which.min(grid.search$OOB_RMSE)\n  \n  # Fit the model on the training set with tuned hyperparameters\n  fit.rf[[fold]] &lt;- ranger(\n    formula = Formula,\n    data = dat.train, \n    case.weights = dat.train$wgt, \n    probability = T,\n    num.trees = grid.search$num.trees[position],\n    min.node.size = grid.search$node.size[position], \n    mtry = grid.search$mtry[position], \n    importance = 'impurity')\n  \n  # Prediction on the test set\n  dat.test$pred.rf &lt;- predict(\n    fit.rf[[fold]], \n    data = dat.test)$predictions[,2]\n  \n  # AUC on the test set with sampling weights\n  auc.rf[fold] &lt;- WeightedAUC(\n    WeightedROC(dat.test$pred.rf, \n                dat.test$HICP, \n                weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.rf[dat.test$pred.rf == 0] &lt;- 0.00001\n  mod.cal &lt;- glm(HICP ~ Logit(dat.test$pred.rf), \n                 data = dat.test, \n                 family = binomial, \n                 weights = wgt)\n  cal.slope.rf[fold] &lt;- summary(mod.cal)$coef[2,1]\n  \n  # Weighted Brier Score\n  brier.rf[fold] &lt;- mean(brierscore(\n    HICP ~ dat.test$pred.rf, \n    data = dat.test,\n    wt = dat.test$wgt))\n}\n\nModel performance\nLet’s check how prediction worked.\n\n# Fitted random forest models\nfit.rf[[1]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  500 \n#&gt; Sample size:                      5829 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.0899798\nfit.rf[[2]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5823 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 2 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.0899217\nfit.rf[[3]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  500 \n#&gt; Sample size:                      5784 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08972587\nfit.rf[[4]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5812 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08934626\nfit.rf[[5]]\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger(formula = Formula, data = dat.train, case.weights = dat.train$wgt,      probability = T, num.trees = grid.search$num.trees[position],      min.node.size = grid.search$node.size[position], mtry = grid.search$mtry[position],      importance = \"impurity\") \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  400 \n#&gt; Sample size:                      5872 \n#&gt; Number of independent variables:  42 \n#&gt; Mtry:                             5 \n#&gt; Target node size:                 3 \n#&gt; Variable importance mode:         impurity \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.08929408\n\n\n# AUCs from different folds\nauc.rf\n#&gt; [1] 0.8393204 0.7905715 0.8244523 0.7811141 0.8301950\n\n# Calibration slope from different folds\ncal.slope.rf\n#&gt; [1] 1.2842866 0.9933553 1.1486583 0.9134864 1.2263184\n\n# Brier score from different folds\nbrier.rf\n#&gt; [1] 0.08745016 0.08881079 0.08913696 0.09163393 0.08895764\n\nNow we will average out the model performance measures:\n\n# Average AUC\nmean(auc.rf)\n#&gt; [1] 0.8131307\n\n# Average calibration slope\nmean(cal.slope.rf)\n#&gt; [1] 1.113221\n\n# Average Brier score\nmean(brier.rf)\n#&gt; [1] 0.0891979\n\nThis AUC from random forest is approximately the same as obtained from the LASSO model.\nVariable importance\nOne nice feature of random forest is that we can rank the variables and generate a variable importance plot.\n\n# Fold 1\nggplot(\n  enframe(fit.rf[[1]]$variable.importance, \n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\n# Fold 5\nggplot(\n  enframe(fit.rf[[5]]$variable.importance,\n          name = \"variable\", \n          value = \"importance\"),\n  aes(x = reorder(variable, importance), \n      y = importance, fill = importance)) +\n  geom_bar(stat = \"identity\", \n           position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Variable Importance\") +\n  xlab(\"\") + \n  ggtitle(\"\") +\n  guides(fill = \"none\") +\n  scale_fill_gradient(low = \"grey\", \n                      high = \"grey10\") + \n  theme_bw()\n\n\n\n\n\n\n\nReferences",
    "crumbs": [
      "Machine learning (ML)",
      "Replicate Results"
    ]
  },
  {
    "objectID": "machinelearningQ.html",
    "href": "machinelearningQ.html",
    "title": "Quiz (L)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningQ.html#live-quiz",
    "href": "machinelearningQ.html#live-quiz",
    "title": "Quiz (L)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningQ.html#download-quiz",
    "href": "machinelearningQ.html#download-quiz",
    "title": "Quiz (L)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Machine learning (ML)",
      "Quiz (L)"
    ]
  },
  {
    "objectID": "machinelearningF.html",
    "href": "machinelearningF.html",
    "title": "R functions (L)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nfancyRpartPlot\nrattle\nTo plot an rpart object\n\n\nfviz_nbclust\nfactoextra\nTo visualize the optimal number of clusters\n\n\nkmeans\nbase/stats\nTo conduct K-Means cluster analysis\n\n\nlowess\nbase/stats\nTo perform scatter plot smoothing aka lowess smoothing\n\n\nrpart\nrpart\nTo fit a classification tree (CART)\n\n\nterms\nbase/stats\nTo extarct terms objects\n\n\nvarImp\ncaret\nTo calculate the variable importance measure",
    "crumbs": [
      "Machine learning (ML)",
      "R functions (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html",
    "href": "machinelearningE.html",
    "title": "Exercise 1 (L)",
    "section": "",
    "text": "Problem Statement\nWe will revisit the article by Flegal et al. (2016). We will use the same dataset as in the previous lab exercise on survey data analysis, with some additional predictors in predicting obesity.\nOur primary aim is to predict grade 3 obesity with the following predictors:",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#problem-statement",
    "href": "machinelearningE.html#problem-statement",
    "title": "Exercise 1 (L)",
    "section": "",
    "text": "Age: Age in years at screening\nGender\nRace: Race/ethnicity\nEducation: Education level\nSmoking: Smoking status\nPhysical activity: Level of vigorous work activity\nSleep: Hours of sleep\nHigh blood pressure: Ever doctor told a high blood pressure\nGeneral health: General health condition",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-1-creating-data",
    "href": "machinelearningE.html#question-1-creating-data",
    "title": "Exercise 1 (L)",
    "section": "Question 1: Creating data",
    "text": "Question 1: Creating data\n1(a) Downloading the datasets\nYou can see how datasets are downloaded and merged:\n\nlibrary(nhanesA)\n# library(SASxport)\nlibrary(plyr)\n\n# Demographic data\ndemo &lt;- nhanes('DEMO_H') # Both males and females: 0 - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\", # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                \"RIDEXPRG\", # Pregnancy status at exam\n                \"WTINT2YR\", #  Full sample 2 year weights\n                \"SDMVPSU\", # Masked variance pseudo-PSU\n                \"SDMVSTRA\")] # Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1)\ndemo2 &lt;- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n\n# BMI\nbmx &lt;- nhanes('BMX_H')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n\n# Smoking\nsmq &lt;- nhanes('SMQ_H')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n\n# Physical activity\npaq &lt;- nhanes('PAQ_H')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n\n# Sleep\nslq &lt;- nhanes('SLQ_H')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Hours of sleep\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n\n# High blood pressure\nbpq &lt;- nhanes('BPQ_H')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ020\")] # Ever told you had high blood pressure\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n\n# General health condition\nhuq &lt;- nhanes('HUQ_H')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ010\")] # General health condition\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n\n# Combined data\ndat.full &lt;- join_all(list(demo2, bmx2, smq2, paq2, slq2, bpq2, huq2), by = \"SEQN\",\n                     type='full') \ndim(dat.full) # N = 10,175\n\n1(b) Recoding\nLet us recode the outcome and predictors to make them suitable for analysis:\n\n# Survey design\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Class 3 obesity - BMI &gt;= 40 kg/m^2\nsummary(dat.full$BMXBMI)\ndat.full$obesity &lt;- ifelse(dat.full$BMXBMI &gt;= 40, 1, 0)\ntable(dat.full$obesity, useNA = \"always\")\n\n# Age\ndat.full$age.cat &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\ntable(dat.full$age.cat, useNA = \"always\")\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\ntable(dat.full$gender, useNA = \"always\")\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ntable(dat.full$age.cat, dat.full$race, useNA = \"always\")\ndat.full$race &lt;- car::recode(dat.full$race, \n                             \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black' = 'Black'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             c('Non-Hispanic Asian', \n                             'Other Race - Including Multi-Rac')= 'Other';\n                             else=NA\", \n                             levels = c(\"White\", \"Black\", \"Hispanic\", \"Other\"))\ntable(dat.full$race, useNA = \"always\")\n\n# Education\ndat.full$education &lt;- dat.full$DMDEDUC2\ndat.full$education &lt;- car::recode(dat.full$education, \n                                  \" c('Some college or AA degree', \n                                  'College graduate or above') = '&gt;High school'; \n                                  'High school graduate/GED or equi' = 'High school';\n                                  c('Less than 9th grade',\n                                  '9-11th grade (Includes 12th grad') = \n                                  '&lt;High school'; \n                                  else = NA\", \n                                  levels = c(\"&lt;High school\", \"High school\", \n                                                    \"&gt;High school\"))\ntable(dat.full$education, useNA = \"always\")\n\n# Smoking status\ndat.full$smoking &lt;- dat.full$SMQ020\ntable(dat.full$smoking, useNA = \"always\")\ndat.full$smoking &lt;- car::recode(dat.full$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA  \",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat.full$smoking[dat.full$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\ntable(dat.full$smoking, useNA = \"always\")\n\n# Physical activity\ndat.full$physical.activity &lt;- dat.full$PAQ605\ntable(dat.full$physical.activity, useNA = \"always\")\ndat.full$physical.activity &lt;- car::recode(dat.full$physical.activity, \n                                          \" 'Yes'='Yes'; 'No'='No'; else=NA \", \n                                          levels = c(\"No\", \"Yes\"))\ntable(dat.full$physical.activity, useNA = \"always\")\n\n# Sleep\ndat.full$sleep &lt;- dat.full$SLD010H\ndat.full$sleep &lt;- car::recode(dat.full$sleep, \" 1:6 = 'Less than 7'; 7:9 = '7-9'; \n                              10:24 = 'More than 9';  else=NA \",\n                              levels = c(\"Less than 7\", \"7-9\", \"More than 9\"))\ntable(dat.full$sleep, useNA = \"always\")\n\n# High blood pressure\ndat.full$high.blood.pressure &lt;- dat.full$BPQ020\ntable(dat.full$high.blood.pressure, useNA = \"always\")\ndat.full$high.blood.pressure &lt;- car::recode(dat.full$high.blood.pressure, \n                                            \" 'Yes'='Yes'; 'No'='No'; else=NA \",\n                                            levels = c(\"No\", \"Yes\"))\ntable(dat.full$high.blood.pressure, useNA = \"always\")\n\n# General health condition\ndat.full$general.health &lt;- dat.full$HUQ010\ntable(dat.full$general.health, useNA = \"always\")\ndat.full$general.health &lt;- car::recode(dat.full$general.health, \n                                       \"c('Excellent,', 'Very good,')=\n                                       'Very good or Excellent'; \n                                       'Good,'='Good';\n                                       c('Fair, or', 'Poor?') ='Poor or Fair'; \n                                       else=NA  \",\n                                       levels = c(\"Poor or Fair\", \"Good\", \n                                                  \"Very good or Excellent\"))\ntable(dat.full$general.health, useNA = \"always\")\n\n1(c) Keep relevant variables\nLet’s keep only the relevant variables for this exercise:\n\n# Keep relevant variables\nvars &lt;- c(\n  # Unique identifier\n  \"SEQN\", \n  \n  # Survey features\n  \"survey.weight\", \"psu\", \"strata\", \n  \n  # Eligibility\n  \"RIDAGEYR\", \"BMXBMI\", \"RIDEXPRG\",\n  \n  # Outcome\n  \"obesity\", \n  \n  # Predictors\n  \"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \"physical.activity\", \n  \"sleep\", \"high.blood.pressure\", \"general.health\")\n\ndat.full2 &lt;- dat.full[,vars]\n\n1(d) Weight normalization\nLarge weights can cause issues when evaluating model performance. Let’s normalize the survey weights to address this problem:\n\ndat.full2$wgt &lt;- dat.full2$survey.weight * nrow(dat.full2)/sum(dat.full2$survey.weight)\nsummary(dat.full2$wgt)\n\n1(e) Analytic dataset\nThe authors restricted their study to - adults aged 20 years and more, - non-missing body mass index, and - non-pregnant\n\n# Aged 20 years or more\ndat.analytic &lt;- subset(dat.full2, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ntable(dat.analytic$RIDEXPRG)\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\nnrow(dat.analytic)\n\n# Drop irrelevant variables\ndat.analytic$RIDAGEYR &lt;- dat.analytic$BMXBMI &lt;- dat.analytic$RIDEXPRG &lt;- NULL\n\n1(f) Complete case data\nBelow is the code for creating the complete case dataset (no missing for the outcome or predictors):\n\n# Drop missing values\ndat.complete &lt;- dat.analytic[complete.cases(dat.analytic),] # N = 5,433\n\n1(g) Save daatsets\n\nsave(dat.full, dat.full2, dat.analytic, dat.complete, file = \"Data/machinelearning/Flegal2016_v2.RData\")",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-2-importing-data-and-creating-table-1",
    "href": "machinelearningE.html#question-2-importing-data-and-creating-table-1",
    "title": "Exercise 1 (L)",
    "section": "Question 2: Importing data and creating Table 1",
    "text": "Question 2: Importing data and creating Table 1\n2(a) Importing dataset\nLet’s load the dataset:\n\nload(\"Data/machinelearning/Flegal2016_v2.RData\")\nls()\n#&gt; [1] \"dat.analytic\" \"dat.complete\" \"dat.full\"     \"dat.full2\"\n\nHere,\n\ndat.full: the full dataset with all variables\ndat.full2: the full dataset with only relevant variables for this exercise\ndat.analytic: the analytic dataset with only adults aged 20 years and more, non-missing BMI, and non-pregnant\ndat.complete: the complete case dataset without missing values in the outcome and predictors\n\n\nnames(dat.full2)\n#&gt;  [1] \"SEQN\"                \"survey.weight\"       \"psu\"                \n#&gt;  [4] \"strata\"              \"RIDAGEYR\"            \"BMXBMI\"             \n#&gt;  [7] \"RIDEXPRG\"            \"obesity\"             \"age.cat\"            \n#&gt; [10] \"gender\"              \"race\"                \"education\"          \n#&gt; [13] \"smoking\"             \"physical.activity\"   \"sleep\"              \n#&gt; [16] \"high.blood.pressure\" \"general.health\"      \"wgt\"\n\n2(b) Creating Table 1\nLet’s create Table 1 for the complete case dataset with unweighted frequencies:\n\nlibrary(tableone)\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \n                \"physical.activity\", \"sleep\", \"high.blood.pressure\", \n                \"general.health\")\ntab1 &lt;- CreateTableOne(vars = predictors, strata = \"obesity\", \n                       data = dat.complete, test = F, addOverall = T)\nprint(tab1, format=\"f\") # Showing only frequencies \n#&gt;                            Stratified by obesity\n#&gt;                             Overall 0    1  \n#&gt;   n                         5433    5023 410\n#&gt;   age.cat                                   \n#&gt;      [20,40)                1806    1659 147\n#&gt;      [40,60)                1892    1728 164\n#&gt;      [60,Inf)               1735    1636  99\n#&gt;   gender = Female           2807    2531 276\n#&gt;   race                                      \n#&gt;      White                  2333    2157 176\n#&gt;      Black                  1109     976 133\n#&gt;      Hispanic               1208    1125  83\n#&gt;      Other                   783     765  18\n#&gt;   education                                 \n#&gt;      &lt;High school           1171    1092  79\n#&gt;      High school            1216    1116 100\n#&gt;      &gt;High school           3046    2815 231\n#&gt;   smoking                                   \n#&gt;      Never smoker           3054    2828 226\n#&gt;      Former smoker          1261    1159 102\n#&gt;      Current smoker         1118    1036  82\n#&gt;   physical.activity = Yes    984     917  67\n#&gt;   sleep                                     \n#&gt;      7-9                    3174    2971 203\n#&gt;      Less than 7            2084    1896 188\n#&gt;      More than 9             175     156  19\n#&gt;   high.blood.pressure = Yes 2037    1810 227\n#&gt;   general.health                            \n#&gt;      Poor or Fair           1260    1084 176\n#&gt;      Good                   2065    1911 154\n#&gt;      Very good or Excellent 2108    2028  80",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-3-prediction-using-split-sample-approach",
    "href": "machinelearningE.html#question-3-prediction-using-split-sample-approach",
    "title": "Exercise 1 (L)",
    "section": "Question 3: Prediction using split sample approach",
    "text": "Question 3: Prediction using split sample approach\nIn this exercise, we will use the split-sample approach to predict obesity. We will create our training and test data using a 60-40 split for the training and test data. We will use the following two methods to predict obesity:\n\nDesign-adjusted logistic with all survey features (psu, strata, and survey weights)\nLASSO with survey weights\n\n3(a) Split the data into training and test\nLet us create our training and test data using the split-sample approach:\n\nset.seed(900)\ndat.complete$datasplit &lt;- rbinom(nrow(dat.complete), size = 1, prob = 0.6) \ntable(dat.complete$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2130 3303\n\n# Training data\ndat.train &lt;- dat.complete[dat.complete$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 3303   16\n\n# Test data\ndat.test &lt;- dat.complete[dat.complete$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2130   16\n\n3(b) Prediction with design-adjusted logistic\nWe will use the design-adjusted logistic regression to predict obesity with the following predictors:\n\nage.cat, gender, race, high.blood.pressure, general.health\n\nInstructions:\n\n1: Create the survey design on the full data and subset the design for those individuals in the training data.\n2: Use the training data design created in step 1 to fit the model\n3: Use the test data to predict the probability of obesity.\n4: Calculate AUC on the test data.\n5: Calculate calibration slope with 95% confidence interval on the test data.\n\nHints:\n\n\nWeightedAUC and WeightedROC are helpful functions in calculating AUC.\nThe Logit function from the DescTools package is helpful in calculating the logit of predicted probabilities for calculating calibration slope.\nUse the normalized weight variable to calculate the AUC and calibration slope.\n\nSimpler Model\n\nlibrary(survey)\nlibrary(DescTools)\nlibrary(WeightedROC)\nlibrary(Publish)\nlibrary(boot)\nlibrary(scoring)\n\n# Design\ndat.full2$miss &lt;- 1\ndat.full2$miss[dat.full2$SEQN %in% dat.train$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full2, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Formula\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \n                \"high.blood.pressure\", \"general.health\")\nFormula &lt;- formula(paste(\"obesity ~ \", paste(predictors, collapse=\" + \")))\n\n# Model\nfit.glm &lt;- svyglm(Formula, design = svy.design, family = binomial)\npublish(fit.glm)\n#&gt;             Variable                  Units OddsRatio       CI.95    p-value \n#&gt;              age.cat                [20,40)       Ref                        \n#&gt;                                     [40,60)      0.63 [0.42;0.95]   0.091799 \n#&gt;                                    [60,Inf)      0.31 [0.17;0.57]   0.018946 \n#&gt;               gender                   Male       Ref                        \n#&gt;                                      Female      1.90 [1.32;2.73]   0.026347 \n#&gt;                 race                  White       Ref                        \n#&gt;                                       Black      1.56 [1.12;2.16]   0.056561 \n#&gt;                                    Hispanic      0.91 [0.53;1.57]   0.758727 \n#&gt;                                       Other      0.37 [0.18;0.75]   0.050646 \n#&gt;            education           &lt;High school       Ref                        \n#&gt;                                 High school      1.34 [0.87;2.07]   0.252853 \n#&gt;                                &gt;High school      1.97 [1.13;3.46]   0.076212 \n#&gt;  high.blood.pressure                     No       Ref                        \n#&gt;                                         Yes      2.20 [1.61;3.02]   0.007835 \n#&gt;       general.health           Poor or Fair       Ref                        \n#&gt;                                        Good      0.52 [0.36;0.74]   0.022344 \n#&gt;                      Very good or Excellent      0.22 [0.14;0.36]   0.003885\n\n# Prediction on the test set\ndat.test$pred.glm &lt;- predict(fit.glm, newdata = dat.test, type = \"response\")\nsummary(dat.test$pred.glm)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.003889 0.027737 0.052118 0.073342 0.094836 0.413143\n\n# AUC on the test set with sampling weights\n# unfortunately strata and cluster omitted\nauc.glm &lt;- WeightedAUC(WeightedROC(dat.test$pred.glm, dat.test$obesity, \n                                   weight = dat.test$wgt))\nauc.glm\n#&gt; [1] 0.6903502\n\n# Function to calculate AUC for bootstrap samples\n# unfortunately strata and cluster omitted\ncalc_auc &lt;- function(data, indices) {\n  d &lt;- data[indices, ]\n  roc_obj &lt;- WeightedROC(d$pred.glm, d$obesity, weight = d$wgt)\n  return(WeightedAUC(roc_obj))\n}\n\n# Perform bootstrapping\nset.seed(123)\nboot_obj &lt;- boot(data = dat.test, statistic = calc_auc, R = 150)\n\n# Get 95% confidence intervals\nci_auc &lt;- boot.ci(boot_obj, type = \"perc\")\nci_auc\n#&gt; BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#&gt; Based on 150 bootstrap replicates\n#&gt; \n#&gt; CALL : \n#&gt; boot.ci(boot.out = boot_obj, type = \"perc\")\n#&gt; \n#&gt; Intervals : \n#&gt; Level     Percentile     \n#&gt; 95%   ( 0.6357,  0.7582 )  \n#&gt; Calculations and Intervals on Original Scale\n#&gt; Some percentile intervals may be unstable\n\n# Weighted calibration slope\n# unfortunately strata and cluster omitted\ndat.test$pred.glm.logit &lt;- DescTools::Logit(dat.test$pred.glm)\nslope.glm &lt;- glm(obesity ~ pred.glm.logit, data = dat.test, family = quasibinomial,\n                 weights = wgt)\npublish(slope.glm)\n#&gt;        Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.glm.logit              0.79 [0.61;0.98] &lt; 1e-04\n\n# Calculate the weighted Brier score\nbrier_score &lt;- mean(brierscore(dat.test$obesity ~ dat.test$pred.glm, \n                               data = dat.test, \n                               wt = dat.test$wgt))\nbrier_score\n#&gt; [1] 0.06878706\n\n3(c) Prediction with design-adjusted logistic with added covariates [90% grade]\nAdd the following variables in the existing model, and assess the model performance in terms of the same 3 performance measures:\n\neducation, smoking, physical.activity, sleep\n\nModel with more variables\n\n# Formula\n# predictors2 &lt;- ...\n# Formula2 &lt;- ...\n\n# Model\n# fit.glm2 &lt;- ...\n\n# Prediction on the test set\n\n# AUC on the test set with sampling weights\n# unfortunately strata and cluster omitted\n# auc.glm2 &lt;-...\n\n# Function to calculate AUC for bootstrap samples\n# unfortunately strata and cluster omitted\n\n# Get 95% confidence intervals from bootstrapping\n# ci_auc2 &lt;- ...\n\n# Weighted calibration slope\n# unfortunately strata and cluster omitted\n# slope.glm2 ...\n\n# Calculate the weighted Brier score\n# brier_score2 &lt;- ...\n\n3(d) Interpretation [10%]\nInterpret the AUC, calibration slope and Brier Score based on the following criteria, and suggest which model you would choose:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting\n\n\n\n\n\nBrier Score\nInterpretation\n\n\n\n0\nPerfect prediction\n\n\n0.01-0.1\nVery good model performance\n\n\n0.11-0.2\nGood model performance\n\n\n0.21-0.3\nFair model performance\n\n\n0.31-0.5\nPoor model performance\n\n\n&gt; 0.5\nVery poor model performance (no skill)\n\n\n\n\nAUC: Higher is better (closer to 1).\nCalibration Slope: Closer to 1 is better.\nBrier Score: Lower is better (closer to 0).\n\nResponse:\n3(e) Prediction with LASSO [Optional]\nNow we will use the LASSO method to predict obesity. We will incorporate sampling weights in the model to account for survey data (no psu or strata). Note that we are not interested in the statistical significance of the beta coefficients. Hence, not utilizing psu and strata should not be an issue in this prediction problem.\nInstructions:\n\n1: Use the training data with normalized weight to fit the model.\n2: Find the optimum lambda using 5-fold cross-validation. Consider the lambda value that gives the minimum prediction error.\n3: Predict the probability of obesity on the test set\n3: Calculate AUC on the test data.\n4: Calculate calibration slope with 95% confidence interval on the test data.\n\nModel\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula2, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$obesity) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula2, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$obesity)\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                          family = \"binomial\", weights = dat.train$wgt)\n\n# Prediction on the test set\ndat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                               s = fit.cv.lasso$lambda.min)\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, dat.test$obesity, \n                                     weight = dat.test$wgt))\nauc.lasso\n\n# Weighted calibration slope\ndat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\nslope.lasso &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, \n                   family = quasibinomial, weights = wgt)\npublish(slope.lasso)\n\nInterpretation [optional]\nInterpret the AUC and calibration slope.",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#question-4-prediction-using-croos-validation-approach-optional",
    "href": "machinelearningE.html#question-4-prediction-using-croos-validation-approach-optional",
    "title": "Exercise 1 (L)",
    "section": "Question 4: Prediction using croos-validation approach [optional]",
    "text": "Question 4: Prediction using croos-validation approach [optional]\nUse LASSO with 5-fold cross-validation to predict obesity with the same set of predictors (from larger model) used in Question 2. Report the average AUC and average calibration slope with 95% confidence interval over 5 folds.\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, size = nrow(dat.complete), replace = T)\ntable(nfolds)\n\nauc.lasso &lt;- cal.slope.lasso &lt;- cal.slope.se.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula2, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$obesity)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula2, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$obesity)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                            family = \"binomial\", weights = dat.train$wgt)\n\n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                                 s = fit.cv.lasso$lambda.min)\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso,dat.test$obesity, \n                                             weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\n  mod.cal &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, family = binomial, \n                 weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  cal.slope.se.lasso[fold] &lt;- summary(mod.cal)$coef[2,2]\n}\n\n# Average AUC\nmean(auc.lasso)\n\n# Average calibration slope\nmean(cal.slope.lasso)\n\n# 95% CI for calibration slope\ncbind(mean(cal.slope.lasso) - 1.96 * mean(cal.slope.se.lasso), \n      mean(cal.slope.lasso) + 1.96 * mean(cal.slope.se.lasso))",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningE.html#knit-your-file",
    "href": "machinelearningE.html#knit-your-file",
    "title": "Exercise 1 (L)",
    "section": "Knit your file",
    "text": "Knit your file\nPlease knit your file once you finished and submit the knitted PDF or doc file. Please also fill-up the following table:\nGroup name: Put the group name here\n\n\nStudent initial\n% contribution\n\n\n\nPut Student 1 initial here\nPut % contribution for Student 1 here\n\n\nPut Student 2 initial here\nPut % contribution for Student 2 here\n\n\nPut Student 3 initial here\nPut % contribution for Student 3 here",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html",
    "href": "machinelearningEsolution.html",
    "title": "Exercise 1 Solution (L)",
    "section": "",
    "text": "Question 1: Creating data\nWe will revisit the article by Flegal et al. (2016). We will use the same dataset as in the previous lab exercise on survey data analysis, with some additional predictors in predicting obesity.\nOur primary aim is to predict grade 3 obesity with the following predictors:",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-1-creating-data",
    "href": "machinelearningEsolution.html#question-1-creating-data",
    "title": "Exercise 1 Solution (L)",
    "section": "",
    "text": "1(a) Downloading the datasets\nYou can see how datasets are downloaded and merged:\n\nlibrary(nhanesA)\n# library(SASxport)\nlibrary(plyr)\n\n# Demographic data\ndemo &lt;- nhanes('DEMO_H') # Both males and females: 0 - 150 YEARS\ndemo1 &lt;- demo[c(\"SEQN\", # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH3\", # Race/Hispanic origin w/ NH Asian\n                \"RIDEXPRG\", # Pregnancy status at exam\n                \"WTINT2YR\", #  Full sample 2 year weights\n                \"SDMVPSU\", # Masked variance pseudo-PSU\n                \"SDMVSTRA\")] # Masked variance pseudo-stratum\ndemo_vars &lt;- names(demo1)\ndemo2 &lt;- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n\n# BMI\nbmx &lt;- nhanes('BMX_H')\nbmx1 &lt;- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars &lt;- names(bmx1)\nbmx2 &lt;- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n\n# Smoking\nsmq &lt;- nhanes('SMQ_H')\nsmq1 &lt;- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars &lt;- names(smq1)\nsmq2 &lt;- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n\n# Physical activity\npaq &lt;- nhanes('PAQ_H')\npaq1 &lt;- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity\npaq_vars &lt;- names(paq1)\npaq2 &lt;- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n\n# Sleep\nslq &lt;- nhanes('SLQ_H')\nslq1 &lt;- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Hours of sleep\nslq_vars &lt;- names(slq1)\nslq2 &lt;- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n\n# High blood pressure\nbpq &lt;- nhanes('BPQ_H')\nbpq1 &lt;- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ020\")] # Ever told you had high blood pressure\nbpq_vars &lt;- names(bpq1)\nbpq2 &lt;- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n\n# General health condition\nhuq &lt;- nhanes('HUQ_H')\nhuq1 &lt;- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ010\")] # General health condition\nhuq_vars &lt;- names(huq1)\nhuq2 &lt;- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n\n# Combined data\ndat.full &lt;- join_all(list(demo2, bmx2, smq2, paq2, slq2, bpq2, huq2), by = \"SEQN\",\n                     type='full') \ndim(dat.full) # N = 10,175\n\n1(b) Recoding\nLet us recode the outcome and predictors to make them suitable for analysis:\n\n# Survey design\ndat.full$survey.weight &lt;- dat.full$WTINT2YR\ndat.full$psu &lt;- dat.full$SDMVPSU\ndat.full$strata &lt;- dat.full$SDMVSTRA\n\n# Class 3 obesity - BMI &gt;= 40 kg/m^2\nsummary(dat.full$BMXBMI)\ndat.full$obesity &lt;- ifelse(dat.full$BMXBMI &gt;= 40, 1, 0)\ntable(dat.full$obesity, useNA = \"always\")\n\n# Age\ndat.full$age.cat &lt;- cut(dat.full$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE)\ntable(dat.full$age.cat, useNA = \"always\")\n\n# Gender\ndat.full$gender &lt;- dat.full$RIAGENDR\ntable(dat.full$gender, useNA = \"always\")\n\n# Race/Hispanic origin group\ndat.full$race &lt;- dat.full$RIDRETH3\ntable(dat.full$age.cat, dat.full$race, useNA = \"always\")\ndat.full$race &lt;- car::recode(dat.full$race, \n                             \" 'Non-Hispanic White'='White'; \n                             'Non-Hispanic Black' = 'Black'; \n                             c('Mexican American','Other Hispanic')='Hispanic'; \n                             c('Non-Hispanic Asian', \n                             'Other Race - Including Multi-Rac')= 'Other';\n                             else=NA\", \n                             levels = c(\"White\", \"Black\", \"Hispanic\", \"Other\"))\ntable(dat.full$race, useNA = \"always\")\n\n# Education\ndat.full$education &lt;- dat.full$DMDEDUC2\ndat.full$education &lt;- car::recode(dat.full$education, \n                                  \" c('Some college or AA degree', \n                                  'College graduate or above') = '&gt;High school'; \n                                  'High school graduate/GED or equi' = 'High school';\n                                  c('Less than 9th grade',\n                                  '9-11th grade (Includes 12th grad') = \n                                  '&lt;High school'; \n                                  else = NA\", \n                                  levels = c(\"&lt;High school\", \"High school\", \n                                                    \"&gt;High school\"))\ntable(dat.full$education, useNA = \"always\")\n\n# Smoking status\ndat.full$smoking &lt;- dat.full$SMQ020\ntable(dat.full$smoking, useNA = \"always\")\ndat.full$smoking &lt;- car::recode(dat.full$smoking, \" 'Yes'='Current smoker'; \n                                'No'='Never smoker'; else=NA  \",\n                                levels = c(\"Never smoker\", \"Former smoker\", \n                                           \"Current smoker\"))\ndat.full$smoking[dat.full$SMQ040 == \"Not at all\"] &lt;- \"Former smoker\"\ntable(dat.full$smoking, useNA = \"always\")\n\n# Physical activity\ndat.full$physical.activity &lt;- dat.full$PAQ605\ntable(dat.full$physical.activity, useNA = \"always\")\ndat.full$physical.activity &lt;- car::recode(dat.full$physical.activity, \n                                          \" 'Yes'='Yes'; 'No'='No'; else=NA \", \n                                          levels = c(\"No\", \"Yes\"))\ntable(dat.full$physical.activity, useNA = \"always\")\n\n# Sleep\ndat.full$sleep &lt;- dat.full$SLD010H\ndat.full$sleep &lt;- car::recode(dat.full$sleep, \" 1:6 = 'Less than 7'; 7:9 = '7-9'; \n                              10:24 = 'More than 9';  else=NA \",\n                              levels = c(\"Less than 7\", \"7-9\", \"More than 9\"))\ntable(dat.full$sleep, useNA = \"always\")\n\n# High blood pressure\ndat.full$high.blood.pressure &lt;- dat.full$BPQ020\ntable(dat.full$high.blood.pressure, useNA = \"always\")\ndat.full$high.blood.pressure &lt;- car::recode(dat.full$high.blood.pressure, \n                                            \" 'Yes'='Yes'; 'No'='No'; else=NA \",\n                                            levels = c(\"No\", \"Yes\"))\ntable(dat.full$high.blood.pressure, useNA = \"always\")\n\n# General health condition\ndat.full$general.health &lt;- dat.full$HUQ010\ntable(dat.full$general.health, useNA = \"always\")\ndat.full$general.health &lt;- car::recode(dat.full$general.health, \n                                       \"c('Excellent,', 'Very good,')=\n                                       'Very good or Excellent'; \n                                       'Good,'='Good';\n                                       c('Fair, or', 'Poor?') ='Poor or Fair'; \n                                       else=NA  \",\n                                       levels = c(\"Poor or Fair\", \"Good\", \n                                                  \"Very good or Excellent\"))\ntable(dat.full$general.health, useNA = \"always\")\n\n1(c) Keep relevant variables\nLet’s keep only the relevant variables for this exercise:\n\n# Keep relevant variables\nvars &lt;- c(\n  # Unique identifier\n  \"SEQN\", \n  \n  # Survey features\n  \"survey.weight\", \"psu\", \"strata\", \n  \n  # Eligibility\n  \"RIDAGEYR\", \"BMXBMI\", \"RIDEXPRG\",\n  \n  # Outcome\n  \"obesity\", \n  \n  # Predictors\n  \"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \"physical.activity\", \n  \"sleep\", \"high.blood.pressure\", \"general.health\")\n\ndat.full2 &lt;- dat.full[,vars]\n\n1(d) Weight normalization\nLarge weights can cause issues when evaluating model performance. Let’s normalize the survey weights to address this problem:\n\ndat.full2$wgt &lt;- dat.full2$survey.weight * nrow(dat.full2)/sum(dat.full2$survey.weight)\nsummary(dat.full2$wgt)\n\n1(e) Analytic dataset\nThe authors restricted their study to - adults aged 20 years and more, - non-missing body mass index, and - non-pregnant\n\n# Aged 20 years or more\ndat.analytic &lt;- subset(dat.full2, RIDAGEYR&gt;=20) # N = 5,769\n\n# Non-missing outcome\ndat.analytic &lt;- subset(dat.analytic, !is.na(BMXBMI)) # N = 5,520\n\n# Non-pregnant\ntable(dat.analytic$RIDEXPRG)\ndat.analytic &lt;- subset(dat.analytic, is.na(RIDEXPRG) | RIDEXPRG != \n                         \"Yes, positive lab pregnancy test\") # N = 5,455\nnrow(dat.analytic)\n\n# Drop irrelevant variables\ndat.analytic$RIDAGEYR &lt;- dat.analytic$BMXBMI &lt;- dat.analytic$RIDEXPRG &lt;- NULL\n\n1(f) Complete case data\nBelow is the code for creating the complete case dataset (no missing for the outcome or predictors):\n\n# Drop missing values\ndat.complete &lt;- dat.analytic[complete.cases(dat.analytic),] # N = 5,433\n\n1(g) Save daatsets\n\nsave(dat.full, dat.full2, dat.analytic, dat.complete, file = \"Data/machinelearning/Flegal2016_v2.RData\")",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-2-importing-data-and-creating-table-1",
    "href": "machinelearningEsolution.html#question-2-importing-data-and-creating-table-1",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 2: Importing data and creating Table 1",
    "text": "Question 2: Importing data and creating Table 1\n2(a) Importing dataset\nLet’s load the dataset:\n\nload(\"Data/machinelearning/Flegal2016_v2.RData\")\nls()\n#&gt; [1] \"dat.analytic\" \"dat.complete\" \"dat.full\"     \"dat.full2\"\n\nHere,\n\ndat.full: the full dataset with all variables\ndat.full2: the full dataset with only relevant variables for this exercise\ndat.analytic: the analytic dataset with only adults aged 20 years and more, non-missing BMI, and non-pregnant\ndat.complete: the complete case dataset without missing values in the outcome and predictors\n\n\nnames(dat.full2)\n#&gt;  [1] \"SEQN\"                \"survey.weight\"       \"psu\"                \n#&gt;  [4] \"strata\"              \"RIDAGEYR\"            \"BMXBMI\"             \n#&gt;  [7] \"RIDEXPRG\"            \"obesity\"             \"age.cat\"            \n#&gt; [10] \"gender\"              \"race\"                \"education\"          \n#&gt; [13] \"smoking\"             \"physical.activity\"   \"sleep\"              \n#&gt; [16] \"high.blood.pressure\" \"general.health\"      \"wgt\"\n\n2(b) Creating Table 1\nLet’s create Table 1 for the complete case dataset with unweighted frequencies:\n\nlibrary(tableone)\npredictors &lt;- c(\"age.cat\", \"gender\", \"race\", \"education\", \"smoking\", \n                \"physical.activity\", \"sleep\", \"high.blood.pressure\", \n                \"general.health\")\ntab1 &lt;- CreateTableOne(vars = predictors, strata = \"obesity\", \n                       data = dat.complete, test = F, addOverall = T)\nprint(tab1, format=\"f\") # Showing only frequencies \n#&gt;                            Stratified by obesity\n#&gt;                             Overall 0    1  \n#&gt;   n                         5433    5023 410\n#&gt;   age.cat                                   \n#&gt;      [20,40)                1806    1659 147\n#&gt;      [40,60)                1892    1728 164\n#&gt;      [60,Inf)               1735    1636  99\n#&gt;   gender = Female           2807    2531 276\n#&gt;   race                                      \n#&gt;      White                  2333    2157 176\n#&gt;      Black                  1109     976 133\n#&gt;      Hispanic               1208    1125  83\n#&gt;      Other                   783     765  18\n#&gt;   education                                 \n#&gt;      &lt;High school           1171    1092  79\n#&gt;      High school            1216    1116 100\n#&gt;      &gt;High school           3046    2815 231\n#&gt;   smoking                                   \n#&gt;      Never smoker           3054    2828 226\n#&gt;      Former smoker          1261    1159 102\n#&gt;      Current smoker         1118    1036  82\n#&gt;   physical.activity = Yes    984     917  67\n#&gt;   sleep                                     \n#&gt;      7-9                    3174    2971 203\n#&gt;      Less than 7            2084    1896 188\n#&gt;      More than 9             175     156  19\n#&gt;   high.blood.pressure = Yes 2037    1810 227\n#&gt;   general.health                            \n#&gt;      Poor or Fair           1260    1084 176\n#&gt;      Good                   2065    1911 154\n#&gt;      Very good or Excellent 2108    2028  80",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-3-prediction-using-split-sample-approach",
    "href": "machinelearningEsolution.html#question-3-prediction-using-split-sample-approach",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 3: Prediction using split sample approach",
    "text": "Question 3: Prediction using split sample approach\nIn this exercise, we will use the split-sample approach to predict obesity. We will create our training and test data using a 60-40 split for the training and test data. We will use the following two methods to predict obesity:\n\nDesign-adjusted logistic with all survey features (psu, strata, and survey weights)\nLASSO with survey weights\n\n3(a) Split the data into training and test\nLet us create our training and test data using the split-sample approach:\n\nset.seed(900)\ndat.complete$datasplit &lt;- rbinom(nrow(dat.complete), size = 1, prob = 0.6) \ntable(dat.complete$datasplit)\n#&gt; \n#&gt;    0    1 \n#&gt; 2130 3303\n\n# Training data\ndat.train &lt;- dat.complete[dat.complete$datasplit == 1,]\ndim(dat.train)\n#&gt; [1] 3303   16\n\n# Test data\ndat.test &lt;- dat.complete[dat.complete$datasplit == 0,]\ndim(dat.test)\n#&gt; [1] 2130   16\n\n3(b) Prediction with design-adjusted logistic\nWe will use the design-adjusted logistic regression to predict obesity with the following predictors:\n\nage.cat, gender, race, education, smoking, physical.activity, sleep, high.blood.pressure, general.health\n\nInstructions:\n\n1: Create the survey design on the full data and subset the design for those individuals in the training data.\n2: Use the training data design created in step 1 to fit the model\n3: Use the test data to predict the probability of obesity.\n4: Calculate AUC on the test data.\n5: Calculate calibration slope with 95% confidence interval on the test data.\n\nHints:\n\n\nWeightedAUC and WeightedROC are helpful functions in calculating AUC.\nThe Logit function from the DescTools package is helpful in calculating the logit of predicted probabilities for calculating calibration slope.\nUse the normalized weight variable to calculate the AUC and calibration slope.\n\nModel\n\nlibrary(survey)\nlibrary(DescTools)\nlibrary(WeightedROC)\nlibrary(Publish)\n\n# Design\ndat.full2$miss &lt;- 1\ndat.full2$miss[dat.full2$SEQN %in% dat.train$SEQN] &lt;- 0\nsvy.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~survey.weight,\n                         data = dat.full2, nest = TRUE)\nsvy.design &lt;- subset(svy.design0, miss == 0)\n\n# Formula\nFormula &lt;- formula(paste(\"obesity ~ \", paste(predictors, collapse=\" + \")))\n\n# Model\nfit.glm &lt;- svyglm(Formula, design = svy.design, family = binomial)\n\n# Prediction on the test set\ndat.test$pred.glm &lt;- predict(fit.glm, newdata = dat.test, type = \"response\")\n\n# AUC on the test set with sampling weights\nauc.glm &lt;- WeightedAUC(WeightedROC(dat.test$pred.glm, dat.test$obesity, \n                                   weight = dat.test$wgt))\nauc.glm\n#&gt; [1] 0.6813118\n\n# Weighted calibration slope\ndat.test$pred.glm.logit &lt;- DescTools::Logit(dat.test$pred.glm)\nslope.glm &lt;- glm(obesity ~ pred.glm.logit, data = dat.test, family = quasibinomial,\n                 weights = wgt)\npublish(slope.glm)\n#&gt;        Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.glm.logit              0.74 [0.56;0.92] &lt; 1e-04\n\nInterpretation [optional]\nInterpret the AUC and calibration slope based on the following criteria:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting\n\n\n3(c) Prediction with LASSO\nNow we will use the LASSO method to predict obesity. We will incorporate sampling weights in the model to account for survey data (no psu or strata). Note that we are not interested in the statistical significance of the beta coefficients. Hence, not utilizing psu and strata should not be an issue in this prediction problem.\nInstructions:\n\n1: Use the training data with normalized weight to fit the model.\n2: Find the optimum lambda using 5-fold cross-validation. Consider the lambda value that gives the minimum prediction error.\n3: Predict the probability of obesity on the test set\n3: Calculate AUC on the test data.\n4: Calculate calibration slope with 95% confidence interval on the test data.\n\nModel\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\n# Training data - X: predictor, y: outcome\nX.train &lt;- model.matrix(Formula, dat.train)[,-1] \ny.train &lt;- as.matrix(dat.train$obesity) \n\n# Test data - X: predictor, y: outcome\nX.test &lt;- model.matrix(Formula, dat.test)[,-1] \ny.test &lt;- as.matrix(dat.test$obesity)\n\n# Find the best lambda using 5-fold CV\nfit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                          family = \"binomial\", weights = dat.train$wgt)\n\n# Prediction on the test set\ndat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                               s = fit.cv.lasso$lambda.min)\n\n# AUC on the test set with sampling weights\nauc.lasso &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso, dat.test$obesity, \n                                     weight = dat.test$wgt))\nauc.lasso\n#&gt; [1] 0.6875668\n\n# Weighted calibration slope\ndat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\nslope.lasso &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, \n                   family = quasibinomial, weights = wgt)\npublish(slope.lasso)\n#&gt;          Variable Units Coefficient       CI.95 p-value \n#&gt;  pred.lasso.logit              0.88 [0.67;1.08] &lt; 1e-04\n\nInterpretation [optional]\nInterpret the AUC and calibration slope based on the following criteria:\n\n\nAUC\nInterpretation\n\n\n\n0.50\nNo better than a random chance\n\n\n0.51-0.70\nPoor discrimination ability\n\n\n0.71-0.80\nAcceptable discrimination ability\n\n\n0.81-0.90\nExcellent discrimination ability\n\n\n0.90-1.00\nOutstanding discrimination ability\n\n\n\n\n\nCalibration slope\nInterpretation\n\n\n\n1 and 95% CI includes 1\nWell-calibration\n\n\nSignificantly less than 1\nOverfitting\n\n\nSignificantly greater than 1\nUnderfitting",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningEsolution.html#question-4-prediction-using-croos-validation-approach-optional",
    "href": "machinelearningEsolution.html#question-4-prediction-using-croos-validation-approach-optional",
    "title": "Exercise 1 Solution (L)",
    "section": "Question 4: Prediction using croos-validation approach [optional]",
    "text": "Question 4: Prediction using croos-validation approach [optional]\nUse LASSO with 5-fold cross-validation to predict obesity with the same set of predictors used in Question 2. Report the average AUC and average calibration slope with 95% confidence interval over 5 folds.\n\nlibrary(glmnet)\nlibrary(DescTools)\nlibrary(WeightedROC)\n\nk &lt;- 5\nset.seed(604)\nnfolds &lt;- sample(1:k, size = nrow(dat.complete), replace = T)\ntable(nfolds)\n#&gt; nfolds\n#&gt;    1    2    3    4    5 \n#&gt; 1090 1074 1130 1083 1056\n\nauc.lasso &lt;- cal.slope.lasso &lt;- cal.slope.se.lasso &lt;- NULL\nfor (fold in 1:k) {\n  # Training data\n  dat.train &lt;- dat.complete[nfolds != fold, ]\n  X.train &lt;- model.matrix(Formula, dat.train)[,-1]\n  y.train &lt;- as.matrix(dat.train$obesity)\n  \n  # Test data\n  dat.test &lt;- dat.complete[nfolds == fold, ]\n  X.test &lt;- model.matrix(Formula, dat.test)[,-1]\n  y.test &lt;- as.matrix(dat.test$obesity)\n  \n  # Find the optimum lambda using 5-fold CV\n  fit.cv.lasso &lt;- cv.glmnet(x = X.train, y = y.train, nfolds = 5, alpha = 1, \n                            family = \"binomial\", weights = dat.train$wgt)\n\n  # Prediction on the test set\n  dat.test$pred.lasso &lt;- predict(fit.cv.lasso, newx = X.test, type = \"response\", \n                                 s = fit.cv.lasso$lambda.min)\n  \n  # AUC on the test set with sampling weights\n  auc.lasso[fold] &lt;- WeightedAUC(WeightedROC(dat.test$pred.lasso,dat.test$obesity, \n                                             weight = dat.test$wgt))\n  \n  # Weighted calibration slope\n  dat.test$pred.lasso.logit &lt;- DescTools::Logit(dat.test$pred.lasso)\n  mod.cal &lt;- glm(obesity ~ pred.lasso.logit, data = dat.test, family = binomial, \n                 weights = wgt)\n  cal.slope.lasso[fold] &lt;- summary(mod.cal)$coef[2,1]\n  cal.slope.se.lasso[fold] &lt;- summary(mod.cal)$coef[2,2]\n}\n\n# Average AUC\nmean(auc.lasso)\n#&gt; [1] 0.7055226\n\n# Average calibration slope\nmean(cal.slope.lasso)\n#&gt; [1] 0.9994\n\n# 95% CI for calibration slope\ncbind(mean(cal.slope.lasso) - 1.96 * mean(cal.slope.se.lasso), \n      mean(cal.slope.lasso) + 1.96 * mean(cal.slope.se.lasso))\n#&gt;           [,1]     [,2]\n#&gt; [1,] 0.7299176 1.268882",
    "crumbs": [
      "Machine learning (ML)",
      "Exercise 1 Solution (L)"
    ]
  },
  {
    "objectID": "machinelearningCausal.html",
    "href": "machinelearningCausal.html",
    "title": "ML in causal inference",
    "section": "",
    "text": "Background\nThis chapter provides a detailed exploration of Targeted Maximum Likelihood Estimation (TMLE) in causal inference. The first tutorial motivates the use of TMLE by highlighting its advantages over traditional methods, focusing on the limitations of these approaches and introducing the RHC dataset. The second tutorial delves into the SuperLearner method for ensemble modeling, discussing the importance of algorithm diversity, cross-validation, and adaptable libraries. The third tutorial offers a comprehensive guide to applying TMLE for binary outcomes, emphasizing diverse SuperLearner libraries, effective sample sizes, and candidate learner selection. The fourth tutorial extends TMLE to continuous outcomes, covering transformations, interpretation, and comparisons with default TMLE libraries and traditional regression. These tutorials should equip readers with a robust understanding of TMLE and its practical applications in causal inference and epidemiology.",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal.html#background",
    "href": "machinelearningCausal.html#background",
    "title": "ML in causal inference",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal.html#overview-of-tutorials",
    "href": "machinelearningCausal.html#overview-of-tutorials",
    "title": "ML in causal inference",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have developed a solid understanding of predictive modeling, data splitting/cross-validation, propensity scores and various machine learning algorithms. These insights have prepared us for the advanced causal inference techniques we will encounter in this chapter. As we explore TMLE and SuperLearner, we will draw upon our knowledge of propensity score and machine learning to unlock the potential of their use in building powerful causal inference tools.\n\nMotivation for learning and using TMLE\nThis tutorial discusses the motivation for using the TMLE method in causal inference. It highlights the limitations of traditional methods such as propensity score approaches and direct application of machine learning in terms of assumptions and statistical inference. TMLE is presented as a doubly robust method that incorporates machine learning while allowing straightforward statistical inference. The tutorial reintroduces RHC data for demonstration.\n\n\nUnderstanding SuperLearner\nThis tutorial focuses on using the SuperLearner method for ensemble modeling. SuperLearner is a type 2 ensemble method that combines various predictive algorithms to create a robust predictive model. It employs cross-validation to determine the best-weighted combination of algorithms, based on specified predictive performance metrics. The tutorial provides guidelines for selecting a diverse set of algorithms, considering computational feasibility, and adapting the library of algorithms based on sample characteristics. Examples of different types of learners that can be included in the SuperLearner library are provided, ranging from parametric to highly data-adaptive and non-linear models. The tutorial also mentions the default libraries for estimating outcomes and propensity scores in the context of TMLE.\n\n\nDealing with binary outcomes within TMLE framework\nThis tutorial is a comprehensive guide to applying TMLE for binary outcomes. It covers the entire TMLE process, from constructing initial outcome and exposure models to targeted adjustment via propensity scores and treatment effect estimation. It emphasizes the importance of specifying a diverse SuperLearner library for both exposure and outcome models, determining effective sample sizes, and selecting candidate learners. The tutorial demonstrates TMLE application using the tmle package and includes a thorough comparison with default SuperLearner libraries and traditional regression, presenting estimates, confidence intervals, and a comparative table of results.\n\n\nDealing with continuous outcomes within TMLE framework\nThis tutorial offers comprehensive guidance on implementing TMLE for continuous outcomes, including transformations and result interpretation. It introduces the application of TMLE for continuous outcomes, emphasizing the process of constructing a SuperLearner and key considerations such as effective sample size and learner selection. Notably, it highlights the essential transformation of continuous outcome variables to a standardized range (0 to 1) using min-max normalization before applying TMLE with a Gaussian family. The tutorial demonstrates the post-TMLE rescaling of treatment effect estimates and confidence intervals to the original scale and offers a comparative analysis with default TMLE libraries and traditional regression.\n\n\nComparing results\nIn this comprehensive tutorial, various statistical methods were applied to investigate the association between RHC and death using the RHC dataset. These methods included logistic regression, propensity score matching and weighting with both logistic regression and Super Learner, as well as TMLE. The results consistently indicated that participants with RHC use had higher odds of death compared to those without RHC use, with odds ratios ranging similarly across different modeling approaches, but also showing a trend when machine learners are incorporated.\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough of the additional resources, feel free to watch the video below.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.\n\n\n\n\nReferences",
    "crumbs": [
      "ML in causal inference"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html",
    "href": "machinelearningCausal0.html",
    "title": "Concepts (C)",
    "section": "",
    "text": "ML in causal inference\nIn comparative effectiveness studies, researchers typically use propensity score methods. However, propensity score methods have known limitations in real-world scenarios, when the true data generating mechanism is unknown. Targeted maximum likelihood estimation (TMLE) is an alternative estimation method with a number of desirable statistical properties. It is a doubly robust method, enabling the integration of machine learning approaches within the framework. Despite the fact that this method has been shown to perform better in terms of statistical properties (e.g., variance estimation) than propensity score methods in a variety of scenarios, it is not widely used in medical research as the implementation details of this approach are generally not well understood. In this section, we will explain this method in details.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#reading-list",
    "href": "machinelearningCausal0.html#reading-list",
    "title": "Concepts (C)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Karim and Frank 2021)\nOptional reading: (Frank and Karim 2023)",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#video-lessons",
    "href": "machinelearningCausal0.html#video-lessons",
    "title": "Concepts (C)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMachine learning\n\n\n\nThe workshop was first developed for R/Medicine Virtual Conference https://r-medicine.org/ 2021, August 24th. What is included in this Video Lesson/workshop:\n\nChapter 1 RHC data description 4:22\nChapter 2 G-computation 23:13\nChapter 3 G-computation using ML 45:02\nChapter 4 IPTW 1:18:50\nChapter 5 IPTW using ML 1:30:11\nChapter 6 TMLE 1:36:41\nChapter 7 Pre-packaged software 1:58:05\nChapter 8 Final Words 2:14:36\n\nThe timestamps are also included in the YouTube video description.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#links",
    "href": "machinelearningCausal0.html#links",
    "title": "Concepts (C)",
    "section": "Links",
    "text": "Links\nThe original workshop materials are available here.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal0.html#references",
    "href": "machinelearningCausal0.html#references",
    "title": "Concepts (C)",
    "section": "References",
    "text": "References\n\n\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. “Implementing TMLE in the Presence of a Continuous Outcome.” Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nKarim, Ehsan, and Hanna Frank. 2021. “ehsanx/TMLEworkshop: R Guide for TMLE in Medical Research.” Zenodo. https://doi.org/10.5281/zenodo.5246085.",
    "crumbs": [
      "ML in causal inference",
      "Concepts (C)"
    ]
  },
  {
    "objectID": "machinelearningCausal1.html",
    "href": "machinelearningCausal1.html",
    "title": "Motivation",
    "section": "",
    "text": "When using methods like propensity score approaches, we are making assumptions about the model specification. For example, we must specify any interaction terms.\nWith machine learning methods, these assumptions can be relaxed somewhat, as some machine learning methods allow automatic detection of data structures such as interactions.\nHowever, machine learning was created for prediction modeling, not with causal inference in mind. Statistical inference such as calculating standard errors and confidence intervals is not as straightforward since the estimator given by machine learning methods does not follow a known statistical distribution. By contrast, the estimators resulting from a standard regression using maximum likelihood estimation will follow an approximately normal distribution, where it is easy to calculate standard errors and confidence intervals.\n\nTargeted maximum likelihood estimation (TMLE) is a causal inference method that can incorporate machine learning in a way that still allows straightforward statistical inference based on theoretical development grounded in semi-parametric theory.\n\nTMLE is a doubly robust method. This means it uses both the exposure (AKA propensity score) model and the outcome model. As long as one of these models is correctly specified, TMLE will give a consistent estimator, meaning it gets closer and closer to the true value as the sample size increases.\nSince TMLE uses both the exposure and the outcome model, machine learning can be used in each of these intermediary modeling steps while allowing straightforward statistical inference.\n\n\nIt has been shown that TMLE outperform singly robust methods with machine learning, such as IPTW.\n\nRevisiting RHC Data\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial uses the same data as some of the previous tutorials, including working with a predictive question, machine learning with a continuous outocome, and machine learning with a binary outcome.\n\n\n\nObsData &lt;- readRDS(file = \n                     \"Data/machinelearningCausal/rhcAnalyticTest.RDS\")\nbaselinevars &lt;- names(dplyr::select(ObsData, \n                         !c(RHC.use,Length.of.Stay,Death)))\nhead(ObsData)\n\n\n  \n\n\n\nTable 1\nOnly for some demographic and comorbidity variables; matches with Table 1 in Connors et al. (1996).\n\ntab0 &lt;- CreateTableOne(vars = c(\"age\", \"sex\", \"race\", \n                                \"Disease.category\", \"Cancer\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab0, showAllLevels = FALSE)\n#&gt;                       Stratified by RHC.use\n#&gt;                        0            1           \n#&gt;   n                    3551         2184        \n#&gt;   age (%)                                       \n#&gt;      [-Inf,50)          884 (24.9)   540 (24.7) \n#&gt;      [50,60)            546 (15.4)   371 (17.0) \n#&gt;      [60,70)            812 (22.9)   577 (26.4) \n#&gt;      [70,80)            809 (22.8)   529 (24.2) \n#&gt;      [80, Inf)          500 (14.1)   167 ( 7.6) \n#&gt;   sex = Female (%)     1637 (46.1)   906 (41.5) \n#&gt;   race (%)                                      \n#&gt;      white             2753 (77.5)  1707 (78.2) \n#&gt;      black              585 (16.5)   335 (15.3) \n#&gt;      other              213 ( 6.0)   142 ( 6.5) \n#&gt;   Disease.category (%)                          \n#&gt;      ARF               1581 (44.5)   909 (41.6) \n#&gt;      CHF                247 ( 7.0)   209 ( 9.6) \n#&gt;      Other              955 (26.9)   208 ( 9.5) \n#&gt;      MOSF               768 (21.6)   858 (39.3) \n#&gt;   Cancer (%)                                    \n#&gt;      None              2652 (74.7)  1727 (79.1) \n#&gt;      Localized (Yes)    638 (18.0)   334 (15.3) \n#&gt;      Metastatic         261 ( 7.4)   123 ( 5.6)\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.",
    "crumbs": [
      "ML in causal inference",
      "Motivation"
    ]
  },
  {
    "objectID": "machinelearningCausal2.html",
    "href": "machinelearningCausal2.html",
    "title": "SuperLearner",
    "section": "",
    "text": "Choosing Learners\nSuperLearner is a type 2 ensemble method, meaning it combines many methods of different types into one predictive model. SuperLearner uses cross-validation to find the best weighted combination of algorithms based on the predictive performance measure specified (default in the SuperLearner package is non-negative least squares based on the Lawson-Hanson algorithm (Mullen and Stokkum 2023), but measures such as AUC can also be used). To run SuperLearner, the user needs to specify a library consisting of all the different methods SuperLearner should incorporate in the final model, as well as the number of cross-validation folds.\n\n\nSee previous chapter for other types of ensemble learning methods.\nSuperLearner will perform as well as possible given the library of algorithms considered. A very recent paper by Phillips et al. (2023) provides some concrete guidelines for the determination of the number of cross-validation folds necessary and the selection of algorithms to include. Overall, we want to make sure the set of algorithms provided is:\n\n\nDiverse: Having a rich library of algorithms allows the SuperLearner to adapt to a range of underlying data structures. Diverse libraries include:\n\nParametric learners such as generalized linear models (GLMs)\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\n\n\nComputationally feasible: Lots of machine learning algorithms take a long time to run. Having multiple computationally intensive algorithms in your library will cause the SuperLearner as a whole to take much too long to run.\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the more specific guidelines depend on our effective sample size. For binary outcomes, this can be calculated as:\n\\[ n_{eff}=min(n, 5*(n*min(\\bar{p},1-\\bar{p})))   \\]\nwhere \\(\\bar{p}\\): prevalence of the outcome.\nFor continuous outcomes, the effective sample size is the same as the sample size (\\(n_{eff} = n\\)).\n\n\nWe also want to consider the characteristics of our particular sample.\n\nIf there are continuous covariates: We should include learners that do not force relationships to be linear/monotonic. For example, we could include regression splines, support vector machines, and tree-based methods like regression trees.\nIf we have high-dimensional data (a large number of covariates e.g. more than \\(n_{eff}/20\\) ): We should include some learners that fall under the class of screeners. These are learners which incorporate dimension reduction such as LASSO and random forests.\nIf the sample size is very large (i.e. \\(n_{eff}&gt;500\\) ): We should include as many learners as is computationally feasible.\nIf the sample size is small (i.e. \\(n_{eff} \\leq 500\\) ): We should include fewer learners (e.g. up to \\(n_{eff}/5\\) ), and include less flexible learners.\n\nSome examples of learners that could be included are given in the table below (Polley 2021):\n\n\n\n\n\n\nType of learner\nExamples\n\n\n\nParametric\n\nSL.mean: simple mean\nSL.glm: generalized linear models\nSL.lm: ordinary least squares\nSL.speedglm: fast version of glm\nSL.speedlm: fast version of lm\nSL.gam: generalized additive methods\nSL.step: choose model based on AIC (backwards or forwards or both)\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression using elastic net (ridge regression and Lasso)\n\nKernel-based methods\n\nSL.kernelKnn: k-nearest neighbours\nSL.ksvm: kernel-based support vector machine\n\n\nSL.xgboost: extreme gradient boosting\nSL.gbm: gradient-boosted machines\nSL.nnet: neural networks\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.earth: multivariate adaptive regression splines\n\nTree-based methods\n\nSL.randomForest: random forests\ntmle.SL.dbarts2: bayesian additive regression trees\nSL.cforest: random forests using conditional inference trees\nSL.ranger: fast implementation of random forest suited for high dimensional data\n\n\nSL.svm: support vector machines\n\n\n\nScreeners\n\nscreen.corP: retain covariates with correlation with outcome p-value &lt;0.1\nscreen.corRank: retain top j covariates with highest correlation with outcome\nscreen.glmnet: Lasso\nscreen.randomForest: random forests\nscreen.SIS: retain covariates based on distance correlation\n\n\n\n\nThere is also a useful tool implemented in the SuperLearner library which allows us to easily see a list of all available learners.\n\nSuperLearner::listWrappers()\n#&gt; All prediction algorithm wrappers in SuperLearner:\n#&gt;  [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n#&gt;  [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n#&gt;  [7] \"SL.earth\"            \"SL.gam\"              \"SL.gbm\"             \n#&gt; [10] \"SL.glm\"              \"SL.glm.interaction\"  \"SL.glmnet\"          \n#&gt; [13] \"SL.ipredbagg\"        \"SL.kernelKnn\"        \"SL.knn\"             \n#&gt; [16] \"SL.ksvm\"             \"SL.lda\"              \"SL.leekasso\"        \n#&gt; [19] \"SL.lm\"               \"SL.loess\"            \"SL.logreg\"          \n#&gt; [22] \"SL.mean\"             \"SL.nnet\"             \"SL.nnls\"            \n#&gt; [25] \"SL.polymars\"         \"SL.qda\"              \"SL.randomForest\"    \n#&gt; [28] \"SL.ranger\"           \"SL.ridge\"            \"SL.rpart\"           \n#&gt; [31] \"SL.rpartPrune\"       \"SL.speedglm\"         \"SL.speedlm\"         \n#&gt; [34] \"SL.step\"             \"SL.step.forward\"     \"SL.step.interaction\"\n#&gt; [37] \"SL.stepAIC\"          \"SL.svm\"              \"SL.template\"        \n#&gt; [40] \"SL.xgboost\"\n#&gt; \n#&gt; All screening algorithm wrappers in SuperLearner:\n#&gt; [1] \"All\"\n#&gt; [1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n#&gt; [4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n#&gt; [7] \"screen.ttest\"          \"write.screen.template\"\n\nSuperLearner in TMLE\n\n\nThe default SuperLearner library for estimating the outcome includes (Gruber, Van Der Laan, and Kennedy 2020)\n\n\nSL.glm: generalized linear models (GLMs)\n\nSL.glmnet: least absolute shrinkage and selection operator (LASSO)\n\ntmle.SL.dbarts2: modeling and prediction using Bayesian Additive Regression Trees (BART)\n\n\n\nThe default library for estimating the propensity scores includes\n\n\nSL.glm: generalized linear models (GLMs)\n\ntmle.SL.dbarts.k.5: SL wrappers for modeling and prediction using BART\n\nSL.gam: generalized additive models (GAMs)\n\n\n\nIt is certainly possible to use different set of learners\n\nMore methods can be added by\n\nspecifying lists of models in the Q.SL.library (for the outcome model) and g.SL.library (for the propensity score model)\n\n\n\n\nReferences\n\n\n\n\nGruber, S., M. Van Der Laan, and C. Kennedy. 2020. Package ’Tmle’. https://cran.r-project.org/web/packages/tmle/tmle.pdf.\n\n\nMullen, Katharine M., and Ivo H. M. van Stokkum. 2023. Package ’Nnls’. https://cran.r-project.org/web/packages/nnls/nnls.pdf.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023.\n\n\nPolley, Eric. 2021. Package ’SuperLearner’. https://cran.r-project.org/web/packages/SuperLearner/SuperLearner.pdf.",
    "crumbs": [
      "ML in causal inference",
      "SuperLearner"
    ]
  },
  {
    "objectID": "machinelearningCausal3.html",
    "href": "machinelearningCausal3.html",
    "title": "Binary Outcomes",
    "section": "",
    "text": "For this example we will be looking at the binary outcome variable death.\n\n# Data\nload(file = \"Data/machinelearningCausal/cl2.RData\")\n\n# Table 1\ntab1 &lt;- CreateTableOne(vars = c(\"Death\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#&gt;                    Stratified by RHC.use\n#&gt;                     0           1          \n#&gt;   n                 3551        2184       \n#&gt;   Death (mean (SD)) 0.63 (0.48) 0.68 (0.47)\n\nTMLE\nTMLE works by first constructing an initial outcome and extracting a crude estimate of the treatment effect. Then, TMLE aims to refine the initial estimate in the direction of the true value of the parameter of interest through use of the exposure model.\nTo label the treatment effect given by TMLE as causal, the same conditional exchangeability, positivity, and consistency assumptions must be met as for other modeling strategies (see introduction to propensity score). TMLE also assumes that at least one of the exposure or outcome model is correctly specified. If this does not hold, TMLE does not necessarily produce a consistent estimator.\n\n\nLuque-Fernandez et al. (2018) discussed the implementation of TMLE, and providing a detailed step-by-step guide, primarily focusing on a binary outcome.\nThe basic steps are:\n\nConstruct initial outcome model & get crude estimate\nConstruct exposure model and use propensity scores to update the initial outcome model through a targeted adjustment\nExtract treatment effect estimate\nEstimate confidence interval based on a closed-form formula\n\nThe tmle package implements TMLE for both binary and continuous outcomes, and uses the SuperLearner to construct the exposure and outcome models.\nThe tmle method takes a number of parameters, including:\n\n\nTerm\nDescription\n\n\n\nY\nOutcome vector\n\n\nA\nExposure vector\n\n\nW\nMatrix that includes vectors of all covariates\n\n\nfamily\nDistribution\n\n\nV\nCross-validation folds for exposure and outcome modeling\n\n\nQ.SL.library\nSet of machine learning methods to use for SuperLearner for outcome modeling\n\n\ng.SL.library\nSet of machine learning methods to use for SuperLearner for exposure modeling\n\n\nConstructing SuperLearner\nWe will need to specify two SuperLearners, one for the exposure and one for the outcome model. We will need to consider the characteristics of our sample in order to decide the number of cross-validation folds and construct a diverse and computationally feasible library of algorithms.\nNumber of folds\nFirst, we need to define the number of cross-validation folds to use for each model. This depends on our effective sample size (Phillips et al. 2023).\nOur effective sample size for the outcome model is:\n\nn &lt;- nrow(ObsData) \np &lt;- nrow(ObsData[ObsData$Death == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(p, 1-p))) \nn_eff\n#&gt; [1] 5735\n\nOur effective sample size for the exposure model is:\n\np_exp &lt;- nrow(ObsData[ObsData$RHC.use == 1,])/n \nn_eff_exp &lt;- min(n, 5*(n*min(p, 1-p))) \nn_eff_exp\n#&gt; [1] 5735\n\nFor both models, the effective sample size is the same as our sample size, \\(n = 5,735\\).\nSince \\(5,000 \\leq n_{eff} \\leq 10,000\\), we should use 5 or more cross-validation folds according to Phillips et al. (2023). For the sake of computational feasibility, we will use 5 folds in this example.\nCandidate learners\nThe second step is to define the library of learners we will feed in to SuperLearner as potential options for each model (exposure and outcome). In this example, some of our covariates are continuous variables, such as temperature and blood pressure, so we need to include learners that allow non-linear/monotonic relationships.\nSince \\(n\\) is large (\\(&gt;5000\\)), we should include as many learners as is computationally feasible in our libraries.\nFurthermore, we have 50 covariates:\n\nlength(c(baselinevars, \"Length.of.Stay\"))\n#&gt; [1] 50\n\n\\(5735/20 = 286.75\\), and \\(50&lt;286.75\\), so we do not have high-dimensional data and including screeners is optional (Phillips et al. 2023).\nSince the requirements for the exposure and outcome models are the same in this example, we will use the same SuperLearner library for both. Overall for this example we need to make sure to include:\n\nParametric learners\nHighly data-adaptive learners\nMultiple variants of the same learner with different parameter specifications\nLearners that allow non-linear/monotonic relationships\n\nFor this example, we will include the following learners:\n\n\nParametric\n\nSL.mean: only mean\nSL.glm: generalized linear model\n\n\n\nHighly data-adaptive\n\nSL.glmnet: penalized regression such as lasso\nSL.xgboost: extreme gradient boosting\n\n\n\nAllowing non-linear/monotonic relationships\n\nSL.randomForest: random forest\ntmle.SL.dbarts2: bayesian additive regression tree\nSL.svm: support vector machine\n\n\n\n\n# Construct the SuperLearner library\nSL.library &lt;- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nTMLE with SuperLearner\nTo run TMLE, we need to install the tmle package and load it on the R environment.\n\n#install.packages(c('tmle', 'xgboost'))\nrequire(tmle)\nrequire(xgboost)\n\nWe also need to create a data frame containing only the covariates:\n\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Death, RHC.use))\nObsData$Death &lt;- as.numeric(ObsData$Death)\n\nThen we can run TMLE using the tmle method from the tmle package:\n\nset.seed(1444) \n\ntmle.fit &lt;- tmle::tmle(Y = ObsData$Death, \n                   A = ObsData$RHC.use, \n                   W = ObsData.noYA, \n                   family = \"binomial\", \n                   V.Q = 5,\n                   V.g = 5,\n                   Q.SL.library = SL.library, \n                   g.SL.library = SL.library)\n\ntmle.est.bin &lt;- tmle.fit$estimates$OR$psi\ntmle.ci.bin &lt;- tmle.fit$estimates$OR$CI\n\nATE for binary outcome using user-specified library: 1.29 and 95% CI is 1.2011697, 1.3878896\nThese results show those who received RHC had odds of death that were 1.29 times as high as the odds of death in those who did not receive RHC.\nUnderstanding defaults\nWe can compare the results using our specified SuperLearner library to the results we would get when using the tmle package’s default SuperLearner libraries. To do this we simply do not specify libraries for the Q.SL.library and g.SL.library arguments.\n\n# small test library \n# with only glm just \n# for sake of making this work\nSL.library.test &lt;- c(\"SL.glm\")\n\n\nset.seed(1444) \n\ntmle.fit.def &lt;- tmle::tmle(Y = ObsData$Death, \n                           A = ObsData$RHC.use, \n                           W = ObsData.noYA, \n                           family = \"binomial\", \n                           V.Q = 5,\n                           V.g = 5)\n# Q.SL.library = SL.library.test,  ## removed this line\n# g.SL.library = SL.library.test)  ## removed this line\n\ntmle.est.bin.def &lt;- tmle.fit.def$estimates$OR$psi\ntmle.ci.bin.def &lt;- tmle.fit.def$estimates$OR$CI\n\nATE for binary outcome using default library: 1.32 with 95% CI 1.1808107, 1.4657624.\nThese ATE when using the default SuperLearner library (1.31) is very close to the ATE when using our user-specified SuperLearner library (1.29). However, the confidence interval from TMLE using the default SuperLearner library (1.17, 1.46) is slightly wider than the confidence interval from TMLE using our user-specified SuperLearner library (1.20, 1.39).\nComparison of results\nWe can also compare these results to those from a basic regression and from the literature.\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.Death &lt;- c(baselinevars, \"Length.of.Stay\")\nout.formula.bin &lt;- as.formula(\n  paste(\"Death~ RHC.use +\",\n        paste(baselineVars.Death, \n              collapse = \"+\")))\nfit1.bin &lt;- glm(out.formula.bin, data = ObsData, family=\"binomial\")\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 4 showed that, after propensity score pair (1-to-1) matching, the odds of in-hospital mortality were 39% higher in those who received RHC (OR: 1.39 (1.15, 1.67)).\n\n\n\n\nmethod.list\nEstimate\n2.5 %\n97.5 %\n\n\n\nAdjusted Regression\n0.36\n0.22\n0.51\n\n\nTMLE (user-specified SL library)\n1.29\n1.20\n1.39\n\n\nTMLE (default SL library)\n1.32\n1.18\n1.47\n\n\nConnors et al. (1996) paper\n1.39\n1.15\n1.67\n\n\n\n\n\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.\n\n\nLuque-Fernandez, Miguel Angel, Michael Schomaker, Bernard Rachet, and Mireille E Schnitzer. 2018. “Targeted Maximum Likelihood Estimation for a Binary Treatment: A Tutorial.” Statistics in Medicine 37 (16): 2530–46.\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023.",
    "crumbs": [
      "ML in causal inference",
      "Binary Outcomes"
    ]
  },
  {
    "objectID": "machinelearningCausal4.html",
    "href": "machinelearningCausal4.html",
    "title": "Continuous Outcomes",
    "section": "",
    "text": "We will now go through an example of using TMLE for a continuous outcome. The setup for SuperLearner in this case is similar to that for binary outcomes, so rather than going through the SuperLearner steps again, we will instead focus on the additional steps that are necessary for running the tmle method on continuous outcomes.\n\n\nFrank and Karim (2023) extensively discussed the implementation of TMLE for continuous outcomes, providing a detailed step-by-step guide using the openly accessible RHC dataset. In this tutorial, we will revisit the same example with additional explanations.\n\n\n\n\n\n\nNote\n\n\n\nOnly outcome variable (Length of stay); slightly different than Table 2 in Connors et al. (1996) (means were 20.5 vs. 25.7; and medians were 16 vs. 17).\n\n\n\ntab1 &lt;- CreateTableOne(vars = c(\"Length.of.Stay\"),\n                       data = ObsData, \n                       strata = \"RHC.use\", \n                       test = FALSE)\nprint(tab1, showAllLevels = FALSE, )\n#&gt;                             Stratified by RHC.use\n#&gt;                              0             1            \n#&gt;   n                           3551          2184        \n#&gt;   Length.of.Stay (mean (SD)) 19.53 (23.59) 24.86 (28.90)\n\n\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==0])\n#&gt; [1] 12\nmedian(ObsData$Length.of.Stay[ObsData$RHC.use==1])\n#&gt; [1] 16\n\nConstructing SuperLearner\nJust as we did for a binary outcome, we will need to specify two SuperLearners, one for the exposure and one for the outcome model.\nThe effective sample size for a continuous outcome is just \\(n_{eff}=n=5,735\\). We calculated the effective sample size for the exposure model earlier, which also turned out to be \\(n_{eff}=n=5,735\\). So once again we will use 5 folds because \\(5,000 \\leq n_{eff} \\leq 10,000\\) (Phillips et al. 2023).\nSimilarly to our example with the binary outcome, the key considerations for the library of learners are:\n\nWe have some continuous covariates, and should therefore include learners that allow non-linear/monotonic relationships.\nWe have a large \\(n\\), so should include as many learners as is computationally feasible.\nWe have 49 covariates and 5,735 observations, so we do not have high-dimensional data and including screeners is optional.\n\nAgain the requirements for the exposure and outcome models are the same and we can use the same library for both models. Note that even though one model will have a binary dependent variable, and one will have a continuous dependent variable, most of the available learners automatically adapt to binary and continuous dependent variables.\nFor this example, we will use the same SuperLearner library as for the binary outcome example.\n\n# Construct the SuperLearner library\nSL.library &lt;- c(\"SL.mean\", \n                \"SL.glm\", \n                \"SL.glmnet\", \n                \"SL.xgboost\", \n                \"SL.randomForest\", \n                \"tmle.SL.dbarts2\", \n                \"SL.svm\")\n\nDealing with continuous outcomes\nFor this example, we will be examining the length of stay in hospital outcome.\nThe key difference between running TMLE on a continuous outcome in comparison to running it with a binary outcome, is that we must transform the outcome to fall within the range of 0 to 1, so that the modeled outcomes fall within the range of the outcome’s true distribution (Gruber and Laan 2010).\nTo transform the outcome, we can use min-max normalization:\n\\[\nY_{transformed} = \\frac{Y-Y_{min}}{Y_{max}-Y_{min}}\n\\]\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y &lt;- min(ObsData$Length.of.Stay)\nmax.Y &lt;- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf &lt;- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nOnce we have transformed the outcome to fall within the range of 0 to 1, we can run TMLE as before, using the tmle method in the tmle package:\n\n# create data frame containing only covariates\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont &lt;- tmle::tmle(Y = ObsData$Length.of.Stay_transf, \n                       A = ObsData$RHC.use, \n                       W = ObsData.noYA, \n                       family = \"gaussian\", \n                       V.Q = 5,\n                       V.g = 5,\n                       Q.SL.library = SL.library,\n                       g.SL.library = SL.library)\n\nOnce the tmle method has run, we still have one step to complete to get our final estimate. At this point, we must transform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome’s original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n# transform back the ATE estimate\ntmle.est.cont &lt;- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$psi\ntmle.est.cont\n#&gt; [1] 2.939622\n\nWe also have to transform the confidence interval back to the original scale:\n\ntmle.ci.cont &lt;- (max.Y-min.Y)*\n  tmle.fit.cont$estimates$ATE$CI\n\nATE for continuous outcome: 2.9396218, and 95 % CI is 1.959698, 3.9195455.\nThe results indicate that if all participants had received RHC, the average length of stay in hospital would be 2.95 (1.99, 3.91) days longer than if no participants had received RHC.\nUnderstanding defaults\nTransform outcome:\n\nset.seed(1444) \n# transform the outcome to fall within the range [0,1]\nmin.Y &lt;- min(ObsData$Length.of.Stay)\nmax.Y &lt;- max(ObsData$Length.of.Stay)\nObsData$Length.of.Stay_transf &lt;- \n  (ObsData$Length.of.Stay-min.Y)/\n  (max.Y-min.Y)\n\nRun TMLE, using the tmle package’s default SuperLearner library:\n\n# create data frame containing only covariates\nObsData.noYA &lt;- dplyr::select(ObsData, \n                              !c(Length.of.Stay_transf, \n                                 Length.of.Stay, \n                                 RHC.use))\n\n\nset.seed(1444) \n\n# run tmle\ntmle.fit.cont.def &lt;- tmle::tmle(\n  Y = ObsData$Length.of.Stay_transf, \n  A = ObsData$RHC.use, \n  W = ObsData.noYA,\n  family = \"gaussian\",\n  V.Q = 5,\n  V.g = 5)\n# Q.SL.library = SL.library.test,  \n## removed this line\n# g.SL.library = SL.library.test)  \n## removed this line\n\nTransform the average treatment effect generated by the tmle method (\\(\\widehat{ATE}\\)) back to the outcome’s original scale:\n\\[\n\\widehat{ATE}_{rescaled} = (Y_{max}-Y_{min})*\\widehat{ATE}\n\\]\n\n# transform back the ATE estimate\ntmle.est.cont.def &lt;- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$psi\ntmle.est.cont.def\n#&gt; [1] 3.036298\n\nTransform the confidence interval back to the original scale:\n\ntmle.ci.cont.def &lt;- (max.Y-min.Y)*\n  tmle.fit.cont.def$estimates$ATE$CI\n\nATE for continuous outcome using default library: 3.0362984, and 95% CI 1.2686301, 4.8039667.\nThe estimate using the default SuperLearner library (2.18) is similar to the estimate we got when using our user-specified SuperLearner library (2.95). However, the confidence interval using the default SuperLearner library (1.25, 4.37) was much wider than that using our user-specified SuperLearner library (1.99, 3.91).\nComparison of results\nAdjusted regression:\n\n# adjust the exposure variable \n# (primary interest) + covariates\nbaselineVars.LoS &lt;- c(baselinevars, \"Death\")\nout.formula.cont &lt;- as.formula(\n  paste(\"Length.of.Stay~ RHC.use +\", \n        paste(baselineVars.LoS,\n              collapse = \"+\")))\nfit1.cont &lt;- lm(out.formula.cont, data = ObsData)\npublish(fit1.cont, digits=1)$regressionTable[2,]\n\nConnors et al. (1996) conducted a propensity score matching analysis. Table 5 showed that, after propensity score pair (1-to-1) matching, means of length of stay (\\(Y\\)), when stratified by RHC (\\(A\\)) were not significantly different (\\(p = 0.14\\)).\n\n\n\n\nmethod.list\nEstimate\n2.5 %\n97.5 %\n\n\n\nAdjusted Regression\n3.04\n1.51\n4.58\n\n\nTMLE (user-specified SL library)\n2.94\n1.96\n3.92\n\n\nTMLE (default SL library)\n3.04\n1.27\n4.80\n\n\nKeele and Small (2021) paper\n2.01\n0.60\n3.41\n\n\n\n\n\nDifferences in results can likely be attributed to the use of different SuperLearner libraries, the use of different combinations of variables used, or random sampling associated with the cross-validation used in the SuperLearner algorithm.\nReferences\n\n\n\n\nConnors, Alfred F, Theodore Speroff, Neal V Dawson, Charles Thomas, Frank E Harrell, Douglas Wagner, Norman Desbiens, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” Jama 276 (11): 889–97. https://tinyurl.com/Connors1996.\n\n\nFrank, Hanna A, and Mohammad Ehsanul Karim. 2023. “Implementing TMLE in the Presence of a Continuous Outcome.” Research Methods in Medicine & Health Sciences, 26320843231176662.\n\n\nGruber, Susan, and Mark J van der Laan. 2010. “A Targeted Maximum Likelihood Estimator of a Causal Effect on a Bounded Continuous Outcome.” The International Journal of Biostatistics 6 (1).\n\n\nPhillips, Rachael V., Mark J. van der Laan, Hana Lee, and Susan Gruber. 2023. “Practical Considerations for Specifying a Super Learner.” International Journal of Epidemiology 52: 1276–85. https://doi.org/10.1093/ije/dyad023.",
    "crumbs": [
      "ML in causal inference",
      "Continuous Outcomes"
    ]
  },
  {
    "objectID": "machinelearningCausal5.html",
    "href": "machinelearningCausal5.html",
    "title": "Comparing results",
    "section": "",
    "text": "In this tutorial, we will use the RHC dataset in exploring the relationship between RHC (yes/no) and death (yes/no). We will use the following approaches:\n\nlogistic regression\npropensity score matching and weighting with logistic regression\npropensity score matching and weighting with Super Learner\nTMLE\n\nLoad packages\nWe load several R packages required for fitting the models.\n\n# Load required packages\nlibrary(tableone)\nlibrary(Publish)\nlibrary(randomForest)\nlibrary(tmle)\nlibrary(xgboost)\nlibrary(kableExtra)\nlibrary(SuperLearner)\nlibrary(dbarts)\nlibrary(MatchIt)\nlibrary(cobalt)\nlibrary(survey)\nlibrary(knitr)\n\nLoad dataset\n\nload(file = \"Data/machinelearningCausal/cl2.RData\")\nls()\n#&gt; [1] \"baselinevars\" \"ObsData\"      \"tab0\"\n\n\n# Data\ndat &lt;- ObsData\nhead(dat)\n\n\n  \n\n\n\n# Data dimension\ndim(dat)\n#&gt; [1] 5735   52\n\nConfounder list in the baselinevars vector is\n\n\n\n\nConfounders\n\n\n\nDisease.category\n\n\nCancer\n\n\nCardiovascular\n\n\nCongestive.HF\n\n\nDementia\n\n\nPsychiatric\n\n\nPulmonary\n\n\nRenal\n\n\nHepatic\n\n\nGI.Bleed\n\n\nTumor\n\n\nImmunosupperssion\n\n\nTransfer.hx\n\n\nMI\n\n\nage\n\n\nsex\n\n\nedu\n\n\nDASIndex\n\n\nAPACHE.score\n\n\nGlasgow.Coma.Score\n\n\nblood.pressure\n\n\nWBC\n\n\nHeart.rate\n\n\nRespiratory.rate\n\n\nTemperature\n\n\nPaO2vs.FIO2\n\n\nAlbumin\n\n\nHematocrit\n\n\nBilirubin\n\n\nCreatinine\n\n\nSodium\n\n\nPotassium\n\n\nPaCo2\n\n\nPH\n\n\nWeight\n\n\nDNR.status\n\n\nMedical.insurance\n\n\nRespiratory.Diag\n\n\nCardiovascular.Diag\n\n\nNeurological.Diag\n\n\nGastrointestinal.Diag\n\n\nRenal.Diag\n\n\nMetabolic.Diag\n\n\nHematologic.Diag\n\n\nSepsis.Diag\n\n\nTrauma.Diag\n\n\nOrthopedic.Diag\n\n\nrace\n\n\nincome\n\n\n\n\n\nLogistic regression\nLet us define the regression formula and fit logistic regression, adjusting for baseline confounders.\n\n# Formula\nFormula &lt;- formula(paste(\"Death ~ RHC.use + \", \n                         paste(baselinevars, \n                               collapse = \" + \")))\n\n# Logistic regression\nfit.glm &lt;- glm(Formula, \n               data = dat, \n               family = binomial)\n\nWe can use flextable package to view fit.glm, the regression output:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.42\n1.23\n1.65\n\n\n\n\n\nAs we can see, the odds of death was 42% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with logistic regression\nNow we will use propensity score matching, where propensity score will be estimated using logistic regression. The first step is to define the propensity score formula and estimate the propensity scores.\nStep 1\n\n# Propensity score model define\nps.formula &lt;- as.formula(paste0(\"RHC.use ~ \", \n                                paste(baselinevars, \n                                      collapse = \"+\")))\n\n# Propensity score model fitting\nfit.ps &lt;- glm(ps.formula, \n              data = dat, \n              family = binomial)\n\n# Propensity scores\ndat$ps &lt;- predict(fit.ps, \n                  type = \"response\")\nsummary(dat$ps)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.002478 0.161446 0.358300 0.380820 0.574319 0.968425\n\nStep 2\nThe second step is to match exposed (i.e., RHC users) to unexposed (i.e., RHC non-users) based on the propensity scores. We will use nearest neighborhood and caliper approach.\n\n# Caliper\ncaliper &lt;- 0.2*sd(log(dat$ps/(1-dat$ps)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj &lt;- matchit(ps.formula, \n                     data = dat,\n                     distance = dat$ps, \n                     method = \"nearest\",\n                     ratio = 1,\n                     caliper = caliper)\n\n\n# See how many matched\nmatch.obj \n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.068)\n#&gt;  - number of obs.: 5735 (original), 3478 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: too many to name\n\n# Extract matched data\nmatched.data &lt;- match.data(match.obj)\n\n# Overlap checking\nbal.plot(match.obj,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nStep 3\nThe third step is to check the balancing on the matched data. We will compare the similarity of baseline characteristics between RHC users and non-users in the propensity score matched sample. Let’s consider SMD &gt;0.1 as imbalanced.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1b &lt;- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data, includeNA = T,\n                        addOverall = F, test = F)\n#print(tab1b, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1b) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.06\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.03\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.01\n\n\nPsychiatric\n0.04\n\n\nPulmonary\n0.04\n\n\nRenal\n0.03\n\n\nHepatic\n0.01\n\n\nGI.Bleed\n0.00\n\n\nTumor\n0.01\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.03\n\n\nMI\n0.01\n\n\nage\n0.04\n\n\nsex\n0.02\n\n\nedu\n0.03\n\n\nDASIndex\n0.03\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.07\n\n\nWBC\n0.01\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.04\n\n\nTemperature\n0.02\n\n\nPaO2vs.FIO2\n0.06\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.03\n\n\nCreatinine\n0.03\n\n\nSodium\n0.02\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.04\n\n\nPH\n0.02\n\n\nWeight\n0.02\n\n\nDNR.status\n0.00\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.06\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.02\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.03\n\n\nTrauma.Diag\n0.02\n\n\nOrthopedic.Diag\n0.05\n\n\nrace\n0.02\n\n\nincome\n0.06\n\n\n\n\n\n\nAfter propensity score matching, all the confounders are balanced in terms of SMD. Now, we will fit the outcome model on the matched data.\nStep 4\n\nfit.psm &lt;- glm(Death ~ RHC.use, \n               data = matched.data, \n               family = binomial)\n\nSummary of fit.psm:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.29\n1.12\n1.48\n\n\n\n\n\nIn the propensity score matched data, the odds of death was 29% higher among those participants with RHC use than those with no RHC use.\nPropensity score weighting with logistic regression\nNow we will use the propensity score weighting approach where propensity scores are estimated using logistic regression.\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw &lt;- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps, \n                            mean(1-RHC.use)/(1-ps)))\nsummary(dat$ipw)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.3932  0.6571  0.7781  0.9953  1.0624 24.1854\n\nStep 3\nNow, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design &lt;- svydesign(id = ~1, weights = ~ipw, data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1e &lt;- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design, includeNA = T, \n                           addOverall = F, test = F)\n#print(tab1e, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1e) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.02\n\n\nCancer\n0.01\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.00\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.02\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.01\n\n\nMI\n0.00\n\n\nage\n0.05\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.00\n\n\nblood.pressure\n0.01\n\n\nWBC\n0.04\n\n\nHeart.rate\n0.02\n\n\nRespiratory.rate\n0.00\n\n\nTemperature\n0.01\n\n\nPaO2vs.FIO2\n0.00\n\n\nAlbumin\n0.03\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.01\n\n\nSodium\n0.01\n\n\nPotassium\n0.03\n\n\nPaCo2\n0.02\n\n\nPH\n0.01\n\n\nWeight\n0.02\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.01\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.01\n\n\nGastrointestinal.Diag\n0.01\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.00\n\n\nHematologic.Diag\n0.00\n\n\nSepsis.Diag\n0.01\n\n\nTrauma.Diag\n0.01\n\n\nOrthopedic.Diag\n0.01\n\n\nrace\n0.02\n\n\nincome\n0.02\n\n\n\n\n\n\nAll confounders are balanced (SMD &lt; 0.1).\nStep 4\n\nfit.ipw &lt;- svyglm(Death ~ RHC.use, \n                  design = w.design, \n                  family = binomial)\n\nSummary of fit.ipw:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.30\n1.11\n1.53\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 30% higher among those participants with RHC use than those with no RHC use.\nPropensity score matching with super learner\nNow we will use the propensity score matching, where we will be using super learner to calculate the propensity scores. We use logistic, LASSO, and XGBoost as the candidate learners.\nStep 1\n\nset.seed(123)\nps.fit &lt;- CV.SuperLearner(\n  Y = dat$RHC.use,\n  X = dat[,baselinevars], \n  family = \"binomial\",\n  SL.library = c(\"SL.glm\", \"SL.glmnet\", \"SL.xgboost\"), \n  verbose = FALSE,\n  V = 5, \n  method = \"method.NNLS\")\n\n\n# Propensity scores for all learners  \npredictions &lt;- cbind(ps.fit$SL.predict, ps.fit$library.predict)\nhead(predictions)\n#&gt;                SL.glm_All SL.glmnet_All SL.xgboost_All\n#&gt; [1,] 0.3632635 0.39789364    0.39569431     0.32144672\n#&gt; [2,] 0.5870492 0.64983590    0.61497078     0.54465985\n#&gt; [3,] 0.5740246 0.66329951    0.65396650     0.46847290\n#&gt; [4,] 0.2172501 0.33044244    0.34478278     0.02363573\n#&gt; [5,] 0.6347749 0.45972157    0.43779434     0.86645132\n#&gt; [6,] 0.0272532 0.03420477    0.03629319     0.01730520\n\n# Propensity scores for super learner\ndat$ps.sl &lt;- predictions[,1]\nsummary(dat$ps.sl)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.001671 0.147762 0.337664 0.376282 0.587237 0.975584\n\nStep 2\nThe same as before, we will match exposed to unexposed based on their propensity scores.\n\n# Caliper\ncaliper &lt;- 0.2*sd(log(dat$ps.sl/(1-dat$ps.sl)))\n\n# 1:1 matching\nset.seed(123)\nmatch.obj2 &lt;- matchit(ps.formula, \n                      data = dat,\n                      distance = dat$ps.sl, \n                      method = \"nearest\",\n                      ratio = 1,\n                      caliper = caliper)\n\n\n# See how many matched\nmatch.obj2 \n#&gt; A `matchit` object\n#&gt;  - method: 1:1 nearest neighbor matching without replacement\n#&gt;  - distance: User-defined [caliper]\n#&gt;  - caliper: &lt;distance&gt; (0.075)\n#&gt;  - number of obs.: 5735 (original), 3430 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: too many to name\n\n# Extract matched data\nmatched.data.sl &lt;- match.data(match.obj2) \n\n# Overlap checking\nbal.plot(match.obj2,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nStep 3\nNow we will check the balancing on the matched data.\n\n# Balance checking in terms of SMD - using love plot\nlove.plot(match.obj2, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\")) \n\n\n\n\n\n\n\n# Balance checking in terms of SMD - using tableone\ntab1c &lt;- CreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                        data = matched.data.sl, includeNA = T, \n                        addOverall = T, test = F)\n#print(tab1c, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1c) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.08\n\n\nCancer\n0.05\n\n\nCardiovascular\n0.01\n\n\nCongestive.HF\n0.02\n\n\nDementia\n0.00\n\n\nPsychiatric\n0.02\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.02\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.05\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.06\n\n\nMI\n0.03\n\n\nage\n0.06\n\n\nsex\n0.04\n\n\nedu\n0.03\n\n\nDASIndex\n0.01\n\n\nAPACHE.score\n0.08\n\n\nGlasgow.Coma.Score\n0.01\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.00\n\n\nHeart.rate\n0.05\n\n\nRespiratory.rate\n0.03\n\n\nTemperature\n0.04\n\n\nPaO2vs.FIO2\n0.09\n\n\nAlbumin\n0.05\n\n\nHematocrit\n0.03\n\n\nBilirubin\n0.00\n\n\nCreatinine\n0.05\n\n\nSodium\n0.01\n\n\nPotassium\n0.01\n\n\nPaCo2\n0.05\n\n\nPH\n0.06\n\n\nWeight\n0.06\n\n\nDNR.status\n0.04\n\n\nMedical.insurance\n0.06\n\n\nRespiratory.Diag\n0.08\n\n\nCardiovascular.Diag\n0.05\n\n\nNeurological.Diag\n0.04\n\n\nGastrointestinal.Diag\n0.02\n\n\nRenal.Diag\n0.00\n\n\nMetabolic.Diag\n0.02\n\n\nHematologic.Diag\n0.02\n\n\nSepsis.Diag\n0.04\n\n\nTrauma.Diag\n0.06\n\n\nOrthopedic.Diag\n0.06\n\n\nrace\n0.02\n\n\nincome\n0.04\n\n\n\n\n\n\nAgain, all confounders are balanced in terms of SMD (all SMDs &lt; 0.1). Next, we will fit the outcome model.\nStep 4\n\nfit.psm.sl &lt;- glm(Death ~ RHC.use, \n                  data = matched.data.sl, \n                  family = binomial)\n\nSummary of fit.psm.sl:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.09\n1.45\n\n\n\n\n\nThe interpretation is the same as before. In the propensity score matched data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nPropensity score weighting with super learner\nStep 1\nStep 1 is the same as we did it for the propensity score matching.\nStep 2\nFor the second step, we will calculate the stabilized inverse probability weight.\n\ndat$ipw.sl &lt;- with(dat, ifelse(RHC.use==1, mean(RHC.use)/ps.sl, \n                               mean(1-RHC.use)/(1-ps.sl)))\nsummary(dat$ipw.sl)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.3903  0.6496  0.7647  1.0191  1.0468 41.4490\n\nStep 3\nNext, we will check the balance in terms of SMD.\n\n# Design with inverse probability weights\nw.design.sl &lt;- svydesign(id = ~1, weights = ~ipw.sl, \n                         data = dat, nest = F)\n\n# Balance checking in terms of SMD\ntab1k &lt;- svyCreateTableOne(vars = baselinevars, strata = \"RHC.use\", \n                           data = w.design.sl, includeNA = T, \n                           addOverall = T, test = F)\n#print(tab1k, showAllLevels = T, catDigits = 2, smd = T)\n\nExtractSmd(tab1k) shows\n\n\n\n\n\n\nVariables\n1 vs 2\n\n\n\nDisease.category\n0.01\n\n\nCancer\n0.03\n\n\nCardiovascular\n0.02\n\n\nCongestive.HF\n0.01\n\n\nDementia\n0.05\n\n\nPsychiatric\n0.03\n\n\nPulmonary\n0.01\n\n\nRenal\n0.01\n\n\nHepatic\n0.00\n\n\nGI.Bleed\n0.02\n\n\nTumor\n0.00\n\n\nImmunosupperssion\n0.01\n\n\nTransfer.hx\n0.00\n\n\nMI\n0.01\n\n\nage\n0.07\n\n\nsex\n0.03\n\n\nedu\n0.00\n\n\nDASIndex\n0.04\n\n\nAPACHE.score\n0.01\n\n\nGlasgow.Coma.Score\n0.02\n\n\nblood.pressure\n0.05\n\n\nWBC\n0.06\n\n\nHeart.rate\n0.00\n\n\nRespiratory.rate\n0.01\n\n\nTemperature\n0.00\n\n\nPaO2vs.FIO2\n0.03\n\n\nAlbumin\n0.00\n\n\nHematocrit\n0.02\n\n\nBilirubin\n0.01\n\n\nCreatinine\n0.00\n\n\nSodium\n0.00\n\n\nPotassium\n0.02\n\n\nPaCo2\n0.05\n\n\nPH\n0.02\n\n\nWeight\n0.01\n\n\nDNR.status\n0.05\n\n\nMedical.insurance\n0.03\n\n\nRespiratory.Diag\n0.02\n\n\nCardiovascular.Diag\n0.01\n\n\nNeurological.Diag\n0.02\n\n\nGastrointestinal.Diag\n0.00\n\n\nRenal.Diag\n0.01\n\n\nMetabolic.Diag\n0.01\n\n\nHematologic.Diag\n0.01\n\n\nSepsis.Diag\n0.00\n\n\nTrauma.Diag\n0.05\n\n\nOrthopedic.Diag\n0.02\n\n\nrace\n0.05\n\n\nincome\n0.05\n\n\n\n\n\n\nAll confounders are balanced (SMD &lt; 0.1).\nStep 4\n\nfit.ipw.sl &lt;- svyglm(Death ~ RHC.use, \n                     design = w.design.sl, \n                     family = binomial)\n\nSummary of fit.ipw.sl:\n\n\n\n\n\n\nVariable\nOdds_Ratio\nCI_Lower\nCI_Upper\n\n\nRHC.use\n1.26\n1.04\n1.52\n\n\n\n\n\nIn the propensity score weighted data, the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nTMLE\nFrom the previous tutorial, we calculated the effective sample size for the outcome model as well as for the exposure model:\n\nn &lt;- nrow(dat) \npY &lt;- nrow(dat[dat$Death == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(pY, 1 - pY))) \nn_eff\n#&gt; [1] 5735\n\n\nn &lt;- nrow(dat) \npA &lt;- nrow(dat[dat$RHC.use == 1,])/n \nn_eff &lt;- min(n, 5*(n*min(pA, 1 - pA))) \nn_eff\n#&gt; [1] 5735\n\nSince the effective sample size is \\(\\ge 5,000\\) for both model, we can consider \\(5 \\le \\text{V} \\le 10\\), where V is the number of folds. Let’s work with 10-fold cross-validation for both the exposure and the outcome model, with the default super learner library.\n\nfit.tmle &lt;- tmle(Y = dat$Death, \n                 A = dat$RHC.use, \n                 W = dat[,baselinevars], \n                 family = \"binomial\", \n                 V.Q = 10, \n                 V.g = 10)\n\n\n# OR\nround(fit.tmle$estimates$OR$psi, 2)\n#&gt; [1] 1.26\n\n# 95% CI\nround(fit.tmle$estimates$OR$CI, 2)\n#&gt; [1] 1.12 1.42\n\nAs we can see that the odds of death was 26% higher among those participants with RHC use than those without RHC use.\nResults comparison\n\n\n\n\nModel\nOR\n95% CI\n\n\n\nLogistic regression\n1.42\n1.32-1.65\n\n\nPropensity score matching with logistic\n1.29\n1.12-1.48\n\n\nPropensity score weighting with logistic\n1.30\n1.11-1.53\n\n\nPropensity score matching with super learner (logistic, LASSO, and XGBoost)\n1.26\n1.09-1.45\n\n\nPropensity score weighting with super learner (logistic, LASSO, and XGBoost)\n1.26\n1.04-1.52\n\n\nTMLE (default SL library)\n1.26\n1.12-1.42\n\n\n\n\n\n\n\n\n\nForest Plot of Odds Ratios",
    "crumbs": [
      "ML in causal inference",
      "Comparing results"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html",
    "href": "machinelearningCausalQ.html",
    "title": "Quiz (C)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html#live-quiz",
    "href": "machinelearningCausalQ.html#live-quiz",
    "title": "Quiz (C)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalQ.html#download-quiz",
    "href": "machinelearningCausalQ.html#download-quiz",
    "title": "Quiz (C)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "ML in causal inference",
      "Quiz (C)"
    ]
  },
  {
    "objectID": "machinelearningCausalF.html",
    "href": "machinelearningCausalF.html",
    "title": "R functions (C)",
    "section": "",
    "text": "The list of new R functions introduced in this Machine learning in causal inference lab component are below:\n\n\n\n\n\nFunction.name\nPackage.name\nUse\n\n\n\npublish\nPublish\nTo publish regression models\n\n\nmedian\nstats\nTo calculate the median of a numeric vector\n\n\nSL.mean\nSuperLearner\nSuperLearner wrapper for the mean learner\n\n\nSL.glm\nSuperLearner\nSuperLearner wrapper for the generalized linear model learner\n\n\nSL.glmnet\nSuperLearner\nSuperLearner wrapper for the generalized linear model with elastic net penalty learner\n\n\nSL.xgboost\nSuperLearner\nSuperLearner wrapper for the extreme gradient boosting learner\n\n\nSL.randomForest\nSuperLearner\nSuperLearner wrapper for the random forest learner\n\n\nSL.svm\nSuperLearner\nSuperLearner wrapper for the support vector machine (SVM) learner\n\n\nCreateTableOne\ntableone\nTo create a summary table for a dataset\n\n\ntmle\ntmle\nTo run Targeted Maximum Likelihood Estimation (TMLE) for causal inference\n\n\ntmle.SL.dbarts2\ntmle\nSuperLearner wrapper for the Bayesian Additive Regression Trees (BART) learner",
    "crumbs": [
      "ML in causal inference",
      "R functions (C)"
    ]
  },
  {
    "objectID": "nonbinary.html",
    "href": "nonbinary.html",
    "title": "Complex outcomes",
    "section": "",
    "text": "Background\nThis chapter offers tutorials on handling non-binary outcomes in data analysis. The first tutorial, on Polytomous and Ordinal outcomes, delves into multinomial and ordinal logistic regressions, teaching methods to analyze datasets with more than two categorical outcomes and how to assess the fit of ordinal logistic models. The second tutorial introduces Survival Analysis, guiding readers through the components of survival data, right censoring, the Kaplan-Meier method, the Cox regression model, and the complexities of analyzing time-dependent covariates and survey data. Lastly, the tutorial on Poisson and Negative Binomial focuses on regressions related to count data, showcasing both regular and survey-weighted methods, and highlights statistical techniques to understand factors influencing certain variables.",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary.html#background",
    "href": "nonbinary.html#background",
    "title": "Complex outcomes",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary.html#overview-of-tutorials",
    "href": "nonbinary.html#overview-of-tutorials",
    "title": "Complex outcomes",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily focused on binary or continuous outcomes. In this chapter, we will discuss about categorical [outcome variable is categorical with more than two categories (i.e., multicategory)], survival [time-to-event outcomes and aims to model the time until an event happens, accounting for censored data (observations where the event has not occurred by the end of the study)] and count data [non-negative integers (i.e., 0, 1, 2, 3, …)] outcomes.\n\nPolytomous and ordinal\nThe tutorial covers methods for analyzing polytomous and ordinal outcomes in R. Initially, it introduces a function to exclude invalid responses from datasets. The data is loaded, and its structure is checked. The tutorial delves into the multinomial logistic regression, where tables are created to explore data characteristics. Two models are fitted: one for unweighted data and another for survey-weighted data. It then transitions into ordinal regression. Here, outcomes are ordered, and ordinal logistic models are fitted, again for both unweighted and survey-weighted data. The final part of the tutorial assesses the fit of the ordinal logistic model using various variables.\n\n\nSurvival analysis\nThe tutorial provides an in-depth understanding of survival analysis. It starts by introducing the lung dataset which contains various attributes related to patients’ health. The concept of censoring, specifically right censoring, is detailed, which happens when a subject leaves the study without experiencing the event of interest. The tutorial delves into the components of survival data, introducing the event indicator, survival function, and survival probability. It then transitions to practical applications, demonstrating how to create survival objects, estimate survival curves using the Kaplan-Meier method, and visualize these curves. There’s also an emphasis on understanding and estimating median survival time and comparing survival times between different groups using the log-rank test. The Cox regression model, which is pivotal in survival analysis, is elaborated, along with the idea of hazard ratios and assessing proportional hazards. Time-dependent covariates, which account for variables that may change over time, are also touched upon. Towards the end, the tutorial addresses how to perform survival analysis on complex survey data, covering design creation, Kaplan-Meier plotting, and the Cox Proportional Hazards model in the context of these surveys.\n\n\nSurvival analysis in NHANES\nThe tutorial demonstrates an analysis of NHANES data linked with mortality data, and conducts a survival analysis.\n\n\nPoisson and negative binomial\nThe tutorial provides a comprehensive guide on statistical analysis techniques related to Poisson and negative binomial regressions. Initially, it prepares the data by converting them into appropriate formats. A weighted summary of the dataset is then generated, emphasizing the distribution of a variable. The tutorial proceeds to demonstrate both regular and survey-weighted Poisson regressions, focusing on the relationship between fruit consumption and various factors. Finally, it explores negative binomial regressions and their survey-weighted counterparts.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Complex outcomes"
    ]
  },
  {
    "objectID": "nonbinary0.html",
    "href": "nonbinary0.html",
    "title": "Concepts (N)",
    "section": "",
    "text": "Complex outcomes beyond binary\nIn this section, we explore regressions for outcomes beyond binary:",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#complex-outcomes-beyond-binary",
    "href": "nonbinary0.html#complex-outcomes-beyond-binary",
    "title": "Concepts (N)",
    "section": "",
    "text": "Polytomous regression\nSurvival analysis\nPoisson regression",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#reading-list",
    "href": "nonbinary0.html#reading-list",
    "title": "Concepts (N)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Hosmer, Lemeshow, and Sturdivant 2013; Kleinbaum and Klein 2011; Coxe, West, and Aiken 2009)",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#video-lessons",
    "href": "nonbinary0.html#video-lessons",
    "title": "Concepts (N)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nPolytomous regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson regression",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#video-lesson-slides",
    "href": "nonbinary0.html#video-lesson-slides",
    "title": "Concepts (N)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#links",
    "href": "nonbinary0.html#links",
    "title": "Concepts (N)",
    "section": "Links",
    "text": "Links\nPolytomous regression\n\nGoogle Slides\nPDF Slides\n\nSurvival analysis\n\nGoogle Slides\nPDF Slides\n\nPoisson regression\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary0.html#references",
    "href": "nonbinary0.html#references",
    "title": "Concepts (N)",
    "section": "References",
    "text": "References\n\n\n\n\nCoxe, Stefany, Stephen G. West, and Leona S. Aiken. 2009. “The Analysis of Count Data: A Gentle Introduction to Poisson Regression and Its Alternatives.” Journal of Personality Assessment 91 (2): 121–36. https://doi.org/10.1080/00223890802634175.\n\n\nHosmer, Jr., David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression, 3rd Edition. Hoboken, NJ: John Wiley & Sons.\n\n\nKleinbaum, David G., and Mitchel Klein. 2011. Survival Analysis: A Self-Learning Text, Third Edition. New York, NY: Springer.",
    "crumbs": [
      "Complex outcomes",
      "Concepts (N)"
    ]
  },
  {
    "objectID": "nonbinary1.html",
    "href": "nonbinary1.html",
    "title": "Polytomous and ordinal",
    "section": "",
    "text": "Let us load the packages:\n\n# Load required packages\nrequire(Publish)\nrequire(survey)\nrequire(svyVGAM)\nrequire(car)\nlibrary(knitr)\n\nIn the code chunk below, we create a function called invalid.exclude. This function can be used to exclude invalid responses from the dataset, where don’t know, refusal, and not stated are considered invalid responses.\n\n# Function to exclude invalid responses\ninvalid.exclude &lt;- function(Data, Var){\n  subset.data &lt;- subset(Data, eval(parse(text = Var)) != \"DON'T KNOW\" & \n                          eval(parse(text = Var)) != \"REFUSAL\" & \n                          eval(parse(text = Var)) != \"NOT STATED\")\n  x1 &lt;- dim(Data)[1]\n  x2 &lt;- dim(subset.data)[1]\n  cat( format(x1-x2, big.mark = \",\"),\n       \"subjects deleted, and current N =\" , format(x2, big.mark = \",\") , \"\\n\")\n  return(subset.data)\n}\n\nData\n\n# Data\nanalytic &lt;- readRDS(\"Data/nonbinary/cmh.Rds\")\nstr(analytic)\n#&gt; 'data.frame':    2628 obs. of  9 variables:\n#&gt;  $ MHcondition       : Factor w/ 3 levels \"Good\",\"Poor or Fair\",..: 2 2 3 3 3 2 3 1 3 3 ...\n#&gt;  $ CommunityBelonging: Factor w/ 4 levels \"SOMEWHAT STRONG\",..: 1 1 3 3 3 2 1 1 3 1 ...\n#&gt;  $ Age               : Factor w/ 6 levels \"15 TO 24 YEARS\",..: 4 1 1 4 3 5 2 5 3 1 ...\n#&gt;  $ Sex               : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 2 2 2 2 2 1 1 2 ...\n#&gt;  $ RaceEthnicity     : Factor w/ 2 levels \"NON-WHITE\",\"WHITE\": 2 1 2 2 1 2 2 2 2 1 ...\n#&gt;  $ MainIncome        : Factor w/ 5 levels \"EI/WORKER'S COMP\",..: 2 2 2 2 2 5 2 2 2 2 ...\n#&gt;  $ ReceivedHelp      : Factor w/ 4 levels \"DON'T KNOW\",\"NO\",..: 4 4 4 2 2 4 2 2 4 2 ...\n#&gt;  $ Weight            : num  678 1298 196 917 2384 ...\n#&gt;  $ Disorder          : Factor w/ 1 level \"YES\": 1 1 1 1 1 1 1 1 1 1 ...\n\nLet us drop invalid responses\n\n# Drop invalid responses\nanalytic &lt;- invalid.exclude(analytic, Var = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \n                                              \"MainIncome\", \"MHcondition\"))\n#&gt; 0 subjects deleted, and current N = 2,628\n\nMultinomial logistic\nUnweighted Tables\nLet us see the summary statistic of the variables, stratified by mental health condition:\n\nrequire(\"tableone\")\n#&gt; Loading required package: tableone\ntab1 &lt;- CreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"),\n                       strata = \"MHcondition\", data = analytic, test = F)\nprint(tab1, showAllLevels = TRUE, smd = T)\n#&gt;                         Stratified by MHcondition\n#&gt;                          level             Good        Poor or Fair\n#&gt;   n                                        885         1002        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   362 (40.9)   288 (28.7) \n#&gt;                          SOMEWHAT WEAK     309 (34.9)   358 (35.7) \n#&gt;                          VERY STRONG        96 (10.8)    74 ( 7.4) \n#&gt;                          VERY WEAK         118 (13.3)   282 (28.1) \n#&gt;   Age (%)                15 TO 24 YEARS    264 (29.8)   191 (19.1) \n#&gt;                          25 TO 34 YEARS    167 (18.9)   141 (14.1) \n#&gt;                          35 TO 44 YEARS    119 (13.4)   185 (18.5) \n#&gt;                          35 TO 54 YEARS    139 (15.7)   220 (22.0) \n#&gt;                          55 TO 64 YEARS    113 (12.8)   198 (19.8) \n#&gt;                          65 years or older  83 ( 9.4)    67 ( 6.7) \n#&gt;   Sex (%)                FEMALE            487 (55.0)   616 (61.5) \n#&gt;                          MALE              398 (45.0)   386 (38.5) \n#&gt;   RaceEthnicity (%)      NON-WHITE         140 (15.8)   184 (18.4) \n#&gt;                          WHITE             745 (84.2)   818 (81.6) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   78 ( 8.8)   195 (19.5) \n#&gt;                          EMPLOYMENT INC.   641 (72.4)   560 (55.9) \n#&gt;                          NOT STATED         23 ( 2.6)    25 ( 2.5) \n#&gt;                          OTHER              36 ( 4.1)    66 ( 6.6) \n#&gt;                          SENIOR BENEFITS   107 (12.1)   156 (15.6) \n#&gt;                         Stratified by MHcondition\n#&gt;                          Very good/excellent SMD   \n#&gt;   n                      741                       \n#&gt;   CommunityBelonging (%) 355 (47.9)           0.425\n#&gt;                          190 (25.6)                \n#&gt;                          116 (15.7)                \n#&gt;                           80 (10.8)                \n#&gt;   Age (%)                285 (38.5)           0.420\n#&gt;                          167 (22.5)                \n#&gt;                           89 (12.0)                \n#&gt;                           79 (10.7)                \n#&gt;                           68 ( 9.2)                \n#&gt;                           53 ( 7.2)                \n#&gt;   Sex (%)                304 (41.0)           0.277\n#&gt;                          437 (59.0)                \n#&gt;   RaceEthnicity (%)      134 (18.1)           0.045\n#&gt;                          607 (81.9)                \n#&gt;   MainIncome (%)          36 ( 4.9)           0.400\n#&gt;                          598 (80.7)                \n#&gt;                            9 ( 1.2)                \n#&gt;                           22 ( 3.0)                \n#&gt;                           76 (10.3)\n\nMultinomial Model fitting\nBefore fitting the multinomial regression, let us redefine the reference categories of the variables.\n\nanalytic$MHcondition2 &lt;- relevel(analytic$MHcondition, ref = \"Poor or Fair\")\nanalytic$CommunityBelonging2 &lt;- relevel(analytic$CommunityBelonging, ref = \"VERY WEAK\")\nanalytic$Age2 &lt;- relevel(analytic$Age, ref = \"65 years or older\")\nanalytic$Sex2 &lt;- relevel(analytic$Sex, ref = \"FEMALE\")\nanalytic$RaceEthnicity2 &lt;- relevel(analytic$RaceEthnicity, ref = \"NON-WHITE\")\n\nNow we will fit the multinomial logistic regression model using the multinom function from the nnet package:\n\nrequire(nnet)\n#&gt; Loading required package: nnet\nfit4 &lt;- multinom(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                    data = analytic)\n#&gt; # weights:  48 (30 variable)\n#&gt; initial  value 2887.153095 \n#&gt; iter  10 value 2640.386436\n#&gt; iter  20 value 2625.127331\n#&gt; iter  30 value 2622.465409\n#&gt; final  value 2622.328038 \n#&gt; converged\nkable(round(exp(cbind(coef(fit4), confint(fit4))),2))\n#&gt; Warning in cbind(coef(fit4), confint(fit4)): number of rows of result is not a\n#&gt; multiple of vector length (arg 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nCommunityBelonging2SOMEWHAT STRONG\nCommunityBelonging2SOMEWHAT WEAK\nCommunityBelonging2VERY STRONG\nSex2MALE\nAge215 TO 24 YEARS\nAge225 TO 34 YEARS\nAge235 TO 44 YEARS\nAge235 TO 54 YEARS\nAge255 TO 64 YEARS\nRaceEthnicity2WHITE\nMainIncomeEMPLOYMENT INC.\nMainIncomeNOT STATED\nMainIncomeOTHER\nMainIncomeSENIOR BENEFITS\n\n\n\n\nGood\n0.38\n2.72\n1.84\n3.08\n1.29\n0.70\n0.62\n0.32\n0.37\n0.36\n1.25\n2.29\n1.57\n1.14\n1.11\n0.21\n\n\nVery good/excellent\n0.09\n3.92\n1.64\n5.65\n2.19\n1.27\n1.08\n0.39\n0.37\n0.37\n1.15\n3.89\n1.25\n1.34\n2.11\n2.07\n\n\n\n\n\nMultinomial logistic for complex survey\nSurvey-weighted Tables\nNow, we will set up the survey design with the survey weights and then see design-adjusted summary statistics.\n\nrequire(survey)\n# Design\nsvy.analytic &lt;- svydesign(ids = ~ 1, weights = ~ Weight, data = analytic)\n\n# Table 1\ntab1a &lt;- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                  data = svy.analytic)\nprint(tab1a, showAllLevels = TRUE)\n#&gt;                         \n#&gt;                          level             Overall          \n#&gt;   n                                        2734564.0        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   1015381.0 (37.1) \n#&gt;                          SOMEWHAT WEAK      948838.7 (34.7) \n#&gt;                          VERY STRONG        297141.6 (10.9) \n#&gt;                          VERY WEAK          473202.7 (17.3) \n#&gt;   Age (%)                15 TO 24 YEARS     795568.6 (29.1) \n#&gt;                          25 TO 34 YEARS     564720.6 (20.7) \n#&gt;                          35 TO 44 YEARS     457821.1 (16.7) \n#&gt;                          35 TO 54 YEARS     447324.8 (16.4) \n#&gt;                          55 TO 64 YEARS     319374.3 (11.7) \n#&gt;                          65 years or older  149754.6 ( 5.5) \n#&gt;   Sex (%)                FEMALE            1312605.0 (48.0) \n#&gt;                          MALE              1421958.9 (52.0) \n#&gt;   RaceEthnicity (%)      NON-WHITE          565784.5 (20.7) \n#&gt;                          WHITE             2168779.4 (79.3) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   245758.5 ( 9.0) \n#&gt;                          EMPLOYMENT INC.   2059689.1 (75.3) \n#&gt;                          NOT STATED          60101.5 ( 2.2) \n#&gt;                          OTHER              116260.6 ( 4.3) \n#&gt;                          SENIOR BENEFITS    252754.4 ( 9.2)\n\n# table 1 stratified by MHcondition\ntab1b &lt;- svyCreateTableOne(vars = c(\"CommunityBelonging\", \"Age\", \"Sex\", \"RaceEthnicity\", \"MainIncome\"), \n                           strata = \"MHcondition\", data = svy.analytic)\nprint(tab1b, showAllLevels = TRUE)\n#&gt;                         Stratified by MHcondition\n#&gt;                          level             Good             Poor or Fair    \n#&gt;   n                                        949208.1         973112.1        \n#&gt;   CommunityBelonging (%) SOMEWHAT STRONG   346216.2 (36.5)  273163.9 (28.1) \n#&gt;                          SOMEWHAT WEAK     378431.4 (39.9)  365731.0 (37.6) \n#&gt;                          VERY STRONG        97528.8 (10.3)   69438.0 ( 7.1) \n#&gt;                          VERY WEAK         127031.7 (13.4)  264779.1 (27.2) \n#&gt;   Age (%)                15 TO 24 YEARS    286103.0 (30.1)  187610.9 (19.3) \n#&gt;                          25 TO 34 YEARS    189084.8 (19.9)  184528.6 (19.0) \n#&gt;                          35 TO 44 YEARS    140796.4 (14.8)  199172.7 (20.5) \n#&gt;                          35 TO 54 YEARS    171968.1 (18.1)  194689.1 (20.0) \n#&gt;                          55 TO 64 YEARS    109114.5 (11.5)  148432.2 (15.3) \n#&gt;                          65 years or older  52141.3 ( 5.5)   58678.6 ( 6.0) \n#&gt;   Sex (%)                FEMALE            436830.4 (46.0)  582311.1 (59.8) \n#&gt;                          MALE              512377.7 (54.0)  390801.0 (40.2) \n#&gt;   RaceEthnicity (%)      NON-WHITE         167887.9 (17.7)  220525.5 (22.7) \n#&gt;                          WHITE             781320.2 (82.3)  752586.6 (77.3) \n#&gt;   MainIncome (%)         EI/WORKER'S COMP   66003.9 ( 7.0)  151095.4 (15.5) \n#&gt;                          EMPLOYMENT INC.   752208.0 (79.2)  608703.4 (62.6) \n#&gt;                          NOT STATED         23589.0 ( 2.5)   28371.0 ( 2.9) \n#&gt;                          OTHER              32247.9 ( 3.4)   68453.1 ( 7.0) \n#&gt;                          SENIOR BENEFITS    75159.3 ( 7.9)  116489.2 (12.0) \n#&gt;                         Stratified by MHcondition\n#&gt;                          Very good/excellent p      test\n#&gt;   n                      812243.7                       \n#&gt;   CommunityBelonging (%) 396000.8 (48.8)     &lt;0.001     \n#&gt;                          204676.3 (25.2)                \n#&gt;                          130174.7 (16.0)                \n#&gt;                           81391.9 (10.0)                \n#&gt;   Age (%)                321854.6 (39.6)     &lt;0.001     \n#&gt;                          191107.2 (23.5)                \n#&gt;                          117852.1 (14.5)                \n#&gt;                           80667.6 ( 9.9)                \n#&gt;                           61827.6 ( 7.6)                \n#&gt;                           38934.6 ( 4.8)                \n#&gt;   Sex (%)                293463.5 (36.1)     &lt;0.001     \n#&gt;                          518780.2 (63.9)                \n#&gt;   RaceEthnicity (%)      177371.1 (21.8)      0.219     \n#&gt;                          634872.7 (78.2)                \n#&gt;   MainIncome (%)          28659.2 ( 3.5)     &lt;0.001     \n#&gt;                          698777.7 (86.0)                \n#&gt;                            8141.5 ( 1.0)                \n#&gt;                           15559.6 ( 1.9)                \n#&gt;                           61105.8 ( 7.5)\n\nSetting up the design\nLet us redefine the reference categories within the survey design and convert the design to use replicate weights.\n\nw.design &lt;- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design &lt;- update(w.design , MHcondition2 = relevel(MHcondition, ref = \"Poor or Fair\"),\n                  CommunityBelonging2 = relevel(CommunityBelonging, ref = \"VERY WEAK\"),\n                  Age2 = relevel(Age, ref = \"65 years or older\"),\n                  Sex2 = relevel(Sex, ref = \"FEMALE\"),\n                  RaceEthnicity2 = relevel(RaceEthnicity, ref = \"NON-WHITE\"))\n\n# Convert a survey design to use replicate weights\nw.design2 &lt;- as.svrepdesign(w.design, type = \"bootstrap\" , replicates = 50)\n\nMultinomial Model fitting\nNow, we will fit the design-adjusted multinomial logistic regression:\n\nrequire(svyVGAM)\nfit5 &lt;- svy_vglm(MHcondition2 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome,\n                    design = w.design2, family = multinomial)\n#&gt; Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :\n#&gt; iterations terminated because half-step sizes are very small\n#&gt; Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\n#&gt; quantities such as z, residuals, SEs may be inaccurate due to convergence at a\n#&gt; half-step\n\n\nkable(round(exp(cbind(coef(fit5), confint(fit5))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n(Intercept):1\n16.57\n6.54\n41.97\n\n\n(Intercept):2\n3.24\n1.32\n7.95\n\n\nCommunityBelonging2SOMEWHAT STRONG:1\n0.22\n0.14\n0.35\n\n\nCommunityBelonging2SOMEWHAT STRONG:2\n0.56\n0.34\n0.92\n\n\nCommunityBelonging2SOMEWHAT WEAK:1\n0.58\n0.36\n0.91\n\n\nCommunityBelonging2SOMEWHAT WEAK:2\n1.18\n0.69\n1.99\n\n\nCommunityBelonging2VERY STRONG:1\n0.15\n0.09\n0.27\n\n\nCommunityBelonging2VERY STRONG:2\n0.46\n0.26\n0.83\n\n\nSex2MALE:1\n0.39\n0.27\n0.55\n\n\nSex2MALE:2\n0.68\n0.51\n0.91\n\n\nAge215 TO 24 YEARS:1\n0.56\n0.26\n1.18\n\n\nAge215 TO 24 YEARS:2\n0.65\n0.35\n1.19\n\n\nAge225 TO 34 YEARS:1\n0.83\n0.40\n1.72\n\n\nAge225 TO 34 YEARS:2\n0.68\n0.34\n1.38\n\n\nAge235 TO 44 YEARS:1\n1.74\n0.80\n3.75\n\n\nAge235 TO 44 YEARS:2\n0.91\n0.44\n1.87\n\n\nAge235 TO 54 YEARS:1\n2.02\n0.93\n4.36\n\n\nAge235 TO 54 YEARS:2\n1.48\n0.71\n3.05\n\n\nAge255 TO 64 YEARS:1\n1.91\n0.89\n4.10\n\n\nAge255 TO 64 YEARS:2\n1.33\n0.63\n2.79\n\n\nRaceEthnicity2WHITE:1\n0.91\n0.64\n1.28\n\n\nRaceEthnicity2WHITE:2\n1.22\n0.90\n1.67\n\n\nMainIncomeEMPLOYMENT INC.:1\n0.25\n0.14\n0.44\n\n\nMainIncomeEMPLOYMENT INC.:2\n0.58\n0.32\n1.07\n\n\nMainIncomeNOT STATED:1\n0.91\n0.21\n4.04\n\n\nMainIncomeNOT STATED:2\n1.37\n0.31\n6.09\n\n\nMainIncomeOTHER:1\n1.43\n0.56\n3.70\n\n\nMainIncomeOTHER:2\n1.30\n0.57\n3.00\n\n\nMainIncomeSENIOR BENEFITS:1\n0.39\n0.19\n0.79\n\n\nMainIncomeSENIOR BENEFITS:2\n0.49\n0.23\n1.04\n\n\n\n\n\nOrdinal Regression\nOrdering outcome\nLet’s define MHcondition as an ordinal outcome. We can do it by using ordered = TRUE in the factor function.\n\n# Ordinal outcome\nanalytic$MHcondition3 &lt;- factor(analytic$MHcondition, \n                               levels = c(\"Poor or Fair\", \"Good\", \"Very good or excellent\"), \n                      ordered = TRUE)\n\nOrdinal logistic\nLet’s fit the ordinal logistic using the polr function:\n\nrequire(MASS)\n#&gt; Loading required package: MASS\nfit5o1 &lt;- polr(MHcondition3 ~CommunityBelonging2 + \n                  Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                data=analytic)\nkable(round(exp(cbind(coef(fit5o1), confint(fit5o1))),2))\n#&gt; Waiting for profiling to be done...\n#&gt; \n#&gt; Re-fitting to get Hessian\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.69\n2.05\n3.55\n\n\nCommunityBelonging2SOMEWHAT WEAK\n1.83\n1.40\n2.41\n\n\nCommunityBelonging2VERY STRONG\n3.15\n2.15\n4.66\n\n\nSex2MALE\n1.30\n1.07\n1.58\n\n\nAge215 TO 24 YEARS\n0.69\n0.42\n1.11\n\n\nAge225 TO 34 YEARS\n0.60\n0.36\n0.98\n\n\nAge235 TO 44 YEARS\n0.32\n0.19\n0.53\n\n\nAge235 TO 54 YEARS\n0.37\n0.23\n0.59\n\n\nAge255 TO 64 YEARS\n0.36\n0.22\n0.56\n\n\nRaceEthnicity2WHITE\n1.26\n0.97\n1.63\n\n\nMainIncomeEMPLOYMENT INC.\n2.26\n1.68\n3.06\n\n\nMainIncomeNOT STATED\n1.53\n0.79\n2.94\n\n\nMainIncomeOTHER\n1.15\n0.70\n1.90\n\n\nMainIncomeSENIOR BENEFITS\n1.10\n0.70\n1.70\n\n\n\n\n\nOrdinal logistic for complex survey\nThe same as before, we can set up the design, relevel variables within the design, define MHcondition as an ordinal variable, and then fit the design-adjusted ordinal logistic regression.\n\nw.design &lt;- svydesign(id=~1, weights=~Weight, data=analytic)\nw.design&lt;-update(w.design , \n                  CommunityBelonging2=relevel(CommunityBelonging, ref=\"VERY WEAK\"),\n                  Age2=relevel(Age, ref=\"65 years or older\"),\n                  Sex2=relevel(Sex, ref=\"FEMALE\"),\n                  RaceEthnicity2=relevel(RaceEthnicity, ref=\"NON-WHITE\"),\n                  MHcondition3 = factor(MHcondition, levels=c(\"Poor or Fair\", \"Good\", \n                                                              \"Very good or excellent\"), \n                                        ordered=TRUE))\n\n# Design-adjusted Ordinal logistic\nfit5o &lt;- svyolr(MHcondition3 ~CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + MainIncome, \n                design=w.design)\nkable(round(exp(cbind(coef(fit5o), confint(fit5o))),2))\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\nCommunityBelonging2SOMEWHAT STRONG\n2.49\n1.65\n3.75\n\n\nCommunityBelonging2SOMEWHAT WEAK\n2.04\n1.37\n3.05\n\n\nCommunityBelonging2VERY STRONG\n3.02\n1.74\n5.26\n\n\nSex2MALE\n1.76\n1.30\n2.38\n\n\nAge215 TO 24 YEARS\n1.13\n0.52\n2.44\n\n\nAge225 TO 34 YEARS\n0.77\n0.34\n1.74\n\n\nAge235 TO 44 YEARS\n0.52\n0.24\n1.13\n\n\nAge235 TO 54 YEARS\n0.70\n0.32\n1.52\n\n\nAge255 TO 64 YEARS\n0.69\n0.32\n1.46\n\n\nRaceEthnicity2WHITE\n1.31\n0.90\n1.91\n\n\nMainIncomeEMPLOYMENT INC.\n2.37\n1.53\n3.67\n\n\nMainIncomeNOT STATED\n1.42\n0.52\n3.86\n\n\nMainIncomeOTHER\n0.88\n0.39\n2.00\n\n\nMainIncomeSENIOR BENEFITS\n1.26\n0.66\n2.41\n\n\nPoor or Fair|Good\n4.79\n1.92\n11.91\n\n\nGood|Very good or excellent\n2636603.90\n996905.04\n6973262.11\n\n\n\n\n\nAssessing model fit\nWe can use the regTermTest function from the survey package to do the Wald test of a regression coefficient.\n\nregTermTest(fit5o, ~CommunityBelonging2 , df = Inf) \n#&gt; Wald test for CommunityBelonging2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  24.53941  on  3  df: p= 1.9272e-05\nregTermTest(fit5o, ~Age2 , df = Inf) \n#&gt; Wald test for Age2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  14.00491  on  5  df: p= 0.015578\nregTermTest(fit5o, ~Sex2 , df = Inf)\n#&gt; Wald test for Sex2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  13.46032  on  1  df: p= 0.00024366\nregTermTest(fit5o, ~RaceEthnicity2 , df = Inf)\n#&gt; Wald test for RaceEthnicity2\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  2.002794  on  1  df: p= 0.15701\nregTermTest(fit5o, ~MainIncome , df = Inf)\n#&gt; Wald test for MainIncome\n#&gt;  in svyolr(MHcondition3 ~ CommunityBelonging2 + Sex2 + Age2 + RaceEthnicity2 + \n#&gt;     MainIncome, design = w.design)\n#&gt; Chisq =  22.13656  on  4  df: p= 0.00018826\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Polytomous and ordinal"
    ]
  },
  {
    "objectID": "nonbinary2.html",
    "href": "nonbinary2.html",
    "title": "Survival analysis",
    "section": "",
    "text": "The lung dataset\nlibrary(survival)\nhead(lung)\n\n\n  \n\n\nkable(head(lung))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninst\ntime\nstatus\nage\nsex\nph.ecog\nph.karno\npat.karno\nmeal.cal\nwt.loss\n\n\n\n3\n306\n2\n74\n1\n1\n90\n100\n1175\nNA\n\n\n3\n455\n2\n68\n1\n0\n90\n90\n1225\n15\n\n\n3\n1010\n1\n56\n1\n0\n90\n90\nNA\n15\n\n\n5\n210\n2\n57\n1\n1\n90\n60\n1150\n11\n\n\n1\n883\n2\n60\n1\n0\n100\n90\nNA\n0\n\n\n12\n1022\n1\n74\n1\n1\n50\n80\n513\n0\n\n\n\n\n# kable(lung)",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis"
    ]
  },
  {
    "objectID": "nonbinary2.html#data-and-variables",
    "href": "nonbinary2.html#data-and-variables",
    "title": "Survival analysis",
    "section": "Data and Variables",
    "text": "Data and Variables\n\nload(\"Data/nonbinary/nh_99-06.Rdata\") # \nanalytic.miss &lt;- as.data.frame(MainTable[c(\"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\", \"PERMTH_INT\", \"PERMTH_EXM\", \"MORTSTAT\", \"female\", \"RIDAGEYR\", \"white\")])\n\n\nMORTSTAT: Final Mortality Status\n\n0 Assumed alive\n1 Assumed deceased\nBlank Ineligible for mortality follow-up or under age 17\n\n\nPERMTH_EXM: Person Months of Follow-up from MEC/Home Exam Date\n\n0 - 217\nBlank Ineligible\n\n\nPERMTH_INT: Person Months of Follow-up from Interview Date\n\nData issues\n\nwith(analytic.miss[1:30,], \n     Surv(PERMTH_EXM, MORTSTAT))\n#&gt;  [1] NA? 90+ NA? NA? 74+ 86+ 76+ NA? NA? 79+ NA? 82+ 16  85+ 92+ 62  NA? NA? NA?\n#&gt; [20] 86+ 87+ NA? NA? 72+ 84+ NA? 85+ 91+ 26  NA?\nsummary(analytic.miss$WTMEC2YR)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0    6365   15572   27245   38896  261361\n# avoiding 0 weight issues\nanalytic.miss$WTMEC2YR[analytic.miss$WTMEC2YR == 0] &lt;- 0.001\nrequire(DataExplorer)\n#&gt; Loading required package: DataExplorer\nplot_missing(analytic.miss)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the DataExplorer package.\n#&gt;   Please report the issue at\n#&gt;   &lt;https://github.com/boxuancui/DataExplorer/issues&gt;.\n\n\n\n\n\n\n\nDesign creation\n\nanalytic.miss$ID &lt;- 1:nrow(analytic.miss)\nanalytic.miss$miss &lt;- 0\nanalytic.cc &lt;- as.data.frame(na.omit(analytic.miss))\ndim(analytic.cc)\n#&gt; [1] 10557    11\nanalytic.miss$miss[analytic.miss$ID %in% \n                     analytic.cc$ID] &lt;- 0\nw.design0 &lt;- svydesign(id=~SDMVPSU, \n                       strata=~SDMVSTRA, \n                       weights=~WTMEC2YR, \n                       nest=TRUE,\n                       data=analytic.miss)\nw.design &lt;- subset(w.design0, \n                   miss == 0)\nsummary(weights(w.design))\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; 1.000e-03 6.365e+03 1.557e+04 2.725e+04 3.890e+04 2.614e+05\n\nSurvival Analysis within Complex Survey\nKM plot\n\nfit0 &lt;- svykm(Surv(PERMTH_EXM, MORTSTAT) ~ 1, \n              design = w.design)\nplot(fit0)\n\n\n\n\n\n\n\nCox PH\n\nfit &lt;- svycoxph(Surv(PERMTH_EXM, MORTSTAT) ~ \n                  white + female + RIDAGEYR, \n                design = w.design) \npublish(fit)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (117) clusters.\n#&gt; subset(w.design0, miss == 0)\n#&gt;  Variable Units HazardRatio       CI.95   p-value \n#&gt;     white              0.78 [0.66;0.93]   0.00441 \n#&gt;    female              0.61 [0.50;0.75]   &lt; 0.001 \n#&gt;  RIDAGEYR              1.09 [1.08;1.10]   &lt; 0.001\n\nPH assumption\n\ntestPh &lt;- cox.zph(fit) \nprint(testPh)\n#&gt;             chisq df    p\n#&gt; white    7.39e-05  1 0.99\n#&gt; female   2.84e-07  1 1.00\n#&gt; RIDAGEYR 9.16e-05  1 0.99\n#&gt; GLOBAL   1.45e-04  3 1.00\nplot(testPh) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis"
    ]
  },
  {
    "objectID": "nonbinary2a.html",
    "href": "nonbinary2a.html",
    "title": "Survival analysis: NHANES",
    "section": "",
    "text": "The tutorial demonstrates analyzing complex survey data (NHANES) with mortality as a survival outcome. See the tutorial in the previous chapter on linking public-use US mortality data with the NHANES.\nWe will explore the relationship between caffeine consumption and mortality in adults with diabetes using NHANES 1999-2010 datasets. We will follow the following article by Neves et al. (2018).\nLoad required packages\n\n# Load required packages\nlibrary(tableone)\nlibrary(survival)\nlibrary(Publish)\nlibrary(survey)\nlibrary(DataExplorer)\n\nLoad datasets\nLet us load NHANES 1999-2000, 2001-2002, 2003-2004, 2005-2006, 2007-2008, and 2009-2010 datasets and merged all cycles.\n\n# Load\nload(\"Data/nonbinary/coffee.RData\")\nls()\n#&gt; [1] \"dat.analytic1999\" \"dat.analytic2001\" \"dat.analytic2003\" \"dat.analytic2005\"\n#&gt; [5] \"dat.analytic2007\" \"dat.analytic2009\"\n\n# Merge all cycles\ndat.full &lt;- rbind(dat.analytic1999, dat.analytic2001, dat.analytic2003, \n                  dat.analytic2005, dat.analytic2007, dat.analytic2009)\nhead(dat.full)\n\n\n  \n\n\ndim(dat.full)\n#&gt; [1] 62160    26\n\nThe merged dataset contains 62,160 subjects with 26 relevant variables:\n\nid: Respondent sequence/ID number\nsurvey.weight: Full sample 2 year weights\npsu: Masked pseudo-PSU\nstrata: Masked pseudo-stratum\ncaff: Caffeine (exposure variable)\nstime: Follow-up time (time from interview date to death or censoring)\nstatus: Mortality status\nsex: Sex\nage: Age in years\nrace: Race/ethnicity\nincome: Annual household income\nsmoking: Smoking status\ndiabetic.nephropathy: Diabetic nephropathy (no data)\nbmi.cat: BMI - categorical\neducation: Education level\ncarbohyd: Carbohydrate in gm\nalcohol: Alcohol consumption\ndiab.years: Years since diabetes\nhtn: Hypertension\ndaib.retino: Diabetes retinopathy\nmacrovascular: Macrovascular complications\ninsulin: Insulin\nsurvey.cycle: Survey cycle\nphysical.activity: Physical activity\ndiabetes: Diabetes status\ncal.total: Total calories in kcal\nData pre-processing\nEligibility criteria\nThe authors considered adults aged 18 years or more, only diabetic, and total calories between 500 and 3500.\n\n# Total samples in the merged dataset\nnrow(dat.full) # N = 62,160\n#&gt; [1] 62160\n\n# Age &gt;= 18 years\ndat2 &lt;- subset(dat.full, age &gt;= 18) \nnrow(dat2) # N = 35,379\n#&gt; [1] 35379\n\n# With diabetes\ndat3 &lt;- subset(dat2, diabetes == \"Yes\") \nnrow(dat3) # N = 4,687 - numbers don't match with the paper (N = 4,544)\n#&gt; [1] 4687\n\n# Implausible alimentary reports\ndat4 &lt;- subset(dat3, cal.total &gt;= 500 & cal.total &lt;= 3500) \nnrow(dat4) # N = 4,083 - numbers don't match with the paper (N = 3,948)\n#&gt; [1] 4083\n\nComplete case data\nLet us drop missing values in the exposure and outcome:\n\n# Drop missing exposure and outcome\ndat &lt;- dat4[complete.cases(dat4$id),]\ndat &lt;- dat[complete.cases(dat$caff),]\ndat &lt;- dat[complete.cases(dat$status),]\ndat &lt;- dat[complete.cases(dat$stime),] # N = 4,080\ndim(dat)\n#&gt; [1] 4080   26\n\n# Missing plot\nplot_missing(dat)\n\n\n\n\n\n\n\nNow, let us drop variables with high missingness for this exercise. As explained in the Missing data analysis chapter, a better approach could be imputing missing values under the missing at random assumption.\n\n# Drop variables with high missingness\ndat$diabetic.nephropathy &lt;- dat$diab.years &lt;- dat$daib.retino &lt;- dat$income &lt;- NULL\n\n# Complete case data\ndat &lt;- na.omit(dat)\ndim(dat) # N = 3,780\n#&gt; [1] 3780   22\n\nTable 1\nNow, let us create Table 1 stratified by coffee consumption (exposure), separately for males and females, as done in the article.\n\nvars &lt;- c(\"age\", \"race\", \"education\", \"smoking\", \"alcohol\", \"carbohyd\", \"physical.activity\",\n          \"bmi.cat\", \"htn\", \"macrovascular\", \"insulin\", \"survey.cycle\")\n\ntab1a &lt;- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Female\",], \n                        test = F)\ntab1b &lt;- CreateTableOne(vars = vars, strata = \"caff\", data = dat[dat$sex==\"Male\",], \n                        test = F)\n\ntab1 &lt;- list(Female = tab1a, Male = tab1b)\nprint(tab1, showAllLevels = T, smd = T)\n#&gt; $Female\n#&gt;                        Stratified by caff\n#&gt;                         level                     No consumption &lt;100 mg/day   \n#&gt;   n                                                  203            916        \n#&gt;   age (mean (SD))                                  59.33 (15.14)  62.92 (14.02)\n#&gt;   race (%)              Non-Hispanic White            38 (18.7)     286 (31.2) \n#&gt;                         Non-Hispanic Black            96 (47.3)     284 (31.0) \n#&gt;                         Mexican American              47 (23.2)     228 (24.9) \n#&gt;                         Other Hispanic                10 ( 4.9)      80 ( 8.7) \n#&gt;                         Other race                    12 ( 5.9)      38 ( 4.1) \n#&gt;   education (%)         Less than 9th grade           58 (28.6)     238 (26.0) \n#&gt;                         9-11th grade                  48 (23.6)     182 (19.9) \n#&gt;                         High school grade             43 (21.2)     224 (24.5) \n#&gt;                         Some college                  39 (19.2)     209 (22.8) \n#&gt;                         College graduate or above     15 ( 7.4)      63 ( 6.9) \n#&gt;   smoking (%)           Never smoker                 143 (70.4)     598 (65.3) \n#&gt;                         Current smoker                24 (11.8)     127 (13.9) \n#&gt;                         Former smoker                 36 (17.7)     191 (20.9) \n#&gt;   alcohol (%)           No consumption               192 (94.6)     826 (90.2) \n#&gt;                         &lt;20 grams/day                  4 ( 2.0)      69 ( 7.5) \n#&gt;                         20+ grams/day                  7 ( 3.4)      21 ( 2.3) \n#&gt;   carbohyd (mean (SD))                            176.17 (73.88) 190.64 (73.34)\n#&gt;   physical.activity (%) Low                           95 (46.8)     393 (42.9) \n#&gt;                         Intermediate                  59 (29.1)     309 (33.7) \n#&gt;                         High                          49 (24.1)     214 (23.4) \n#&gt;   bmi.cat (%)           &lt;20.0                          4 ( 2.0)       6 ( 0.7) \n#&gt;                         20.0 to &lt;25.0                 24 (11.8)     107 (11.7) \n#&gt;                         25.0 to &lt;30.0                 35 (17.2)     254 (27.7) \n#&gt;                         30.0 to &lt;35.0                 45 (22.2)     263 (28.7) \n#&gt;                         35.0 to &lt;40.0                 47 (23.2)     138 (15.1) \n#&gt;                         40.0+                         48 (23.6)     148 (16.2) \n#&gt;   htn (%)               No                            58 (28.6)     273 (29.8) \n#&gt;                         Yes                          145 (71.4)     643 (70.2) \n#&gt;   macrovascular (%)     No                           166 (81.8)     758 (82.8) \n#&gt;                         Yes                           37 (18.2)     158 (17.2) \n#&gt;   insulin (%)           No                           156 (76.8)     755 (82.4) \n#&gt;                         Yes                           47 (23.2)     161 (17.6) \n#&gt;   survey.cycle (%)      1999-00                       45 (22.2)     111 (12.1) \n#&gt;                         2001-02                       45 (22.2)      98 (10.7) \n#&gt;                         2003-04                       26 (12.8)     127 (13.9) \n#&gt;                         2005-06                       20 ( 9.9)     154 (16.8) \n#&gt;                         2007-08                       37 (18.2)     198 (21.6) \n#&gt;                         2009-10                       30 (14.8)     228 (24.9) \n#&gt;                        Stratified by caff\n#&gt;                         100-200 mg/day 200+ mg/day    SMD   \n#&gt;   n                        424            332               \n#&gt;   age (mean (SD))        62.04 (13.75)  60.19 (13.10)  0.150\n#&gt;   race (%)                 154 (36.3)     189 (56.9)   0.527\n#&gt;                            108 (25.5)      40 (12.0)        \n#&gt;                            111 (26.2)      69 (20.8)        \n#&gt;                             30 ( 7.1)      20 ( 6.0)        \n#&gt;                             21 ( 5.0)      14 ( 4.2)        \n#&gt;   education (%)             79 (18.6)      56 (16.9)   0.203\n#&gt;                             89 (21.0)      76 (22.9)        \n#&gt;                            109 (25.7)      81 (24.4)        \n#&gt;                            109 (25.7)      89 (26.8)        \n#&gt;                             38 ( 9.0)      30 ( 9.0)        \n#&gt;   smoking (%)              238 (56.1)     136 (41.0)   0.377\n#&gt;                             81 (19.1)     119 (35.8)        \n#&gt;                            105 (24.8)      77 (23.2)        \n#&gt;   alcohol (%)              370 (87.3)     283 (85.2)   0.211\n#&gt;                             38 ( 9.0)      35 (10.5)        \n#&gt;                             16 ( 3.8)      14 ( 4.2)        \n#&gt;   carbohyd (mean (SD))  193.79 (72.18) 203.36 (76.05)  0.190\n#&gt;   physical.activity (%)    179 (42.2)     148 (44.6)   0.093\n#&gt;                            151 (35.6)     117 (35.2)        \n#&gt;                             94 (22.2)      67 (20.2)        \n#&gt;   bmi.cat (%)                7 ( 1.7)       4 ( 1.2)   0.250\n#&gt;                             40 ( 9.4)      44 (13.3)        \n#&gt;                            109 (25.7)      80 (24.1)        \n#&gt;                            133 (31.4)      82 (24.7)        \n#&gt;                             73 (17.2)      63 (19.0)        \n#&gt;                             62 (14.6)      59 (17.8)        \n#&gt;   htn (%)                  123 (29.0)     110 (33.1)   0.052\n#&gt;                            301 (71.0)     222 (66.9)        \n#&gt;   macrovascular (%)        352 (83.0)     266 (80.1)   0.042\n#&gt;                             72 (17.0)      66 (19.9)        \n#&gt;   insulin (%)              340 (80.2)     252 (75.9)   0.094\n#&gt;                             84 (19.8)      80 (24.1)        \n#&gt;   survey.cycle (%)          42 ( 9.9)      46 (13.9)   0.305\n#&gt;                             53 (12.5)      48 (14.5)        \n#&gt;                             63 (14.9)      46 (13.9)        \n#&gt;                             59 (13.9)      43 (13.0)        \n#&gt;                            111 (26.2)      82 (24.7)        \n#&gt;                             96 (22.6)      67 (20.2)        \n#&gt; \n#&gt; $Male\n#&gt;                        Stratified by caff\n#&gt;                         level                     No consumption &lt;100 mg/day   \n#&gt;   n                                                  168            707        \n#&gt;   age (mean (SD))                                  60.95 (12.68)  62.86 (12.88)\n#&gt;   race (%)              Non-Hispanic White            35 (20.8)     238 (33.7) \n#&gt;                         Non-Hispanic Black            78 (46.4)     209 (29.6) \n#&gt;                         Mexican American              37 (22.0)     190 (26.9) \n#&gt;                         Other Hispanic                13 ( 7.7)      48 ( 6.8) \n#&gt;                         Other race                     5 ( 3.0)      22 ( 3.1) \n#&gt;   education (%)         Less than 9th grade           50 (29.8)     194 (27.4) \n#&gt;                         9-11th grade                  41 (24.4)     135 (19.1) \n#&gt;                         High school grade             31 (18.5)     145 (20.5) \n#&gt;                         Some college                  30 (17.9)     136 (19.2) \n#&gt;                         College graduate or above     16 ( 9.5)      97 (13.7) \n#&gt;   smoking (%)           Never smoker                  72 (42.9)     281 (39.7) \n#&gt;                         Current smoker                34 (20.2)     155 (21.9) \n#&gt;                         Former smoker                 62 (36.9)     271 (38.3) \n#&gt;   alcohol (%)           No consumption               131 (78.0)     539 (76.2) \n#&gt;                         &lt;20 grams/day                 16 ( 9.5)      96 (13.6) \n#&gt;                         20+ grams/day                 21 (12.5)      72 (10.2) \n#&gt;   carbohyd (mean (SD))                            195.01 (85.93) 213.00 (80.40)\n#&gt;   physical.activity (%) Low                           63 (37.5)     230 (32.5) \n#&gt;                         Intermediate                  54 (32.1)     284 (40.2) \n#&gt;                         High                          51 (30.4)     193 (27.3) \n#&gt;   bmi.cat (%)           &lt;20.0                          5 ( 3.0)       4 ( 0.6) \n#&gt;                         20.0 to &lt;25.0                 26 (15.5)      95 (13.4) \n#&gt;                         25.0 to &lt;30.0                 53 (31.5)     276 (39.0) \n#&gt;                         30.0 to &lt;35.0                 46 (27.4)     185 (26.2) \n#&gt;                         35.0 to &lt;40.0                 21 (12.5)      95 (13.4) \n#&gt;                         40.0+                         17 (10.1)      52 ( 7.4) \n#&gt;   htn (%)               No                            50 (29.8)     279 (39.5) \n#&gt;                         Yes                          118 (70.2)     428 (60.5) \n#&gt;   macrovascular (%)     No                           120 (71.4)     526 (74.4) \n#&gt;                         Yes                           48 (28.6)     181 (25.6) \n#&gt;   insulin (%)           No                           135 (80.4)     571 (80.8) \n#&gt;                         Yes                           33 (19.6)     136 (19.2) \n#&gt;   survey.cycle (%)      1999-00                       36 (21.4)      72 (10.2) \n#&gt;                         2001-02                       28 (16.7)      92 (13.0) \n#&gt;                         2003-04                       21 (12.5)     104 (14.7) \n#&gt;                         2005-06                       18 (10.7)     115 (16.3) \n#&gt;                         2007-08                       36 (21.4)     168 (23.8) \n#&gt;                         2009-10                       29 (17.3)     156 (22.1) \n#&gt;                        Stratified by caff\n#&gt;                         100-200 mg/day 200+ mg/day    SMD   \n#&gt;   n                        461            569               \n#&gt;   age (mean (SD))        61.67 (13.66)  61.70 (12.40)  0.074\n#&gt;   race (%)                 214 (46.4)     344 (60.5)   0.575\n#&gt;                             92 (20.0)      67 (11.8)        \n#&gt;                             90 (19.5)     115 (20.2)        \n#&gt;                             54 (11.7)      26 ( 4.6)        \n#&gt;                             11 ( 2.4)      17 ( 3.0)        \n#&gt;   education (%)             98 (21.3)      97 (17.0)   0.269\n#&gt;                             79 (17.1)      95 (16.7)        \n#&gt;                            104 (22.6)     125 (22.0)        \n#&gt;                            100 (21.7)     161 (28.3)        \n#&gt;                             80 (17.4)      91 (16.0)        \n#&gt;   smoking (%)              162 (35.1)     153 (26.9)   0.216\n#&gt;                            109 (23.6)     196 (34.4)        \n#&gt;                            190 (41.2)     220 (38.7)        \n#&gt;   alcohol (%)              349 (75.7)     427 (75.0)   0.080\n#&gt;                             63 (13.7)      80 (14.1)        \n#&gt;                             49 (10.6)      62 (10.9)        \n#&gt;   carbohyd (mean (SD))  225.58 (87.51) 237.26 (85.11)  0.273\n#&gt;   physical.activity (%)    153 (33.2)     195 (34.3)   0.099\n#&gt;                            177 (38.4)     199 (35.0)        \n#&gt;                            131 (28.4)     175 (30.8)        \n#&gt;   bmi.cat (%)                4 ( 0.9)       4 ( 0.7)   0.181\n#&gt;                             61 (13.2)      80 (14.1)        \n#&gt;                            169 (36.7)     187 (32.9)        \n#&gt;                            132 (28.6)     146 (25.7)        \n#&gt;                             64 (13.9)      95 (16.7)        \n#&gt;                             31 ( 6.7)      57 (10.0)        \n#&gt;   htn (%)                  190 (41.2)     234 (41.1)   0.126\n#&gt;                            271 (58.8)     335 (58.9)        \n#&gt;   macrovascular (%)        353 (76.6)     412 (72.4)   0.066\n#&gt;                            108 (23.4)     157 (27.6)        \n#&gt;   insulin (%)              375 (81.3)     449 (78.9)   0.032\n#&gt;                             86 (18.7)     120 (21.1)        \n#&gt;   survey.cycle (%)          48 (10.4)      64 (11.2)   0.284\n#&gt;                             59 (12.8)      69 (12.1)        \n#&gt;                             48 (10.4)     106 (18.6)        \n#&gt;                             70 (15.2)      65 (11.4)        \n#&gt;                            112 (24.3)     130 (22.8)        \n#&gt;                            124 (26.9)     135 (23.7)\n\nSurvey design\nThe paper analyzed the data separately for males and females. Let us create the survey design:\n\n# Revised weight - weight divided by 6 cycles\ndat.full$svy.weight &lt;- dat.full$survey.weight/6 \ndat$svy.weight &lt;- dat$survey.weight/6 \nsummary(dat$svy.weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   223.2  1760.2  3349.3  4555.9  5817.8 25347.4\n\n# Create an indicator variable\ndat.full$miss &lt;- 1\ndat.full$miss[dat.full$id %in% dat$id] &lt;- 0\n\n# Set up the design\nw.design0 &lt;- svydesign(strata = ~strata, id = ~psu, weights = ~svy.weight, \n                       data = dat.full, nest = TRUE)\n\n# Subset the design\nw.design1 &lt;- subset(w.design0, miss == 0)\n\n# Subset the design for females\nw.design.f &lt;- subset(w.design1, sex == \"Female\")\ndim(w.design.f)\n#&gt; [1] 1875   28\n\n# Subset the design for males\nw.design.m &lt;- subset(w.design1, sex == \"Male\")\ndim(w.design.m)\n#&gt; [1] 1905   28\n\nKaplan-Meier plot\nLet us create the Kaplan-Meier plot for males and females:\n\n# KM for females\nfit0.f &lt;- svykm(Surv(stime, status) ~ caff, design = w.design.f)\nplot(fit0.f)\n\n\n\n\n\n\n\n# KM for males\nfit0.m &lt;- svykm(Surv(stime, status) ~ caff, design = w.design.m)\nplot(fit0.m)\n\n\n\n\n\n\n\nCox PH\nNow, we will fit the Cox proportional hazards model, separately for males and females. Let us run the unadjusted model first.\n\n# Unadjusted Cox PH for females\nfit1.f &lt;- svycoxph(Surv(stime, status) ~ caff, design = w.design.f)\npublish(fit1.f)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Female\")\n#&gt;  Variable          Units HazardRatio       CI.95   p-value \n#&gt;      caff No consumption         Ref                       \n#&gt;              &lt;100 mg/day        0.80 [0.62;1.03]   0.08543 \n#&gt;           100-200 mg/day        0.58 [0.42;0.80]   &lt; 0.001 \n#&gt;              200+ mg/day        0.61 [0.43;0.86]   0.00507\n\n# Unadjusted Cox PH for males\nfit1.m &lt;- svycoxph(Surv(stime, status) ~ caff, design = w.design.m)\npublish(fit1.m)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Male\")\n#&gt;  Variable          Units HazardRatio       CI.95 p-value \n#&gt;      caff No consumption         Ref                     \n#&gt;              &lt;100 mg/day        1.20 [0.88;1.63]   0.246 \n#&gt;           100-200 mg/day        1.10 [0.75;1.59]   0.632 \n#&gt;              200+ mg/day        1.10 [0.76;1.58]   0.607\n\nNow, we will fit the Cox PH model, adjusting for covariates.\n\n# Covariate adjusted Cox PH for females\nfit2.f &lt;- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.f)\npublish(fit2.f)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Female\")\n#&gt;           Variable                     Units HazardRatio       CI.95   p-value \n#&gt;               caff            No consumption         Ref                       \n#&gt;                                  &lt;100 mg/day        0.62 [0.46;0.84]   0.00166 \n#&gt;                               100-200 mg/day        0.44 [0.30;0.64]   &lt; 0.001 \n#&gt;                                  200+ mg/day        0.43 [0.28;0.66]   &lt; 0.001 \n#&gt;                age                                  1.08 [1.07;1.09]   &lt; 0.001 \n#&gt;               race        Non-Hispanic White         Ref                       \n#&gt;                           Non-Hispanic Black        0.81 [0.65;1.01]   0.06322 \n#&gt;                             Mexican American        0.79 [0.61;1.03]   0.07976 \n#&gt;                               Other Hispanic        0.65 [0.41;1.01]   0.05634 \n#&gt;                                   Other race        0.64 [0.38;1.08]   0.09394 \n#&gt;          education       Less than 9th grade         Ref                       \n#&gt;                                 9-11th grade        1.15 [0.84;1.57]   0.37396 \n#&gt;                            High school grade        1.03 [0.75;1.41]   0.84759 \n#&gt;                                 Some college        1.06 [0.79;1.44]   0.69484 \n#&gt;                    College graduate or above        0.61 [0.40;0.93]   0.02192 \n#&gt;            smoking              Never smoker         Ref                       \n#&gt;                               Current smoker        1.84 [1.42;2.38]   &lt; 0.001 \n#&gt;                                Former smoker        1.17 [0.95;1.46]   0.14379 \n#&gt;            alcohol            No consumption         Ref                       \n#&gt;                                &lt;20 grams/day        0.94 [0.65;1.36]   0.75056 \n#&gt;                                20+ grams/day        1.05 [0.58;1.93]   0.86786 \n#&gt;           carbohyd                                  1.00 [1.00;1.00]   0.69680 \n#&gt;  physical.activity                       Low         Ref                       \n#&gt;                                 Intermediate        0.73 [0.57;0.92]   0.00834 \n#&gt;                                         High        0.66 [0.49;0.89]   0.00638 \n#&gt;            bmi.cat                     &lt;20.0         Ref                       \n#&gt;                                20.0 to &lt;25.0        0.39 [0.21;0.73]   0.00301 \n#&gt;                                25.0 to &lt;30.0        0.33 [0.19;0.57]   &lt; 0.001 \n#&gt;                                30.0 to &lt;35.0        0.30 [0.17;0.54]   &lt; 0.001 \n#&gt;                                35.0 to &lt;40.0        0.26 [0.13;0.49]   &lt; 0.001 \n#&gt;                                        40.0+        0.31 [0.16;0.62]   &lt; 0.001 \n#&gt;                htn                        No         Ref                       \n#&gt;                                          Yes        1.00 [0.80;1.24]   0.98084 \n#&gt;      macrovascular                        No         Ref                       \n#&gt;                                          Yes        1.85 [1.48;2.32]   &lt; 0.001 \n#&gt;            insulin                        No         Ref                       \n#&gt;                                          Yes        1.87 [1.50;2.32]   &lt; 0.001 \n#&gt;       survey.cycle                   1999-00         Ref                       \n#&gt;                                      2001-02        0.58 [0.43;0.78]   &lt; 0.001 \n#&gt;                                      2003-04        0.75 [0.54;1.04]   0.08384 \n#&gt;                                      2005-06        0.81 [0.57;1.15]   0.24147 \n#&gt;                                      2007-08        0.88 [0.60;1.28]   0.49860 \n#&gt;                                      2009-10        0.76 [0.56;1.04]   0.08292\n\n# Covariate adjusted Cox PH for males\nfit2.m &lt;- svycoxph(Surv(stime, status) ~ caff + age + race + education + smoking + \n                     alcohol +  carbohyd + physical.activity + bmi.cat + htn +\n                     macrovascular + insulin + survey.cycle, design = w.design.m)\npublish(fit2.m)\n#&gt; Stratified 1 - level Cluster Sampling design (with replacement)\n#&gt; With (180) clusters.\n#&gt; subset(w.design1, sex == \"Male\")\n#&gt;           Variable                     Units HazardRatio       CI.95   p-value \n#&gt;               caff            No consumption         Ref                       \n#&gt;                                  &lt;100 mg/day        1.06 [0.75;1.49]   0.74199 \n#&gt;                               100-200 mg/day        1.04 [0.69;1.57]   0.84938 \n#&gt;                                  200+ mg/day        1.05 [0.71;1.54]   0.81249 \n#&gt;                age                                  1.08 [1.06;1.09]   &lt; 0.001 \n#&gt;               race        Non-Hispanic White         Ref                       \n#&gt;                           Non-Hispanic Black        0.75 [0.61;0.91]   0.00423 \n#&gt;                             Mexican American        0.68 [0.52;0.88]   0.00348 \n#&gt;                               Other Hispanic        0.83 [0.51;1.36]   0.46367 \n#&gt;                                   Other race        1.03 [0.60;1.76]   0.91073 \n#&gt;          education       Less than 9th grade         Ref                       \n#&gt;                                 9-11th grade        1.47 [1.10;1.96]   0.00846 \n#&gt;                            High school grade        1.03 [0.80;1.33]   0.81768 \n#&gt;                                 Some college        1.02 [0.76;1.36]   0.91606 \n#&gt;                    College graduate or above        0.68 [0.47;0.98]   0.03833 \n#&gt;            smoking              Never smoker         Ref                       \n#&gt;                               Current smoker        1.90 [1.38;2.62]   &lt; 0.001 \n#&gt;                                Former smoker        1.03 [0.85;1.26]   0.74101 \n#&gt;            alcohol            No consumption         Ref                       \n#&gt;                                &lt;20 grams/day        0.95 [0.74;1.21]   0.67612 \n#&gt;                                20+ grams/day        1.14 [0.83;1.58]   0.42116 \n#&gt;           carbohyd                                  1.00 [1.00;1.00]   0.37461 \n#&gt;  physical.activity                       Low         Ref                       \n#&gt;                                 Intermediate        0.64 [0.52;0.80]   &lt; 0.001 \n#&gt;                                         High        0.69 [0.56;0.87]   0.00137 \n#&gt;            bmi.cat                     &lt;20.0         Ref                       \n#&gt;                                20.0 to &lt;25.0        0.25 [0.11;0.58]   0.00134 \n#&gt;                                25.0 to &lt;30.0        0.20 [0.09;0.48]   &lt; 0.001 \n#&gt;                                30.0 to &lt;35.0        0.19 [0.08;0.45]   &lt; 0.001 \n#&gt;                                35.0 to &lt;40.0        0.25 [0.10;0.64]   0.00380 \n#&gt;                                        40.0+        0.26 [0.11;0.60]   0.00160 \n#&gt;                htn                        No         Ref                       \n#&gt;                                          Yes        1.18 [0.98;1.42]   0.08442 \n#&gt;      macrovascular                        No         Ref                       \n#&gt;                                          Yes        1.40 [1.17;1.66]   &lt; 0.001 \n#&gt;            insulin                        No         Ref                       \n#&gt;                                          Yes        1.48 [1.21;1.81]   &lt; 0.001 \n#&gt;       survey.cycle                   1999-00         Ref                       \n#&gt;                                      2001-02        1.37 [1.04;1.80]   0.02569 \n#&gt;                                      2003-04        0.98 [0.68;1.43]   0.93245 \n#&gt;                                      2005-06        1.07 [0.77;1.48]   0.67918 \n#&gt;                                      2007-08        1.05 [0.78;1.41]   0.75981 \n#&gt;                                      2009-10        0.83 [0.58;1.18]   0.29625\n\nThe adjusted results are approximately the same as in Table 2 of the article. Caffeine consumption was associated with mortality among women but not among men.\nPH assumption\nNow, we will check the proportional hazard assumption.\n\n# PH assumption among females\ncox.zph(fit2.f)\n#&gt;                      chisq df    p\n#&gt; caff              3.65e-03  3 1.00\n#&gt; age               2.15e-06  1 1.00\n#&gt; race              2.87e-03  4 1.00\n#&gt; education         2.42e-03  4 1.00\n#&gt; smoking           2.44e-03  2 1.00\n#&gt; alcohol           1.14e-03  2 1.00\n#&gt; carbohyd          7.38e-04  1 0.98\n#&gt; physical.activity 3.34e-03  2 1.00\n#&gt; bmi.cat           2.21e-03  5 1.00\n#&gt; htn               1.33e-03  1 0.97\n#&gt; macrovascular     4.24e-04  1 0.98\n#&gt; insulin           2.26e-04  1 0.99\n#&gt; survey.cycle      3.49e-03  5 1.00\n#&gt; GLOBAL            2.55e-02 32 1.00\n\n# PH assumption among males\ncox.zph(fit2.m)\n#&gt;                      chisq df    p\n#&gt; caff              2.70e-03  3 1.00\n#&gt; age               2.45e-03  1 0.96\n#&gt; race              2.40e-03  4 1.00\n#&gt; education         1.13e-03  4 1.00\n#&gt; smoking           1.67e-03  2 1.00\n#&gt; alcohol           1.50e-03  2 1.00\n#&gt; carbohyd          1.33e-03  1 0.97\n#&gt; physical.activity 7.73e-03  2 1.00\n#&gt; bmi.cat           3.60e-03  5 1.00\n#&gt; htn               4.45e-06  1 1.00\n#&gt; macrovascular     4.32e-06  1 1.00\n#&gt; insulin           9.24e-05  1 0.99\n#&gt; survey.cycle      2.21e-03  5 1.00\n#&gt; GLOBAL            2.78e-02 32 1.00\n\nThe large p-values indicate that the proportional hazard assumption was met for both models.",
    "crumbs": [
      "Complex outcomes",
      "Survival analysis: NHANES"
    ]
  },
  {
    "objectID": "nonbinary3.html",
    "href": "nonbinary3.html",
    "title": "Poisson",
    "section": "",
    "text": "In this tutorial, we will see how to use Poisson and negative binomial regression. In practice, we use Poisson regression to model a count outcome. Note that we assume the mean is equal to the variance in Poisson. When the variance is greater than what’s assumed by the model, overdispersion occurs. Poisson regression of overdispersed data leads to under-estimated or deflated standard errors, which leads to inflated test statistics and p-values. We can use negative binomial regression to model overdispersed data.\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(Publish)\nrequire(survey)\n\nData\n\n# Load\nload(\"Data/nonbinary/OAcvd.RData\")\n\n# Survey weight\nanalytic2$weight &lt;- analytic2$weight/3\n\n# Make fruit.cont as a numeric variable\nanalytic2$fruit.cont &lt;- as.numeric(as.character(analytic2$fruit.cont))\n\n# Make fruit.cont as a integer/count variable\nanalytic2$fruit.cont &lt;- floor(analytic2$fruit.cont) # round\n\n# Factor variables using lapply\nvar.names &lt;- c(\"age\", \"sex\", \"income\", \"race\", \"bmicat\", \"phyact\", \"smoke\",\n               \"fruit\", \"painmed\", \"ht\", \"copd\", \"diab\", \"edu\", \"CVD\", \"OA\")\nanalytic2[var.names] &lt;- lapply(analytic2[var.names] , factor)\nstr(analytic2)\n#&gt; 'data.frame':    21623 obs. of  17 variables:\n#&gt;  $ CVD       : Factor w/ 2 levels \"0 event\",\"event\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ age       : Factor w/ 4 levels \"20-39 years\",..: 2 3 4 1 3 1 3 1 3 3 ...\n#&gt;  $ sex       : Factor w/ 2 levels \"Female\",\"Male\": 2 1 1 1 2 1 2 1 1 2 ...\n#&gt;  $ income    : Factor w/ 4 levels \"$29,999 or less\",..: 3 4 1 2 1 2 2 3 3 4 ...\n#&gt;  $ race      : Factor w/ 2 levels \"Non-white\",\"White\": 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ bmicat    : Factor w/ 3 levels \"Normal\",\"Overweight\",..: 1 1 2 2 2 1 2 1 2 2 ...\n#&gt;  $ phyact    : Factor w/ 3 levels \"Active\",\"Inactive\",..: 3 1 1 2 1 3 2 3 2 3 ...\n#&gt;  $ smoke     : Factor w/ 3 levels \"Current smoker\",..: 2 3 3 2 2 2 2 2 3 2 ...\n#&gt;  $ fruit     : Factor w/ 3 levels \"0-3 daily serving\",..: 2 2 3 2 1 3 1 3 2 2 ...\n#&gt;  $ painmed   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 1 1 ...\n#&gt;  $ ht        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ copd      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ diab      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ edu       : Factor w/ 4 levels \"&lt; 2ndary\",\"2nd grad.\",..: 2 2 1 4 4 4 1 4 4 1 ...\n#&gt;  $ weight    : num  4.8 13.29 4.43 11.1 9.61 ...\n#&gt;  $ OA        : Factor w/ 2 levels \"Control\",\"OA\": 2 1 2 1 1 1 2 1 1 1 ...\n#&gt;  $ fruit.cont: num  3 4 8 3 2 10 1 8 5 3 ...\n#&gt;  - attr(*, \"na.action\")= 'omit' Named int [1:219757] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;   ..- attr(*, \"names\")= chr [1:219757] \"3\" \"5\" \"7\" \"9\" ...\n\nSurvey weighted summary\n\n# Survey design\nw.design &lt;- svydesign(id=~1, weights=~weight, data=analytic2)\n\n# Cross-tabulation\nxtabs(~fruit.cont, analytic2)\n#&gt; fruit.cont\n#&gt;    0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n#&gt;  407 1628 3304 4261 3759 2887 1980 1288  809  480  303  174  109   80   51   25 \n#&gt;   16   17   18   19   20   21   22   23   24   25   26   27   29   42 \n#&gt;   21   14    9    4    5    4    2    6    7    1    2    1    1    1\nsvytable(~fruit.cont, w.design)\n#&gt; fruit.cont\n#&gt;            0            1            2            3            4            5 \n#&gt;   9085.00222  39974.10444  88216.88222 119206.45778 108665.00556  82149.23111 \n#&gt;            6            7            8            9           10           11 \n#&gt;  53791.70444  35463.49111  22593.73000  13023.83556   8878.24778   4520.85000 \n#&gt;           12           13           14           15           16           17 \n#&gt;   3476.42667   2210.76000   1911.70556    973.93444    571.37667    414.44444 \n#&gt;           18           19           20           21           22           23 \n#&gt;    270.82556     77.76222    253.13000     24.74778     34.17444    185.70778 \n#&gt;           24           25           26           27           29           42 \n#&gt;    154.67778    109.88444    149.81222     89.61444     14.91111     93.76889\nsvyhist(~fruit.cont, w.design)\n\n\n\n\n\n\nsvyby(~fruit.cont, ~phyact, w.design, svymean, deff = TRUE)\n\n\n  \n\n\n\nPoisson regression\nLet us fit the traditional (not design-adjusted) Poisson regression using the glm function:\n\nrequire(jtools)\n#&gt; Loading required package: jtools\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Poisson regression - crude\nfit1 &lt;- glm(fruit.cont ~phyact2, data=analytic2, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nχ²(2)\n1219.347\n\n\np\n0.000\n\n\nPseudo-R² (Cragg-Uhler)\n0.055\n\n\nPseudo-R² (McFadden)\n0.012\n\n\nAIC\n98570.197\n\n\nBIC\n98594.142\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nz val.\np\n\n\n\n(Intercept)\n1.337\n1.328\n1.347\n267.387\n0.000\n\n\nphyact2Active\n0.274\n0.259\n0.289\n34.983\n0.000\n\n\nphyact2Moderate\n0.136\n0.120\n0.152\n16.741\n0.000\n\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n# Poisson regression - adjusted for covariates\nfit2 &lt;- glm(fruit.cont ~ phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, data=analytic2, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nχ²(17)\n2984.005\n\n\np\n0.000\n\n\nPseudo-R² (Cragg-Uhler)\n0.130\n\n\nPseudo-R² (McFadden)\n0.030\n\n\nAIC\n96835.540\n\n\nBIC\n96979.207\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nz val.\np\nVIF\n\n\n\n(Intercept)\n1.156\n1.123\n1.189\n68.773\n0.000\nNA\n\n\nphyact2Active\n0.250\n0.235\n0.266\n31.432\n0.000\n1.040\n\n\nphyact2Moderate\n0.112\n0.095\n0.128\n13.640\n0.000\n1.040\n\n\nage40-49 years\n0.027\n0.010\n0.043\n3.196\n0.001\n1.102\n\n\nage50-59 years\n0.083\n0.066\n0.100\n9.400\n0.000\n1.102\n\n\nage60-64 years\n0.127\n0.103\n0.151\n10.287\n0.000\n1.102\n\n\nsexMale\n-0.173\n-0.187\n-0.160\n-25.108\n0.000\n1.082\n\n\nincome$30,000-$49,999\n0.039\n0.017\n0.060\n3.557\n0.000\n1.144\n\n\nincome$50,000-$79,999\n0.050\n0.030\n0.070\n4.923\n0.000\n1.144\n\n\nincome$80,000 or more\n0.083\n0.062\n0.103\n7.964\n0.000\n1.144\n\n\nraceWhite\n-0.003\n-0.024\n0.018\n-0.240\n0.811\n1.077\n\n\nbmicatOverweight\n-0.021\n-0.035\n-0.008\n-3.054\n0.002\n1.096\n\n\nbmicatUnderweight\n-0.001\n-0.034\n0.033\n-0.030\n0.976\n1.096\n\n\nsmokeFormer smoker\n0.131\n0.114\n0.148\n15.050\n0.000\n1.132\n\n\nsmokeNever smoker\n0.181\n0.163\n0.199\n19.436\n0.000\n1.132\n\n\nedu2nd grad.\n0.040\n0.016\n0.064\n3.291\n0.001\n1.125\n\n\neduOther 2nd grad.\n0.051\n0.020\n0.083\n3.201\n0.001\n1.125\n\n\neduPost-2nd grad.\n0.122\n0.101\n0.144\n11.219\n0.000\n1.125\n\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nSurvey weighted Poisson regression\nNow, let’s fit the design-adjusted Poisson regression using the svyglm function:\n\nrequire(jtools)\n# Design\nw.design &lt;- update (w.design , phyact2=relevel(phyact, ref =\"Inactive\"))\n\n# Design-adjusted Poisson - crude\nfit1 &lt;- svyglm(fruit.cont ~phyact2, design=w.design, family=poisson)\nsumm(fit1, confint = TRUE, digits = 3)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.064\n\n\nPseudo-R² (McFadden)\n0.035\n\n\nAIC\n99087.436\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n(Intercept)\n1.373\n1.354\n1.391\n146.410\n0.000\n\n\nphyact2Active\n0.261\n0.231\n0.292\n16.877\n0.000\n\n\nphyact2Moderate\n0.124\n0.095\n0.154\n8.230\n0.000\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n# Design-adjusted Poisson - adjusted for covariates\nfit2&lt;-svyglm(fruit.cont ~phyact2 + age + sex + \n               income + race + bmicat + \n               smoke + edu, design=w.design, family=poisson)\nsumm(fit2, confint = TRUE, digits = 3, vifs = TRUE)\n\n\n\n\nObservations\n21623\n\n\nDependent variable\nfruit.cont\n\n\nType\nSurvey-weighted generalized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n \n\n\n\nPseudo-R² (Cragg-Uhler)\n0.137\n\n\nPseudo-R² (McFadden)\n0.076\n\n\nAIC\n97795.011\n\n\n\n \n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\nVIF\n\n\n\n(Intercept)\n1.175\n1.113\n1.237\n37.230\n0.000\nNA\n\n\nphyact2Active\n0.241\n0.210\n0.272\n15.211\n0.000\n1.346\n\n\nphyact2Moderate\n0.105\n0.074\n0.135\n6.775\n0.000\n1.346\n\n\nage40-49 years\n0.041\n0.010\n0.071\n2.623\n0.009\n1.447\n\n\nage50-59 years\n0.102\n0.071\n0.133\n6.402\n0.000\n1.447\n\n\nage60-64 years\n0.149\n0.096\n0.203\n5.461\n0.000\n1.447\n\n\nsexMale\n-0.136\n-0.161\n-0.111\n-10.500\n0.000\n1.148\n\n\nincome$30,000-$49,999\n0.025\n-0.015\n0.066\n1.217\n0.224\n1.294\n\n\nincome$50,000-$79,999\n0.031\n-0.008\n0.070\n1.559\n0.119\n1.294\n\n\nincome$80,000 or more\n0.057\n0.018\n0.096\n2.874\n0.004\n1.294\n\n\nraceWhite\n0.023\n-0.012\n0.059\n1.289\n0.197\n1.193\n\n\nbmicatOverweight\n-0.009\n-0.035\n0.017\n-0.708\n0.479\n1.331\n\n\nbmicatUnderweight\n0.029\n-0.033\n0.091\n0.928\n0.353\n1.331\n\n\nsmokeFormer smoker\n0.094\n0.059\n0.128\n5.352\n0.000\n1.474\n\n\nsmokeNever smoker\n0.140\n0.102\n0.177\n7.290\n0.000\n1.474\n\n\nedu2nd grad.\n0.026\n-0.020\n0.072\n1.093\n0.274\n1.302\n\n\neduOther 2nd grad.\n0.033\n-0.024\n0.090\n1.136\n0.256\n1.302\n\n\neduPost-2nd grad.\n0.129\n0.085\n0.173\n5.807\n0.000\n1.302\n\n\n\n\n Standard errors: Robust\n\n\n\n\n\n\n\n\n\n\n\nNegative binomial regression\nLet’s fit the negative binomial regression model using the glm function. Below, we specify the dispersion parameter (theta) is equal to 1, which suggests that the ratio of mean and variance is assumed to be 1.\n\nrequire(MASS)\n#&gt; Loading required package: MASS\nanalytic2$phyact2=relevel(analytic2$phyact, ref =\"Inactive\")\n\n# Negative binomial regression - crude\nfit3&lt;- glm(fruit.cont ~ phyact2, data=analytic2, family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#&gt; Waiting for profiling to be done...\n#&gt;                      2.5 % 97.5 %\n#&gt; (Intercept)     3.81  3.76   3.85\n#&gt; phyact2Active   1.32  1.29   1.34\n#&gt; phyact2Moderate 1.15  1.12   1.17\n\n# Negative binomial regression - adjusted for covariates\nfit4&lt;-glm(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, data=analytic2,\n          family = negative.binomial(theta = 1))\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#&gt; Waiting for profiling to be done...\n#&gt;                            2.5 % 97.5 %\n#&gt; (Intercept)           3.17  3.05   3.30\n#&gt; phyact2Active         1.29  1.26   1.31\n#&gt; phyact2Moderate       1.12  1.10   1.14\n#&gt; age40-49 years        1.03  1.01   1.05\n#&gt; age50-59 years        1.08  1.06   1.11\n#&gt; age60-64 years        1.14  1.10   1.17\n#&gt; sexMale               0.84  0.83   0.86\n#&gt; income$30,000-$49,999 1.05  1.02   1.07\n#&gt; income$50,000-$79,999 1.06  1.03   1.08\n#&gt; income$80,000 or more 1.09  1.07   1.12\n#&gt; raceWhite             0.99  0.97   1.02\n#&gt; bmicatOverweight      0.98  0.96   1.00\n#&gt; bmicatUnderweight     0.99  0.95   1.04\n#&gt; smokeFormer smoker    1.14  1.12   1.16\n#&gt; smokeNever smoker     1.20  1.17   1.23\n#&gt; edu2nd grad.          1.04  1.01   1.07\n#&gt; eduOther 2nd grad.    1.05  1.01   1.09\n#&gt; eduPost-2nd grad.     1.13  1.10   1.16\n\nSurvey weighted negative binomial regression\nNow, let’s fit the design-adjusted negative binomial regression model:\n\nrequire(sjstats)\n#&gt; Loading required package: sjstats\n#&gt; \n#&gt; Attaching package: 'sjstats'\n#&gt; The following object is masked from 'package:survey':\n#&gt; \n#&gt;     cv\n\n# Design-adjusted negative binomial - crude\nfit3&lt;- svyglm.nb(fruit.cont ~phyact2, design=w.design)\nround(exp(cbind(coef(fit3), confint(fit3))),2)\n#&gt;                                2.5 %    97.5 %\n#&gt; theta.(Intercept)   28859.34 5797.98 143646.88\n#&gt; eta.(Intercept)         3.95    3.87      4.02\n#&gt; eta.phyact2Active       1.30    1.26      1.34\n#&gt; eta.phyact2Moderate     1.13    1.10      1.17\n\n# Design-adjusted negative binomial - adjusted for covariates\nfit4&lt;-svyglm.nb(fruit.cont ~phyact2 + age + sex + income + race + bmicat + smoke + edu, \n                design=w.design)\nround(exp(cbind(coef(fit4), confint(fit4))),2)\n#&gt;                                        2.5 %     97.5 %\n#&gt; theta.(Intercept)         132340.25 15727.48 1113588.86\n#&gt; eta.(Intercept)                3.24     3.04       3.44\n#&gt; eta.phyact2Active              1.27     1.23       1.31\n#&gt; eta.phyact2Moderate            1.11     1.08       1.14\n#&gt; eta.age40-49 years             1.04     1.01       1.07\n#&gt; eta.age50-59 years             1.11     1.07       1.14\n#&gt; eta.age60-64 years             1.16     1.10       1.22\n#&gt; eta.sexMale                    0.87     0.85       0.90\n#&gt; eta.income$30,000-$49,999      1.03     0.99       1.07\n#&gt; eta.income$50,000-$79,999      1.03     0.99       1.07\n#&gt; eta.income$80,000 or more      1.06     1.02       1.10\n#&gt; eta.raceWhite                  1.02     0.99       1.06\n#&gt; eta.bmicatOverweight           0.99     0.97       1.02\n#&gt; eta.bmicatUnderweight          1.03     0.97       1.09\n#&gt; eta.smokeFormer smoker         1.10     1.06       1.14\n#&gt; eta.smokeNever smoker          1.15     1.11       1.19\n#&gt; eta.edu2nd grad.               1.03     0.98       1.07\n#&gt; eta.eduOther 2nd grad.         1.03     0.98       1.09\n#&gt; eta.eduPost-2nd grad.          1.14     1.09       1.19\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Complex outcomes",
      "Poisson"
    ]
  },
  {
    "objectID": "nonbinaryF.html",
    "href": "nonbinaryF.html",
    "title": "R functions (N)",
    "section": "",
    "text": "The list of new R functions introduced in this non-binary data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\ncox.zph\nsurvival\nTo assess the proportional hazard assumption\n\n\ncoxph\nsurvival\nTo fit cox regression model\n\n\nggforest\nsurvminer\nTo produce a forest plot\n\n\nmultinom\nnnet\nTo fit multinomial models\n\n\npolr\nMASS\nTo fir ordinal logistic and probit regressions\n\n\nSurv\nsurvival\nTo create a survival object\n\n\nsurvdiff\nsurvival\nTo compare survival times between groups\n\n\nsurvfit\nsurvival\nTo create survival curves\n\n\nsvy_vglm\nsvyVGAM\nTo fit design-based generalised linear models\n\n\nsvycoxph\nsurvey\nTo fit cox regression model for complex survey data\n\n\nsvyglm.nb\nsjstats\nNegative binomial model for complex survey data\n\n\nsvykm\nsurvey\nEstimate survival function for complex survey data\n\n\nsvyolr\nsurvey\nOrdinal logistic for complex survey",
    "crumbs": [
      "Complex outcomes",
      "R functions (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html",
    "href": "nonbinaryQ.html",
    "title": "Quiz (N)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html#live-quiz",
    "href": "nonbinaryQ.html#live-quiz",
    "title": "Quiz (N)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "nonbinaryQ.html#download-quiz",
    "href": "nonbinaryQ.html#download-quiz",
    "title": "Quiz (N)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Complex outcomes",
      "Quiz (N)"
    ]
  },
  {
    "objectID": "longitudinal.html",
    "href": "longitudinal.html",
    "title": "Longitudinal data",
    "section": "",
    "text": "Background\nThe chapter focuses on longitudinal data analysis. The first dives into mixed effects models, highlighting their importance in studying repeated measurements, especially in longitudinal or clustered data. These models comprise two essential components: fixed effects, which represent universal trends across subjects or clusters, and random effects, capturing individual attributes of each subject or cluster. The tutorial explains their application using datasets, emphasizing model selection metrics and validation methods. The second tutorial introduces the generalized estimating equation (GEE), suitable for modeling non-normally distributed longitudinal data. GEE differentiates from mixed-effects models in its distribution assumptions and its capability to handle various correlation structures. The tutorial illustrates GEE’s application, emphasizing its advantages over other regression models for repeated measurements, and ends with a comparison between GEE and mixed models’ random effects.",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal.html#background",
    "href": "longitudinal.html#background",
    "title": "Longitudinal data",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal.html#overview-of-tutorials",
    "href": "longitudinal.html#overview-of-tutorials",
    "title": "Longitudinal data",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nLongitudinal data analysis is specialized for handling data collected over time with repeated measurements on the same subjects. It accounts for within-subject correlation, time trends, and subject-specific effects, making it distinct from traditional regression, propensity score analysis, and machine learning approaches we discussed earlier, which are not designed for the longitudinal aspect of the data. Choosing the appropriate analysis method depends on the specific research question and the nature of the data at hand.\n\nMixed effects models\nThis tutorial provides an in-depth look into mixed effects models, which are instrumental in analyzing repeated measurements, especially in longitudinal or clustered data scenarios. Within the context of such models, there are two core components: fixed effects and random effects. Fixed effects depict broad trends applicable universally across subjects or clusters, offering insights such as average student performance across different subjects. In contrast, random effects spotlight the distinct attributes of each subject or cluster, accounting for variables such as the quality of resources or the experience of teachers in schools. Using a dataset, the tutorial demonstrates the application and visualization of linear mixed effects models. Emphasis is also placed on model selection, using metrics like AIC and BIC, and on validating the assumptions of the model using residual plots and QQ-plots.\n\n\nGEE\nThe tutorial introduces the generalized estimating equation (GEE) as a method for modeling longitudinal or clustered data that can be non-normally distributed, such as binary, count, or skewed data. Unlike mixed-effects models that assume a normal distribution for error terms and beta coefficients, GEE is distribution-free but assumes specific correlation structures within subjects or clusters. The tutorial exemplifies the application of GEE using respiratory data with a binary response variable. While logistic regression and Poisson regression handle binary and count data, they do not account for repeated measurement structures, potentially leading to incorrect standard error estimates. GEE allows for various correlation structures, and the tutorial details several types, including independent, exchangeable, AR-1 autoregressive, and unstructured correlations. The GEE models produce consistent estimates even if the correlation structure is misspecified. The tutorial concludes by comparing GEE with random effects in mixed models using several code examples.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Longitudinal data"
    ]
  },
  {
    "objectID": "longitudinal0.html",
    "href": "longitudinal0.html",
    "title": "Concepts (T)",
    "section": "",
    "text": "Longitudinal Data Analysis\nThe section offers a brief overview of longitudinal data analysis, focusing on key concepts and methods. It explains that longitudinal data involves the repeated measurement of variables of interest over time, often referred to as panel data. The example study data is initially in wide-form, with multiple variables for each time point, and then it is reshaped into long-form data for analysis.\nThe analysis of longitudinal data includes various models, such as linear models, mixed-effect models, and marginal models (GEE). Linear models are introduced initially, where each subject has a common slope and intercept. These ideas can be expanded to incorporate random intercepts, random slopes, and combinations of both in mixed-effect models. Model diagnostics, including AIC, BIC, and -loglik, are discussed for model evaluation. The section briefly discussing different correlation structures and highlights the differences in interpretation between mixed models and marginal models. This section also includes an introduction to marginal structual model, which is a GEE model under a situation when treatment-confounder feedback exists.",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#reading-list",
    "href": "longitudinal0.html#reading-list",
    "title": "Concepts (T)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Faraway 2016; Hothorn and Everitt 2014)\nOptional references: (Karim et al. 2021; Cui and Qian 2007)",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#video-lessons",
    "href": "longitudinal0.html#video-lessons",
    "title": "Concepts (T)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nLongitudinal data formatting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: mixed effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal models: GEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarginal structural model",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#video-lesson-slides",
    "href": "longitudinal0.html#video-lesson-slides",
    "title": "Concepts (T)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides\nLongitudinal models\n\n\nMarginal structural model",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#links",
    "href": "longitudinal0.html#links",
    "title": "Concepts (T)",
    "section": "Links",
    "text": "Links\nLongitudinal models\n\nGoogle Slides\nPDF Slides\n\nMarginal structural model\n\nGoogle Slides\nPDF Slides\nGitHub page about marginal structural model simulation",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal0.html#references",
    "href": "longitudinal0.html#references",
    "title": "Concepts (T)",
    "section": "References",
    "text": "References\n\n\n\n\nCui, Jianwen, and Guoqing Qian. 2007. “Selection of Working Correlation Structure and Best Model in GEE Analyses of Longitudinal Data.” Communications in Statistics—Simulation and Computation® 36 (5): 987–96.\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.\n\n\nKarim, Mohammad Ehsanul, Helen Tremlett, Feng Zhu, John Petkau, and Elaine Kingwell. 2021. “Dealing with Treatment-Confounder Feedback and Sparse Follow-up in Longitudinal Studies: Application of a Marginal Structural Model in a Multiple Sclerosis Cohort.” American Journal of Epidemiology 190 (5): 908–17.",
    "crumbs": [
      "Longitudinal data",
      "Concepts (T)"
    ]
  },
  {
    "objectID": "longitudinal1.html",
    "href": "longitudinal1.html",
    "title": "Mixed effects models",
    "section": "",
    "text": "In this section, we will learn about mixed effects models. Mixed effects models are popular choices for modeling repeated measurements, such as longitudinal or clustered data. Examples of longitudinal data include blood pressure measurements taken over time from the same individuals and CD4 count over time from the same individuals. Examples of clustered data include students within schools and patients within hospitals. There are two components in a mixed effects model: fixed effects and random effects:\n\nFixed effects refer to general trends that are applicable to all subjects/clusters. This implies that we might want to investigate how students perform (on average) in different subjects such as math and history. These effects are commonly observed, i.e., fixed, across all schools.\nRandom Effects capture the unique characteristics of each subject/cluster that differentiate them from one another. For instance, some schools may have better resources, more experienced teachers, or a more supportive learning environment. These differences are specific to each school, and we use random effects to capture them.\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(kableExtra)\nrequire(Matrix)\nrequire(jtools)\n\nRepeated measures\n\n\nA useful reference on repeated measures is Section 9.2 of Faraway (2016).\nIn repeated measurement design, the response variable are measured multiple times for each individuals.\nLinear Mixed Effects Models for Repeated Measures Data\n\n\nA useful reference on linear mixed effects models is Section 12.4 of Hothorn and Everitt (2014).\n\n\n\n\n\n\nImportant\n\n\n\n\nThe linear mixed effect models are based on the idea that the correlation of an individual’s responses depends on some unobserved individual characteristics.\nIn linear mixed effect models, we treat these unobserved characteristics as random effects in our model. If we could conditional on the random effects, then the repeated measurements can be assumed to be independent.\n\n\n\nData\nWe are going to use the BtheB dataset from the HSAUR2 package to explain the linear mixed effect model:\n\nlibrary(HSAUR2)\n#&gt; Loading required package: tools\ndata(\"BtheB\")\nkable(head(BtheB))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nbdi.2m\nbdi.3m\nbdi.5m\nbdi.8m\n\n\n\nNo\n&gt;6m\nTAU\n29\n2\n2\nNA\nNA\n\n\nYes\n&gt;6m\nBtheB\n32\n16\n24\n17\n20\n\n\nYes\n&lt;6m\nTAU\n25\n20\nNA\nNA\nNA\n\n\nNo\n&gt;6m\nBtheB\n21\n17\n16\n10\n9\n\n\nYes\n&gt;6m\nBtheB\n26\n23\nNA\nNA\nNA\n\n\nYes\n&lt;6m\nBtheB\n7\n0\n0\n0\n0\n\n\n\n\n\n\nA typical form of repeated measurement data from a clinical trial data.\nThe individuals are allocated to different treatments then the response Beck Depression Inventory II were taken at baseline, 2, 3, 5, and 8 months\n\n\n## Box-plot of responses at different time points in treatment and control groups\ndata(\"BtheB\", package = \"HSAUR2\")\nlayout(matrix(1:2, nrow = 1))\nylim &lt;- range(BtheB[,grep(\"bdi\", names(BtheB))],na.rm = TRUE)\ntau &lt;- subset(BtheB, treatment == \"TAU\")[, grep(\"bdi\", names(BtheB))]\nboxplot(tau, main = \"Treated as Usual\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\nbtheb &lt;- subset(BtheB, treatment == \"BtheB\")[, grep(\"bdi\", names(BtheB))]\nboxplot(btheb, main = \"Beat the Blues\", ylab = \"BDI\",\n        xlab = \"Time (in months)\", \n        names = c(0, 2, 3, 5, 8),ylim = ylim)\n\n\n\n\n\n\n\n\nThe side-by-side box plots show the distributions of BDI overtime between control (Treated as Usual) and intervention (Beat the Blues) groups.\nAs time goes, drops in BDI are more obvious in intervention which may indicate the intervention is effective.\nRegular model fixed intercept and slope\nTo compare, we start with fixed effect linear model, i.e., a regular linear model without any random effect:\n\n## To analyze the data, we first need to convert the dataset to a analysis-ready form\nBtheB$subject &lt;- factor(rownames(BtheB))\nnobs &lt;- nrow(BtheB)\nBtheB_long &lt;- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \n                                  \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time &lt;- rep(c(2, 3, 5, 8), rep(nobs, 4))\nkable(head(BtheB_long))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n1.2m\nNo\n&gt;6m\nTAU\n29\n1\n2\n2\n\n\n2.2m\nYes\n&gt;6m\nBtheB\n32\n2\n2\n16\n\n\n3.2m\nYes\n&lt;6m\nTAU\n25\n3\n2\n20\n\n\n4.2m\nNo\n&gt;6m\nBtheB\n21\n4\n2\n17\n\n\n5.2m\nYes\n&gt;6m\nBtheB\n26\n5\n2\n23\n\n\n6.2m\nYes\n&lt;6m\nBtheB\n7\n6\n2\n0\n\n\n\n\nunique(BtheB_long$subject)\n#&gt;   [1] 1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18 \n#&gt;  [19] 19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 \n#&gt;  [37] 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 \n#&gt;  [55] 55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 \n#&gt;  [73] 73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 \n#&gt;  [91] 91  92  93  94  95  96  97  98  99  100\n#&gt; 100 Levels: 1 10 100 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 ... 99\nkable(subset(BtheB_long,  subject == 2))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n2.2m\nYes\n&gt;6m\nBtheB\n32\n2\n2\n16\n\n\n2.3m\nYes\n&gt;6m\nBtheB\n32\n2\n3\n24\n\n\n2.5m\nYes\n&gt;6m\nBtheB\n32\n2\n5\n17\n\n\n2.8m\nYes\n&gt;6m\nBtheB\n32\n2\n8\n20\n\n\n\n\nkable(subset(BtheB_long,  subject == 99))%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, \n                position = \"left\")\n\n\n\n\ndrug\nlength\ntreatment\nbdi.pre\nsubject\ntime\nbdi\n\n\n\n99.2m\nNo\n&lt;6m\nTAU\n13\n99\n2\n5\n\n\n99.3m\nNo\n&lt;6m\nTAU\n13\n99\n3\n5\n\n\n99.5m\nNo\n&lt;6m\nTAU\n13\n99\n5\n0\n\n\n99.8m\nNo\n&lt;6m\nTAU\n13\n99\n8\n6\n\n\n\n\n\n\nlmfit &lt;- lm(bdi ~ bdi.pre + time + treatment + drug +length, \n            data = BtheB_long, na.action = na.omit)\nrequire(jtools)\nsumm(lmfit)\n\n\n\n\nObservations\n280 (120 missing obs. deleted)\n\n\nDependent variable\nbdi\n\n\nType\nOLS linear regression\n\n\n\n \n\n\n\nF(5,274)\n35.78\n\n\nR²\n0.40\n\n\nAdj. R²\n0.38\n\n\n\n \n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n(Intercept)\n7.32\n1.73\n4.24\n0.00\n\n\nbdi.pre\n0.57\n0.05\n10.44\n0.00\n\n\ntime\n-0.94\n0.24\n-3.97\n0.00\n\n\ntreatmentBtheB\n-3.32\n1.10\n-3.02\n0.00\n\n\ndrugYes\n-3.57\n1.15\n-3.11\n0.00\n\n\nlength&gt;6m\n1.71\n1.11\n1.54\n0.12\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\nRandom intercept but fixed slope\nLet us start with a model with a random intercept but fixed slope. In this case, the resulting regression line for each individual is parallel (for fixed slope) but have different intercepts (for random intercept).\n\n## Fit a random intercept model with lme4 package\nlibrary(\"lme4\")\nBtheB_lmer1 &lt;- lmer(bdi ~ bdi.pre + time + treatment + drug +length \n                    + (1 | subject), data = BtheB_long, \n                    REML = FALSE, na.action = na.omit)\n\nsummary(BtheB_lmer1)\n#&gt; Linear mixed model fit by maximum likelihood  ['lmerMod']\n#&gt; Formula: bdi ~ bdi.pre + time + treatment + drug + length + (1 | subject)\n#&gt;    Data: BtheB_long\n#&gt; \n#&gt;       AIC       BIC    logLik -2*log(L)  df.resid \n#&gt;    1887.5    1916.6    -935.7    1871.5       272 \n#&gt; \n#&gt; Scaled residuals: \n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.6975 -0.5026 -0.0638  0.4124  3.8203 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  subject  (Intercept) 48.78    6.984   \n#&gt;  Residual             25.14    5.014   \n#&gt; Number of obs: 280, groups:  subject, 97\n#&gt; \n#&gt; Fixed effects:\n#&gt;                Estimate Std. Error t value\n#&gt; (Intercept)     5.59239    2.24244   2.494\n#&gt; bdi.pre         0.63968    0.07789   8.212\n#&gt; time           -0.70476    0.14639  -4.814\n#&gt; treatmentBtheB -2.32908    1.67036  -1.394\n#&gt; drugYes        -2.82495    1.72684  -1.636\n#&gt; length&gt;6m       0.19708    1.63832   0.120\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;             (Intr) bdi.pr time   trtmBB drugYs\n#&gt; bdi.pre     -0.682                            \n#&gt; time        -0.238  0.020                     \n#&gt; tretmntBthB -0.390  0.121  0.018              \n#&gt; drugYes     -0.073 -0.237 -0.022 -0.323       \n#&gt; length&gt;6m   -0.243 -0.242 -0.036  0.002  0.157\nsumm(BtheB_lmer1)\n\n\n\n\nObservations\n280\n\n\nDependent variable\nbdi\n\n\nType\nMixed effects linear regression\n\n\n\n \n\n\n\nAIC\n1887.49\n\n\nBIC\n1916.57\n\n\nPseudo-R² (fixed effects)\n0.39\n\n\nPseudo-R² (total)\n0.79\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nEst.\nS.E.\nt val.\nd.f.\np\n\n\n\n\n(Intercept)\n5.59\n2.24\n2.49\n108.98\n0.01\n\n\nbdi.pre\n0.64\n0.08\n8.21\n104.08\n0.00\n\n\ntime\n-0.70\n0.15\n-4.81\n199.32\n0.00\n\n\ntreatmentBtheB\n-2.33\n1.67\n-1.39\n97.12\n0.17\n\n\ndrugYes\n-2.82\n1.73\n-1.64\n98.20\n0.11\n\n\nlength&gt;6m\n0.20\n1.64\n0.12\n100.26\n0.90\n\n\n\n\n p values calculated using Satterthwaite d.f.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nsubject\n(Intercept)\n6.98\n\n\nResidual\n\n5.01\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\nsubject\n97\n0.66\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nAIC, BIC, and -loglik etc are goodness-of-fit statistics, which tells you how well the model fits your data. Since there is no standard to tell what values of these statistics are good, without comparison with other models, they have little information to tell.\nFixed effects: this is the standard output we will have in any fixed-effect model. The interpretation of estimated coefficients will be similar to a regular linear model.\nYou may compare the outputs with the regular linear model, then you will find that lm tends to underestimate the SE for estimated coefficients.\n\n\n\n\nlibrary(ggplot2)\nBtheB_longna &lt;- na.omit(BtheB_long)\ndat &lt;- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer1),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, \n                    color=Subject)) + theme_classic() +\n    geom_line() \n\n\n\n\n\n\n\nAs we can see, for each individual, we have different intercepts but the slope over follow-up time is the same. Next, we will fit the model with random intercept and random slope.\nRandom intercept and random slope\nIn the codes below, we fitted a mixed effects model with both random intercept and random slope:\n\n## We can fit a random slope and intercept model using lme4 package and treat variable time as random slope. \nlibrary(\"lme4\")\nBtheB_lmer2 &lt;- lmer(bdi ~ bdi.pre + time + treatment + drug +length + \n                   (time | subject), data = BtheB_long, REML = FALSE, \n                   na.action = na.omit)\n\nsumm(BtheB_lmer2)\n\n\n\n\nObservations\n280\n\n\nDependent variable\nbdi\n\n\nType\nMixed effects linear regression\n\n\n\n \n\n\n\nAIC\n1891.04\n\n\nBIC\n1927.39\n\n\nPseudo-R² (fixed effects)\n0.39\n\n\nPseudo-R² (total)\n0.80\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nEst.\nS.E.\nt val.\nd.f.\np\n\n\n\n\n(Intercept)\n5.61\n2.25\n2.50\n106.79\n0.01\n\n\nbdi.pre\n0.64\n0.08\n8.25\n102.78\n0.00\n\n\ntime\n-0.70\n0.15\n-4.56\n57.69\n0.00\n\n\ntreatmentBtheB\n-2.38\n1.67\n-1.42\n97.12\n0.16\n\n\ndrugYes\n-2.87\n1.73\n-1.66\n98.18\n0.10\n\n\nlength&gt;6m\n0.14\n1.64\n0.09\n100.04\n0.93\n\n\n\n\n p values calculated using Satterthwaite d.f.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nsubject\n(Intercept)\n7.12\n\n\nsubject\ntime\n0.43\n\n\nResidual\n\n4.90\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\nsubject\n97\n0.68\n\n\n\n\n\nThe interpretation of the model outputs will be similar to the model with only random intercepts. Let us plot the data:\n\nlibrary(ggplot2)\nBtheB_longna &lt;- na.omit(BtheB_long)\ndat &lt;- data.frame(time=BtheB_longna$time,pred=fitted(BtheB_lmer2),\n                  Subject= BtheB_longna$subject)\nggplot(data=dat,aes(x=time, y=pred, group=Subject, color=Subject)) + \n  theme_classic() +\n    geom_line() \n\n\n\n\n\n\n\nFrom the figure, we can see, we have different intercepts and different slopes over follow-up time for each individual.\nChoice among models\nA common question is to ask should I add random slope to our model or random intercept is good enough. We may want to compare the models in terms of AIC and BIC. Smaller values indicate a better model.\n\nlm usually will not be considered as a competitor of lme as they basically apply to different types of data.\nWhen choosing between random intercept and random slope, a quick solution is to fit all possible models then do likelihood ratio tests.\nFor example, I am not sure whether I should use random intercept only or random intercept + random slope. I could fit both model, then do a likelihood ratio test:\n\n\nanova(BtheB_lmer1, BtheB_lmer2)\n\n\n  \n\n\n## The non-significant p-value shows that the second model is not \n## statistically different from the first model. Therefore, adding a \n## random slope is not necessary\n\nThe p-value is greater than 0.05, which indicate that adding a random slope does not make the fitting significantly better. To keep the model simple, we may just use random intercept.\nPrediction of Random Effects\n\nRef: (Hothorn and Everitt 2014) Section 12.5\n\nIf you have noticed in the R output of linear mixed effect model. Random effects are not estimated in the model.\n\nWe could use the fitted model to predict random effects.\nAlso, the predicted random effects can be used to examine the assumptions we have for linear mixed effect model.\n\nThe ranef function is used to predict the random effect in R\n\nqint &lt;- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqint\n#&gt;  [1] -16.36173411  -2.44276827  -3.81655251   3.05333928   3.45142248\n#&gt;  [6]   7.76598376   2.72263773  -7.22484569   6.84680617  -1.02552084\n#&gt; [11]   2.40417048   1.16611114  -8.55346533  -6.25844483  -2.26117896\n#&gt; [16]  -5.23031336   2.52509381  -1.64263857  -1.07523649   3.93689163\n#&gt; [21]   7.66903473 -16.38156595   1.74883174  -1.31871410  -9.02883854\n#&gt; [26]  -2.75150756   2.38884151   2.72740404  -3.41918542   6.27519245\n#&gt; [31]  -4.52154811  -8.67437225  -0.34470208  -0.63972054  -0.08927194\n#&gt; [36]   7.61914282   2.91050412  -2.58455318   4.24637616 -16.01664364\n#&gt; [41]   5.90993779   3.21014012  -7.04631398   3.09608824   4.71327710\n#&gt; [46]  16.70161338   2.26241522   2.26584476  15.77002952   1.75410838\n#&gt; [51]   6.18741474   1.99791717   0.56774362   4.04107968  10.72891321\n#&gt; [56]  -1.54551028  -5.28330207   2.04552526   2.36472056  -1.56879952\n#&gt; [61]   6.63565636   5.45929329  -4.19695096  -6.39632488  -2.21496288\n#&gt; [66]  -0.95956136  -3.96021851   7.17628719  -1.41644518  -4.99763144\n#&gt; [71]  -2.83374092  -1.28015494   8.79345988  -0.33213428   1.43146353\n#&gt; [76]  -3.13316295  -6.52989461   2.51445573   5.93415004   3.42003755\n#&gt; [81]   4.49954804   3.75178702  -6.44832897  12.88173218  -9.36110814\n#&gt; [86]   2.02028152 -18.09960241  -6.53428535   5.06613705  -3.31816912\n#&gt; [91]   7.26187445   0.03103101  -8.14350181  -4.89326763   2.92437698\n#&gt; [96]   5.24835927  -5.96778945\n## predict random effects using the fitted model\n\nCheck assumptions\nRemember, we have two assumptions in the linear mixed effect model with random intercept:\n\nerror term follows normal distribution\nbeta for subject \\(i\\) follows normal distribution\n\nWe have predict the values of random effect and we could extract residuals from the fitted model. Therefore, we can use QQ-plot to check their normality\n\n# Assumption 1\nqres &lt;- residuals(BtheB_lmer1)\nqqnorm(qres, xlim = c(-3, 3), ylim = c(-20, 20), \n       ylab = \"Estimated residuals\",\n       main = \"Residuals\")\nqqline(qres)\n\n\n\n\n\n\n\n# Assumption 2\nqint &lt;- ranef(BtheB_lmer1)$subject[[\"(Intercept)\"]]\nqqnorm(qint, ylab = \"Estimated random intercepts\", \n       xlim = c(-3, 3), ylim = c(-20, 20),\n       main = \"Random intercepts\")\nqqline(qint)\n\n\n\n\n\n\n\nSince points are almost on the lines, we can say that the normality assumption is met.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.",
    "crumbs": [
      "Longitudinal data",
      "Mixed effects models"
    ]
  },
  {
    "objectID": "longitudinal2.html",
    "href": "longitudinal2.html",
    "title": "GEE",
    "section": "",
    "text": "In this section, we will learn about generalized estimating equation (GEE). GEE is another popular method for modeling longitudinal or clustered data.\n\n\n\n\n\n\nNote\n\n\n\n\nGEE is a population-averaged (e.g., marginal) model, whereas mixed effects models are subject/cluster-specific. For example, we can use GEE when we are interested in exploring the population average effect. On the other hand, when we are interested in individual/cluster-specific effects, we can use the mixed effects models.\nAlso, in contrast to the mixed effects models where we assume error term and beta coefficients for subject \\(i\\) both follow normal distribution, GEE is distribution free. However, in GEE, we assume some correlation structures for within subjects/clusters. Hence, we can use GEE for non-normal data such as skewed data, binary data, or count data.\n\n\n\n\n# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(HSAUR2)\nlibrary(gee)\nlibrary(MuMIn)\nlibrary(geepack)\nlibrary(\"lme4\")\nrequire(afex)\n\nData Description\nIn addition to BtheB dataset in part 1, we used respiratory data from HSAUR2 package to demonstrate the analysis with non-normal responses:\n\nThe response variable in this dataset is status (the respiratory status), which is a binary response\nOther covariates are: treatment, age, gender, the study center\nThe response has been measured at 0, 1, 2, 3, 4 mths for each subject\n\n\ndata(\"respiratory\", package = \"HSAUR2\")\nhead(respiratory)\n\n\n  \n\n\n\nMethods for Non-normal Distributions\n\nref: (Hothorn and Everitt 2014) Section 13.2\nIn addition to normally distributed response variables, the response variable can also follow non-normal distributions, such as binary or count responses.\nWe have learned logistic regression or Poisson regression (glm) to analyze binary and count data but they are not considering the “repeated measurement” structure.\nThe consequence of ignoring the longitudinal/repeated measurements structure is to have the wrong estimated standard errors for regression coefficients. Hence, our inference will be invalid. However, the glm still gives consistent estimated beta coefficients.\nThere are many correlation structures in modelling GEE.\nWith non-normal responses, different assumptions about these correlation structures can lead to different interpretations of the beta coefficients. Here, we will introduce two different GEE models: marginal model and conditional model.\nMarginal Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.1\n\nRepeated measurement and longitudinal data has responses taken at different time points. Therefore, we could simply review them as many series of cross-sectional data. Each cross-sectional data can be analyzed using glm introduced in previous lectures. Then a correlation structure can be assumed to connect different “cross-sections”. The common correlation structures are:\nAn independent structure\nAn independent structure which assumes the repeated measures are independent. An example of independent structure is basically is an identity matrix:\n\\[\n\\left(\\begin{array}{ccc}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{array}\\right)\n\\]\nAn exchangeable correlation\nAn exchangeable correlation assumes that for the each individual, the correlation between each pair of repeated measurements is the same, i.e., \\(Cor(Y_{ij},Y_{ik})=\\rho\\). In the correlation matrix, only \\(\\rho\\) is unknown. Therefore, it is a single parameter working correlation matrix. An example of exchangeable correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho\\\\\n\\rho & 1 & \\rho\\\\\n\\rho & \\rho & 1\n\\end{array}\\right)\n\\]\nAn AR-1 autoregressive correlation\nDefined as \\(Cor(Y_{ij},Y_{ik})=\\rho^{|k-j|}\\). If two measurements are taken at two closer time points the correlation is higher than these taken at two farther apart time points. It is also a single parameter working correlation matrix. An example of AR-1 correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho & \\rho^2\\\\\n\\rho & 1 & \\rho\\\\\n\\rho^2 & \\rho & 1\n\\end{array}\\right)\n\\]\nAn unstructured correlation\nDifferent pairs of observation for each individual have different correlations \\(Cor(Y_{ij},Y_{ik})=\\rho_{jk}\\). Assume that each individual has \\(K\\) pairs of measurements, it is a \\(K(K-1)/2\\) parameters working correlation matrix. An example of unstructured correlation matrix is:\n\\[\n\\left(\\begin{array}{ccc}\n1 & \\rho_{12} & \\rho_{13}\\\\\n\\rho_{12} & 1 & \\rho_{23}\\\\\n\\rho_{13} & \\rho_{23} & 1\n\\end{array}\\right)\n\\]\n\nSometimes, specifying a “right” correlation matrix is hard. However, the marginal model (usually we use GEE) gives us consistent estimated coefficients even with misspecified correlation structure.\nConditional Models\n\nref: (Hothorn and Everitt 2014) Section 13.2.2\nIn GEE marginal models, the estimated regression coefficients are marginal (or population-averaged) effects. Therefore, the interpretation are at population-level. It is almost impossible to make inference on any specific individual or cluster.\nOne solution is to do conditional models. The random effect approach in the part 1 can be extended to non-Gaussian response.\nGEE models\nAfter the short introduction of two models, let’s take a look at real examples\n\nref: (Hothorn and Everitt 2014) Section 13.3\nref: (Faraway 2016) Section 10.2\n\nBinary response\nWe started with binary response using respiratory dataset:\n\nlibrary(gee)\ndata(\"respiratory\", package = \"HSAUR2\")\n## Data manipulation\nresp &lt;- subset(respiratory, month &gt; \"0\")\nresp$baseline &lt;- rep(subset(respiratory, month == \"0\")$status, rep(4, 111))\n## Change the response to 0 and 1\nresp$nstat &lt;- as.numeric(resp$status == \"good\")\nresp$month &lt;- resp$month[, drop = TRUE]\n\nNow we will fit a regular glm, i.e., model without random effect or any correlation structures. For binary outcomes, the estimated coefficients are log odds.\n\n## Regular GLM \nresp_glm &lt;- glm(status ~ centre + treatment + gender + baseline+ age, \n                data = resp, family = \"binomial\")\nsummary(resp_glm)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = status ~ centre + treatment + gender + baseline + \n#&gt;     age, family = \"binomial\", data = resp)\n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)        -0.900171   0.337653  -2.666  0.00768 ** \n#&gt; centre2             0.671601   0.239567   2.803  0.00506 ** \n#&gt; treatmenttreatment  1.299216   0.236841   5.486 4.12e-08 ***\n#&gt; gendermale          0.119244   0.294671   0.405  0.68572    \n#&gt; baselinegood        1.882029   0.241290   7.800 6.20e-15 ***\n#&gt; age                -0.018166   0.008864  -2.049  0.04043 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 608.93  on 443  degrees of freedom\n#&gt; Residual deviance: 483.22  on 438  degrees of freedom\n#&gt; AIC: 495.22\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\nNow we will fit GEE with independent correlation structure:\n\n## GEE with identity matrix\nresp_gee.in &lt;- gee(nstat ~ centre + treatment + gender + baseline + age, \n                 data = resp, family = \"binomial\", \n                 id = subject,corstr = \"independence\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)            centre2 treatmenttreatment         gendermale \n#&gt;        -0.90017133         0.67160098         1.29921589         0.11924365 \n#&gt;       baselinegood                age \n#&gt;         1.88202860        -0.01816588\nsummary(resp_gee.in)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Logit \n#&gt;  Variance to Mean Relation: Binomial \n#&gt;  Correlation Structure:     Independent \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = nstat ~ centre + treatment + gender + baseline + \n#&gt;     age, id = subject, data = resp, family = \"binomial\", corstr = \"independence\", \n#&gt;     scale.fix = TRUE, scale.value = 1)\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#&gt; centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#&gt; gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#&gt; age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\n#&gt; \n#&gt; Estimated Scale Parameter:  1\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    0    0    0\n#&gt; [2,]    0    1    0    0\n#&gt; [3,]    0    0    1    0\n#&gt; [4,]    0    0    0    1\n\n\nThis model assumes an independent correlation structure, the output will be equal to glm.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same as GLM. For binary outcome, you may still interpret them as log odds. Naive SE and z value are estimated directly from this model. Robust SE and z are sandwich estimates.\nThe difference between naive and robust indicates that the correlation structure may not be good.\nWorking Correlation is the correlation structure estimated from the data (identity matrix for independence).\n\nLet fit the GEE model with exchangeable correlation structure:\n\n## GEE with exchangeable matrix\nresp_gee.ex &lt;- gee(nstat ~ centre + treatment + gender + baseline+ age, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)            centre2 treatmenttreatment         gendermale \n#&gt;        -0.90017133         0.67160098         1.29921589         0.11924365 \n#&gt;       baselinegood                age \n#&gt;         1.88202860        -0.01816588\nsummary(resp_gee.ex)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Logit \n#&gt;  Variance to Mean Relation: Binomial \n#&gt;  Correlation Structure:     Exchangeable \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = nstat ~ centre + treatment + gender + baseline + \n#&gt;     age, id = subject, data = resp, family = \"binomial\", corstr = \"exchangeable\", \n#&gt;     scale.fix = TRUE, scale.value = 1)\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -0.93134415 -0.30623174  0.08973552  0.33018952  0.84307712 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#&gt; centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#&gt; gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#&gt; age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n#&gt; \n#&gt; Estimated Scale Parameter:  1\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;           [,1]      [,2]      [,3]      [,4]\n#&gt; [1,] 1.0000000 0.3359883 0.3359883 0.3359883\n#&gt; [2,] 0.3359883 1.0000000 0.3359883 0.3359883\n#&gt; [3,] 0.3359883 0.3359883 1.0000000 0.3359883\n#&gt; [4,] 0.3359883 0.3359883 0.3359883 1.0000000\n\n\nThis model assumes an exchangeable correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficients are the same GLM. For binary outcome, you may still interpret them as log odds. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nThe difference between naive and robust is smaller, which indicates that the correlation structure is better specified.\nWorking Correlation is the correlation structure estimated from the data\n\nLet’s check the estimated coefficients from all three models.\n\nsummary(resp_glm)$coefficients\n#&gt;                       Estimate  Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)        -0.90017133 0.337652992 -2.6659658 7.676750e-03\n#&gt; centre2             0.67160098 0.239566555  2.8034004 5.056684e-03\n#&gt; treatmenttreatment  1.29921589 0.236840962  5.4856047 4.120574e-08\n#&gt; gendermale          0.11924365 0.294671000  0.4046671 6.857223e-01\n#&gt; baselinegood        1.88202860 0.241290163  7.7998563 6.197770e-15\n#&gt; age                -0.01816588 0.008864401 -2.0493065 4.043215e-02\nsummary(resp_gee.in)$coefficients\n#&gt;                       Estimate  Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133 0.337653052 -2.665965  0.46032700 -1.9555041\n#&gt; centre2             0.67160098 0.239566599  2.803400  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589 0.236841017  5.485603  0.35077797  3.7038127\n#&gt; gendermale          0.11924365 0.294671045  0.404667  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860 0.241290221  7.799854  0.35005152  5.3764332\n#&gt; age                -0.01816588 0.008864403 -2.049306  0.01300426 -1.3969169\nsummary(resp_gee.ex)$coefficients\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90017133  0.4784634 -1.8813796  0.46032700 -1.9555041\n#&gt; centre2             0.67160098  0.3394723  1.9783676  0.35681913  1.8821889\n#&gt; treatmenttreatment  1.29921589  0.3356101  3.8712064  0.35077797  3.7038127\n#&gt; gendermale          0.11924365  0.4175568  0.2855747  0.44320235  0.2690501\n#&gt; baselinegood        1.88202860  0.3419147  5.5043802  0.35005152  5.3764332\n#&gt; age                -0.01816588  0.0125611 -1.4462014  0.01300426 -1.3969169\n# Same estimated coefficients but different SEs\n\nGEE with identity matrix is the same as GLM model. If we change the correlation structure to exchangeable does not change the beta estimates, but the naive SEs are closer to Robust SE, which indicates that the exchangeable correlation structure is a better reflection of the correlation structures.\nGaussian response\nGEE can also be applied to Gaussian response\n\nlibrary(gee)\nlibrary(HSAUR2)\nBtheB$subject &lt;- factor(rownames(BtheB))\nnobs &lt;- nrow(BtheB)\nBtheB_long &lt;- reshape(BtheB, idvar = \"subject\", \n                      varying = c(\"bdi.2m\", \"bdi.3m\", \"bdi.5m\", \"bdi.8m\"), \n                      direction = \"long\")\nBtheB_long$time &lt;- rep(c(2, 3, 5, 8), rep(nobs, 4))\nosub &lt;- order(as.integer(BtheB_long$subject))\nBtheB_long &lt;- BtheB_long[osub,]\nbtb_gee.ind &lt;- gee(bdi ~ bdi.pre + treatment + length + drug, \n               data = BtheB_long, id = subject, \n               family = gaussian, corstr = \"independence\")\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;    (Intercept)        bdi.pre treatmentBtheB      length&gt;6m        drugYes \n#&gt;      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ind)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Identity \n#&gt;  Variance to Mean Relation: Gaussian \n#&gt;  Correlation Structure:     Independent \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#&gt;     data = BtheB_long, family = gaussian, corstr = \"independence\")\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -21.6497810  -5.8485100   0.1131663   5.5838383  28.1871039 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Naive S.E.   Naive z Robust S.E.   Robust z\n#&gt; (Intercept)     3.5686314  1.4833349  2.405816  2.26947617  1.5724472\n#&gt; bdi.pre         0.5818494  0.0563904 10.318235  0.09156455  6.3545274\n#&gt; treatmentBtheB -3.2372285  1.1295569 -2.865928  1.77459534 -1.8242066\n#&gt; length&gt;6m       1.4577182  1.1380277  1.280916  1.48255866  0.9832449\n#&gt; drugYes        -3.7412982  1.1766321 -3.179667  1.78271179 -2.0986557\n#&gt; \n#&gt; Estimated Scale Parameter:  79.25813\n#&gt; Number of Iterations:  1\n#&gt; \n#&gt; Working Correlation\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    0    0    0\n#&gt; [2,]    0    1    0    0\n#&gt; [3,]    0    0    1    0\n#&gt; [4,]    0    0    0    1\n# require(Publish)\n# publish(btb_gee.ind)\n\n\nThis model assumes an independent correlation structure.\nThe outputs started from a summary of residuals\nThe estimated coefficient will be interpreted the same way as linear model. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation struture estimated from the data (identity matrix for indepedence)\n\nWith exchangeable correlation matrix:\n\nbtb_gee.ex &lt;- gee(bdi ~ bdi.pre + treatment + length + drug,\n                data = BtheB_long, id = subject, \n                family = gaussian, corstr = \"exchangeable\")\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;    (Intercept)        bdi.pre treatmentBtheB      length&gt;6m        drugYes \n#&gt;      3.5686314      0.5818494     -3.2372285      1.4577182     -3.7412982\nsummary(btb_gee.ex)\n#&gt; \n#&gt;  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n#&gt;  gee S-function, version 4.13 modified 98/01/27 (1998) \n#&gt; \n#&gt; Model:\n#&gt;  Link:                      Identity \n#&gt;  Variance to Mean Relation: Gaussian \n#&gt;  Correlation Structure:     Exchangeable \n#&gt; \n#&gt; Call:\n#&gt; gee(formula = bdi ~ bdi.pre + treatment + length + drug, id = subject, \n#&gt;     data = BtheB_long, family = gaussian, corstr = \"exchangeable\")\n#&gt; \n#&gt; Summary of Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -23.955980  -6.643864  -1.109741   4.257688  25.452310 \n#&gt; \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Naive S.E.     Naive z Robust S.E.   Robust z\n#&gt; (Intercept)     3.0231602 2.30390185  1.31219140  2.23204410  1.3544357\n#&gt; bdi.pre         0.6479276 0.08228567  7.87412417  0.08351405  7.7583066\n#&gt; treatmentBtheB -2.1692863 1.76642861 -1.22806339  1.73614385 -1.2494854\n#&gt; length&gt;6m      -0.1112910 1.73091679 -0.06429596  1.55092705 -0.0717577\n#&gt; drugYes        -2.9995608 1.82569913 -1.64296559  1.73155411 -1.7322940\n#&gt; \n#&gt; Estimated Scale Parameter:  81.7349\n#&gt; Number of Iterations:  5\n#&gt; \n#&gt; Working Correlation\n#&gt;           [,1]      [,2]      [,3]      [,4]\n#&gt; [1,] 1.0000000 0.6757951 0.6757951 0.6757951\n#&gt; [2,] 0.6757951 1.0000000 0.6757951 0.6757951\n#&gt; [3,] 0.6757951 0.6757951 1.0000000 0.6757951\n#&gt; [4,] 0.6757951 0.6757951 0.6757951 1.0000000\n#publish(btb_gee.ex)\n\n\nThe interpretation of estimated coefficients are similar to LM. Naive S.E and z value are estimated directly from this model. Robust SE and z are sandwich estimates\nWorking Correlation is the correlation structure estimated from the data\nWhen we change the structure of correlation, the estimates and naive SE and z changed. A closer naive SE to robust SE indicates that the correlation structure is better specified.\nCompare with Random Effects\n\nref: (Hothorn and Everitt 2014) Section 13.4 We then use the conditional models (i.e., adding random effect) with non-normal response\n\nLet us compare the GEE models with the mixed effect models.\n\n## Generalized mixed effect model model\nlibrary(\"lme4\")\nresp_lmer &lt;- glmer(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.197382 (tol = 0.002, component 1)\n\n\nrequire(afex)\nresp_afex &lt;- mixed(nstat ~ baseline + month + treatment + \n                   gender + age + centre + \n                    (1 | subject), family = binomial(), \n                  data = resp, method = \"LRT\")\n#&gt; Contrasts set to contr.sum for the following variables: baseline, month, treatment, gender, centre, subject\n#&gt; Numerical variables NOT centered on 0: age\n#&gt; If in interactions, interpretation of lower order (e.g., main) effects difficult.\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.00495329 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.00478889 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0466122 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0949357 (tol = 0.002, component 1)\n#&gt; Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n#&gt; Model failed to converge with max|grad| = 0.0416611 (tol = 0.002, component 1)\n\n\n## GEE model\nresp_gee3 &lt;- gee(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE, scale.value = 1)\n#&gt; Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n#&gt; running glm to get initial regression estimate\n#&gt;        (Intercept)       baselinegood            month.L            month.Q \n#&gt;        -0.90363465         1.88945124        -0.14372490        -0.02455122 \n#&gt;            month.C treatmenttreatment         gendermale                age \n#&gt;        -0.23255161         1.30410916         0.11969528        -0.01823703 \n#&gt;            centre2 \n#&gt;         0.67417628\nlibrary(MuMIn)\nlibrary(geepack)\nresp_gee4 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nresp_gee5 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"independence\", \n                 scale.fix = TRUE)\nresp_gee6 &lt;- geeglm(nstat ~ baseline + month + treatment + \n                   gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"ar1\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee5)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  511.14981  499.69791 -240.84895   14.72595    9.00000  512.93199\nQIC(resp_gee6)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  511.81242  500.27657 -241.13829   14.76792    9.00000  514.01242\n# Smaller QIC values for correlation structure represents better models\n# i.e., \"exchangeable\" in our case\n\nresp_gee4a &lt;- geeglm(nstat ~ month + treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4b &lt;- geeglm(nstat ~ treatment + gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4c &lt;- geeglm(nstat ~ gender + age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4d &lt;- geeglm(nstat ~ age + centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\n\nresp_gee4e &lt;- geeglm(nstat ~ centre, \n                 data = resp, family = \"binomial\", \n                 id = subject, corstr = \"exchangeable\", \n                 scale.fix = TRUE)\nQIC(resp_gee4)\n#&gt;        QIC       QICu  Quasi Lik        CIC     params       QICC \n#&gt;  510.94098  499.71690 -240.85845   14.61204    9.00000  513.14098\nQIC(resp_gee4a)[2]\n#&gt;   QICu \n#&gt; 567.35\nQIC(resp_gee4b)[2]\n#&gt;     QICu \n#&gt; 562.6247\nQIC(resp_gee4c)[2]\n#&gt;     QICu \n#&gt; 585.3099\nQIC(resp_gee4d)[2]\n#&gt;     QICu \n#&gt; 586.1581\nQIC(resp_gee4e)[2]\n#&gt;     QICu \n#&gt; 592.3437\nQIC(resp_gee4d)[2]\n#&gt;     QICu \n#&gt; 586.1581\n# Covariates are selected based on the QICu criteria\n\n\n## compare estimates (conditional vs. marginal)\nsummary(resp_lmer)$coefficients # Model failed to converge\n#&gt;                       Estimate Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)        -1.65459986 0.77620476 -2.1316538 3.303531e-02\n#&gt; baselinegood        3.08897132 0.59858148  5.1604860 2.463096e-07\n#&gt; month.L            -0.20348327 0.27957504 -0.7278306 4.667173e-01\n#&gt; month.Q            -0.02821444 0.27907489 -0.1010999 9.194712e-01\n#&gt; month.C            -0.35570944 0.28084948 -1.2665483 2.053168e-01\n#&gt; treatmenttreatment  2.16620464 0.55157208  3.9273283 8.589470e-05\n#&gt; gendermale          0.23836010 0.66606345  0.3578640 7.204451e-01\n#&gt; age                -0.02557320 0.01993984 -1.2825178 1.996611e-01\n#&gt; centre2             1.03849878 0.54182274  1.9166762 5.527908e-02\nsummary(resp_afex)$coefficients # Model failed to converge\n#&gt;                Estimate Std. Error    z value     Pr(&gt;|z|)\n#&gt; (Intercept)  1.61366355 0.79598978  2.0272415 4.263772e-02\n#&gt; baseline1   -1.55321951 0.30467553 -5.0979463 3.433581e-07\n#&gt; month1       0.21660822 0.24441676  0.8862249 3.754963e-01\n#&gt; month2      -0.17701748 0.24250584 -0.7299514 4.654199e-01\n#&gt; month3       0.21559489 0.24440444  0.8821235 3.777101e-01\n#&gt; treatment1  -1.09162957 0.28098789 -3.8849701 1.023425e-04\n#&gt; gender1     -0.10313396 0.33932981 -0.3039343 7.611780e-01\n#&gt; age         -0.02576276 0.02031589 -1.2681088 2.047591e-01\n#&gt; centre1     -0.52829986 0.27639204 -1.9114149 5.595128e-02\nsummary(resp_gee3)$coefficients # Marginalized model\n#&gt;                       Estimate Naive S.E.    Naive z Robust S.E.   Robust z\n#&gt; (Intercept)        -0.90565507 0.47940039 -1.8891413  0.46042640 -1.9669920\n#&gt; baselinegood        1.86665412 0.34220231  5.4548261  0.35023043  5.3297885\n#&gt; month.L            -0.14330949 0.18044542 -0.7941986  0.18210027 -0.7869812\n#&gt; month.Q            -0.02448267 0.18034926 -0.1357514  0.18329974 -0.1335663\n#&gt; month.C            -0.23187407 0.18073662 -1.2829391  0.15929035 -1.4556693\n#&gt; treatmenttreatment  1.28311415 0.33592099  3.8196903  0.35055323  3.6602548\n#&gt; gendermale          0.11513183 0.41851611  0.2750953  0.44135628  0.2608592\n#&gt; age                -0.01785399 0.01258158 -1.4190585  0.01299686 -1.3737157\n#&gt; centre2             0.68481139 0.34010911  2.0135050  0.35681635  1.9192265\nsummary(resp_gee4)$coefficients # Marginalized model\n\n\n  \n\n\n\nThe significance of variables are similar in both variables, but the estimated coefficients are larger in generalized mixed effect model. However, it does not mean the estimated coefficients are inconsistent. Instead, two models are estimating different parameters. Remember, the mixed effect model is conditional on random effects, while the GEE is a marginalized model.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\nReference\n\n\n\n\nFaraway, Julian J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nHothorn, Torsten, and Brian S Everitt. 2014. A Handbook of Statistical Analyses Using r. CRC press.",
    "crumbs": [
      "Longitudinal data",
      "GEE"
    ]
  },
  {
    "objectID": "longitudinalF.html",
    "href": "longitudinalF.html",
    "title": "R functions (T)",
    "section": "",
    "text": "The list of new R functions introduced in this longitudinal data analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\ngee\ngee\nTo fit a generalized estimation equation model\n\n\ngeeglm\ngeepack\nTo fit a generalized estimation equation model\n\n\nlmer\nlme4\nTo fit linear mixed effects models\n\n\nglmer\nlme4\nTo fit generalized linear mixed effects models\n\n\nmixed\nafex\nTo fit generalized linear mixed effects models\n\n\nqqnorm\nbase/stats\nTo fit a QQ plot\n\n\nranef\nlme5\nTo extract the random effects from a model\n\n\nreshape\nbase/stats\nReshape data, e.g., into wide to long or long to wide format\n\n\nresiduals\nbase/stats\nTo extract residuals of a model",
    "crumbs": [
      "Longitudinal data",
      "R functions (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html",
    "href": "longitudinalQ.html",
    "title": "Quiz (T)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html#live-quiz",
    "href": "longitudinalQ.html#live-quiz",
    "title": "Quiz (T)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "longitudinalQ.html#download-quiz",
    "href": "longitudinalQ.html#download-quiz",
    "title": "Quiz (T)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Longitudinal data",
      "Quiz (T)"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "Mediation analysis",
    "section": "",
    "text": "Background\nThis chapter provides comprehensive tutorials on mediation analysis. The Baron and Kenny approach explores non-binary outcomes through directed acyclic graphs (DAGs) and regression in big data scenarios, with a focus on both continuous and binary mediators and outcomes. The justification of mediation analysis evaluates the connection between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD), considering various covariates like BMI, smoking status, and associations with diseases like diabetes. The final mediation example centers on decomposing the total effect of OA on CVD through direct and indirect pathways via pain medication, including data preparation, weight computation, and outcome evaluation, accompanied by considerations of non-linearity and potential interactions.",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation.html#background",
    "href": "mediation.html#background",
    "title": "Mediation analysis",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation.html#overview-of-tutorials",
    "href": "mediation.html#overview-of-tutorials",
    "title": "Mediation analysis",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\n\nIn the preceding chapters, we have primarily discussed about total effect of an exposure to the outcome. In this chapter, we will discuss about decomposition of the effect in the presence of a mediator.\n\nBaron and Kenny Approach\nThe chapter, referencing the Baron and Kenny approach, delves into the analysis of non-binary outcomes using directed acyclic graphs (DAGs) and regression techniques for big data scenarios with a million observations. Initially, the chapter focuses on a continuous outcome and continuous mediator, where the true treatment effect is known. Through the data generating process, a DAG is formulated and data simulated, followed by an estimation of effects using generalized linear models. Subsequently, the Baron and Kenny approaches are applied to determine direct, total, and indirect effects. The chapter progresses to explore binary outcomes with both continuous and binary mediators, each time employing a similar approach: creating a DAG, generating data, estimating effects using regression models, and then using the Baron and Kenny methodology to elucidate the relationships.\n\n\nJustification of Mediation Analysis\nIn this chapter, the data analysis process centers on understanding the relationship between osteoarthritis (OA), pain medication, and cardiovascular disease (CVD) using a mediation analysis. Specifically, the analysis seeks to determine if OA, the exposure, is associated with an increased risk of CVD, the outcome. Additionally, it investigates whether pain medication acts as a mediator in this causal pathway. The total effect of OA on CVD risk was found to be significant. Furthermore, OA was observed to significantly influence the use of pain medication, which is the proposed mediator. To facilitate the analysis, the study considers various adjustment covariates, including demographic variables, confounders such as BMI and smoking status, and associations with other diseases like diabetes. The data used in this study is pre-processed, analyzed, and subsequently saved for further use.\n\n\nMediation Example\nIn the chapter, the focus is on decomposing the “total effect” of a given exposure, OA (\\(A\\)), on the outcome CVD (\\(Y\\)) into its natural direct effect (NDE; \\(A \\rightarrow Y\\)) and a natural indirect effect (NIE) that routes through a mediator, in this case, pain medication (\\(M\\)). Initially, the required data is loaded and preprocessed. The mediation analysis involves several steps: (1) Preparing the data and ensuring it has the necessary variables; (2) Modifying data for different exposures and duplicating it; (3) Computing weights for the mediation based on logistic regressions, where the weights are applied to factor in the mediator’s effect; (4) Building a weighted outcome model, which is a logistic regression to evaluate the outcome. To quantify these effects, the chapter derives point estimates for the total effect, direct effect, and indirect effect. Furthermore, confidence intervals for these effects are determined using bootstrap methods. The results, including the proportion mediated by pain medication, are visualized using graphs. The chapter also delves into considerations of non-linearity and potential interactions between variables.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Mediation analysis"
    ]
  },
  {
    "objectID": "mediation0.html",
    "href": "mediation0.html",
    "title": "Concepts (I)",
    "section": "",
    "text": "Mediation Analysis\nThe section provides an overview of the concept and methods of mediation analysis in the context of epidemiology and statistical modeling. The section discusses total effects, indirect and direct paths, and highlights the limitations of the Baron and Kenny approach (Baron and Kenny 1986). It introduces the counterfactual definition, emphasizing the importance of adjusting for confounders and providing insights into the mechanics of mediation analysis through imputation and weighting methods. The section also covers sensitivity analysis, proportion mediated, and methodological extensions, including multi-category mediators and software references for conducting mediation analysis.",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#reading-list",
    "href": "mediation0.html#reading-list",
    "title": "Concepts (I)",
    "section": "Reading list",
    "text": "Reading list\nKey reference: (Rochon, Bois, and Lange 2014; Lange et al. 2017)",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#video-lessons",
    "href": "mediation0.html#video-lessons",
    "title": "Concepts (I)",
    "section": "Video Lessons",
    "text": "Video Lessons\n\n\n\n\n\n\nMediation Analysis - Baron and Kenny (1986)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Mediation analysis with counterfactual definitions and why confounding adjustment helps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis mechanism under counterfactual definition (i) Outcome imputation & (ii) Weighting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediation analysis using survey data, assumptions, extensions, software and references",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#video-lesson-slides",
    "href": "mediation0.html#video-lesson-slides",
    "title": "Concepts (I)",
    "section": "Video Lesson Slides",
    "text": "Video Lesson Slides",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#links",
    "href": "mediation0.html#links",
    "title": "Concepts (I)",
    "section": "Links",
    "text": "Links\n\nGoogle Slides\nPDF Slides",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation0.html#references",
    "href": "mediation0.html#references",
    "title": "Concepts (I)",
    "section": "References",
    "text": "References\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. “The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173.\n\n\nLange, Theis, Kristoffer W Hansen, Rune Sørensen, and Søren Galatius. 2017. “Applied Mediation Analyses: A Review and Tutorial.” Epidemiology and Health 39.\n\n\nRochon, Justine, Andreas du Bois, and Theis Lange. 2014. “Mediation Analysis of the Relationship Between Institutional Research Activity and Patient Survival.” BMC Medical Research Methodology 14 (1): 9.",
    "crumbs": [
      "Mediation analysis",
      "Concepts (I)"
    ]
  },
  {
    "objectID": "mediation1.html",
    "href": "mediation1.html",
    "title": "Baron and Kenny",
    "section": "",
    "text": "Baron and Kenny (1986) approach 1\nIn Baron and Kenny approach 1 (Baron and Kenny 1986), two regression models are fitted to estimate the total effect, direct effect, and indirect effect.\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] where \\(Y\\) is the outcome, \\(A\\) is the exposure, \\(M\\) is the mediator, and \\(\\alpha, \\beta, \\gamma\\) are regression coefficients. The effects are then calculated as:\n\nTotal effect of A on Y: \\(\\hat{\\beta}_1\\)\n\nDirect effect of A on Y: \\(\\hat{\\beta}_0\\)\n\nIndirect effect of A on Y through M: \\(\\hat{\\beta}_1 - \\hat{\\beta}_0\\),\n\nwhere \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\nBaron and Kenny (1986) approach 2\nIn the second approach, three models are fitted:\n\\[ Y = \\alpha_{0} + \\beta_0 A + \\gamma_0 M, \\] \\[ Y = \\alpha_{1} + \\beta_1 A, \\] \\[ M = \\alpha_{2} + \\beta_2 A, \\]\nThe indirect effect of A on Y through M can be calculated as: \\(\\hat{\\beta}_2 \\times \\hat{\\beta}_0\\), where \\(\\hat{\\beta}\\) is estimated regression coefficient of \\(\\beta\\).\n\n# Load required packages\nrequire(simcausal)\n\nBig data: What if we had 1,000,000 (1 million) observations?\nLet us explore the mediation analysis using Baron and Kenny (1986) approaches. We first simulate a big dataset and then show the results from the mediation analysis.\nContinuous outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n    node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n    node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n    node(\"Y\", distr = \"rnorm\", mean = 0.5 * M + 1.3 * A, sd = .1)\nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.55\nfit.am &lt;- glm(Y ~ A + M, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.55\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.55\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.25\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nBinary outcome, continuous mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rnorm\", mean = 0.5 * A, sd = 1) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.00        1.48\nfit.am &lt;- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=\"gaussian\", data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.48\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.48\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.18\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nBinary outcome, binary mediator\n\nTrue treatment effect = 1.3\n\nData generating process\n\nrequire(simcausal)\nD &lt;- DAG.empty()\nD &lt;- D + \n  node(\"A\", distr = \"rnorm\", mean = 0, sd = 1) + \n  node(\"M\", distr = \"rbern\", prob = plogis(0.5 * A)) + \n  node(\"Y\", distr = \"rbern\", prob = plogis(0.5 * M + 1.3 * A)) \nDset &lt;- set.DAG(D)\n#&gt; ...automatically assigning order attribute to some nodes...\n#&gt; node A, order:1\n#&gt; node M, order:2\n#&gt; node Y, order:3\n\nGenerate DAG\n\nplotDAG(Dset, xjitter = 0.1, yjitter = .9,\n        edge_attrs = list(width = 0.5, arrow.width = 0.4, arrow.size = 0.7),\n        vertex_attrs = list(size = 12, label.cex = 0.8))\n#&gt; using the following vertex attributes:\n#&gt; 120.8NAdarkbluenone0\n#&gt; using the following edge attributes:\n#&gt; 0.50.40.7black1\n\n\n\n\n\n\n\nGenerate Data\n\nObs.Data &lt;- sim(DAG = Dset, n = 1000000, rndseed = 123)\n#&gt; simulating observed dataset from the DAG object\nhead(Obs.Data)\n\n\n  \n\n\n\nEstimate effect (beta-coef)\n\nfit &lt;- glm(Y ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit),2)\n#&gt; (Intercept)           A \n#&gt;        0.25        1.34\nfit.am &lt;- glm(Y ~ A + M, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.am),2)\n#&gt; (Intercept)           A           M \n#&gt;         0.0         1.3         0.5\nfit.m &lt;- glm(M ~ A, family=binomial(link = \"logit\"), data=Obs.Data)\nround(coef(fit.m),2)\n#&gt; (Intercept)           A \n#&gt;         0.0         0.5\n\n\n# from 1st model\na.coef &lt;- round(coef(fit),2)[2]\na.coef\n#&gt;    A \n#&gt; 1.34\n# from 2nd (adjusted) model\nam.coef &lt;- round(coef(fit.am),2)[2]\nam.coef\n#&gt;   A \n#&gt; 1.3\nm.coef &lt;- round(coef(fit.am),2)[3]\nm.coef\n#&gt;   M \n#&gt; 0.5\n# from 3rd (mediator) model\nma.coef &lt;- round(coef(fit.m),2)[2]\nma.coef\n#&gt;   A \n#&gt; 0.5\n\nBaron and Kenny (1986) approach 1\n\n# Direct effect\nam.coef\n#&gt;   A \n#&gt; 1.3\n# Total effect\na.coef\n#&gt;    A \n#&gt; 1.34\n# Indirect effect\na.coef - am.coef\n#&gt;    A \n#&gt; 0.04\n\nBaron and Kenny (1986) approach 2\n\n# Indirect effect\nm.coef * ma.coef\n#&gt;    M \n#&gt; 0.25\n\nAs we can see, the estimated indirect effects are different from the two approaches. That means Baron and Kenny’s (1986) approach doesn’t work for a non-collapsible effect measure such as the odds ratio.\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.\n\n\n\n\n\n\n\n\n\n\nBaron, Reuben M, and David A Kenny. 1986. “The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173.",
    "crumbs": [
      "Mediation analysis",
      "Baron and Kenny"
    ]
  },
  {
    "objectID": "mediation2.html",
    "href": "mediation2.html",
    "title": "Justification",
    "section": "",
    "text": "We must show enough justification to do a mediation analysis. A causal diagram would be the first step to conceptualize the mediation analysis problem hypothetically. Then, we can empirically verify where doing the mediation analysis makes sense in that particular context.\n\n# Load required packages\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\n\nPre-processing\nLoad saved data\n\nload(\"Data/mediation/cchs123pain.RData\")\n\nPrepared the data\n\nanalytic.miss$mediator &lt;- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure &lt;- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome &lt;- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nNotation\n\nOutcome (\\(Y\\)): Cardiovascular disease (CVD)\nExposure (\\(A\\)): Osteoarthritis (OA)\nMediator (\\(M\\)): Pain medication\nAdjustment covariates (\\(C\\))\nHypothesis\n\nFor total effect (TE): Is OA (\\(A\\)) associated with CVD (\\(Y\\))?\n\n\n\n\n\n\n\n\n\n\nFor mediation analysis: Does pain-medication (\\(M\\)) play a mediating role in the causal pathway between OA (\\(A\\)) and CVD (\\(Y\\))? Here, we will decompose total effect (TE) to a natural direct effect (NDE) and a natural indirect effect (NIE).\n\n\n\n\n\n\n\n\n\nAdjustment variables (\\(C\\)):\n\nDemographics\n\nage\nsex\nincome\nrace\neducation status\n\n\nImportant confounders\n\nBMI\nphysical activity\nsmoking status\nfruit and vegetable consumption\n\n\nRelation with other diseases\n\nhypertension\nCOPD\ndiabetes\n\n\nAnalysis/empirical exploration\nTotal effect\nOutcome model (weighted) is\n\\(logit [P(Y_{a}=1 | C = c] = \\theta_0 + \\theta_1 a + \\theta_3 c\\)\nSetting Design\n\nrequire(survey)\nsummary(analytic.miss$weight)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Survey design\nw.design0 &lt;- svydesign(id=~1, weights=~weight, data=analytic.miss)\nsummary(weights(w.design0))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\nsd(weights(w.design0))\n#&gt; [1] 80.34263\n\n# Subset the design\nw.design &lt;- subset(w.design0, miss == 0)\nsummary(weights(w.design))\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt;    2.053   28.480   53.218   82.472  102.860 1295.970\nsd(weights(w.design))\n#&gt; [1] 91.98946\n\n\n# Model\nTE &lt;- svyglm(outcome ~ exposure + age + sex + income + race + bmi + edu + phyact + \n               smoke + fruit + diab, design = w.design, family = quasibinomial(\"logit\"))\n# painmed is mediator; not included here.\nTE.save &lt;- exp(c(summary(TE)$coef[\"exposure\",1], \n                confint(TE)[\"exposure\",]))\nTE.save\n#&gt;             2.5 %   97.5 % \n#&gt; 1.537005 1.230735 1.919489\n\nExposure to OA has a detrimental effect on CVD risk (significant!).\nEffect on the mediators\n\nfit.m = svyglm(mediator ~ exposure + age + sex + income + race + bmi + edu +\n                 phyact + smoke + fruit + diab, design = w.design,\n               family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nmed.save &lt;- exp(c(summary(fit.m)$coef[\"exposure\",1], confint(fit.m)[\"exposure\",]))\nmed.save\n#&gt;             2.5 %   97.5 % \n#&gt; 2.428463 2.059265 2.863853\n\nExposure to OA has a substantial effect on the mediator (Pain medication) as well (significant!). Hence, it would be interesting to explore a mediation analysis to assess the mediating role.\nSave data\n\nsave(w.design, file = \"Data/mediation/cchs123painW.RData\")\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Mediation analysis",
      "Justification"
    ]
  },
  {
    "objectID": "mediation3.html",
    "href": "mediation3.html",
    "title": "Mediation Example",
    "section": "",
    "text": "# Load required packages\nknitr::opts_chunk$set(echo = TRUE)\nrequire(survey)\nrequire(DiagrammeR)\nrequire(DiagrammeRsvg)\nrequire(rsvg)\nlibrary(magrittr)\nlibrary(svglite)\nlibrary(png)\nrequire(Publish)\n\nWe want to decompose of the “total effect” of a given exposure OA (\\(A\\)) on the outcome CVD (\\(Y\\)) into\n\na natural direct effect (NDE; \\(A \\rightarrow Y\\)) and\na natural indirect effect (NIE) through a mediator pain medication (\\(M\\)) through (\\(A \\rightarrow M \\rightarrow Y\\)).\n\nStep 0: Build data first\n\nload(\"Data/mediation/cchs123pain.RData\")\nsource(\"Data/mediation/medFunc.R\")\nls()\n#&gt; [1] \"analytic.cc\"        \"analytic.miss\"      \"doEffectDecomp\"    \n#&gt; [4] \"doEffectDecomp.int\"\n\nvarlist &lt;- c(\"age\", \"sex\", \"income\", \"race\", \"bmi\", \"edu\", \"phyact\", \"smoke\", \"fruit\", \"diab\")\nanalytic.miss$mediator &lt;- ifelse(analytic.miss$painmed == \"Yes\", 1, 0)\nanalytic.miss$exposure &lt;- ifelse(analytic.miss$OA == \"OA\", 1, 0)\nanalytic.miss$outcome &lt;- ifelse(analytic.miss$CVD == \"event\", 1, 0)\n\nPre-run step 3 model\nWe will utilize this fit in step 3\n\n# A = actual exposure (without any change)\nanalytic.miss$exposureTemp &lt;- analytic.miss$exposure\n\n# Design\nw.design0 &lt;- svydesign(id=~1, weights=~weight, data=analytic.miss)\nw.design &lt;- subset(w.design0, miss == 0)\n\n# Replace exposure with exposureTemp. This will be necessary in step 3\nfit.m &lt;- svyglm(mediator ~ exposureTemp + \n                 age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab,\n                design = w.design, family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\npublish(fit.m)\n#&gt;      Variable             Units OddsRatio       CI.95     p-value \n#&gt;  exposureTemp                        2.43 [2.06;2.86]     &lt; 1e-04 \n#&gt;           age       20-29 years       Ref                         \n#&gt;                     30-39 years      1.00 [0.88;1.13]   0.9442989 \n#&gt;                     40-49 years      0.93 [0.82;1.06]   0.2651302 \n#&gt;                     50-59 years      0.66 [0.58;0.76]     &lt; 1e-04 \n#&gt;                     60-64 years      0.61 [0.51;0.72]     &lt; 1e-04 \n#&gt;               65 years and over      0.61 [0.52;0.71]     &lt; 1e-04 \n#&gt;           sex            Female       Ref                         \n#&gt;                            Male      0.50 [0.46;0.55]     &lt; 1e-04 \n#&gt;        income   $29,999 or less       Ref                         \n#&gt;                 $30,000-$49,999      1.20 [1.06;1.35]   0.0043533 \n#&gt;                 $50,000-$79,999      1.21 [1.08;1.37]   0.0014914 \n#&gt;                 $80,000 or more      1.28 [1.14;1.45]     &lt; 1e-04 \n#&gt;          race         Non-white       Ref                         \n#&gt;                           White      1.81 [1.62;2.02]     &lt; 1e-04 \n#&gt;           bmi       Underweight       Ref                         \n#&gt;                  healthy weight      1.09 [0.82;1.44]   0.5631582 \n#&gt;                      Overweight      1.33 [1.01;1.77]   0.0449616 \n#&gt;           edu          &lt; 2ndary       Ref                         \n#&gt;                       2nd grad.      1.13 [0.98;1.30]   0.1014986 \n#&gt;                 Other 2nd grad.      1.30 [1.08;1.55]   0.0050596 \n#&gt;                  Post-2nd grad.      1.25 [1.10;1.42]   0.0008252 \n#&gt;        phyact            Active       Ref                         \n#&gt;                        Inactive      1.12 [1.02;1.23]   0.0184447 \n#&gt;                        Moderate      1.12 [1.01;1.25]   0.0364592 \n#&gt;         smoke      Never smoker       Ref                         \n#&gt;                  Current smoker      1.29 [1.16;1.44]     &lt; 1e-04 \n#&gt;                   Former smoker      1.28 [1.17;1.40]     &lt; 1e-04 \n#&gt;         fruit 0-3 daily serving       Ref                         \n#&gt;               4-6 daily serving      0.92 [0.83;1.02]   0.0976967 \n#&gt;                6+ daily serving      0.80 [0.71;0.90]   0.0001979 \n#&gt;          diab                No       Ref                         \n#&gt;                             Yes      1.23 [0.99;1.52]   0.0626501\n\nStep 1 and 2: Replicate data with different exposures\nWe manipulate and duplicate data here\n\ndim(analytic.miss)\n#&gt; [1] 397173     28\ndim(analytic.cc)\n#&gt; [1] 28734    23\nnrow(analytic.miss) - nrow(analytic.cc)\n#&gt; [1] 368439\n\n# Create counterfactual data. This will be necessary in step 3\nd1 &lt;- d2 &lt;- analytic.miss\n\n# Exposed = Exposed\nd1$exposure.counterfactual &lt;- d1$exposure\n\n# Exposed = Not exposed\nd2$exposure.counterfactual &lt;- !d2$exposure \n\n# duplicated data (double the amount)\nnewd &lt;- rbind(d1, d2)\nnewd &lt;- newd[order(newd$ID), ]\ndim(newd)\n#&gt; [1] 794346     29\n\nStep 3: Compute weights for the mediation\nWeight is computed by\n\\(W^{M|C} = \\frac{P(M|A^*, C)}{P(M|A, C)}\\)\nin all data newd (fact d1 + alternative fact d2).\n\n\n\\(P(M|A, C)\\) is computed from a logistic regression of \\(M\\) on \\(A\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta_1 a + \\beta_3 c\\)\n\n\n\n\\(P(M|A^{*}, C)\\) is computed from a logistic regression of \\(M\\) on \\(A^*\\) + \\(C\\).\n\n\\(logit [P(M=1 | C = c]) = \\beta_0 + \\beta'_1 a^* + \\beta'_3 c\\)\n\n\n\n\n# First, use original exposure (all A + all A):\n# A = actual exposure (without any change)\n# A = exposure\nnewd$exposureTemp &lt;- newd$exposure\n\n# Probability of M given A + C\nw &lt;- predict(fit.m, newdata=newd, type='response') \ndirect &lt;- ifelse(newd$mediator, w, 1-w)\n\n# Second, use counterfactual exposures (all A + all !A):\n# A* = Opposite (counterfactual) values of the exposure\n# A* = exposure.counterfactual\nnewd$exposureTemp &lt;- newd$exposure.counterfactual\n\n# Probability of M given A* + C\nw &lt;- predict(fit.m, newdata=newd, type='response') \nindirect &lt;- ifelse(newd$mediator, w, 1-w)\n\n# Mediator weights\nnewd$W.mediator &lt;- indirect/direct\nsummary(newd$W.mediator)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.45    1.00    1.00    1.01    1.15    2.25  670758\nhist(newd$W.mediator)\n\n\n\n\n\n\n\nIncorporating the survey weights:\nNote: scaling can often be helpful if there exists extreme weights.\n\n# scale survey weights\n#newd$S.w &lt;- with(newd,(weight)/mean(weight))\nnewd$S.w &lt;- with(newd,weight)\nnewd$S.w[is.na(newd$S.w)]\n#&gt; numeric(0)\nsummary(newd$S.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.39   21.76   42.21   66.70   81.07 2384.98\n\n# Multiply mediator weights with scaled survey weights\nnewd$SM.w &lt;- with(newd,(W.mediator * S.w))\n\nsummary(newd$SM.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.40   24.65   46.41   73.92   88.84 3531.63  670758\ntable(newd$miss[is.na(newd$SM.w)])\n#&gt; \n#&gt;      1 \n#&gt; 670758\nnewd$SM.w[is.na(newd$SM.w)] &lt;- 0\nsummary(newd$SM.w)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     0.0     0.0     0.0    11.5     0.0  3531.6\n\nhist(newd$SM.w, main = \"\", xlab = \"Combined weights\", \n     ylab = \"Frequency\", freq = TRUE)\n\n\n\n\n\n\n\nHere all missing weights are associated with incomplete cases (miss==1)! Hence, doesn’t matter if they are missing or other value (0) in them.\nStep 4: Weighted outcome Model\nOutcome model is\n\\(logit [P(Y_{a,M(a^*)}=1 | C = c)] = \\theta_0 + \\theta_1 a + \\theta_2 a^* + \\theta_3 c\\)\nafter weighting (combination of mediator weight + sampling weight).\n\n# Outcome analysis\nw.design0 &lt;- svydesign(id=~1, weights=~SM.w, data=newd)\nw.design &lt;- subset(w.design0, miss == 0)\n\n# Fit Y on (A + A* + C)\nfit &lt;- svyglm(outcome ~ exposure + exposure.counterfactual + \n                age + sex + income + race + bmi + edu + phyact + smoke + fruit + diab, \n             design = w.design, family = binomial(\"logit\"))\n#&gt; Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nPoint estimates\nFollowing are the conditional ORs:\n\n\\(OR_{TE}(C=c) = \\exp(\\theta_1 + \\theta_2)\\)\n\\(OR_{NDE}(A=1,M=0,C=c) = \\exp(\\theta_1)\\)\n\\(OR_{NIE}(A^{*}=1,M=0,C=c) = \\exp(\\theta_2)\\)\n\n\n# total effect of A-&gt; Y + A -&gt; M -&gt; Y\nTE &lt;- exp(sum(coef(fit)[c('exposure', 'exposure.counterfactual')])) \nTE \n#&gt; [1] 1.544694\n\n# direct effect of A-&gt; Y (not through M)\nDE &lt;- exp(unname(coef(fit)['exposure']))\nDE \n#&gt; [1] 1.488554\n\n# indirect effect of A-&gt; Y (A -&gt; M -&gt; Y)\nIE &lt;- exp(unname(coef(fit)[c('exposure.counterfactual')])) \nIE \n#&gt; [1] 1.037714\n\n# Product of ORs; same as TE \nDE * IE \n#&gt; [1] 1.544694\n\n# Proportion mediated\nPM &lt;- log(IE) / log(TE)\nPM \n#&gt; [1] 0.08513902\n\nObtaining results fast\nUser-written function doEffectDecomp() (specific to OA-CVD problem):\n\ndoEffectDecomp(analytic.miss, ind = NULL, varlist = varlist)\n#&gt;         TE         DE         IE         PM \n#&gt; 1.54469393 1.48855395 1.03771444 0.08513902\n# function is provided in the appendix\n\nConfidence intervals\nStandard errors and confidence intervals are determined by bootstrap methods.\n\nrequire(boot)\n#&gt; Loading required package: boot\n#&gt; \n#&gt; Attaching package: 'boot'\n#&gt; The following object is masked from 'package:survival':\n#&gt; \n#&gt;     aml\n# I ran the computation on a 24 core computer,\n# hence set ncpus = 5 (keep some free). \n# If you have more / less cores, adjust accordingly. \n# Try parallel package to find how many cores you have.\n# library(parallel)\n# detectCores()\n# doEffectDecomp is a user-written function\n# See appendix for the function\nset.seed(504)\nbootresBin &lt;- boot(data=analytic.miss, statistic=doEffectDecomp, \n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1b &lt;- boot.ci(bootresBin,type = \"perc\",index=1)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2b &lt;- boot.ci(bootresBin,type = \"perc\",index=2)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3b &lt;- boot.ci(bootresBin,type = \"perc\",index=3)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4b &lt;- boot.ci(bootresBin,type = \"perc\",index=4)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\n# Number of bootstraps\nbootresBin$R\n#&gt; [1] 5\n\n# Total Effect\nc(bootresBin$t0[1], bootci1b$percent[4:5])\n#&gt;       TE                   \n#&gt; 1.544694 1.293208 1.894417\n\n# Direct Effect\nc(bootresBin$t0[2], bootci2b$percent[4:5])\n#&gt;       DE                   \n#&gt; 1.488554 1.303554 1.876916\n\n# Indirect Effect\nc(bootresBin$t0[3], bootci3b$percent[4:5])\n#&gt;        IE                     \n#&gt; 1.0377144 0.9738072 1.0093246\n\n# Proportion Mediated\nc(bootresBin$t0[4], bootci4b$percent[4:5])\n#&gt;          PM                         \n#&gt;  0.08513902 -0.08360848  0.01655013\n\nThe proportion mediated through pain medication was about 8.51% on the log odds ratio scale.\nVisualization for main effects\n\nrequire(plotrix)\n#&gt; Loading required package: plotrix\nTEc &lt;- c(bootresBin$t0[1], bootci1b$percent[4:5])\nDEc &lt;- c(bootresBin$t0[2], bootci2b$percent[4:5])\nIEc &lt;- c(bootresBin$t0[3], bootci3b$percent[4:5])\nmat &lt;- rbind(TEc,DEc,IEc)\ncolnames(mat) &lt;- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#&gt;        Point      2.5%    97.5%\n#&gt; TEc 1.544694 1.2932078 1.894417\n#&gt; DEc 1.488554 1.3035540 1.876916\n#&gt; IEc 1.037714 0.9738072 1.009325\n\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2], xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\n\n\n\nNon-linearity\nConsider\n\nnon-linear relationships (polynomials) and interactions between exposure, demographic/baseline covariates and mediators,\nIs misclassification of the mediators possible?\n\nHere we are again using a user-written function doEffectDecomp.int() (including interaction phyact*diab in the mediation model as well as the outcome model):\n\n# doEffectDecomp.int is a user-written function\n# See appendix for the function\ndoEffectDecomp.int(analytic.miss, ind = NULL, varlist = varlist)\n#&gt;         TE         DE         IE         PM \n#&gt; 1.54472021 1.48868864 1.03763821 0.08496674\n# try bootstrap on it?\n\nVisualization for main + interactions\n\nset.seed(504)\nbootresInt &lt;- boot(data=analytic.miss, statistic=doEffectDecomp.int,\n                R = 5, parallel = \"multicore\", ncpus=5,\n                varlist = varlist)\n\nR = 5 is not reliable for bootstrap. In real applications, try 250 at least.\n\nbootci1i &lt;- boot.ci(bootresInt,type = \"perc\",index=1)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci2i &lt;- boot.ci(bootresInt,type = \"perc\",index=2)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci3i &lt;- boot.ci(bootresInt,type = \"perc\",index=3)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\nbootci4i &lt;- boot.ci(bootresInt,type = \"perc\",index=4)\n#&gt; Warning in norm.inter(t, alpha): extreme order statistics used as endpoints\n\n\nbootresInt$R\n#&gt; [1] 5\n# from saved boostrap results: bootresInt \n# (similar as before)\nTEc &lt;- c(bootresInt$t0[1], bootci1i$percent[4:5])\nDEc &lt;- c(bootresInt$t0[2], bootci2i$percent[4:5])\nIEc &lt;- c(bootresInt$t0[3], bootci3i$percent[4:5])\nmat&lt;- rbind(TEc,DEc,IEc)\ncolnames(mat) &lt;- c(\"Point\", \"2.5%\", \"97.5%\")\nmat\n#&gt;        Point      2.5%    97.5%\n#&gt; TEc 1.544720 1.2931358 1.893575\n#&gt; DEc 1.488689 1.3040953 1.876521\n#&gt; IEc 1.037638 0.9742042 1.009088\nplotCI(1:3, mat[,1], ui=mat[,3], li=mat[,2],\n       xlab = \"Estimates\", ylab = \"\", xaxt=\"n\")\naxis(1, at=1:3,labels=c(\"TE\",\"NDE\",\"NIE\"))\nabline(h=1, lty = 2)\n\n\n\n\n\n\n\nAppendix: OA-CVD Functions for bootstrap\nThese functions are written basically for performing bootstrap for the OA-CVD analysis. However, changing the covariates names/model-specifications should not be too hard, once you understand the basic steps.\n\n# without interactions (binary mediator)\ndoEffectDecomp\n#&gt; function (dat, ind = NULL, varlist) \n#&gt; {\n#&gt;     if (is.null(ind)) \n#&gt;         ind &lt;- 1:nrow(dat)\n#&gt;     d &lt;- dat[ind, ]\n#&gt;     d$mediator &lt;- ifelse(as.character(d$painmed) == \"Yes\", 1, \n#&gt;         0)\n#&gt;     d$exposure &lt;- ifelse(as.character(d$OA) == \"OA\", 1, 0)\n#&gt;     d$outcome &lt;- ifelse(as.character(d$CVD) == \"event\", 1, 0)\n#&gt;     d$exposureTemp &lt;- d$exposure\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~weight, data = d)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit.m &lt;- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp  + \"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     d1 &lt;- d2 &lt;- d\n#&gt;     d1$exposure.counterfactual &lt;- d1$exposure\n#&gt;     d2$exposure.counterfactual &lt;- !d2$exposure\n#&gt;     newd &lt;- rbind(d1, d2)\n#&gt;     newd &lt;- newd[order(newd$ID), ]\n#&gt;     newd$exposureTemp &lt;- newd$exposure\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     direct &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$exposureTemp &lt;- newd$exposure.counterfactual\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     indirect &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$W.mediator &lt;- indirect/direct\n#&gt;     summary(newd$W.mediator)\n#&gt;     newd$S.w &lt;- with(newd, weight)\n#&gt;     summary(newd$S.w)\n#&gt;     newd$SM.w &lt;- with(newd, (W.mediator * S.w))\n#&gt;     newd$SM.w[is.na(newd$SM.w)] &lt;- 0\n#&gt;     summary(newd$SM.w)\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit &lt;- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     TE &lt;- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#&gt;     DE &lt;- exp(unname(coef(fit)[\"exposure\"]))\n#&gt;     IE &lt;- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#&gt;     PM &lt;- log(IE)/log(TE)\n#&gt;     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#&gt; }\n#&gt; &lt;bytecode: 0x0000022e10416e10&gt;\n\n# with interactions (binary mediator)\ndoEffectDecomp.int\n#&gt; function (dat, ind = NULL, varlist) \n#&gt; {\n#&gt;     if (is.null(ind)) \n#&gt;         ind &lt;- 1:nrow(dat)\n#&gt;     d &lt;- dat[ind, ]\n#&gt;     d$exposureTemp &lt;- d$exposure\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~weight, data = d)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit.m &lt;- svyglm(as.formula(paste0(paste0(\"mediator ~ exposureTemp + phyact*diab +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     d1 &lt;- d2 &lt;- d\n#&gt;     d1$exposure.counterfactual &lt;- d1$exposure\n#&gt;     d2$exposure.counterfactual &lt;- !d2$exposure\n#&gt;     newd &lt;- rbind(d1, d2)\n#&gt;     newd &lt;- newd[order(newd$ID), ]\n#&gt;     newd$exposureTemp &lt;- newd$exposure\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     direct &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$exposureTemp &lt;- newd$exposure.counterfactual\n#&gt;     w &lt;- predict(fit.m, newdata = newd, type = \"response\")\n#&gt;     indirect &lt;- ifelse(newd$mediator, w, 1 - w)\n#&gt;     newd$W.mediator &lt;- indirect/direct\n#&gt;     summary(newd$W.mediator)\n#&gt;     newd$S.w &lt;- with(newd, weight)\n#&gt;     summary(newd$S.w)\n#&gt;     newd$SM.w &lt;- with(newd, (W.mediator * S.w))\n#&gt;     newd$SM.w[is.na(newd$SM.w)] &lt;- 0\n#&gt;     summary(newd$SM.w)\n#&gt;     w.design0 &lt;- svydesign(id = ~1, weights = ~SM.w, data = newd)\n#&gt;     w.design &lt;- subset(w.design0, miss == 0)\n#&gt;     fit &lt;- svyglm(as.formula(paste0(paste0(\"outcome ~ exposure + exposure.counterfactual +\"), \n#&gt;         paste0(varlist, collapse = \"+\"))), design = w.design, \n#&gt;         family = quasibinomial(\"logit\"))\n#&gt;     TE &lt;- exp(sum(coef(fit)[c(\"exposure\", \"exposure.counterfactual\")]))\n#&gt;     DE &lt;- exp(unname(coef(fit)[\"exposure\"]))\n#&gt;     IE &lt;- exp(unname(coef(fit)[c(\"exposure.counterfactual\")]))\n#&gt;     PM &lt;- log(IE)/log(TE)\n#&gt;     return(c(TE = TE, DE = DE, IE = IE, PM = PM))\n#&gt; }\n#&gt; &lt;bytecode: 0x0000022ec5e867f0&gt;\n\nVideo content (optional)\n\n\n\n\n\n\nTip\n\n\n\nFor those who prefer a video walkthrough, feel free to watch the video below, which offers a description of an earlier version of the above content.",
    "crumbs": [
      "Mediation analysis",
      "Mediation Example"
    ]
  },
  {
    "objectID": "mediationF.html",
    "href": "mediationF.html",
    "title": "R functions (I)",
    "section": "",
    "text": "The list of new R functions introduced in this mediation analysis lab component are below:\n\n\n\n\n\nFunction_name\nPackage_name\nUse\n\n\n\nboot\nboot\nTo conduct bootstrap resampling\n\n\nboot.ci\nboot\nTo calculate confidence intervals from bootstrap samples",
    "crumbs": [
      "Mediation analysis",
      "R functions (I)"
    ]
  },
  {
    "objectID": "mediationQ.html",
    "href": "mediationQ.html",
    "title": "Quiz (I)",
    "section": "",
    "text": "Live Quiz\nYou can click on the live quiz options to either see hints of why your response may be incorrect (response marked in red), or whether your choice of response is correct (response marked in green):",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "mediationQ.html#live-quiz",
    "href": "mediationQ.html#live-quiz",
    "title": "Quiz (I)",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support iframes.&lt;/p&gt;",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "mediationQ.html#download-quiz",
    "href": "mediationQ.html#download-quiz",
    "title": "Quiz (I)",
    "section": "Download Quiz",
    "text": "Download Quiz\n\nDownloading the File:\n\nNavigate to the link: See here.\nRight-click on the link and select “Save link as…” from the dropdown menu.\nAlternatively click here for downloading the quizzes.\nChoose a destination folder on your computer where you’d like to save the file (e.g., Desktop). Remember this location, as you’ll need to navigate to it later.\n\nSetting Up RStudio:\n\nIf you don’t have RStudio installed, see the download link in here.\nLaunch RStudio after installing.\n\nInstalling Necessary R Packages:\n\nBefore running the R Markdown file, ensure you have the required packages. In RStudio’s console (located at the bottom left by default), enter the following commands to install them:\ninstall.packages(\"learnr\")\ninstall.packages(\"xfun\")\n\nOpening and Running the File:\n\nIn RStudio, go to File &gt; Open File....\nNavigate to the folder where you saved the Rmd file and select it to open.\nOnce the file is open in RStudio, you’ll see a “Run Document” button (green) at the top of the script editor. Click on it to run the R Markdown Quiz file.",
    "crumbs": [
      "Mediation analysis",
      "Quiz (I)"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "Writing tools",
    "section": "",
    "text": "Background\nThis tutorial provides a step-by-step guide to using R, RStudio, GitHub, and GitHub Desktop for collaborative manuscript development. These tools enable version control, streamline teamwork, and enhance documentation throughout the research and writing process. Additionally, it introduces various supplementary resources, including LaTex, TablesGenerator, and Zotero, to further assist and streamline the research and writing processes.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting.html#background",
    "href": "reporting.html#background",
    "title": "Writing tools",
    "section": "",
    "text": "Important\n\n\n\nDatasets:\nAll of the datasets used in this tutorial can be accessed from this GitHub repository folder\nIf you’re new to R Markdown, consider reviewing the introductory tutorial.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting.html#overview-of-tutorials",
    "href": "reporting.html#overview-of-tutorials",
    "title": "Writing tools",
    "section": "Overview of tutorials",
    "text": "Overview of tutorials\n\nGit and GitHub\nThis tutorial introduces the fundamentals of Git and GitHub as essential tools for version control and collaborative research. It explains how Git tracks changes over time, enabling reproducibility and accountability in manuscript writing, while GitHub serves as a cloud-based platform to host and manage Git repositories.\n\n\nConfigure Git in RStudio\nLearn how to configure Git by linking your GitHub credentials to RStudio. This is a necessary step for making authenticated changes to remote repositories.\n\n\nCreate and Clone a Git Repository\nUnderstand how to create a repository on GitHub and “clone” it locally. This step sets up the shared workspace for manuscript development.\n\n\nUpdating Repository Contents from RStudio\nGain familiarity with essential Git commands—pull, commit, and push—for synchronizing changes and managing version control across collaborators.\n\n\nFormatting Resources\nThis section provides a wealth of tools and platforms beneficial for researchers and writers in managing and creating scientific documents. It encompasses LaTex and ShareLaTeX for document preparation, TablesGenerator for converting tables from MS Word to various formats, and R packages like “officer” and “flextable” for generating tables and charts. It also introduces draw.io for crafting flow charts, platforms like jane for identifying suitable journals, officetimeline for creating Gantt charts, and Google Docs for real-time collaborative writing. Moreover, it highlights Zotero and ZoteroBib as comprehensive tools for reference management and bibliography creation, facilitating organized, collaborative, and streamlined research and writing processes.\n\n\n\n\n\n\nTip\n\n\n\nOptional Content:\nYou’ll find that some sections conclude with an optional video walkthrough that demonstrates the code. Keep in mind that the content might have been updated since these videos were recorded. Watching these videos is optional.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBug Report:\nFill out this form to report any issues with the tutorial.",
    "crumbs": [
      "Writing tools"
    ]
  },
  {
    "objectID": "reporting1.html",
    "href": "reporting1.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Git is a free and open-source version control system that tracks changes made to documents over time. It allows you to revisit earlier versions of your manuscript, methods, or scripts—essential for reproducibility and collaboration.\nGitHub is a cloud-based hosting platform for Git repositories. It is widely used by researchers and developers to facilitate collaboration, version tracking, and code sharing. GitHub features such as Copilot, Issues, branching, and GitHub Pages provide powerful tools for managing projects and showcasing your work.\nThis tutorial series will introduce the fundamentals of using Git and GitHub in conjunction with RStudio. Topics include setting up Git locally, cloning repositories, pushing updates to GitHub, and best practices for managing collaborative work.\nFor more in-depth guidance, consult the UBC Library Git & GitHub workshop or the textbook Happy Git and GitHub for the useR.\n\nVideo Content (Optional)\n\n\n\n\n\n\nTip\n\n\n\nPrefer a video walkthrough? Watch the tutorial below for an overview of collaborative writing with GitHub.\n\n\n\n\n\n\n\n\nUseful Resources\n\nManuscript template (view output)\nUBC thesis template in R Markdown\nApply for a GitHub student license\nFree Tableau license for students\nSet up GitHub Copilot in RStudio\nVideo: Using ChatGPT and Copilot in RStudio\nVideo: Creating and managing GitHub branches\nVideo: Resolving Git conflicts in RStudio\nData Management in Large-Scale Education Research by Crystal Lewis\nusethis package documentation\nIntro Git & GitHub workshop – UBC Library\nChapter 8 of Data Management in Large-Scale Education Research for README best practices\nManaging Personal Access Tokens\nGit command-line setup\n.gitignore configuration\nChoosing a license for your project",
    "crumbs": [
      "Writing tools",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "reporting1a.html",
    "href": "reporting1a.html",
    "title": "Configure Git in RStudio",
    "section": "",
    "text": "After creating a GitHub account, you must configure Git on your local machine to enable authentication. This process, known as “configuring Git,” ensures that Git operations (e.g., initialize, commit, push, pull) can securely interact with GitHub’s API. Enabling two-factor authentication (2FA) is strongly recommended for added security.\nThis section walks you through setting up Git in RStudio using the usethis package, a user-friendly interface for configuring Git. Every usethis function corresponds to an underlying Git command. For more detail, refer to the usethis documentation and UBC’s Git and GitHub workshop.\nAll example files are available here. If you’re new to R Markdown, review the introductory tutorial.\n\nRequirements and Necessary Software\n\nGitHub Account: Click “Sign up” and follow the instructions.\nR and RStudio (see previous chapter)\nGitHub Desktop: Install and sign in.\n\n\n\nConfigure Git in RStudio\n\nOpen Tools &gt; Global Options in RStudio.\n\nNavigate to the “Git/SVN” section on the left panel.\nConfirm that the “Git executable” field correctly identifies the Git installation path.\n\n\nInstall required packages:\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\nSet your Git identity:\nusethis::use_git_config(\n  user.name = \"Jane\",           # Replace with your full name\n  user.email = \"jane@example.com\"  # Use your GitHub email\n)\nVerify your configuration:\nusethis::git_sitrep()\n\n\nYour Git name doesn’t need to match your GitHub username. Use your full name for clarity. If you use multiple computers, consider including the device name for easier identification.\n\n\nAuthenticate GitHub with a Personal Access Token (PAT). PATs are GitHub’s secure alternative to passwords.\n\nGenerate a token:\nusethis::create_github_token()\nThis opens a GitHub page in your browser with recommended scopes pre-selected. Describe the token’s purpose clearly (e.g., “SPPH VM” or “Personal Laptop”).\nClick Generate token, copy the token, and keep the tab open.\nStore the token:\ngitcreds::gitcreds_set()\nIf a previous PAT exists, this command allows you to inspect and update it.\n\n\n\n\nUseful Resources\n\nusethis setup guide\nUBC Git & GitHub workshop\nManaging PATs securely\nGit command-line setup",
    "crumbs": [
      "Writing tools",
      "Configure Git in RStudio"
    ]
  },
  {
    "objectID": "reporting1b.html",
    "href": "reporting1b.html",
    "title": "Create and Clone a Git Repository",
    "section": "",
    "text": "This tutorial provides a step-by-step guide for creating repositories on GitHub and cloning them to your local computer. GitHub repositories are remote storage spaces that allow you to track script changes, collaborate with colleagues, and document your research process. Public repositories can also be cloned by others to facilitate reproducibility and collaboration.\n\nRequirements and Necessary Software\n\nGitHub Account: Click “Sign up” and follow the instructions.\nR and RStudio (see previous chapter)\nGitHub Desktop: Install and sign in.\n\n\n\nCreating a Repository on GitHub\n\nSign in to GitHub. Click the “+” icon at the top right and select New repository.\n\nEnter repository details:\n\nName your repository and provide a brief description.\n\nChoose public or private visibility.\n\nSelect initialization options.\n\n\nA README is recommended for basic project documentation. For formatting tips, see Chapter 8 of Data Management in Large-Scale Education Research.\n.gitignore lets you exclude specific files from version control (e.g., .Rhistory or local .csv files). See Git’s documentation.\nA license defines how others may use your code. The MIT License is commonly used. For guidance, visit choosealicense.com.\n\nClick Create repository to finish.\n\n\n\n\n\nCloning a Repository via Git Bash\n\nVisit the repository you want to clone.\n\nExample: Advanced Epidemiological Methods by Dr. Ehsan Karim.\n\nClick the green &lt; &gt; Code button and copy the HTTPS link.\n\n\nOpen Git Bash.\n\nNavigate to the target directory:\n\nUse pwd to check your current path.\n\nUse cd to move to your preferred folder.\n\n\n\nThe path after ~ shows your current location.\n\nRun the clone command:\ngit clone https://github.com/ehsanx/EpiMethods.git\n\n\n\n\nCloning a Repository via RStudio\n\nGo to the GitHub repository and copy the HTTPS URL.\n\n\nOpen RStudio: File &gt; New Project\n\nSelect Version Control.\n\n\nThen choose Git.\n\n\nComplete the Git project setup:\n\n\nPaste the HTTPS link under Repository URL.\nOptionally, edit the Project directory name.\nClick Browse to choose a folder (e.g., your Desktop).\n\nClick Create Project to finish.\n\nThe cloned repository will now be available in your selected folder.\n\n\n\n\nUseful Resources\n\nBest practices for structuring a repository and writing a README\n.gitignore guide\nChoosing a license\nUBC Library Git & GitHub workshop\nManaging Personal Access Tokens",
    "crumbs": [
      "Writing tools",
      "Create and Clone a Git Repository"
    ]
  },
  {
    "objectID": "reporting1c.html",
    "href": "reporting1c.html",
    "title": "Updating Repository Contents from RStudio",
    "section": "",
    "text": "This section outlines how to update the contents of a cloned GitHub repository using RStudio. It also introduces key Git commands and common workflows for version control and collaboration.\n\nExample Git Workflow in RStudio\nGit is used through a series of commands. The most commonly used are:\n\npull: Retrieves the most recent version of the repository, ensuring all local files are up to date.\n\n\nAlways begin your session by pulling changes to prevent conflicts. See this video for guidance on resolving merge issues.\n\n\ncommit: Records a snapshot of changes made to local files.\n\n\nWrite clear, concise commit messages to help collaborators understand the purpose of each change.\n\n\npush: Uploads committed changes to the remote repository on GitHub.\n\n\nConflicts may arise if changes contradict those already made in the repository. See this video for resolving conflicts in RStudio.\n\nAdditional Git commands and advanced options are available in the Git documentation.\n\n\nRequirements and Necessary Software\n\nA GitHub account\nR and RStudio (refer to previous chapter)\nGitHub Desktop\n\n\n\nExample: Modify a Test Repository\n\nClone the repository and locate the Git tab in RStudio.\n\nPull the latest version of the repository.\n\nEdit the README file and commit changes.\n\n\n\n\n\nStage the modified files, ensure you are working on the main branch, and add a descriptive commit message.\n\nConfirm the commit.\n\n\n\nPush the committed changes to GitHub.\n\n\nVerify the changes on the GitHub repository page.\n\n\n\n\nBookdown Template in RStudio\n\nGo to File &gt; New Project &gt; New Directory &gt; Book Project using bookdown\nName your project (e.g., \"test1\")\nCopy project files to your main repository folder\nRename the project file as appropriate (e.g., \"template\")\n\n\n\nCompiling\n\nOpen index.Rmd\nModify the YAML header and section titles to match your content\nCompile using the Build tab in RStudio\n\n\n\nPublishing Using GitHub Pages\n\nCreate a folder named docs and move compiled HTML files there\nIn GitHub repository settings, set Pages source to the docs folder\nYour site will be available at username.github.io/repository_name\n\n\n\nInvite Collaborators\nNavigate to your repository &gt; Settings &gt; Manage Access &gt; Invite a collaborator.\nEnter the user’s name or email, assign appropriate access permissions, and send the invitation.\n\n\nUseful Resources\n\nManuscript template (view output)\nUBC thesis template\nGitHub Copilot in RStudio\nSetup video\nManaging branches\nConflict resolution in RStudio",
    "crumbs": [
      "Writing tools",
      "Updating Repository Contents from RStudio"
    ]
  },
  {
    "objectID": "reporting2.html",
    "href": "reporting2.html",
    "title": "Formatting Tools",
    "section": "",
    "text": "The tutorial elucidates a variety of tools and methodologies aimed at streamlining and enhancing academic writing and presentation creation, discussing topics such as utilizing a typesetting system, converting tables across different formats, employing various R packages for enhanced data visualization and presentation, drawing flow and Gantt charts, crafting HTML5 presentations, enabling collaborative writing and document sharing, managing references efficiently, formatting articles, and employing specific platforms for identifying appropriate journals for publishing academic articles.\nLaTex\nLaTeX is a high-quality software system for typesetting document. LaTeX is designed for the production of technical and scientific documentation, such as book, articles.\nGet an account in ShareLaTeX/Overleaf.\nTable conversion\nFrom MS word to latex / markdown / HTML in TablesGenerator.\nYou can use tableone package to generate csv file, and then import them in the TablesGenerator to convert to HTML to paste to doc file!\n\ntab1x &lt;- print(tab1, quote = FALSE, noSpaces = TRUE, printToggle = FALSE)\nwrite.csv(tab1x, file = \"tab1x.csv\")\n\nFancy table and chart generators:\nR Packages\n\nofficer\nofficedown\nflextable\nmschart\nDrawing flow chart\ndraw.io\nPresentation\nThe “xaringan” package, derived from a love for the Japanese manga and anime “Naruto”, serves as an R Markdown extension, and it facilitates the creation of distinctively styled HTML5 presentations by leveraging the JavaScript library remark.js. Originating from an intent to produce a unique, though not widely adopted style, due to its potentially challenging pronunciation unless familiar with the anime, “xaringan” offers significant customizability in presentation design and has garnered additional theme contributions from its user community. Despite only supporting Markdown, “xaringan” enhances remark.js by introducing support for R Markdown and additional utilities, simplifying the slide-building and previewing processes. Further insights into “xaringan”, its background, and its utility can be explored through its documentation.\nGantt charts\nofficetimeline\nSimultaneous collaborative writing\nGoogle Docs offers a platform for real-time collaborative writing. Multiple users can edit documents simultaneously, and changes are saved automatically. Note that Google Docs requires sign-in for a Google account.\n\nCommenting and Suggesting: Use the comment and suggest features to provide feedback without altering the original text.\nRevision History: Navigate through the revision history to view changes and revert to previous versions if needed.\nSharing and Permissions: Manage who can view, comment, or edit the document with varied permission levels.\nReference manager\nZotero stands out as a free, open-source reference management software that assists researchers, academics, and students in organizing, managing, and formatting their citations and bibliographies. It’s not just a reference manager but also a powerful tool for collaborative work on research projects. Try the Zotero desktop manager as well for assisting with reference inserting.\nZotero syncs data across devices, ensuring that users can access their libraries from any location. Users can work offline with Zotero, and any changes made will be synchronized when the internet connection is restored.\nZoteroBib: Use ZoteroBib to generate bibliographies instantly without creating an account or installing software.\nArticle formatting\nThe rticles package in R provides a diverse selection of templates for creating academic articles and is easily accessible directly within the RStudio environment by navigating through File -&gt; New File -&gt; R Markdown, where users can select their desired template. For users not utilizing RStudio, the installation of Pandoc is requisite, with articles being creatable using the rmarkdown::draft() function, and specifying the template and package parameters as needed. Additionally, the package enables viewing a list of available journal names using rticles::journals(). To employ enhanced features, such as automatic figure numbering and cross-referencing of tables, users can utilize functionalities from the bookdown package. This involves adjusting the YAML to use bookdown::pdf_book as the output format, and designating the chosen rticles template as the base_format. Comprehensive details and tutorials regarding the use of the rticles package can be found in its online documentation. The complete array of options can be explored within the R Markdown templates window in RStudio, via the packages’s GitHub readme or accessed programmatically via the following function:\n\nrticles::journals()\n#&gt;  [1] \"acm\"            \"acs\"            \"aea\"            \"agu\"           \n#&gt;  [5] \"ajs\"            \"amq\"            \"ams\"            \"arxiv\"         \n#&gt;  [9] \"asa\"            \"bioinformatics\" \"biometrics\"     \"copernicus\"    \n#&gt; [13] \"ctex\"           \"elsevier\"       \"frontiers\"      \"glossa\"        \n#&gt; [17] \"ieee\"           \"ims\"            \"informs\"        \"iop\"           \n#&gt; [21] \"isba\"           \"jasa\"           \"jedm\"           \"joss\"          \n#&gt; [25] \"jss\"            \"lipics\"         \"lncs\"           \"mdpi\"          \n#&gt; [29] \"mnras\"          \"oup_v0\"         \"oup_v1\"         \"peerj\"         \n#&gt; [33] \"pihph\"          \"plos\"           \"pnas\"           \"rjournal\"      \n#&gt; [37] \"rsos\"           \"rss\"            \"sage\"           \"sim\"           \n#&gt; [41] \"springer\"       \"tf\"             \"trb\"            \"wellcomeor\"\n\nFind appropriate journals\n\njane\nSee more extensive list here\n\nSpecific Epidemiology-focus journals\n\nSummary\n\n\nCategory\nTool\nDescription\n\n\n\nAcademic Search Engine\nGoogle Scholar\nFreely accessible search engine indexing scholarly literature.\n\n\nArticle Formatting\nrticles (R Package)\nR package providing templates for various academic journals.\n\n\nBrainstorming & Mapping\nMindMeister\nOnline mind mapping tool for brainstorming and collaborative visualization.\n\n\nDocument Creation & Editing\nOverleaf\nCollaborative LaTeX editor online.\n\n\n\nGoogle Docs\nPlatform for real-time collaborative writing.\n\n\n\nofficer (R Package)\nR package for generating Word and PowerPoint files.\n\n\n\nofficedown (R Package)\nR package to produce Word documents with officer.\n\n\nGantt Chart Generators\nOffice Timeline\nOnline tool for creating Gantt charts.\n\n\nJournal Finding\njane\nTool to assist in finding the right journal for publishing.\n\n\n\nCustom List\nExtensive list and guide on finding suitable journals.\n\n\n\nEpidemiology Journals\nSpecific list of epidemiology-focus journals.\n\n\nNote & Research Management\nEvernote\nNote-taking and organization tool for managing research notes and drafts.\n\n\nPresentation & Sharing\nPrezi\nDynamic and visually engaging presentation creation tool.\n\n\n\nSlideShare\nPlatform for sharing presentations and professional documents.\n\n\nPresentation\nxaringan (R Package)\nR Markdown extension for creating presentations using remark.js.\n\n\nProject Management\nAsana\nProject management tool for workflow organization and collaboration.\n\n\n\nTrello\nProject management tool for task tracking and collaboration.\n\n\nReference Management\nEndNote\nReference management software for organizing and integrating references.\n\n\n\nJabRef\nOpen-source bibliography reference manager using BibTeX.\n\n\n\nMendeley\nReference management and academic social network.\n\n\n\nPaperpile\nReference management and academic research library.\n\n\n\nZotero\nReference management and collaborative tool.\n\n\n\nZoteroBib\nQuick bibliography generation tool.\n\n\nResearch Identity Management\nORCID\nProvides a persistent digital identifier to distinguish researchers.\n\n\nTable & Chart Generators\ndraw.io\nOnline diagram software for flow charts and various diagrams.\n\n\n\nTablesGenerator\nConverts tables to LaTeX, markdown, HTML formats.\n\n\n\nflextable (R Package)\nR package for tabular reporting in various formats (Word, HTML, etc.).\n\n\n\nmschart (R Package)\nR package to create PowerPoint charts.\n\n\nWriting & Editing\nAuthorea\nCollaborative platform for writing, citing, and publishing.\n\n\n\nGrammarly\nWriting assistant for grammar and style enhancement.",
    "crumbs": [
      "Writing tools",
      "Formatting Tools"
    ]
  },
  {
    "objectID": "confounding9.html#reporting-guideline",
    "href": "confounding9.html#reporting-guideline",
    "title": "Interaction",
    "section": "Reporting guideline",
    "text": "Reporting guideline\nThe NHANES example illustrates that, under saturation, the joint-variable and interaction-term parameterizations are equivalent on the multiplicative scale. The joint-variable model directly presents the set of contrasts most often recommended for reporting, while the interaction model provides the traditional interaction coefficient and requires transformation to recover the same joint effects. Either model supports simple effects within strata. Reporting both joint and selected simple effects, together with additive interaction measures, provides a more complete summary.",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  },
  {
    "objectID": "confounding9.html#key-messages",
    "href": "confounding9.html#key-messages",
    "title": "Interaction",
    "section": "Key messages",
    "text": "Key messages\n\nUnder saturation, joint and interaction models encode the same multiplicative contrasts.\nPresent the full set of joint effects against a single reference profile.\nInclude selected simple effects and additive interaction measures for clarity.",
    "crumbs": [
      "Causal roles",
      "Interaction"
    ]
  }
]