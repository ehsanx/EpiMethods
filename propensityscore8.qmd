## PS weighting with MI for subpopulation {.unnumbered}

```{r setup, warning=FALSE, message=FALSE, cache=TRUE, include=FALSE}
# Load required packages
library(dplyr)
library(kableExtra)
library(tableone)
library(survey)
library(Publish)
library(DataExplorer)
library(mice)
library(mitools)
library(MatchIt)
library(optmatch)
library(data.table)
```

### Problem

In this chapter, we will use **propensity score (PS) weighting** with **multiple imputation (MI)**, focusing on specific subpopulations defined by the study's eligibility criteria. Similar to the previous chapter on [PSM with MI for subpopulation](propensityscore7.html), the modified dataset from NHANES 2017- 2018, will be used. 

### Load data

Let us import the dataset:

```{r load, cache=TRUE}
rm(list = ls())
load("Data/propensityscore/analytic_imputed.RData")
ls()
```

-   `dat.full`: Full dataset of 9,254 subjects
-   `dat.analytic` and `dat.analytic2`: Analytic dataset of 4,191 participants with only missing in the covariates. There are no values for the exposure or outcomes.
-   `imputation`: m = 5 imputed datasets from `dat.analytic2` using MI.

The **general strategy of solution** to implement PS weighting with MI is as follows:

-   We will build the imputation model on 4,191 eligible subjects.
-   Apply PS weghting on each of the imputed datasets, where we will utilize survey features for population-level estimate
-   Pool the estimates using Rubin's rule

### Dealing with missing values in covariates

#### Step 1: Imputing missing values using mice for eligible subjects
We already completed this step in the [previous chapter](propensityscore7.html), where we imputed m = 5 datasets using MI.

Now we will combine 5 datasets and create a stacked dataset. This dataset should contain 5\*4,191 = 20,955 rows.

```{r step1b, cache=TRUE}
# Stacked imputed dataset
impdata <- mice::complete(imputation, action="long")
dim(impdata)

#Remove .id variable from the model as it was created in an intermediate step
impdata$.id <- NULL

# Missing after imputation
DataExplorer::plot_missing(impdata)
```

#### Step 2: PS weighting steps 1-3 by DuGoff et al. (2014)
Our next step is to use steps 1-3 of the PS weighting analysis: 

-   Step 2.1: Fit the PS model by considering survey features as covariates. 
-   Step 2.2: Calculate PS weights 
-   Step 2.3: Balance checking using SMD. Consider SMD <0.2 as a good covariate balancing.

##### Step 2.1: PS model specification

```{r duGstep1, cache=TRUE}
# Specify the PS model to estimate propensity scores
ps.formula <- as.formula(I(arthritis == "Rheumatoid arthritis") ~ age.cat + sex + 
                           education + race + income + bmi + smoking + htn + 
                           diabetes + psu + strata + survey.weight)
```

##### Step 2.2: Estimating PS and calculating weights 

```{r duGstep2, cache=TRUE}
dat.ps <- list(NULL)

m <- 5 # 5 imputed dataset

# PS weighting on each of the imputed datasets
for (ii in 1:m) {
  # Imputed dataset
  dat.imputed <- subset(impdata, .imp == ii)
  
  # Propensity scores
  ps.fit <- glm(ps.formula, data = dat.imputed, family = binomial("logit"))
  dat.imputed$ps <- fitted(ps.fit)
  
  # Stabilized weight
  dat.imputed$sweight <- with(dat.imputed, 
                              ifelse(I(arthritis == "Rheumatoid arthritis"), 
                                     mean(I(arthritis == "Rheumatoid arthritis"))/ps, 
                                     (1-mean(I(arthritis == "Rheumatoid arthritis")))/(1-ps)))

  # Dataset
  dat.ps[[ii]] <- dat.imputed
}

# Weight summary
purrr::map_df(dat.ps, function(df){summary(df$sweight)})
```

```{r duGstep2b, cache=TRUE}
# Dimension of each of the imputed dataset
lapply(dat.ps, dim)
```

##### Step 2.3: Balance checking for each imputed dataset

Now we will check balance in terms of SMD on each dataset.

```{r duGstep3, cache=TRUE}
tab1m <- list(NULL)
for (ii in 1:m) {
  # PS weighted imputed data
  dat <- dat.ps[[ii]]
  
  # Covariates
  vars <- c("age.cat", "sex", "education", "race", "income", "bmi", "smoking", 
            "htn", "diabetes")
  
  # Design with truncated stabilized weight
  wdesign <- svydesign(ids = ~studyid, weights = ~sweight, data = dat)
  
  # Balance checking 
  tab1m[[ii]] <- svyCreateTableOne(vars = vars, strata = "arthritis", data = wdesign,
                                   test = F)
}
print(tab1m, smd = TRUE)
```

For each of the datasets, all SMDs except for smoking are less than our specified cut-point of 0.2. We will adjust our outcome model for smoking. 

#### Step 3: Outcome modelling 
Our next step is to fit the outcome model on each of the imputed dataset. Note that, we must utilize survey features to correctly estimate the standard error. For this step, we will multiply PS weight and survey weight and create a `new weight` variable. 

##### 3.1 Calculating new weights

```{r step3aWgt, cache=TRUE}
dat.ps2 <- list(NULL)

for (ii in 1:m) {
  # PS weighted imputed data
  dat <- dat.ps[[ii]]
  
  # New weight = survey weight * PS weight 
  dat$new_weight <- with(dat, survey.weight * sweight)
  
  dat.ps2[[ii]] <- dat
}
```

##### 3.2 Preparing dataset for ineligible subjects

Now we will add the **ineligible subjects**(ineligible by study restriction) with the PS weighted datasets, so that we can set up the survey design on the full dataset and then subset the design. 

Let us subset the data for ineligible subjects: 

```{r step3a, cache=TRUE}
# Subset for ineligible
dat.ineligible <- list(NULL)

for(ii in 1:m){
  # Matched dataset
  dat <- dat.ps[[ii]]
  
  # Create an indicator variable in the full dataset
  dat.full$ineligible <- 1
  dat.full$ineligible[dat.full$studyid %in% dat$studyid] <- 0
  
  # Subset for ineligible
  dat.ineligible[[ii]] <- subset(dat.full, ineligible == 1)
  
  # Create the .imp variable on each dataset with .imp 1 to m = 5
  dat.ineligible[[ii]]$.imp <- ii
}

# Dimension of each dataset
lapply(dat.ineligible, dim)
```

The next step is combining matched and ineligible datasets. Before merging, we must ensure the variable names are the same.

```{r step3b, cache=TRUE}
# Variables in the matched datasets
names(dat.ps2[[3]])

# Variables in the ineligible datasets
names(dat.ineligible[[3]])
```

```{r step3c, cache=TRUE}
dat.ineligible2 <- list(NULL)

for (ii in 1:m) {
  dat <- dat.ineligible[[ii]]
  
  # Drop the ineligible variable from the dataset
  dat$ineligible <- NULL
  
  # Create ps and sweight
  dat$ps <- NA
  dat$sweight <- NA
  dat$new_weight <- NA
  
  # Keep only those variables available in the matched dataset
  vars <- names(dat.ps2[[1]])
  dat <- dat[,vars]
  
  # Ineligible datasets in list format
  dat.ineligible2[[ii]] <- dat
}
```

##### 3.2 Combining eligible (imputed and PS weighted) and ineligible (unimputed) subjects

Now, we will merge imputed eligible and unimputed ineligible subjects. We should have m = 5 copies of the full dataset with 9,254 subjects on each.

```{r step3d, cache=TRUE}
dat.full2 <- list(NULL)

for (ii in 1:m) {
  # Eligible
  d1 <- data.frame(dat.ps2[[ii]])
  d1$eligible <- 1
  
  # Ineligible
  d2 <- data.frame(dat.ineligible2[[ii]])
  d2$eligible <- 0
  
  # Full data
  d3 <- rbind(d1, d2)
  
  #  New weight variable in the full dataset
  d3$new_weight <- 0
  d3$new_weight[d3$studyid %in% d1$studyid] <- d1$new_weight
  
  # Full data in list format
  dat.full2[[ii]] <- d3
}
lapply(dat.full2, dim)

# Stacked dataset
dat.stacked <- rbindlist(dat.full2)
dim(dat.stacked)
```

##### 3.3 Prepating Survey design and subpopulation of eligible

The next step is to create the design on the combined dataset. Make sure to use the `new weight` variable that combines survey weights and PS weights. 

```{r step33, cache=TRUE}
allImputations <- imputationList(lapply(1:m, function(n) subset(dat.stacked, subset=.imp==n)))

# Design on full data
w.design0 <- svydesign(ids = ~psu, 
                       weights = ~new_weight, 
                       strata = ~strata,
                      data = allImputations, 
                      nest = TRUE) 

# Subset the design
w.design <- subset(w.design0, eligible == 1) 
```

We can see the length of the subsetted design:

```{r step33b, cache=TRUE}
lapply(w.design$designs, dim)
```

Now we will run the design-adjusted logistic regression, adjusting for smoking since smoking was not balanced in terms of SMD. 

##### Step 3.4: Design adjusted regression analysis

```{r step34, cache=TRUE}
# Design-adjusted logistic regression
fit <- with(w.design, svyglm(I(cvd == "Yes") ~ arthritis + smoking, family = quasibinomial))
res <- exp(as.data.frame(cbind(coef(fit[[1]]),
                               coef(fit[[2]]),
                               coef(fit[[3]]),
                               coef(fit[[4]]),
                               coef(fit[[5]]))))
names(res) <- paste("OR from m =", 1:m)
round(t(res),2)
```

##### Step 3.5: Pooling estimates

Now, we will  pool the estimate using Rubin's rule:

```{r step35, cache=TRUE}
# Pooled estimate
pooled.estimates <- MIcombine(fit)
OR <- round(exp(pooled.estimates$coefficients), 2)
OR <- as.data.frame(OR)
CI <- round(exp(confint(pooled.estimates)), 2)
OR <- cbind(OR, CI)
OR
```

##### Double adjustment

```{r Ausdoubleadj, warning=F, message=F, cache=TRUE}
# Outcome model with covariates adjustment
fit2 <- with(w.design, svyglm(I(cvd == "Yes") ~ arthritis + age.cat + sex + education + 
                               race + income + bmi + smoking + htn + diabetes, 
                             family = quasibinomial))

# Pooled estimate
pooled.estimates <- MIcombine(fit2)
OR <- round(exp(pooled.estimates$coefficients), 2)
OR <- as.data.frame(OR)
CI <- round(exp(confint(pooled.estimates)), 2)
OR <- cbind(OR, CI)
OR
```

### References